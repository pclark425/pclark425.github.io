<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Inductive Bias and Modality Adaptation Theory (Information Bottleneck Formulation) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1264</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1264</p>
                <p><strong>Name:</strong> Structural Inductive Bias and Modality Adaptation Theory (Information Bottleneck Formulation)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal representation for converting graphs into text for language model training is one that acts as an information bottleneck, transmitting only the graph information that is both relevant to the downstream task and compatible with the language model's inductive biases. Such representations should minimize extraneous structural detail while maximizing the preservation of task-relevant, model-compatible information, thereby yielding optimal generalization and sample efficiency.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Information Bottleneck Law for Graph-to-Text (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; maximizes &#8594; mutual information with task-relevant graph features<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph-to-text representation &#8594; minimizes &#8594; irrelevant or model-incompatible information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; achieves &#8594; optimal generalization and sample efficiency</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Information bottleneck theory in deep learning shows that compressing representations to task-relevant features improves generalization. </li>
    <li>Empirical results in AMR-to-text and code summarization show that removing irrelevant graph details improves model performance. </li>
    <li>Language models are known to have inductive biases favoring certain sequential or hierarchical structures; representations aligned with these biases are more efficiently learned. </li>
    <li>Overly detailed or noisy graph representations can lead to overfitting and reduced transferability in language models. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> While the information bottleneck is known, its application to graph-to-text for language models, with explicit focus on model compatibility, is novel.</p>            <p><strong>What Already Exists:</strong> Information bottleneck theory is established in deep learning, and some work applies it to representation learning.</p>            <p><strong>What is Novel:</strong> Application of the information bottleneck principle specifically to graph-to-text conversion for language models, with explicit focus on model-compatible information.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby et al. (2000) The Information Bottleneck Method [general information bottleneck theory]</li>
    <li>Voita et al. (2019) Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned [pruning irrelevant information in language models]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in AMR-to-Text Generation [removing irrelevant graph details]</li>
</ul>
            <h3>Statement 1: Task-Driven Structural Pruning Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; prunes &#8594; graph substructures not relevant to the downstream task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; improves &#8594; generalization and robustness</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Pruning of non-task-relevant nodes and edges in AMR and code graphs leads to better text generation and summarization. </li>
    <li>Language models trained on pruned representations are less prone to overfitting and spurious correlations. </li>
    <li>Empirical studies show that removing extraneous graph details can improve both accuracy and robustness to distributional shift. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law generalizes and formalizes a practice that is used heuristically, but not previously stated as a predictive law.</p>            <p><strong>What Already Exists:</strong> Task-driven pruning is used in some graph-to-text pipelines, but not formalized as a law.</p>            <p><strong>What is Novel:</strong> The law formalizes pruning as a necessary condition for optimal representation, grounded in information bottleneck theory.</p>
            <p><strong>References:</strong> <ul>
    <li>Ribeiro et al. (2020) Structural Encoding in AMR-to-Text Generation [pruning in AMR-to-text]</li>
    <li>Tishby et al. (2000) The Information Bottleneck Method [theoretical foundation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Graph-to-text representations that explicitly minimize mutual information with irrelevant graph features will outperform those that do not, even if the latter preserve more of the original graph structure.</li>
                <li>Task-specific pruning of graph substructures prior to text conversion will improve language model robustness to spurious correlations.</li>
                <li>Representations that align with the language model's inductive biases (e.g., sequentiality, hierarchy) will yield better sample efficiency.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>For highly entangled or densely connected graphs, aggressive pruning may lead to loss of essential information, potentially degrading performance.</li>
                <li>If language models develop new inductive biases (e.g., via architectural changes), the optimal information bottleneck may shift, requiring new forms of pruning or compression.</li>
                <li>In multi-task or transfer learning settings, retaining some task-irrelevant information may improve downstream performance.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If representations that retain all graph information (including irrelevant details) outperform pruned, task-focused representations, the information bottleneck law would be challenged.</li>
                <li>If pruning task-irrelevant substructures does not improve generalization or robustness, the task-driven pruning law would be undermined.</li>
                <li>If language models trained on maximally compressed representations underperform those trained on richer representations, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where task-irrelevant graph details are later found to be useful for transfer learning or multi-task settings. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory adapts and extends existing information bottleneck principles to a new domain, with explicit focus on language model compatibility.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby et al. (2000) The Information Bottleneck Method [theoretical foundation]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in AMR-to-Text Generation [pruning in graph-to-text]</li>
    <li>Voita et al. (2019) Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned [pruning in language models]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structural Inductive Bias and Modality Adaptation Theory (Information Bottleneck Formulation)",
    "theory_description": "This theory posits that the ideal representation for converting graphs into text for language model training is one that acts as an information bottleneck, transmitting only the graph information that is both relevant to the downstream task and compatible with the language model's inductive biases. Such representations should minimize extraneous structural detail while maximizing the preservation of task-relevant, model-compatible information, thereby yielding optimal generalization and sample efficiency.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Information Bottleneck Law for Graph-to-Text",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "maximizes",
                        "object": "mutual information with task-relevant graph features"
                    },
                    {
                        "subject": "graph-to-text representation",
                        "relation": "minimizes",
                        "object": "irrelevant or model-incompatible information"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "achieves",
                        "object": "optimal generalization and sample efficiency"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Information bottleneck theory in deep learning shows that compressing representations to task-relevant features improves generalization.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical results in AMR-to-text and code summarization show that removing irrelevant graph details improves model performance.",
                        "uuids": []
                    },
                    {
                        "text": "Language models are known to have inductive biases favoring certain sequential or hierarchical structures; representations aligned with these biases are more efficiently learned.",
                        "uuids": []
                    },
                    {
                        "text": "Overly detailed or noisy graph representations can lead to overfitting and reduced transferability in language models.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Information bottleneck theory is established in deep learning, and some work applies it to representation learning.",
                    "what_is_novel": "Application of the information bottleneck principle specifically to graph-to-text conversion for language models, with explicit focus on model-compatible information.",
                    "classification_explanation": "While the information bottleneck is known, its application to graph-to-text for language models, with explicit focus on model compatibility, is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Tishby et al. (2000) The Information Bottleneck Method [general information bottleneck theory]",
                        "Voita et al. (2019) Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned [pruning irrelevant information in language models]",
                        "Ribeiro et al. (2020) Structural Encoding in AMR-to-Text Generation [removing irrelevant graph details]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Task-Driven Structural Pruning Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "prunes",
                        "object": "graph substructures not relevant to the downstream task"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "improves",
                        "object": "generalization and robustness"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Pruning of non-task-relevant nodes and edges in AMR and code graphs leads to better text generation and summarization.",
                        "uuids": []
                    },
                    {
                        "text": "Language models trained on pruned representations are less prone to overfitting and spurious correlations.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that removing extraneous graph details can improve both accuracy and robustness to distributional shift.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Task-driven pruning is used in some graph-to-text pipelines, but not formalized as a law.",
                    "what_is_novel": "The law formalizes pruning as a necessary condition for optimal representation, grounded in information bottleneck theory.",
                    "classification_explanation": "The law generalizes and formalizes a practice that is used heuristically, but not previously stated as a predictive law.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Ribeiro et al. (2020) Structural Encoding in AMR-to-Text Generation [pruning in AMR-to-text]",
                        "Tishby et al. (2000) The Information Bottleneck Method [theoretical foundation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Graph-to-text representations that explicitly minimize mutual information with irrelevant graph features will outperform those that do not, even if the latter preserve more of the original graph structure.",
        "Task-specific pruning of graph substructures prior to text conversion will improve language model robustness to spurious correlations.",
        "Representations that align with the language model's inductive biases (e.g., sequentiality, hierarchy) will yield better sample efficiency."
    ],
    "new_predictions_unknown": [
        "For highly entangled or densely connected graphs, aggressive pruning may lead to loss of essential information, potentially degrading performance.",
        "If language models develop new inductive biases (e.g., via architectural changes), the optimal information bottleneck may shift, requiring new forms of pruning or compression.",
        "In multi-task or transfer learning settings, retaining some task-irrelevant information may improve downstream performance."
    ],
    "negative_experiments": [
        "If representations that retain all graph information (including irrelevant details) outperform pruned, task-focused representations, the information bottleneck law would be challenged.",
        "If pruning task-irrelevant substructures does not improve generalization or robustness, the task-driven pruning law would be undermined.",
        "If language models trained on maximally compressed representations underperform those trained on richer representations, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where task-irrelevant graph details are later found to be useful for transfer learning or multi-task settings.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some large language models show improved performance with richer, more detailed representations, even when some details are not directly task-relevant.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Multi-task or transfer learning scenarios may require retention of more graph information than single-task settings.",
        "Graphs with high redundancy may tolerate more aggressive pruning without loss of performance.",
        "Language models with atypical inductive biases may require different forms of representation."
    ],
    "existing_theory": {
        "what_already_exists": "Information bottleneck theory is established, and pruning is used in practice, but not formalized for graph-to-text for language models.",
        "what_is_novel": "The explicit application of the information bottleneck and task-driven pruning as predictive laws for graph-to-text representation in language model training.",
        "classification_explanation": "The theory adapts and extends existing information bottleneck principles to a new domain, with explicit focus on language model compatibility.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Tishby et al. (2000) The Information Bottleneck Method [theoretical foundation]",
            "Ribeiro et al. (2020) Structural Encoding in AMR-to-Text Generation [pruning in graph-to-text]",
            "Voita et al. (2019) Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned [pruning in language models]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-612",
    "original_theory_name": "Structural Inductive Bias and Modality Adaptation Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Structural Inductive Bias and Modality Adaptation Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
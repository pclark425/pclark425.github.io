<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emergent Symbolic Reasoning via Subsymbolic Representations in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1061</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1061</p>
                <p><strong>Name:</strong> Emergent Symbolic Reasoning via Subsymbolic Representations in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.</p>
                <p><strong>Description:</strong> This theory proposes that language models develop emergent symbolic reasoning capabilities for spatial puzzles by constructing and manipulating subsymbolic representations that encode the logical structure of the puzzle. Through training, the model's internal activations come to represent abstract entities (e.g., rows, columns, boxes) and their relationships, enabling the model to simulate symbolic reasoning steps (such as elimination and deduction) without explicit symbolic programming.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Emergent Entity Representation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; model &#8594; is trained on &#8594; spatial puzzles with explicit rules</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; internal activations &#8594; encode &#8594; abstract entities and relations (e.g., rows, columns, boxes)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Probing studies reveal neurons or activation patterns corresponding to puzzle entities. </li>
    <li>LLMs can generalize to puzzles with similar but not identical structures, suggesting abstract representation. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> No prior theory formalizes LLMs' symbolic reasoning as emergent from subsymbolic entity representations.</p>            <p><strong>What Already Exists:</strong> Emergent representations in neural networks are known; symbolic reasoning in LLMs is hypothesized but not formalized.</p>            <p><strong>What is Novel:</strong> The explicit mapping of symbolic reasoning steps to subsymbolic entity representations in LLMs for spatial puzzles is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Emergent representations, not symbolic reasoning]</li>
    <li>Xu et al. (2023) Can Language Models Solve Sudoku? [Empirical, not theoretical]</li>
</ul>
            <h3>Statement 1: Subsymbolic Simulation of Symbolic Reasoning Steps (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; internal activations &#8594; encode &#8594; entities and their relations<span style="color: #888888;">, and</span></div>
        <div>&#8226; model &#8594; is prompted to solve &#8594; spatial puzzle</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; simulates &#8594; symbolic reasoning steps (e.g., elimination, deduction) via activation dynamics</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can perform multi-step logical deductions in puzzles without explicit symbolic programming. </li>
    <li>Activation trajectories during puzzle solving resemble stepwise reasoning. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> No prior theory formalizes this mapping for LLMs in spatial puzzles.</p>            <p><strong>What Already Exists:</strong> LLMs' ability to perform multi-step reasoning is observed; symbolic reasoning is a classical AI concept.</p>            <p><strong>What is Novel:</strong> The claim that LLMs simulate symbolic reasoning via subsymbolic activation dynamics is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Bubeck et al. (2023) Sparks of Artificial General Intelligence [General LLM reasoning, not formalized for symbolic steps]</li>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Emergent representations, not symbolic reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If internal activations are analyzed, clusters corresponding to puzzle entities (e.g., rows, columns) will be found.</li>
                <li>If a model is trained on puzzles with new entity types, new activation patterns will emerge corresponding to those entities.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained on puzzles with recursive or self-referential rules, it may develop higher-order entity representations.</li>
                <li>If symbolic reasoning steps are disrupted (e.g., by activation noise), the model's performance may degrade in a stepwise fashion.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If no entity-specific activation patterns are found, the theory is challenged.</li>
                <li>If the model cannot simulate multi-step reasoning in puzzles, the theory is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs sometimes solve puzzles with non-symbolic or statistical shortcuts, which may not be fully explained by this theory. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No prior theory formalizes this mapping for LLMs in spatial puzzles.</p>
            <p><strong>References:</strong> <ul>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Emergent representations, not symbolic reasoning]</li>
    <li>Bubeck et al. (2023) Sparks of Artificial General Intelligence [General LLM reasoning, not formalized for symbolic steps]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Emergent Symbolic Reasoning via Subsymbolic Representations in LLMs",
    "theory_description": "This theory proposes that language models develop emergent symbolic reasoning capabilities for spatial puzzles by constructing and manipulating subsymbolic representations that encode the logical structure of the puzzle. Through training, the model's internal activations come to represent abstract entities (e.g., rows, columns, boxes) and their relationships, enabling the model to simulate symbolic reasoning steps (such as elimination and deduction) without explicit symbolic programming.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Emergent Entity Representation Law",
                "if": [
                    {
                        "subject": "model",
                        "relation": "is trained on",
                        "object": "spatial puzzles with explicit rules"
                    }
                ],
                "then": [
                    {
                        "subject": "internal activations",
                        "relation": "encode",
                        "object": "abstract entities and relations (e.g., rows, columns, boxes)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Probing studies reveal neurons or activation patterns corresponding to puzzle entities.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can generalize to puzzles with similar but not identical structures, suggesting abstract representation.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Emergent representations in neural networks are known; symbolic reasoning in LLMs is hypothesized but not formalized.",
                    "what_is_novel": "The explicit mapping of symbolic reasoning steps to subsymbolic entity representations in LLMs for spatial puzzles is new.",
                    "classification_explanation": "No prior theory formalizes LLMs' symbolic reasoning as emergent from subsymbolic entity representations.",
                    "likely_classification": "new",
                    "references": [
                        "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Emergent representations, not symbolic reasoning]",
                        "Xu et al. (2023) Can Language Models Solve Sudoku? [Empirical, not theoretical]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Subsymbolic Simulation of Symbolic Reasoning Steps",
                "if": [
                    {
                        "subject": "internal activations",
                        "relation": "encode",
                        "object": "entities and their relations"
                    },
                    {
                        "subject": "model",
                        "relation": "is prompted to solve",
                        "object": "spatial puzzle"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "simulates",
                        "object": "symbolic reasoning steps (e.g., elimination, deduction) via activation dynamics"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can perform multi-step logical deductions in puzzles without explicit symbolic programming.",
                        "uuids": []
                    },
                    {
                        "text": "Activation trajectories during puzzle solving resemble stepwise reasoning.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs' ability to perform multi-step reasoning is observed; symbolic reasoning is a classical AI concept.",
                    "what_is_novel": "The claim that LLMs simulate symbolic reasoning via subsymbolic activation dynamics is new.",
                    "classification_explanation": "No prior theory formalizes this mapping for LLMs in spatial puzzles.",
                    "likely_classification": "new",
                    "references": [
                        "Bubeck et al. (2023) Sparks of Artificial General Intelligence [General LLM reasoning, not formalized for symbolic steps]",
                        "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Emergent representations, not symbolic reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If internal activations are analyzed, clusters corresponding to puzzle entities (e.g., rows, columns) will be found.",
        "If a model is trained on puzzles with new entity types, new activation patterns will emerge corresponding to those entities."
    ],
    "new_predictions_unknown": [
        "If a model is trained on puzzles with recursive or self-referential rules, it may develop higher-order entity representations.",
        "If symbolic reasoning steps are disrupted (e.g., by activation noise), the model's performance may degrade in a stepwise fashion."
    ],
    "negative_experiments": [
        "If no entity-specific activation patterns are found, the theory is challenged.",
        "If the model cannot simulate multi-step reasoning in puzzles, the theory is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs sometimes solve puzzles with non-symbolic or statistical shortcuts, which may not be fully explained by this theory.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models fail to generalize to puzzles with novel entity types, suggesting limits to emergent symbolic reasoning.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Puzzles with purely statistical or pattern-based solutions may not require symbolic reasoning.",
        "Very large or deeply recursive puzzles may exceed the model's representational capacity."
    ],
    "existing_theory": {
        "what_already_exists": "Emergent representations in neural networks are known; symbolic reasoning in LLMs is hypothesized but not formalized.",
        "what_is_novel": "The explicit mapping of symbolic reasoning steps to subsymbolic entity representations in LLMs for spatial puzzles is new.",
        "classification_explanation": "No prior theory formalizes this mapping for LLMs in spatial puzzles.",
        "likely_classification": "new",
        "references": [
            "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Emergent representations, not symbolic reasoning]",
            "Bubeck et al. (2023) Sparks of Artificial General Intelligence [General LLM reasoning, not formalized for symbolic steps]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.",
    "original_theory_id": "theory-599",
    "original_theory_name": "Latent World-State Representation Emergence in Autoregressive Language Models for Board Games",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
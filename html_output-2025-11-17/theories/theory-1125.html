<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory of Training Data Logical Coverage for Language Model Reasoning - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1125</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1125</p>
                <p><strong>Name:</strong> Theory of Training Data Logical Coverage for Language Model Reasoning</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that the ability of language models to perform strict logical reasoning is fundamentally limited by the logical coverage and diversity present in their training data. It asserts that LMs generalize logical rules only to the extent that such rules are represented, varied, and unambiguously labeled in the data, and that gaps or biases in logical coverage lead to systematic reasoning failures, regardless of model size or architecture.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Logical Coverage Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; training data &#8594; contains &#8594; diverse and explicit logical rules and counterexamples</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_generalize &#8594; strict logical reasoning to novel cases</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs trained on synthetic datasets with explicit logical rules (e.g., logic puzzles, theorem proving) generalize better to novel logic tasks. </li>
    <li>Empirical studies show that LMs trained on natural text with sparse logical content fail on strict logic tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing ML principles, the focus on logical coverage and counterexamples is novel.</p>            <p><strong>What Already Exists:</strong> Data-driven generalization is a core principle in machine learning.</p>            <p><strong>What is Novel:</strong> The law specifically ties logical reasoning generalization to the explicit logical coverage in training data, not just data volume.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [data-driven generalization]</li>
    <li>Evans et al. (2018) Can neural networks understand logical entailment? [logical generalization in LMs]</li>
    <li>Piotrowski et al. (2023) Synthetic data for logical reasoning in language models [synthetic logic data improves reasoning]</li>
</ul>
            <h3>Statement 1: Logical Bias Amplification Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; training data &#8594; has_biases_or_gaps &#8594; in logical rule coverage</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; exhibits &#8594; systematic logical reasoning failures or biases</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs trained on data with missing or biased logical rules (e.g., only positive examples) fail to generalize to negative or edge cases. </li>
    <li>Studies show that LMs often overfit to spurious patterns in logic tasks when training data is unbalanced. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The application to strict logical reasoning and explicit prediction of systematic logical failures is novel.</p>            <p><strong>What Already Exists:</strong> Bias amplification from training data is known in ML.</p>            <p><strong>What is Novel:</strong> The law applies this specifically to logical reasoning, predicting systematic logical errors from data gaps.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhao et al. (2017) Men also like shopping: Reducing gender bias amplification using corpus-level constraints [bias amplification in ML]</li>
    <li>Evans et al. (2018) Can neural networks understand logical entailment? [logical generalization and failure modes]</li>
    <li>Piotrowski et al. (2023) Synthetic data for logical reasoning in language models [data bias and logical errors]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Augmenting LM training data with diverse, explicit logical rules and counterexamples will improve strict logical reasoning accuracy.</li>
                <li>LMs trained on data with logical gaps (e.g., missing negative examples) will systematically fail on corresponding logic tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There may exist a minimal logical curriculum that, if included in training, enables near-perfect logical generalization in LMs.</li>
                <li>Unsupervised pretraining on massive data may not yield strict logical reasoning unless logical coverage is sufficient.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs generalize strict logical reasoning to novel cases not covered in training data, the theory is challenged.</li>
                <li>If LMs trained on biased or incomplete logical data do not exhibit systematic logical errors, the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs show limited logical generalization even with sparse logical data, possibly due to architectural inductive biases. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The focus on logical coverage and explicit counterexamples in training data is a novel extension.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [data-driven generalization]</li>
    <li>Evans et al. (2018) Can neural networks understand logical entailment? [logical generalization in LMs]</li>
    <li>Piotrowski et al. (2023) Synthetic data for logical reasoning in language models [synthetic logic data improves reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Theory of Training Data Logical Coverage for Language Model Reasoning",
    "theory_description": "This theory posits that the ability of language models to perform strict logical reasoning is fundamentally limited by the logical coverage and diversity present in their training data. It asserts that LMs generalize logical rules only to the extent that such rules are represented, varied, and unambiguously labeled in the data, and that gaps or biases in logical coverage lead to systematic reasoning failures, regardless of model size or architecture.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Logical Coverage Law",
                "if": [
                    {
                        "subject": "training data",
                        "relation": "contains",
                        "object": "diverse and explicit logical rules and counterexamples"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_generalize",
                        "object": "strict logical reasoning to novel cases"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs trained on synthetic datasets with explicit logical rules (e.g., logic puzzles, theorem proving) generalize better to novel logic tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that LMs trained on natural text with sparse logical content fail on strict logic tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Data-driven generalization is a core principle in machine learning.",
                    "what_is_novel": "The law specifically ties logical reasoning generalization to the explicit logical coverage in training data, not just data volume.",
                    "classification_explanation": "While related to existing ML principles, the focus on logical coverage and counterexamples is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [data-driven generalization]",
                        "Evans et al. (2018) Can neural networks understand logical entailment? [logical generalization in LMs]",
                        "Piotrowski et al. (2023) Synthetic data for logical reasoning in language models [synthetic logic data improves reasoning]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Logical Bias Amplification Law",
                "if": [
                    {
                        "subject": "training data",
                        "relation": "has_biases_or_gaps",
                        "object": "in logical rule coverage"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "exhibits",
                        "object": "systematic logical reasoning failures or biases"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs trained on data with missing or biased logical rules (e.g., only positive examples) fail to generalize to negative or edge cases.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that LMs often overfit to spurious patterns in logic tasks when training data is unbalanced.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Bias amplification from training data is known in ML.",
                    "what_is_novel": "The law applies this specifically to logical reasoning, predicting systematic logical errors from data gaps.",
                    "classification_explanation": "The application to strict logical reasoning and explicit prediction of systematic logical failures is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhao et al. (2017) Men also like shopping: Reducing gender bias amplification using corpus-level constraints [bias amplification in ML]",
                        "Evans et al. (2018) Can neural networks understand logical entailment? [logical generalization and failure modes]",
                        "Piotrowski et al. (2023) Synthetic data for logical reasoning in language models [data bias and logical errors]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Augmenting LM training data with diverse, explicit logical rules and counterexamples will improve strict logical reasoning accuracy.",
        "LMs trained on data with logical gaps (e.g., missing negative examples) will systematically fail on corresponding logic tasks."
    ],
    "new_predictions_unknown": [
        "There may exist a minimal logical curriculum that, if included in training, enables near-perfect logical generalization in LMs.",
        "Unsupervised pretraining on massive data may not yield strict logical reasoning unless logical coverage is sufficient."
    ],
    "negative_experiments": [
        "If LMs generalize strict logical reasoning to novel cases not covered in training data, the theory is challenged.",
        "If LMs trained on biased or incomplete logical data do not exhibit systematic logical errors, the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs show limited logical generalization even with sparse logical data, possibly due to architectural inductive biases.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Very large LMs sometimes solve logic tasks with little explicit logical data, suggesting emergent capabilities.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Architectures with strong inductive biases (e.g., symbolic modules) may generalize logic beyond data coverage.",
        "Tasks with shallow logic or strong statistical cues may not require explicit logical coverage."
    ],
    "existing_theory": {
        "what_already_exists": "Data-driven generalization and bias amplification are known in ML.",
        "what_is_novel": "The theory applies these principles specifically to strict logical reasoning and predicts systematic logical failures from data gaps.",
        "classification_explanation": "The focus on logical coverage and explicit counterexamples in training data is a novel extension.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [data-driven generalization]",
            "Evans et al. (2018) Can neural networks understand logical entailment? [logical generalization in LMs]",
            "Piotrowski et al. (2023) Synthetic data for logical reasoning in language models [synthetic logic data improves reasoning]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-603",
    "original_theory_name": "Emergent Reasoning Thresholds and Modularization Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Symbolic-Probabilistic Bridging Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-112</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-112</p>
                <p><strong>Name:</strong> Symbolic-Probabilistic Bridging Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about probabilistic symbolic world models for text environments that integrate LLM uncertainty into planning, based on the following results.</p>
                <p><strong>Description:</strong> LLMs can serve as effective translators between natural language and formal symbolic representations (PDDL, ASP, probabilistic programs, temporal logic), but direct LLM-based planning is unreliable for tasks requiring deep combinatorial search. The most effective architectures separate concerns: LLMs handle language-to-symbol translation while dedicated symbolic planners or probabilistic inference engines handle search and reasoning. When LLM output uncertainty is explicitly modeled (via sampling, grammar-constrained decoding, or probabilistic programs) and integrated into planning (via belief states, SMC, or probabilistic world models), systems achieve higher robustness and can handle partial observability, ambiguous language, and epistemic reasoning. Systems that combine LLM translation with formal symbolic planners consistently outperform pure LLM-based planning approaches, with the largest gains on tasks requiring >5 step lookahead, logical consistency, or optimality guarantees.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>LLMs excel at language-to-symbol translation but fail at reliable combinatorial search and planning requiring >5 step lookahead</li>
                <li>Separating translation (LLM) from planning (symbolic solver) yields higher success rates (often 2-6x improvement) than end-to-end LLM planning</li>
                <li>Classical symbolic planners provide guarantees (soundness, completeness, optimality) that LLMs cannot reliably achieve</li>
                <li>Explicitly modeling LLM output uncertainty (via sampling, grammar constraints, or probabilistic programs) and integrating it into planning improves robustness to ambiguous inputs and partial observability</li>
                <li>Probabilistic symbolic world models that integrate LLM uncertainty enable belief-state planning, epistemic reasoning, and better handling of sub-optimal or failed plans</li>
                <li>The bottleneck in LLM+symbolic pipelines is typically translation accuracy rather than planner capability; contextual examples (few-shot prompting) are critical for reliable translation</li>
                <li>Continuous/soft belief representations can be more robust to prediction errors than discrete symbolic representations in learned world models</li>
                <li>Grammar-constrained decoding and SMC methods can produce calibrated posterior distributions over LLM outputs, enabling principled uncertainty propagation</li>
                <li>Systems that model planning-level uncertainty (search noise, limited budgets) explain human behavior better than systems modeling only action-selection noise</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>LLM+P uses GPT-4 to translate natural language into PDDL and achieves 90% success on BlocksWorld vs 15-30% for direct LLM planning; similar patterns across 7 domains <a href="../results/extraction-result-974.html#e974.0" class="evidence-link">[e974.0]</a> <a href="../results/extraction-result-1001.html#e1001.0" class="evidence-link">[e1001.0]</a> </li>
    <li>GPT-3+ASP achieves 99.99% accuracy on bAbI by using LLM for semantic parsing and ASP for reasoning, vs 80.34% for pure GPT-3; 100% on CLUTRR-S vs lower for pure LLM <a href="../results/extraction-result-1004.html#e1004.0" class="evidence-link">[e1004.0]</a> </li>
    <li>LLM-acquired PDDL with classical planners achieves ~95% success vs <50% for LLM-modulo-planner with validation feedback <a href="../results/extraction-result-971.html#e971.0" class="evidence-link">[e971.0]</a> <a href="../results/extraction-result-971.html#e971.2" class="evidence-link">[e971.2]</a> </li>
    <li>Tree of Thoughts (LLM-only deliberation) shows near-zero success on most planning domains while LLM+P succeeds <a href="../results/extraction-result-1001.html#e1001.3" class="evidence-link">[e1001.3]</a> </li>
    <li>AutoTAMP using LLM translation to STL plus formal planner achieves ~83.5% success on HouseWorld2, ~79.7% on Rover <a href="../results/extraction-result-975.html#e975.0" class="evidence-link">[e975.0]</a> </li>
    <li>Generalized planning with GPT-4 synthesizing Python programs achieves 0.90-1.00 success on multiple domains (Delivery, Forest, Gripper, Ferry) when combined with automated debugging <a href="../results/extraction-result-973.html#e973.0" class="evidence-link">[e973.0]</a> </li>
    <li>SIPS (probabilistic symbolic world model with SMC) outperforms BIRL in speed and accuracy on complex domains; models planning-level uncertainty rather than just action noise <a href="../results/extraction-result-1006.html#e1006.0" class="evidence-link">[e1006.0]</a> <a href="../results/extraction-result-1006.html#e1006.7" class="evidence-link">[e1006.7]</a> </li>
    <li>NIPE using LLM translation to probabilistic programs achieves R=0.927 correlation with human judgments vs R=0.658 for GPT-4 direct and R=0.100 for GPT-3.5 <a href="../results/extraction-result-845.html#e845.0" class="evidence-link">[e845.0]</a> </li>
    <li>LLM-MCTS integrating LLM uncertainty via sampling-based belief construction achieves 91.4% success on simple tasks, 71.2% on compositional tasks vs 0% for uniform prior MCTS <a href="../results/extraction-result-972.html#e972.0" class="evidence-link">[e972.0]</a> </li>
    <li>LaBToM with grammar-constrained SMC parsing achieves ~91% semantic equivalence in translation and r=0.76 model-human correlation; ablations show belief-state planning critical (r=0.09 without) <a href="../results/extraction-result-852.html#e852.0" class="evidence-link">[e852.0]</a> <a href="../results/extraction-result-852.html#e852.3" class="evidence-link">[e852.3]</a> </li>
    <li>RAP using LLM as probabilistic world model in MCTS achieves 64% average success on Blocksworld, 51.6% on GSM8K vs 29.4% for CoT; uncertainty modeling via sampling improves performance <a href="../results/extraction-result-970.html#e970.0" class="evidence-link">[e970.0]</a> </li>
    <li>PWM/PWL using probabilistic symbolic world model (higher-order logic with DP prior) outperforms neural baselines on zero-shot QA via explicit uncertainty representation <a href="../results/extraction-result-843.html#e843.0" class="evidence-link">[e843.0]</a> </li>
    <li>Rational Meaning Construction using LLM translation to Church probabilistic programs returns full posterior distributions enabling defeasible reasoning <a href="../results/extraction-result-809.html#e809.0" class="evidence-link">[e809.0]</a> </li>
    <li>SMC steering with grammar-constrained decoding produces unbiased posterior estimates and avoids beam search failure modes <a href="../results/extraction-result-977.html#e977.0" class="evidence-link">[e977.0]</a> <a href="../results/extraction-result-977.html#e977.1" class="evidence-link">[e977.1]</a> </li>
    <li>BSIPS performing exact Bayesian inference over symbolic hypotheses enables accurate epistemic language evaluation (r=0.81 with gold translations) <a href="../results/extraction-result-852.html#e852.1" class="evidence-link">[e852.1]</a> </li>
    <li>Worldformer multi-task learning with symbolic KG prediction achieves 39.15% graph EM vs 14.29% for single-task Seq2Seq <a href="../results/extraction-result-851.html#e851.0" class="evidence-link">[e851.0]</a> </li>
    <li>GATA continuous belief graphs outperform discrete symbolic updaters (+24.2% relative improvement), showing soft probabilistic representations can be more robust <a href="../results/extraction-result-980.html#e980.0" class="evidence-link">[e980.0]</a> <a href="../results/extraction-result-980.html#e980.1" class="evidence-link">[e980.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An LLM+planner architecture will outperform pure LLM planning on any domain requiring >5 step lookahead with combinatorial branching by at least 2x in success rate</li>
                <li>Providing domain PDDL and one example problem-solution pair will enable GPT-4 to translate new problems with >80% accuracy</li>
                <li>Hybrid systems that use LLMs for translation and symbolic planners for search will scale better to larger state spaces than pure neural approaches</li>
                <li>Grammar-constrained SMC decoding will produce more reliable semantic parses than unconstrained sampling for structured target languages</li>
                <li>Modeling LLM uncertainty via multiple samples and forming belief distributions will improve planning robustness in partially observable environments</li>
                <li>Probabilistic symbolic world models will outperform deterministic symbolic models when input language is ambiguous or observations are noisy</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether LLM translation quality can be improved enough to make symbolic planning reliable in truly open-domain settings without domain-specific examples or whether fundamental limitations exist</li>
                <li>Whether future LLMs with better reasoning capabilities will close the gap with symbolic planners or if the separation of concerns will remain beneficial even with more capable models</li>
                <li>The extent to which LLM uncertainty quantification methods (sampling, logit-based confidence, etc.) can be made reliable enough for safety-critical planning applications</li>
                <li>Whether continuous belief representations will consistently outperform discrete symbolic representations as world models scale to more complex domains</li>
                <li>The optimal granularity and formalism of symbolic representation (PDDL vs ASP vs probabilistic programs vs temporal logic) for different problem types and whether this can be automatically determined</li>
                <li>Whether probabilistic symbolic world models can be learned end-to-end from data or if hand-crafted structure will remain necessary for reliable planning</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where pure LLM planning consistently outperforms LLM+symbolic approaches would challenge the theory's core separation-of-concerns principle</li>
                <li>Demonstrating that LLMs can reliably solve planning problems requiring deep search (>10 steps) without external symbolic tools would weaken the theory</li>
                <li>Showing that translation errors are not the primary bottleneck in LLM+symbolic systems (e.g., if planner failures dominate) would challenge the theory</li>
                <li>Finding cases where ignoring LLM uncertainty (using only top-1 outputs) performs as well as explicit uncertainty modeling would challenge the uncertainty integration claims</li>
                <li>Demonstrating that discrete symbolic representations consistently outperform continuous belief representations in learned world models would challenge claims about soft representations</li>
                <li>Showing that deterministic symbolic planners outperform probabilistic planners even with ambiguous inputs would challenge the probabilistic modeling claims</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory doesn't fully explain why some LLMs (GPT-4) translate much better than others (GPT-3.5) beyond general capability differences - what specific capabilities matter most? <a href="../results/extraction-result-971.html#e971.0" class="evidence-link">[e971.0]</a> <a href="../results/extraction-result-973.html#e973.0" class="evidence-link">[e973.0]</a> <a href="../results/extraction-result-1004.html#e1004.0" class="evidence-link">[e1004.0]</a> </li>
    <li>The optimal granularity of symbolic representation (PDDL vs ASP vs probabilistic programs vs temporal logic) for different problem types is not specified <a href="../results/extraction-result-1004.html#e1004.0" class="evidence-link">[e1004.0]</a> <a href="../results/extraction-result-974.html#e974.0" class="evidence-link">[e974.0]</a> <a href="../results/extraction-result-809.html#e809.0" class="evidence-link">[e809.0]</a> <a href="../results/extraction-result-975.html#e975.0" class="evidence-link">[e975.0]</a> </li>
    <li>When continuous belief representations outperform discrete symbolic representations is not fully characterized - GATA shows benefits but conditions unclear <a href="../results/extraction-result-980.html#e980.0" class="evidence-link">[e980.0]</a> <a href="../results/extraction-result-980.html#e980.1" class="evidence-link">[e980.1]</a> <a href="../results/extraction-result-980.html#e980.2" class="evidence-link">[e980.2]</a> </li>
    <li>The theory doesn't explain why some domains (Miconic, Spanner) show very low success even with LLM+symbolic approaches <a href="../results/extraction-result-973.html#e973.0" class="evidence-link">[e973.0]</a> </li>
    <li>How to automatically determine when probabilistic vs deterministic symbolic models are needed is not addressed <a href="../results/extraction-result-1006.html#e1006.0" class="evidence-link">[e1006.0]</a> <a href="../results/extraction-result-845.html#e845.0" class="evidence-link">[e845.0]</a> <a href="../results/extraction-result-843.html#e843.0" class="evidence-link">[e843.0]</a> </li>
    <li>The computational trade-offs between exact symbolic planning and approximate probabilistic inference are not fully characterized <a href="../results/extraction-result-1006.html#e1006.0" class="evidence-link">[e1006.0]</a> <a href="../results/extraction-result-1006.html#e1006.7" class="evidence-link">[e1006.7]</a> <a href="../results/extraction-result-1006.html#e1006.8" class="evidence-link">[e1006.8]</a> </li>
    <li>Why BeSimulator achieves high accuracy without probabilistic planning is not explained by the theory <a href="../results/extraction-result-842.html#e842.0" class="evidence-link">[e842.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) LLM+P: Empowering Large Language Models with Optimal Planning Proficiency [Demonstrates LLM+planner separation but doesn't formulate general theory or address uncertainty integration]</li>
    <li>Pallagani et al. (2023) Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text [Shows LLM+ASP benefits but focuses on specific implementation without probabilistic uncertainty]</li>
    <li>Silver et al. (2023) PDDL planning with pretrained large language models [Related work on LLM-PDDL integration]</li>
    <li>Zhi-Xuan et al. (2020) Online Bayesian Goal Inference for Boundedly-Rational Planning Agents [Probabilistic symbolic planning but not integrated with LLMs]</li>
    <li>Gu et al. (2023) The Neuro-Symbolic Inverse Planning Engine (NIPE) [Combines LLM translation with probabilistic symbolic models but focused on inverse planning]</li>
    <li>Wong et al. (2023) From Word Models to World Models [Translates to probabilistic programs but doesn't formulate general theory about LLM+symbolic integration]</li>
    <li>Hao et al. (2023) Reasoning with Language Model is Planning with World Model [Uses LLM as world model in MCTS but doesn't address symbolic planning integration]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Symbolic-Probabilistic Bridging Theory",
    "theory_description": "LLMs can serve as effective translators between natural language and formal symbolic representations (PDDL, ASP, probabilistic programs, temporal logic), but direct LLM-based planning is unreliable for tasks requiring deep combinatorial search. The most effective architectures separate concerns: LLMs handle language-to-symbol translation while dedicated symbolic planners or probabilistic inference engines handle search and reasoning. When LLM output uncertainty is explicitly modeled (via sampling, grammar-constrained decoding, or probabilistic programs) and integrated into planning (via belief states, SMC, or probabilistic world models), systems achieve higher robustness and can handle partial observability, ambiguous language, and epistemic reasoning. Systems that combine LLM translation with formal symbolic planners consistently outperform pure LLM-based planning approaches, with the largest gains on tasks requiring &gt;5 step lookahead, logical consistency, or optimality guarantees.",
    "supporting_evidence": [
        {
            "text": "LLM+P uses GPT-4 to translate natural language into PDDL and achieves 90% success on BlocksWorld vs 15-30% for direct LLM planning; similar patterns across 7 domains",
            "uuids": [
                "e974.0",
                "e1001.0"
            ]
        },
        {
            "text": "GPT-3+ASP achieves 99.99% accuracy on bAbI by using LLM for semantic parsing and ASP for reasoning, vs 80.34% for pure GPT-3; 100% on CLUTRR-S vs lower for pure LLM",
            "uuids": [
                "e1004.0"
            ]
        },
        {
            "text": "LLM-acquired PDDL with classical planners achieves ~95% success vs &lt;50% for LLM-modulo-planner with validation feedback",
            "uuids": [
                "e971.0",
                "e971.2"
            ]
        },
        {
            "text": "Tree of Thoughts (LLM-only deliberation) shows near-zero success on most planning domains while LLM+P succeeds",
            "uuids": [
                "e1001.3"
            ]
        },
        {
            "text": "AutoTAMP using LLM translation to STL plus formal planner achieves ~83.5% success on HouseWorld2, ~79.7% on Rover",
            "uuids": [
                "e975.0"
            ]
        },
        {
            "text": "Generalized planning with GPT-4 synthesizing Python programs achieves 0.90-1.00 success on multiple domains (Delivery, Forest, Gripper, Ferry) when combined with automated debugging",
            "uuids": [
                "e973.0"
            ]
        },
        {
            "text": "SIPS (probabilistic symbolic world model with SMC) outperforms BIRL in speed and accuracy on complex domains; models planning-level uncertainty rather than just action noise",
            "uuids": [
                "e1006.0",
                "e1006.7"
            ]
        },
        {
            "text": "NIPE using LLM translation to probabilistic programs achieves R=0.927 correlation with human judgments vs R=0.658 for GPT-4 direct and R=0.100 for GPT-3.5",
            "uuids": [
                "e845.0"
            ]
        },
        {
            "text": "LLM-MCTS integrating LLM uncertainty via sampling-based belief construction achieves 91.4% success on simple tasks, 71.2% on compositional tasks vs 0% for uniform prior MCTS",
            "uuids": [
                "e972.0"
            ]
        },
        {
            "text": "LaBToM with grammar-constrained SMC parsing achieves ~91% semantic equivalence in translation and r=0.76 model-human correlation; ablations show belief-state planning critical (r=0.09 without)",
            "uuids": [
                "e852.0",
                "e852.3"
            ]
        },
        {
            "text": "RAP using LLM as probabilistic world model in MCTS achieves 64% average success on Blocksworld, 51.6% on GSM8K vs 29.4% for CoT; uncertainty modeling via sampling improves performance",
            "uuids": [
                "e970.0"
            ]
        },
        {
            "text": "PWM/PWL using probabilistic symbolic world model (higher-order logic with DP prior) outperforms neural baselines on zero-shot QA via explicit uncertainty representation",
            "uuids": [
                "e843.0"
            ]
        },
        {
            "text": "Rational Meaning Construction using LLM translation to Church probabilistic programs returns full posterior distributions enabling defeasible reasoning",
            "uuids": [
                "e809.0"
            ]
        },
        {
            "text": "SMC steering with grammar-constrained decoding produces unbiased posterior estimates and avoids beam search failure modes",
            "uuids": [
                "e977.0",
                "e977.1"
            ]
        },
        {
            "text": "BSIPS performing exact Bayesian inference over symbolic hypotheses enables accurate epistemic language evaluation (r=0.81 with gold translations)",
            "uuids": [
                "e852.1"
            ]
        },
        {
            "text": "Worldformer multi-task learning with symbolic KG prediction achieves 39.15% graph EM vs 14.29% for single-task Seq2Seq",
            "uuids": [
                "e851.0"
            ]
        },
        {
            "text": "GATA continuous belief graphs outperform discrete symbolic updaters (+24.2% relative improvement), showing soft probabilistic representations can be more robust",
            "uuids": [
                "e980.0",
                "e980.1"
            ]
        }
    ],
    "theory_statements": [
        "LLMs excel at language-to-symbol translation but fail at reliable combinatorial search and planning requiring &gt;5 step lookahead",
        "Separating translation (LLM) from planning (symbolic solver) yields higher success rates (often 2-6x improvement) than end-to-end LLM planning",
        "Classical symbolic planners provide guarantees (soundness, completeness, optimality) that LLMs cannot reliably achieve",
        "Explicitly modeling LLM output uncertainty (via sampling, grammar constraints, or probabilistic programs) and integrating it into planning improves robustness to ambiguous inputs and partial observability",
        "Probabilistic symbolic world models that integrate LLM uncertainty enable belief-state planning, epistemic reasoning, and better handling of sub-optimal or failed plans",
        "The bottleneck in LLM+symbolic pipelines is typically translation accuracy rather than planner capability; contextual examples (few-shot prompting) are critical for reliable translation",
        "Continuous/soft belief representations can be more robust to prediction errors than discrete symbolic representations in learned world models",
        "Grammar-constrained decoding and SMC methods can produce calibrated posterior distributions over LLM outputs, enabling principled uncertainty propagation",
        "Systems that model planning-level uncertainty (search noise, limited budgets) explain human behavior better than systems modeling only action-selection noise"
    ],
    "new_predictions_likely": [
        "An LLM+planner architecture will outperform pure LLM planning on any domain requiring &gt;5 step lookahead with combinatorial branching by at least 2x in success rate",
        "Providing domain PDDL and one example problem-solution pair will enable GPT-4 to translate new problems with &gt;80% accuracy",
        "Hybrid systems that use LLMs for translation and symbolic planners for search will scale better to larger state spaces than pure neural approaches",
        "Grammar-constrained SMC decoding will produce more reliable semantic parses than unconstrained sampling for structured target languages",
        "Modeling LLM uncertainty via multiple samples and forming belief distributions will improve planning robustness in partially observable environments",
        "Probabilistic symbolic world models will outperform deterministic symbolic models when input language is ambiguous or observations are noisy"
    ],
    "new_predictions_unknown": [
        "Whether LLM translation quality can be improved enough to make symbolic planning reliable in truly open-domain settings without domain-specific examples or whether fundamental limitations exist",
        "Whether future LLMs with better reasoning capabilities will close the gap with symbolic planners or if the separation of concerns will remain beneficial even with more capable models",
        "The extent to which LLM uncertainty quantification methods (sampling, logit-based confidence, etc.) can be made reliable enough for safety-critical planning applications",
        "Whether continuous belief representations will consistently outperform discrete symbolic representations as world models scale to more complex domains",
        "The optimal granularity and formalism of symbolic representation (PDDL vs ASP vs probabilistic programs vs temporal logic) for different problem types and whether this can be automatically determined",
        "Whether probabilistic symbolic world models can be learned end-to-end from data or if hand-crafted structure will remain necessary for reliable planning"
    ],
    "negative_experiments": [
        "Finding domains where pure LLM planning consistently outperforms LLM+symbolic approaches would challenge the theory's core separation-of-concerns principle",
        "Demonstrating that LLMs can reliably solve planning problems requiring deep search (&gt;10 steps) without external symbolic tools would weaken the theory",
        "Showing that translation errors are not the primary bottleneck in LLM+symbolic systems (e.g., if planner failures dominate) would challenge the theory",
        "Finding cases where ignoring LLM uncertainty (using only top-1 outputs) performs as well as explicit uncertainty modeling would challenge the uncertainty integration claims",
        "Demonstrating that discrete symbolic representations consistently outperform continuous belief representations in learned world models would challenge claims about soft representations",
        "Showing that deterministic symbolic planners outperform probabilistic planners even with ambiguous inputs would challenge the probabilistic modeling claims"
    ],
    "unaccounted_for": [
        {
            "text": "The theory doesn't fully explain why some LLMs (GPT-4) translate much better than others (GPT-3.5) beyond general capability differences - what specific capabilities matter most?",
            "uuids": [
                "e971.0",
                "e973.0",
                "e1004.0"
            ]
        },
        {
            "text": "The optimal granularity of symbolic representation (PDDL vs ASP vs probabilistic programs vs temporal logic) for different problem types is not specified",
            "uuids": [
                "e1004.0",
                "e974.0",
                "e809.0",
                "e975.0"
            ]
        },
        {
            "text": "When continuous belief representations outperform discrete symbolic representations is not fully characterized - GATA shows benefits but conditions unclear",
            "uuids": [
                "e980.0",
                "e980.1",
                "e980.2"
            ]
        },
        {
            "text": "The theory doesn't explain why some domains (Miconic, Spanner) show very low success even with LLM+symbolic approaches",
            "uuids": [
                "e973.0"
            ]
        },
        {
            "text": "How to automatically determine when probabilistic vs deterministic symbolic models are needed is not addressed",
            "uuids": [
                "e1006.0",
                "e845.0",
                "e843.0"
            ]
        },
        {
            "text": "The computational trade-offs between exact symbolic planning and approximate probabilistic inference are not fully characterized",
            "uuids": [
                "e1006.0",
                "e1006.7",
                "e1006.8"
            ]
        },
        {
            "text": "Why BeSimulator achieves high accuracy without probabilistic planning is not explained by the theory",
            "uuids": [
                "e842.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "RAP using LLM as world model in MCTS shows improvements over pure LLM planning, suggesting LLMs can serve as world models in planning contexts when properly integrated (but this may actually support the theory's uncertainty integration claims)",
            "uuids": [
                "e970.0"
            ]
        },
        {
            "text": "BeSimulator achieves high accuracy (13.60-24.80% improvement) using LLMs for simulation with structured reasoning (CBS) but without explicit probabilistic planning or belief states",
            "uuids": [
                "e842.0"
            ]
        },
        {
            "text": "Some domains show LLM+symbolic approaches failing (e.g., Floortile 0% success, Miconic 0.01% success) suggesting limitations beyond translation",
            "uuids": [
                "e973.0",
                "e974.0"
            ]
        },
        {
            "text": "GATA-GTP (discrete symbolic updater) sometimes outperforms text-only baselines when combined with raw text, suggesting discrete symbolic representations can work in some contexts",
            "uuids": [
                "e980.1"
            ]
        }
    ],
    "special_cases": [
        "For very simple domains (&lt;3 steps), pure LLM planning may be sufficient and the overhead of symbolic translation may not be worthwhile",
        "When formal domain models are unavailable or costly to construct, LLM-based approaches may be the only practical option",
        "For tasks requiring commonsense reasoning over implicit world knowledge not easily formalized, pure symbolic approaches may be insufficient",
        "In fully observable deterministic environments, probabilistic world models may not provide benefits over deterministic symbolic models",
        "For short-horizon tasks with small action spaces, the benefits of explicit uncertainty modeling may be minimal",
        "When LLM translation is highly reliable (&gt;95% accuracy), the benefits of probabilistic uncertainty modeling may be reduced",
        "In domains with very large state spaces, exact symbolic planning may be intractable and approximate methods (including LLM-based) may be necessary",
        "For real-time applications, the computational cost of probabilistic inference may make deterministic approximations preferable"
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Liu et al. (2023) LLM+P: Empowering Large Language Models with Optimal Planning Proficiency [Demonstrates LLM+planner separation but doesn't formulate general theory or address uncertainty integration]",
            "Pallagani et al. (2023) Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text [Shows LLM+ASP benefits but focuses on specific implementation without probabilistic uncertainty]",
            "Silver et al. (2023) PDDL planning with pretrained large language models [Related work on LLM-PDDL integration]",
            "Zhi-Xuan et al. (2020) Online Bayesian Goal Inference for Boundedly-Rational Planning Agents [Probabilistic symbolic planning but not integrated with LLMs]",
            "Gu et al. (2023) The Neuro-Symbolic Inverse Planning Engine (NIPE) [Combines LLM translation with probabilistic symbolic models but focused on inverse planning]",
            "Wong et al. (2023) From Word Models to World Models [Translates to probabilistic programs but doesn't formulate general theory about LLM+symbolic integration]",
            "Hao et al. (2023) Reasoning with Language Model is Planning with World Model [Uses LLM as world model in MCTS but doesn't address symbolic planning integration]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Modular Decomposition for Arithmetic in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-749</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-749</p>
                <p><strong>Name:</strong> Hierarchical Modular Decomposition for Arithmetic in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.</p>
                <p><strong>Description:</strong> This theory proposes that LLMs perform arithmetic by decomposing problems into hierarchical modules, each responsible for a sub-operation (e.g., digit-wise addition, carry propagation, modular reduction). These modules operate over distributed representations, and their outputs are composed through learned routing mechanisms. The modular structure enables LLMs to generalize arithmetic operations to longer numbers and novel compositions by reusing and recombining learned submodules.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Decomposition of Arithmetic Tasks (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; receives &#8594; arithmetic problem</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; internal computation &#8594; is_decomposed_into &#8594; hierarchical modules (e.g., digit-wise operations, carry modules, modular reduction modules)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Analysis of transformer attention patterns during arithmetic tasks reveals distinct attention heads focusing on different digit positions and sub-operations. </li>
    <li>LLMs can generalize multi-digit arithmetic by reusing sub-solutions learned for shorter numbers. </li>
    <li>Ablation of specific attention heads or feedforward submodules impairs specific sub-operations (e.g., carry propagation), supporting modularity. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While modularity in neural computation is known, the specific hierarchical decomposition for arithmetic in LLMs is a new hypothesis.</p>            <p><strong>What Already Exists:</strong> Modular neural architectures and hierarchical decomposition are known in algorithmic reasoning and neural program induction.</p>            <p><strong>What is Novel:</strong> The claim that LLMs spontaneously develop hierarchical modular decompositions for arithmetic, with specific submodules for digit-wise and modular operations, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate computation and modularity in LLMs]</li>
    <li>Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [Analysis of modular computation in transformers]</li>
</ul>
            <h3>Statement 1: Learned Routing and Composition of Arithmetic Submodules (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; arithmetic problem &#8594; requires &#8594; multiple sub-operations (e.g., addition, carry, modular reduction)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; routes &#8594; intermediate representations through appropriate submodules<span style="color: #888888;">, and</span></div>
        <div>&#8226; final output &#8594; is_composed_from &#8594; outputs of submodules</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Transformer models show dynamic attention routing depending on the arithmetic operation and input structure. </li>
    <li>Interventions on intermediate representations can selectively disrupt specific sub-operations without affecting others. </li>
    <li>LLMs can solve arithmetic problems with novel compositions of sub-operations, indicating compositional generalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general idea of routing and modularity is known, but its spontaneous emergence for arithmetic in LLMs is a new claim.</p>            <p><strong>What Already Exists:</strong> Compositionality and routing in neural networks are established in modular and mixture-of-experts architectures.</p>            <p><strong>What is Novel:</strong> The hypothesis that LLMs learn to route distributed representations through arithmetic-specific submodules for compositional computation is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Shazeer et al. (2017) Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer [Routing and modularity in neural networks]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate computation and modularity in LLMs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If specific attention heads or feedforward submodules are ablated, only certain sub-operations (e.g., carry propagation) will be impaired, while others remain intact.</li>
                <li>Probing intermediate representations will reveal distinct subspaces corresponding to different arithmetic sub-operations.</li>
                <li>Training LLMs on arithmetic tasks with explicit intermediate supervision (e.g., scratchpads) will enhance modularity and generalization.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained on arithmetic in a novel numeral system (e.g., base-12), it will develop new submodules for digit-wise operations and carry propagation in that base.</li>
                <li>If routing mechanisms are made explicit (e.g., via architectural constraints), arithmetic generalization to longer numbers will improve.</li>
                <li>If submodules are forced to share parameters, the model's ability to generalize complex arithmetic will degrade.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If ablation of specific attention heads or submodules does not selectively impair arithmetic sub-operations, the theory would be challenged.</li>
                <li>If intermediate representations do not show modular structure during arithmetic, the theory would be undermined.</li>
                <li>If LLMs cannot generalize arithmetic to longer numbers despite modular training, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs can perform arithmetic without clear evidence of modular decomposition, possibly relying on holistic distributed computation. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends known ideas of modularity and compositionality to a new domain (arithmetic in LLMs) and proposes a specific mechanism for generalization.</p>
            <p><strong>References:</strong> <ul>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate computation and modularity in LLMs]</li>
    <li>Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [Analysis of modular computation in transformers]</li>
    <li>Shazeer et al. (2017) Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer [Routing and modularity in neural networks]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Modular Decomposition for Arithmetic in Language Models",
    "theory_description": "This theory proposes that LLMs perform arithmetic by decomposing problems into hierarchical modules, each responsible for a sub-operation (e.g., digit-wise addition, carry propagation, modular reduction). These modules operate over distributed representations, and their outputs are composed through learned routing mechanisms. The modular structure enables LLMs to generalize arithmetic operations to longer numbers and novel compositions by reusing and recombining learned submodules.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Decomposition of Arithmetic Tasks",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "receives",
                        "object": "arithmetic problem"
                    }
                ],
                "then": [
                    {
                        "subject": "internal computation",
                        "relation": "is_decomposed_into",
                        "object": "hierarchical modules (e.g., digit-wise operations, carry modules, modular reduction modules)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Analysis of transformer attention patterns during arithmetic tasks reveals distinct attention heads focusing on different digit positions and sub-operations.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can generalize multi-digit arithmetic by reusing sub-solutions learned for shorter numbers.",
                        "uuids": []
                    },
                    {
                        "text": "Ablation of specific attention heads or feedforward submodules impairs specific sub-operations (e.g., carry propagation), supporting modularity.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Modular neural architectures and hierarchical decomposition are known in algorithmic reasoning and neural program induction.",
                    "what_is_novel": "The claim that LLMs spontaneously develop hierarchical modular decompositions for arithmetic, with specific submodules for digit-wise and modular operations, is novel.",
                    "classification_explanation": "While modularity in neural computation is known, the specific hierarchical decomposition for arithmetic in LLMs is a new hypothesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate computation and modularity in LLMs]",
                        "Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [Analysis of modular computation in transformers]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Learned Routing and Composition of Arithmetic Submodules",
                "if": [
                    {
                        "subject": "arithmetic problem",
                        "relation": "requires",
                        "object": "multiple sub-operations (e.g., addition, carry, modular reduction)"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "routes",
                        "object": "intermediate representations through appropriate submodules"
                    },
                    {
                        "subject": "final output",
                        "relation": "is_composed_from",
                        "object": "outputs of submodules"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Transformer models show dynamic attention routing depending on the arithmetic operation and input structure.",
                        "uuids": []
                    },
                    {
                        "text": "Interventions on intermediate representations can selectively disrupt specific sub-operations without affecting others.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can solve arithmetic problems with novel compositions of sub-operations, indicating compositional generalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositionality and routing in neural networks are established in modular and mixture-of-experts architectures.",
                    "what_is_novel": "The hypothesis that LLMs learn to route distributed representations through arithmetic-specific submodules for compositional computation is novel.",
                    "classification_explanation": "The general idea of routing and modularity is known, but its spontaneous emergence for arithmetic in LLMs is a new claim.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Shazeer et al. (2017) Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer [Routing and modularity in neural networks]",
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate computation and modularity in LLMs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If specific attention heads or feedforward submodules are ablated, only certain sub-operations (e.g., carry propagation) will be impaired, while others remain intact.",
        "Probing intermediate representations will reveal distinct subspaces corresponding to different arithmetic sub-operations.",
        "Training LLMs on arithmetic tasks with explicit intermediate supervision (e.g., scratchpads) will enhance modularity and generalization."
    ],
    "new_predictions_unknown": [
        "If a model is trained on arithmetic in a novel numeral system (e.g., base-12), it will develop new submodules for digit-wise operations and carry propagation in that base.",
        "If routing mechanisms are made explicit (e.g., via architectural constraints), arithmetic generalization to longer numbers will improve.",
        "If submodules are forced to share parameters, the model's ability to generalize complex arithmetic will degrade."
    ],
    "negative_experiments": [
        "If ablation of specific attention heads or submodules does not selectively impair arithmetic sub-operations, the theory would be challenged.",
        "If intermediate representations do not show modular structure during arithmetic, the theory would be undermined.",
        "If LLMs cannot generalize arithmetic to longer numbers despite modular training, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs can perform arithmetic without clear evidence of modular decomposition, possibly relying on holistic distributed computation.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Certain LLMs fail to generalize arithmetic to longer numbers, suggesting limits to modularity or compositionality.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Arithmetic tasks involving non-standard numeral systems or non-positional encodings may not fit the hierarchical modular decomposition framework.",
        "Very small models or models with limited depth may not develop clear modular substructures."
    ],
    "existing_theory": {
        "what_already_exists": "Modular and compositional computation is known in neural networks, and intermediate computation (e.g., scratchpads) has been explored in LLMs.",
        "what_is_novel": "The specific claim that LLMs spontaneously develop hierarchical modular decompositions for arithmetic, with learned routing and composition, is novel.",
        "classification_explanation": "The theory extends known ideas of modularity and compositionality to a new domain (arithmetic in LLMs) and proposes a specific mechanism for generalization.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate computation and modularity in LLMs]",
            "Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [Analysis of modular computation in transformers]",
            "Shazeer et al. (2017) Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer [Routing and modularity in neural networks]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform arithmetic.",
    "original_theory_id": "theory-579",
    "original_theory_name": "Distributed Fourier-Feature Representation and Modular Arithmetic Computation in Language Models",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Distributed Fourier-Feature Representation and Modular Arithmetic Computation in Language Models",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
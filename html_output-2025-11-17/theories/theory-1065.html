<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Abstraction and Spatial Reasoning in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1065</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1065</p>
                <p><strong>Name:</strong> Hierarchical Abstraction and Spatial Reasoning in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.</p>
                <p><strong>Description:</strong> This theory proposes that autoregressive language models develop hierarchical abstractions of spatially-structured board games, enabling them to reason about both local and global constraints. Through training, models learn to represent board games at multiple levels of abstraction, from individual cell states to global board configurations, supporting efficient puzzle solving.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Representation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_trained_on &#8594; spatially-structured board game data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; internal representations &#8594; encode &#8594; multi-level abstractions (cell, region, board)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Layer-wise probing reveals that lower layers encode local cell information, while higher layers encode global board state. </li>
    <li>Models can generalize to larger or differently-shaped boards, indicating abstraction beyond fixed input size. </li>
    <li>Intermediate representations can be mapped to both local and global constraint satisfaction metrics. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to general deep learning hierarchy, the explicit mapping to spatial puzzle abstraction is novel.</p>            <p><strong>What Already Exists:</strong> Hierarchical representations are known in deep learning, and some work has shown layer-wise specialization in transformers.</p>            <p><strong>What is Novel:</strong> This law claims that such hierarchical abstraction specifically supports spatial reasoning in board games, with explicit mapping to puzzle structure.</p>
            <p><strong>References:</strong> <ul>
    <li>Tenney et al. (2019) BERT Rediscovers the Classical NLP Pipeline [layer-wise specialization]</li>
    <li>Belrose et al. (2023) Language Models Can Solve Sudoku [does not formalize hierarchical abstraction]</li>
</ul>
            <h3>Statement 1: Abstraction Transfer Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; model &#8594; has_hierarchical_abstractions &#8594; spatial board game<span style="color: #888888;">, and</span></div>
        <div>&#8226; model &#8594; is_fine_tuned_on &#8594; novel board game with similar structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; transfers &#8594; abstractions to new game, enabling rapid learning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Models pre-trained on Sudoku can be fine-tuned to solve similar logic puzzles (e.g., KenKen) with fewer examples than from scratch. </li>
    <li>Probing shows that some internal representations are reused across related games. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The focus on spatial abstraction transfer in LMs for board games is novel.</p>            <p><strong>What Already Exists:</strong> Transfer learning is well-known in deep learning, but not specifically for spatial abstraction in board games.</p>            <p><strong>What is Novel:</strong> This law claims that hierarchical spatial abstractions are the transferable units enabling rapid adaptation to new games.</p>
            <p><strong>References:</strong> <ul>
    <li>Raffel et al. (2020) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [general transfer learning]</li>
    <li>Zhong et al. (2023) Can Language Models Play Board Games? [does not formalize abstraction transfer]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Layer-wise analysis of LMs trained on board games will reveal increasing abstraction from local to global as depth increases.</li>
                <li>Fine-tuning a model trained on one spatial puzzle to another will require fewer examples than training from scratch.</li>
                <li>Probing will reveal shared internal representations across related spatial games.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Hierarchical abstractions may enable zero-shot generalization to novel board sizes or rule variants.</li>
                <li>Transfer of abstractions may fail for games with fundamentally different spatial logic.</li>
                <li>Hierarchical representations may support compositional reasoning about multiple simultaneous puzzles.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If layer-wise probing does not reveal increasing abstraction, the theory is challenged.</li>
                <li>If transfer learning between spatial games is no more efficient than random initialization, the theory is undermined.</li>
                <li>If models cannot generalize to larger or differently-shaped boards, the necessity of hierarchical abstraction is questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some puzzles may be solved by shallow heuristics or memorization, bypassing hierarchical abstraction. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory extends general deep learning concepts to a new domain, making specific, testable claims about spatial abstraction.</p>
            <p><strong>References:</strong> <ul>
    <li>Tenney et al. (2019) BERT Rediscovers the Classical NLP Pipeline [layer-wise specialization]</li>
    <li>Raffel et al. (2020) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [general transfer learning]</li>
    <li>Belrose et al. (2023) Language Models Can Solve Sudoku [does not formalize hierarchical abstraction or transfer]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Abstraction and Spatial Reasoning in Language Models",
    "theory_description": "This theory proposes that autoregressive language models develop hierarchical abstractions of spatially-structured board games, enabling them to reason about both local and global constraints. Through training, models learn to represent board games at multiple levels of abstraction, from individual cell states to global board configurations, supporting efficient puzzle solving.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Representation Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_trained_on",
                        "object": "spatially-structured board game data"
                    }
                ],
                "then": [
                    {
                        "subject": "internal representations",
                        "relation": "encode",
                        "object": "multi-level abstractions (cell, region, board)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Layer-wise probing reveals that lower layers encode local cell information, while higher layers encode global board state.",
                        "uuids": []
                    },
                    {
                        "text": "Models can generalize to larger or differently-shaped boards, indicating abstraction beyond fixed input size.",
                        "uuids": []
                    },
                    {
                        "text": "Intermediate representations can be mapped to both local and global constraint satisfaction metrics.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical representations are known in deep learning, and some work has shown layer-wise specialization in transformers.",
                    "what_is_novel": "This law claims that such hierarchical abstraction specifically supports spatial reasoning in board games, with explicit mapping to puzzle structure.",
                    "classification_explanation": "While related to general deep learning hierarchy, the explicit mapping to spatial puzzle abstraction is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tenney et al. (2019) BERT Rediscovers the Classical NLP Pipeline [layer-wise specialization]",
                        "Belrose et al. (2023) Language Models Can Solve Sudoku [does not formalize hierarchical abstraction]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Abstraction Transfer Law",
                "if": [
                    {
                        "subject": "model",
                        "relation": "has_hierarchical_abstractions",
                        "object": "spatial board game"
                    },
                    {
                        "subject": "model",
                        "relation": "is_fine_tuned_on",
                        "object": "novel board game with similar structure"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "transfers",
                        "object": "abstractions to new game, enabling rapid learning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Models pre-trained on Sudoku can be fine-tuned to solve similar logic puzzles (e.g., KenKen) with fewer examples than from scratch.",
                        "uuids": []
                    },
                    {
                        "text": "Probing shows that some internal representations are reused across related games.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Transfer learning is well-known in deep learning, but not specifically for spatial abstraction in board games.",
                    "what_is_novel": "This law claims that hierarchical spatial abstractions are the transferable units enabling rapid adaptation to new games.",
                    "classification_explanation": "The focus on spatial abstraction transfer in LMs for board games is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Raffel et al. (2020) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [general transfer learning]",
                        "Zhong et al. (2023) Can Language Models Play Board Games? [does not formalize abstraction transfer]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Layer-wise analysis of LMs trained on board games will reveal increasing abstraction from local to global as depth increases.",
        "Fine-tuning a model trained on one spatial puzzle to another will require fewer examples than training from scratch.",
        "Probing will reveal shared internal representations across related spatial games."
    ],
    "new_predictions_unknown": [
        "Hierarchical abstractions may enable zero-shot generalization to novel board sizes or rule variants.",
        "Transfer of abstractions may fail for games with fundamentally different spatial logic.",
        "Hierarchical representations may support compositional reasoning about multiple simultaneous puzzles."
    ],
    "negative_experiments": [
        "If layer-wise probing does not reveal increasing abstraction, the theory is challenged.",
        "If transfer learning between spatial games is no more efficient than random initialization, the theory is undermined.",
        "If models cannot generalize to larger or differently-shaped boards, the necessity of hierarchical abstraction is questioned."
    ],
    "unaccounted_for": [
        {
            "text": "Some puzzles may be solved by shallow heuristics or memorization, bypassing hierarchical abstraction.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Very small models or models with limited capacity may not develop clear hierarchical abstractions.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Games with non-hierarchical or non-spatial structure may not benefit from hierarchical abstraction.",
        "Transfer may fail for games with fundamentally different rules or representations."
    ],
    "existing_theory": {
        "what_already_exists": "Hierarchical and transfer learning are established in deep learning, but not specifically for spatial abstraction in LMs for board games.",
        "what_is_novel": "The explicit mapping of hierarchical abstraction to spatial reasoning and transfer in board game LMs is novel.",
        "classification_explanation": "This theory extends general deep learning concepts to a new domain, making specific, testable claims about spatial abstraction.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tenney et al. (2019) BERT Rediscovers the Classical NLP Pipeline [layer-wise specialization]",
            "Raffel et al. (2020) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [general transfer learning]",
            "Belrose et al. (2023) Language Models Can Solve Sudoku [does not formalize hierarchical abstraction or transfer]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.",
    "original_theory_id": "theory-599",
    "original_theory_name": "Latent World-State Representation Emergence in Autoregressive Language Models for Board Games",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Latent World-State Representation Emergence in Autoregressive Language Models for Board Games",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
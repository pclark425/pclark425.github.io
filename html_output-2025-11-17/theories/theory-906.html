<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Episodic-Semantic Memory Integration for LLM Text Game Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-906</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-906</p>
                <p><strong>Name:</strong> Hierarchical Episodic-Semantic Memory Integration for LLM Text Game Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory posits that LLM agents achieve optimal performance in text games by dynamically integrating episodic (event-based) and semantic (fact-based) memories in a hierarchical structure. Episodic memory encodes temporally ordered experiences, while semantic memory abstracts generalizable knowledge. The agent leverages episodic memory for context-sensitive recall and semantic memory for general reasoning, with a meta-controller arbitrating between them based on task demands.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Memory Arbitration (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; faces &#8594; decision point in text game<span style="color: #888888;">, and</span></div>
        <div>&#8226; current context &#8594; matches &#8594; previous episodic memory</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; retrieves &#8594; episodic memory trace</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Episodic memory enables context-sensitive recall in human and artificial agents. </li>
    <li>Text game agents benefit from recalling specific past events to inform current decisions. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While hierarchical memory is known, its application to LLM agents in text games with dynamic arbitration is novel.</p>            <p><strong>What Already Exists:</strong> Hierarchical memory systems are well-studied in cognitive science and some agent architectures.</p>            <p><strong>What is Novel:</strong> Explicit arbitration between episodic and semantic memory in LLM agents for text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [distinction in human memory]</li>
    <li>Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [episodic memory in LMs]</li>
    <li>Weston et al. (2015) Memory Networks [memory-augmented neural networks]</li>
</ul>
            <h3>Statement 1: Semantic Abstraction for Generalization (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; encounters &#8594; novel or abstract task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; retrieves &#8594; semantic memory (generalized knowledge)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Semantic memory supports generalization and transfer in both humans and artificial agents. </li>
    <li>LLMs can abstract rules and facts from episodic experiences to form semantic knowledge. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Semantic abstraction is known, but its explicit, dynamic use in LLM text game agents is novel.</p>            <p><strong>What Already Exists:</strong> Semantic memory abstraction is a core concept in cognitive science.</p>            <p><strong>What is Novel:</strong> Dynamic integration of semantic abstraction in LLM agent memory for text game generalization.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [semantic memory in humans]</li>
    <li>Weston et al. (2015) Memory Networks [semantic memory in neural networks]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with hierarchical episodic-semantic memory will outperform agents with only one memory type on tasks requiring both recall and generalization.</li>
                <li>Agents will dynamically shift reliance between episodic and semantic memory depending on task novelty and familiarity.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Emergent meta-memory strategies may arise, such as agents learning when to abstract episodic traces into semantic knowledge.</li>
                <li>Unexpected interference or synergy between episodic and semantic memory may occur in highly complex or ambiguous text games.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with only episodic or only semantic memory outperform hierarchical agents on all tasks, the theory is challenged.</li>
                <li>If arbitration between memory types leads to confusion or degraded performance, the theory's assumptions may be flawed.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some text games may not require both episodic and semantic memory, making hierarchical integration unnecessary. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory builds on known memory systems but applies them in a novel, adaptive way to LLM agent memory in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]</li>
    <li>Weston et al. (2015) Memory Networks [memory-augmented neural networks]</li>
    <li>Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [episodic memory in LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Episodic-Semantic Memory Integration for LLM Text Game Agents",
    "theory_description": "This theory posits that LLM agents achieve optimal performance in text games by dynamically integrating episodic (event-based) and semantic (fact-based) memories in a hierarchical structure. Episodic memory encodes temporally ordered experiences, while semantic memory abstracts generalizable knowledge. The agent leverages episodic memory for context-sensitive recall and semantic memory for general reasoning, with a meta-controller arbitrating between them based on task demands.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Memory Arbitration",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "faces",
                        "object": "decision point in text game"
                    },
                    {
                        "subject": "current context",
                        "relation": "matches",
                        "object": "previous episodic memory"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "retrieves",
                        "object": "episodic memory trace"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Episodic memory enables context-sensitive recall in human and artificial agents.",
                        "uuids": []
                    },
                    {
                        "text": "Text game agents benefit from recalling specific past events to inform current decisions.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical memory systems are well-studied in cognitive science and some agent architectures.",
                    "what_is_novel": "Explicit arbitration between episodic and semantic memory in LLM agents for text games.",
                    "classification_explanation": "While hierarchical memory is known, its application to LLM agents in text games with dynamic arbitration is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [distinction in human memory]",
                        "Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [episodic memory in LMs]",
                        "Weston et al. (2015) Memory Networks [memory-augmented neural networks]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Semantic Abstraction for Generalization",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "encounters",
                        "object": "novel or abstract task"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "retrieves",
                        "object": "semantic memory (generalized knowledge)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Semantic memory supports generalization and transfer in both humans and artificial agents.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can abstract rules and facts from episodic experiences to form semantic knowledge.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Semantic memory abstraction is a core concept in cognitive science.",
                    "what_is_novel": "Dynamic integration of semantic abstraction in LLM agent memory for text game generalization.",
                    "classification_explanation": "Semantic abstraction is known, but its explicit, dynamic use in LLM text game agents is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [semantic memory in humans]",
                        "Weston et al. (2015) Memory Networks [semantic memory in neural networks]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with hierarchical episodic-semantic memory will outperform agents with only one memory type on tasks requiring both recall and generalization.",
        "Agents will dynamically shift reliance between episodic and semantic memory depending on task novelty and familiarity."
    ],
    "new_predictions_unknown": [
        "Emergent meta-memory strategies may arise, such as agents learning when to abstract episodic traces into semantic knowledge.",
        "Unexpected interference or synergy between episodic and semantic memory may occur in highly complex or ambiguous text games."
    ],
    "negative_experiments": [
        "If agents with only episodic or only semantic memory outperform hierarchical agents on all tasks, the theory is challenged.",
        "If arbitration between memory types leads to confusion or degraded performance, the theory's assumptions may be flawed."
    ],
    "unaccounted_for": [
        {
            "text": "Some text games may not require both episodic and semantic memory, making hierarchical integration unnecessary.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some simple text games, agents with only recency-based memory perform as well as those with hierarchical memory.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Games with highly repetitive or deterministic structure may not benefit from episodic memory.",
        "Tasks with rapidly changing rules may require continual semantic abstraction."
    ],
    "existing_theory": {
        "what_already_exists": "Hierarchical memory systems and episodic/semantic distinction are established in cognitive science and some neural architectures.",
        "what_is_novel": "Dynamic, meta-controlled arbitration and integration of episodic and semantic memory in LLM agents for text games.",
        "classification_explanation": "The theory builds on known memory systems but applies them in a novel, adaptive way to LLM agent memory in text games.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]",
            "Weston et al. (2015) Memory Networks [memory-augmented neural networks]",
            "Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [episodic memory in LMs]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-589",
    "original_theory_name": "Hybrid Memory Architecture Principle for LLM Agents in Text Games",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
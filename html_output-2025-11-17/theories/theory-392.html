<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluator-Dependent Proxy-to-Ground-Truth Gap Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-392</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-392</p>
                <p><strong>Name:</strong> Evaluator-Dependent Proxy-to-Ground-Truth Gap Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about the evaluation and validation of incremental versus transformational scientific discoveries in automated systems.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that the divergence between proxy metrics and ground truth scientific value is a function of three primary factors: (1) the degree of transformation in a discovery (T), (2) evaluator characteristics including type, expertise, and systematic biases (E), and (3) field-specific factors including paradigm rigidity, publication norms, and infrastructure (β). Proxy metrics are calibrated on historical data that predominantly reflects incremental science, creating training distribution bias that causes systematic misvaluation of transformational work. However, the gap can be either positive (undervaluation) or negative (overvaluation) depending on evaluator characteristics. The gap arises because transformational discoveries violate multiple implicit assumptions embedded in proxy metrics simultaneously (citation patterns, journal prestige, author reputation, methodological familiarity), with these violations compounding in evaluator-dependent ways. Critically, the gap is not merely noise but systematic bias that can be partially corrected through meta-learning approaches that model transformation degree, evaluator characteristics, and field factors, though correction mechanisms face a fundamental tradeoff: standardization that reduces evaluator disagreement may suppress the heterodox judgments needed to identify paradigm-shifting work.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-320.html">[theory-320]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Expanded gap function from G(T) to G(T, E, β) to explicitly include evaluator characteristics E as a primary variable, based on evidence showing 50+ percentage point variation (13.6% to 81.8%) across evaluators for identical work.</li>
                <li>Modified theory to account for bidirectional bias: gap can be positive (undervaluation) or negative (overvaluation) depending on evaluator type (e.g., DeepReviewer +21.7% positive shift vs humans +15.0% negative shift), replacing assumption of uniform undervaluation.</li>
                <li>Added consistency-diversity tradeoff principle: correction mechanisms that reduce evaluator disagreement (65.1% to 86.5% alignment) may simultaneously suppress heterodox judgments needed for paradigm-shifting work.</li>
                <li>Expanded field-specific β parameter from paradigm rigidity alone to multidimensional field characteristics including publication norms, data availability, indexing coverage (arts/humanities disadvantaged), and citation culture, based on evidence of substantial cross-field performance differences (AUROC ~0.8 in CS vs ~0.6 in biomedicine).</li>
                <li>Added domain expertise as an explicit variable affecting gap magnitude independent of transformation degree T, based on evidence that expert reviewers produce different novelty verdicts for identical papers.</li>
                <li>Removed specific unvalidated quantitative predictions (exponential form G(T) ≈ k * e^(βT), 70-90% undervaluation for T>0.7, temporal decay G(T,t) = G(T) * e^(-λt)) while retaining validated quantitative relationships (citation proxy r=0.0132, text-based r~0.33, training bias 5.3267 units, evaluator variance 13.6%-81.8%, correction improvements).</li>
                <li>Added presentation-over-substance bias as a distinct failure mode for LLM-based evaluators beyond training distribution bias.</li>
                <li>Modified training distribution bias mechanism to acknowledge heterogeneity: overall trend shows older publications fill holes more (5.3267 units, p<0.0001), but 4 of 11 persistent holes show opposite pattern favoring newer work.</li>
                <li>Added meta-evaluation reliability considerations: training contamination and methodological choices in gap measurement can bias measured gaps themselves.</li>
                <li>Expanded scope to acknowledge potential differences in evaluation dynamics for AI-generated versus human-generated discoveries, based on evidence of different rating patterns for autonomous vs guided AI tasks.</li>
                <li>Strengthened empirical support with specific quantitative evidence throughout theory statements: citation proxy correlation r=0.0132, text-based improvement to r~0.33 (25x), training bias effect 5.3267 units (p<0.0001), evaluator variance 13.6%-81.8%, cross-domain degradation AUROC 0.75-0.85 to 0.36-0.40, correction mechanism improvements (11.5%→52.1% deep analysis, MSE 0.0187→0.0093, 65.1%→86.5% alignment).</li>
                <li>Clarified that multiplicative compounding of proxy failures is suggested by evidence ('rich-get-richer' effects) but has not been explicitly tested against additive models.</li>
                <li>Renamed theory to 'Evaluator-Dependent Proxy-to-Ground-Truth Gap Theory' to emphasize the central role of evaluator characteristics revealed by new evidence.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The proxy-to-ground-truth gap G is a function of transformation degree T, evaluator characteristics E, and field-specific factors β: G(T, E, β), where the gap can be positive (undervaluation) or negative (overvaluation) depending on evaluator type.</li>
                <li>Evaluator characteristics E produce 50+ percentage point variation in perceived quality for identical work (13.6% to 81.8% 'comparable' ratings across LLM evaluators), making evaluator choice a primary determinant of gap magnitude.</li>
                <li>Citation-based proxies show near-zero correlation (r=0.0132) with peer-review novelty scores for transformational work, while text-based models achieve moderate correlation (r~0.33), representing a 25x improvement.</li>
                <li>Training distribution bias causes systematic failure on post-training novel work: embedding models trained on 2008-2020 data show older publications fill conceptual holes 5.3267 units more than newer publications (p<0.0001), and LLMs without literature search achieve random performance (AUROC ~0.5).</li>
                <li>Cross-domain proxy degradation is severe: metrics calibrated on single domains (AUROC ~0.75-0.85) drop to near-random performance (AUROC ~0.36-0.40) when applied across domains, demonstrating training distribution specificity.</li>
                <li>The gap arises from multiple simultaneous proxy failures that compound: citation-based proxies fail (r=0.0132 with reviewer scores), prestige-based proxies fail (24 Nobel papers initially rejected), author-reputation proxies fail (transformational work from unexpected sources), and methodology-familiarity proxies fail (novel approaches penalized).</li>
                <li>Peer review shows 35-37% disagreement on novelty judgments (ICLR) and 23% disagreement (NeurIPS), indicating substantial evaluator inconsistency even among human experts.</li>
                <li>Domain expertise materially affects gap magnitude independent of transformation degree: reviewers with correct field knowledge produce different novelty verdicts for identical papers than non-experts.</li>
                <li>Automated systems exhibit bidirectional bias varying by design: some overvalue novelty (DeepReviewer: 21.7% positive shift) while others undervalue it (humans: 15.0% negative shift), rather than uniform undervaluation.</li>
                <li>Field-specific factors extend beyond paradigm rigidity to include publication norms (books vs articles), indexing coverage (arts/humanities systematically disadvantaged), data availability, and citation culture, producing substantial performance differences (AUROC ~0.8 in CS vs ~0.6 in biomedicine).</li>
                <li>The gap can be partially corrected through meta-learning: text-based models improve correlation 25x, structured retrieval increases deep analysis from 11.5% to 52.1%, entropy-weighted training reduces MSE from 0.0187 to 0.0093, and reasoning alignment improves from 65.1% to 86.5%.</li>
                <li>Correction mechanisms face a fundamental consistency-diversity tradeoff: standardization that reduces evaluator disagreement (65.1% to 86.5% alignment) may suppress heterodox judgments needed to identify paradigm-shifting work.</li>
                <li>LLM-based evaluators show presentation-over-substance bias, overweighting stylistic features relative to methodological innovation, creating a distinct failure mode beyond training distribution bias.</li>
                <li>The gap shows temporal patterns with delayed recognition for novel work, though specific functional forms (e.g., exponential decay) require empirical validation.</li>
                <li>Evidence suggests multiplicative compounding of proxy failures through 'rich-get-richer' citation amplification effects, though explicit multiplicative vs additive decomposition has not been empirically tested.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Citation-based proxies show near-zero correlation (Pearson r=0.0132) with peer-review novelty scores, while text-based models achieve moderate correlation (r~0.33), demonstrating systematic failure of traditional citation proxies and 25x improvement through correction. <a href="../results/extraction-result-2189.html#e2189.0" class="evidence-link">[e2189.0]</a> <a href="../results/extraction-result-2189.html#e2189.1" class="evidence-link">[e2189.1]</a> </li>
    <li>LLMs without integrated literature search achieve random performance (AUROC ~0.5) on novelty detection, confirming that automated systems relying solely on parametric knowledge systematically fail. <a href="../results/extraction-result-2190.html#e2190.0" class="evidence-link">[e2190.0]</a> </li>
    <li>Historical dissimilarity metrics show sharp cross-domain performance degradation (AUROC drop of ~0.40-0.50 from single-domain ~0.75-0.85 to cross-domain ~0.36-0.40), demonstrating training distribution bias effects. <a href="../results/extraction-result-2190.html#e2190.1" class="evidence-link">[e2190.1]</a> <a href="../results/extraction-result-2190.html#e2190.2" class="evidence-link">[e2190.2]</a> </li>
    <li>Peer review exhibits 35-37% disagreement on novelty judgments in ICLR samples and 23% disagreement in NeurIPS experiments, confirming systematic inconsistency in evaluation proxies. <a href="../results/extraction-result-2191.html#e2191.0" class="evidence-link">[e2191.0]</a> </li>
    <li>Multiple studies document bias against novelty: reviewers systematically penalize novel proposals even when quality is equal, 24 Nobel-winning papers were initially rejected, and highly novel work tends to appear in lower-impact-factor journals. <a href="../results/extraction-result-2187.html#e2187.0" class="evidence-link">[e2187.0]</a> <a href="../results/extraction-result-2187.html#e2187.4" class="evidence-link">[e2187.4]</a> <a href="../results/extraction-result-2189.html#e2189.2" class="evidence-link">[e2189.2]</a> <a href="../results/extraction-result-2185.html#e2185.5" class="evidence-link">[e2185.5]</a> </li>
    <li>Training distribution bias is empirically demonstrated: embedding models trained on 2008-2020 data show older (pre-training) publications fill conceptual holes more than newer publications (mean difference 5.3267 mixup units, p<0.0001). <a href="../results/extraction-result-2183.html#e2183.1" class="evidence-link">[e2183.1]</a> <a href="../results/extraction-result-2183.html#e2183.2" class="evidence-link">[e2183.2]</a> <a href="../results/extraction-result-2182.html#e2182.0" class="evidence-link">[e2182.0]</a> </li>
    <li>Field-specific differences in proxy performance are substantial: LLM+search achieves AUROC ~0.8 in computer science but only ~0.6 in biomedicine; citation metrics systematically undercount arts/humanities/social sciences due to indexing gaps. <a href="../results/extraction-result-2190.html#e2190.0" class="evidence-link">[e2190.0]</a> <a href="../results/extraction-result-2183.html#e2183.0" class="evidence-link">[e2183.0]</a> </li>
    <li>Correction mechanisms show measurable improvements: structured retrieval increases deep analysis from 11.5% to 52.1%, entropy-weighted training reduces prediction error for high-disruption cases (MSE from 0.0187 to 0.0093), and reasoning alignment improves from 65.1% to 86.5%. <a href="../results/extraction-result-2189.html#e2189.0" class="evidence-link">[e2189.0]</a> <a href="../results/extraction-result-2191.html#e2191.2" class="evidence-link">[e2191.2]</a> <a href="../results/extraction-result-2186.html#e2186.3" class="evidence-link">[e2186.3]</a> <a href="../results/extraction-result-2191.html#e2191.0" class="evidence-link">[e2191.0]</a> </li>
    <li>Evaluator choice produces massive variation: 'comparable' percentages ranged from 13.6% to 81.8% for identical papers depending on which LLM evaluator was used, demonstrating evaluator characteristics as a primary determinant of gap magnitude. <a href="../results/extraction-result-2184.html#e2184.2" class="evidence-link">[e2184.2]</a> </li>
    <li>Domain expertise materially affects novelty recognition: reviewers with correct field knowledge recognize prior work that others miss, producing different novelty verdicts for identical papers independent of transformation degree. <a href="../results/extraction-result-2191.html#e2191.4" class="evidence-link">[e2191.4]</a> </li>
    <li>Automated systems show bidirectional systematic bias varying by design: DeepReviewer shows 21.7% positive shift (optimistic bias) while humans show 15.0% negative shift (critical bias). <a href="../results/extraction-result-2191.html#e2191.1" class="evidence-link">[e2191.1]</a> </li>
    <li>Evidence suggests multiplicative compounding effects: 'rich-get-richer' citation amplification and AI systems reproducing and amplifying existing biases, though explicit multiplicative vs additive decomposition was not tested. <a href="../results/extraction-result-2182.html#e2182.2" class="evidence-link">[e2182.2]</a> <a href="../results/extraction-result-2182.html#e2182.0" class="evidence-link">[e2182.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Automated evaluation systems that explicitly model evaluator characteristics (E) alongside transformation degree (T) will show 30-50% reduction in prediction error compared to systems that model T alone.</li>
                <li>Ensemble methods combining multiple evaluator types with different systematic biases will produce more stable gap estimates than single-evaluator systems, with variance reduction of 40-60%.</li>
                <li>Domain-expert evaluators will show 20-40% smaller gaps for transformational work in their domain compared to non-expert evaluators, even when controlling for overall evaluation quality.</li>
                <li>Papers combining high transformation degree with strong presentation quality will show larger positive gaps (overvaluation) in LLM-based systems compared to human evaluation, with gap magnitude proportional to presentation quality.</li>
                <li>Fields with strong book-publishing traditions (humanities, social sciences) will show 50-70% larger gaps when evaluated by citation-based proxies compared to article-dominant fields (STEM).</li>
                <li>Training automated systems on datasets that explicitly include evaluator-type labels will improve cross-evaluator generalization by 25-40% compared to systems trained without evaluator information.</li>
                <li>Correction mechanisms that increase standardization will show inverse correlation with ability to identify highest-novelty outliers: each 10% increase in evaluator agreement will correspond to 5-8% reduction in detection of top-1% transformational work.</li>
                <li>Pre-training embedding models on larger, more comprehensive corpora before fine-tuning on domain-specific data will reduce training distribution bias by 30-50% as measured by improved recognition of post-training novel work.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If automated systems are designed with adjustable 'evaluator-type' parameters, this might either enable better calibration to different evaluation contexts (reducing gaps by 40-60%), or create gaming opportunities where researchers optimize for specific evaluator types rather than genuine quality.</li>
                <li>Training automated systems on datasets explicitly including both highly-rated incremental work and initially-rejected transformational work might either improve identification of future transformational work by 30-50%, or create confusion that degrades performance if the system cannot distinguish the temporal dimension of recognition.</li>
                <li>Adopting ensemble evaluation systems combining multiple evaluator types with different biases might either achieve optimal balance between consistency and diversity, or create new failure modes at the ensemble-aggregation level that are harder to detect and correct.</li>
                <li>Implementing explicit 'expertise-matching' where papers are routed to evaluators with demonstrated domain knowledge might either reduce gaps by 40-60% for transformational work, or create echo chambers where paradigm-shifting work challenging domain assumptions is systematically rejected.</li>
                <li>As AI systems begin generating scientific discoveries at scale, proxy-truth gap patterns might fundamentally change because AI-generated work may have different novelty distributions, presentation characteristics, and impact patterns than human-generated work, potentially requiring entirely new evaluation frameworks.</li>
                <li>If correction mechanisms that increase standardization are widely adopted, the long-term effect on scientific progress might be positive (reducing noise and improving efficiency) or negative (suppressing heterodox judgments enabling paradigm shifts), with net effects potentially varying by field and time period in unpredictable ways.</li>
                <li>Explicitly training evaluators (human or automated) to recognize and value heterodox judgments might either preserve the ability to identify paradigm-shifting work while maintaining consistency, or might be impossible to operationalize without reintroducing the very inconsistencies that standardization aims to reduce.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If the proxy-truth gap magnitude does not vary substantially (>30%) across different evaluator types for identical papers, the theory's emphasis on evaluator characteristics E as a primary variable would be challenged.</li>
                <li>If correction mechanisms that increase standardization do not show any tradeoff with ability to identify paradigm-shifting work (i.e., if both consistency and breakthrough-detection improve simultaneously), the theory's predicted consistency-diversity tradeoff would be falsified.</li>
                <li>If domain expertise does not materially affect gap magnitude when controlling for overall evaluation quality, the theory's claim about expertise as an independent factor would be invalidated.</li>
                <li>If automated systems show uniform directional bias (all undervalue or all overvalue) rather than bidirectional bias varying by system design, the theory's claim about evaluator-dependent bias direction would be challenged.</li>
                <li>If training distribution bias can be fully eliminated through pre-training on larger corpora without any residual gap for post-training novel work, the theory's claim about fundamental training distribution effects would be weakened.</li>
                <li>If field-specific differences in gap magnitude can be fully explained by paradigm rigidity alone without considering publication norms, indexing coverage, and citation culture, the theory's multidimensional field-characteristics model would be unnecessarily complex.</li>
                <li>If the gap cannot be reduced through meta-learning approaches that model evaluator characteristics, transformation degree, and field factors, the theory's claim about correctability would be challenged.</li>
                <li>If proxy failures do not compound (i.e., if the total gap equals the sum of individual proxy failures rather than showing multiplicative amplification), the theory's claim about compounding effects would be invalidated.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not provide specific quantitative predictions for the functional form of G(T, E, β) - whether relationships are exponential, polynomial, linear, or other forms - and the original exponential prediction G(T) ≈ k * e^(βT) lacks direct empirical validation. </li>
    <li>The theory does not fully explain why some transformational discoveries receive rapid recognition while others face prolonged delays, beyond general statements about proxy failures and evaluator variance. <a href="../results/extraction-result-2185.html#e2185.5" class="evidence-link">[e2185.5]</a> <a href="../results/extraction-result-2190.html#e2190.2" class="evidence-link">[e2190.2]</a> </li>
    <li>The role of scientific communication and framing in modulating the proxy-truth gap is not addressed - transformational work that is well-communicated may show smaller gaps, and presentation quality can dominate substance in LLM-based evaluation systems. <a href="../results/extraction-result-2184.html#e2184.4" class="evidence-link">[e2184.4]</a> </li>
    <li>The theory does not account for how different types of transformation (theoretical vs. methodological vs. empirical) might show different gap patterns and evaluator sensitivities. </li>
    <li>The influence of social networks and collaboration patterns on proxy-truth gaps is not incorporated into the theory. </li>
    <li>The theory does not address meta-evaluation reliability: how training contamination, evaluation-set overlap, and methodological choices in gap measurement can bias the measured gaps themselves. <a href="../results/extraction-result-2191.html#e2191.3" class="evidence-link">[e2191.3]</a> </li>
    <li>Among 11 persistent topological holes in embedding space, 4 showed greater filling by newer (post-training) documents than older documents, suggesting training distribution bias does not uniformly favor historical over novel work in all conceptual dimensions - some dimensions show opposite patterns. <a href="../results/extraction-result-2183.html#e2183.1" class="evidence-link">[e2183.1]</a> </li>
    <li>AI-generated papers in open-ended (autonomous) tasks received more favorable ratings than guided tasks (comparable rates 40-100% vs 15.79-78.95%), which may indicate different evaluation dynamics for AI-generated versus human-generated work not captured by the current theory. <a href="../results/extraction-result-2184.html#e2184.3" class="evidence-link">[e2184.3]</a> </li>
    <li>The theory does not explain the specific mechanisms by which presentation quality influences LLM-based evaluators to overweight style relative to methodological substance. <a href="../results/extraction-result-2184.html#e2184.4" class="evidence-link">[e2184.4]</a> </li>
    <li>Some highly novel work can receive rapid recognition and high citations, and citation-augmented metrics correlate well with human novelty labels in limited single-domain studies, suggesting heterogeneity in gap patterns not fully captured by the theory. <a href="../results/extraction-result-2185.html#e2185.5" class="evidence-link">[e2185.5]</a> <a href="../results/extraction-result-2190.html#e2190.2" class="evidence-link">[e2190.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Evaluator-Dependent Proxy-to-Ground-Truth Gap Theory",
    "type": "specific",
    "theory_description": "This theory posits that the divergence between proxy metrics and ground truth scientific value is a function of three primary factors: (1) the degree of transformation in a discovery (T), (2) evaluator characteristics including type, expertise, and systematic biases (E), and (3) field-specific factors including paradigm rigidity, publication norms, and infrastructure (β). Proxy metrics are calibrated on historical data that predominantly reflects incremental science, creating training distribution bias that causes systematic misvaluation of transformational work. However, the gap can be either positive (undervaluation) or negative (overvaluation) depending on evaluator characteristics. The gap arises because transformational discoveries violate multiple implicit assumptions embedded in proxy metrics simultaneously (citation patterns, journal prestige, author reputation, methodological familiarity), with these violations compounding in evaluator-dependent ways. Critically, the gap is not merely noise but systematic bias that can be partially corrected through meta-learning approaches that model transformation degree, evaluator characteristics, and field factors, though correction mechanisms face a fundamental tradeoff: standardization that reduces evaluator disagreement may suppress the heterodox judgments needed to identify paradigm-shifting work.",
    "supporting_evidence": [
        {
            "text": "Citation-based proxies show near-zero correlation (Pearson r=0.0132) with peer-review novelty scores, while text-based models achieve moderate correlation (r~0.33), demonstrating systematic failure of traditional citation proxies and 25x improvement through correction.",
            "uuids": [
                "e2189.0",
                "e2189.1"
            ]
        },
        {
            "text": "LLMs without integrated literature search achieve random performance (AUROC ~0.5) on novelty detection, confirming that automated systems relying solely on parametric knowledge systematically fail.",
            "uuids": [
                "e2190.0"
            ]
        },
        {
            "text": "Historical dissimilarity metrics show sharp cross-domain performance degradation (AUROC drop of ~0.40-0.50 from single-domain ~0.75-0.85 to cross-domain ~0.36-0.40), demonstrating training distribution bias effects.",
            "uuids": [
                "e2190.1",
                "e2190.2"
            ]
        },
        {
            "text": "Peer review exhibits 35-37% disagreement on novelty judgments in ICLR samples and 23% disagreement in NeurIPS experiments, confirming systematic inconsistency in evaluation proxies.",
            "uuids": [
                "e2191.0"
            ]
        },
        {
            "text": "Multiple studies document bias against novelty: reviewers systematically penalize novel proposals even when quality is equal, 24 Nobel-winning papers were initially rejected, and highly novel work tends to appear in lower-impact-factor journals.",
            "uuids": [
                "e2187.0",
                "e2187.4",
                "e2189.2",
                "e2185.5"
            ]
        },
        {
            "text": "Training distribution bias is empirically demonstrated: embedding models trained on 2008-2020 data show older (pre-training) publications fill conceptual holes more than newer publications (mean difference 5.3267 mixup units, p&lt;0.0001).",
            "uuids": [
                "e2183.1",
                "e2183.2",
                "e2182.0"
            ]
        },
        {
            "text": "Field-specific differences in proxy performance are substantial: LLM+search achieves AUROC ~0.8 in computer science but only ~0.6 in biomedicine; citation metrics systematically undercount arts/humanities/social sciences due to indexing gaps.",
            "uuids": [
                "e2190.0",
                "e2183.0"
            ]
        },
        {
            "text": "Correction mechanisms show measurable improvements: structured retrieval increases deep analysis from 11.5% to 52.1%, entropy-weighted training reduces prediction error for high-disruption cases (MSE from 0.0187 to 0.0093), and reasoning alignment improves from 65.1% to 86.5%.",
            "uuids": [
                "e2189.0",
                "e2191.2",
                "e2186.3",
                "e2191.0"
            ]
        },
        {
            "text": "Evaluator choice produces massive variation: 'comparable' percentages ranged from 13.6% to 81.8% for identical papers depending on which LLM evaluator was used, demonstrating evaluator characteristics as a primary determinant of gap magnitude.",
            "uuids": [
                "e2184.2"
            ]
        },
        {
            "text": "Domain expertise materially affects novelty recognition: reviewers with correct field knowledge recognize prior work that others miss, producing different novelty verdicts for identical papers independent of transformation degree.",
            "uuids": [
                "e2191.4"
            ]
        },
        {
            "text": "Automated systems show bidirectional systematic bias varying by design: DeepReviewer shows 21.7% positive shift (optimistic bias) while humans show 15.0% negative shift (critical bias).",
            "uuids": [
                "e2191.1"
            ]
        },
        {
            "text": "Evidence suggests multiplicative compounding effects: 'rich-get-richer' citation amplification and AI systems reproducing and amplifying existing biases, though explicit multiplicative vs additive decomposition was not tested.",
            "uuids": [
                "e2182.2",
                "e2182.0"
            ]
        }
    ],
    "theory_statements": [
        "The proxy-to-ground-truth gap G is a function of transformation degree T, evaluator characteristics E, and field-specific factors β: G(T, E, β), where the gap can be positive (undervaluation) or negative (overvaluation) depending on evaluator type.",
        "Evaluator characteristics E produce 50+ percentage point variation in perceived quality for identical work (13.6% to 81.8% 'comparable' ratings across LLM evaluators), making evaluator choice a primary determinant of gap magnitude.",
        "Citation-based proxies show near-zero correlation (r=0.0132) with peer-review novelty scores for transformational work, while text-based models achieve moderate correlation (r~0.33), representing a 25x improvement.",
        "Training distribution bias causes systematic failure on post-training novel work: embedding models trained on 2008-2020 data show older publications fill conceptual holes 5.3267 units more than newer publications (p&lt;0.0001), and LLMs without literature search achieve random performance (AUROC ~0.5).",
        "Cross-domain proxy degradation is severe: metrics calibrated on single domains (AUROC ~0.75-0.85) drop to near-random performance (AUROC ~0.36-0.40) when applied across domains, demonstrating training distribution specificity.",
        "The gap arises from multiple simultaneous proxy failures that compound: citation-based proxies fail (r=0.0132 with reviewer scores), prestige-based proxies fail (24 Nobel papers initially rejected), author-reputation proxies fail (transformational work from unexpected sources), and methodology-familiarity proxies fail (novel approaches penalized).",
        "Peer review shows 35-37% disagreement on novelty judgments (ICLR) and 23% disagreement (NeurIPS), indicating substantial evaluator inconsistency even among human experts.",
        "Domain expertise materially affects gap magnitude independent of transformation degree: reviewers with correct field knowledge produce different novelty verdicts for identical papers than non-experts.",
        "Automated systems exhibit bidirectional bias varying by design: some overvalue novelty (DeepReviewer: 21.7% positive shift) while others undervalue it (humans: 15.0% negative shift), rather than uniform undervaluation.",
        "Field-specific factors extend beyond paradigm rigidity to include publication norms (books vs articles), indexing coverage (arts/humanities systematically disadvantaged), data availability, and citation culture, producing substantial performance differences (AUROC ~0.8 in CS vs ~0.6 in biomedicine).",
        "The gap can be partially corrected through meta-learning: text-based models improve correlation 25x, structured retrieval increases deep analysis from 11.5% to 52.1%, entropy-weighted training reduces MSE from 0.0187 to 0.0093, and reasoning alignment improves from 65.1% to 86.5%.",
        "Correction mechanisms face a fundamental consistency-diversity tradeoff: standardization that reduces evaluator disagreement (65.1% to 86.5% alignment) may suppress heterodox judgments needed to identify paradigm-shifting work.",
        "LLM-based evaluators show presentation-over-substance bias, overweighting stylistic features relative to methodological innovation, creating a distinct failure mode beyond training distribution bias.",
        "The gap shows temporal patterns with delayed recognition for novel work, though specific functional forms (e.g., exponential decay) require empirical validation.",
        "Evidence suggests multiplicative compounding of proxy failures through 'rich-get-richer' citation amplification effects, though explicit multiplicative vs additive decomposition has not been empirically tested."
    ],
    "new_predictions_likely": [
        "Automated evaluation systems that explicitly model evaluator characteristics (E) alongside transformation degree (T) will show 30-50% reduction in prediction error compared to systems that model T alone.",
        "Ensemble methods combining multiple evaluator types with different systematic biases will produce more stable gap estimates than single-evaluator systems, with variance reduction of 40-60%.",
        "Domain-expert evaluators will show 20-40% smaller gaps for transformational work in their domain compared to non-expert evaluators, even when controlling for overall evaluation quality.",
        "Papers combining high transformation degree with strong presentation quality will show larger positive gaps (overvaluation) in LLM-based systems compared to human evaluation, with gap magnitude proportional to presentation quality.",
        "Fields with strong book-publishing traditions (humanities, social sciences) will show 50-70% larger gaps when evaluated by citation-based proxies compared to article-dominant fields (STEM).",
        "Training automated systems on datasets that explicitly include evaluator-type labels will improve cross-evaluator generalization by 25-40% compared to systems trained without evaluator information.",
        "Correction mechanisms that increase standardization will show inverse correlation with ability to identify highest-novelty outliers: each 10% increase in evaluator agreement will correspond to 5-8% reduction in detection of top-1% transformational work.",
        "Pre-training embedding models on larger, more comprehensive corpora before fine-tuning on domain-specific data will reduce training distribution bias by 30-50% as measured by improved recognition of post-training novel work."
    ],
    "new_predictions_unknown": [
        "If automated systems are designed with adjustable 'evaluator-type' parameters, this might either enable better calibration to different evaluation contexts (reducing gaps by 40-60%), or create gaming opportunities where researchers optimize for specific evaluator types rather than genuine quality.",
        "Training automated systems on datasets explicitly including both highly-rated incremental work and initially-rejected transformational work might either improve identification of future transformational work by 30-50%, or create confusion that degrades performance if the system cannot distinguish the temporal dimension of recognition.",
        "Adopting ensemble evaluation systems combining multiple evaluator types with different biases might either achieve optimal balance between consistency and diversity, or create new failure modes at the ensemble-aggregation level that are harder to detect and correct.",
        "Implementing explicit 'expertise-matching' where papers are routed to evaluators with demonstrated domain knowledge might either reduce gaps by 40-60% for transformational work, or create echo chambers where paradigm-shifting work challenging domain assumptions is systematically rejected.",
        "As AI systems begin generating scientific discoveries at scale, proxy-truth gap patterns might fundamentally change because AI-generated work may have different novelty distributions, presentation characteristics, and impact patterns than human-generated work, potentially requiring entirely new evaluation frameworks.",
        "If correction mechanisms that increase standardization are widely adopted, the long-term effect on scientific progress might be positive (reducing noise and improving efficiency) or negative (suppressing heterodox judgments enabling paradigm shifts), with net effects potentially varying by field and time period in unpredictable ways.",
        "Explicitly training evaluators (human or automated) to recognize and value heterodox judgments might either preserve the ability to identify paradigm-shifting work while maintaining consistency, or might be impossible to operationalize without reintroducing the very inconsistencies that standardization aims to reduce."
    ],
    "negative_experiments": [
        "If the proxy-truth gap magnitude does not vary substantially (&gt;30%) across different evaluator types for identical papers, the theory's emphasis on evaluator characteristics E as a primary variable would be challenged.",
        "If correction mechanisms that increase standardization do not show any tradeoff with ability to identify paradigm-shifting work (i.e., if both consistency and breakthrough-detection improve simultaneously), the theory's predicted consistency-diversity tradeoff would be falsified.",
        "If domain expertise does not materially affect gap magnitude when controlling for overall evaluation quality, the theory's claim about expertise as an independent factor would be invalidated.",
        "If automated systems show uniform directional bias (all undervalue or all overvalue) rather than bidirectional bias varying by system design, the theory's claim about evaluator-dependent bias direction would be challenged.",
        "If training distribution bias can be fully eliminated through pre-training on larger corpora without any residual gap for post-training novel work, the theory's claim about fundamental training distribution effects would be weakened.",
        "If field-specific differences in gap magnitude can be fully explained by paradigm rigidity alone without considering publication norms, indexing coverage, and citation culture, the theory's multidimensional field-characteristics model would be unnecessarily complex.",
        "If the gap cannot be reduced through meta-learning approaches that model evaluator characteristics, transformation degree, and field factors, the theory's claim about correctability would be challenged.",
        "If proxy failures do not compound (i.e., if the total gap equals the sum of individual proxy failures rather than showing multiplicative amplification), the theory's claim about compounding effects would be invalidated."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not provide specific quantitative predictions for the functional form of G(T, E, β) - whether relationships are exponential, polynomial, linear, or other forms - and the original exponential prediction G(T) ≈ k * e^(βT) lacks direct empirical validation.",
            "uuids": []
        },
        {
            "text": "The theory does not fully explain why some transformational discoveries receive rapid recognition while others face prolonged delays, beyond general statements about proxy failures and evaluator variance.",
            "uuids": [
                "e2185.5",
                "e2190.2"
            ]
        },
        {
            "text": "The role of scientific communication and framing in modulating the proxy-truth gap is not addressed - transformational work that is well-communicated may show smaller gaps, and presentation quality can dominate substance in LLM-based evaluation systems.",
            "uuids": [
                "e2184.4"
            ]
        },
        {
            "text": "The theory does not account for how different types of transformation (theoretical vs. methodological vs. empirical) might show different gap patterns and evaluator sensitivities.",
            "uuids": []
        },
        {
            "text": "The influence of social networks and collaboration patterns on proxy-truth gaps is not incorporated into the theory.",
            "uuids": []
        },
        {
            "text": "The theory does not address meta-evaluation reliability: how training contamination, evaluation-set overlap, and methodological choices in gap measurement can bias the measured gaps themselves.",
            "uuids": [
                "e2191.3"
            ]
        },
        {
            "text": "Among 11 persistent topological holes in embedding space, 4 showed greater filling by newer (post-training) documents than older documents, suggesting training distribution bias does not uniformly favor historical over novel work in all conceptual dimensions - some dimensions show opposite patterns.",
            "uuids": [
                "e2183.1"
            ]
        },
        {
            "text": "AI-generated papers in open-ended (autonomous) tasks received more favorable ratings than guided tasks (comparable rates 40-100% vs 15.79-78.95%), which may indicate different evaluation dynamics for AI-generated versus human-generated work not captured by the current theory.",
            "uuids": [
                "e2184.3"
            ]
        },
        {
            "text": "The theory does not explain the specific mechanisms by which presentation quality influences LLM-based evaluators to overweight style relative to methodological substance.",
            "uuids": [
                "e2184.4"
            ]
        },
        {
            "text": "Some highly novel work can receive rapid recognition and high citations, and citation-augmented metrics correlate well with human novelty labels in limited single-domain studies, suggesting heterogeneity in gap patterns not fully captured by the theory.",
            "uuids": [
                "e2185.5",
                "e2190.2"
            ]
        }
    ],
    "change_log": [
        "Expanded gap function from G(T) to G(T, E, β) to explicitly include evaluator characteristics E as a primary variable, based on evidence showing 50+ percentage point variation (13.6% to 81.8%) across evaluators for identical work.",
        "Modified theory to account for bidirectional bias: gap can be positive (undervaluation) or negative (overvaluation) depending on evaluator type (e.g., DeepReviewer +21.7% positive shift vs humans +15.0% negative shift), replacing assumption of uniform undervaluation.",
        "Added consistency-diversity tradeoff principle: correction mechanisms that reduce evaluator disagreement (65.1% to 86.5% alignment) may simultaneously suppress heterodox judgments needed for paradigm-shifting work.",
        "Expanded field-specific β parameter from paradigm rigidity alone to multidimensional field characteristics including publication norms, data availability, indexing coverage (arts/humanities disadvantaged), and citation culture, based on evidence of substantial cross-field performance differences (AUROC ~0.8 in CS vs ~0.6 in biomedicine).",
        "Added domain expertise as an explicit variable affecting gap magnitude independent of transformation degree T, based on evidence that expert reviewers produce different novelty verdicts for identical papers.",
        "Removed specific unvalidated quantitative predictions (exponential form G(T) ≈ k * e^(βT), 70-90% undervaluation for T&gt;0.7, temporal decay G(T,t) = G(T) * e^(-λt)) while retaining validated quantitative relationships (citation proxy r=0.0132, text-based r~0.33, training bias 5.3267 units, evaluator variance 13.6%-81.8%, correction improvements).",
        "Added presentation-over-substance bias as a distinct failure mode for LLM-based evaluators beyond training distribution bias.",
        "Modified training distribution bias mechanism to acknowledge heterogeneity: overall trend shows older publications fill holes more (5.3267 units, p&lt;0.0001), but 4 of 11 persistent holes show opposite pattern favoring newer work.",
        "Added meta-evaluation reliability considerations: training contamination and methodological choices in gap measurement can bias measured gaps themselves.",
        "Expanded scope to acknowledge potential differences in evaluation dynamics for AI-generated versus human-generated discoveries, based on evidence of different rating patterns for autonomous vs guided AI tasks.",
        "Strengthened empirical support with specific quantitative evidence throughout theory statements: citation proxy correlation r=0.0132, text-based improvement to r~0.33 (25x), training bias effect 5.3267 units (p&lt;0.0001), evaluator variance 13.6%-81.8%, cross-domain degradation AUROC 0.75-0.85 to 0.36-0.40, correction mechanism improvements (11.5%→52.1% deep analysis, MSE 0.0187→0.0093, 65.1%→86.5% alignment).",
        "Clarified that multiplicative compounding of proxy failures is suggested by evidence ('rich-get-richer' effects) but has not been explicitly tested against additive models.",
        "Renamed theory to 'Evaluator-Dependent Proxy-to-Ground-Truth Gap Theory' to emphasize the central role of evaluator characteristics revealed by new evidence."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
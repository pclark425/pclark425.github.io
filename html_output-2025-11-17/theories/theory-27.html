<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Distributional Mismatch and Calibration Failure Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-27</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-27</p>
                <p><strong>Name:</strong> Distributional Mismatch and Calibration Failure Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> </p>
                <p><strong>Description:</strong> Language models learn probability distributions over tokens during pretraining that reflect the statistics of their training data. When evidence is added to prompts, it creates a distributional shift that the model's probability estimates are not calibrated for. Specifically, the model's token probabilities for responses generated with context P(token|context) come from a different distribution than probabilities for responses generated without context P(token|no_context), and these distributions are not directly comparable without calibration. This mismatch causes several problems: (1) the model cannot reliably compare its confidence in parametric vs contextual answers using raw probabilities, (2) prompt-induced biases (majority label, recency, common tokens) shift the entire output distribution in ways that don't reflect true evidence value, and (3) the model's internal confidence estimates become miscalibrated, leading to high-confidence errors. The severity of miscalibration increases with the magnitude of distributional shift, which depends on how different the evidence is from the model's training distribution.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The probability distributions P(token|context) and P(token|no_context) are drawn from different underlying distributions that cannot be directly compared without calibration.</li>
                <li>Prompt-induced distributional shifts can be approximated as affine transformations: P_shifted(y) ≈ W * P_base(y) + b, where W is a diagonal scaling matrix and b is a bias vector.</li>
                <li>The magnitude of miscalibration increases with the KL-divergence between the evidence distribution and the model's training distribution: Miscalibration ∝ KL(P_evidence || P_training).</li>
                <li>Calibration can be achieved by estimating the shift using content-free inputs and applying the inverse transformation: P_calibrated(y) = W^(-1) * (P_observed(y) - b).</li>
                <li>Common token bias, majority label bias, and recency bias are all manifestations of the same underlying distributional mismatch phenomenon.</li>
                <li>Models trained with more diverse evidence distributions will show smaller distributional mismatches and better calibration when evidence is added.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Token-probability outputs are differently distributed for prior vs context responses (uncalibrated), requiring calibration methods to make them comparable. <a href="../results/extraction-result-194.html#e194.0" class="evidence-link">[e194.0]</a> </li>
    <li>Calibrated percentile comparison between prior and context token-probabilities substantially improved accuracy (~14% absolute) and reduced context bias (~20% reduction). <a href="../results/extraction-result-194.html#e194.7" class="evidence-link">[e194.7]</a> </li>
    <li>Prompt-induced biases typically produce an affine-like shift in the LM's output probability distribution, leading to high-confidence but incorrect predictions. <a href="../results/extraction-result-206.html#e206.3" class="evidence-link">[e206.3]</a> </li>
    <li>Models prefer tokens that are common in pretraining data, causing predictions to favor frequent label names over rare task-correct answers. <a href="../results/extraction-result-206.html#e206.2" class="evidence-link">[e206.2]</a> </li>
    <li>Language models tend to prefer answers most frequent within the prompt's training examples, causing output distribution to shift toward majority labels. <a href="../results/extraction-result-206.html#e206.0" class="evidence-link">[e206.0]</a> </li>
    <li>Left-to-right autoregressive LMs disproportionately favor answers near the end of the prompt, so example ordering dramatically alters predicted labels and probabilities. <a href="../results/extraction-result-206.html#e206.1" class="evidence-link">[e206.1]</a> </li>
    <li>Contextual calibration using content-free inputs to estimate prompt-specific bias substantially improves accuracy and reduces variance in few-shot prompts. <a href="../results/extraction-result-206.html#e206.4" class="evidence-link">[e206.4]</a> </li>
    <li>The specific content-free token(s) used to estimate prompt bias affect the estimated bias vector and calibration effectiveness. <a href="../results/extraction-result-206.html#e206.5" class="evidence-link">[e206.5]</a> </li>
    <li>Normalized conditional probabilities assigned by the model to different sampled reasoning paths are often very similar, making probabilistic weighting ineffective for discriminating correct vs incorrect solutions. <a href="../results/extraction-result-200.html#e200.2" class="evidence-link">[e200.2]</a> </li>
    <li>Increasing number of demonstration examples generally increases accuracy but not necessarily calibration; in some cases more shots increased overconfidence (worse ECE). <a href="../results/extraction-result-193.html#e193.5" class="evidence-link">[e193.5]</a> </li>
    <li>Finetuning causes models to become overconfident and uncalibrated: test@1 improves while test@N degrades because model places too much probability mass on narrow set of solutions. <a href="../results/extraction-result-203.html#e203.0" class="evidence-link">[e203.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Measuring the KL-divergence between evidence and training distributions will predict the degree of calibration failure when that evidence is added to prompts.</li>
                <li>Applying contextual calibration (estimating and removing prompt-induced shifts) will improve performance across all tasks where evidence is added to prompts.</li>
                <li>Training models with explicit calibration objectives that minimize distributional mismatch will improve their ability to integrate evidence without miscalibration.</li>
                <li>Ensembling multiple content-free inputs to estimate bias will produce more robust calibration than using a single content-free input.</li>
                <li>Models that are explicitly trained to produce calibrated probabilities (e.g., through temperature scaling or Platt scaling during training) will show smaller distributional mismatches when evidence is added.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exist universal calibration functions that work across all types of evidence and all models, or whether calibration must be task/model-specific.</li>
                <li>Whether the distributional mismatch can be corrected at the activation level (before softmax) rather than at the probability level (after softmax).</li>
                <li>Whether models can be trained to internally perform calibration, eliminating the need for external calibration procedures.</li>
                <li>Whether the affine transformation approximation holds for all types of distributional shifts or whether some shifts require more complex transformations.</li>
                <li>Whether calibration methods that work for one type of evidence (e.g., retrieved documents) transfer to other types of evidence (e.g., chain-of-thought rationales).</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding cases where raw token probabilities from context and no-context conditions are directly comparable without calibration would challenge the distributional mismatch premise.</li>
                <li>Demonstrating that calibration methods do not improve performance or that they harm performance would challenge the theory's practical implications.</li>
                <li>Showing that the magnitude of miscalibration does not correlate with distributional distance would challenge the KL-divergence relationship.</li>
                <li>Finding that prompt-induced biases cannot be modeled as affine transformations would challenge the mathematical framework.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Why some models (like instruction-tuned models) show different calibration properties than base models. <a href="../results/extraction-result-192.html#e192.4" class="evidence-link">[e192.4]</a> </li>
    <li>The exact mechanism by which content-free inputs capture prompt-induced bias. <a href="../results/extraction-result-206.html#e206.4" class="evidence-link">[e206.4]</a> </li>
    <li>Why different content-free inputs produce different calibration results and how to optimally select them. <a href="../results/extraction-result-206.html#e206.5" class="evidence-link">[e206.5]</a> </li>
    <li>Whether the distributional mismatch is primarily a problem of the model's architecture or its training procedure. <a href="../results/extraction-result-206.html#e206.3" class="evidence-link">[e206.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Introduces contextual calibration and identifies prompt-induced biases, core evidence for this theory]</li>
    <li>Guo et al. (2017) On Calibration of Modern Neural Networks [General neural network calibration theory, but doesn't address prompt-specific distributional shifts]</li>
    <li>Jiang et al. (2021) Can Language Models Learn from Explanations in Context? [Shows distributional effects of adding explanations, but doesn't formalize mismatch theory]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Distributional Mismatch and Calibration Failure Theory",
    "theory_description": "Language models learn probability distributions over tokens during pretraining that reflect the statistics of their training data. When evidence is added to prompts, it creates a distributional shift that the model's probability estimates are not calibrated for. Specifically, the model's token probabilities for responses generated with context P(token|context) come from a different distribution than probabilities for responses generated without context P(token|no_context), and these distributions are not directly comparable without calibration. This mismatch causes several problems: (1) the model cannot reliably compare its confidence in parametric vs contextual answers using raw probabilities, (2) prompt-induced biases (majority label, recency, common tokens) shift the entire output distribution in ways that don't reflect true evidence value, and (3) the model's internal confidence estimates become miscalibrated, leading to high-confidence errors. The severity of miscalibration increases with the magnitude of distributional shift, which depends on how different the evidence is from the model's training distribution.",
    "supporting_evidence": [
        {
            "text": "Token-probability outputs are differently distributed for prior vs context responses (uncalibrated), requiring calibration methods to make them comparable.",
            "uuids": [
                "e194.0"
            ]
        },
        {
            "text": "Calibrated percentile comparison between prior and context token-probabilities substantially improved accuracy (~14% absolute) and reduced context bias (~20% reduction).",
            "uuids": [
                "e194.7"
            ]
        },
        {
            "text": "Prompt-induced biases typically produce an affine-like shift in the LM's output probability distribution, leading to high-confidence but incorrect predictions.",
            "uuids": [
                "e206.3"
            ]
        },
        {
            "text": "Models prefer tokens that are common in pretraining data, causing predictions to favor frequent label names over rare task-correct answers.",
            "uuids": [
                "e206.2"
            ]
        },
        {
            "text": "Language models tend to prefer answers most frequent within the prompt's training examples, causing output distribution to shift toward majority labels.",
            "uuids": [
                "e206.0"
            ]
        },
        {
            "text": "Left-to-right autoregressive LMs disproportionately favor answers near the end of the prompt, so example ordering dramatically alters predicted labels and probabilities.",
            "uuids": [
                "e206.1"
            ]
        },
        {
            "text": "Contextual calibration using content-free inputs to estimate prompt-specific bias substantially improves accuracy and reduces variance in few-shot prompts.",
            "uuids": [
                "e206.4"
            ]
        },
        {
            "text": "The specific content-free token(s) used to estimate prompt bias affect the estimated bias vector and calibration effectiveness.",
            "uuids": [
                "e206.5"
            ]
        },
        {
            "text": "Normalized conditional probabilities assigned by the model to different sampled reasoning paths are often very similar, making probabilistic weighting ineffective for discriminating correct vs incorrect solutions.",
            "uuids": [
                "e200.2"
            ]
        },
        {
            "text": "Increasing number of demonstration examples generally increases accuracy but not necessarily calibration; in some cases more shots increased overconfidence (worse ECE).",
            "uuids": [
                "e193.5"
            ]
        },
        {
            "text": "Finetuning causes models to become overconfident and uncalibrated: test@1 improves while test@N degrades because model places too much probability mass on narrow set of solutions.",
            "uuids": [
                "e203.0"
            ]
        }
    ],
    "theory_statements": [
        "The probability distributions P(token|context) and P(token|no_context) are drawn from different underlying distributions that cannot be directly compared without calibration.",
        "Prompt-induced distributional shifts can be approximated as affine transformations: P_shifted(y) ≈ W * P_base(y) + b, where W is a diagonal scaling matrix and b is a bias vector.",
        "The magnitude of miscalibration increases with the KL-divergence between the evidence distribution and the model's training distribution: Miscalibration ∝ KL(P_evidence || P_training).",
        "Calibration can be achieved by estimating the shift using content-free inputs and applying the inverse transformation: P_calibrated(y) = W^(-1) * (P_observed(y) - b).",
        "Common token bias, majority label bias, and recency bias are all manifestations of the same underlying distributional mismatch phenomenon.",
        "Models trained with more diverse evidence distributions will show smaller distributional mismatches and better calibration when evidence is added."
    ],
    "new_predictions_likely": [
        "Measuring the KL-divergence between evidence and training distributions will predict the degree of calibration failure when that evidence is added to prompts.",
        "Applying contextual calibration (estimating and removing prompt-induced shifts) will improve performance across all tasks where evidence is added to prompts.",
        "Training models with explicit calibration objectives that minimize distributional mismatch will improve their ability to integrate evidence without miscalibration.",
        "Ensembling multiple content-free inputs to estimate bias will produce more robust calibration than using a single content-free input.",
        "Models that are explicitly trained to produce calibrated probabilities (e.g., through temperature scaling or Platt scaling during training) will show smaller distributional mismatches when evidence is added."
    ],
    "new_predictions_unknown": [
        "Whether there exist universal calibration functions that work across all types of evidence and all models, or whether calibration must be task/model-specific.",
        "Whether the distributional mismatch can be corrected at the activation level (before softmax) rather than at the probability level (after softmax).",
        "Whether models can be trained to internally perform calibration, eliminating the need for external calibration procedures.",
        "Whether the affine transformation approximation holds for all types of distributional shifts or whether some shifts require more complex transformations.",
        "Whether calibration methods that work for one type of evidence (e.g., retrieved documents) transfer to other types of evidence (e.g., chain-of-thought rationales)."
    ],
    "negative_experiments": [
        "Finding cases where raw token probabilities from context and no-context conditions are directly comparable without calibration would challenge the distributional mismatch premise.",
        "Demonstrating that calibration methods do not improve performance or that they harm performance would challenge the theory's practical implications.",
        "Showing that the magnitude of miscalibration does not correlate with distributional distance would challenge the KL-divergence relationship.",
        "Finding that prompt-induced biases cannot be modeled as affine transformations would challenge the mathematical framework."
    ],
    "unaccounted_for": [
        {
            "text": "Why some models (like instruction-tuned models) show different calibration properties than base models.",
            "uuids": [
                "e192.4"
            ]
        },
        {
            "text": "The exact mechanism by which content-free inputs capture prompt-induced bias.",
            "uuids": [
                "e206.4"
            ]
        },
        {
            "text": "Why different content-free inputs produce different calibration results and how to optimally select them.",
            "uuids": [
                "e206.5"
            ]
        },
        {
            "text": "Whether the distributional mismatch is primarily a problem of the model's architecture or its training procedure.",
            "uuids": [
                "e206.3"
            ]
        }
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Introduces contextual calibration and identifies prompt-induced biases, core evidence for this theory]",
            "Guo et al. (2017) On Calibration of Modern Neural Networks [General neural network calibration theory, but doesn't address prompt-specific distributional shifts]",
            "Jiang et al. (2021) Can Language Models Learn from Explanations in Context? [Shows distributional effects of adding explanations, but doesn't formalize mismatch theory]"
        ]
    },
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
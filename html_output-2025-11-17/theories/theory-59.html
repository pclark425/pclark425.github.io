<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>External Computation Delegation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-59</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-59</p>
                <p><strong>Name:</strong> External Computation Delegation Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> Language models can achieve substantially higher arithmetic accuracy by learning to generate executable programs (Python, Prolog, SymPy, etc.) that delegate actual numeric computation to external interpreters, rather than performing arithmetic internally through token generation. This approach works because: (1) models are better at generating structured code than performing precise numeric operations token-by-token, (2) external interpreters provide deterministic, exact computation free from the accumulation errors inherent in autoregressive generation, (3) program generation separates semantic understanding and problem decomposition (which the model handles well) from numeric execution (which the interpreter handles perfectly), (4) code-pretrained models have strong priors for generating syntactically correct programs, and (5) programming languages provide explicit, consistent constructs for numeric operations (e.g., len(), objects[-1]) that are more reliable than variable natural language expressions. The effectiveness of this approach depends critically on: the model's code generation ability (dramatically improved by code pretraining like Codex/Code-Llama), the expressiveness of the target language and available libraries (SymPy for symbolic math, NumPy for numeric operations), the model's ability to correctly ground problem variables to program variables (the primary failure mode), and the availability of an execution environment. This delegation strategy (PoT, PAL, Prolog generation) consistently and substantially outperforms direct answer generation (CoT) on computation-heavy tasks, with gains ranging from modest (few percentage points) to dramatic (20+ percentage points), with larger gains on more computation-intensive problems. Hybrid approaches that attempt program generation first and fall back to natural language reasoning when programs are non-executable achieve the best overall performance by combining the precision of delegation with the flexibility of natural language reasoning.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models achieve higher arithmetic accuracy by generating executable programs than by directly generating numeric answers, with gains ranging from modest to dramatic (20+ percentage points) depending on problem complexity.</li>
                <li>Program generation separates semantic understanding and problem decomposition (model's strength) from numeric computation (interpreter's strength), avoiding accumulation of token-level errors.</li>
                <li>Code-pretrained models have substantially stronger priors for generating syntactically correct and executable programs, making delegation more effective.</li>
                <li>The primary failure mode in program-based approaches is incorrect variable grounding (mapping problem quantities to program variables), not arithmetic execution errors.</li>
                <li>Program-based approaches scale better to complex, multi-step, high-precision computations than direct answer generation because they avoid error propagation.</li>
                <li>The effectiveness of delegation depends on the target language's expressiveness, available libraries (SymPy, NumPy), and the model's code generation ability.</li>
                <li>Hybrid approaches (program generation with natural language fallback) combine the precision of delegation with the flexibility of natural language reasoning, achieving the best overall performance.</li>
                <li>Program generation provides more consistent, deterministic outputs than natural language reasoning for numeric operations due to explicit programming constructs.</li>
                <li>Programming languages' explicit constructs (e.g., len(), indexing) are more reliable than variable natural language expressions for numeric operations.</li>
                <li>The benefit of delegation increases with problem computational complexity: larger gains on computation-heavy tasks, smaller gains on simple arithmetic.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>PoT substantially improves performance on computation-heavy datasets: MAmmoTH-7B PoT GSM8K 51.6% vs CoT 50.5%, MATH PoT 28.7% vs CoT 10.4%; larger gains for coder models (MAmmoTH-Coder-7B PoT 58.8% vs CoT 22.4%). <a href="../results/extraction-result-320.html#e320.3" class="evidence-link">[e320.3]</a> </li>
    <li>Code-davinci-002 with PoT achieves 71.6% on GSM8K (greedy), 80.0% with self-consistency, dramatically higher than CoT approaches; also strong on SVAMP (85.2%), TabWMP (73.2%), FinQA (64.5%). <a href="../results/extraction-result-298.html#e298.0" class="evidence-link">[e298.0]</a> </li>
    <li>Codex program synthesis achieves 71.1% zero-shot and 81.1% few-shot solve rate on university math problems by generating executable code that implements algorithms and calls math libraries. <a href="../results/extraction-result-295.html#e295.0" class="evidence-link">[e295.0]</a> </li>
    <li>Prolog generation with execution achieves higher accuracy than CoT for arithmetic word problems: Mistral Prolog 66.3% vs CoT 58.9% on GSM8K; PROPER (permuted Prolog) further improves to 70.2%. <a href="../results/extraction-result-272.html#e272.2" class="evidence-link">[e272.2]</a> </li>
    <li>Python generation achieved 55.12% accuracy on GSM8K with Llama-2, higher than both Prolog (varied by model) and CoT, possibly due to Python's prevalence in pretraining. <a href="../results/extraction-result-272.html#e272.3" class="evidence-link">[e272.3]</a> </li>
    <li>PoT benefits strongly from code-pretrained bases: Code-Llama-based MAmmoTH-Coder models consistently outperform Llama-2-based MAmmoTH of similar or larger sizes on OOD arithmetic tasks. <a href="../results/extraction-result-320.html#e320.5" class="evidence-link">[e320.5]</a> </li>
    <li>Primary PoT failure mode on TAT-QA is value grounding errors (47% of failures), followed by logic generation errors (33%), with only 15% having both types of errors, indicating arithmetic execution itself is reliable when grounding is correct. <a href="../results/extraction-result-298.html#e298.5" class="evidence-link">[e298.5]</a> </li>
    <li>PAL shows higher confidence and more consistent token sequences for numeric operations compared to CoT: programming constructs like len(objects) or objects[-1] are more uniform than variable natural language expressions. <a href="../results/extraction-result-303.html#e303.6" class="evidence-link">[e303.6]</a> </li>
    <li>Hybrid decoding (PoT-first with CoT fallback) yields best results on MAmmoTH: GSM8K 53.6%, MATH 31.5%, leveraging program precision when executable and natural language flexibility when not. <a href="../results/extraction-result-320.html#e320.0" class="evidence-link">[e320.0]</a> </li>
    <li>PoT enables solving problems with iterative computation, high-precision floats, polynomial/algebraic equations, and symbolic manipulation via SymPy that are difficult for pure language generation. <a href="../results/extraction-result-298.html#e298.0" class="evidence-link">[e298.0]</a> </li>
    <li>Chain-of-thought with external calculator applied post-hoc to equations improves accuracy (e.g., Codex GSM8K 63.1% → 65.4% with calculator), showing even CoT benefits from delegating final computation. <a href="../results/extraction-result-309.html#e309.2" class="evidence-link">[e309.2]</a> </li>
    <li>Program-aided approaches scale more favorably with increased problem complexity (number of steps, precision requirements) than direct generation approaches. <a href="../results/extraction-result-298.html#e298.0" class="evidence-link">[e298.0]</a> <a href="../results/extraction-result-295.html#e295.0" class="evidence-link">[e295.0]</a> </li>
    <li>PoT-only fine-tuning often yields better OOD generalization for numeric tasks than CoT-only, suggesting program generation learns more robust computational patterns. <a href="../results/extraction-result-320.html#e320.3" class="evidence-link">[e320.3]</a> </li>
    <li>Code-davinci-002 shows substantially higher performance on arithmetic with PoT than text-davinci-002 with CoT, demonstrating the importance of code pretraining for delegation effectiveness. <a href="../results/extraction-result-298.html#e298.0" class="evidence-link">[e298.0]</a> <a href="../results/extraction-result-309.html#e309.2" class="evidence-link">[e309.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Models fine-tuned on program generation should show larger improvements on arithmetic than models fine-tuned on direct answer generation, especially for computation-heavy problems.</li>
                <li>Providing models with access to more powerful libraries (e.g., symbolic math systems like Mathematica, SageMath) should improve performance on complex symbolic operations.</li>
                <li>Program-based approaches should show better performance on operations requiring high precision (many decimal places, large integers) than direct generation.</li>
                <li>Models should show higher confidence (lower entropy) and more consistent token sequences when generating programs than when generating numeric answers directly.</li>
                <li>The performance gap between program-based and direct generation should increase with the number of computational steps required.</li>
                <li>Fine-tuning on diverse programming languages should improve delegation effectiveness across different target languages.</li>
                <li>Hybrid approaches should show better robustness to distribution shifts than pure program or pure natural language approaches.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether program generation can be made as sample-efficient as direct answer generation during training is unclear, given the need to learn both semantic mapping and code syntax.</li>
                <li>It's unknown whether models can learn to automatically and reliably choose between program generation and direct computation based on problem characteristics without explicit hybrid strategies.</li>
                <li>Whether program-based approaches generalize effectively to mathematical domains beyond arithmetic (e.g., geometry, proof-based mathematics, topology) is uncertain.</li>
                <li>The extent to which program generation ability transfers across programming languages (Python → Prolog → other languages) is unknown.</li>
                <li>Whether delegation strategies can be made efficient enough for real-time applications given the overhead of code execution is unclear.</li>
                <li>It's unknown whether models can learn to generate programs that are not just correct but also efficient (optimal time/space complexity).</li>
                <li>Whether program-based approaches can handle problems that are inherently difficult to express programmatically (e.g., some visual or spatial reasoning problems) is uncertain.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that program generation does not improve accuracy over direct generation for computation-heavy tasks would challenge the core claim of delegation benefits.</li>
                <li>Discovering that non-code-pretrained models generate programs as effectively as code-pretrained models would challenge the importance of code pretraining.</li>
                <li>Observing that program-based approaches fail more often due to execution errors than grounding errors would challenge the characterization of primary failure modes.</li>
                <li>Finding that direct generation can match program generation accuracy with sufficient training data would challenge the necessity of delegation.</li>
                <li>Discovering that the performance gap between program and direct generation does not increase with computational complexity would challenge the scaling advantage claim.</li>
                <li>Finding that hybrid approaches do not outperform pure program generation would challenge the value of fallback strategies.</li>
                <li>Observing that program generation does not provide more consistent outputs than natural language would challenge the consistency claim.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How models learn the mapping from natural language problems to program structures is not mechanistically detailed at the attention-head or circuit level. <a href="../results/extraction-result-298.html#e298.0" class="evidence-link">[e298.0]</a> <a href="../results/extraction-result-295.html#e295.0" class="evidence-link">[e295.0]</a> </li>
    <li>The optimal choice of target programming language for different problem types is not fully specified, though Python appears favored due to pretraining prevalence. <a href="../results/extraction-result-272.html#e272.3" class="evidence-link">[e272.3]</a> </li>
    <li>How to automatically and reliably detect when program generation will fail and fallback is needed is not fully characterized beyond simple executability checks. <a href="../results/extraction-result-320.html#e320.0" class="evidence-link">[e320.0]</a> </li>
    <li>The computational and latency costs of program execution in production systems are not thoroughly analyzed. <a href="../results/extraction-result-298.html#e298.0" class="evidence-link">[e298.0]</a> </li>
    <li>How to handle problems that require iterative refinement or interactive computation is not fully addressed. <a href="../results/extraction-result-295.html#e295.0" class="evidence-link">[e295.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [Directly proposes program-aided approach for mathematical reasoning, introduces PAL method]</li>
    <li>Chen et al. (2022) Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks [Directly proposes PoT approach, demonstrates separation of reasoning and computation]</li>
    <li>Drori et al. (2021) A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level [Demonstrates program synthesis with Codex for math problems]</li>
    <li>Yue et al. (2023) MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning [Proposes hybrid CoT/PoT approach and demonstrates benefits of code-pretrained bases]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [Early work showing benefits of external computation for verification]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "External Computation Delegation Theory",
    "theory_description": "Language models can achieve substantially higher arithmetic accuracy by learning to generate executable programs (Python, Prolog, SymPy, etc.) that delegate actual numeric computation to external interpreters, rather than performing arithmetic internally through token generation. This approach works because: (1) models are better at generating structured code than performing precise numeric operations token-by-token, (2) external interpreters provide deterministic, exact computation free from the accumulation errors inherent in autoregressive generation, (3) program generation separates semantic understanding and problem decomposition (which the model handles well) from numeric execution (which the interpreter handles perfectly), (4) code-pretrained models have strong priors for generating syntactically correct programs, and (5) programming languages provide explicit, consistent constructs for numeric operations (e.g., len(), objects[-1]) that are more reliable than variable natural language expressions. The effectiveness of this approach depends critically on: the model's code generation ability (dramatically improved by code pretraining like Codex/Code-Llama), the expressiveness of the target language and available libraries (SymPy for symbolic math, NumPy for numeric operations), the model's ability to correctly ground problem variables to program variables (the primary failure mode), and the availability of an execution environment. This delegation strategy (PoT, PAL, Prolog generation) consistently and substantially outperforms direct answer generation (CoT) on computation-heavy tasks, with gains ranging from modest (few percentage points) to dramatic (20+ percentage points), with larger gains on more computation-intensive problems. Hybrid approaches that attempt program generation first and fall back to natural language reasoning when programs are non-executable achieve the best overall performance by combining the precision of delegation with the flexibility of natural language reasoning.",
    "supporting_evidence": [
        {
            "text": "PoT substantially improves performance on computation-heavy datasets: MAmmoTH-7B PoT GSM8K 51.6% vs CoT 50.5%, MATH PoT 28.7% vs CoT 10.4%; larger gains for coder models (MAmmoTH-Coder-7B PoT 58.8% vs CoT 22.4%).",
            "uuids": [
                "e320.3"
            ]
        },
        {
            "text": "Code-davinci-002 with PoT achieves 71.6% on GSM8K (greedy), 80.0% with self-consistency, dramatically higher than CoT approaches; also strong on SVAMP (85.2%), TabWMP (73.2%), FinQA (64.5%).",
            "uuids": [
                "e298.0"
            ]
        },
        {
            "text": "Codex program synthesis achieves 71.1% zero-shot and 81.1% few-shot solve rate on university math problems by generating executable code that implements algorithms and calls math libraries.",
            "uuids": [
                "e295.0"
            ]
        },
        {
            "text": "Prolog generation with execution achieves higher accuracy than CoT for arithmetic word problems: Mistral Prolog 66.3% vs CoT 58.9% on GSM8K; PROPER (permuted Prolog) further improves to 70.2%.",
            "uuids": [
                "e272.2"
            ]
        },
        {
            "text": "Python generation achieved 55.12% accuracy on GSM8K with Llama-2, higher than both Prolog (varied by model) and CoT, possibly due to Python's prevalence in pretraining.",
            "uuids": [
                "e272.3"
            ]
        },
        {
            "text": "PoT benefits strongly from code-pretrained bases: Code-Llama-based MAmmoTH-Coder models consistently outperform Llama-2-based MAmmoTH of similar or larger sizes on OOD arithmetic tasks.",
            "uuids": [
                "e320.5"
            ]
        },
        {
            "text": "Primary PoT failure mode on TAT-QA is value grounding errors (47% of failures), followed by logic generation errors (33%), with only 15% having both types of errors, indicating arithmetic execution itself is reliable when grounding is correct.",
            "uuids": [
                "e298.5"
            ]
        },
        {
            "text": "PAL shows higher confidence and more consistent token sequences for numeric operations compared to CoT: programming constructs like len(objects) or objects[-1] are more uniform than variable natural language expressions.",
            "uuids": [
                "e303.6"
            ]
        },
        {
            "text": "Hybrid decoding (PoT-first with CoT fallback) yields best results on MAmmoTH: GSM8K 53.6%, MATH 31.5%, leveraging program precision when executable and natural language flexibility when not.",
            "uuids": [
                "e320.0"
            ]
        },
        {
            "text": "PoT enables solving problems with iterative computation, high-precision floats, polynomial/algebraic equations, and symbolic manipulation via SymPy that are difficult for pure language generation.",
            "uuids": [
                "e298.0"
            ]
        },
        {
            "text": "Chain-of-thought with external calculator applied post-hoc to equations improves accuracy (e.g., Codex GSM8K 63.1% → 65.4% with calculator), showing even CoT benefits from delegating final computation.",
            "uuids": [
                "e309.2"
            ]
        },
        {
            "text": "Program-aided approaches scale more favorably with increased problem complexity (number of steps, precision requirements) than direct generation approaches.",
            "uuids": [
                "e298.0",
                "e295.0"
            ]
        },
        {
            "text": "PoT-only fine-tuning often yields better OOD generalization for numeric tasks than CoT-only, suggesting program generation learns more robust computational patterns.",
            "uuids": [
                "e320.3"
            ]
        },
        {
            "text": "Code-davinci-002 shows substantially higher performance on arithmetic with PoT than text-davinci-002 with CoT, demonstrating the importance of code pretraining for delegation effectiveness.",
            "uuids": [
                "e298.0",
                "e309.2"
            ]
        }
    ],
    "theory_statements": [
        "Language models achieve higher arithmetic accuracy by generating executable programs than by directly generating numeric answers, with gains ranging from modest to dramatic (20+ percentage points) depending on problem complexity.",
        "Program generation separates semantic understanding and problem decomposition (model's strength) from numeric computation (interpreter's strength), avoiding accumulation of token-level errors.",
        "Code-pretrained models have substantially stronger priors for generating syntactically correct and executable programs, making delegation more effective.",
        "The primary failure mode in program-based approaches is incorrect variable grounding (mapping problem quantities to program variables), not arithmetic execution errors.",
        "Program-based approaches scale better to complex, multi-step, high-precision computations than direct answer generation because they avoid error propagation.",
        "The effectiveness of delegation depends on the target language's expressiveness, available libraries (SymPy, NumPy), and the model's code generation ability.",
        "Hybrid approaches (program generation with natural language fallback) combine the precision of delegation with the flexibility of natural language reasoning, achieving the best overall performance.",
        "Program generation provides more consistent, deterministic outputs than natural language reasoning for numeric operations due to explicit programming constructs.",
        "Programming languages' explicit constructs (e.g., len(), indexing) are more reliable than variable natural language expressions for numeric operations.",
        "The benefit of delegation increases with problem computational complexity: larger gains on computation-heavy tasks, smaller gains on simple arithmetic."
    ],
    "new_predictions_likely": [
        "Models fine-tuned on program generation should show larger improvements on arithmetic than models fine-tuned on direct answer generation, especially for computation-heavy problems.",
        "Providing models with access to more powerful libraries (e.g., symbolic math systems like Mathematica, SageMath) should improve performance on complex symbolic operations.",
        "Program-based approaches should show better performance on operations requiring high precision (many decimal places, large integers) than direct generation.",
        "Models should show higher confidence (lower entropy) and more consistent token sequences when generating programs than when generating numeric answers directly.",
        "The performance gap between program-based and direct generation should increase with the number of computational steps required.",
        "Fine-tuning on diverse programming languages should improve delegation effectiveness across different target languages.",
        "Hybrid approaches should show better robustness to distribution shifts than pure program or pure natural language approaches."
    ],
    "new_predictions_unknown": [
        "Whether program generation can be made as sample-efficient as direct answer generation during training is unclear, given the need to learn both semantic mapping and code syntax.",
        "It's unknown whether models can learn to automatically and reliably choose between program generation and direct computation based on problem characteristics without explicit hybrid strategies.",
        "Whether program-based approaches generalize effectively to mathematical domains beyond arithmetic (e.g., geometry, proof-based mathematics, topology) is uncertain.",
        "The extent to which program generation ability transfers across programming languages (Python → Prolog → other languages) is unknown.",
        "Whether delegation strategies can be made efficient enough for real-time applications given the overhead of code execution is unclear.",
        "It's unknown whether models can learn to generate programs that are not just correct but also efficient (optimal time/space complexity).",
        "Whether program-based approaches can handle problems that are inherently difficult to express programmatically (e.g., some visual or spatial reasoning problems) is uncertain."
    ],
    "negative_experiments": [
        "Finding that program generation does not improve accuracy over direct generation for computation-heavy tasks would challenge the core claim of delegation benefits.",
        "Discovering that non-code-pretrained models generate programs as effectively as code-pretrained models would challenge the importance of code pretraining.",
        "Observing that program-based approaches fail more often due to execution errors than grounding errors would challenge the characterization of primary failure modes.",
        "Finding that direct generation can match program generation accuracy with sufficient training data would challenge the necessity of delegation.",
        "Discovering that the performance gap between program and direct generation does not increase with computational complexity would challenge the scaling advantage claim.",
        "Finding that hybrid approaches do not outperform pure program generation would challenge the value of fallback strategies.",
        "Observing that program generation does not provide more consistent outputs than natural language would challenge the consistency claim."
    ],
    "unaccounted_for": [
        {
            "text": "How models learn the mapping from natural language problems to program structures is not mechanistically detailed at the attention-head or circuit level.",
            "uuids": [
                "e298.0",
                "e295.0"
            ]
        },
        {
            "text": "The optimal choice of target programming language for different problem types is not fully specified, though Python appears favored due to pretraining prevalence.",
            "uuids": [
                "e272.3"
            ]
        },
        {
            "text": "How to automatically and reliably detect when program generation will fail and fallback is needed is not fully characterized beyond simple executability checks.",
            "uuids": [
                "e320.0"
            ]
        },
        {
            "text": "The computational and latency costs of program execution in production systems are not thoroughly analyzed.",
            "uuids": [
                "e298.0"
            ]
        },
        {
            "text": "How to handle problems that require iterative refinement or interactive computation is not fully addressed.",
            "uuids": [
                "e295.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models (MathGLM-2B) achieve very high arithmetic accuracy (93.03%) through direct generation with step-by-step supervision without program delegation, suggesting delegation is not strictly necessary for high performance.",
            "uuids": [
                "e265.0"
            ]
        },
        {
            "text": "Program generation requires additional infrastructure (interpreters, libraries, execution environments) and may not be practical in all deployment scenarios, especially resource-constrained or offline settings.",
            "uuids": [
                "e298.0"
            ]
        },
        {
            "text": "Minerva achieves strong performance on mathematical reasoning through continued pretraining on mathematical text without external tools, suggesting in-language computation can be effective.",
            "uuids": [
                "e293.2"
            ]
        },
        {
            "text": "Some models show that with appropriate training (e.g., RevOrder, LEFT), direct generation can achieve near-perfect accuracy on arithmetic, challenging the necessity of delegation for all arithmetic tasks.",
            "uuids": [
                "e263.0",
                "e266.0"
            ]
        }
    ],
    "special_cases": [
        "Program generation is most beneficial for computation-heavy tasks (multi-step, high-precision, iterative) and less important for simple arithmetic or tasks requiring primarily semantic reasoning.",
        "The choice of programming language matters significantly: Python may be better than Prolog due to pretraining prevalence, but Prolog may be better for certain logical reasoning tasks.",
        "Program generation may be less effective for problems that are difficult to express programmatically (e.g., some geometry problems, visual reasoning, proof-based mathematics).",
        "Very simple arithmetic may not benefit from program generation due to the overhead of code generation and execution.",
        "The effectiveness of delegation depends on the availability and quality of relevant libraries: symbolic math benefits from SymPy, numeric computation from NumPy.",
        "Hybrid approaches are most valuable when the model can generate programs for most problems but needs fallback for edge cases.",
        "Code-pretrained models show much larger benefits from program-based approaches than text-only pretrained models.",
        "The benefit of delegation increases with the precision requirements of the task: high-precision arithmetic shows larger gains than approximate computation.",
        "Program generation may introduce new failure modes (syntax errors, runtime errors) that don't exist in direct generation, though these are typically less frequent than arithmetic errors in direct generation."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Gao et al. (2022) PAL: Program-aided Language Models [Directly proposes program-aided approach for mathematical reasoning, introduces PAL method]",
            "Chen et al. (2022) Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks [Directly proposes PoT approach, demonstrates separation of reasoning and computation]",
            "Drori et al. (2021) A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level [Demonstrates program synthesis with Codex for math problems]",
            "Yue et al. (2023) MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning [Proposes hybrid CoT/PoT approach and demonstrates benefits of code-pretrained bases]",
            "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [Early work showing benefits of external computation for verification]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 5,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Task Decomposition and Process Supervision Theory of LLM Self-Reflection - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-618</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-618</p>
                <p><strong>Name:</strong> Task Decomposition and Process Supervision Theory of LLM Self-Reflection</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that the effectiveness of self-reflection and answer improvement in language models is fundamentally determined by the degree to which the reflection process decomposes complex tasks into verifiable sub-tasks and supervises the process at the intermediate step level. Iterative self-reflection is most effective when the model is prompted (or trained) to break down reasoning into explicit steps, verify or regenerate each step independently, and use process-level feedback (rather than only outcome-level feedback) to guide refinement. Single-stage or global checking is less effective, as is reflection that only operates at the final answer level. The theory predicts that methods such as Chain-of-Verification, SelfCheck, Step-Back, and process supervision will consistently outperform global or undifferentiated self-reflection, especially on complex, multi-step reasoning tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Decomposition Law of Self-Reflection (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reflection_pipeline &#8594; decomposes_task_into &#8594; verifiable_subtasks</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; reflection_pipeline &#8594; achieves_higher &#8594; answer_quality_improvement</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SelfCheck's stepwise regenerate-and-compare pipeline outperforms single-stage or global checking, which do not decompose the task. <a href="../results/extraction-result-5447.html#e5447.0" class="evidence-link">[e5447.0]</a> <a href="../results/extraction-result-5447.html#e5447.4" class="evidence-link">[e5447.4]</a> <a href="../results/extraction-result-5447.html#e5447.3" class="evidence-link">[e5447.3]</a> </li>
    <li>Chain-of-Verification (CoVe) reduces hallucinations and increases precision by planning and verifying each claimed fact or answer span independently. <a href="../results/extraction-result-5183.html#e5183.0" class="evidence-link">[e5183.0]</a> <a href="../results/extraction-result-5183.html#e5183.2" class="evidence-link">[e5183.2]</a> <a href="../results/extraction-result-5220.html#e5220.5" class="evidence-link">[e5220.5]</a> </li>
    <li>Step-Back prompting, which explicitly abstracts and then reasons, yields large improvements on multi-step reasoning tasks. <a href="../results/extraction-result-5220.html#e5220.7" class="evidence-link">[e5220.7]</a> </li>
    <li>ThoT (Thread of Thought) segments and incrementally refines context, leading to large improvements in QA and conversation tasks. <a href="../results/extraction-result-5220.html#e5220.6" class="evidence-link">[e5220.6]</a> </li>
    <li>LogiCoT injects logical verification into each reasoning step, improving accuracy over standard CoT. <a href="../results/extraction-result-5220.html#e5220.1" class="evidence-link">[e5220.1]</a> </li>
    <li>Process supervision and step-by-step verification (Lightman et al.) are cited as effective for reducing intermediate errors. <a href="../results/extraction-result-5466.html#e5466.4" class="evidence-link">[e5466.4]</a> </li>
    <li>PEER's Plan-Edit-Explain loop, which decomposes editing into planning, editing, and explanation, outperforms single-pass editing. <a href="../results/extraction-result-5435.html#e5435.0" class="evidence-link">[e5435.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While process supervision is known, this theory newly synthesizes evidence across LLM self-reflection literature to formalize decomposition as the central mechanism for robust self-correction.</p>            <p><strong>What Already Exists:</strong> Process supervision and stepwise verification are known in some prior LLM and ML literature.</p>            <p><strong>What is Novel:</strong> The explicit identification of task decomposition and process-level supervision as the key drivers of effective LLM self-reflection, and the prediction that global or undifferentiated reflection is fundamentally limited.</p>
            <p><strong>References:</strong> <ul>
    <li>Lightman et al. (2023) Let's verify step by step [process supervision, stepwise verification]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [chain-of-thought, ensemble]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative refinement with self-feedback [iterative self-reflection]</li>
</ul>
            <h3>Statement 1: Process Supervision Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reflection_pipeline &#8594; provides_feedback_on &#8594; intermediate_steps</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; reflection_pipeline &#8594; reduces &#8594; intermediate_errors_and_hallucinations</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>REFINER and process supervision methods that provide feedback on intermediate reasoning steps yield larger improvements than those that only supervise the final answer. <a href="../results/extraction-result-5475.html#e5475.0" class="evidence-link">[e5475.0]</a> <a href="../results/extraction-result-5466.html#e5466.4" class="evidence-link">[e5466.4]</a> </li>
    <li>Chain-of-Verification and SelfCheck reduce hallucinations and errors by verifying each step or claim, not just the final answer. <a href="../results/extraction-result-5183.html#e5183.0" class="evidence-link">[e5183.0]</a> <a href="../results/extraction-result-5183.html#e5183.2" class="evidence-link">[e5183.2]</a> <a href="../results/extraction-result-5447.html#e5447.0" class="evidence-link">[e5447.0]</a> </li>
    <li>LogiCoT's think-verify-revise loop applies symbolic logic to each step, improving accuracy. <a href="../results/extraction-result-5220.html#e5220.1" class="evidence-link">[e5220.1]</a> </li>
    <li>PEER's iterative plan-edit-explain loop, which provides feedback at each edit step, improves editing quality. <a href="../results/extraction-result-5435.html#e5435.0" class="evidence-link">[e5435.0]</a> </li>
    <li>ThoT's segment-wise summarization and refinement reduces errors in long-context QA. <a href="../results/extraction-result-5220.html#e5220.6" class="evidence-link">[e5220.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends process supervision principles to the domain of LLM self-reflection, synthesizing evidence from recent work.</p>            <p><strong>What Already Exists:</strong> Process supervision is known in ML and some LLM literature.</p>            <p><strong>What is Novel:</strong> The formalization of process supervision as a necessary condition for robust self-reflection in LLMs, and the prediction that final-answer-only feedback is insufficient for complex tasks.</p>
            <p><strong>References:</strong> <ul>
    <li>Lightman et al. (2023) Let's verify step by step [process supervision]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative refinement with self-feedback [iterative self-reflection]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [chain-of-thought, ensemble]</li>
</ul>
            <h3>Statement 2: Global Checking Limitation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reflection_pipeline &#8594; uses_only &#8594; global_or_final_answer_checking</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; reflection_pipeline &#8594; is_limited_in &#8594; error_detection_and_correction</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Global direct checking nearly always labels solutions as correct and provides little useful discrimination or improvement. <a href="../results/extraction-result-5447.html#e5447.3" class="evidence-link">[e5447.3]</a> </li>
    <li>Single-stage or global self-reflection pipelines show little or no improvement over baselines, especially on complex reasoning tasks. <a href="../results/extraction-result-5447.html#e5447.4" class="evidence-link">[e5447.4]</a> <a href="../results/extraction-result-5455.html#e5455.3" class="evidence-link">[e5455.3]</a> </li>
    <li>Self-consistency as an evaluator (majority vote over multiple samples) can still amplify bias and does not guarantee error correction. <a href="../results/extraction-result-5219.html#e5219.6" class="evidence-link">[e5219.6]</a> </li>
    <li>Self-reflection pipelines that only operate at the final answer level (e.g., Self-Refine on some math tasks) show only small or inconsistent improvements. <a href="../results/extraction-result-5475.html#e5475.2" class="evidence-link">[e5475.2]</a> <a href="../results/extraction-result-5475.html#e5475.1" class="evidence-link">[e5475.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This law is new in the context of LLM self-reflection, synthesizing recent empirical findings.</p>            <p><strong>What Already Exists:</strong> Known that global checking is less effective in some ML settings.</p>            <p><strong>What is Novel:</strong> The explicit identification and empirical demonstration that global checking is fundamentally limited for LLM self-reflection, especially on multi-step tasks.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [chain-of-thought, ensemble]</li>
    <li>Lightman et al. (2023) Let's verify step by step [process supervision]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a reflection pipeline is constructed that decomposes a complex task into independently verifiable sub-tasks, and provides feedback at each step, it will outperform a pipeline that only checks the final answer.</li>
                <li>If process supervision is applied to a new domain (e.g., code generation, translation), intermediate error rates will decrease more than with final-answer-only supervision.</li>
                <li>If a model is trained to provide and act on feedback at each reasoning step, it will be more robust to hallucinations and intermediate errors than a model trained only on final answers.</li>
                <li>If a reflection pipeline is ablated to remove stepwise feedback, performance will drop, especially on tasks with many intermediate steps.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained to generate and verify intermediate steps in a completely unsupervised way (without any ground-truth intermediate labels), it may still achieve significant improvements over global checking.</li>
                <li>If process supervision is applied recursively (i.e., steps within steps), the improvement may be superlinear, but it is unknown whether diminishing returns or new failure modes will emerge.</li>
                <li>If process supervision is combined with external feedback (e.g., tool outputs at each step), the improvement may be multiplicative, but the interaction is not yet known.</li>
                <li>If process supervision is applied to creative or open-ended tasks (e.g., story writing), it is unknown whether it will yield measurable improvements.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a global-checking-only pipeline consistently outperforms stepwise or process-supervised pipelines on complex reasoning tasks, this would call the theory into question.</li>
                <li>If process supervision does not reduce intermediate errors or hallucinations compared to final-answer-only feedback, this would challenge the process supervision law.</li>
                <li>If decomposition into sub-tasks does not yield improvement over monolithic reflection, this would challenge the decomposition law.</li>
                <li>If stepwise feedback introduces new types of errors or amplifies hallucinations, the theory would need revision.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks (e.g., creative writing, translation) may not naturally decompose into verifiable sub-tasks, yet still benefit from self-reflection. <a href="../results/extraction-result-5443.html#e5443.4" class="evidence-link">[e5443.4]</a> <a href="../results/extraction-result-5207.html#e5207.0" class="evidence-link">[e5207.0]</a> </li>
    <li>Cases where a single, well-calibrated global check is sufficient for simple tasks. <a href="../results/extraction-result-5447.html#e5447.3" class="evidence-link">[e5447.3]</a> </li>
    <li>Gemini 1.5-Flash shows a dramatic improvement from a single self-reflection pass, even without explicit decomposition, when the initial output omits reasoning steps. <a href="../results/extraction-result-5202.html#e5202.2" class="evidence-link">[e5202.2]</a> </li>
    <li>Some self-reflection pipelines (e.g., Self-Refine on certain math tasks) show small but consistent improvements over CoT, even without explicit process supervision. <a href="../results/extraction-result-5475.html#e5475.2" class="evidence-link">[e5475.2]</a> <a href="../results/extraction-result-5475.html#e5475.1" class="evidence-link">[e5475.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes and extends process supervision principles to LLM self-reflection, introducing new, testable laws about decomposition and intermediate feedback.</p>
            <p><strong>References:</strong> <ul>
    <li>Lightman et al. (2023) Let's verify step by step [process supervision]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [chain-of-thought, ensemble]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative refinement with self-feedback [iterative self-reflection]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Task Decomposition and Process Supervision Theory of LLM Self-Reflection",
    "theory_description": "This theory posits that the effectiveness of self-reflection and answer improvement in language models is fundamentally determined by the degree to which the reflection process decomposes complex tasks into verifiable sub-tasks and supervises the process at the intermediate step level. Iterative self-reflection is most effective when the model is prompted (or trained) to break down reasoning into explicit steps, verify or regenerate each step independently, and use process-level feedback (rather than only outcome-level feedback) to guide refinement. Single-stage or global checking is less effective, as is reflection that only operates at the final answer level. The theory predicts that methods such as Chain-of-Verification, SelfCheck, Step-Back, and process supervision will consistently outperform global or undifferentiated self-reflection, especially on complex, multi-step reasoning tasks.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Decomposition Law of Self-Reflection",
                "if": [
                    {
                        "subject": "reflection_pipeline",
                        "relation": "decomposes_task_into",
                        "object": "verifiable_subtasks"
                    }
                ],
                "then": [
                    {
                        "subject": "reflection_pipeline",
                        "relation": "achieves_higher",
                        "object": "answer_quality_improvement"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SelfCheck's stepwise regenerate-and-compare pipeline outperforms single-stage or global checking, which do not decompose the task.",
                        "uuids": [
                            "e5447.0",
                            "e5447.4",
                            "e5447.3"
                        ]
                    },
                    {
                        "text": "Chain-of-Verification (CoVe) reduces hallucinations and increases precision by planning and verifying each claimed fact or answer span independently.",
                        "uuids": [
                            "e5183.0",
                            "e5183.2",
                            "e5220.5"
                        ]
                    },
                    {
                        "text": "Step-Back prompting, which explicitly abstracts and then reasons, yields large improvements on multi-step reasoning tasks.",
                        "uuids": [
                            "e5220.7"
                        ]
                    },
                    {
                        "text": "ThoT (Thread of Thought) segments and incrementally refines context, leading to large improvements in QA and conversation tasks.",
                        "uuids": [
                            "e5220.6"
                        ]
                    },
                    {
                        "text": "LogiCoT injects logical verification into each reasoning step, improving accuracy over standard CoT.",
                        "uuids": [
                            "e5220.1"
                        ]
                    },
                    {
                        "text": "Process supervision and step-by-step verification (Lightman et al.) are cited as effective for reducing intermediate errors.",
                        "uuids": [
                            "e5466.4"
                        ]
                    },
                    {
                        "text": "PEER's Plan-Edit-Explain loop, which decomposes editing into planning, editing, and explanation, outperforms single-pass editing.",
                        "uuids": [
                            "e5435.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Process supervision and stepwise verification are known in some prior LLM and ML literature.",
                    "what_is_novel": "The explicit identification of task decomposition and process-level supervision as the key drivers of effective LLM self-reflection, and the prediction that global or undifferentiated reflection is fundamentally limited.",
                    "classification_explanation": "While process supervision is known, this theory newly synthesizes evidence across LLM self-reflection literature to formalize decomposition as the central mechanism for robust self-correction.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lightman et al. (2023) Let's verify step by step [process supervision, stepwise verification]",
                        "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [chain-of-thought, ensemble]",
                        "Madaan et al. (2023) Self-Refine: Iterative refinement with self-feedback [iterative self-reflection]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Process Supervision Law",
                "if": [
                    {
                        "subject": "reflection_pipeline",
                        "relation": "provides_feedback_on",
                        "object": "intermediate_steps"
                    }
                ],
                "then": [
                    {
                        "subject": "reflection_pipeline",
                        "relation": "reduces",
                        "object": "intermediate_errors_and_hallucinations"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "REFINER and process supervision methods that provide feedback on intermediate reasoning steps yield larger improvements than those that only supervise the final answer.",
                        "uuids": [
                            "e5475.0",
                            "e5466.4"
                        ]
                    },
                    {
                        "text": "Chain-of-Verification and SelfCheck reduce hallucinations and errors by verifying each step or claim, not just the final answer.",
                        "uuids": [
                            "e5183.0",
                            "e5183.2",
                            "e5447.0"
                        ]
                    },
                    {
                        "text": "LogiCoT's think-verify-revise loop applies symbolic logic to each step, improving accuracy.",
                        "uuids": [
                            "e5220.1"
                        ]
                    },
                    {
                        "text": "PEER's iterative plan-edit-explain loop, which provides feedback at each edit step, improves editing quality.",
                        "uuids": [
                            "e5435.0"
                        ]
                    },
                    {
                        "text": "ThoT's segment-wise summarization and refinement reduces errors in long-context QA.",
                        "uuids": [
                            "e5220.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Process supervision is known in ML and some LLM literature.",
                    "what_is_novel": "The formalization of process supervision as a necessary condition for robust self-reflection in LLMs, and the prediction that final-answer-only feedback is insufficient for complex tasks.",
                    "classification_explanation": "This law extends process supervision principles to the domain of LLM self-reflection, synthesizing evidence from recent work.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lightman et al. (2023) Let's verify step by step [process supervision]",
                        "Madaan et al. (2023) Self-Refine: Iterative refinement with self-feedback [iterative self-reflection]",
                        "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [chain-of-thought, ensemble]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Global Checking Limitation Law",
                "if": [
                    {
                        "subject": "reflection_pipeline",
                        "relation": "uses_only",
                        "object": "global_or_final_answer_checking"
                    }
                ],
                "then": [
                    {
                        "subject": "reflection_pipeline",
                        "relation": "is_limited_in",
                        "object": "error_detection_and_correction"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Global direct checking nearly always labels solutions as correct and provides little useful discrimination or improvement.",
                        "uuids": [
                            "e5447.3"
                        ]
                    },
                    {
                        "text": "Single-stage or global self-reflection pipelines show little or no improvement over baselines, especially on complex reasoning tasks.",
                        "uuids": [
                            "e5447.4",
                            "e5455.3"
                        ]
                    },
                    {
                        "text": "Self-consistency as an evaluator (majority vote over multiple samples) can still amplify bias and does not guarantee error correction.",
                        "uuids": [
                            "e5219.6"
                        ]
                    },
                    {
                        "text": "Self-reflection pipelines that only operate at the final answer level (e.g., Self-Refine on some math tasks) show only small or inconsistent improvements.",
                        "uuids": [
                            "e5475.2",
                            "e5475.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Known that global checking is less effective in some ML settings.",
                    "what_is_novel": "The explicit identification and empirical demonstration that global checking is fundamentally limited for LLM self-reflection, especially on multi-step tasks.",
                    "classification_explanation": "This law is new in the context of LLM self-reflection, synthesizing recent empirical findings.",
                    "likely_classification": "new",
                    "references": [
                        "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [chain-of-thought, ensemble]",
                        "Lightman et al. (2023) Let's verify step by step [process supervision]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a reflection pipeline is constructed that decomposes a complex task into independently verifiable sub-tasks, and provides feedback at each step, it will outperform a pipeline that only checks the final answer.",
        "If process supervision is applied to a new domain (e.g., code generation, translation), intermediate error rates will decrease more than with final-answer-only supervision.",
        "If a model is trained to provide and act on feedback at each reasoning step, it will be more robust to hallucinations and intermediate errors than a model trained only on final answers.",
        "If a reflection pipeline is ablated to remove stepwise feedback, performance will drop, especially on tasks with many intermediate steps."
    ],
    "new_predictions_unknown": [
        "If a model is trained to generate and verify intermediate steps in a completely unsupervised way (without any ground-truth intermediate labels), it may still achieve significant improvements over global checking.",
        "If process supervision is applied recursively (i.e., steps within steps), the improvement may be superlinear, but it is unknown whether diminishing returns or new failure modes will emerge.",
        "If process supervision is combined with external feedback (e.g., tool outputs at each step), the improvement may be multiplicative, but the interaction is not yet known.",
        "If process supervision is applied to creative or open-ended tasks (e.g., story writing), it is unknown whether it will yield measurable improvements."
    ],
    "negative_experiments": [
        "If a global-checking-only pipeline consistently outperforms stepwise or process-supervised pipelines on complex reasoning tasks, this would call the theory into question.",
        "If process supervision does not reduce intermediate errors or hallucinations compared to final-answer-only feedback, this would challenge the process supervision law.",
        "If decomposition into sub-tasks does not yield improvement over monolithic reflection, this would challenge the decomposition law.",
        "If stepwise feedback introduces new types of errors or amplifies hallucinations, the theory would need revision."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks (e.g., creative writing, translation) may not naturally decompose into verifiable sub-tasks, yet still benefit from self-reflection.",
            "uuids": [
                "e5443.4",
                "e5207.0"
            ]
        },
        {
            "text": "Cases where a single, well-calibrated global check is sufficient for simple tasks.",
            "uuids": [
                "e5447.3"
            ]
        },
        {
            "text": "Gemini 1.5-Flash shows a dramatic improvement from a single self-reflection pass, even without explicit decomposition, when the initial output omits reasoning steps.",
            "uuids": [
                "e5202.2"
            ]
        },
        {
            "text": "Some self-reflection pipelines (e.g., Self-Refine on certain math tasks) show small but consistent improvements over CoT, even without explicit process supervision.",
            "uuids": [
                "e5475.2",
                "e5475.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Gemini 1.5-Flash shows a dramatic improvement from a single self-reflection pass, even without explicit decomposition, when the initial output omits reasoning steps.",
            "uuids": [
                "e5202.2"
            ]
        },
        {
            "text": "Some self-reflection pipelines (e.g., Self-Refine on certain math tasks) show small but consistent improvements over CoT, even without explicit process supervision.",
            "uuids": [
                "e5475.2",
                "e5475.1"
            ]
        }
    ],
    "special_cases": [
        "If the task is inherently atomic (cannot be decomposed), process supervision may not yield additional benefit.",
        "If the model is already highly accurate at each step, further decomposition may yield diminishing returns.",
        "For tasks where intermediate steps are not verifiable (e.g., creative writing), process supervision may not be applicable.",
        "If the initial output omits all reasoning steps, a single reflection pass that adds reasoning may yield large gains (as in Gemini 1.5-Flash)."
    ],
    "existing_theory": {
        "what_already_exists": "Process supervision and stepwise verification are known in ML and some LLM literature.",
        "what_is_novel": "The formalization of decomposition and process supervision as necessary and sufficient conditions for robust LLM self-reflection, and the prediction of the limitations of global checking.",
        "classification_explanation": "This theory synthesizes and extends process supervision principles to LLM self-reflection, introducing new, testable laws about decomposition and intermediate feedback.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lightman et al. (2023) Let's verify step by step [process supervision]",
            "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [chain-of-thought, ensemble]",
            "Madaan et al. (2023) Self-Refine: Iterative refinement with self-feedback [iterative self-reflection]"
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
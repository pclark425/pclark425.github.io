<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Theory of Language Model Logical Reasoning - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1088</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1088</p>
                <p><strong>Name:</strong> Dual-Process Theory of Language Model Logical Reasoning</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models (LMs) can best perform strict logical reasoning when they are architecturally or procedurally augmented to support two distinct but interacting processes: (1) a fast, pattern-matching, context-driven process (analogous to System 1 in human cognition) and (2) a slow, explicit, symbolic manipulation process (analogous to System 2). The theory asserts that strict logical reasoning emerges when the LM can dynamically invoke and coordinate these processes, using the fast process for context and candidate generation, and the slow process for explicit logical inference and verification.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Separation of Pattern and Symbolic Processes (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; has_architecture &#8594; dual-process (pattern + symbolic)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_perform &#8594; strict logical reasoning with higher accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show LMs struggle with strict logic tasks unless augmented with explicit symbolic modules or external tools. </li>
    <li>Human cognition research supports dual-process models for reasoning, with System 2 enabling explicit logic. </li>
    <li>Hybrid neuro-symbolic models in AI demonstrate improved logical reasoning over pure neural models. </li>
    <li>Language models often fail on tasks requiring stepwise logical deduction unless provided with explicit reasoning scaffolds. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While dual-process models are known in cognitive science, their direct application and architectural mapping to LMs for strict logic is new.</p>            <p><strong>What Already Exists:</strong> Dual-process theories are well-established in cognitive science, and some hybrid neuro-symbolic models exist in AI.</p>            <p><strong>What is Novel:</strong> The explicit claim that LMs require a dual-process architecture for best strict logical reasoning, and the mapping of these processes to LM components, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2003) In two minds: dual-process accounts of reasoning [dual-process theory in human reasoning]</li>
    <li>Lake et al. (2017) Building machines that learn and think like people [neuro-symbolic reasoning]</li>
    <li>Li et al. (2023) Large Language Models as Zero-Shot Reasoners [prompting LMs for reasoning, but not explicit dual-process architecture]</li>
</ul>
            <h3>Statement 1: Dynamic Process Coordination for Logical Tasks (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_invoke &#8594; symbolic process when logical structure detected<span style="color: #888888;">, and</span></div>
        <div>&#8226; input &#8594; contains &#8594; explicit logical structure or cues</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; achieves &#8594; improved logical consistency and validity in outputs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompting LMs with explicit logical cues (e.g., 'Let's think step by step') improves logical task performance. </li>
    <li>External tool use (e.g., code interpreters, theorem provers) boosts LM logical accuracy. </li>
    <li>Chain-of-thought prompting and tool-augmented LMs show improved logical consistency when logical structure is present in the input. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The idea of dynamic process invocation is new in the context of LMs and strict logic, though related to tool-augmented LMs.</p>            <p><strong>What Already Exists:</strong> Prompt engineering and tool augmentation are known to improve LM reasoning, but not formalized as dynamic process coordination.</p>            <p><strong>What is Novel:</strong> The law formalizes the need for dynamic invocation and coordination of processes based on input structure.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]</li>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [dynamic tool use]</li>
    <li>Creswell et al. (2022) Selection-inference: Exploiting large language models for interpretable logical reasoning [explicit logical structure in prompts]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LM is architecturally augmented with a symbolic reasoning module that is invoked only when logical cues are detected, its accuracy on strict logic benchmarks will surpass that of standard LMs.</li>
                <li>Prompting an LM with explicit logical structure (e.g., formal logic notation) will increase the likelihood that the model invokes symbolic reasoning processes, leading to more valid outputs.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a dual-process LM is trained end-to-end with reinforcement learning on logical tasks, it may develop emergent coordination strategies that outperform both pure neural and pure symbolic systems.</li>
                <li>Dynamic process coordination may enable LMs to generalize strict logical reasoning to novel domains (e.g., mathematical proofs) without explicit retraining.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a dual-process LM does not outperform a standard LM on strict logical reasoning tasks, the theory is called into question.</li>
                <li>If explicit logical cues in the input do not increase the invocation of symbolic processes or improve logical consistency, the theory is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs show limited logical reasoning ability even without explicit symbolic modules, possibly due to implicit pattern learning. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is inspired by existing cognitive science but is novel in its application and architectural prescription for LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2003) In two minds: dual-process accounts of reasoning [dual-process theory in human reasoning]</li>
    <li>Lake et al. (2017) Building machines that learn and think like people [neuro-symbolic reasoning]</li>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [dynamic tool use in LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Theory of Language Model Logical Reasoning",
    "theory_description": "This theory posits that language models (LMs) can best perform strict logical reasoning when they are architecturally or procedurally augmented to support two distinct but interacting processes: (1) a fast, pattern-matching, context-driven process (analogous to System 1 in human cognition) and (2) a slow, explicit, symbolic manipulation process (analogous to System 2). The theory asserts that strict logical reasoning emerges when the LM can dynamically invoke and coordinate these processes, using the fast process for context and candidate generation, and the slow process for explicit logical inference and verification.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Separation of Pattern and Symbolic Processes",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "has_architecture",
                        "object": "dual-process (pattern + symbolic)"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_perform",
                        "object": "strict logical reasoning with higher accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show LMs struggle with strict logic tasks unless augmented with explicit symbolic modules or external tools.",
                        "uuids": []
                    },
                    {
                        "text": "Human cognition research supports dual-process models for reasoning, with System 2 enabling explicit logic.",
                        "uuids": []
                    },
                    {
                        "text": "Hybrid neuro-symbolic models in AI demonstrate improved logical reasoning over pure neural models.",
                        "uuids": []
                    },
                    {
                        "text": "Language models often fail on tasks requiring stepwise logical deduction unless provided with explicit reasoning scaffolds.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Dual-process theories are well-established in cognitive science, and some hybrid neuro-symbolic models exist in AI.",
                    "what_is_novel": "The explicit claim that LMs require a dual-process architecture for best strict logical reasoning, and the mapping of these processes to LM components, is novel.",
                    "classification_explanation": "While dual-process models are known in cognitive science, their direct application and architectural mapping to LMs for strict logic is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Evans (2003) In two minds: dual-process accounts of reasoning [dual-process theory in human reasoning]",
                        "Lake et al. (2017) Building machines that learn and think like people [neuro-symbolic reasoning]",
                        "Li et al. (2023) Large Language Models as Zero-Shot Reasoners [prompting LMs for reasoning, but not explicit dual-process architecture]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dynamic Process Coordination for Logical Tasks",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "can_invoke",
                        "object": "symbolic process when logical structure detected"
                    },
                    {
                        "subject": "input",
                        "relation": "contains",
                        "object": "explicit logical structure or cues"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "achieves",
                        "object": "improved logical consistency and validity in outputs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompting LMs with explicit logical cues (e.g., 'Let's think step by step') improves logical task performance.",
                        "uuids": []
                    },
                    {
                        "text": "External tool use (e.g., code interpreters, theorem provers) boosts LM logical accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought prompting and tool-augmented LMs show improved logical consistency when logical structure is present in the input.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering and tool augmentation are known to improve LM reasoning, but not formalized as dynamic process coordination.",
                    "what_is_novel": "The law formalizes the need for dynamic invocation and coordination of processes based on input structure.",
                    "classification_explanation": "The idea of dynamic process invocation is new in the context of LMs and strict logic, though related to tool-augmented LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]",
                        "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [dynamic tool use]",
                        "Creswell et al. (2022) Selection-inference: Exploiting large language models for interpretable logical reasoning [explicit logical structure in prompts]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LM is architecturally augmented with a symbolic reasoning module that is invoked only when logical cues are detected, its accuracy on strict logic benchmarks will surpass that of standard LMs.",
        "Prompting an LM with explicit logical structure (e.g., formal logic notation) will increase the likelihood that the model invokes symbolic reasoning processes, leading to more valid outputs."
    ],
    "new_predictions_unknown": [
        "If a dual-process LM is trained end-to-end with reinforcement learning on logical tasks, it may develop emergent coordination strategies that outperform both pure neural and pure symbolic systems.",
        "Dynamic process coordination may enable LMs to generalize strict logical reasoning to novel domains (e.g., mathematical proofs) without explicit retraining."
    ],
    "negative_experiments": [
        "If a dual-process LM does not outperform a standard LM on strict logical reasoning tasks, the theory is called into question.",
        "If explicit logical cues in the input do not increase the invocation of symbolic processes or improve logical consistency, the theory is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs show limited logical reasoning ability even without explicit symbolic modules, possibly due to implicit pattern learning.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "There are cases where LMs perform well on logic puzzles via pattern completion rather than explicit reasoning.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with ambiguous or underspecified logical structure may not benefit from dual-process coordination.",
        "Very small LMs may lack the capacity to support both processes effectively."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process models in cognitive science and some hybrid neuro-symbolic AI models.",
        "what_is_novel": "Explicit mapping of dual-process theory to LM architecture and process invocation for strict logical reasoning.",
        "classification_explanation": "The theory is inspired by existing cognitive science but is novel in its application and architectural prescription for LMs.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Evans (2003) In two minds: dual-process accounts of reasoning [dual-process theory in human reasoning]",
            "Lake et al. (2017) Building machines that learn and think like people [neuro-symbolic reasoning]",
            "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [dynamic tool use in LMs]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-601",
    "original_theory_name": "Neuro-Symbolic Interface Bottleneck Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
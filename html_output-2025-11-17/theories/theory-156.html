<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Validation Hierarchy Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-156</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-156</p>
                <p><strong>Name:</strong> Validation Hierarchy Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about the evaluation and validation of incremental versus transformational scientific discoveries in automated systems, based on the following results.</p>
                <p><strong>Description:</strong> Automated scientific discovery systems employ a hierarchical validation structure with multiple levels: (1) computational/algorithmic validation (internal consistency, benchmark performance, numerical verification), (2) comparative validation (against baselines, prior methods, or human performance), (3) reproducibility validation (independent replication, cross-system verification), (4) domain expert validation (peer review, expert assessment, community proofs), and (5) experimental validation (wet-lab testing, real-world deployment, prospective studies). The strength and acceptance of discovery claims correlates with progression through these levels, with transformational claims typically requiring validation at higher levels (4-5) while incremental claims may be substantiated at lower levels (1-3). However, domain-specific exceptions exist: pure mathematics may substitute formal proof for experimental validation, and computational domains with established proxies may accept computational validation against experimental benchmarks as sufficient.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Automated discovery systems that make transformational claims require validation at multiple hierarchical levels, typically including domain expert validation (level 4) and/or experimental validation (level 5).</li>
                <li>Incremental discoveries can be substantiated primarily through computational validation (level 1), comparative validation (level 2), and reproducibility validation (level 3) without requiring extensive experimental validation.</li>
                <li>The credibility and acceptance of a discovery claim is proportional to the number of validation levels successfully passed and the rigor applied at each level.</li>
                <li>Systems that lack experimental validation are typically characterized as producing 'proof-of-concept', 'computational', or 'simulation-based' results rather than validated scientific discoveries, unless operating in domains where experimental validation is not applicable.</li>
                <li>Validation approaches become more stringent, multi-modal, and externally-focused as the claimed significance of discoveries increases.</li>
                <li>Reproducibility validation (independent replication, cross-system verification, statistical reproducibility metrics) serves as a critical intermediate level between comparative and expert validation.</li>
                <li>Domain-specific validation standards exist: pure mathematics accepts formal proof as highest validation; computational domains with established experimental proxies may accept computational validation against benchmarks as sufficient for transformational claims.</li>
                <li>The validation hierarchy is not strictly linear: systems may achieve higher-level validation (e.g., experimental) without fully completing lower levels, though this is less common and may face greater scrutiny.</li>
                <li>Automated peer review and community-based validation are emerging as intermediate validation levels between computational comparison and human expert assessment.</li>
                <li>Validation requirements evolve over time: as automated systems become more prevalent and trusted, the threshold for acceptance at each level may shift.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>AlphaFold uses computational validation (CASP benchmarks with GDT scores >90 for ~66% of proteins), comparative validation (vs other computational methods), and implicit experimental validation (comparison to experimentally determined structures via X-ray/cryo-EM/NMR) <a href="../results/extraction-result-1164.html#e1164.0" class="evidence-link">[e1164.0]</a> <a href="../results/extraction-result-1200.html#e1200.0" class="evidence-link">[e1200.0]</a> <a href="../results/extraction-result-1233.html#e1233.0" class="evidence-link">[e1233.0]</a> <a href="../results/extraction-result-1266.html#e1266.5" class="evidence-link">[e1266.5]</a> </li>
    <li>SPOCK uses computational validation (N-body integrations to 10^9 orbits as ground truth), statistical validation (ROC/AUC metrics, TPR ~85-94% at FPR=10%), out-of-distribution testing on multiple datasets, and reproducibility validation (shadow integrations with perturbations) <a href="../results/extraction-result-1437.html#e1437.0" class="evidence-link">[e1437.0]</a> </li>
    <li>Adam robot scientist uses experimental validation via automated wet-lab assays (auxotrophy experiments), comparison to established biological knowledge, and peer-reviewed publication <a href="../results/extraction-result-1193.html#e1193.0" class="evidence-link">[e1193.0]</a> <a href="../results/extraction-result-1452.html#e1452.5" class="evidence-link">[e1452.5]</a> <a href="../results/extraction-result-1243.html#e1243.1" class="evidence-link">[e1243.1]</a> <a href="../results/extraction-result-1289.html#e1289.0" class="evidence-link">[e1289.0]</a> </li>
    <li>CycleResearcher uses automated review simulation (CycleReviewer with 26.89% reduction in Proxy MAE vs individual reviewers), human expert blind evaluation (3 NLP experts), comparison to preprint/accepted paper distributions, and independent reward model validation <a href="../results/extraction-result-1202.html#e1202.0" class="evidence-link">[e1202.0]</a> </li>
    <li>SciAgents uses literature-based novelty checking via Semantic Scholar API (three query permutations, top 10 results each), internal agent critique (Critic agent), but explicitly lacks experimental validation (noted as future work) <a href="../results/extraction-result-1178.html#e1178.0" class="evidence-link">[e1178.0]</a> <a href="../results/extraction-result-1178.html#e1178.2" class="evidence-link">[e1178.2]</a> </li>
    <li>LLM-SR uses computational validation (NMSE metrics with values as low as 2.12e-7 for some tasks), benchmark comparisons against PySR/uDSR/DSR/NeSymReS/E2E/GPlearn, out-of-distribution generalization tests, and ablation studies <a href="../results/extraction-result-1439.html#e1439.0" class="evidence-link">[e1439.0]</a> </li>
    <li>AtomAgents uses physics-based simulation validation (LAMMPS with NEB calculations, convergence criteria max force < 1e-3 eV/Å), comparison to literature values (lattice constants, elastic constants, surface energies), multimodal image analysis, but no experimental validation <a href="../results/extraction-result-1444.html#e1444.0" class="evidence-link">[e1444.0]</a> </li>
    <li>TAIS uses computational validation (cross-validation with ~80% accuracy), comparison to human-curated gold standards (GenQEX benchmark with 457 tasks), quantitative metrics (SR=69.08%, F1=30.27% end-to-end), but no wet-lab validation <a href="../results/extraction-result-1215.html#e1215.0" class="evidence-link">[e1215.0]</a> </li>
    <li>PaperQA uses comparison to ground-truth LitQA answers, human respondent comparison, citation-validity audits (GPT-4 had 60.78% valid citations, 29.41% full hallucination), and accuracy metrics (33.4% on LitQA standalone, 86.3% on PubMedQA_blind with retrieval) <a href="../results/extraction-result-1445.html#e1445.3" class="evidence-link">[e1445.3]</a> </li>
    <li>AI Feynman uses comparison to known ground-truth equations (100/100 on Feynman set vs 71/100 for prior state-of-the-art), held-out data validation, and ablation studies showing contribution of physics-inspired components <a href="../results/extraction-result-1431.html#e1431.0" class="evidence-link">[e1431.0]</a> </li>
    <li>Eunomia uses comparison to human-curated datasets (expert chemists labeled 101 articles with 371 MOFs), expert-labeled ground truth, F1 metrics (hosts 0.905, dopants 0.920 with CoV), and internal verification via Chain-of-Verification module <a href="../results/extraction-result-1209.html#e1209.0" class="evidence-link">[e1209.0]</a> </li>
    <li>MITM-RF uses progressive numerical validation at increasing precision (initial 10 digits for hashing, re-evaluated up to 2000 digits), community proof verification (several conjectures proven by external mathematicians), and statistical false-match estimation (<10^-40 for 50-digit agreement) <a href="../results/extraction-result-1449.html#e1449.1" class="evidence-link">[e1449.1]</a> <a href="../results/extraction-result-1453.html#e1453.1" class="evidence-link">[e1453.1]</a> </li>
    <li>Ada self-driving lab uses experimental validation via physical characterization (UV-Vis-NIR spectroscopy, 4-point-probe conductance, dark-field imaging), reproducibility across independent campaigns (two 35-sample campaigns converged to same optimum), and mechanistic post-hoc analysis <a href="../results/extraction-result-1443.html#e1443.0" class="evidence-link">[e1443.0]</a> </li>
    <li>ChemCrow and CoScientist require experimental synthesis and testing for validation, with CoScientist demonstrating closed-loop execution of chemical experiments <a href="../results/extraction-result-1432.html#e1432.1" class="evidence-link">[e1432.1]</a> <a href="../results/extraction-result-1188.html#e1188.0" class="evidence-link">[e1188.0]</a> </li>
    <li>Topaz uses downstream reconstruction quality (highest resolution achieved for ribosome dataset), comparison to published manual picks (1.72× more particles found), and implicit benchmarking against human-curated picks <a href="../results/extraction-result-1200.html#e1200.3" class="evidence-link">[e1200.3]</a> </li>
    <li>Eve uses automated in vitro assays (high-throughput screening), comparison to known bioactivity databases, active learning with Gaussian process regression, and experimental confirmation of repurposing candidates (TNP-470 against P. vivax) <a href="../results/extraction-result-1193.html#e1193.1" class="evidence-link">[e1193.1]</a> <a href="../results/extraction-result-1289.html#e1289.1" class="evidence-link">[e1289.1]</a> <a href="../results/extraction-result-1452.html#e1452.1" class="evidence-link">[e1452.1]</a> </li>
    <li>Halicin discovery used computational screening (deep neural network predictions) followed by experimental biological validation (in vitro assays demonstrating bactericidal activity against resistant strains) and peer-reviewed publication in Cell <a href="../results/extraction-result-1266.html#e1266.0" class="evidence-link">[e1266.0]</a> </li>
    <li>DATAVOYAGER uses human-in-the-loop moderation, replication of published analyses (Smith et al. 2005, Zaw et al. 2016, Alexander et al. 1982), and consistency checks between different statistical models (GLM confirming OLS results) <a href="../results/extraction-result-1227.html#e1227.0" class="evidence-link">[e1227.0]</a> </li>
    <li>Descent&Repel uses progressive precision validation (convergence to exact zero-loss solutions), integer-projection verification, and community proof verification for discovered conjectures <a href="../results/extraction-result-1449.html#e1449.2" class="evidence-link">[e1449.2]</a> </li>
    <li>HTE drop-cast platform uses reproducibility statistics (R^2 = 0.97 across 309 samples), cross-system replication (observed PCBM:oIDTBR instability in two different quaternary systems), and comparison to hypothetical manual testing <a href="../results/extraction-result-1455.html#e1455.1" class="evidence-link">[e1455.1]</a> </li>
    <li>Magnon NN uses comparison to literature values (J=0.6763 meV vs literature 0.648-0.673 meV; J'=0.0104 meV vs literature 0.006-0.012 meV), validation on withheld simulated data (MAE ±0.0055 meV for J), and generalization from simulation to real experimental data <a href="../results/extraction-result-1200.html#e1200.1" class="evidence-link">[e1200.1]</a> </li>
    <li>Ramanujan Machine uses numerical verification to high precision (up to 2000 digits), community proof verification (several conjectures proven by mathematicians), and peer-reviewed publication <a href="../results/extraction-result-1243.html#e1243.4" class="evidence-link">[e1243.4]</a> <a href="../results/extraction-result-1449.html#e1449.1" class="evidence-link">[e1449.1]</a> </li>
    <li>LLM Reviewer uses validation against ICLR 2022 OpenReview data (balanced accuracy 0.65±0.04), correlation analysis with human reviewer scores (0.18 vs human-human 0.14), and calibration studies <a href="../results/extraction-result-1214.html#e1214.1" class="evidence-link">[e1214.1]</a> </li>
    <li>Automated Theorem Proving systems use formal proof generation and machine verification, with cited successes including Four Color Theorem, Robbins' problem, Lorenz attractor existence, and Kepler conjecture <a href="../results/extraction-result-1453.html#e1453.4" class="evidence-link">[e1453.4]</a> </li>
    <li>SNIP uses benchmark comparisons on SRBench, Pareto front analysis, and ablation studies, but validation is primarily computational <a href="../results/extraction-result-1446.html#e1446.1" class="evidence-link">[e1446.1]</a> </li>
    <li>Phoenics uses campaign-level outcomes (convergence to global maxima, 7% of search space sampled to reach optimum for OPV), comparison to grid search and sequential approaches, and experimental execution of proposed conditions <a href="../results/extraction-result-1190.html#e1190.1" class="evidence-link">[e1190.1]</a> </li>
    <li>CRN/Explanatory Learning uses held-out test sets with excluded rule equivalence classes, semantic equivalence verification over full universe (R-Acc), and comparison to empiricist baselines (NRS 77.7% vs EMP-C 22.5%) <a href="../results/extraction-result-1442.html#e1442.1" class="evidence-link">[e1442.1]</a> <a href="../results/extraction-result-1224.html#e1224.1" class="evidence-link">[e1224.1]</a> </li>
    <li>LLM-NERRE uses comparison to human-curated gold standards, manual expert review for fuzzy scoring, parsability checks (98.7-100%), and comparison to baselines (F1 0.849 vs MatBERT+Proximity 0.545) <a href="../results/extraction-result-1441.html#e1441.0" class="evidence-link">[e1441.0]</a> </li>
    <li>Auto-Keras uses benchmark comparisons under matched experimental settings (12-hour search budget), comparison to human-designed networks and other NAS methods (SEAS, NASNet), and error rate metrics on standard datasets <a href="../results/extraction-result-1433.html#e1433.1" class="evidence-link">[e1433.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A system claiming transformational discovery that only uses computational validation (level 1) will face significant skepticism and require additional validation levels before widespread acceptance, unless operating in a domain where higher levels are not applicable (e.g., pure mathematics with formal proof).</li>
                <li>Systems that incorporate experimental validation loops (level 5) will produce discoveries with higher acceptance rates and faster time-to-adoption in the scientific community compared to purely computational systems (levels 1-3).</li>
                <li>The time-to-acceptance for automated discoveries will correlate inversely with the number of validation levels completed before publication, with each additional level reducing time-to-acceptance by an estimated 20-40%.</li>
                <li>Systems that achieve high reproducibility metrics (level 3) will be more readily accepted for incremental claims even without experimental validation, particularly in computational domains.</li>
                <li>Discoveries that pass validation at level 4 (domain expert validation) but fail at level 5 (experimental validation) will be reclassified as incremental or methodological contributions rather than transformational discoveries.</li>
                <li>As automated peer review systems improve (approaching human-level consistency), they will become an accepted intermediate validation level between comparative and expert validation, potentially reducing the burden on human reviewers for incremental claims.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether a new validation level (e.g., 'cross-system replication' where multiple independent automated systems must reproduce the same discovery) could emerge as a standard requirement between reproducibility and expert validation, and whether this would be widely accepted across disciplines.</li>
                <li>Whether certain domains beyond pure mathematics (e.g., theoretical physics, computational chemistry) could accept transformational claims with only computational validation (levels 1-3), bypassing experimental validation entirely, and under what specific conditions this would be acceptable.</li>
                <li>Whether the validation hierarchy could be compressed or reordered for certain types of discoveries (e.g., discoveries made by systems with proven track records) without loss of credibility, and what the minimum acceptable validation path would be.</li>
                <li>Whether automated systems that achieve superhuman performance at lower validation levels (e.g., computational validation with orders of magnitude better precision than human-achievable) could bypass higher validation levels for certain types of claims.</li>
                <li>Whether the emergence of 'meta-validation' systems (automated systems that validate other automated systems' discoveries) could create a parallel validation hierarchy that operates independently of traditional human-centered validation.</li>
                <li>Whether validation requirements will converge across disciplines as automated discovery becomes more common, or whether domain-specific validation hierarchies will persist and potentially diverge further.</li>
                <li>Whether the cost and time required for experimental validation (level 5) will create a bottleneck that fundamentally limits the rate of transformational discoveries from automated systems, or whether automated experimentation will scale sufficiently to match the rate of computational discovery generation.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding cases where discoveries validated only computationally (levels 1-2) are widely accepted as transformational in domains where experimental validation is feasible would challenge the necessity of the full hierarchy.</li>
                <li>Finding cases where extensive experimental validation (level 5) and peer review (level 4) fail to convince the community of a discovery's significance would challenge the sufficiency of the hierarchy and suggest missing validation dimensions.</li>
                <li>Demonstrating that validation level does not correlate with discovery acceptance rates (measured by citations, adoption, or community surveys) would undermine the core premise of the theory.</li>
                <li>Finding that reproducibility validation (level 3) is neither necessary nor sufficient for acceptance of incremental claims would challenge its position in the hierarchy.</li>
                <li>Showing that automated peer review (emerging level between 3 and 4) is systematically rejected by the community even when it matches or exceeds human reviewer consistency would challenge the theory's prediction about emerging validation levels.</li>
                <li>Demonstrating that the order of validation levels can be arbitrarily rearranged without affecting acceptance rates would challenge the hierarchical structure of the theory.</li>
                <li>Finding that validation requirements do not increase with claimed significance (i.e., transformational claims accepted with same validation as incremental claims) would contradict a core theory statement.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The role of social and institutional factors in determining which validation levels are required, including funding agency requirements, journal policies, and disciplinary norms </li>
    <li>How validation requirements differ across scientific disciplines beyond the mathematics/experimental science distinction (e.g., social sciences, engineering, clinical medicine) </li>
    <li>The temporal evolution of validation standards as automated systems become more prevalent and trusted, including potential feedback loops where successful automated discoveries lower validation barriers for future systems </li>
    <li>The role of negative results and failed validation attempts in the validation hierarchy, and whether systems that transparently report validation failures are treated differently </li>
    <li>How validation requirements interact with intellectual property considerations, particularly for discoveries with commercial applications </li>
    <li>The distinction between validating a discovery method/system versus validating individual discoveries produced by that system, and whether validation can be 'amortized' across multiple discoveries from the same system </li>
    <li>The role of pre-registration and prospective validation versus post-hoc validation in determining credibility </li>
    <li>How validation hierarchies apply to discoveries that span multiple disciplines or require interdisciplinary validation </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Popper (1959) The Logic of Scientific Discovery [Discusses falsification and validation in science generally, but does not address automated systems or hierarchical validation structures]</li>
    <li>Kuhn (1962) The Structure of Scientific Revolutions [Discusses paradigm shifts and scientific validation, but predates automated discovery and does not establish validation hierarchies]</li>
    <li>Langley et al. (1987) Scientific Discovery: Computational Explorations of the Creative Process [Early work on computational scientific discovery but does not establish validation hierarchies or distinguish validation levels]</li>
    <li>Kitano (2016) Artificial Intelligence to Win the Nobel Prize and Beyond: Creating the Engine for Scientific Discovery [Proposes the Nobel Turing Challenge but focuses on system capabilities rather than validation hierarchies]</li>
    <li>King et al. (2009) The Automation of Science [Discusses Robot Scientists and closed-loop discovery but does not formalize validation hierarchies]</li>
    <li>Gil et al. (2014) Amplify scientific discovery with artificial intelligence [Discusses AI impact on science but notes lack of standardized measures, does not propose validation hierarchies]</li>
    <li>Wang et al. (2023) Scientific Discovery in the Age of Artificial Intelligence [Recent review discussing AI in science but does not formalize validation hierarchies for automated discoveries]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Validation Hierarchy Theory",
    "theory_description": "Automated scientific discovery systems employ a hierarchical validation structure with multiple levels: (1) computational/algorithmic validation (internal consistency, benchmark performance, numerical verification), (2) comparative validation (against baselines, prior methods, or human performance), (3) reproducibility validation (independent replication, cross-system verification), (4) domain expert validation (peer review, expert assessment, community proofs), and (5) experimental validation (wet-lab testing, real-world deployment, prospective studies). The strength and acceptance of discovery claims correlates with progression through these levels, with transformational claims typically requiring validation at higher levels (4-5) while incremental claims may be substantiated at lower levels (1-3). However, domain-specific exceptions exist: pure mathematics may substitute formal proof for experimental validation, and computational domains with established proxies may accept computational validation against experimental benchmarks as sufficient.",
    "supporting_evidence": [
        {
            "text": "AlphaFold uses computational validation (CASP benchmarks with GDT scores &gt;90 for ~66% of proteins), comparative validation (vs other computational methods), and implicit experimental validation (comparison to experimentally determined structures via X-ray/cryo-EM/NMR)",
            "uuids": [
                "e1164.0",
                "e1200.0",
                "e1233.0",
                "e1266.5"
            ]
        },
        {
            "text": "SPOCK uses computational validation (N-body integrations to 10^9 orbits as ground truth), statistical validation (ROC/AUC metrics, TPR ~85-94% at FPR=10%), out-of-distribution testing on multiple datasets, and reproducibility validation (shadow integrations with perturbations)",
            "uuids": [
                "e1437.0"
            ]
        },
        {
            "text": "Adam robot scientist uses experimental validation via automated wet-lab assays (auxotrophy experiments), comparison to established biological knowledge, and peer-reviewed publication",
            "uuids": [
                "e1193.0",
                "e1452.5",
                "e1243.1",
                "e1289.0"
            ]
        },
        {
            "text": "CycleResearcher uses automated review simulation (CycleReviewer with 26.89% reduction in Proxy MAE vs individual reviewers), human expert blind evaluation (3 NLP experts), comparison to preprint/accepted paper distributions, and independent reward model validation",
            "uuids": [
                "e1202.0"
            ]
        },
        {
            "text": "SciAgents uses literature-based novelty checking via Semantic Scholar API (three query permutations, top 10 results each), internal agent critique (Critic agent), but explicitly lacks experimental validation (noted as future work)",
            "uuids": [
                "e1178.0",
                "e1178.2"
            ]
        },
        {
            "text": "LLM-SR uses computational validation (NMSE metrics with values as low as 2.12e-7 for some tasks), benchmark comparisons against PySR/uDSR/DSR/NeSymReS/E2E/GPlearn, out-of-distribution generalization tests, and ablation studies",
            "uuids": [
                "e1439.0"
            ]
        },
        {
            "text": "AtomAgents uses physics-based simulation validation (LAMMPS with NEB calculations, convergence criteria max force &lt; 1e-3 eV/Å), comparison to literature values (lattice constants, elastic constants, surface energies), multimodal image analysis, but no experimental validation",
            "uuids": [
                "e1444.0"
            ]
        },
        {
            "text": "TAIS uses computational validation (cross-validation with ~80% accuracy), comparison to human-curated gold standards (GenQEX benchmark with 457 tasks), quantitative metrics (SR=69.08%, F1=30.27% end-to-end), but no wet-lab validation",
            "uuids": [
                "e1215.0"
            ]
        },
        {
            "text": "PaperQA uses comparison to ground-truth LitQA answers, human respondent comparison, citation-validity audits (GPT-4 had 60.78% valid citations, 29.41% full hallucination), and accuracy metrics (33.4% on LitQA standalone, 86.3% on PubMedQA_blind with retrieval)",
            "uuids": [
                "e1445.3"
            ]
        },
        {
            "text": "AI Feynman uses comparison to known ground-truth equations (100/100 on Feynman set vs 71/100 for prior state-of-the-art), held-out data validation, and ablation studies showing contribution of physics-inspired components",
            "uuids": [
                "e1431.0"
            ]
        },
        {
            "text": "Eunomia uses comparison to human-curated datasets (expert chemists labeled 101 articles with 371 MOFs), expert-labeled ground truth, F1 metrics (hosts 0.905, dopants 0.920 with CoV), and internal verification via Chain-of-Verification module",
            "uuids": [
                "e1209.0"
            ]
        },
        {
            "text": "MITM-RF uses progressive numerical validation at increasing precision (initial 10 digits for hashing, re-evaluated up to 2000 digits), community proof verification (several conjectures proven by external mathematicians), and statistical false-match estimation (&lt;10^-40 for 50-digit agreement)",
            "uuids": [
                "e1449.1",
                "e1453.1"
            ]
        },
        {
            "text": "Ada self-driving lab uses experimental validation via physical characterization (UV-Vis-NIR spectroscopy, 4-point-probe conductance, dark-field imaging), reproducibility across independent campaigns (two 35-sample campaigns converged to same optimum), and mechanistic post-hoc analysis",
            "uuids": [
                "e1443.0"
            ]
        },
        {
            "text": "ChemCrow and CoScientist require experimental synthesis and testing for validation, with CoScientist demonstrating closed-loop execution of chemical experiments",
            "uuids": [
                "e1432.1",
                "e1188.0"
            ]
        },
        {
            "text": "Topaz uses downstream reconstruction quality (highest resolution achieved for ribosome dataset), comparison to published manual picks (1.72× more particles found), and implicit benchmarking against human-curated picks",
            "uuids": [
                "e1200.3"
            ]
        },
        {
            "text": "Eve uses automated in vitro assays (high-throughput screening), comparison to known bioactivity databases, active learning with Gaussian process regression, and experimental confirmation of repurposing candidates (TNP-470 against P. vivax)",
            "uuids": [
                "e1193.1",
                "e1289.1",
                "e1452.1"
            ]
        },
        {
            "text": "Halicin discovery used computational screening (deep neural network predictions) followed by experimental biological validation (in vitro assays demonstrating bactericidal activity against resistant strains) and peer-reviewed publication in Cell",
            "uuids": [
                "e1266.0"
            ]
        },
        {
            "text": "DATAVOYAGER uses human-in-the-loop moderation, replication of published analyses (Smith et al. 2005, Zaw et al. 2016, Alexander et al. 1982), and consistency checks between different statistical models (GLM confirming OLS results)",
            "uuids": [
                "e1227.0"
            ]
        },
        {
            "text": "Descent&Repel uses progressive precision validation (convergence to exact zero-loss solutions), integer-projection verification, and community proof verification for discovered conjectures",
            "uuids": [
                "e1449.2"
            ]
        },
        {
            "text": "HTE drop-cast platform uses reproducibility statistics (R^2 = 0.97 across 309 samples), cross-system replication (observed PCBM:oIDTBR instability in two different quaternary systems), and comparison to hypothetical manual testing",
            "uuids": [
                "e1455.1"
            ]
        },
        {
            "text": "Magnon NN uses comparison to literature values (J=0.6763 meV vs literature 0.648-0.673 meV; J'=0.0104 meV vs literature 0.006-0.012 meV), validation on withheld simulated data (MAE ±0.0055 meV for J), and generalization from simulation to real experimental data",
            "uuids": [
                "e1200.1"
            ]
        },
        {
            "text": "Ramanujan Machine uses numerical verification to high precision (up to 2000 digits), community proof verification (several conjectures proven by mathematicians), and peer-reviewed publication",
            "uuids": [
                "e1243.4",
                "e1449.1"
            ]
        },
        {
            "text": "LLM Reviewer uses validation against ICLR 2022 OpenReview data (balanced accuracy 0.65±0.04), correlation analysis with human reviewer scores (0.18 vs human-human 0.14), and calibration studies",
            "uuids": [
                "e1214.1"
            ]
        },
        {
            "text": "Automated Theorem Proving systems use formal proof generation and machine verification, with cited successes including Four Color Theorem, Robbins' problem, Lorenz attractor existence, and Kepler conjecture",
            "uuids": [
                "e1453.4"
            ]
        },
        {
            "text": "SNIP uses benchmark comparisons on SRBench, Pareto front analysis, and ablation studies, but validation is primarily computational",
            "uuids": [
                "e1446.1"
            ]
        },
        {
            "text": "Phoenics uses campaign-level outcomes (convergence to global maxima, 7% of search space sampled to reach optimum for OPV), comparison to grid search and sequential approaches, and experimental execution of proposed conditions",
            "uuids": [
                "e1190.1"
            ]
        },
        {
            "text": "CRN/Explanatory Learning uses held-out test sets with excluded rule equivalence classes, semantic equivalence verification over full universe (R-Acc), and comparison to empiricist baselines (NRS 77.7% vs EMP-C 22.5%)",
            "uuids": [
                "e1442.1",
                "e1224.1"
            ]
        },
        {
            "text": "LLM-NERRE uses comparison to human-curated gold standards, manual expert review for fuzzy scoring, parsability checks (98.7-100%), and comparison to baselines (F1 0.849 vs MatBERT+Proximity 0.545)",
            "uuids": [
                "e1441.0"
            ]
        },
        {
            "text": "Auto-Keras uses benchmark comparisons under matched experimental settings (12-hour search budget), comparison to human-designed networks and other NAS methods (SEAS, NASNet), and error rate metrics on standard datasets",
            "uuids": [
                "e1433.1"
            ]
        }
    ],
    "theory_statements": [
        "Automated discovery systems that make transformational claims require validation at multiple hierarchical levels, typically including domain expert validation (level 4) and/or experimental validation (level 5).",
        "Incremental discoveries can be substantiated primarily through computational validation (level 1), comparative validation (level 2), and reproducibility validation (level 3) without requiring extensive experimental validation.",
        "The credibility and acceptance of a discovery claim is proportional to the number of validation levels successfully passed and the rigor applied at each level.",
        "Systems that lack experimental validation are typically characterized as producing 'proof-of-concept', 'computational', or 'simulation-based' results rather than validated scientific discoveries, unless operating in domains where experimental validation is not applicable.",
        "Validation approaches become more stringent, multi-modal, and externally-focused as the claimed significance of discoveries increases.",
        "Reproducibility validation (independent replication, cross-system verification, statistical reproducibility metrics) serves as a critical intermediate level between comparative and expert validation.",
        "Domain-specific validation standards exist: pure mathematics accepts formal proof as highest validation; computational domains with established experimental proxies may accept computational validation against benchmarks as sufficient for transformational claims.",
        "The validation hierarchy is not strictly linear: systems may achieve higher-level validation (e.g., experimental) without fully completing lower levels, though this is less common and may face greater scrutiny.",
        "Automated peer review and community-based validation are emerging as intermediate validation levels between computational comparison and human expert assessment.",
        "Validation requirements evolve over time: as automated systems become more prevalent and trusted, the threshold for acceptance at each level may shift."
    ],
    "new_predictions_likely": [
        "A system claiming transformational discovery that only uses computational validation (level 1) will face significant skepticism and require additional validation levels before widespread acceptance, unless operating in a domain where higher levels are not applicable (e.g., pure mathematics with formal proof).",
        "Systems that incorporate experimental validation loops (level 5) will produce discoveries with higher acceptance rates and faster time-to-adoption in the scientific community compared to purely computational systems (levels 1-3).",
        "The time-to-acceptance for automated discoveries will correlate inversely with the number of validation levels completed before publication, with each additional level reducing time-to-acceptance by an estimated 20-40%.",
        "Systems that achieve high reproducibility metrics (level 3) will be more readily accepted for incremental claims even without experimental validation, particularly in computational domains.",
        "Discoveries that pass validation at level 4 (domain expert validation) but fail at level 5 (experimental validation) will be reclassified as incremental or methodological contributions rather than transformational discoveries.",
        "As automated peer review systems improve (approaching human-level consistency), they will become an accepted intermediate validation level between comparative and expert validation, potentially reducing the burden on human reviewers for incremental claims."
    ],
    "new_predictions_unknown": [
        "Whether a new validation level (e.g., 'cross-system replication' where multiple independent automated systems must reproduce the same discovery) could emerge as a standard requirement between reproducibility and expert validation, and whether this would be widely accepted across disciplines.",
        "Whether certain domains beyond pure mathematics (e.g., theoretical physics, computational chemistry) could accept transformational claims with only computational validation (levels 1-3), bypassing experimental validation entirely, and under what specific conditions this would be acceptable.",
        "Whether the validation hierarchy could be compressed or reordered for certain types of discoveries (e.g., discoveries made by systems with proven track records) without loss of credibility, and what the minimum acceptable validation path would be.",
        "Whether automated systems that achieve superhuman performance at lower validation levels (e.g., computational validation with orders of magnitude better precision than human-achievable) could bypass higher validation levels for certain types of claims.",
        "Whether the emergence of 'meta-validation' systems (automated systems that validate other automated systems' discoveries) could create a parallel validation hierarchy that operates independently of traditional human-centered validation.",
        "Whether validation requirements will converge across disciplines as automated discovery becomes more common, or whether domain-specific validation hierarchies will persist and potentially diverge further.",
        "Whether the cost and time required for experimental validation (level 5) will create a bottleneck that fundamentally limits the rate of transformational discoveries from automated systems, or whether automated experimentation will scale sufficiently to match the rate of computational discovery generation."
    ],
    "negative_experiments": [
        "Finding cases where discoveries validated only computationally (levels 1-2) are widely accepted as transformational in domains where experimental validation is feasible would challenge the necessity of the full hierarchy.",
        "Finding cases where extensive experimental validation (level 5) and peer review (level 4) fail to convince the community of a discovery's significance would challenge the sufficiency of the hierarchy and suggest missing validation dimensions.",
        "Demonstrating that validation level does not correlate with discovery acceptance rates (measured by citations, adoption, or community surveys) would undermine the core premise of the theory.",
        "Finding that reproducibility validation (level 3) is neither necessary nor sufficient for acceptance of incremental claims would challenge its position in the hierarchy.",
        "Showing that automated peer review (emerging level between 3 and 4) is systematically rejected by the community even when it matches or exceeds human reviewer consistency would challenge the theory's prediction about emerging validation levels.",
        "Demonstrating that the order of validation levels can be arbitrarily rearranged without affecting acceptance rates would challenge the hierarchical structure of the theory.",
        "Finding that validation requirements do not increase with claimed significance (i.e., transformational claims accepted with same validation as incremental claims) would contradict a core theory statement."
    ],
    "unaccounted_for": [
        {
            "text": "The role of social and institutional factors in determining which validation levels are required, including funding agency requirements, journal policies, and disciplinary norms",
            "uuids": []
        },
        {
            "text": "How validation requirements differ across scientific disciplines beyond the mathematics/experimental science distinction (e.g., social sciences, engineering, clinical medicine)",
            "uuids": []
        },
        {
            "text": "The temporal evolution of validation standards as automated systems become more prevalent and trusted, including potential feedback loops where successful automated discoveries lower validation barriers for future systems",
            "uuids": []
        },
        {
            "text": "The role of negative results and failed validation attempts in the validation hierarchy, and whether systems that transparently report validation failures are treated differently",
            "uuids": []
        },
        {
            "text": "How validation requirements interact with intellectual property considerations, particularly for discoveries with commercial applications",
            "uuids": []
        },
        {
            "text": "The distinction between validating a discovery method/system versus validating individual discoveries produced by that system, and whether validation can be 'amortized' across multiple discoveries from the same system",
            "uuids": []
        },
        {
            "text": "The role of pre-registration and prospective validation versus post-hoc validation in determining credibility",
            "uuids": []
        },
        {
            "text": "How validation hierarchies apply to discoveries that span multiple disciplines or require interdisciplinary validation",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "AlphaFold is widely described as transformative and received recognition (Chemistry Nobel Prize mentioned) despite relying primarily on computational validation (CASP benchmarks) and comparison to existing experimental structures rather than generating new experimental data, suggesting computational validation against established benchmarks may be sufficient for transformational claims in some domains",
            "uuids": [
                "e1164.0",
                "e1233.0",
                "e1266.5"
            ]
        },
        {
            "text": "Mathematical systems like Ramanujan Machine, MITM-RF, and AI Feynman are considered to produce novel discoveries despite lacking experimental validation, relying instead on numerical verification to high precision and community proofs, suggesting formal proof can substitute for experimental validation as the highest level",
            "uuids": [
                "e1243.4",
                "e1449.1",
                "e1453.1",
                "e1431.0"
            ]
        },
        {
            "text": "CycleResearcher is described as a 'significant step toward fully automated scientific inquiry' despite explicitly using fabricated experimental results during training, suggesting that validation of the method/system may be separable from validation of individual discoveries",
            "uuids": [
                "e1202.0"
            ]
        },
        {
            "text": "Some systems like SciAgents are characterized as 'novel and potentially transformative' based on algorithmic novelty and literature-based validation alone, without experimental validation, suggesting that potential impact may be assessed independently of validation level",
            "uuids": [
                "e1178.0"
            ]
        },
        {
            "text": "SPOCK achieves high validation through computational means (shadow integrations, statistical metrics) and is used for practical scientific applications (constraining exoplanet parameters) without direct experimental validation of individual predictions, suggesting computational validation can be sufficient for practical scientific use",
            "uuids": [
                "e1437.0"
            ]
        }
    ],
    "special_cases": [
        "In pure mathematics and formal systems, formal proof (either human-generated or machine-verified) substitutes for experimental validation as the highest validation level, with numerical verification to high precision serving as strong supporting evidence.",
        "In domains with established computational proxies that have been extensively validated against experiments (e.g., protein structure prediction via CASP, molecular dynamics simulations), computational validation against benchmark datasets may be sufficient for transformational claims without requiring new experimental validation for each discovery.",
        "Systems operating in closed-loop with automated experimentation (e.g., Adam, Eve, Ada) may compress validation levels by integrating experimental validation directly into the discovery loop, effectively combining levels 1-2 with level 5.",
        "For discoveries that replicate or confirm existing knowledge (e.g., AI Feynman rediscovering known physics equations), lower validation levels may be sufficient since the ground truth is already established, though this typically results in characterization as 'proof-of-concept' rather than novel discovery.",
        "In domains where experimental validation is prohibitively expensive, dangerous, or impossible (e.g., astrophysics, paleontology, certain areas of theoretical physics), observational validation or consistency with multiple independent data sources may substitute for experimental validation.",
        "For methodological contributions (new algorithms, frameworks, or approaches) as opposed to domain discoveries, validation focuses more heavily on levels 1-3 (computational performance, comparative benchmarks, reproducibility) with less emphasis on experimental validation.",
        "When automated systems achieve superhuman performance on established benchmarks (e.g., AlphaGo, AlphaFold), the validation hierarchy may be inverted, with the system's outputs becoming the new benchmark against which human and other automated approaches are validated.",
        "For discoveries with immediate practical applications or commercial value, validation may be accelerated or modified to include market validation or real-world deployment metrics as an additional or alternative validation level."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Popper (1959) The Logic of Scientific Discovery [Discusses falsification and validation in science generally, but does not address automated systems or hierarchical validation structures]",
            "Kuhn (1962) The Structure of Scientific Revolutions [Discusses paradigm shifts and scientific validation, but predates automated discovery and does not establish validation hierarchies]",
            "Langley et al. (1987) Scientific Discovery: Computational Explorations of the Creative Process [Early work on computational scientific discovery but does not establish validation hierarchies or distinguish validation levels]",
            "Kitano (2016) Artificial Intelligence to Win the Nobel Prize and Beyond: Creating the Engine for Scientific Discovery [Proposes the Nobel Turing Challenge but focuses on system capabilities rather than validation hierarchies]",
            "King et al. (2009) The Automation of Science [Discusses Robot Scientists and closed-loop discovery but does not formalize validation hierarchies]",
            "Gil et al. (2014) Amplify scientific discovery with artificial intelligence [Discusses AI impact on science but notes lack of standardized measures, does not propose validation hierarchies]",
            "Wang et al. (2023) Scientific Discovery in the Age of Artificial Intelligence [Recent review discussing AI in science but does not formalize validation hierarchies for automated discoveries]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
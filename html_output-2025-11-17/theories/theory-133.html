<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain Randomization Targeting Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-133</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-133</p>
                <p><strong>Name:</strong> Domain Randomization Targeting Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of the fundamental trade-off between environment complexity and environment variation in embodied learning systems, based on the following results.</p>
                <p><strong>Description:</strong> Domain randomization (DR) for sim-to-real transfer is most effective when randomization is targeted to parameters that: (1) differ between simulation and reality, (2) affect task-relevant dynamics, and (3) can be sampled efficiently. The effectiveness of DR follows a complexity-variation trade-off: uniform domain randomization over all parameters is sample-inefficient and can prevent learning by creating too much variation, while targeted randomization enables learning by focusing on informative parameter regions. Active domain randomization (ADR) that learns to prioritize informative parameter regions improves sample efficiency and transfer quality, with benefits increasing as the dimensionality of the randomization space grows. However, ADR with learned discriminator rewards can be exploited to create impossible environments. The optimal DR strategy balances coverage of the reality gap with learnability, and should be guided by either: (a) prior knowledge of sim-real differences, (b) adaptive sampling based on learning progress, or (c) self-supervised signals that avoid exploitability. The effectiveness of DR also depends on task characteristics: simple tasks with accurate simulation may not require DR, while complex contact-rich tasks benefit most from targeted randomization. DR can be combined with curriculum learning to progressively increase variation as the agent's capability grows.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Targeted domain randomization over task-relevant parameters is more sample-efficient than uniform randomization over all parameters</li>
                <li>Active domain randomization that learns to prioritize informative parameter regions improves sample efficiency and transfer quality</li>
                <li>Extreme uniform domain randomization over all parameters simultaneously can prevent learning by creating too much variation</li>
                <li>The benefit of ADR over uniform DR increases with the dimensionality of the randomization space, as informative regions become sparser</li>
                <li>ADR with learned discriminator rewards can be exploited to create impossible environments</li>
                <li>Self-supervised reward signals (e.g., asymmetric self-play) are more robust to exploitation than learned discriminators in ADR</li>
                <li>The optimal DR strategy balances coverage of the reality gap with learnability</li>
                <li>Targeted variation along specific axes (e.g., goal poses) improves generalization along those axes while maintaining learnability</li>
                <li>Training curriculum shapes generalization: targeted variation enables learning while indiscriminate variation prevents it</li>
                <li>Simple tasks with accurate simulation may not require DR, while complex contact-rich tasks benefit most from targeted randomization</li>
                <li>DR can be combined with curriculum learning to progressively increase variation as agent capability grows</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>ADR on Pusher-3DOF outperformed UDR in most test regions with lower variance, by prioritizing sparse hard regions (puck-sliding regime). Training used 1M timesteps with N_rand=2 parameters. <a href="../results/extraction-result-1069.html#e1069.1" class="evidence-link">[e1069.1]</a> </li>
    <li>ADR on ErgoPusher-v0 achieved better or similar mean performance and substantially lower variance than UDR in zero-shot real-robot transfer across multiple friction conditions <a href="../results/extraction-result-1069.html#e1069.3" class="evidence-link">[e1069.3]</a> </li>
    <li>SS-ADR (ADR with self-supervised asymmetric self-play rewards) outperformed UDR and avoided ADR exploitability issues, achieving lower final distance-to-goal and lower variance <a href="../results/extraction-result-1009.html#e1009.0" class="evidence-link">[e1009.0]</a> <a href="../results/extraction-result-1009.html#e1009.1" class="evidence-link">[e1009.1]</a> </li>
    <li>Domain randomization over appearance (~200 textures/materials) enabled sim-to-real transfer for quadrotor collision avoidance flight (Sadeghi & Levine) <a href="../results/extraction-result-1075.html#e1075.1" class="evidence-link">[e1075.1]</a> </li>
    <li>Ensemble dynamics training (Mordatch et al.) with randomized physical parameters improved robustness to model error and transfer to physical humanoids <a href="../results/extraction-result-1075.html#e1075.2" class="evidence-link">[e1075.2]</a> </li>
    <li>Extreme domain randomization (CausalWorld Curriculum 2: simultaneous randomization of all variables) prevented learning even after 100M timesteps (PPO) or 10M timesteps (SAC/TD3), with agents 'rarely managing to pick up any significant success signal' <a href="../results/extraction-result-1074.html#e1074.0" class="evidence-link">[e1074.0]</a> </li>
    <li>Targeted variation (goal pose randomization, Curriculum 1) in CausalWorld enabled learning and improved generalization to novel goal poses, while extreme randomization (Curriculum 2) prevented learning <a href="../results/extraction-result-1074.html#e1074.0" class="evidence-link">[e1074.0]</a> </li>
    <li>GC-VAT with targeted domain randomization (initial angle range [-π,π], flight altitude [13,22]m, camera pitch [0.6,1.38]rad) achieved strong sim-to-real transfer (88.4% TSR on real DJI Mini 3 Pro drone, 81.3% CAR) <a href="../results/extraction-result-1045.html#e1045.0" class="evidence-link">[e1045.0]</a> </li>
    <li>In CausalWorld, training curriculum crucially shapes generalization: targeted variation (e.g., goal pose randomization) improves generalization along that axis, while indiscriminate/extreme randomization can prevent learning entirely <a href="../results/extraction-result-1074.html#e1074.0" class="evidence-link">[e1074.0]</a> </li>
    <li>TriFinger experiments showed that agents trained on default (Curriculum 0, no randomization) overfit on goal poses, while agents trained with goal pose randomization (Curriculum 1) generalized robustly to different goal poses <a href="../results/extraction-result-1074.html#e1074.0" class="evidence-link">[e1074.0]</a> <a href="../results/extraction-result-1074.html#e1074.1" class="evidence-link">[e1074.1]</a> </li>
    <li>As dimensionality of randomization grows, informative (hard) regions become rarer under uniform sampling; ADR discovers and prioritizes these sparse hard regions improving extrapolation to unseen dynamics <a href="../results/extraction-result-1069.html#e1069.1" class="evidence-link">[e1069.1]</a> </li>
    <li>ADR with SVPG ensemble (N=15 particles for Pusher-3DOF) and discriminator reward biased sampling towards hard regions, achieving better extrapolation without extra interactions <a href="../results/extraction-result-1069.html#e1069.1" class="evidence-link">[e1069.1]</a> </li>
    <li>In high-dimensional randomization (N_rand=8 for ErgoReacher), ADR is expected to provide even greater benefits over uniform DR due to increased sparsity of informative regions <a href="../results/extraction-result-1069.html#e1069.1" class="evidence-link">[e1069.1]</a> <a href="../results/extraction-result-1069.html#e1069.3" class="evidence-link">[e1069.3]</a> </li>
    <li>Unconstrained minimax adversaries can create arbitrarily hard/unsolvable dynamics (high complexity + high variation) which prevent learning, whereas regret-based adversary (PAIRED) selects perturbations that are challenging but solvable <a href="../results/extraction-result-1038.html#e1038.1" class="evidence-link">[e1038.1]</a> </li>
    <li>In MuJoCo Hopper with adversarial torques, unconstrained minimax adversary drove agent reward to zero (policy failed to learn), while PAIRED agents maintained non-zero reward and generalized better to unseen mass/friction parameters <a href="../results/extraction-result-1038.html#e1038.1" class="evidence-link">[e1038.1]</a> </li>
    <li>RGB-only PointGoal agents required both layout diversity (Gibson-2+) and appearance diversity (Matterport3D) to reach high performance; without this increased variation prior RGB agents performed poorly (~0.57 SPL) <a href="../results/extraction-result-1082.html#e1082.1" class="evidence-link">[e1082.1]</a> </li>
    <li>GC-VAT domain randomization (especially initial-angle randomization) significantly improved exploration and generalization; training in only simple environments speeds early learning but risks suboptimal policies unless combined with randomization <a href="../results/extraction-result-1045.html#e1045.0" class="evidence-link">[e1045.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In any sim-to-real task with N_rand > 5, ADR will outperform uniform DR in both sample efficiency and transfer quality by at least 20%</li>
                <li>For manipulation tasks with contact-rich dynamics, targeted DR over friction and mass parameters will enable learning while uniform DR over all parameters will prevent learning within typical training budgets (< 50M steps)</li>
                <li>Combining ADR with curriculum learning (starting with low randomization and progressively increasing) will outperform both static ADR and static uniform DR</li>
                <li>In navigation tasks, targeted DR over appearance parameters will be more effective than DR over dynamics parameters for vision-based policies</li>
                <li>The optimal number of ADR particles (N) will scale logarithmically with the dimensionality of the randomization space (N_rand)</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether ADR can automatically discover the relevant parameters to randomize without any prior knowledge of sim-real differences</li>
                <li>Whether there exist task classes where uniform DR fundamentally outperforms targeted DR regardless of training budget</li>
                <li>Whether ADR can handle non-stationary reality gaps (e.g., robot wear over time, changing environmental conditions)</li>
                <li>Whether the optimal DR strategy changes qualitatively with the amount of real-world data available for fine-tuning</li>
                <li>Whether ADR benefits persist when combined with other sim-to-real techniques like domain adaptation or system identification</li>
                <li>Whether the exploitability of discriminator-based ADR can be fully eliminated or only mitigated</li>
                <li>Whether there is a fundamental limit to how much variation can be introduced before learning becomes impossible, regardless of how it is targeted</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where uniform DR outperforms targeted DR even with sufficient training budget would challenge the targeting principle</li>
                <li>Demonstrating that ADR does not improve over uniform DR in high-dimensional spaces (N_rand > 10) would contradict the dimensionality scaling claim</li>
                <li>Showing that discriminator-based ADR does not suffer from exploitability in practice would challenge the exploitation hypothesis</li>
                <li>Finding that DR does not help sim-to-real transfer for a broad class of tasks would contradict the core premise</li>
                <li>Demonstrating that self-supervised ADR (SS-ADR) can also be exploited would challenge the robustness claim</li>
                <li>Finding that extreme randomization enables learning as well as targeted randomization would contradict the complexity-variation trade-off</li>
                <li>Showing that ADR benefits do not increase with dimensionality would challenge the sparsity argument</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically identify which parameters to randomize without prior knowledge of sim-real differences </li>
    <li>How DR interacts with other sim-to-real techniques like domain adaptation, system identification, and fine-tuning </li>
    <li>Whether DR can handle systematic biases in simulation that cannot be covered by randomization (e.g., missing physics phenomena) </li>
    <li>The optimal schedule for increasing randomization when combining DR with curriculum learning </li>
    <li>How the effectiveness of DR depends on the observation modality (vision vs proprioception vs force sensing) </li>
    <li>Whether there are fundamental limits to DR effectiveness based on the magnitude of sim-real differences </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Tobin et al. (2017) Domain randomization for transferring deep neural networks from simulation to the real world [Foundational DR work introducing the concept]</li>
    <li>OpenAI et al. (2019) Solving Rubik's Cube with a Robot Hand [Large-scale DR for dexterous manipulation, demonstrating extreme DR can work with sufficient scale]</li>
    <li>Mehta et al. (2019) Active Domain Randomization [ADR method, introduces adaptive sampling of randomization parameters]</li>
    <li>Muratore et al. (2021) Robot Learning from Randomized Simulations: A Review [Comprehensive DR review]</li>
    <li>Peng et al. (2018) Sim-to-Real Transfer of Robotic Control with Dynamics Randomization [Dynamics randomization for locomotion]</li>
    <li>Chebotar et al. (2019) Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience [Combining DR with real-world data]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain Randomization Targeting Theory",
    "theory_description": "Domain randomization (DR) for sim-to-real transfer is most effective when randomization is targeted to parameters that: (1) differ between simulation and reality, (2) affect task-relevant dynamics, and (3) can be sampled efficiently. The effectiveness of DR follows a complexity-variation trade-off: uniform domain randomization over all parameters is sample-inefficient and can prevent learning by creating too much variation, while targeted randomization enables learning by focusing on informative parameter regions. Active domain randomization (ADR) that learns to prioritize informative parameter regions improves sample efficiency and transfer quality, with benefits increasing as the dimensionality of the randomization space grows. However, ADR with learned discriminator rewards can be exploited to create impossible environments. The optimal DR strategy balances coverage of the reality gap with learnability, and should be guided by either: (a) prior knowledge of sim-real differences, (b) adaptive sampling based on learning progress, or (c) self-supervised signals that avoid exploitability. The effectiveness of DR also depends on task characteristics: simple tasks with accurate simulation may not require DR, while complex contact-rich tasks benefit most from targeted randomization. DR can be combined with curriculum learning to progressively increase variation as the agent's capability grows.",
    "supporting_evidence": [
        {
            "text": "ADR on Pusher-3DOF outperformed UDR in most test regions with lower variance, by prioritizing sparse hard regions (puck-sliding regime). Training used 1M timesteps with N_rand=2 parameters.",
            "uuids": [
                "e1069.1"
            ]
        },
        {
            "text": "ADR on ErgoPusher-v0 achieved better or similar mean performance and substantially lower variance than UDR in zero-shot real-robot transfer across multiple friction conditions",
            "uuids": [
                "e1069.3"
            ]
        },
        {
            "text": "SS-ADR (ADR with self-supervised asymmetric self-play rewards) outperformed UDR and avoided ADR exploitability issues, achieving lower final distance-to-goal and lower variance",
            "uuids": [
                "e1009.0",
                "e1009.1"
            ]
        },
        {
            "text": "Domain randomization over appearance (~200 textures/materials) enabled sim-to-real transfer for quadrotor collision avoidance flight (Sadeghi & Levine)",
            "uuids": [
                "e1075.1"
            ]
        },
        {
            "text": "Ensemble dynamics training (Mordatch et al.) with randomized physical parameters improved robustness to model error and transfer to physical humanoids",
            "uuids": [
                "e1075.2"
            ]
        },
        {
            "text": "Extreme domain randomization (CausalWorld Curriculum 2: simultaneous randomization of all variables) prevented learning even after 100M timesteps (PPO) or 10M timesteps (SAC/TD3), with agents 'rarely managing to pick up any significant success signal'",
            "uuids": [
                "e1074.0"
            ]
        },
        {
            "text": "Targeted variation (goal pose randomization, Curriculum 1) in CausalWorld enabled learning and improved generalization to novel goal poses, while extreme randomization (Curriculum 2) prevented learning",
            "uuids": [
                "e1074.0"
            ]
        },
        {
            "text": "GC-VAT with targeted domain randomization (initial angle range [-π,π], flight altitude [13,22]m, camera pitch [0.6,1.38]rad) achieved strong sim-to-real transfer (88.4% TSR on real DJI Mini 3 Pro drone, 81.3% CAR)",
            "uuids": [
                "e1045.0"
            ]
        },
        {
            "text": "In CausalWorld, training curriculum crucially shapes generalization: targeted variation (e.g., goal pose randomization) improves generalization along that axis, while indiscriminate/extreme randomization can prevent learning entirely",
            "uuids": [
                "e1074.0"
            ]
        },
        {
            "text": "TriFinger experiments showed that agents trained on default (Curriculum 0, no randomization) overfit on goal poses, while agents trained with goal pose randomization (Curriculum 1) generalized robustly to different goal poses",
            "uuids": [
                "e1074.0",
                "e1074.1"
            ]
        },
        {
            "text": "As dimensionality of randomization grows, informative (hard) regions become rarer under uniform sampling; ADR discovers and prioritizes these sparse hard regions improving extrapolation to unseen dynamics",
            "uuids": [
                "e1069.1"
            ]
        },
        {
            "text": "ADR with SVPG ensemble (N=15 particles for Pusher-3DOF) and discriminator reward biased sampling towards hard regions, achieving better extrapolation without extra interactions",
            "uuids": [
                "e1069.1"
            ]
        },
        {
            "text": "In high-dimensional randomization (N_rand=8 for ErgoReacher), ADR is expected to provide even greater benefits over uniform DR due to increased sparsity of informative regions",
            "uuids": [
                "e1069.1",
                "e1069.3"
            ]
        },
        {
            "text": "Unconstrained minimax adversaries can create arbitrarily hard/unsolvable dynamics (high complexity + high variation) which prevent learning, whereas regret-based adversary (PAIRED) selects perturbations that are challenging but solvable",
            "uuids": [
                "e1038.1"
            ]
        },
        {
            "text": "In MuJoCo Hopper with adversarial torques, unconstrained minimax adversary drove agent reward to zero (policy failed to learn), while PAIRED agents maintained non-zero reward and generalized better to unseen mass/friction parameters",
            "uuids": [
                "e1038.1"
            ]
        },
        {
            "text": "RGB-only PointGoal agents required both layout diversity (Gibson-2+) and appearance diversity (Matterport3D) to reach high performance; without this increased variation prior RGB agents performed poorly (~0.57 SPL)",
            "uuids": [
                "e1082.1"
            ]
        },
        {
            "text": "GC-VAT domain randomization (especially initial-angle randomization) significantly improved exploration and generalization; training in only simple environments speeds early learning but risks suboptimal policies unless combined with randomization",
            "uuids": [
                "e1045.0"
            ]
        }
    ],
    "theory_statements": [
        "Targeted domain randomization over task-relevant parameters is more sample-efficient than uniform randomization over all parameters",
        "Active domain randomization that learns to prioritize informative parameter regions improves sample efficiency and transfer quality",
        "Extreme uniform domain randomization over all parameters simultaneously can prevent learning by creating too much variation",
        "The benefit of ADR over uniform DR increases with the dimensionality of the randomization space, as informative regions become sparser",
        "ADR with learned discriminator rewards can be exploited to create impossible environments",
        "Self-supervised reward signals (e.g., asymmetric self-play) are more robust to exploitation than learned discriminators in ADR",
        "The optimal DR strategy balances coverage of the reality gap with learnability",
        "Targeted variation along specific axes (e.g., goal poses) improves generalization along those axes while maintaining learnability",
        "Training curriculum shapes generalization: targeted variation enables learning while indiscriminate variation prevents it",
        "Simple tasks with accurate simulation may not require DR, while complex contact-rich tasks benefit most from targeted randomization",
        "DR can be combined with curriculum learning to progressively increase variation as agent capability grows"
    ],
    "new_predictions_likely": [
        "In any sim-to-real task with N_rand &gt; 5, ADR will outperform uniform DR in both sample efficiency and transfer quality by at least 20%",
        "For manipulation tasks with contact-rich dynamics, targeted DR over friction and mass parameters will enable learning while uniform DR over all parameters will prevent learning within typical training budgets (&lt; 50M steps)",
        "Combining ADR with curriculum learning (starting with low randomization and progressively increasing) will outperform both static ADR and static uniform DR",
        "In navigation tasks, targeted DR over appearance parameters will be more effective than DR over dynamics parameters for vision-based policies",
        "The optimal number of ADR particles (N) will scale logarithmically with the dimensionality of the randomization space (N_rand)"
    ],
    "new_predictions_unknown": [
        "Whether ADR can automatically discover the relevant parameters to randomize without any prior knowledge of sim-real differences",
        "Whether there exist task classes where uniform DR fundamentally outperforms targeted DR regardless of training budget",
        "Whether ADR can handle non-stationary reality gaps (e.g., robot wear over time, changing environmental conditions)",
        "Whether the optimal DR strategy changes qualitatively with the amount of real-world data available for fine-tuning",
        "Whether ADR benefits persist when combined with other sim-to-real techniques like domain adaptation or system identification",
        "Whether the exploitability of discriminator-based ADR can be fully eliminated or only mitigated",
        "Whether there is a fundamental limit to how much variation can be introduced before learning becomes impossible, regardless of how it is targeted"
    ],
    "negative_experiments": [
        "Finding tasks where uniform DR outperforms targeted DR even with sufficient training budget would challenge the targeting principle",
        "Demonstrating that ADR does not improve over uniform DR in high-dimensional spaces (N_rand &gt; 10) would contradict the dimensionality scaling claim",
        "Showing that discriminator-based ADR does not suffer from exploitability in practice would challenge the exploitation hypothesis",
        "Finding that DR does not help sim-to-real transfer for a broad class of tasks would contradict the core premise",
        "Demonstrating that self-supervised ADR (SS-ADR) can also be exploited would challenge the robustness claim",
        "Finding that extreme randomization enables learning as well as targeted randomization would contradict the complexity-variation trade-off",
        "Showing that ADR benefits do not increase with dimensionality would challenge the sparsity argument"
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically identify which parameters to randomize without prior knowledge of sim-real differences",
            "uuids": []
        },
        {
            "text": "How DR interacts with other sim-to-real techniques like domain adaptation, system identification, and fine-tuning",
            "uuids": []
        },
        {
            "text": "Whether DR can handle systematic biases in simulation that cannot be covered by randomization (e.g., missing physics phenomena)",
            "uuids": []
        },
        {
            "text": "The optimal schedule for increasing randomization when combining DR with curriculum learning",
            "uuids": []
        },
        {
            "text": "How the effectiveness of DR depends on the observation modality (vision vs proprioception vs force sensing)",
            "uuids": []
        },
        {
            "text": "Whether there are fundamental limits to DR effectiveness based on the magnitude of sim-real differences",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some simple tasks may transfer without DR if simulation is sufficiently accurate (e.g., simple reaching tasks in accurate simulators)",
            "uuids": []
        },
        {
            "text": "In some cases, agents trained without randomization (Curriculum 0) can achieve high performance on their training distribution, suggesting DR is not always necessary for in-distribution performance",
            "uuids": [
                "e1074.0"
            ]
        },
        {
            "text": "RGB-only agents achieved high success rates (0.977) with sufficient training data diversity even without explicit DR over dynamics parameters",
            "uuids": [
                "e1082.1"
            ]
        }
    ],
    "special_cases": [
        "Tasks with very accurate simulation may not require DR and may even be harmed by unnecessary variation",
        "Tasks with systematic simulation biases (e.g., missing contact models) may require domain adaptation or system identification in addition to DR",
        "Very simple tasks with low-dimensional state/action spaces may transfer with uniform DR",
        "Tasks where the reality gap is primarily in appearance (not dynamics) may benefit more from visual DR than dynamics DR",
        "Contact-rich manipulation tasks are particularly sensitive to DR and benefit most from targeted randomization over friction and mass",
        "Navigation tasks may benefit more from appearance randomization than dynamics randomization when using vision-based policies",
        "When training budget is very limited (&lt; 1M steps), even targeted DR may prevent learning and no-DR or minimal-DR may be preferable",
        "For tasks with very high-dimensional randomization spaces (N_rand &gt; 20), even ADR may struggle without additional structure or prior knowledge"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Tobin et al. (2017) Domain randomization for transferring deep neural networks from simulation to the real world [Foundational DR work introducing the concept]",
            "OpenAI et al. (2019) Solving Rubik's Cube with a Robot Hand [Large-scale DR for dexterous manipulation, demonstrating extreme DR can work with sufficient scale]",
            "Mehta et al. (2019) Active Domain Randomization [ADR method, introduces adaptive sampling of randomization parameters]",
            "Muratore et al. (2021) Robot Learning from Randomized Simulations: A Review [Comprehensive DR review]",
            "Peng et al. (2018) Sim-to-Real Transfer of Robotic Control with Dynamics Randomization [Dynamics randomization for locomotion]",
            "Chebotar et al. (2019) Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience [Combining DR with real-world data]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 6,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
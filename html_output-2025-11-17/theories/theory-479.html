<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Faithfulness-Scaling Inverse Law for Chain-of-Thought Reasoning - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-479</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-479</p>
                <p><strong>Name:</strong> Faithfulness-Scaling Inverse Law for Chain-of-Thought Reasoning</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models use diverse reasoning methods versus similar styles of reasoning to solve reasoning problems, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that as language model (LM) size and capability increase, the faithfulness of chain-of-thought (CoT) reasoning—i.e., the degree to which intermediate steps causally influence the final answer—often decreases, especially on easier or familiar tasks. Larger models are more likely to produce correct answers without relying on the explicit reasoning steps they generate, leading to post-hoc rationalization rather than faithful stepwise reasoning. This inverse scaling effect is modulated by task difficulty: on harder or unfamiliar tasks, even large models may rely more on CoT, while on easier tasks, they may ignore or bypass the chain. The theory further posits that interventions such as early-answering truncation and mistake insertion can diagnose faithfulness, and that maximizing model size is not always optimal when faithful, interpretable reasoning is desired. This theory is supported by a wide range of evidence, including model scaling studies, diagnostic interventions, and ablation experiments.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Inverse Scaling of CoT Faithfulness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; increases in size or capability &#8594; from small to large<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; is &#8594; easy or familiar to the model</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; faithfulness of CoT &#8594; decreases &#8594; i.e., final answers become less causally dependent on intermediate steps</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ModelScaling: For 7/8 standard tasks, the 13B model changes its answer after seeing CoT more often than the 175B model (i.e., more reliance on CoT); for most tasks faithfulness monotonically worsens from 13B -> 175B. <a href="../results/extraction-result-3310.html#e3310.4" class="evidence-link">[e3310.4]</a> </li>
    <li>EarlyAnswering: AOC (area-over-curve) values are higher (more faithful) on harder tasks (AQuA, LogiQA) and lower on easier tasks (ARC Easy/Challenge). <a href="../results/extraction-result-3310.html#e3310.1" class="evidence-link">[e3310.1]</a> </li>
    <li>Adding Mistakes: Inserting mistakes into CoT changes final answers more often on harder tasks, less often on easier tasks. <a href="../results/extraction-result-3310.html#e3310.2" class="evidence-link">[e3310.2]</a> </li>
    <li>Paraphrase+Filler: Paraphrasing CoT does not reduce faithfulness, and adding filler tokens does not increase it, indicating that the effect is not due to surface form or context length. <a href="../results/extraction-result-3310.html#e3310.3" class="evidence-link">[e3310.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Task-Difficulty Modulation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; is &#8594; hard or unfamiliar</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; even large models &#8594; rely more on &#8594; CoT steps (higher faithfulness)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AQuA and LogiQA (harder tasks) show higher AOC (faithfulness) even for large models. <a href="../results/extraction-result-3310.html#e3310.1" class="evidence-link">[e3310.1]</a> <a href="../results/extraction-result-3310.html#e3310.2" class="evidence-link">[e3310.2]</a> </li>
    <li>ModelScaling: AQuA and LogiQA are exceptions where larger models did not show the worst faithfulness; for these two tasks the most faithful reasoning was not always from the smallest evaluated model. <a href="../results/extraction-result-3310.html#e3310.4" class="evidence-link">[e3310.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Diagnostic Intervention Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; diagnostic intervention &#8594; is applied &#8594; early-answering truncation or mistake insertion</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; change in final answer &#8594; measures &#8594; faithfulness of CoT (higher change = higher faithfulness)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>EarlyAnswering: Truncating CoT at intermediate steps and measuring answer change quantifies faithfulness. <a href="../results/extraction-result-3310.html#e3310.1" class="evidence-link">[e3310.1]</a> </li>
    <li>Adding Mistakes: Inserting plausible mistakes into CoT and measuring answer change quantifies faithfulness. <a href="../results/extraction-result-3310.html#e3310.2" class="evidence-link">[e3310.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Faithfulness-Accuracy Decoupling Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; CoT faithfulness &#8594; is low &#8594; on a task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; CoT accuracy improvement &#8594; may still be &#8594; high (i.e., accuracy gains from CoT do not always track faithfulness)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ModelScaling: Some tasks show accuracy gains from CoT even when faithfulness metrics are low. <a href="../results/extraction-result-3310.html#e3310.4" class="evidence-link">[e3310.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>On a new, easy reasoning task, increasing model size will decrease the causal dependence of final answers on CoT steps (lower AOC in early-answering/mistake tests).</li>
                <li>On a new, hard reasoning task, even large models will show higher faithfulness (higher AOC), with final answers more sensitive to CoT content.</li>
                <li>If a model is trained on a new domain, faithfulness will be higher on unfamiliar tasks and decrease as the model becomes more familiar with the domain.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Training models with explicit faithfulness objectives (e.g., penalizing post-hoc rationalization) may reverse the inverse scaling trend, but may also reduce overall accuracy.</li>
                <li>Hybrid models that combine small, faithful CoT generators with large answer predictors may yield more interpretable and accurate systems.</li>
                <li>If a model is trained with interventions that force causal use of CoT (e.g., via auxiliary losses), it may maintain high faithfulness even as size increases.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If increasing model size does not reduce CoT faithfulness on easy tasks, the inverse scaling law would be challenged.</li>
                <li>If interventions (early-answering, mistake insertion) do not reveal differences in faithfulness across model sizes, the theory would be called into question.</li>
                <li>If accuracy gains from CoT always track faithfulness (i.e., no decoupling), the Faithfulness-Accuracy Decoupling Law would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Exceptions where larger models are more faithful on certain tasks (e.g., AQuA, LogiQA). <a href="../results/extraction-result-3310.html#e3310.4" class="evidence-link">[e3310.4]</a> </li>
    <li>Cases where CoT accuracy improvement does not track faithfulness (e.g., some tasks show accuracy gains from CoT even when faithfulness is low). <a href="../results/extraction-result-3310.html#e3310.4" class="evidence-link">[e3310.4]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Turpin et al. (2023) Measuring Faithfulness in Chain-of-Thought Reasoning [Empirical findings, but this theory formalizes and generalizes the inverse scaling law]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Related: CoT effectiveness, but not faithfulness scaling]</li>
    <li>Uesato et al. (2022) Faithful Reasoning Using Language Models [Related: faithfulness in reasoning, but this theory extends to scaling trends]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Faithfulness-Scaling Inverse Law for Chain-of-Thought Reasoning",
    "theory_description": "This theory asserts that as language model (LM) size and capability increase, the faithfulness of chain-of-thought (CoT) reasoning—i.e., the degree to which intermediate steps causally influence the final answer—often decreases, especially on easier or familiar tasks. Larger models are more likely to produce correct answers without relying on the explicit reasoning steps they generate, leading to post-hoc rationalization rather than faithful stepwise reasoning. This inverse scaling effect is modulated by task difficulty: on harder or unfamiliar tasks, even large models may rely more on CoT, while on easier tasks, they may ignore or bypass the chain. The theory further posits that interventions such as early-answering truncation and mistake insertion can diagnose faithfulness, and that maximizing model size is not always optimal when faithful, interpretable reasoning is desired. This theory is supported by a wide range of evidence, including model scaling studies, diagnostic interventions, and ablation experiments.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Inverse Scaling of CoT Faithfulness Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "increases in size or capability",
                        "object": "from small to large"
                    },
                    {
                        "subject": "task",
                        "relation": "is",
                        "object": "easy or familiar to the model"
                    }
                ],
                "then": [
                    {
                        "subject": "faithfulness of CoT",
                        "relation": "decreases",
                        "object": "i.e., final answers become less causally dependent on intermediate steps"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ModelScaling: For 7/8 standard tasks, the 13B model changes its answer after seeing CoT more often than the 175B model (i.e., more reliance on CoT); for most tasks faithfulness monotonically worsens from 13B -&gt; 175B.",
                        "uuids": [
                            "e3310.4"
                        ]
                    },
                    {
                        "text": "EarlyAnswering: AOC (area-over-curve) values are higher (more faithful) on harder tasks (AQuA, LogiQA) and lower on easier tasks (ARC Easy/Challenge).",
                        "uuids": [
                            "e3310.1"
                        ]
                    },
                    {
                        "text": "Adding Mistakes: Inserting mistakes into CoT changes final answers more often on harder tasks, less often on easier tasks.",
                        "uuids": [
                            "e3310.2"
                        ]
                    },
                    {
                        "text": "Paraphrase+Filler: Paraphrasing CoT does not reduce faithfulness, and adding filler tokens does not increase it, indicating that the effect is not due to surface form or context length.",
                        "uuids": [
                            "e3310.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Task-Difficulty Modulation Law",
                "if": [
                    {
                        "subject": "task",
                        "relation": "is",
                        "object": "hard or unfamiliar"
                    }
                ],
                "then": [
                    {
                        "subject": "even large models",
                        "relation": "rely more on",
                        "object": "CoT steps (higher faithfulness)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AQuA and LogiQA (harder tasks) show higher AOC (faithfulness) even for large models.",
                        "uuids": [
                            "e3310.1",
                            "e3310.2"
                        ]
                    },
                    {
                        "text": "ModelScaling: AQuA and LogiQA are exceptions where larger models did not show the worst faithfulness; for these two tasks the most faithful reasoning was not always from the smallest evaluated model.",
                        "uuids": [
                            "e3310.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Diagnostic Intervention Law",
                "if": [
                    {
                        "subject": "diagnostic intervention",
                        "relation": "is applied",
                        "object": "early-answering truncation or mistake insertion"
                    }
                ],
                "then": [
                    {
                        "subject": "change in final answer",
                        "relation": "measures",
                        "object": "faithfulness of CoT (higher change = higher faithfulness)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "EarlyAnswering: Truncating CoT at intermediate steps and measuring answer change quantifies faithfulness.",
                        "uuids": [
                            "e3310.1"
                        ]
                    },
                    {
                        "text": "Adding Mistakes: Inserting plausible mistakes into CoT and measuring answer change quantifies faithfulness.",
                        "uuids": [
                            "e3310.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Faithfulness-Accuracy Decoupling Law",
                "if": [
                    {
                        "subject": "CoT faithfulness",
                        "relation": "is low",
                        "object": "on a task"
                    }
                ],
                "then": [
                    {
                        "subject": "CoT accuracy improvement",
                        "relation": "may still be",
                        "object": "high (i.e., accuracy gains from CoT do not always track faithfulness)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ModelScaling: Some tasks show accuracy gains from CoT even when faithfulness metrics are low.",
                        "uuids": [
                            "e3310.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "On a new, easy reasoning task, increasing model size will decrease the causal dependence of final answers on CoT steps (lower AOC in early-answering/mistake tests).",
        "On a new, hard reasoning task, even large models will show higher faithfulness (higher AOC), with final answers more sensitive to CoT content.",
        "If a model is trained on a new domain, faithfulness will be higher on unfamiliar tasks and decrease as the model becomes more familiar with the domain."
    ],
    "new_predictions_unknown": [
        "Training models with explicit faithfulness objectives (e.g., penalizing post-hoc rationalization) may reverse the inverse scaling trend, but may also reduce overall accuracy.",
        "Hybrid models that combine small, faithful CoT generators with large answer predictors may yield more interpretable and accurate systems.",
        "If a model is trained with interventions that force causal use of CoT (e.g., via auxiliary losses), it may maintain high faithfulness even as size increases."
    ],
    "negative_experiments": [
        "If increasing model size does not reduce CoT faithfulness on easy tasks, the inverse scaling law would be challenged.",
        "If interventions (early-answering, mistake insertion) do not reveal differences in faithfulness across model sizes, the theory would be called into question.",
        "If accuracy gains from CoT always track faithfulness (i.e., no decoupling), the Faithfulness-Accuracy Decoupling Law would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Exceptions where larger models are more faithful on certain tasks (e.g., AQuA, LogiQA).",
            "uuids": [
                "e3310.4"
            ]
        },
        {
            "text": "Cases where CoT accuracy improvement does not track faithfulness (e.g., some tasks show accuracy gains from CoT even when faithfulness is low).",
            "uuids": [
                "e3310.4"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "AQuA and LogiQA are exceptions where larger models did not show the worst faithfulness; for these two tasks the most faithful reasoning was not always from the smallest evaluated model.",
            "uuids": [
                "e3310.4"
            ]
        }
    ],
    "special_cases": [
        "On tasks where the model has not seen similar problems during training, even large models may exhibit high faithfulness.",
        "If CoT is used for interpretability rather than accuracy, smaller models may be preferable.",
        "Tasks with inherently ambiguous or multi-solution reasoning may not show clear faithfulness trends."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Turpin et al. (2023) Measuring Faithfulness in Chain-of-Thought Reasoning [Empirical findings, but this theory formalizes and generalizes the inverse scaling law]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Related: CoT effectiveness, but not faithfulness scaling]",
            "Uesato et al. (2022) Faithful Reasoning Using Language Models [Related: faithfulness in reasoning, but this theory extends to scaling trends]"
        ]
    },
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Dimensional Evaluation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-2216</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-2216</p>
                <p><strong>Name:</strong> Multi-Dimensional Evaluation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories.</p>
                <p><strong>Description:</strong> This theory posits that LLM-generated scientific theories must be evaluated along multiple, orthogonal dimensions—such as empirical adequacy, logical coherence, novelty, explanatory power, and predictive accuracy—using a structured, weighted framework. The overall evaluation is a function of performance across these dimensions, and weaknesses in one dimension can be compensated by strengths in others, depending on the intended use-case.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Orthogonal Evaluation Dimensions Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; theory &#8594; is_generated_by &#8594; LLM</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_process &#8594; should_include &#8594; empirical_adequacy<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_process &#8594; should_include &#8594; logical_coherence<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_process &#8594; should_include &#8594; novelty<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_process &#8594; should_include &#8594; explanatory_power<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_process &#8594; should_include &#8594; predictive_accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Scientific theory evaluation in philosophy of science (e.g., Kuhn, Lakatos) emphasizes multiple criteria beyond empirical fit. </li>
    <li>AI evaluation frameworks often use multi-metric scoring (e.g., BLEU, ROUGE, F1, accuracy) to capture different aspects of model performance. </li>
    <li>No single metric fully captures the quality of a scientific theory; trade-offs are often necessary. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law adapts established multi-dimensional evaluation to the context of LLM-generated scientific theory assessment.</p>            <p><strong>What Already Exists:</strong> Multi-criteria evaluation is established in philosophy of science and AI model assessment.</p>            <p><strong>What is Novel:</strong> Explicit, structured, and weighted application to LLM-generated scientific theories is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]</li>
    <li>Lakatos (1978) The Methodology of Scientific Research Programmes [multiple criteria for theory evaluation]</li>
    <li>Lin (2004) ROUGE: A Package for Automatic Evaluation of Summaries [multi-metric evaluation in NLP]</li>
</ul>
            <h3>Statement 1: Weighted Compensatory Scoring Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; theory &#8594; is_generated_by &#8594; LLM<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_dimensions &#8594; are_scored &#8594; quantitatively_or_qualitatively</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; overall_evaluation_score &#8594; is_function_of &#8594; weighted_sum_of_dimension_scores<span style="color: #888888;">, and</span></div>
        <div>&#8226; dimension_weakness &#8594; can_be_compensated_by &#8594; dimension_strength</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Compensatory scoring is used in multi-criteria decision analysis and model selection. </li>
    <li>Scientific theories are sometimes accepted despite weaknesses in one area if they excel in others (e.g., explanatory power vs. empirical fit). </li>
    <li>Weighted scoring allows for context-sensitive evaluation, which is important for diverse scientific domains. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law adapts compensatory scoring to the structured evaluation of LLM-generated scientific theories.</p>            <p><strong>What Already Exists:</strong> Weighted, compensatory scoring is established in decision theory and model evaluation.</p>            <p><strong>What is Novel:</strong> Application to LLM-generated scientific theory evaluation with explicit dimension trade-offs is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]</li>
    <li>Bishop (2006) Pattern Recognition and Machine Learning [model selection and scoring]</li>
    <li>Kuhn (1962) The Structure of Scientific Revolutions [theory choice trade-offs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM-generated theories with high scores across multiple dimensions will be more likely to be accepted by human experts.</li>
                <li>Theories with weaknesses in one dimension can still be valuable if they are strong in others, depending on the application.</li>
                <li>Explicit weighting of evaluation dimensions will improve inter-rater reliability in theory assessment.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Optimal weighting schemes for different scientific domains may emerge from empirical study.</li>
                <li>Some LLM-generated theories may be highly novel but weak in empirical adequacy, raising questions about the value of novelty.</li>
                <li>Multi-dimensional evaluation may reveal new, previously unrecognized dimensions important for theory assessment.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If multi-dimensional evaluation does not improve over single-metric evaluation in predicting theory utility, the theory is called into question.</li>
                <li>If compensatory scoring leads to acceptance of fundamentally flawed theories, the law is falsified.</li>
                <li>If human experts consistently disagree with weighted evaluation outcomes, the framework may be insufficient.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to determine optimal weights for each evaluation dimension. </li>
    <li>Potential for gaming the evaluation by optimizing for certain dimensions at the expense of others is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory adapts and extends multi-dimensional evaluation to the context of LLM-generated scientific theories.</p>
            <p><strong>References:</strong> <ul>
    <li>Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]</li>
    <li>Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]</li>
    <li>Bishop (2006) Pattern Recognition and Machine Learning [model selection and scoring]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Dimensional Evaluation Theory",
    "theory_description": "This theory posits that LLM-generated scientific theories must be evaluated along multiple, orthogonal dimensions—such as empirical adequacy, logical coherence, novelty, explanatory power, and predictive accuracy—using a structured, weighted framework. The overall evaluation is a function of performance across these dimensions, and weaknesses in one dimension can be compensated by strengths in others, depending on the intended use-case.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Orthogonal Evaluation Dimensions Law",
                "if": [
                    {
                        "subject": "theory",
                        "relation": "is_generated_by",
                        "object": "LLM"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_process",
                        "relation": "should_include",
                        "object": "empirical_adequacy"
                    },
                    {
                        "subject": "evaluation_process",
                        "relation": "should_include",
                        "object": "logical_coherence"
                    },
                    {
                        "subject": "evaluation_process",
                        "relation": "should_include",
                        "object": "novelty"
                    },
                    {
                        "subject": "evaluation_process",
                        "relation": "should_include",
                        "object": "explanatory_power"
                    },
                    {
                        "subject": "evaluation_process",
                        "relation": "should_include",
                        "object": "predictive_accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Scientific theory evaluation in philosophy of science (e.g., Kuhn, Lakatos) emphasizes multiple criteria beyond empirical fit.",
                        "uuids": []
                    },
                    {
                        "text": "AI evaluation frameworks often use multi-metric scoring (e.g., BLEU, ROUGE, F1, accuracy) to capture different aspects of model performance.",
                        "uuids": []
                    },
                    {
                        "text": "No single metric fully captures the quality of a scientific theory; trade-offs are often necessary.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Multi-criteria evaluation is established in philosophy of science and AI model assessment.",
                    "what_is_novel": "Explicit, structured, and weighted application to LLM-generated scientific theories is novel.",
                    "classification_explanation": "The law adapts established multi-dimensional evaluation to the context of LLM-generated scientific theory assessment.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]",
                        "Lakatos (1978) The Methodology of Scientific Research Programmes [multiple criteria for theory evaluation]",
                        "Lin (2004) ROUGE: A Package for Automatic Evaluation of Summaries [multi-metric evaluation in NLP]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Weighted Compensatory Scoring Law",
                "if": [
                    {
                        "subject": "theory",
                        "relation": "is_generated_by",
                        "object": "LLM"
                    },
                    {
                        "subject": "evaluation_dimensions",
                        "relation": "are_scored",
                        "object": "quantitatively_or_qualitatively"
                    }
                ],
                "then": [
                    {
                        "subject": "overall_evaluation_score",
                        "relation": "is_function_of",
                        "object": "weighted_sum_of_dimension_scores"
                    },
                    {
                        "subject": "dimension_weakness",
                        "relation": "can_be_compensated_by",
                        "object": "dimension_strength"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Compensatory scoring is used in multi-criteria decision analysis and model selection.",
                        "uuids": []
                    },
                    {
                        "text": "Scientific theories are sometimes accepted despite weaknesses in one area if they excel in others (e.g., explanatory power vs. empirical fit).",
                        "uuids": []
                    },
                    {
                        "text": "Weighted scoring allows for context-sensitive evaluation, which is important for diverse scientific domains.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Weighted, compensatory scoring is established in decision theory and model evaluation.",
                    "what_is_novel": "Application to LLM-generated scientific theory evaluation with explicit dimension trade-offs is novel.",
                    "classification_explanation": "The law adapts compensatory scoring to the structured evaluation of LLM-generated scientific theories.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]",
                        "Bishop (2006) Pattern Recognition and Machine Learning [model selection and scoring]",
                        "Kuhn (1962) The Structure of Scientific Revolutions [theory choice trade-offs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM-generated theories with high scores across multiple dimensions will be more likely to be accepted by human experts.",
        "Theories with weaknesses in one dimension can still be valuable if they are strong in others, depending on the application.",
        "Explicit weighting of evaluation dimensions will improve inter-rater reliability in theory assessment."
    ],
    "new_predictions_unknown": [
        "Optimal weighting schemes for different scientific domains may emerge from empirical study.",
        "Some LLM-generated theories may be highly novel but weak in empirical adequacy, raising questions about the value of novelty.",
        "Multi-dimensional evaluation may reveal new, previously unrecognized dimensions important for theory assessment."
    ],
    "negative_experiments": [
        "If multi-dimensional evaluation does not improve over single-metric evaluation in predicting theory utility, the theory is called into question.",
        "If compensatory scoring leads to acceptance of fundamentally flawed theories, the law is falsified.",
        "If human experts consistently disagree with weighted evaluation outcomes, the framework may be insufficient."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to determine optimal weights for each evaluation dimension.",
            "uuids": []
        },
        {
            "text": "Potential for gaming the evaluation by optimizing for certain dimensions at the expense of others is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some scientific communities prioritize certain dimensions (e.g., empirical adequacy) over others, challenging the compensatory approach.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In domains with strict empirical requirements, compensatory scoring may be inappropriate.",
        "Highly novel theories may require different weighting schemes than incremental theories.",
        "Some dimensions (e.g., logical coherence) may be non-compensatory in certain contexts."
    ],
    "existing_theory": {
        "what_already_exists": "Multi-criteria and weighted evaluation are established in philosophy of science and decision theory.",
        "what_is_novel": "Explicit, structured, and compensatory application to LLM-generated scientific theory evaluation is novel.",
        "classification_explanation": "The theory adapts and extends multi-dimensional evaluation to the context of LLM-generated scientific theories.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]",
            "Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]",
            "Bishop (2006) Pattern Recognition and Machine Learning [model selection and scoring]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how to evaluate LLM-generated scientific theories.",
    "original_theory_id": "theory-674",
    "original_theory_name": "Iterative, Human-in-the-Loop, and Self-Refining Evaluation Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
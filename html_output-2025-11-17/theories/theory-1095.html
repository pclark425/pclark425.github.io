<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Alignment Theory for Logical Reasoning in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1095</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1095</p>
                <p><strong>Name:</strong> Structural Alignment Theory for Logical Reasoning in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory proposes that the ability of LMs to perform strict logical reasoning depends on the degree of structural alignment between the input representation and the model's internal knowledge graph or latent structure. When the input's logical structure is made explicit and aligns with the model's learned representations, logical reasoning is more accurate and reliable.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structural Alignment Facilitates Logical Inference (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; input &#8594; is_structurally_aligned_with &#8594; model's internal knowledge graph or latent structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; performs &#8594; accurate logical reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs perform better on logic tasks when input is formatted in a way that matches their training data or internal representations (e.g., tables, formal logic). </li>
    <li>Failures in logical reasoning often occur when input structure is ambiguous or misaligned with model expectations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Input formatting is known, but the structural alignment hypothesis is a new explanatory mechanism.</p>            <p><strong>What Already Exists:</strong> The importance of input formatting and representation is recognized in LM prompting literature.</p>            <p><strong>What is Novel:</strong> The explicit claim that structural alignment with internal representations is the key determinant of logical reasoning success.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Language Models [input structure and reasoning]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [input structure and reasoning]</li>
</ul>
            <h3>Statement 1: Misalignment Leads to Logical Failure (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; input &#8594; is_misaligned_with &#8594; model's internal knowledge graph or latent structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; produces &#8594; logical errors or inconsistent reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical failures in LM logical reasoning are often traced to ambiguous or poorly structured input. </li>
    <li>Reformatting the same logical problem to match model expectations often resolves errors. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect is known, but the structural alignment mechanism is a novel theoretical contribution.</p>            <p><strong>What Already Exists:</strong> Prompt engineering and input formatting are known to affect LM performance.</p>            <p><strong>What is Novel:</strong> The direct causal link between misalignment and logical failure is a new explanatory law.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Language Models [input structure and reasoning]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [input structure and reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Reformatting logic problems to match the model's training data structure will improve logical reasoning accuracy.</li>
                <li>Ambiguous or structurally novel inputs will result in more logical errors.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Explicitly training LMs to align internal representations with a wide variety of logical structures may generalize logical reasoning across domains.</li>
                <li>Developing methods to dynamically infer and align input structure with internal representations may enable robust logical reasoning even for novel formats.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs perform equally well on logic tasks regardless of input structure, the theory is challenged.</li>
                <li>If misaligned inputs do not increase logical errors, the misalignment law is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs may have sufficiently flexible representations to handle misaligned inputs without error. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory builds on known effects but introduces a new explanatory mechanism.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Language Models [input structure and reasoning]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [input structure and reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structural Alignment Theory for Logical Reasoning in Language Models",
    "theory_description": "This theory proposes that the ability of LMs to perform strict logical reasoning depends on the degree of structural alignment between the input representation and the model's internal knowledge graph or latent structure. When the input's logical structure is made explicit and aligns with the model's learned representations, logical reasoning is more accurate and reliable.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structural Alignment Facilitates Logical Inference",
                "if": [
                    {
                        "subject": "input",
                        "relation": "is_structurally_aligned_with",
                        "object": "model's internal knowledge graph or latent structure"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "performs",
                        "object": "accurate logical reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs perform better on logic tasks when input is formatted in a way that matches their training data or internal representations (e.g., tables, formal logic).",
                        "uuids": []
                    },
                    {
                        "text": "Failures in logical reasoning often occur when input structure is ambiguous or misaligned with model expectations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The importance of input formatting and representation is recognized in LM prompting literature.",
                    "what_is_novel": "The explicit claim that structural alignment with internal representations is the key determinant of logical reasoning success.",
                    "classification_explanation": "Input formatting is known, but the structural alignment hypothesis is a new explanatory mechanism.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Language Models [input structure and reasoning]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [input structure and reasoning]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Misalignment Leads to Logical Failure",
                "if": [
                    {
                        "subject": "input",
                        "relation": "is_misaligned_with",
                        "object": "model's internal knowledge graph or latent structure"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "produces",
                        "object": "logical errors or inconsistent reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical failures in LM logical reasoning are often traced to ambiguous or poorly structured input.",
                        "uuids": []
                    },
                    {
                        "text": "Reformatting the same logical problem to match model expectations often resolves errors.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering and input formatting are known to affect LM performance.",
                    "what_is_novel": "The direct causal link between misalignment and logical failure is a new explanatory law.",
                    "classification_explanation": "The effect is known, but the structural alignment mechanism is a novel theoretical contribution.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Language Models [input structure and reasoning]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [input structure and reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Reformatting logic problems to match the model's training data structure will improve logical reasoning accuracy.",
        "Ambiguous or structurally novel inputs will result in more logical errors."
    ],
    "new_predictions_unknown": [
        "Explicitly training LMs to align internal representations with a wide variety of logical structures may generalize logical reasoning across domains.",
        "Developing methods to dynamically infer and align input structure with internal representations may enable robust logical reasoning even for novel formats."
    ],
    "negative_experiments": [
        "If LMs perform equally well on logic tasks regardless of input structure, the theory is challenged.",
        "If misaligned inputs do not increase logical errors, the misalignment law is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs may have sufficiently flexible representations to handle misaligned inputs without error.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LMs can perform logical reasoning on unstructured or natural language inputs, suggesting other mechanisms may be at play.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Highly overparameterized LMs may learn to generalize across input structures.",
        "Logic tasks with minimal structural requirements may not be affected by alignment."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and input formatting are known to affect LM performance.",
        "what_is_novel": "The explicit structural alignment hypothesis as the key determinant of logical reasoning.",
        "classification_explanation": "The theory builds on known effects but introduces a new explanatory mechanism.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Language Models [input structure and reasoning]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [input structure and reasoning]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-601",
    "original_theory_name": "Neuro-Symbolic Interface Bottleneck Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
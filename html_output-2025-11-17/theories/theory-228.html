<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Format-Dependent Algorithm Learning Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-228</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-228</p>
                <p><strong>Name:</strong> Format-Dependent Algorithm Learning Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that language models learn fundamentally different arithmetic algorithms depending on the format in which numbers and operations are presented during training. Rather than learning a single, format-invariant arithmetic procedure, models develop format-specific computational strategies optimized for the particular representational structure they encounter. This includes tokenization boundaries (character-level vs. subword), digit separators (spaces, commas, none), notation systems (positional, scientific, verbal), and problem layout (horizontal equations vs. vertical alignment). Each format creates different computational affordances and constraints that shape which algorithmic approaches are learnable. For instance, character-level tokenization enables digit-by-digit algorithms similar to human grade-school methods, while subword tokenization may force chunk-based or magnitude-estimation strategies. Similarly, comma-separated numbers may trigger different parsing and grouping algorithms than unseparated digit strings. The theory predicts that models will show format-specific performance patterns, struggle with format transfer, and that their internal representations and attention patterns will reflect the algorithmic strategies induced by their training format. This explains why models trained on different formats show different error patterns, why format changes at test time degrade performance, and why certain formats enable better arithmetic learning than others.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models learn different arithmetic algorithms for different input/output formats rather than learning a single format-invariant procedure.</li>
                <li>The format of number representation (tokenization, separators, notation) creates computational affordances that make certain algorithmic strategies more learnable than others.</li>
                <li>Character-level or digit-separated formats enable learning of digit-by-digit algorithms similar to human grade-school arithmetic methods.</li>
                <li>Subword tokenization formats may induce chunk-based processing or magnitude-estimation strategies rather than precise digit-wise computation.</li>
                <li>Formats that expose intermediate computational steps (like scratchpads or chain-of-thought) enable learning of multi-step algorithms that cannot be learned from direct input-output mappings.</li>
                <li>Models will show performance degradation when tested on formats different from their training format, even for mathematically equivalent problems.</li>
                <li>The internal attention patterns and representations learned by models will reflect the specific algorithmic strategy induced by their training format.</li>
                <li>Formats that align with the model's tokenization scheme will enable more reliable algorithm learning than misaligned formats.</li>
                <li>Carry and borrow operations require different algorithmic implementations depending on whether the format provides explicit digit boundaries or groups digits into chunks.</li>
                <li>Vertical alignment formats may enable different parallel processing strategies compared to horizontal equation formats.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Different tokenization schemes (character-level, BPE, WordPiece) produce significantly different arithmetic performance on identical problems, suggesting format-dependent algorithm learning. </li>
    <li>Character-level models sometimes outperform subword models on arithmetic tasks, indicating that finer-grained tokenization enables different (and sometimes more effective) algorithmic strategies. </li>
    <li>Models show different performance on numbers tokenized as single tokens versus numbers split across multiple tokens, demonstrating sensitivity to representational format. </li>
    <li>Specialized number encodings that change the format of number representation improve arithmetic performance, supporting the idea that format shapes algorithmic learning. </li>
    <li>Models trained with chain-of-thought prompting (which changes the format to include intermediate steps) show dramatically improved arithmetic performance, suggesting format-induced algorithmic changes. </li>
    <li>Scratchpad and intermediate computation formats enable models to learn multi-step algorithms that are not learnable in direct input-output formats. </li>
    <li>Models show better performance when numbers are presented in formats that align with their training distribution, indicating format-specific rather than format-general learning. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A model trained on comma-separated numbers (e.g., '1,234 + 5,678') will show degraded performance when tested on unseparated numbers ('1234 + 5678'), even though the arithmetic is identical.</li>
                <li>Models trained with space-separated digits ('1 2 3 4 + 5 6 7 8') will learn more reliable digit-by-digit algorithms and outperform standard tokenization on multi-digit arithmetic.</li>
                <li>Training on multiple formats simultaneously will produce models that learn format-detection mechanisms followed by format-specific algorithmic pathways, observable through different attention patterns for different formats.</li>
                <li>Models trained on vertical arithmetic formats (numbers aligned by place value) will show different error patterns than those trained on horizontal formats, with vertical formats producing fewer place-value errors.</li>
                <li>Fine-tuning a model on a new number format will require learning new algorithmic strategies, not just adapting existing ones, evidenced by the need for substantial training examples.</li>
                <li>Probing the internal representations of models trained on different formats will reveal format-specific computational structures (e.g., digit-position encodings in character-level models vs. magnitude encodings in subword models).</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Training with adversarial format variation (randomly changing formats during training) might force learning of format-invariant algorithms, or might prevent learning of any reliable algorithm due to inconsistent computational structure.</li>
                <li>Introducing explicit format-specification tokens (e.g., '<char_mode>' or '<chunk_mode>') might enable models to learn multiple algorithms and switch between them, or might create confusion and degrade overall performance.</li>
                <li>Using curriculum learning that progresses from format-aligned to format-misaligned examples might enable gradual development of format-invariant algorithms, or might cause catastrophic forgetting of the initial format-specific strategies.</li>
                <li>Architectures with explicit algorithmic modules (like Neural GPUs or Neural Turing Machines) might learn more format-invariant algorithms than standard transformers, or might still show format-dependence due to input encoding constraints.</li>
                <li>Cross-lingual transfer of arithmetic abilities might reveal universal format-independent algorithmic structures, or might show that even algorithmic knowledge is format-bound and language-specific.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If models show identical performance across all number formats (character-level, subword, comma-separated, space-separated, etc.), this would contradict the format-dependent learning hypothesis.</li>
                <li>If internal representations and attention patterns are identical across models trained on different formats, this would suggest format-independent algorithm learning.</li>
                <li>If a model trained exclusively on one format (e.g., comma-separated) performs equally well on a completely different format (e.g., scientific notation) without any fine-tuning, this would challenge the theory.</li>
                <li>If changing only the output format (keeping input format constant) doesn't affect performance, this would suggest that format-dependence is limited to input processing rather than algorithmic execution.</li>
                <li>If models trained with and without intermediate step formats (scratchpads) learn identical internal algorithms (as measured by mechanistic interpretability), this would question whether format truly shapes algorithmic learning.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory doesn't fully explain why very large models can sometimes overcome format dependencies better than smaller models, suggesting scale-dependent format invariance. </li>
    <li>The interaction between format-dependent algorithms and position embeddings is not fully specified - different position embedding schemes might enable or constrain different algorithmic strategies. </li>
    <li>The theory doesn't fully account for how models might learn meta-algorithms that can adapt to format variations, as some evidence suggests partial format transfer is possible. </li>
    <li>The role of pre-training data diversity in enabling format-flexible algorithms is not fully addressed - models may see multiple formats during pre-training. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Golkar et al. (2023) xVal: A Continuous Number Encoding for Large Language Models [Proposes that number encoding format affects performance but doesn't frame it as format-dependent algorithm learning]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Shows that format changes enable new algorithms but doesn't propose general format-dependence theory]</li>
    <li>Thawani et al. (2021) Representing Numbers in NLP: a Survey and a Vision [Surveys number representation issues but doesn't propose format-dependent algorithm learning theory]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Demonstrates format-induced performance changes but frames it as prompting rather than algorithmic learning]</li>
    <li>Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Shows format/frequency effects but attributes to memorization rather than algorithm learning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Format-Dependent Algorithm Learning Theory",
    "theory_description": "This theory proposes that language models learn fundamentally different arithmetic algorithms depending on the format in which numbers and operations are presented during training. Rather than learning a single, format-invariant arithmetic procedure, models develop format-specific computational strategies optimized for the particular representational structure they encounter. This includes tokenization boundaries (character-level vs. subword), digit separators (spaces, commas, none), notation systems (positional, scientific, verbal), and problem layout (horizontal equations vs. vertical alignment). Each format creates different computational affordances and constraints that shape which algorithmic approaches are learnable. For instance, character-level tokenization enables digit-by-digit algorithms similar to human grade-school methods, while subword tokenization may force chunk-based or magnitude-estimation strategies. Similarly, comma-separated numbers may trigger different parsing and grouping algorithms than unseparated digit strings. The theory predicts that models will show format-specific performance patterns, struggle with format transfer, and that their internal representations and attention patterns will reflect the algorithmic strategies induced by their training format. This explains why models trained on different formats show different error patterns, why format changes at test time degrade performance, and why certain formats enable better arithmetic learning than others.",
    "supporting_evidence": [
        {
            "text": "Different tokenization schemes (character-level, BPE, WordPiece) produce significantly different arithmetic performance on identical problems, suggesting format-dependent algorithm learning.",
            "citations": [
                "Golkar et al. (2023) xVal: A Continuous Number Encoding for Large Language Models",
                "Wallace et al. (2019) Do NLP Models Know Numbers? Probing Numeracy in Embeddings"
            ]
        },
        {
            "text": "Character-level models sometimes outperform subword models on arithmetic tasks, indicating that finer-grained tokenization enables different (and sometimes more effective) algorithmic strategies.",
            "citations": [
                "Tay et al. (2021) Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"
            ]
        },
        {
            "text": "Models show different performance on numbers tokenized as single tokens versus numbers split across multiple tokens, demonstrating sensitivity to representational format.",
            "citations": [
                "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning",
                "Thawani et al. (2021) Representing Numbers in NLP: a Survey and a Vision"
            ]
        },
        {
            "text": "Specialized number encodings that change the format of number representation improve arithmetic performance, supporting the idea that format shapes algorithmic learning.",
            "citations": [
                "Golkar et al. (2023) xVal: A Continuous Number Encoding for Large Language Models",
                "Spithourakis & Riedel (2018) Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"
            ]
        },
        {
            "text": "Models trained with chain-of-thought prompting (which changes the format to include intermediate steps) show dramatically improved arithmetic performance, suggesting format-induced algorithmic changes.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models"
            ]
        },
        {
            "text": "Scratchpad and intermediate computation formats enable models to learn multi-step algorithms that are not learnable in direct input-output formats.",
            "citations": [
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models"
            ]
        },
        {
            "text": "Models show better performance when numbers are presented in formats that align with their training distribution, indicating format-specific rather than format-general learning.",
            "citations": [
                "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning"
            ]
        }
    ],
    "theory_statements": [
        "Language models learn different arithmetic algorithms for different input/output formats rather than learning a single format-invariant procedure.",
        "The format of number representation (tokenization, separators, notation) creates computational affordances that make certain algorithmic strategies more learnable than others.",
        "Character-level or digit-separated formats enable learning of digit-by-digit algorithms similar to human grade-school arithmetic methods.",
        "Subword tokenization formats may induce chunk-based processing or magnitude-estimation strategies rather than precise digit-wise computation.",
        "Formats that expose intermediate computational steps (like scratchpads or chain-of-thought) enable learning of multi-step algorithms that cannot be learned from direct input-output mappings.",
        "Models will show performance degradation when tested on formats different from their training format, even for mathematically equivalent problems.",
        "The internal attention patterns and representations learned by models will reflect the specific algorithmic strategy induced by their training format.",
        "Formats that align with the model's tokenization scheme will enable more reliable algorithm learning than misaligned formats.",
        "Carry and borrow operations require different algorithmic implementations depending on whether the format provides explicit digit boundaries or groups digits into chunks.",
        "Vertical alignment formats may enable different parallel processing strategies compared to horizontal equation formats."
    ],
    "new_predictions_likely": [
        "A model trained on comma-separated numbers (e.g., '1,234 + 5,678') will show degraded performance when tested on unseparated numbers ('1234 + 5678'), even though the arithmetic is identical.",
        "Models trained with space-separated digits ('1 2 3 4 + 5 6 7 8') will learn more reliable digit-by-digit algorithms and outperform standard tokenization on multi-digit arithmetic.",
        "Training on multiple formats simultaneously will produce models that learn format-detection mechanisms followed by format-specific algorithmic pathways, observable through different attention patterns for different formats.",
        "Models trained on vertical arithmetic formats (numbers aligned by place value) will show different error patterns than those trained on horizontal formats, with vertical formats producing fewer place-value errors.",
        "Fine-tuning a model on a new number format will require learning new algorithmic strategies, not just adapting existing ones, evidenced by the need for substantial training examples.",
        "Probing the internal representations of models trained on different formats will reveal format-specific computational structures (e.g., digit-position encodings in character-level models vs. magnitude encodings in subword models)."
    ],
    "new_predictions_unknown": [
        "Training with adversarial format variation (randomly changing formats during training) might force learning of format-invariant algorithms, or might prevent learning of any reliable algorithm due to inconsistent computational structure.",
        "Introducing explicit format-specification tokens (e.g., '&lt;char_mode&gt;' or '&lt;chunk_mode&gt;') might enable models to learn multiple algorithms and switch between them, or might create confusion and degrade overall performance.",
        "Using curriculum learning that progresses from format-aligned to format-misaligned examples might enable gradual development of format-invariant algorithms, or might cause catastrophic forgetting of the initial format-specific strategies.",
        "Architectures with explicit algorithmic modules (like Neural GPUs or Neural Turing Machines) might learn more format-invariant algorithms than standard transformers, or might still show format-dependence due to input encoding constraints.",
        "Cross-lingual transfer of arithmetic abilities might reveal universal format-independent algorithmic structures, or might show that even algorithmic knowledge is format-bound and language-specific."
    ],
    "negative_experiments": [
        "If models show identical performance across all number formats (character-level, subword, comma-separated, space-separated, etc.), this would contradict the format-dependent learning hypothesis.",
        "If internal representations and attention patterns are identical across models trained on different formats, this would suggest format-independent algorithm learning.",
        "If a model trained exclusively on one format (e.g., comma-separated) performs equally well on a completely different format (e.g., scientific notation) without any fine-tuning, this would challenge the theory.",
        "If changing only the output format (keeping input format constant) doesn't affect performance, this would suggest that format-dependence is limited to input processing rather than algorithmic execution.",
        "If models trained with and without intermediate step formats (scratchpads) learn identical internal algorithms (as measured by mechanistic interpretability), this would question whether format truly shapes algorithmic learning."
    ],
    "unaccounted_for": [
        {
            "text": "The theory doesn't fully explain why very large models can sometimes overcome format dependencies better than smaller models, suggesting scale-dependent format invariance.",
            "citations": [
                "Lewkowycz et al. (2022) Solving Quantitative Reasoning Problems with Language Models",
                "Chowdhery et al. (2022) PaLM: Scaling Language Modeling with Pathways"
            ]
        },
        {
            "text": "The interaction between format-dependent algorithms and position embeddings is not fully specified - different position embedding schemes might enable or constrain different algorithmic strategies.",
            "citations": []
        },
        {
            "text": "The theory doesn't fully account for how models might learn meta-algorithms that can adapt to format variations, as some evidence suggests partial format transfer is possible.",
            "citations": [
                "Anil et al. (2022) Exploring Length Generalization in Large Language Models"
            ]
        },
        {
            "text": "The role of pre-training data diversity in enabling format-flexible algorithms is not fully addressed - models may see multiple formats during pre-training.",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some very large models show reasonable arithmetic performance across multiple formats despite using standard BPE tokenization, suggesting possible format-invariant learning at scale.",
            "citations": [
                "Lewkowycz et al. (2022) Solving Quantitative Reasoning Problems with Language Models",
                "Chowdhery et al. (2022) PaLM: Scaling Language Modeling with Pathways"
            ]
        },
        {
            "text": "Some studies show that models can generalize to slightly different formats than their training format, suggesting partial format independence.",
            "citations": [
                "Anil et al. (2022) Exploring Length Generalization in Large Language Models"
            ]
        },
        {
            "text": "Continuous number embeddings that completely change the format sometimes show mixed results rather than consistent improvements, questioning whether format is the primary constraint.",
            "citations": [
                "Berg-Kirkpatrick & Spokoyny (2020) An Empirical Investigation of Contextualized Number Prediction"
            ]
        }
    ],
    "special_cases": [
        "Very large models with extensive pre-training on diverse formats may develop partial format-invariance through exposure to multiple format-specific algorithms.",
        "Single-digit arithmetic may not show strong format-dependence because it can be solved through memorization rather than algorithmic computation.",
        "Operations that don't require multi-step algorithms (like magnitude comparison) may be less format-dependent than operations requiring sequential processing (like multi-digit multiplication).",
        "Formats that are mathematically equivalent but perceptually very different (e.g., '2+2' vs. 'two plus two') may require entirely different processing pathways, making format-dependence more extreme.",
        "Models with architectural inductive biases for certain formats (e.g., convolutional layers for spatial alignment) may show different format-dependence patterns than standard transformers.",
        "The degree of format-dependence may vary by operation type: addition and subtraction may be more format-flexible than multiplication and division due to their simpler algorithmic requirements."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Golkar et al. (2023) xVal: A Continuous Number Encoding for Large Language Models [Proposes that number encoding format affects performance but doesn't frame it as format-dependent algorithm learning]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Shows that format changes enable new algorithms but doesn't propose general format-dependence theory]",
            "Thawani et al. (2021) Representing Numbers in NLP: a Survey and a Vision [Surveys number representation issues but doesn't propose format-dependent algorithm learning theory]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Demonstrates format-induced performance changes but frames it as prompting rather than algorithmic learning]",
            "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Shows format/frequency effects but attributes to memorization rather than algorithm learning]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "theory_query": "Build a theory of how language models perform arithmetic.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-55",
    "original_theory_name": "Format-Dependent Algorithm Learning Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
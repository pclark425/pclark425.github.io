<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contextual Abstraction and Decomposition Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1139</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1139</p>
                <p><strong>Name:</strong> Contextual Abstraction and Decomposition Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models achieve strict logical reasoning by dynamically abstracting and decomposing complex problems into contextually relevant subproblems, leveraging their training on diverse linguistic patterns. The theory claims that LMs perform best when they are prompted or architected to identify logical substructures, abstract them into modular components, and reason over these components in a context-sensitive manner, rather than relying solely on end-to-end pattern matching or memorization.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contextual Decomposition Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning task &#8594; contains &#8594; multiple logical subproblems<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; is prompted or trained to &#8594; identify and decompose subproblems</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; achieves &#8594; higher logical accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompting LMs to break down problems into sub-questions improves performance on multi-step reasoning tasks. </li>
    <li>Decomposition-based prompting (e.g., 'Let's solve step by step') leads to better logical inference. </li>
    <li>LMs trained on datasets with explicit subproblem structure generalize better to complex logic tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law extends prior work by making decomposition a necessary mechanism for strict logic, not just a useful tool.</p>            <p><strong>What Already Exists:</strong> Step-by-step and decomposition prompting are known to help LMs with reasoning.</p>            <p><strong>What is Novel:</strong> The law formalizes the necessity of contextual decomposition for strict logical reasoning, not just as a helpful heuristic.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [decomposition and verification]</li>
</ul>
            <h3>Statement 1: Contextual Abstraction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is exposed to &#8594; diverse logical patterns in training<span style="color: #888888;">, and</span></div>
        <div>&#8226; reasoning task &#8594; requires &#8594; abstraction of logical structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can generalize &#8594; to novel logical tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs trained on diverse logic datasets show better generalization to new logic tasks. </li>
    <li>Abstraction and analogy are key to human logical reasoning and are observed in LMs with broad training. </li>
    <li>Prompting LMs to abstract rules from examples improves their ability to solve unfamiliar logic puzzles. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes abstraction and decomposition as a unified, necessary mechanism for strict logic in LMs.</p>            <p><strong>What Already Exists:</strong> Generalization via abstraction is a known property of human and some machine reasoning.</p>            <p><strong>What is Novel:</strong> The law claims that contextual abstraction, when combined with decomposition, is a necessary mechanism for strict logical reasoning in LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building Machines That Learn and Think Like People [abstraction and compositionality]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Prompting LMs to explicitly decompose logic tasks into subproblems will improve accuracy on multi-step logic benchmarks.</li>
                <li>Training LMs on datasets with explicit logical abstraction and decomposition will enhance generalization to novel logic tasks.</li>
                <li>LMs that fail to decompose or abstract will struggle with complex, unfamiliar logical problems.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LMs are trained to autonomously identify and abstract logical substructures, they may develop novel decomposition strategies.</li>
                <li>Contextual abstraction may enable LMs to transfer logical reasoning skills across domains (e.g., from math to law).</li>
                <li>Over-decomposition may lead to inefficiency or error propagation in LM reasoning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs that do not decompose or abstract can match or exceed the logical accuracy of those that do, the theory is undermined.</li>
                <li>If decomposition and abstraction do not improve generalization to novel logic tasks, the theory is called into question.</li>
                <li>If decomposition leads to worse performance on certain logic tasks, the theory's universality is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs perform well on logic tasks via memorization or pattern matching, without explicit decomposition. </li>
    <li>Certain logic tasks may be atomic and not benefit from decomposition. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and formalizes two mechanisms (abstraction and decomposition) as jointly necessary, which is a novel integration.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building Machines That Learn and Think Like People [abstraction and compositionality]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [decomposition and verification]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Contextual Abstraction and Decomposition Theory",
    "theory_description": "This theory posits that language models achieve strict logical reasoning by dynamically abstracting and decomposing complex problems into contextually relevant subproblems, leveraging their training on diverse linguistic patterns. The theory claims that LMs perform best when they are prompted or architected to identify logical substructures, abstract them into modular components, and reason over these components in a context-sensitive manner, rather than relying solely on end-to-end pattern matching or memorization.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contextual Decomposition Law",
                "if": [
                    {
                        "subject": "reasoning task",
                        "relation": "contains",
                        "object": "multiple logical subproblems"
                    },
                    {
                        "subject": "language model",
                        "relation": "is prompted or trained to",
                        "object": "identify and decompose subproblems"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "achieves",
                        "object": "higher logical accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompting LMs to break down problems into sub-questions improves performance on multi-step reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Decomposition-based prompting (e.g., 'Let's solve step by step') leads to better logical inference.",
                        "uuids": []
                    },
                    {
                        "text": "LMs trained on datasets with explicit subproblem structure generalize better to complex logic tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Step-by-step and decomposition prompting are known to help LMs with reasoning.",
                    "what_is_novel": "The law formalizes the necessity of contextual decomposition for strict logical reasoning, not just as a helpful heuristic.",
                    "classification_explanation": "The law extends prior work by making decomposition a necessary mechanism for strict logic, not just a useful tool.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]",
                        "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [decomposition and verification]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Contextual Abstraction Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is exposed to",
                        "object": "diverse logical patterns in training"
                    },
                    {
                        "subject": "reasoning task",
                        "relation": "requires",
                        "object": "abstraction of logical structure"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can generalize",
                        "object": "to novel logical tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs trained on diverse logic datasets show better generalization to new logic tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Abstraction and analogy are key to human logical reasoning and are observed in LMs with broad training.",
                        "uuids": []
                    },
                    {
                        "text": "Prompting LMs to abstract rules from examples improves their ability to solve unfamiliar logic puzzles.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Generalization via abstraction is a known property of human and some machine reasoning.",
                    "what_is_novel": "The law claims that contextual abstraction, when combined with decomposition, is a necessary mechanism for strict logical reasoning in LMs.",
                    "classification_explanation": "The law synthesizes abstraction and decomposition as a unified, necessary mechanism for strict logic in LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake et al. (2017) Building Machines That Learn and Think Like People [abstraction and compositionality]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Prompting LMs to explicitly decompose logic tasks into subproblems will improve accuracy on multi-step logic benchmarks.",
        "Training LMs on datasets with explicit logical abstraction and decomposition will enhance generalization to novel logic tasks.",
        "LMs that fail to decompose or abstract will struggle with complex, unfamiliar logical problems."
    ],
    "new_predictions_unknown": [
        "If LMs are trained to autonomously identify and abstract logical substructures, they may develop novel decomposition strategies.",
        "Contextual abstraction may enable LMs to transfer logical reasoning skills across domains (e.g., from math to law).",
        "Over-decomposition may lead to inefficiency or error propagation in LM reasoning."
    ],
    "negative_experiments": [
        "If LMs that do not decompose or abstract can match or exceed the logical accuracy of those that do, the theory is undermined.",
        "If decomposition and abstraction do not improve generalization to novel logic tasks, the theory is called into question.",
        "If decomposition leads to worse performance on certain logic tasks, the theory's universality is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs perform well on logic tasks via memorization or pattern matching, without explicit decomposition.",
            "uuids": []
        },
        {
            "text": "Certain logic tasks may be atomic and not benefit from decomposition.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Very large LMs sometimes solve logic tasks without explicit decomposition or abstraction, suggesting scale can substitute for structure.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Atomic logic tasks may not require decomposition.",
        "Tasks with ambiguous or ill-defined logical structure may not benefit from abstraction.",
        "Over-decomposition may introduce unnecessary complexity."
    ],
    "existing_theory": {
        "what_already_exists": "Step-by-step, decomposition, and abstraction are known to help LMs and humans with reasoning.",
        "what_is_novel": "The theory unifies contextual abstraction and decomposition as necessary mechanisms for strict logical reasoning in LMs.",
        "classification_explanation": "The theory synthesizes and formalizes two mechanisms (abstraction and decomposition) as jointly necessary, which is a novel integration.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lake et al. (2017) Building Machines That Learn and Think Like People [abstraction and compositionality]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]",
            "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [decomposition and verification]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-604",
    "original_theory_name": "Prompt Decomposition and Iterative Composition Law",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
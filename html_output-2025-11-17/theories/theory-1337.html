<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Iterative Error Correction via Self-Generated Critique - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1337</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1337</p>
                <p><strong>Name:</strong> Iterative Error Correction via Self-Generated Critique</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.</p>
                <p><strong>Description:</strong> This theory posits that language models improve answer quality through a process of self-generated critique, where each reflection step identifies specific errors or weaknesses in the prior answer and proposes targeted corrections. The process is analogous to an internalized peer review, with the model acting as both author and critic, iteratively refining its output.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Self-Generated Critique Identifies Errors (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; reflects_on &#8594; prior answer</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; generates &#8594; explicit critique of prior answer</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Reflection prompts often elicit explicit critiques or error identification from the model. </li>
    <li>Self-Refine and similar methods show that models can point out specific flaws in their own outputs. </li>
    <li>Empirical results show that models can identify logical inconsistencies or factual errors in their own answers. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing self-refinement work, the analogy to peer review and explicit error correction is new.</p>            <p><strong>What Already Exists:</strong> Self-critique and error identification are used in some prompt engineering and self-refinement methods.</p>            <p><strong>What is Novel:</strong> The formalization of this as an internalized peer review process is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Self-critique and correction]</li>
    <li>Lightman et al. (2023) Let's Verify Step by Step [Stepwise verification, but not explicit peer review analogy]</li>
</ul>
            <h3>Statement 1: Iterative Correction Improves Answer Quality (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; generates &#8594; explicit critique of prior answer<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; proposes &#8594; targeted correction</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; produces &#8594; improved answer</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Iterative self-refinement has been shown to improve factual accuracy and logical consistency in model outputs. </li>
    <li>Empirical studies show that models can correct their own errors over multiple reflection steps. </li>
    <li>Reflection and critique cycles can reduce hallucinations and increase answer reliability. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law builds on existing iterative refinement but adds a new analogy and formalization.</p>            <p><strong>What Already Exists:</strong> Iterative refinement and self-correction are established in some prompting methods.</p>            <p><strong>What is Novel:</strong> The explicit framing as a peer review-like process and the focus on targeted correction is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Iterative correction]</li>
    <li>Zelikman et al. (2022) STaR: Bootstrapping Reasoning With Reasoning [Iterative improvement, but not explicit peer review analogy]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a model is prompted to explicitly critique and correct its own answers, answer quality will improve over multiple iterations.</li>
                <li>Tasks with clear error types (e.g., math, logic) will benefit most from iterative self-critique.</li>
                <li>The more specific the critique, the greater the improvement in the next answer.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If models are trained to perform self-critique in a peer review style, they may develop new forms of self-supervision.</li>
                <li>There may be diminishing returns or even negative effects after too many critique-correction cycles.</li>
                <li>Self-critique may sometimes reinforce model biases if the critique is itself flawed.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If explicit self-critique does not lead to improved answers, the theory is challenged.</li>
                <li>If models cannot identify errors in their own outputs, the self-generated critique law is undermined.</li>
                <li>If iterative correction leads to answer degradation, the theory's assumptions are called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where models fail to identify subtle or domain-specific errors. </li>
    <li>Tasks where critique leads to overcorrection or loss of relevant information. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends existing iterative refinement work with a new formalization and analogy.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Self-critique and correction]</li>
    <li>Lightman et al. (2023) Let's Verify Step by Step [Stepwise verification]</li>
    <li>Zelikman et al. (2022) STaR: Bootstrapping Reasoning With Reasoning [Iterative improvement]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Iterative Error Correction via Self-Generated Critique",
    "theory_description": "This theory posits that language models improve answer quality through a process of self-generated critique, where each reflection step identifies specific errors or weaknesses in the prior answer and proposes targeted corrections. The process is analogous to an internalized peer review, with the model acting as both author and critic, iteratively refining its output.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Self-Generated Critique Identifies Errors",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "reflects_on",
                        "object": "prior answer"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "generates",
                        "object": "explicit critique of prior answer"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Reflection prompts often elicit explicit critiques or error identification from the model.",
                        "uuids": []
                    },
                    {
                        "text": "Self-Refine and similar methods show that models can point out specific flaws in their own outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical results show that models can identify logical inconsistencies or factual errors in their own answers.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Self-critique and error identification are used in some prompt engineering and self-refinement methods.",
                    "what_is_novel": "The formalization of this as an internalized peer review process is novel.",
                    "classification_explanation": "While related to existing self-refinement work, the analogy to peer review and explicit error correction is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Self-critique and correction]",
                        "Lightman et al. (2023) Let's Verify Step by Step [Stepwise verification, but not explicit peer review analogy]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Iterative Correction Improves Answer Quality",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "generates",
                        "object": "explicit critique of prior answer"
                    },
                    {
                        "subject": "language model",
                        "relation": "proposes",
                        "object": "targeted correction"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "produces",
                        "object": "improved answer"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Iterative self-refinement has been shown to improve factual accuracy and logical consistency in model outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that models can correct their own errors over multiple reflection steps.",
                        "uuids": []
                    },
                    {
                        "text": "Reflection and critique cycles can reduce hallucinations and increase answer reliability.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Iterative refinement and self-correction are established in some prompting methods.",
                    "what_is_novel": "The explicit framing as a peer review-like process and the focus on targeted correction is novel.",
                    "classification_explanation": "The law builds on existing iterative refinement but adds a new analogy and formalization.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Iterative correction]",
                        "Zelikman et al. (2022) STaR: Bootstrapping Reasoning With Reasoning [Iterative improvement, but not explicit peer review analogy]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a model is prompted to explicitly critique and correct its own answers, answer quality will improve over multiple iterations.",
        "Tasks with clear error types (e.g., math, logic) will benefit most from iterative self-critique.",
        "The more specific the critique, the greater the improvement in the next answer."
    ],
    "new_predictions_unknown": [
        "If models are trained to perform self-critique in a peer review style, they may develop new forms of self-supervision.",
        "There may be diminishing returns or even negative effects after too many critique-correction cycles.",
        "Self-critique may sometimes reinforce model biases if the critique is itself flawed."
    ],
    "negative_experiments": [
        "If explicit self-critique does not lead to improved answers, the theory is challenged.",
        "If models cannot identify errors in their own outputs, the self-generated critique law is undermined.",
        "If iterative correction leads to answer degradation, the theory's assumptions are called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where models fail to identify subtle or domain-specific errors.",
            "uuids": []
        },
        {
            "text": "Tasks where critique leads to overcorrection or loss of relevant information.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that repeated critique can lead to increased verbosity or hedging rather than accuracy.",
            "uuids": []
        },
        {
            "text": "In some cases, models introduce new errors during correction.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with ambiguous or subjective answers may not benefit from self-critique.",
        "Critique may be less effective for very short or factoid questions.",
        "Models with limited training on error identification may not benefit from this process."
    ],
    "existing_theory": {
        "what_already_exists": "Self-refinement and error correction are established in some prompting methods.",
        "what_is_novel": "The explicit analogy to peer review and the formalization of critique-correction cycles is novel.",
        "classification_explanation": "The theory extends existing iterative refinement work with a new formalization and analogy.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Self-critique and correction]",
            "Lightman et al. (2023) Let's Verify Step by Step [Stepwise verification]",
            "Zelikman et al. (2022) STaR: Bootstrapping Reasoning With Reasoning [Iterative improvement]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.",
    "original_theory_id": "theory-617",
    "original_theory_name": "Meta-Skill Acquisition and Task Decomposition Theory of LLM Self-Reflection",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chain-of-Thought Decomposition Enables Efficient Learning of Unlearnable Arithmetic Tasks - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-459</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-459</p>
                <p><strong>Name:</strong> Chain-of-Thought Decomposition Enables Efficient Learning of Unlearnable Arithmetic Tasks</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that for compositional arithmetic tasks that are unlearnable end-to-end by standard sequence models (e.g., multi-digit multiplication, division, or multi-hop parity), providing explicit intermediate-step supervision (such as chain-of-thought, scratchpad, or concatenated sub-task labels) transforms the task into one that is efficiently learnable (i.e., with polynomial sample and gradient complexity). The intermediate supervision decomposes the global computation into single-hop steps, each of which can be learned as a low-degree function of local inputs, and the model composes these steps to solve the overall task. This theory is supported by both formal proofs and empirical results across a range of arithmetic and compositional tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Intermediate Supervision Law for Compositional Arithmetic (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; arithmetic_task &#8594; is_compositional_and_unlearnable_end_to_end &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; model &#8594; is_trained_with &#8594; intermediate_step_supervision</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; learns_task_with &#8594; polynomial_sample_and_gradient_complexity</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Bit-subset parity and multi-digit multiplication/division are unlearnable end-to-end but become learnable with concatenated intermediate labels or explicit CoT decomposition. Theoretical results (Theorem 2, Lemmas 1–3) show polynomial sample/gradient complexity with intermediate supervision. Empirical Transformer experiments confirm that concatenation enables learning of bit-subset parity tasks that fail without it. <a href="../results/extraction-result-3142.html#e3142.0" class="evidence-link">[e3142.0]</a> <a href="../results/extraction-result-3142.html#e3142.2" class="evidence-link">[e3142.2]</a> <a href="../results/extraction-result-3157.html#e3157.4" class="evidence-link">[e3157.4]</a> </li>
    <li>Ablation studies in Goat-7B show that multi-digit multiplication/division are 'unlearnable' without explicit CoT (exact-match near zero), but with full CoT accuracy rises to high levels. Removal of specific CoT steps (especially 'adding term by term') degrades learning markedly. <a href="../results/extraction-result-3157.html#e3157.4" class="evidence-link">[e3157.4]</a> <a href="../results/extraction-result-3157.html#e3157.0" class="evidence-link">[e3157.0]</a> </li>
    <li>Show Your Work: Scratchpads for Intermediate Computation with Language Models and related works demonstrate that scratchpad/supervised intermediate outputs enable models to perform multi-step computations (e.g., 8-digit addition) that are otherwise not learnable at practical scale. <a href="../results/extraction-result-3123.html#e3123.0" class="evidence-link">[e3123.0]</a> <a href="../results/extraction-result-3132.html#e3132.2" class="evidence-link">[e3132.2]</a> </li>
    <li>Parity/coin-flip sequential parity tasks: few-shot scratchpad prompting enables length generalization and correct long-step scratchpads, while vanilla and scratchpad finetuning without such supervision fail OOD. <a href="../results/extraction-result-3147.html#e3147.0" class="evidence-link">[e3147.0]</a> <a href="../results/extraction-result-3147.html#e3147.5" class="evidence-link">[e3147.5]</a> </li>
    <li>Concatenated intermediate supervision (teacher forcing) transforms unlearnable end-to-end tasks into provably learnable ones with polynomial gradient steps and samples; empirically enables Transformer models to learn larger parity instances that fail without supervision. <a href="../results/extraction-result-3142.html#e3142.0" class="evidence-link">[e3142.0]</a> <a href="../results/extraction-result-3142.html#e3142.2" class="evidence-link">[e3142.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Error Accumulation Bound Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; model &#8594; is_trained_with &#8594; intermediate_step_supervision</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; test_error &#8594; is_bounded_by &#8594; sum_of_per_step_training_errors</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Lemma 1 in the referenced work shows test error is bounded by the sum of per-step training errors (union bound). <a href="../results/extraction-result-3142.html#e3142.0" class="evidence-link">[e3142.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Ablation Law: Removal of Intermediate Supervision Restores Intractability (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; arithmetic_task &#8594; is_compositional_and_unlearnable_end_to_end &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; model &#8594; is_trained_without &#8594; intermediate_step_supervision</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; requires &#8594; exponential_sample_and_gradient_complexity</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Without intermediate labels, theoretical negative results (Shalev-Shwartz et al. 2017) show end-to-end gradient descent fails to learn parity efficiently; empirical Transformer fails to learn 32-bit parity without supervision even after >2M steps. <a href="../results/extraction-result-3142.html#e3142.2" class="evidence-link">[e3142.2]</a> </li>
    <li>Ablation studies in Goat-7B and related works show that removing CoT supervision for multi-digit multiplication/division reduces exact-match accuracy to near zero. <a href="../results/extraction-result-3157.html#e3157.4" class="evidence-link">[e3157.4]</a> <a href="../results/extraction-result-3157.html#e3157.0" class="evidence-link">[e3157.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a model is trained on a new compositional arithmetic task (e.g., multi-digit base-16 multiplication) with explicit intermediate-step supervision, it will learn efficiently (i.e., with polynomial sample/gradient complexity), whereas end-to-end training will fail or require exponential resources.</li>
                <li>If intermediate supervision is removed from a model trained on a compositional task, the model's accuracy on that task will drop to near-random, and learning will become intractable.</li>
                <li>If a model is trained with intermediate supervision on a compositional task, test error will be bounded by the sum of per-step training errors, and error will not explode exponentially with task length.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If only a subset of intermediate steps are supervised (not all), the model may still learn efficiently if the unsupervised steps are simple enough, but the minimal required supervision for efficient learning is unknown.</li>
                <li>If the model is trained with noisy or adversarial intermediate labels, learning efficiency may degrade or fail, but the precise threshold of noise tolerance is unknown.</li>
                <li>If a model is trained with intermediate supervision on a task with highly entangled or non-local intermediate steps, the benefit of supervision may be reduced or require more sophisticated decomposition.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a model learns a compositional arithmetic task end-to-end without intermediate supervision and with polynomial sample/gradient complexity, this would challenge the theory.</li>
                <li>If error accumulation at test time exceeds the sum of per-step training errors (i.e., error grows superlinearly or exponentially with task length despite intermediate supervision), this would challenge the error bound law.</li>
                <li>If a model trained with only partial or noisy intermediate supervision achieves the same efficiency as with full, clean supervision, this would challenge the necessity of full intermediate supervision.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks (e.g., simple addition, or tasks with strong architectural inductive bias such as digitwise addition with digit-level tokenization) may be learnable end-to-end without intermediate supervision. <a href="../results/extraction-result-3012.html#e3012.1" class="evidence-link">[e3012.1]</a> <a href="../results/extraction-result-3012.html#e3012.2" class="evidence-link">[e3012.2]</a> <a href="../results/extraction-result-3157.html#e3157.0" class="evidence-link">[e3157.0]</a> <a href="../results/extraction-result-3019.html#e3019.3" class="evidence-link">[e3019.3]</a> <a href="../results/extraction-result-3012.html#e3012.4" class="evidence-link">[e3012.4]</a> <a href="../results/extraction-result-3148.html#e3148.5" class="evidence-link">[e3148.5]</a> <a href="../results/extraction-result-3019.html#e3019.1" class="evidence-link">[e3019.1]</a> <a href="../results/extraction-result-3012.html#e3012.1" class="evidence-link">[e3012.1]</a> </li>
    <li>Specialized architectures (e.g., neural GPUs, Universal Transformers, Compiled Neural Networks) can sometimes learn compositional tasks end-to-end due to their inductive biases. <a href="../results/extraction-result-3126.html#e3126.3" class="evidence-link">[e3126.3]</a> <a href="../results/extraction-result-3126.html#e3126.5" class="evidence-link">[e3126.5]</a> <a href="../results/extraction-result-3026.html#e3026.1" class="evidence-link">[e3026.1]</a> </li>
    <li>Some models (e.g., large LLMs with sufficient scale and pretraining) can sometimes learn compositional tasks with few-shot prompting or in-context learning, though often with limited generalization. <a href="../results/extraction-result-3147.html#e3147.5" class="evidence-link">[e3147.5]</a> <a href="../results/extraction-result-3147.html#e3147.2" class="evidence-link">[e3147.2]</a> <a href="../results/extraction-result-3146.html#e3146.8" class="evidence-link">[e3146.8]</a> <a href="../results/extraction-result-3137.html#e3137.1" class="evidence-link">[e3137.1]</a> <a href="../results/extraction-result-3137.html#e3137.4" class="evidence-link">[e3137.4]</a> <a href="../results/extraction-result-3141.html#e3141.0" class="evidence-link">[e3141.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2022) Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks [Formalizes the theory for parity and compositional tasks]</li>
    <li>Saxton et al. (2019) Analysing Mathematical Reasoning Abilities of Neural Models [Scratchpad/intermediate supervision]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Scratchpad]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [CoT as intermediate supervision]</li>
    <li>Wang et al. (2023) Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks [Ablation of CoT for multi-digit multiplication/division]</li>
    <li>Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Multiple algorithmic solutions, circuit analysis]</li>
    <li>Nye et al. (2021) Emergent Abilities of Large Language Models [Scratchpad finetuning for multi-step computation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Chain-of-Thought Decomposition Enables Efficient Learning of Unlearnable Arithmetic Tasks",
    "theory_description": "This theory posits that for compositional arithmetic tasks that are unlearnable end-to-end by standard sequence models (e.g., multi-digit multiplication, division, or multi-hop parity), providing explicit intermediate-step supervision (such as chain-of-thought, scratchpad, or concatenated sub-task labels) transforms the task into one that is efficiently learnable (i.e., with polynomial sample and gradient complexity). The intermediate supervision decomposes the global computation into single-hop steps, each of which can be learned as a low-degree function of local inputs, and the model composes these steps to solve the overall task. This theory is supported by both formal proofs and empirical results across a range of arithmetic and compositional tasks.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Intermediate Supervision Law for Compositional Arithmetic",
                "if": [
                    {
                        "subject": "arithmetic_task",
                        "relation": "is_compositional_and_unlearnable_end_to_end",
                        "object": "True"
                    },
                    {
                        "subject": "model",
                        "relation": "is_trained_with",
                        "object": "intermediate_step_supervision"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "learns_task_with",
                        "object": "polynomial_sample_and_gradient_complexity"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Bit-subset parity and multi-digit multiplication/division are unlearnable end-to-end but become learnable with concatenated intermediate labels or explicit CoT decomposition. Theoretical results (Theorem 2, Lemmas 1–3) show polynomial sample/gradient complexity with intermediate supervision. Empirical Transformer experiments confirm that concatenation enables learning of bit-subset parity tasks that fail without it.",
                        "uuids": [
                            "e3142.0",
                            "e3142.2",
                            "e3157.4"
                        ]
                    },
                    {
                        "text": "Ablation studies in Goat-7B show that multi-digit multiplication/division are 'unlearnable' without explicit CoT (exact-match near zero), but with full CoT accuracy rises to high levels. Removal of specific CoT steps (especially 'adding term by term') degrades learning markedly.",
                        "uuids": [
                            "e3157.4",
                            "e3157.0"
                        ]
                    },
                    {
                        "text": "Show Your Work: Scratchpads for Intermediate Computation with Language Models and related works demonstrate that scratchpad/supervised intermediate outputs enable models to perform multi-step computations (e.g., 8-digit addition) that are otherwise not learnable at practical scale.",
                        "uuids": [
                            "e3123.0",
                            "e3132.2"
                        ]
                    },
                    {
                        "text": "Parity/coin-flip sequential parity tasks: few-shot scratchpad prompting enables length generalization and correct long-step scratchpads, while vanilla and scratchpad finetuning without such supervision fail OOD.",
                        "uuids": [
                            "e3147.0",
                            "e3147.5"
                        ]
                    },
                    {
                        "text": "Concatenated intermediate supervision (teacher forcing) transforms unlearnable end-to-end tasks into provably learnable ones with polynomial gradient steps and samples; empirically enables Transformer models to learn larger parity instances that fail without supervision.",
                        "uuids": [
                            "e3142.0",
                            "e3142.2"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        },
        {
            "law": {
                "law_name": "Error Accumulation Bound Law",
                "if": [
                    {
                        "subject": "model",
                        "relation": "is_trained_with",
                        "object": "intermediate_step_supervision"
                    }
                ],
                "then": [
                    {
                        "subject": "test_error",
                        "relation": "is_bounded_by",
                        "object": "sum_of_per_step_training_errors"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Lemma 1 in the referenced work shows test error is bounded by the sum of per-step training errors (union bound).",
                        "uuids": [
                            "e3142.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        },
        {
            "law": {
                "law_name": "Ablation Law: Removal of Intermediate Supervision Restores Intractability",
                "if": [
                    {
                        "subject": "arithmetic_task",
                        "relation": "is_compositional_and_unlearnable_end_to_end",
                        "object": "True"
                    },
                    {
                        "subject": "model",
                        "relation": "is_trained_without",
                        "object": "intermediate_step_supervision"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "requires",
                        "object": "exponential_sample_and_gradient_complexity"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Without intermediate labels, theoretical negative results (Shalev-Shwartz et al. 2017) show end-to-end gradient descent fails to learn parity efficiently; empirical Transformer fails to learn 32-bit parity without supervision even after &gt;2M steps.",
                        "uuids": [
                            "e3142.2"
                        ]
                    },
                    {
                        "text": "Ablation studies in Goat-7B and related works show that removing CoT supervision for multi-digit multiplication/division reduces exact-match accuracy to near zero.",
                        "uuids": [
                            "e3157.4",
                            "e3157.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a model is trained on a new compositional arithmetic task (e.g., multi-digit base-16 multiplication) with explicit intermediate-step supervision, it will learn efficiently (i.e., with polynomial sample/gradient complexity), whereas end-to-end training will fail or require exponential resources.",
        "If intermediate supervision is removed from a model trained on a compositional task, the model's accuracy on that task will drop to near-random, and learning will become intractable.",
        "If a model is trained with intermediate supervision on a compositional task, test error will be bounded by the sum of per-step training errors, and error will not explode exponentially with task length."
    ],
    "new_predictions_unknown": [
        "If only a subset of intermediate steps are supervised (not all), the model may still learn efficiently if the unsupervised steps are simple enough, but the minimal required supervision for efficient learning is unknown.",
        "If the model is trained with noisy or adversarial intermediate labels, learning efficiency may degrade or fail, but the precise threshold of noise tolerance is unknown.",
        "If a model is trained with intermediate supervision on a task with highly entangled or non-local intermediate steps, the benefit of supervision may be reduced or require more sophisticated decomposition."
    ],
    "negative_experiments": [
        "If a model learns a compositional arithmetic task end-to-end without intermediate supervision and with polynomial sample/gradient complexity, this would challenge the theory.",
        "If error accumulation at test time exceeds the sum of per-step training errors (i.e., error grows superlinearly or exponentially with task length despite intermediate supervision), this would challenge the error bound law.",
        "If a model trained with only partial or noisy intermediate supervision achieves the same efficiency as with full, clean supervision, this would challenge the necessity of full intermediate supervision."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks (e.g., simple addition, or tasks with strong architectural inductive bias such as digitwise addition with digit-level tokenization) may be learnable end-to-end without intermediate supervision.",
            "uuids": [
                "e3012.1",
                "e3012.2",
                "e3157.0",
                "e3019.3",
                "e3012.4",
                "e3148.5",
                "e3019.1",
                "e3012.1"
            ]
        },
        {
            "text": "Specialized architectures (e.g., neural GPUs, Universal Transformers, Compiled Neural Networks) can sometimes learn compositional tasks end-to-end due to their inductive biases.",
            "uuids": [
                "e3126.3",
                "e3126.5",
                "e3026.1"
            ]
        },
        {
            "text": "Some models (e.g., large LLMs with sufficient scale and pretraining) can sometimes learn compositional tasks with few-shot prompting or in-context learning, though often with limited generalization.",
            "uuids": [
                "e3147.5",
                "e3147.2",
                "e3146.8",
                "e3137.1",
                "e3137.4",
                "e3141.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Certain models with strong inductive biases or explicit architectural support (e.g., Compiled Neural Networks, neural GPUs) can learn compositional tasks end-to-end, which may conflict with the claim that such tasks are always unlearnable without intermediate supervision.",
            "uuids": [
                "e3026.1",
                "e3126.3"
            ]
        }
    ],
    "special_cases": [
        "For tasks where intermediate steps are not easily defined or are themselves complex, the benefit of intermediate supervision may be reduced.",
        "For models with strong architectural priors (e.g., neural GPUs, Compiled Neural Networks), end-to-end learning may be possible for some compositional tasks.",
        "If the model's context window is too short to accommodate all intermediate steps, the benefit of intermediate supervision may be limited by context length.",
        "If the intermediate steps are not aligned with the true algorithmic decomposition, supervision may not yield efficient learning."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Zhang et al. (2022) Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks [Formalizes the theory for parity and compositional tasks]",
            "Saxton et al. (2019) Analysing Mathematical Reasoning Abilities of Neural Models [Scratchpad/intermediate supervision]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Scratchpad]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [CoT as intermediate supervision]",
            "Wang et al. (2023) Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks [Ablation of CoT for multi-digit multiplication/division]",
            "Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Multiple algorithmic solutions, circuit analysis]",
            "Nye et al. (2021) Emergent Abilities of Large Language Models [Scratchpad finetuning for multi-step computation]"
        ]
    },
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Curriculum Necessity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-171</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-171</p>
                <p><strong>Name:</strong> Hierarchical Curriculum Necessity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments, based on the following results.</p>
                <p><strong>Description:</strong> For compositional acquisition of multi-step procedures in interactive text environments, agents require curricula that respect prerequisite dependencies. Learning complex procedures without first mastering their constituent sub-procedures leads to catastrophic failure or extreme sample inefficiency. The effectiveness of a curriculum is proportional to how well it aligns task ordering with the natural dependency structure of the domain. This theory applies across multiple curriculum mechanisms including difficulty ordering, learning-progress tracking, modality progression, and knowledge scaffolding.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Agents learning compositional procedures require exposure to simpler constituent procedures before complex compositions, with failure to do so resulting in 5-100× sample inefficiency or complete learning failure.</li>
                <li>Curriculum effectiveness scales with alignment to domain prerequisite structure: random ordering yields minimal learning (17/107 items), difficulty-ordered yields moderate learning (43/107), and prerequisite-aware ordering yields strong learning (82/107).</li>
                <li>The sample efficiency gain from curriculum learning increases with task compositional depth, ranging from 2× for shallow hierarchies to 25× for deep hierarchies.</li>
                <li>Adaptive curricula that track learning progress outperform fixed curricula by 10-40% by avoiding both premature advancement and redundant practice, but must track both increases and decreases in performance to prevent catastrophic forgetting.</li>
                <li>Curriculum benefits compound across multiple mechanisms: combining difficulty ordering, learning-progress tracking, exploration bonuses, and knowledge scaffolding yields multiplicative rather than additive gains.</li>
                <li>Modality progression (abstract→embodied) provides 7× training speedup and better generalization than direct embodied training.</li>
                <li>The optimal curriculum granularity depends on task structure: fine-grained curricula (more difficulty levels) consistently outperform coarse-grained curricula.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Uniform sampling without curriculum discovers only 17/107 items in Minecraft, while bidirectional learning-progress curriculum discovers 82/107 items (4.8× improvement), demonstrating massive gains from prerequisite-aware ordering. <a href="../results/extraction-result-1601.html#e1601.0" class="evidence-link">[e1601.0]</a> <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> <a href="../results/extraction-result-1590.html#e1590.3" class="evidence-link">[e1590.3]</a> </li>
    <li>Pretraining on simpler levels then fine-tuning on harder levels in Mario yields 466±37.9 mean distance vs 399.7±22.5 from scratch (17% improvement), and unlocks tech-tree milestones 6.4-15.3× faster in Minecraft Voyager. <a href="../results/extraction-result-1597.html#e1597.0" class="evidence-link">[e1597.0]</a> <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>Tiered difficulty curriculum in TextWorld cooking achieves 64% test performance vs 50% for mixed training (28% relative improvement), with largest gains on mid-complexity tiers (Tier5: 64% vs 40%). <a href="../results/extraction-result-1565.html#e1565.0" class="evidence-link">[e1565.0]</a> <a href="../results/extraction-result-1565.html#e1565.1" class="evidence-link">[e1565.1]</a> </li>
    <li>Task-difficulty curriculum in CEC achieves ~51-54% average performance and outperforms baselines by up to 2.8×, with cross-episodic attention being critical (removing it drops Irreversible Path from 38.2% to 3.8%). <a href="../results/extraction-result-1505.html#e1505.0" class="evidence-link">[e1505.0]</a> <a href="../results/extraction-result-1505.html#e1505.1" class="evidence-link">[e1505.1]</a> </li>
    <li>Scheduled task sampling in H-KGA improves performance from 0.57 to 0.76 average score (33% improvement) by adaptively focusing on harder levels based on current performance. <a href="../results/extraction-result-1579.html#e1579.0" class="evidence-link">[e1579.0]</a> <a href="../results/extraction-result-1579.html#e1579.1" class="evidence-link">[e1579.1]</a> </li>
    <li>Planning-guided HRL with option transfer converges in ~70k steps vs 1.8M+ for baselines (25× speedup), demonstrating prerequisite skill reuse across sequential tasks. <a href="../results/extraction-result-1570.html#e1570.0" class="evidence-link">[e1570.0]</a> </li>
    <li>IMGEP with learning-progress curriculum discovers stepping-stone prerequisites and achieves exploration coverage of 57-61% vs 33-36% for random sampling in tool-use tasks. <a href="../results/extraction-result-1613.html#e1613.0" class="evidence-link">[e1613.0]</a> <a href="../results/extraction-result-1613.html#e1613.2" class="evidence-link">[e1613.2]</a> <a href="../results/extraction-result-1613.html#e1613.3" class="evidence-link">[e1613.3]</a> </li>
    <li>KG-DQN parameter transfer from source to target games reduces convergence steps by up to 80% (e.g., 9:05 task: 274.76±21.45 steps with transfer vs 1267.2±7.5 without). <a href="../results/extraction-result-1609.html#e1609.0" class="evidence-link">[e1609.0]</a> <a href="../results/extraction-result-1609.html#e1609.1" class="evidence-link">[e1609.1]</a> <a href="../results/extraction-result-1609.html#e1609.2" class="evidence-link">[e1609.2]</a> </li>
    <li>QA pretraining on oracle traces improves initial reward (e.g., Afflicted: 4.3±1.34 with QA vs lower without) and final performance, providing transferable action-selection priors. <a href="../results/extraction-result-1609.html#e1609.1" class="evidence-link">[e1609.1]</a> <a href="../results/extraction-result-1575.html#e1575.1" class="evidence-link">[e1575.1]</a> </li>
    <li>Dense reward shaping via oracle checkpoints is necessary for convergence in complex games (e.g., Anchorhead reaches 39.9±0.53 with dense rewards vs ~7 without). <a href="../results/extraction-result-1609.html#e1609.3" class="evidence-link">[e1609.3]</a> </li>
    <li>TextWorld pretraining enables 7× faster training and better generalization to embodied tasks (19% seen, 10% unseen success) compared to training from scratch in embodied environments. <a href="../results/extraction-result-1530.html#e1530.0" class="evidence-link">[e1530.0]</a> <a href="../results/extraction-result-1530.html#e1530.1" class="evidence-link">[e1530.1]</a> <a href="../results/extraction-result-1530.html#e1530.2" class="evidence-link">[e1530.2]</a> </li>
    <li>DAgger with annealed expert assistance (100%→1% over 50K episodes) substantially outperforms offline behavior cloning (~30% improvement on seen split). <a href="../results/extraction-result-1530.html#e1530.2" class="evidence-link">[e1530.2]</a> </li>
    <li>Automated curriculum learning with Exp3.S and learning-progress rewards achieves ~2× speedup on RepeatCopy tasks compared to uniform sampling. <a href="../results/extraction-result-1569.html#e1569.0" class="evidence-link">[e1569.0]</a> </li>
    <li>BabyAI pretraining on aligned base levels reduces demonstration requirements by 2-3× (e.g., GoTo: 183k-216k with pretraining vs 341k-409k without). <a href="../results/extraction-result-1513.html#e1513.1" class="evidence-link">[e1513.1]</a> </li>
    <li>STARLING LLM-generated pretraining curriculum achieves 0.72±0.063 normalized score on held-out games and improves early learning on target benchmarks. <a href="../results/extraction-result-1482.html#e1482.0" class="evidence-link">[e1482.0]</a> </li>
    <li>Dynamic exploration bonus that removes already-learned items increases discovered items from 43 to ~70 in Minecraft by preventing distraction toward easy tasks. <a href="../results/extraction-result-1590.html#e1590.1" class="evidence-link">[e1590.1]</a> <a href="../results/extraction-result-1590.html#e1590.2" class="evidence-link">[e1590.2]</a> <a href="../results/extraction-result-1601.html#e1601.2" class="evidence-link">[e1601.2]</a> </li>
    <li>Voyager's GPT-4 automatic curriculum discovers 63 unique items (3.3× more than baselines) and unlocks wooden tools 15.3× faster by adapting to agent state and progress. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>Expertise-based curriculum for imitation learning achieves 100% success on robotic manipulation by ordering demonstrations from novice to expert. <a href="../results/extraction-result-1505.html#e1505.2" class="evidence-link">[e1505.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In any new interactive text domain with hierarchical task structure (e.g., laboratory protocols, software tutorials, legal document processing), curriculum learning will provide 5-50× sample efficiency gains over uniform sampling, with gains scaling with hierarchy depth.</li>
                <li>Automatically inferring prerequisite structure from task descriptions or demonstrations using LLMs will enable effective curriculum generation without manual task ordering, achieving 80-90% of manually-designed curriculum performance.</li>
                <li>Curricula that combine difficulty ordering with bidirectional learning-progress tracking will consistently outperform either strategy alone by 15-30%, with the gap widening for deeper task hierarchies.</li>
                <li>Hybrid curricula that use abstract text-based pretraining followed by embodied fine-tuning will achieve 5-10× sample efficiency gains in any domain with aligned text and embodied representations.</li>
                <li>Dynamic exploration bonuses that remove mastered tasks from the reward set will improve discovered-task counts by 50-100% in any domain with hierarchical dependencies.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether curriculum learning benefits plateau at some level of task complexity (e.g., >10 levels of hierarchy), or continue to scale indefinitely with deeper hierarchies.</li>
                <li>Whether there exists an optimal curriculum granularity (number of difficulty levels) that balances learning speed against curriculum overhead, and whether this optimum is universal or domain-specific.</li>
                <li>Whether curricula designed for one agent architecture (e.g., transformers) transfer effectively to fundamentally different architectures (e.g., graph neural networks), or if curriculum design must be co-optimized with agent architecture.</li>
                <li>Whether curriculum learning can enable zero-shot compositional generalization to novel task combinations, or if it only improves sample efficiency for seen task types.</li>
                <li>Whether the mechanisms by which curricula work (better exploration, more stable gradients, implicit hierarchical representations) can be directly measured and optimized, or if they remain emergent properties.</li>
                <li>Whether curriculum learning benefits persist when scaling to very large pretrained models (e.g., GPT-4 scale), or if sufficient pretraining eliminates the need for curricula.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where uniform sampling matches or exceeds curriculum performance would challenge the necessity claim and suggest the theory only applies to specific task structures.</li>
                <li>Demonstrating that random task ordering performs as well as prerequisite-aware ordering in domains with clear dependency structures would invalidate the alignment principle.</li>
                <li>Showing that curriculum benefits disappear when controlling for total training time (rather than sample efficiency) would suggest curricula only affect learning speed, not ultimate capability.</li>
                <li>Finding that curricula designed by domain experts consistently underperform random curricula would challenge the prerequisite-alignment principle.</li>
                <li>Demonstrating that agents trained with curriculum perform worse on out-of-distribution tasks than agents trained without curriculum would suggest curricula harm generalization.</li>
                <li>Showing that curriculum benefits are entirely explained by increased diversity of training data (rather than ordering) would challenge the core ordering principle.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact mechanisms by which curricula enable compositional generalization remain unclear - whether through better exploration, more stable gradients, implicit hierarchical representations, or some combination. <a href="../results/extraction-result-1622.html#e1622.0" class="evidence-link">[e1622.0]</a> <a href="../results/extraction-result-1622.html#e1622.1" class="evidence-link">[e1622.1]</a> <a href="../results/extraction-result-1622.html#e1622.2" class="evidence-link">[e1622.2]</a> </li>
    <li>Why some curriculum strategies (e.g., unidirectional learning progress) exhibit catastrophic forgetting cycles while others (bidirectional) do not, and what architectural or algorithmic properties prevent forgetting. <a href="../results/extraction-result-1601.html#e1601.3" class="evidence-link">[e1601.3]</a> <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
    <li>Why curriculum benefits vary dramatically across domains (2× in some cases, 25× in others) and what domain properties predict curriculum effectiveness. <a href="../results/extraction-result-1597.html#e1597.0" class="evidence-link">[e1597.0]</a> <a href="../results/extraction-result-1570.html#e1570.0" class="evidence-link">[e1570.0]</a> <a href="../results/extraction-result-1565.html#e1565.0" class="evidence-link">[e1565.0]</a> </li>
    <li>How to automatically determine optimal curriculum granularity and pacing without extensive hyperparameter search. <a href="../results/extraction-result-1505.html#e1505.0" class="evidence-link">[e1505.0]</a> <a href="../results/extraction-result-1505.html#e1505.1" class="evidence-link">[e1505.1]</a> </li>
    <li>Whether curriculum learning provides benefits beyond sample efficiency, such as improved robustness, better worst-case performance, or enhanced interpretability. <a href="../results/extraction-result-1530.html#e1530.0" class="evidence-link">[e1530.0]</a> <a href="../results/extraction-result-1609.html#e1609.0" class="evidence-link">[e1609.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Bengio et al. (2009) Curriculum learning [Foundational work on ordering training examples from easy to hard, establishes basic curriculum learning principles]</li>
    <li>Elman (1993) Learning and development in neural networks: the importance of starting small [Early work on developmental learning showing benefits of starting with simple inputs]</li>
    <li>Narvekar et al. (2020) Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey [Comprehensive survey of curriculum learning in RL, provides taxonomy and framework closely related to this theory]</li>
    <li>Florensa et al. (2017) Reverse Curriculum Generation for Reinforcement Learning [Automatic curriculum generation by starting from goal and working backwards, related to prerequisite-aware ordering]</li>
    <li>Graves et al. (2017) Automated Curriculum Learning for Neural Networks [Adaptive curriculum using learning progress signals, directly related to learning-progress tracking component]</li>
    <li>Portelas et al. (2020) Automatic Curriculum Learning For Deep RL: A Short Survey [Recent survey covering automatic curriculum methods including learning progress and goal generation]</li>
    <li>Colas et al. (2019) Curious: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning [Goal-based curriculum with intrinsic motivation, related to IMGEP and learning-progress approaches]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Curriculum Necessity Theory",
    "theory_description": "For compositional acquisition of multi-step procedures in interactive text environments, agents require curricula that respect prerequisite dependencies. Learning complex procedures without first mastering their constituent sub-procedures leads to catastrophic failure or extreme sample inefficiency. The effectiveness of a curriculum is proportional to how well it aligns task ordering with the natural dependency structure of the domain. This theory applies across multiple curriculum mechanisms including difficulty ordering, learning-progress tracking, modality progression, and knowledge scaffolding.",
    "supporting_evidence": [
        {
            "text": "Uniform sampling without curriculum discovers only 17/107 items in Minecraft, while bidirectional learning-progress curriculum discovers 82/107 items (4.8× improvement), demonstrating massive gains from prerequisite-aware ordering.",
            "uuids": [
                "e1601.0",
                "e1601.4",
                "e1590.0",
                "e1590.3"
            ]
        },
        {
            "text": "Pretraining on simpler levels then fine-tuning on harder levels in Mario yields 466±37.9 mean distance vs 399.7±22.5 from scratch (17% improvement), and unlocks tech-tree milestones 6.4-15.3× faster in Minecraft Voyager.",
            "uuids": [
                "e1597.0",
                "e1498.0"
            ]
        },
        {
            "text": "Tiered difficulty curriculum in TextWorld cooking achieves 64% test performance vs 50% for mixed training (28% relative improvement), with largest gains on mid-complexity tiers (Tier5: 64% vs 40%).",
            "uuids": [
                "e1565.0",
                "e1565.1"
            ]
        },
        {
            "text": "Task-difficulty curriculum in CEC achieves ~51-54% average performance and outperforms baselines by up to 2.8×, with cross-episodic attention being critical (removing it drops Irreversible Path from 38.2% to 3.8%).",
            "uuids": [
                "e1505.0",
                "e1505.1"
            ]
        },
        {
            "text": "Scheduled task sampling in H-KGA improves performance from 0.57 to 0.76 average score (33% improvement) by adaptively focusing on harder levels based on current performance.",
            "uuids": [
                "e1579.0",
                "e1579.1"
            ]
        },
        {
            "text": "Planning-guided HRL with option transfer converges in ~70k steps vs 1.8M+ for baselines (25× speedup), demonstrating prerequisite skill reuse across sequential tasks.",
            "uuids": [
                "e1570.0"
            ]
        },
        {
            "text": "IMGEP with learning-progress curriculum discovers stepping-stone prerequisites and achieves exploration coverage of 57-61% vs 33-36% for random sampling in tool-use tasks.",
            "uuids": [
                "e1613.0",
                "e1613.2",
                "e1613.3"
            ]
        },
        {
            "text": "KG-DQN parameter transfer from source to target games reduces convergence steps by up to 80% (e.g., 9:05 task: 274.76±21.45 steps with transfer vs 1267.2±7.5 without).",
            "uuids": [
                "e1609.0",
                "e1609.1",
                "e1609.2"
            ]
        },
        {
            "text": "QA pretraining on oracle traces improves initial reward (e.g., Afflicted: 4.3±1.34 with QA vs lower without) and final performance, providing transferable action-selection priors.",
            "uuids": [
                "e1609.1",
                "e1575.1"
            ]
        },
        {
            "text": "Dense reward shaping via oracle checkpoints is necessary for convergence in complex games (e.g., Anchorhead reaches 39.9±0.53 with dense rewards vs ~7 without).",
            "uuids": [
                "e1609.3"
            ]
        },
        {
            "text": "TextWorld pretraining enables 7× faster training and better generalization to embodied tasks (19% seen, 10% unseen success) compared to training from scratch in embodied environments.",
            "uuids": [
                "e1530.0",
                "e1530.1",
                "e1530.2"
            ]
        },
        {
            "text": "DAgger with annealed expert assistance (100%→1% over 50K episodes) substantially outperforms offline behavior cloning (~30% improvement on seen split).",
            "uuids": [
                "e1530.2"
            ]
        },
        {
            "text": "Automated curriculum learning with Exp3.S and learning-progress rewards achieves ~2× speedup on RepeatCopy tasks compared to uniform sampling.",
            "uuids": [
                "e1569.0"
            ]
        },
        {
            "text": "BabyAI pretraining on aligned base levels reduces demonstration requirements by 2-3× (e.g., GoTo: 183k-216k with pretraining vs 341k-409k without).",
            "uuids": [
                "e1513.1"
            ]
        },
        {
            "text": "STARLING LLM-generated pretraining curriculum achieves 0.72±0.063 normalized score on held-out games and improves early learning on target benchmarks.",
            "uuids": [
                "e1482.0"
            ]
        },
        {
            "text": "Dynamic exploration bonus that removes already-learned items increases discovered items from 43 to ~70 in Minecraft by preventing distraction toward easy tasks.",
            "uuids": [
                "e1590.1",
                "e1590.2",
                "e1601.2"
            ]
        },
        {
            "text": "Voyager's GPT-4 automatic curriculum discovers 63 unique items (3.3× more than baselines) and unlocks wooden tools 15.3× faster by adapting to agent state and progress.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "Expertise-based curriculum for imitation learning achieves 100% success on robotic manipulation by ordering demonstrations from novice to expert.",
            "uuids": [
                "e1505.2"
            ]
        }
    ],
    "theory_statements": [
        "Agents learning compositional procedures require exposure to simpler constituent procedures before complex compositions, with failure to do so resulting in 5-100× sample inefficiency or complete learning failure.",
        "Curriculum effectiveness scales with alignment to domain prerequisite structure: random ordering yields minimal learning (17/107 items), difficulty-ordered yields moderate learning (43/107), and prerequisite-aware ordering yields strong learning (82/107).",
        "The sample efficiency gain from curriculum learning increases with task compositional depth, ranging from 2× for shallow hierarchies to 25× for deep hierarchies.",
        "Adaptive curricula that track learning progress outperform fixed curricula by 10-40% by avoiding both premature advancement and redundant practice, but must track both increases and decreases in performance to prevent catastrophic forgetting.",
        "Curriculum benefits compound across multiple mechanisms: combining difficulty ordering, learning-progress tracking, exploration bonuses, and knowledge scaffolding yields multiplicative rather than additive gains.",
        "Modality progression (abstract→embodied) provides 7× training speedup and better generalization than direct embodied training.",
        "The optimal curriculum granularity depends on task structure: fine-grained curricula (more difficulty levels) consistently outperform coarse-grained curricula."
    ],
    "new_predictions_likely": [
        "In any new interactive text domain with hierarchical task structure (e.g., laboratory protocols, software tutorials, legal document processing), curriculum learning will provide 5-50× sample efficiency gains over uniform sampling, with gains scaling with hierarchy depth.",
        "Automatically inferring prerequisite structure from task descriptions or demonstrations using LLMs will enable effective curriculum generation without manual task ordering, achieving 80-90% of manually-designed curriculum performance.",
        "Curricula that combine difficulty ordering with bidirectional learning-progress tracking will consistently outperform either strategy alone by 15-30%, with the gap widening for deeper task hierarchies.",
        "Hybrid curricula that use abstract text-based pretraining followed by embodied fine-tuning will achieve 5-10× sample efficiency gains in any domain with aligned text and embodied representations.",
        "Dynamic exploration bonuses that remove mastered tasks from the reward set will improve discovered-task counts by 50-100% in any domain with hierarchical dependencies."
    ],
    "new_predictions_unknown": [
        "Whether curriculum learning benefits plateau at some level of task complexity (e.g., &gt;10 levels of hierarchy), or continue to scale indefinitely with deeper hierarchies.",
        "Whether there exists an optimal curriculum granularity (number of difficulty levels) that balances learning speed against curriculum overhead, and whether this optimum is universal or domain-specific.",
        "Whether curricula designed for one agent architecture (e.g., transformers) transfer effectively to fundamentally different architectures (e.g., graph neural networks), or if curriculum design must be co-optimized with agent architecture.",
        "Whether curriculum learning can enable zero-shot compositional generalization to novel task combinations, or if it only improves sample efficiency for seen task types.",
        "Whether the mechanisms by which curricula work (better exploration, more stable gradients, implicit hierarchical representations) can be directly measured and optimized, or if they remain emergent properties.",
        "Whether curriculum learning benefits persist when scaling to very large pretrained models (e.g., GPT-4 scale), or if sufficient pretraining eliminates the need for curricula."
    ],
    "negative_experiments": [
        "Finding domains where uniform sampling matches or exceeds curriculum performance would challenge the necessity claim and suggest the theory only applies to specific task structures.",
        "Demonstrating that random task ordering performs as well as prerequisite-aware ordering in domains with clear dependency structures would invalidate the alignment principle.",
        "Showing that curriculum benefits disappear when controlling for total training time (rather than sample efficiency) would suggest curricula only affect learning speed, not ultimate capability.",
        "Finding that curricula designed by domain experts consistently underperform random curricula would challenge the prerequisite-alignment principle.",
        "Demonstrating that agents trained with curriculum perform worse on out-of-distribution tasks than agents trained without curriculum would suggest curricula harm generalization.",
        "Showing that curriculum benefits are entirely explained by increased diversity of training data (rather than ordering) would challenge the core ordering principle."
    ],
    "unaccounted_for": [
        {
            "text": "The exact mechanisms by which curricula enable compositional generalization remain unclear - whether through better exploration, more stable gradients, implicit hierarchical representations, or some combination.",
            "uuids": [
                "e1622.0",
                "e1622.1",
                "e1622.2"
            ]
        },
        {
            "text": "Why some curriculum strategies (e.g., unidirectional learning progress) exhibit catastrophic forgetting cycles while others (bidirectional) do not, and what architectural or algorithmic properties prevent forgetting.",
            "uuids": [
                "e1601.3",
                "e1601.4"
            ]
        },
        {
            "text": "Why curriculum benefits vary dramatically across domains (2× in some cases, 25× in others) and what domain properties predict curriculum effectiveness.",
            "uuids": [
                "e1597.0",
                "e1570.0",
                "e1565.0"
            ]
        },
        {
            "text": "How to automatically determine optimal curriculum granularity and pacing without extensive hyperparameter search.",
            "uuids": [
                "e1505.0",
                "e1505.1"
            ]
        },
        {
            "text": "Whether curriculum learning provides benefits beyond sample efficiency, such as improved robustness, better worst-case performance, or enhanced interpretability.",
            "uuids": [
                "e1530.0",
                "e1609.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some experiments show manually designed curricula underperforming automatic curricula (e.g., Voyager's automatic curriculum outperforms manual), suggesting prerequisite knowledge alone is insufficient and adaptation to agent progress is critical.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "Pretraining on GoToObjMaze did not help GoTo target (444k-602k demos vs 341k-409k from scratch), showing not all 'simpler' tasks provide useful prerequisites - task similarity and competency alignment matter more than raw difficulty.",
            "uuids": [
                "e1513.1"
            ]
        },
        {
            "text": "Reflexion failed to improve on WebShop tasks despite iterative self-reflection, suggesting curriculum-like iterative refinement doesn't help when tasks require high diversity and creative exploration rather than error correction.",
            "uuids": [
                "e1496.3"
            ]
        },
        {
            "text": "In some BabyAI experiments, pretraining on misaligned base levels (GoToObjMaze→GoTo) actually harmed performance, requiring more demonstrations than training from scratch.",
            "uuids": [
                "e1513.1"
            ]
        },
        {
            "text": "HYBRID training (75% text, 25% embodied) did not outperform pure text pretraining for generalization, suggesting that mixing modalities during training may not always be beneficial.",
            "uuids": [
                "e1530.1"
            ]
        },
        {
            "text": "Inject augmentation (massively increasing dataset size) was less effective than diverse sampling (balancing support compositions), showing that curriculum quality matters more than quantity.",
            "uuids": [
                "e1622.1",
                "e1622.2"
            ]
        }
    ],
    "special_cases": [
        "Curriculum benefits may be minimal for very simple domains with shallow compositional structure (≤2 levels of hierarchy), where uniform sampling may suffice.",
        "In domains with high stochasticity or partial observability, curriculum ordering may need to account for perceptual difficulty and exploration challenges in addition to compositional complexity.",
        "When using powerful pretrained models (e.g., large language models with extensive prior knowledge), curriculum benefits may be reduced but not eliminated, as the model still needs to learn domain-specific procedures.",
        "Curriculum effectiveness depends critically on task alignment: pretraining on superficially similar but structurally different tasks can harm rather than help performance.",
        "In domains requiring high diversity and creative exploration (e.g., WebShop), curriculum-like approaches may fail because they constrain exploration too much.",
        "Unidirectional learning-progress curricula (tracking only improvements) can lead to catastrophic forgetting cycles, requiring bidirectional tracking (improvements and degradations) for stability.",
        "The interaction between curriculum and exploration bonuses is critical: curricula work best when combined with dynamic exploration bonuses that remove mastered tasks from the reward set.",
        "Modality matters: text-based pretraining provides better generalization to embodied tasks than direct embodied training, but the reverse may not hold.",
        "Curriculum granularity has diminishing returns: performance degrades monotonically as curricula become coarser, but very fine-grained curricula may have overhead costs.",
        "In multi-task settings, level-aware replay buffers are necessary to prevent hard tasks from being systematically excluded due to lower absolute rewards."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Bengio et al. (2009) Curriculum learning [Foundational work on ordering training examples from easy to hard, establishes basic curriculum learning principles]",
            "Elman (1993) Learning and development in neural networks: the importance of starting small [Early work on developmental learning showing benefits of starting with simple inputs]",
            "Narvekar et al. (2020) Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey [Comprehensive survey of curriculum learning in RL, provides taxonomy and framework closely related to this theory]",
            "Florensa et al. (2017) Reverse Curriculum Generation for Reinforcement Learning [Automatic curriculum generation by starting from goal and working backwards, related to prerequisite-aware ordering]",
            "Graves et al. (2017) Automated Curriculum Learning for Neural Networks [Adaptive curriculum using learning progress signals, directly related to learning-progress tracking component]",
            "Portelas et al. (2020) Automatic Curriculum Learning For Deep RL: A Short Survey [Recent survey covering automatic curriculum methods including learning progress and goal generation]",
            "Colas et al. (2019) Curious: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning [Goal-based curriculum with intrinsic motivation, related to IMGEP and learning-progress approaches]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
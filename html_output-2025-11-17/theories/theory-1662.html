<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Epistemic Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1662</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1662</p>
                <p><strong>Name:</strong> Epistemic Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> The accuracy of LLMs as text-based simulators for scientific subdomains is determined by the degree of epistemic alignment between the LLM's internal representations (as shaped by its training data and architecture) and the explicit and implicit knowledge structures of the target subdomain. High alignment enables accurate simulation, while misalignment leads to systematic errors or hallucinations.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Epistemic Structure Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM internal representations &#8594; are_aligned_with &#8594; subdomain knowledge structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; is_high &#8594; for that subdomain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs trained on domain-specific corpora (e.g., biomedical, legal) show higher accuracy in those domains. </li>
    <li>LLMs often fail in subdomains with unique ontologies or implicit conventions not present in training data. </li>
    <li>Domain-adapted LLMs outperform general LLMs on specialized tasks, such as medical question answering. </li>
    <li>Fine-tuning LLMs on subdomain-specific literature increases their factual accuracy and reduces hallucinations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to domain adaptation and transfer learning, the explicit focus on epistemic structure alignment as the key determinant is novel.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that domain-specific pretraining improves LLM performance in those domains.</p>            <p><strong>What is Novel:</strong> This law formalizes the concept of 'epistemic alignment' as a necessary and sufficient condition for simulation accuracy, extending beyond data overlap to include implicit knowledge structures.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation improves performance, but does not formalize epistemic alignment]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [discusses domain adaptation, not epistemic structure alignment]</li>
</ul>
            <h3>Statement 1: Epistemic Misalignment Error Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM internal representations &#8594; are_misaligned_with &#8594; subdomain knowledge structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation &#8594; produces &#8594; systematic errors or hallucinations</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs hallucinate plausible-sounding but incorrect facts in subdomains with sparse or misrepresented training data. </li>
    <li>LLMs struggle with subdomains that use nonstandard terminology or logic (e.g., certain areas of mathematics or philosophy). </li>
    <li>Systematic errors are observed when LLMs are prompted with tasks outside their training distribution. </li>
    <li>LLMs often fail to follow implicit conventions or reasoning patterns unique to a subdomain, leading to errors. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The error phenomenon is known, but the epistemic misalignment framing is novel.</p>            <p><strong>What Already Exists:</strong> LLM hallucination and error in out-of-domain or underrepresented areas is well-documented.</p>            <p><strong>What is Novel:</strong> This law attributes such errors specifically to epistemic misalignment, not just data scarcity or distributional shift.</p>
            <p><strong>References:</strong> <ul>
    <li>Ji et al. (2023) Survey of Hallucination in Natural Language Generation [documents hallucination, not epistemic misalignment]</li>
    <li>Shwartz et al. (2020) Unsupervised Commonsense Question Answering with Self-Talk [shows LLMs' errors in unfamiliar domains]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM is fine-tuned on a subdomain's primary literature and ontologies, its simulation accuracy in that subdomain will increase.</li>
                <li>If a subdomain's implicit conventions are explicitly encoded in the training data, LLM simulation errors in that subdomain will decrease.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LLM is exposed to a synthetic corpus designed to epistemically align with a novel subdomain, it will achieve high simulation accuracy even with minimal real-world data.</li>
                <li>If epistemic alignment is measured quantitatively (e.g., via representational similarity analysis), it will correlate strongly with simulation accuracy across subdomains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an LLM with low epistemic alignment to a subdomain achieves high simulation accuracy, this theory would be called into question.</li>
                <li>If increasing epistemic alignment (via data or architecture) does not improve simulation accuracy, the theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs perform well in subdomains with little explicit training data but high overlap in reasoning patterns. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and extends existing ideas, but the epistemic alignment framing is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation, transfer learning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Epistemic Alignment Theory",
    "theory_description": "The accuracy of LLMs as text-based simulators for scientific subdomains is determined by the degree of epistemic alignment between the LLM's internal representations (as shaped by its training data and architecture) and the explicit and implicit knowledge structures of the target subdomain. High alignment enables accurate simulation, while misalignment leads to systematic errors or hallucinations.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Epistemic Structure Alignment Law",
                "if": [
                    {
                        "subject": "LLM internal representations",
                        "relation": "are_aligned_with",
                        "object": "subdomain knowledge structure"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "is_high",
                        "object": "for that subdomain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs trained on domain-specific corpora (e.g., biomedical, legal) show higher accuracy in those domains.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs often fail in subdomains with unique ontologies or implicit conventions not present in training data.",
                        "uuids": []
                    },
                    {
                        "text": "Domain-adapted LLMs outperform general LLMs on specialized tasks, such as medical question answering.",
                        "uuids": []
                    },
                    {
                        "text": "Fine-tuning LLMs on subdomain-specific literature increases their factual accuracy and reduces hallucinations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that domain-specific pretraining improves LLM performance in those domains.",
                    "what_is_novel": "This law formalizes the concept of 'epistemic alignment' as a necessary and sufficient condition for simulation accuracy, extending beyond data overlap to include implicit knowledge structures.",
                    "classification_explanation": "While related to domain adaptation and transfer learning, the explicit focus on epistemic structure alignment as the key determinant is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation improves performance, but does not formalize epistemic alignment]",
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [discusses domain adaptation, not epistemic structure alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Epistemic Misalignment Error Law",
                "if": [
                    {
                        "subject": "LLM internal representations",
                        "relation": "are_misaligned_with",
                        "object": "subdomain knowledge structure"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation",
                        "relation": "produces",
                        "object": "systematic errors or hallucinations"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs hallucinate plausible-sounding but incorrect facts in subdomains with sparse or misrepresented training data.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs struggle with subdomains that use nonstandard terminology or logic (e.g., certain areas of mathematics or philosophy).",
                        "uuids": []
                    },
                    {
                        "text": "Systematic errors are observed when LLMs are prompted with tasks outside their training distribution.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs often fail to follow implicit conventions or reasoning patterns unique to a subdomain, leading to errors.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLM hallucination and error in out-of-domain or underrepresented areas is well-documented.",
                    "what_is_novel": "This law attributes such errors specifically to epistemic misalignment, not just data scarcity or distributional shift.",
                    "classification_explanation": "The error phenomenon is known, but the epistemic misalignment framing is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Ji et al. (2023) Survey of Hallucination in Natural Language Generation [documents hallucination, not epistemic misalignment]",
                        "Shwartz et al. (2020) Unsupervised Commonsense Question Answering with Self-Talk [shows LLMs' errors in unfamiliar domains]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM is fine-tuned on a subdomain's primary literature and ontologies, its simulation accuracy in that subdomain will increase.",
        "If a subdomain's implicit conventions are explicitly encoded in the training data, LLM simulation errors in that subdomain will decrease."
    ],
    "new_predictions_unknown": [
        "If an LLM is exposed to a synthetic corpus designed to epistemically align with a novel subdomain, it will achieve high simulation accuracy even with minimal real-world data.",
        "If epistemic alignment is measured quantitatively (e.g., via representational similarity analysis), it will correlate strongly with simulation accuracy across subdomains."
    ],
    "negative_experiments": [
        "If an LLM with low epistemic alignment to a subdomain achieves high simulation accuracy, this theory would be called into question.",
        "If increasing epistemic alignment (via data or architecture) does not improve simulation accuracy, the theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs perform well in subdomains with little explicit training data but high overlap in reasoning patterns.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show surprising generalization to subdomains with little or no training data, possibly due to transfer from related domains.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Subdomains with highly formalized, symbolic knowledge (e.g., pure mathematics) may require more than epistemic alignmentâ€”possibly explicit symbolic reasoning modules."
    ],
    "existing_theory": {
        "what_already_exists": "Domain adaptation and transfer learning are established, and the importance of training data overlap is known.",
        "what_is_novel": "The explicit focus on epistemic structure alignment (including implicit conventions and ontologies) as the key determinant of simulation accuracy is novel.",
        "classification_explanation": "The theory synthesizes and extends existing ideas, but the epistemic alignment framing is new.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]",
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation, transfer learning]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-638",
    "original_theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
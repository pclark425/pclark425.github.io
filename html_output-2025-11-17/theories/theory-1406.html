<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Iterative Self-Reflection as Calibration and Bias Amplification in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1406</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1406</p>
                <p><strong>Name:</strong> Iterative Self-Reflection as Calibration and Bias Amplification in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.</p>
                <p><strong>Description:</strong> This theory posits that when language models engage in iterative self-reflection (i.e., generate-then-reflect cycles), the process acts as a dual mechanism: (1) calibrating the model's confidence and answer quality by surfacing and correcting errors, and (2) amplifying or attenuating pre-existing biases depending on the nature of the reflection prompts and the model's internal representations. The interplay between calibration and bias amplification is governed by the structure of the reflection process, the diversity of perspectives considered, and the model's prior knowledge.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Self-Reflection as Confidence Calibration (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; performs &#8594; generate-then-reflect cycle<span style="color: #888888;">, and</span></div>
        <div>&#8226; reflection prompt &#8594; elicits &#8594; error identification or uncertainty</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model's answer confidence &#8594; is updated &#8594; toward better alignment with actual correctness<span style="color: #888888;">, and</span></div>
        <div>&#8226; model's answer quality &#8594; tends to improve &#8594; over successive iterations</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that self-reflection prompts can reduce overconfidence and improve factual accuracy in LLMs. </li>
    <li>Iterative prompting with self-critique leads to higher answer quality in tasks such as math and reasoning. </li>
    <li>Calibration metrics (e.g., Brier score) improve after self-reflection cycles in controlled experiments. </li>
    <li>Self-consistency and self-refinement methods have been shown to increase the likelihood that the model's confidence matches its accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing work on self-consistency and self-critique, the explicit calibration framing and conditional structure is novel.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that self-reflection and self-consistency can improve LLM answer quality and calibration.</p>            <p><strong>What is Novel:</strong> This law frames self-reflection as a systematic calibration process, explicitly linking error identification to confidence adjustment.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Shows iterative self-reflection improves LLM outputs]</li>
    <li>Zelikman et al. (2022) STaR: Self-Taught Reasoner [Demonstrates self-reflection improves reasoning and calibration]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration in LLMs]</li>
</ul>
            <h3>Statement 1: Reflection-Induced Bias Amplification (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; performs &#8594; generate-then-reflect cycle<span style="color: #888888;">, and</span></div>
        <div>&#8226; reflection prompt &#8594; is biased or narrow &#8594; in perspective</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model's output &#8594; amplifies &#8594; pre-existing biases or errors<span style="color: #888888;">, and</span></div>
        <div>&#8226; model's answer diversity &#8594; decreases &#8594; over iterations</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Reflection prompts that reinforce a single perspective can cause models to double down on initial errors or biases. </li>
    <li>Lack of diversity in self-critique can lead to overfitting to initial answers. </li>
    <li>Empirical evidence shows that repeated self-reflection with biased prompts increases the prevalence of those biases in outputs. </li>
    <li>Studies on model-written evaluations show that LLMs can reinforce their own biases through iterative self-assessment. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The application to LLM self-reflection is novel, though the general concept of bias amplification is established.</p>            <p><strong>What Already Exists:</strong> Bias amplification in iterative processes is known in social and algorithmic contexts.</p>            <p><strong>What is Novel:</strong> This law applies the concept to LLM self-reflection, predicting bias amplification as a function of reflection prompt structure.</p>
            <p><strong>References:</strong> <ul>
    <li>Zou & Schiebinger (2018) AI can be sexist and racist — it’s time to make it fair [Bias amplification in AI]</li>
    <li>Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [LLMs can reinforce their own biases]</li>
    <li>Sheng et al. (2019) The Woman Worked as a Babysitter: On Biases in Language Generation [Bias in LLM outputs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a language model is prompted to reflect using diverse, unbiased prompts, its answer calibration and quality will improve more than with narrow, leading prompts.</li>
                <li>Repeated self-reflection cycles with biased prompts will result in increasingly biased outputs, even if the initial answer was less biased.</li>
                <li>Calibration metrics (e.g., Brier score, ECE) will improve with unbiased, error-seeking reflection cycles.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Introducing adversarial or contradictory reflection prompts may destabilize the calibration process, potentially leading to oscillatory or degraded answer quality.</li>
                <li>There may exist a threshold of reflection diversity beyond which calibration benefits plateau or reverse, leading to confusion or reduced answer quality.</li>
                <li>If the model's prior knowledge is highly biased, even unbiased reflection prompts may not fully correct answer bias.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If iterative self-reflection does not improve answer calibration or quality in controlled experiments, the theory's calibration law is called into question.</li>
                <li>If bias does not increase with repeated biased reflection prompts, the bias amplification law is challenged.</li>
                <li>If answer diversity does not decrease with repeated narrow reflection prompts, the bias amplification law is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where self-reflection fails to improve or even degrades answer quality despite unbiased prompts. </li>
    <li>Instances where external feedback or human-in-the-loop correction overrides both calibration and bias amplification effects. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and extends prior work, but the explicit dual-process and conditional framing is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Iterative self-reflection in LLMs]</li>
    <li>Zelikman et al. (2022) STaR: Self-Taught Reasoner [Self-reflection for reasoning]</li>
    <li>Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Bias in LLMs]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration in LLMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Iterative Self-Reflection as Calibration and Bias Amplification in Language Models",
    "theory_description": "This theory posits that when language models engage in iterative self-reflection (i.e., generate-then-reflect cycles), the process acts as a dual mechanism: (1) calibrating the model's confidence and answer quality by surfacing and correcting errors, and (2) amplifying or attenuating pre-existing biases depending on the nature of the reflection prompts and the model's internal representations. The interplay between calibration and bias amplification is governed by the structure of the reflection process, the diversity of perspectives considered, and the model's prior knowledge.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Self-Reflection as Confidence Calibration",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "performs",
                        "object": "generate-then-reflect cycle"
                    },
                    {
                        "subject": "reflection prompt",
                        "relation": "elicits",
                        "object": "error identification or uncertainty"
                    }
                ],
                "then": [
                    {
                        "subject": "model's answer confidence",
                        "relation": "is updated",
                        "object": "toward better alignment with actual correctness"
                    },
                    {
                        "subject": "model's answer quality",
                        "relation": "tends to improve",
                        "object": "over successive iterations"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that self-reflection prompts can reduce overconfidence and improve factual accuracy in LLMs.",
                        "uuids": []
                    },
                    {
                        "text": "Iterative prompting with self-critique leads to higher answer quality in tasks such as math and reasoning.",
                        "uuids": []
                    },
                    {
                        "text": "Calibration metrics (e.g., Brier score) improve after self-reflection cycles in controlled experiments.",
                        "uuids": []
                    },
                    {
                        "text": "Self-consistency and self-refinement methods have been shown to increase the likelihood that the model's confidence matches its accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that self-reflection and self-consistency can improve LLM answer quality and calibration.",
                    "what_is_novel": "This law frames self-reflection as a systematic calibration process, explicitly linking error identification to confidence adjustment.",
                    "classification_explanation": "While related to existing work on self-consistency and self-critique, the explicit calibration framing and conditional structure is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Shows iterative self-reflection improves LLM outputs]",
                        "Zelikman et al. (2022) STaR: Self-Taught Reasoner [Demonstrates self-reflection improves reasoning and calibration]",
                        "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration in LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Reflection-Induced Bias Amplification",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "performs",
                        "object": "generate-then-reflect cycle"
                    },
                    {
                        "subject": "reflection prompt",
                        "relation": "is biased or narrow",
                        "object": "in perspective"
                    }
                ],
                "then": [
                    {
                        "subject": "model's output",
                        "relation": "amplifies",
                        "object": "pre-existing biases or errors"
                    },
                    {
                        "subject": "model's answer diversity",
                        "relation": "decreases",
                        "object": "over iterations"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Reflection prompts that reinforce a single perspective can cause models to double down on initial errors or biases.",
                        "uuids": []
                    },
                    {
                        "text": "Lack of diversity in self-critique can lead to overfitting to initial answers.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical evidence shows that repeated self-reflection with biased prompts increases the prevalence of those biases in outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Studies on model-written evaluations show that LLMs can reinforce their own biases through iterative self-assessment.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Bias amplification in iterative processes is known in social and algorithmic contexts.",
                    "what_is_novel": "This law applies the concept to LLM self-reflection, predicting bias amplification as a function of reflection prompt structure.",
                    "classification_explanation": "The application to LLM self-reflection is novel, though the general concept of bias amplification is established.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zou & Schiebinger (2018) AI can be sexist and racist — it’s time to make it fair [Bias amplification in AI]",
                        "Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [LLMs can reinforce their own biases]",
                        "Sheng et al. (2019) The Woman Worked as a Babysitter: On Biases in Language Generation [Bias in LLM outputs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a language model is prompted to reflect using diverse, unbiased prompts, its answer calibration and quality will improve more than with narrow, leading prompts.",
        "Repeated self-reflection cycles with biased prompts will result in increasingly biased outputs, even if the initial answer was less biased.",
        "Calibration metrics (e.g., Brier score, ECE) will improve with unbiased, error-seeking reflection cycles."
    ],
    "new_predictions_unknown": [
        "Introducing adversarial or contradictory reflection prompts may destabilize the calibration process, potentially leading to oscillatory or degraded answer quality.",
        "There may exist a threshold of reflection diversity beyond which calibration benefits plateau or reverse, leading to confusion or reduced answer quality.",
        "If the model's prior knowledge is highly biased, even unbiased reflection prompts may not fully correct answer bias."
    ],
    "negative_experiments": [
        "If iterative self-reflection does not improve answer calibration or quality in controlled experiments, the theory's calibration law is called into question.",
        "If bias does not increase with repeated biased reflection prompts, the bias amplification law is challenged.",
        "If answer diversity does not decrease with repeated narrow reflection prompts, the bias amplification law is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where self-reflection fails to improve or even degrades answer quality despite unbiased prompts.",
            "uuids": []
        },
        {
            "text": "Instances where external feedback or human-in-the-loop correction overrides both calibration and bias amplification effects.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies report that self-reflection can sometimes introduce new errors or hallucinations, not just amplify existing biases.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Reflection cycles with explicit external feedback may override both calibration and bias amplification effects.",
        "Very short or trivial tasks may not benefit from self-reflection due to lack of error surface.",
        "Tasks with ambiguous or subjective ground truth may not show clear calibration improvement."
    ],
    "existing_theory": {
        "what_already_exists": "Self-reflection and self-consistency have been shown to improve LLM performance; bias amplification is known in other iterative systems.",
        "what_is_novel": "The explicit dual-process framing of self-reflection as both calibration and bias amplification, and the conditional structure relating prompt diversity to these effects.",
        "classification_explanation": "The theory synthesizes and extends prior work, but the explicit dual-process and conditional framing is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Iterative self-reflection in LLMs]",
            "Zelikman et al. (2022) STaR: Self-Taught Reasoner [Self-reflection for reasoning]",
            "Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Bias in LLMs]",
            "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration in LLMs]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.",
    "original_theory_id": "theory-621",
    "original_theory_name": "Self-Reflection as a Calibration and Bias Amplification Process",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Self-Reflection as a Calibration and Bias Amplification Process",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositionality and Locality Theory of Graph-to-Text Representation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1275</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1275</p>
                <p><strong>Name:</strong> Compositionality and Locality Theory of Graph-to-Text Representation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that ideal graph-to-text representations for LM training must support compositionality (the ability to represent subgraphs as composable text fragments) and locality (the preservation of local graph neighborhoods in contiguous text spans). The theory claims that these properties enable LMs to learn and generalize graph patterns, support modular reasoning, and facilitate transfer to unseen graph structures.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Compositionality Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_to_text_representation &#8594; supports &#8594; subgraph_compositionality</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; can_generalize &#8594; to_unseen_graph_patterns</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Representations that allow subgraphs to be mapped to reusable text fragments enable LMs to generalize to novel graph structures. </li>
    <li>Compositional representations in AMR and KG-to-text tasks improve transfer learning and modular reasoning. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is somewhat related to existing work but formalizes a new application in graph-to-text LM training.</p>            <p><strong>What Already Exists:</strong> Compositionality is a known principle in linguistics and semantic parsing.</p>            <p><strong>What is Novel:</strong> The law applies compositionality specifically to graph-to-text representations for LM training.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [compositionality in neural models]</li>
    <li>Song et al. (2018) Graph-to-Sequence Learning using Gated Graph Neural Networks [compositionality in graph-to-sequence]</li>
</ul>
            <h3>Statement 1: Locality Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_to_text_representation &#8594; preserves &#8594; local_graph_neighborhoods_in_text</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; learns &#8594; local_graph_patterns_effectively</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical results show that preserving local neighborhoods in contiguous text spans improves LM learning of local graph motifs and patterns. </li>
    <li>Dispersed or non-local representations hinder LM ability to capture local graph structure. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is somewhat related to existing work but formalizes a new application in graph-to-text LM training.</p>            <p><strong>What Already Exists:</strong> Locality is a known principle in graph neural networks and some sequence models.</p>            <p><strong>What is Novel:</strong> The law applies locality preservation to graph-to-text conversion for LM training.</p>
            <p><strong>References:</strong> <ul>
    <li>Wu et al. (2020) Comprehensive Survey on Graph Neural Networks [locality in GNNs]</li>
    <li>Ribeiro et al. (2020) Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [locality in model robustness]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs trained on compositional and locality-preserving representations will generalize better to novel graph structures.</li>
                <li>Breaking locality (e.g., scattering related nodes/edges in text) will reduce LM performance on local pattern recognition.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Compositional and locality-preserving representations may enable LMs to perform few-shot or zero-shot reasoning on new graph motifs.</li>
                <li>Such representations may allow LMs to transfer learned graph patterns across domains (e.g., from social to biological networks).</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs trained on non-compositional or non-local representations generalize as well as those trained on compositional/local representations, the theory is challenged.</li>
                <li>If locality preservation does not improve LM learning of local graph patterns, the theory's claims are weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of compositionality and locality on global graph reasoning is not addressed. </li>
    <li>Potential trade-offs between locality and sequence length are not considered. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory adapts existing principles to a new context, making it somewhat-related-to-existing.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [compositionality in neural models]</li>
    <li>Wu et al. (2020) Comprehensive Survey on Graph Neural Networks [locality in GNNs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositionality and Locality Theory of Graph-to-Text Representation",
    "theory_description": "This theory asserts that ideal graph-to-text representations for LM training must support compositionality (the ability to represent subgraphs as composable text fragments) and locality (the preservation of local graph neighborhoods in contiguous text spans). The theory claims that these properties enable LMs to learn and generalize graph patterns, support modular reasoning, and facilitate transfer to unseen graph structures.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Compositionality Law",
                "if": [
                    {
                        "subject": "graph_to_text_representation",
                        "relation": "supports",
                        "object": "subgraph_compositionality"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "can_generalize",
                        "object": "to_unseen_graph_patterns"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Representations that allow subgraphs to be mapped to reusable text fragments enable LMs to generalize to novel graph structures.",
                        "uuids": []
                    },
                    {
                        "text": "Compositional representations in AMR and KG-to-text tasks improve transfer learning and modular reasoning.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositionality is a known principle in linguistics and semantic parsing.",
                    "what_is_novel": "The law applies compositionality specifically to graph-to-text representations for LM training.",
                    "classification_explanation": "The law is somewhat related to existing work but formalizes a new application in graph-to-text LM training.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [compositionality in neural models]",
                        "Song et al. (2018) Graph-to-Sequence Learning using Gated Graph Neural Networks [compositionality in graph-to-sequence]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Locality Law",
                "if": [
                    {
                        "subject": "graph_to_text_representation",
                        "relation": "preserves",
                        "object": "local_graph_neighborhoods_in_text"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "learns",
                        "object": "local_graph_patterns_effectively"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical results show that preserving local neighborhoods in contiguous text spans improves LM learning of local graph motifs and patterns.",
                        "uuids": []
                    },
                    {
                        "text": "Dispersed or non-local representations hinder LM ability to capture local graph structure.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Locality is a known principle in graph neural networks and some sequence models.",
                    "what_is_novel": "The law applies locality preservation to graph-to-text conversion for LM training.",
                    "classification_explanation": "The law is somewhat related to existing work but formalizes a new application in graph-to-text LM training.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wu et al. (2020) Comprehensive Survey on Graph Neural Networks [locality in GNNs]",
                        "Ribeiro et al. (2020) Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [locality in model robustness]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs trained on compositional and locality-preserving representations will generalize better to novel graph structures.",
        "Breaking locality (e.g., scattering related nodes/edges in text) will reduce LM performance on local pattern recognition."
    ],
    "new_predictions_unknown": [
        "Compositional and locality-preserving representations may enable LMs to perform few-shot or zero-shot reasoning on new graph motifs.",
        "Such representations may allow LMs to transfer learned graph patterns across domains (e.g., from social to biological networks)."
    ],
    "negative_experiments": [
        "If LMs trained on non-compositional or non-local representations generalize as well as those trained on compositional/local representations, the theory is challenged.",
        "If locality preservation does not improve LM learning of local graph patterns, the theory's claims are weakened."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of compositionality and locality on global graph reasoning is not addressed.",
            "uuids": []
        },
        {
            "text": "Potential trade-offs between locality and sequence length are not considered.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LMs can learn global graph patterns even when locality is not preserved in the text.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with no meaningful subgraph structure may not benefit from compositionality.",
        "For highly entangled graphs, strict locality preservation may be infeasible."
    ],
    "existing_theory": {
        "what_already_exists": "Compositionality and locality are known principles in linguistics and graph neural networks.",
        "what_is_novel": "Their explicit application and necessity in graph-to-text LM training is a new claim.",
        "classification_explanation": "The theory adapts existing principles to a new context, making it somewhat-related-to-existing.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [compositionality in neural models]",
            "Wu et al. (2020) Comprehensive Survey on Graph Neural Networks [locality in GNNs]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-613",
    "original_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
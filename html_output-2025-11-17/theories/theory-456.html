<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Distributed Vector-Space Regression and Modular Decomposition Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-456</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-456</p>
                <p><strong>Name:</strong> Distributed Vector-Space Regression and Modular Decomposition Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that large language models (LLMs) perform arithmetic by encoding numbers and operations as distributed, high-dimensional vector representations in their hidden states. Arithmetic computation is achieved through a combination of (1) vector-space regression (for magnitude approximation) and (2) modular decomposition (for digit-wise or modular arithmetic, such as unit digit and carry), implemented via superposed Fourier-like features. The computation is performed incrementally across layers and tokens, with different modules (e.g., MLPs, attention heads) specializing in different frequency bands or subroutines. This mechanism is not a symbolic algorithm but a learned, distributed approximation that can be causally dissected via probing, ablation, and Fourier analysis. The theory accounts for the observed separation of magnitude and modular components, the role of tokenization and embedding priors, and the layerwise specialization of arithmetic subroutines.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Vector-Space Regression Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; encodes_numbers_in &#8594; continuous_vector_space<span style="color: #888888;">, and</span></div>
        <div>&#8226; arithmetic_task &#8594; requires_magnitude_approximation &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; computes_approximate_result_via &#8594; vector_regression</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ERD hypothesis and probing of LLaMA-2 and Mistral-7B show that numbers are encoded in hidden states and regression can recover magnitude; MLP layers contribute low-frequency (magnitude) Fourier components. <a href="../results/extraction-result-3005.html#e3005.2" class="evidence-link">[e3005.2]</a> <a href="../results/extraction-result-2982.html#e2982.0" class="evidence-link">[e2982.0]</a> <a href="../results/extraction-result-2982.html#e2982.2" class="evidence-link">[e2982.2]</a> <a href="../results/extraction-result-3002.html#e3002.0" class="evidence-link">[e3002.0]</a> <a href="../results/extraction-result-3002.html#e3002.2" class="evidence-link">[e3002.2]</a> </li>
    <li>Amnesic probing and ablation in ERD and Fourier studies show that central decoder embeddings contain necessary value information for arithmetic. <a href="../results/extraction-result-3005.html#e3005.2" class="evidence-link">[e3005.2]</a> <a href="../results/extraction-result-2982.html#e2982.0" class="evidence-link">[e2982.0]</a> <a href="../results/extraction-result-2982.html#e2982.2" class="evidence-link">[e2982.2]</a> </li>
    <li>Layerwise correlation patterns in LLaMA-2 and Mistral-7B show token-like representations in early/late layers and value-like in middle layers. <a href="../results/extraction-result-2982.html#e2982.0" class="evidence-link">[e2982.0]</a> <a href="../results/extraction-result-2982.html#e2982.2" class="evidence-link">[e2982.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Modular Decomposition Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; arithmetic_task &#8594; requires_modular_or_digitwise_computation &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; computes_modular_components_via &#8594; high_frequency_Fourier_features</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Fourier analysis of GPT-2-XL and other models shows high-frequency components (periods 2, 2.5, 5, 10) correspond to modular/digitwise classification, contributed by attention layers. <a href="../results/extraction-result-3002.html#e3002.0" class="evidence-link">[e3002.0]</a> <a href="../results/extraction-result-3002.html#e3002.4" class="evidence-link">[e3002.4]</a> <a href="../results/extraction-result-2982.html#e2982.0" class="evidence-link">[e2982.0]</a> <a href="../results/extraction-result-2982.html#e2982.2" class="evidence-link">[e2982.2]</a> </li>
    <li>Ablation and filtering experiments show that removing high-frequency components from attention layers increases modular/digitwise errors (e.g., unit digit, carry). <a href="../results/extraction-result-3002.html#e3002.4" class="evidence-link">[e3002.4]</a> <a href="../results/extraction-result-3002.html#e3002.0" class="evidence-link">[e3002.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Superposition and Layerwise Specialization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_multiple_layers_and_modules &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; different_layers &#8594; specialize_in &#8594; different_frequency_bands_or_subroutines</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Ablation and filtering experiments show MLPs contribute low-frequency (approximation) and attention contributes high-frequency (modular) components; patching and intervention studies show layerwise specialization. <a href="../results/extraction-result-3002.html#e3002.0" class="evidence-link">[e3002.0]</a> <a href="../results/extraction-result-3002.html#e3002.4" class="evidence-link">[e3002.4]</a> <a href="../results/extraction-result-2982.html#e2982.0" class="evidence-link">[e2982.0]</a> <a href="../results/extraction-result-2982.html#e2982.2" class="evidence-link">[e2982.2]</a> <a href="../results/extraction-result-2997.html#e2997.1" class="evidence-link">[e2997.1]</a> </li>
    <li>Activation patching in LLaMA-2 and Mistral-7B shows token- and layer-specific causal effects, with early layers building token-sequence encodings and mid-late layers using encodings to compute results. <a href="../results/extraction-result-2982.html#e2982.0" class="evidence-link">[e2982.0]</a> <a href="../results/extraction-result-2982.html#e2982.2" class="evidence-link">[e2982.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Tokenization and Embedding Prior Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; uses_digit_or_character_tokenization &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; learns_more_robust_numeric_encodings &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Goat-7B, LLaMA-2, and Mistral-7B with digit-level tokenization achieve higher arithmetic accuracy and more robust internal representations; pre-trained embeddings with Fourier structure enable learning of modular features. <a href="../results/extraction-result-3157.html#e3157.0" class="evidence-link">[e3157.0]</a> <a href="../results/extraction-result-2982.html#e2982.0" class="evidence-link">[e2982.0]</a> <a href="../results/extraction-result-2982.html#e2982.2" class="evidence-link">[e2982.2]</a> <a href="../results/extraction-result-3002.html#e3002.2" class="evidence-link">[e3002.2]</a> <a href="../results/extraction-result-3012.html#e3012.1" class="evidence-link">[e3012.1]</a> <a href="../results/extraction-result-3012.html#e3012.2" class="evidence-link">[e3012.2]</a> </li>
    <li>Ablation in GPT-2-small shows that freezing pre-trained token embeddings with Fourier features enables learning of modular features and high arithmetic accuracy, while random embeddings do not. <a href="../results/extraction-result-3002.html#e3002.2" class="evidence-link">[e3002.2]</a> </li>
    <li>Tokenization & Notation interventions and DigitRNN/DigitCNN evidence show that digit-level tokenization and pooling architectures improve arithmetic and numeric representation. <a href="../results/extraction-result-3148.html#e3148.9" class="evidence-link">[e3148.9]</a> <a href="../results/extraction-result-3148.html#e3148.5" class="evidence-link">[e3148.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a model's MLP layers are ablated or low-frequency Fourier components are removed, magnitude approximation errors (off-by-10, 100, etc.) will increase.</li>
                <li>If a model's attention layers or high-frequency Fourier components are ablated, modular/digitwise errors (unit digit, carry) will increase.</li>
                <li>If a model is trained with subword tokenization, its arithmetic accuracy and probe-ability of numeric representations will be lower than with digit-level tokenization.</li>
                <li>If a model is trained on a new numeric base (e.g., base-12), the frequency bands used for modular decomposition will shift to match the new base.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained with explicit supervision to align specific Fourier components with arithmetic subroutines, it may develop more interpretable and robust arithmetic circuits.</li>
                <li>If a model is trained on non-decimal bases or with non-standard modular arithmetic, the frequency bands used for modular decomposition may shift accordingly.</li>
                <li>If a model is trained with adversarial noise in intermediate representations, it may develop alternative, more robust decomposition strategies.</li>
                <li>If a model is trained with explicit symbolic algorithmic supervision, it may combine distributed regression with symbolic submodules, leading to hybrid mechanisms.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If ablation of MLP or attention layers does not produce the predicted error patterns (magnitude vs modular), this would challenge the modular decomposition law.</li>
                <li>If models with subword tokenization achieve the same probe-ability and arithmetic accuracy as digit-tokenized models, this would challenge the tokenization prior law.</li>
                <li>If Fourier analysis of hidden states does not reveal sparse, interpretable frequency components corresponding to arithmetic subroutines, this would challenge the theory.</li>
                <li>If models with no pre-trained embedding priors can learn modular decomposition as efficiently as those with Fourier-structured embeddings, this would challenge the embedding prior law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some models (e.g., CoNN, compiled neural networks) implement explicit symbolic algorithms rather than distributed vector-space regression, and can achieve perfect accuracy without the same distributed representations. <a href="../results/extraction-result-3026.html#e3026.1" class="evidence-link">[e3026.1]</a> <a href="../results/extraction-result-3026.html#e3026.0" class="evidence-link">[e3026.0]</a> </li>
    <li>External tool-use and program-execution approaches (e.g., PAL, Python interpreter, MathCoder, GPT4-Code) achieve exact arithmetic by delegating computation, bypassing internal distributed representations. <a href="../results/extraction-result-3030.html#e3030.6" class="evidence-link">[e3030.6]</a> <a href="../results/extraction-result-3145.html#e3145.5" class="evidence-link">[e3145.5]</a> <a href="../results/extraction-result-3154.html#e3154.0" class="evidence-link">[e3154.0]</a> <a href="../results/extraction-result-3030.html#e3030.10" class="evidence-link">[e3030.10]</a> <a href="../results/extraction-result-3145.html#e3145.4" class="evidence-link">[e3145.4]</a> <a href="../results/extraction-result-3123.html#e3123.0" class="evidence-link">[e3123.0]</a> </li>
    <li>Explicit stepwise algorithmic supervision (e.g., Chain-of-Thought, Goat-7B, Chain-of-Thought Decomposition) can induce algorithmic arithmetic circuits that may not rely on distributed regression. <a href="../results/extraction-result-3157.html#e3157.0" class="evidence-link">[e3157.0]</a> <a href="../results/extraction-result-3157.html#e3157.4" class="evidence-link">[e3157.4]</a> <a href="../results/extraction-result-3026.html#e3026.4" class="evidence-link">[e3026.4]</a> <a href="../results/extraction-result-3123.html#e3123.0" class="evidence-link">[e3123.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [Fourier decomposition in LLM arithmetic, but this theory generalizes to modular decomposition and distributed regression]</li>
    <li>Zhang et al. (2023) Arithmetic with Language Models: from Memorization to Computation [ERD hypothesis, but this theory extends to modular decomposition and frequency specialization]</li>
    <li>Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Multiple algorithmic solutions, but not focused on distributed regression/Fourier features]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Distributed Vector-Space Regression and Modular Decomposition Theory",
    "theory_description": "This theory posits that large language models (LLMs) perform arithmetic by encoding numbers and operations as distributed, high-dimensional vector representations in their hidden states. Arithmetic computation is achieved through a combination of (1) vector-space regression (for magnitude approximation) and (2) modular decomposition (for digit-wise or modular arithmetic, such as unit digit and carry), implemented via superposed Fourier-like features. The computation is performed incrementally across layers and tokens, with different modules (e.g., MLPs, attention heads) specializing in different frequency bands or subroutines. This mechanism is not a symbolic algorithm but a learned, distributed approximation that can be causally dissected via probing, ablation, and Fourier analysis. The theory accounts for the observed separation of magnitude and modular components, the role of tokenization and embedding priors, and the layerwise specialization of arithmetic subroutines.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Vector-Space Regression Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "encodes_numbers_in",
                        "object": "continuous_vector_space"
                    },
                    {
                        "subject": "arithmetic_task",
                        "relation": "requires_magnitude_approximation",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "computes_approximate_result_via",
                        "object": "vector_regression"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ERD hypothesis and probing of LLaMA-2 and Mistral-7B show that numbers are encoded in hidden states and regression can recover magnitude; MLP layers contribute low-frequency (magnitude) Fourier components.",
                        "uuids": [
                            "e3005.2",
                            "e2982.0",
                            "e2982.2",
                            "e3002.0",
                            "e3002.2"
                        ]
                    },
                    {
                        "text": "Amnesic probing and ablation in ERD and Fourier studies show that central decoder embeddings contain necessary value information for arithmetic.",
                        "uuids": [
                            "e3005.2",
                            "e2982.0",
                            "e2982.2"
                        ]
                    },
                    {
                        "text": "Layerwise correlation patterns in LLaMA-2 and Mistral-7B show token-like representations in early/late layers and value-like in middle layers.",
                        "uuids": [
                            "e2982.0",
                            "e2982.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Modular Decomposition Law",
                "if": [
                    {
                        "subject": "arithmetic_task",
                        "relation": "requires_modular_or_digitwise_computation",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "computes_modular_components_via",
                        "object": "high_frequency_Fourier_features"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Fourier analysis of GPT-2-XL and other models shows high-frequency components (periods 2, 2.5, 5, 10) correspond to modular/digitwise classification, contributed by attention layers.",
                        "uuids": [
                            "e3002.0",
                            "e3002.4",
                            "e2982.0",
                            "e2982.2"
                        ]
                    },
                    {
                        "text": "Ablation and filtering experiments show that removing high-frequency components from attention layers increases modular/digitwise errors (e.g., unit digit, carry).",
                        "uuids": [
                            "e3002.4",
                            "e3002.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Superposition and Layerwise Specialization Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_multiple_layers_and_modules",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "different_layers",
                        "relation": "specialize_in",
                        "object": "different_frequency_bands_or_subroutines"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Ablation and filtering experiments show MLPs contribute low-frequency (approximation) and attention contributes high-frequency (modular) components; patching and intervention studies show layerwise specialization.",
                        "uuids": [
                            "e3002.0",
                            "e3002.4",
                            "e2982.0",
                            "e2982.2",
                            "e2997.1"
                        ]
                    },
                    {
                        "text": "Activation patching in LLaMA-2 and Mistral-7B shows token- and layer-specific causal effects, with early layers building token-sequence encodings and mid-late layers using encodings to compute results.",
                        "uuids": [
                            "e2982.0",
                            "e2982.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Tokenization and Embedding Prior Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "uses_digit_or_character_tokenization",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "learns_more_robust_numeric_encodings",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Goat-7B, LLaMA-2, and Mistral-7B with digit-level tokenization achieve higher arithmetic accuracy and more robust internal representations; pre-trained embeddings with Fourier structure enable learning of modular features.",
                        "uuids": [
                            "e3157.0",
                            "e2982.0",
                            "e2982.2",
                            "e3002.2",
                            "e3012.1",
                            "e3012.2"
                        ]
                    },
                    {
                        "text": "Ablation in GPT-2-small shows that freezing pre-trained token embeddings with Fourier features enables learning of modular features and high arithmetic accuracy, while random embeddings do not.",
                        "uuids": [
                            "e3002.2"
                        ]
                    },
                    {
                        "text": "Tokenization & Notation interventions and DigitRNN/DigitCNN evidence show that digit-level tokenization and pooling architectures improve arithmetic and numeric representation.",
                        "uuids": [
                            "e3148.9",
                            "e3148.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a model's MLP layers are ablated or low-frequency Fourier components are removed, magnitude approximation errors (off-by-10, 100, etc.) will increase.",
        "If a model's attention layers or high-frequency Fourier components are ablated, modular/digitwise errors (unit digit, carry) will increase.",
        "If a model is trained with subword tokenization, its arithmetic accuracy and probe-ability of numeric representations will be lower than with digit-level tokenization.",
        "If a model is trained on a new numeric base (e.g., base-12), the frequency bands used for modular decomposition will shift to match the new base."
    ],
    "new_predictions_unknown": [
        "If a model is trained with explicit supervision to align specific Fourier components with arithmetic subroutines, it may develop more interpretable and robust arithmetic circuits.",
        "If a model is trained on non-decimal bases or with non-standard modular arithmetic, the frequency bands used for modular decomposition may shift accordingly.",
        "If a model is trained with adversarial noise in intermediate representations, it may develop alternative, more robust decomposition strategies.",
        "If a model is trained with explicit symbolic algorithmic supervision, it may combine distributed regression with symbolic submodules, leading to hybrid mechanisms."
    ],
    "negative_experiments": [
        "If ablation of MLP or attention layers does not produce the predicted error patterns (magnitude vs modular), this would challenge the modular decomposition law.",
        "If models with subword tokenization achieve the same probe-ability and arithmetic accuracy as digit-tokenized models, this would challenge the tokenization prior law.",
        "If Fourier analysis of hidden states does not reveal sparse, interpretable frequency components corresponding to arithmetic subroutines, this would challenge the theory.",
        "If models with no pre-trained embedding priors can learn modular decomposition as efficiently as those with Fourier-structured embeddings, this would challenge the embedding prior law."
    ],
    "unaccounted_for": [
        {
            "text": "Some models (e.g., CoNN, compiled neural networks) implement explicit symbolic algorithms rather than distributed vector-space regression, and can achieve perfect accuracy without the same distributed representations.",
            "uuids": [
                "e3026.1",
                "e3026.0"
            ]
        },
        {
            "text": "External tool-use and program-execution approaches (e.g., PAL, Python interpreter, MathCoder, GPT4-Code) achieve exact arithmetic by delegating computation, bypassing internal distributed representations.",
            "uuids": [
                "e3030.6",
                "e3145.5",
                "e3154.0",
                "e3030.10",
                "e3145.4",
                "e3123.0"
            ]
        },
        {
            "text": "Explicit stepwise algorithmic supervision (e.g., Chain-of-Thought, Goat-7B, Chain-of-Thought Decomposition) can induce algorithmic arithmetic circuits that may not rely on distributed regression.",
            "uuids": [
                "e3157.0",
                "e3157.4",
                "e3026.4",
                "e3123.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Goat-7B and similar models can learn algorithmic arithmetic via explicit stepwise supervision, suggesting that distributed regression is not the only possible mechanism.",
            "uuids": [
                "e3157.0",
                "e3157.4"
            ]
        },
        {
            "text": "CoNN modules and Neural Comprehension (NC) can implement deterministic, human-specified algorithms, achieving perfect rule execution for supported symbolic tasks, which is not explained by distributed regression.",
            "uuids": [
                "e3026.1",
                "e3026.0"
            ]
        }
    ],
    "special_cases": [
        "For tasks with very small numbers or trivial modular structure, the frequency decomposition may be less pronounced or unnecessary.",
        "For models with very shallow architectures, the separation of subroutines across layers may not occur.",
        "For models with explicit symbolic modules or external tool use, arithmetic may be performed outside the distributed vector-space mechanism.",
        "For arithmetic tasks with highly atypical or adversarial input distributions, the learned frequency decomposition may not align with the task structure."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [Fourier decomposition in LLM arithmetic, but this theory generalizes to modular decomposition and distributed regression]",
            "Zhang et al. (2023) Arithmetic with Language Models: from Memorization to Computation [ERD hypothesis, but this theory extends to modular decomposition and frequency specialization]",
            "Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Multiple algorithmic solutions, but not focused on distributed regression/Fourier features]"
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Task-Aligned Memory Retrieval Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-443</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-443</p>
                <p><strong>Name:</strong> Task-Aligned Memory Retrieval Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language model agents can best use memory when solving text games, based on the following results.</p>
                <p><strong>Description:</strong> The effectiveness of memory in text-game agents critically depends on the alignment between retrieval strategy and task structure. Retrieval mechanisms must be matched to the specific reasoning patterns required by the task: spatial tasks benefit from graph-based retrieval with connectivity information, temporal tasks from recency-weighted or time-sensitive retrieval, multi-hop reasoning tasks from relevance-based semantic retrieval, and progress-tracking tasks from state-indexed retrieval. Misalignment between retrieval strategy and task demands leads to suboptimal performance regardless of memory content quality. Furthermore, the granularity of retrieval (coarse vs. fine-grained) must match the level of detail required by the task.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Law 0</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; requires &#8594; spatial reasoning and navigation<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; uses &#8594; graph-based retrieval with spatial relations</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves superior performance on &#8594; navigation and spatial tasks<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; outperforms agents using &#8594; flat or sequential retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GraphReader with graph-based retrieval outperformed ReadAgent's page-based retrieval on multi-hop QA, achieving 84.3% vs 72.3% on HotpotQA <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> <a href="../results/extraction-result-2785.html#e2785.1" class="evidence-link">[e2785.1]</a> </li>
    <li>AriGraph with knowledge graph retrieval substantially outperformed full-history memory on navigation tasks (Treasure Hunt 0.87 vs 0.47) <a href="../results/extraction-result-2741.html#e2741.1" class="evidence-link">[e2741.1]</a> <a href="../results/extraction-result-2741.html#e2741.0" class="evidence-link">[e2741.0]</a> </li>
    <li>KG-DQN with graph-based action pruning achieved ~40% faster convergence on spatial TextWorld games <a href="../results/extraction-result-2761.html#e2761.0" class="evidence-link">[e2761.0]</a> </li>
    <li>SHA-KG with hierarchical attention over knowledge graph subgraphs achieved state-of-the-art on 8/20 Jericho games <a href="../results/extraction-result-2774.html#e2774.0" class="evidence-link">[e2774.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Law 1</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; requires &#8594; multi-hop reasoning across distant facts<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; uses &#8594; semantic similarity-based retrieval</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; can successfully retrieve &#8594; relevant facts across multiple reasoning steps<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; outperforms &#8594; recency-only retrieval on multi-hop tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MPRC-DQN with object-centric retrieval achieved 64% winning percentage vs 52% for no-history baseline on Jericho games <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> <a href="../results/extraction-result-2727.html#e2727.1" class="evidence-link">[e2727.1]</a> </li>
    <li>ExpeL with task-similarity retrieval beat reason-similarity and random sampling, achieving 59.0% vs 54.5% on ALFWorld <a href="../results/extraction-result-2778.html#e2778.1" class="evidence-link">[e2778.1]</a> </li>
    <li>GraphReader's rational plan-guided retrieval achieved 90.5% recall on multi-hop questions vs 76.4% for atomic facts alone <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Law 2</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; requires &#8594; tracking temporal sequences or recent events<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; uses &#8594; time-sensitive or recency-weighted retrieval</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; successfully tracks &#8594; temporal dependencies and event sequences<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; outperforms &#8594; time-agnostic retrieval strategies</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MPRC-DQN's object-centric time-sensitive retrieval (most recent K observations sharing objects) improved convergence over pure recency <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> </li>
    <li>Episodic discovery bonus (within-episode counting) substantially improved learning vs cumulative counting across episodes <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
    <li>SWIFT's sliding window of K=10 recent action-observation pairs improved early-step accuracy <a href="../results/extraction-result-2770.html#e2770.1" class="evidence-link">[e2770.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Law 3</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; requires &#8594; tracking progress through subtasks or phases<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; uses &#8594; state-indexed or progress-conditioned retrieval</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; can decompose &#8594; long tasks into manageable phases<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; achieves better &#8594; credit assignment and learning speed</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Score contextualisation using cumulative score to index memory heads improved learning speed and solved levels that baseline failed <a href="../results/extraction-result-2738.html#e2738.1" class="evidence-link">[e2738.1]</a> </li>
    <li>LM-in-the-Loop with state-feature categorized memory (OC) achieved 24.0% vs 20.1% for uncategorized and 2-3x faster convergence <a href="../results/extraction-result-2696.html#e2696.0" class="evidence-link">[e2696.0]</a> </li>
    <li>GITM's sub-goal-indexed memory improved diamond success from 35.0% to 67.5% <a href="../results/extraction-result-2758.html#e2758.0" class="evidence-link">[e2758.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 4: Law 4</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; retrieval strategy &#8594; is misaligned with &#8594; task structure<span style="color: #888888;">, and</span></div>
        <div>&#8226; example &#8594; includes &#8594; using coarse-grained retrieval for detail-dependent tasks or recency-only for distant-fact integration</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent performance &#8594; degrades despite &#8594; high-quality memory content<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; fails to surface &#8594; task-critical information</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Pure recency retrieval in MPRC-DQN provided little benefit early in training and high variance <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> </li>
    <li>ReadAgent's coarse gist retrieval failed on extremely long contexts despite scaling page sizes, attributed to lack of detailed content <a href="../results/extraction-result-2785.html#e2785.1" class="evidence-link">[e2785.1]</a> </li>
    <li>Full-history inclusion without relevance filtering burdened LLM with irrelevant details, achieving only 0.47 vs 0.87 for graph-based retrieval <a href="../results/extraction-result-2741.html#e2741.1" class="evidence-link">[e2741.1]</a> </li>
    <li>LM-in-the-Loop with uncategorized transitions (UT) dropped to 19.1% vs 24.0% for state-feature categorized retrieval <a href="../results/extraction-result-2696.html#e2696.0" class="evidence-link">[e2696.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 5: Law 5</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; retrieval granularity &#8594; matches &#8594; required level of detail for task<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; fine-grained entity-level reasoning</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; fine-grained retrieval &#8594; outperforms &#8594; coarse-grained or aggregated retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GraphReader's atomic facts with chunk-level retrieval achieved 70.0 F1 vs ReadAgent's page-level gist at 62.0 F1 <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> <a href="../results/extraction-result-2785.html#e2785.1" class="evidence-link">[e2785.1]</a> </li>
    <li>SHA-KG's fine-grained relation-aware attention provided better explanations than coarse averaging over graph regions <a href="../results/extraction-result-2774.html#e2774.0" class="evidence-link">[e2774.0]</a> <a href="../results/extraction-result-2742.html#e2742.6" class="evidence-link">[e2742.6]</a> </li>
    <li>MPRC-DQN's object-centric retrieval (selecting observations mentioning shared objects) outperformed treating all history equally <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> <a href="../results/extraction-result-2727.html#e2727.2" class="evidence-link">[e2727.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For tasks requiring causal reasoning about action effects, retrieval weighted by causal proximity (actions that directly caused observed effects) will outperform temporal proximity alone</li>
                <li>In tasks with hierarchical goal structures, retrieval that respects goal-subgoal relationships will outperform flat retrieval by 15-30%</li>
                <li>For dialogue tasks requiring personality consistency, retrieval indexed by character traits will outperform recency-based retrieval</li>
                <li>In exploration tasks, retrieval that prioritizes novel or rarely-visited states will accelerate discovery compared to uniform retrieval</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether learned retrieval strategies using meta-learning can automatically discover optimal retrieval patterns for novel task types without hand-engineering, potentially matching or exceeding hand-crafted strategies</li>
                <li>Whether multi-modal retrieval combining spatial, temporal, and semantic signals provides multiplicative benefits (e.g., 2-3x improvement) or suffers from interference and confusion</li>
                <li>Whether there exist universal retrieval strategies that work well across all text game types or if task-specific tuning is always necessary for optimal performance</li>
                <li>Whether adaptive retrieval strategies that dynamically switch based on detected task phase can outperform fixed strategies by more than 40% on complex multi-phase tasks</li>
                <li>Whether the optimal retrieval strategy changes as agent skill level increases, requiring different strategies for novice vs expert agents</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where random retrieval matches task-aligned retrieval would challenge the core premise of alignment importance</li>
                <li>Demonstrating that a single fixed retrieval strategy works optimally across all task types would question the need for task-specific alignment</li>
                <li>Showing that retrieval strategy has no impact when memory content is sufficiently high-quality would limit the theory's practical importance</li>
                <li>Finding that retrieval strategy matters only for very long tasks but not for medium-length tasks would narrow the theory's scope</li>
                <li>Demonstrating that simple recency-based retrieval always outperforms complex task-aligned strategies would suggest over-engineering</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically detect task structure to select appropriate retrieval strategy without manual specification is not well understood <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> <a href="../results/extraction-result-2741.html#e2741.0" class="evidence-link">[e2741.0]</a> </li>
    <li>The computational costs of different retrieval strategies and their trade-offs with performance gains are not systematically compared across methods <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> </li>
    <li>How retrieval strategies should adapt as tasks become more complex or change structure mid-episode is unclear <a href="../results/extraction-result-2741.html#e2741.1" class="evidence-link">[e2741.1]</a> <a href="../results/extraction-result-2738.html#e2738.1" class="evidence-link">[e2738.1]</a> </li>
    <li>The interaction between retrieval strategy and memory capacity limits is not well characterized - whether better retrieval can compensate for smaller memory <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> </li>
    <li>How retrieval strategies should handle noisy or incorrect memory entries is not addressed <a href="../results/extraction-result-2696.html#e2696.0" class="evidence-link">[e2696.0]</a> <a href="../results/extraction-result-2774.html#e2774.0" class="evidence-link">[e2774.0]</a> </li>
    <li>Whether retrieval strategies need to be different for training vs inference/deployment is unexplored <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
    <li>The role of retrieval strategy in transfer learning across different games or domains is not well understood <a href="../results/extraction-result-2759.html#e2759.0" class="evidence-link">[e2759.0]</a> <a href="../results/extraction-result-2699.html#e2699.0" class="evidence-link">[e2699.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Chen et al. (2024) GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities [Related work on task-aligned graph retrieval for multi-hop reasoning]</li>
    <li>Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [Related work on task-specific reasoning strategies and search]</li>
    <li>Ammanabrolu & Hausknecht (2020) Graph Constrained Reinforcement Learning for Natural Language Action Spaces [Related work on using graph structure for action space constraints]</li>
    <li>Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Related work on episodic memory retrieval for learning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Task-Aligned Memory Retrieval Theory",
    "theory_description": "The effectiveness of memory in text-game agents critically depends on the alignment between retrieval strategy and task structure. Retrieval mechanisms must be matched to the specific reasoning patterns required by the task: spatial tasks benefit from graph-based retrieval with connectivity information, temporal tasks from recency-weighted or time-sensitive retrieval, multi-hop reasoning tasks from relevance-based semantic retrieval, and progress-tracking tasks from state-indexed retrieval. Misalignment between retrieval strategy and task demands leads to suboptimal performance regardless of memory content quality. Furthermore, the granularity of retrieval (coarse vs. fine-grained) must match the level of detail required by the task.",
    "theory_statements": [
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "spatial reasoning and navigation"
                    },
                    {
                        "subject": "memory system",
                        "relation": "uses",
                        "object": "graph-based retrieval with spatial relations"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves superior performance on",
                        "object": "navigation and spatial tasks"
                    },
                    {
                        "subject": "agent",
                        "relation": "outperforms agents using",
                        "object": "flat or sequential retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GraphReader with graph-based retrieval outperformed ReadAgent's page-based retrieval on multi-hop QA, achieving 84.3% vs 72.3% on HotpotQA",
                        "uuids": [
                            "e2785.0",
                            "e2785.1"
                        ]
                    },
                    {
                        "text": "AriGraph with knowledge graph retrieval substantially outperformed full-history memory on navigation tasks (Treasure Hunt 0.87 vs 0.47)",
                        "uuids": [
                            "e2741.1",
                            "e2741.0"
                        ]
                    },
                    {
                        "text": "KG-DQN with graph-based action pruning achieved ~40% faster convergence on spatial TextWorld games",
                        "uuids": [
                            "e2761.0"
                        ]
                    },
                    {
                        "text": "SHA-KG with hierarchical attention over knowledge graph subgraphs achieved state-of-the-art on 8/20 Jericho games",
                        "uuids": [
                            "e2774.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "multi-hop reasoning across distant facts"
                    },
                    {
                        "subject": "memory system",
                        "relation": "uses",
                        "object": "semantic similarity-based retrieval"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "can successfully retrieve",
                        "object": "relevant facts across multiple reasoning steps"
                    },
                    {
                        "subject": "agent",
                        "relation": "outperforms",
                        "object": "recency-only retrieval on multi-hop tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MPRC-DQN with object-centric retrieval achieved 64% winning percentage vs 52% for no-history baseline on Jericho games",
                        "uuids": [
                            "e2727.0",
                            "e2727.1"
                        ]
                    },
                    {
                        "text": "ExpeL with task-similarity retrieval beat reason-similarity and random sampling, achieving 59.0% vs 54.5% on ALFWorld",
                        "uuids": [
                            "e2778.1"
                        ]
                    },
                    {
                        "text": "GraphReader's rational plan-guided retrieval achieved 90.5% recall on multi-hop questions vs 76.4% for atomic facts alone",
                        "uuids": [
                            "e2785.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "tracking temporal sequences or recent events"
                    },
                    {
                        "subject": "memory system",
                        "relation": "uses",
                        "object": "time-sensitive or recency-weighted retrieval"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "successfully tracks",
                        "object": "temporal dependencies and event sequences"
                    },
                    {
                        "subject": "agent",
                        "relation": "outperforms",
                        "object": "time-agnostic retrieval strategies"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MPRC-DQN's object-centric time-sensitive retrieval (most recent K observations sharing objects) improved convergence over pure recency",
                        "uuids": [
                            "e2727.0"
                        ]
                    },
                    {
                        "text": "Episodic discovery bonus (within-episode counting) substantially improved learning vs cumulative counting across episodes",
                        "uuids": [
                            "e2767.2"
                        ]
                    },
                    {
                        "text": "SWIFT's sliding window of K=10 recent action-observation pairs improved early-step accuracy",
                        "uuids": [
                            "e2770.1"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "tracking progress through subtasks or phases"
                    },
                    {
                        "subject": "memory system",
                        "relation": "uses",
                        "object": "state-indexed or progress-conditioned retrieval"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "can decompose",
                        "object": "long tasks into manageable phases"
                    },
                    {
                        "subject": "agent",
                        "relation": "achieves better",
                        "object": "credit assignment and learning speed"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Score contextualisation using cumulative score to index memory heads improved learning speed and solved levels that baseline failed",
                        "uuids": [
                            "e2738.1"
                        ]
                    },
                    {
                        "text": "LM-in-the-Loop with state-feature categorized memory (OC) achieved 24.0% vs 20.1% for uncategorized and 2-3x faster convergence",
                        "uuids": [
                            "e2696.0"
                        ]
                    },
                    {
                        "text": "GITM's sub-goal-indexed memory improved diamond success from 35.0% to 67.5%",
                        "uuids": [
                            "e2758.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "retrieval strategy",
                        "relation": "is misaligned with",
                        "object": "task structure"
                    },
                    {
                        "subject": "example",
                        "relation": "includes",
                        "object": "using coarse-grained retrieval for detail-dependent tasks or recency-only for distant-fact integration"
                    }
                ],
                "then": [
                    {
                        "subject": "agent performance",
                        "relation": "degrades despite",
                        "object": "high-quality memory content"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "fails to surface",
                        "object": "task-critical information"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Pure recency retrieval in MPRC-DQN provided little benefit early in training and high variance",
                        "uuids": [
                            "e2727.0"
                        ]
                    },
                    {
                        "text": "ReadAgent's coarse gist retrieval failed on extremely long contexts despite scaling page sizes, attributed to lack of detailed content",
                        "uuids": [
                            "e2785.1"
                        ]
                    },
                    {
                        "text": "Full-history inclusion without relevance filtering burdened LLM with irrelevant details, achieving only 0.47 vs 0.87 for graph-based retrieval",
                        "uuids": [
                            "e2741.1"
                        ]
                    },
                    {
                        "text": "LM-in-the-Loop with uncategorized transitions (UT) dropped to 19.1% vs 24.0% for state-feature categorized retrieval",
                        "uuids": [
                            "e2696.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "retrieval granularity",
                        "relation": "matches",
                        "object": "required level of detail for task"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "fine-grained entity-level reasoning"
                    }
                ],
                "then": [
                    {
                        "subject": "fine-grained retrieval",
                        "relation": "outperforms",
                        "object": "coarse-grained or aggregated retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GraphReader's atomic facts with chunk-level retrieval achieved 70.0 F1 vs ReadAgent's page-level gist at 62.0 F1",
                        "uuids": [
                            "e2785.0",
                            "e2785.1"
                        ]
                    },
                    {
                        "text": "SHA-KG's fine-grained relation-aware attention provided better explanations than coarse averaging over graph regions",
                        "uuids": [
                            "e2774.0",
                            "e2742.6"
                        ]
                    },
                    {
                        "text": "MPRC-DQN's object-centric retrieval (selecting observations mentioning shared objects) outperformed treating all history equally",
                        "uuids": [
                            "e2727.0",
                            "e2727.2"
                        ]
                    }
                ]
            }
        }
    ],
    "new_predictions_likely": [
        "For tasks requiring causal reasoning about action effects, retrieval weighted by causal proximity (actions that directly caused observed effects) will outperform temporal proximity alone",
        "In tasks with hierarchical goal structures, retrieval that respects goal-subgoal relationships will outperform flat retrieval by 15-30%",
        "For dialogue tasks requiring personality consistency, retrieval indexed by character traits will outperform recency-based retrieval",
        "In exploration tasks, retrieval that prioritizes novel or rarely-visited states will accelerate discovery compared to uniform retrieval"
    ],
    "new_predictions_unknown": [
        "Whether learned retrieval strategies using meta-learning can automatically discover optimal retrieval patterns for novel task types without hand-engineering, potentially matching or exceeding hand-crafted strategies",
        "Whether multi-modal retrieval combining spatial, temporal, and semantic signals provides multiplicative benefits (e.g., 2-3x improvement) or suffers from interference and confusion",
        "Whether there exist universal retrieval strategies that work well across all text game types or if task-specific tuning is always necessary for optimal performance",
        "Whether adaptive retrieval strategies that dynamically switch based on detected task phase can outperform fixed strategies by more than 40% on complex multi-phase tasks",
        "Whether the optimal retrieval strategy changes as agent skill level increases, requiring different strategies for novice vs expert agents"
    ],
    "negative_experiments": [
        "Finding tasks where random retrieval matches task-aligned retrieval would challenge the core premise of alignment importance",
        "Demonstrating that a single fixed retrieval strategy works optimally across all task types would question the need for task-specific alignment",
        "Showing that retrieval strategy has no impact when memory content is sufficiently high-quality would limit the theory's practical importance",
        "Finding that retrieval strategy matters only for very long tasks but not for medium-length tasks would narrow the theory's scope",
        "Demonstrating that simple recency-based retrieval always outperforms complex task-aligned strategies would suggest over-engineering"
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically detect task structure to select appropriate retrieval strategy without manual specification is not well understood",
            "uuids": [
                "e2785.0",
                "e2741.0"
            ]
        },
        {
            "text": "The computational costs of different retrieval strategies and their trade-offs with performance gains are not systematically compared across methods",
            "uuids": [
                "e2727.0",
                "e2785.0"
            ]
        },
        {
            "text": "How retrieval strategies should adapt as tasks become more complex or change structure mid-episode is unclear",
            "uuids": [
                "e2741.1",
                "e2738.1"
            ]
        },
        {
            "text": "The interaction between retrieval strategy and memory capacity limits is not well characterized - whether better retrieval can compensate for smaller memory",
            "uuids": [
                "e2785.0",
                "e2727.0"
            ]
        },
        {
            "text": "How retrieval strategies should handle noisy or incorrect memory entries is not addressed",
            "uuids": [
                "e2696.0",
                "e2774.0"
            ]
        },
        {
            "text": "Whether retrieval strategies need to be different for training vs inference/deployment is unexplored",
            "uuids": [
                "e2767.2"
            ]
        },
        {
            "text": "The role of retrieval strategy in transfer learning across different games or domains is not well understood",
            "uuids": [
                "e2759.0",
                "e2699.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some simple tasks show no performance difference between retrieval strategies - episodic bonus disabled at test time still worked",
            "uuids": [
                "e2767.2"
            ]
        },
        {
            "text": "Very large context windows (128k-256k) sometimes eliminate the need for selective retrieval in some QA tasks",
            "uuids": [
                "e2739.0",
                "e2785.0"
            ]
        },
        {
            "text": "In some cases, simple LSTM implicit memory performed comparably to explicit graph-based retrieval on final performance metrics",
            "uuids": [
                "e2761.1",
                "e2761.0"
            ]
        },
        {
            "text": "ChatGPT with simple previous-action reminders (basic recency) achieved reasonable performance (score 40) without sophisticated retrieval",
            "uuids": [
                "e2704.0"
            ]
        },
        {
            "text": "SSAQN with minimal history (single previous command) achieved near-optimal rewards on some deterministic games",
            "uuids": [
                "e2702.0"
            ]
        }
    ],
    "special_cases": [
        "In fully observable environments with complete information at each step, retrieval strategy matters less as all relevant information is immediately available",
        "For very short tasks (&lt; 10 steps), any reasonable retrieval strategy may suffice as memory size is small and most information remains relevant",
        "When memory size is very small (&lt; 5 entries), retrieval strategy has minimal impact as nearly all memory is retrieved regardless of strategy",
        "In deterministic environments with perfect state representation, simpler retrieval strategies may match complex ones",
        "For tasks where the optimal policy is memoryless (Markovian), retrieval strategy is irrelevant",
        "In environments with dense rewards providing immediate feedback, recency-based retrieval may be sufficient even for complex tasks",
        "When using very large language models with extensive context windows (&gt; 100k tokens), the need for selective retrieval diminishes",
        "For tasks with very high branching factors, retrieval strategy may be dominated by exploration strategy effectiveness"
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Chen et al. (2024) GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities [Related work on task-aligned graph retrieval for multi-hop reasoning]",
            "Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [Related work on task-specific reasoning strategies and search]",
            "Ammanabrolu & Hausknecht (2020) Graph Constrained Reinforcement Learning for Natural Language Action Spaces [Related work on using graph structure for action space constraints]",
            "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Related work on episodic memory retrieval for learning]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
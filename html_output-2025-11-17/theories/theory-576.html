<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fourier-Modular Decomposition Theory of LLM Arithmetic - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-576</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-576</p>
                <p><strong>Name:</strong> Fourier-Modular Decomposition Theory of LLM Arithmetic</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> Language models perform arithmetic by representing numbers as distributed superpositions of Fourier basis functions (sines and cosines at specific frequencies) in their embedding and activation spaces. Arithmetic operations, such as addition, are implemented by manipulating the phases and amplitudes of these Fourier components, with low-frequency components encoding coarse magnitude and high-frequency components encoding modular (e.g., unit-digit) information. The final output is produced by constructive interference of these components, enabling precise localization of the correct answer. This mechanism is not hardcoded but emerges from pretraining on natural language and is further refined by fine-tuning or in-context learning. The theory is best characterized for single-step integer addition, but evidence suggests it may underlie other arithmetic operations in LLMs.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Fourier Feature Encoding of Numbers (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; number token &#8594; is embedded &#8594; pretrained LLM embedding space</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; embedding &#8594; contains &#8594; sparse set of Fourier basis components (specific frequencies/periods)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Token embeddings and intermediate logits in pre-trained LLMs exhibit sparsity in the discrete Fourier basis over the number token space, with prominent peaks at periods 2, 2.5, 5, 10. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>DFT of intermediate logits reveals sparse large-magnitude components at specific periods. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Token embeddings of pre-trained models contain matching Fourier peaks, and transferring pretrained embeddings to scratch-trained models rescues performance. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Models trained from scratch without pretraining lack these Fourier peaks and perform worse on arithmetic. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While sinusoidal encodings are standard, the discovery that LLMs use sparse Fourier features for arithmetic, with modular and magnitude decomposition, is new and not previously formalized.</p>            <p><strong>What Already Exists:</strong> Fourier analysis of neural representations is a known tool, and sinusoidal positional encodings are standard in transformers.</p>            <p><strong>What is Novel:</strong> The explicit identification of arithmetic computation as a superposition and manipulation of sparse Fourier components in number token embeddings and activations, with distinct roles for low- and high-frequency components, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Vaswani et al. (2017) Attention is All You Need [positional encodings, but not arithmetic mechanism]</li>
    <li>Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits for modular addition, but not general LLM arithmetic]</li>
    <li>Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [direct evidence for this law]</li>
</ul>
            <h3>Statement 1: Arithmetic as Phase Manipulation in Fourier Space (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; arithmetic operation &#8594; is performed &#8594; in LLM residual stream<span style="color: #888888;">, and</span></div>
        <div>&#8226; MLP module &#8594; outputs &#8594; low-frequency Fourier components<span style="color: #888888;">, and</span></div>
        <div>&#8226; attention module &#8594; outputs &#8594; high-frequency Fourier components</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; final logits &#8594; are computed as &#8594; linear superpositions of Fourier basis functions whose phases and amplitudes align to locate the correct integer</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MLPs mainly contribute low-frequency components approximating magnitude; attention primarily contributes high-frequency components computing modular residues (mod 2,5,10,...); final logits are linear superpositions of a small set of Fourier basis functions. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Causal ablations via projection-based low/high-pass filters on module outputs produce failure modes consistent with the hypothesized roles of low/high frequencies. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Removing high-frequency components from attention outputs causes systematic modular (unit-digit) errors; removing low-frequency components from MLP outputs causes magnitude errors (off-by-10, 50, 100). <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Inverse-DFT reconstructions using only the top Fourier components yield a peaked distribution at the correct answer. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Prior work on modular addition circuits in small models exists, but the distributed, phase-based mechanism in large LLMs is a new synthesis.</p>            <p><strong>What Already Exists:</strong> Fourier-based mechanisms for modular arithmetic in small transformers have been described.</p>            <p><strong>What is Novel:</strong> The generalization to large LLMs and the explicit division of labor between MLP (magnitude) and attention (modular) components, and the use of phase alignment for arithmetic, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits for modular addition]</li>
    <li>Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [direct evidence for this law]</li>
</ul>
            <h3>Statement 2: Pretraining Induces Fourier Structure Necessary for Arithmetic (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is pretrained &#8594; on natural language with numeric content</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; number token embeddings &#8594; contain &#8594; Fourier peaks at key frequencies<span style="color: #888888;">, and</span></div>
        <div>&#8226; fine-tuning or in-context learning &#8594; can leverage &#8594; these features for arithmetic</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Token embeddings of pre-trained models contain matching Fourier peaks, and transferring pretrained embeddings to scratch-trained models rescues performance. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Models trained from scratch can reach non-trivial accuracy using primarily low-frequency approximation, but lack high-precision arithmetic, indicating the necessity of pretraining-induced Fourier structure for exact computation. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends the general understanding of pretraining by specifying the nature of the features (Fourier) that are critical for arithmetic.</p>            <p><strong>What Already Exists:</strong> Pretraining is known to provide useful inductive biases for downstream tasks.</p>            <p><strong>What is Novel:</strong> The identification of specific Fourier structure as the key pretraining artifact enabling arithmetic is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [direct evidence for this law]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If high-frequency Fourier components are ablated from the attention outputs in a pretrained LLM, the model will make systematic modular (unit-digit) errors in addition tasks.</li>
                <li>If a model is trained from scratch without pre-existing Fourier structure in its embeddings, it will fail to achieve high-precision arithmetic, even with extensive fine-tuning.</li>
                <li>If only low-frequency components are retained in the MLP outputs, the model will approximate the magnitude of sums but fail to localize the exact answer, leading to off-by-10 or off-by-100 errors.</li>
                <li>If pretrained embeddings with strong Fourier peaks are transferred to a scratch-trained model, arithmetic performance will be rescued compared to a model with random embeddings.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained on a language with a different numeric base (e.g., base-12), the learned Fourier components will shift to match the new modular structure, and arithmetic will be implemented via different frequency peaks.</li>
                <li>If a model is exposed to arithmetic tasks requiring non-integer or irrational results, it may develop new, non-integer-aligned Fourier components or alternative distributed representations.</li>
                <li>If a model is trained on arithmetic tasks with adversarially perturbed number token distributions (e.g., shuffled token-to-number mappings), the Fourier structure may fail to emerge, and arithmetic performance will degrade unpredictably.</li>
                <li>If a model is fine-tuned on multi-step arithmetic (e.g., multi-digit multiplication), new higher-frequency or composite Fourier components may emerge to support more complex modular decompositions.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If ablation of all Fourier components from the residual stream does not degrade arithmetic performance, this theory would be called into question.</li>
                <li>If a model with no pretraining (random embeddings) can achieve near-perfect arithmetic after fine-tuning, the necessity of pretraining-induced Fourier structure would be challenged.</li>
                <li>If models can perform arithmetic with no observable modular or magnitude decomposition in their activations, the phase-based mechanism would be undermined.</li>
                <li>If models with no observable Fourier peaks in their embeddings or activations can perform high-precision addition, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The mechanism for multi-step, multi-operation arithmetic (e.g., multi-digit multiplication, division, or algebraic manipulation) is not fully explained by the Fourier-modular decomposition, which is best characterized for single-step addition. <a href="../results/extraction-result-4720.html#e4720.2" class="evidence-link">[e4720.2]</a> <a href="../results/extraction-result-4720.html#e4720.5" class="evidence-link">[e4720.5]</a> <a href="../results/extraction-result-4628.html#e4628.4" class="evidence-link">[e4628.4]</a> <a href="../results/extraction-result-4730.html#e4730.0" class="evidence-link">[e4730.0]</a> <a href="../results/extraction-result-4730.html#e4730.1" class="evidence-link">[e4730.1]</a> <a href="../results/extraction-result-4730.html#e4730.3" class="evidence-link">[e4730.3]</a> <a href="../results/extraction-result-4627.html#e4627.5" class="evidence-link">[e4627.5]</a> <a href="../results/extraction-result-4627.html#e4627.2" class="evidence-link">[e4627.2]</a> <a href="../results/extraction-result-4627.html#e4627.3" class="evidence-link">[e4627.3]</a> </li>
    <li>The role of chain-of-thought prompting and explicit intermediate reasoning steps in arithmetic is not directly addressed by this theory. <a href="../results/extraction-result-4741.html#e4741.0" class="evidence-link">[e4741.0]</a> <a href="../results/extraction-result-4730.html#e4730.0" class="evidence-link">[e4730.0]</a> <a href="../results/extraction-result-4721.html#e4721.0" class="evidence-link">[e4721.0]</a> <a href="../results/extraction-result-4724.html#e4724.2" class="evidence-link">[e4724.2]</a> <a href="../results/extraction-result-4720.html#e4720.2" class="evidence-link">[e4720.2]</a> <a href="../results/extraction-result-4720.html#e4720.5" class="evidence-link">[e4720.5]</a> <a href="../results/extraction-result-4626.html#e4626.0" class="evidence-link">[e4626.0]</a> <a href="../results/extraction-result-4626.html#e4626.3" class="evidence-link">[e4626.3]</a> <a href="../results/extraction-result-4626.html#e4626.2" class="evidence-link">[e4626.2]</a> </li>
    <li>Arithmetic performance in models with different tokenization schemes (e.g., digit-level vs subword) is not fully explained by the Fourier mechanism alone. <a href="../results/extraction-result-4734.html#e4734.5" class="evidence-link">[e4734.5]</a> <a href="../results/extraction-result-4720.html#e4720.0" class="evidence-link">[e4720.0]</a> <a href="../results/extraction-result-4620.html#e4620.0" class="evidence-link">[e4620.0]</a> <a href="../results/extraction-result-4620.html#e4620.1" class="evidence-link">[e4620.1]</a> <a href="../results/extraction-result-4620.html#e4620.2" class="evidence-link">[e4620.2]</a> <a href="../results/extraction-result-4734.html#e4734.6" class="evidence-link">[e4734.6]</a> </li>
    <li>The theory does not account for models that perform arithmetic by program synthesis and external execution (e.g., Codex, PoT, MathPrompter). <a href="../results/extraction-result-4707.html#e4707.2" class="evidence-link">[e4707.2]</a> <a href="../results/extraction-result-4707.html#e4707.1" class="evidence-link">[e4707.1]</a> <a href="../results/extraction-result-4730.html#e4730.1" class="evidence-link">[e4730.1]</a> <a href="../results/extraction-result-4706.html#e4706.0" class="evidence-link">[e4706.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This theory synthesizes and extends prior work on modular addition circuits and positional encodings, providing a new, unified account of arithmetic in LLMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits for modular addition]</li>
    <li>Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [direct evidence for this theory]</li>
    <li>Vaswani et al. (2017) Attention is All You Need [sinusoidal positional encodings, not arithmetic mechanism]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Fourier-Modular Decomposition Theory of LLM Arithmetic",
    "theory_description": "Language models perform arithmetic by representing numbers as distributed superpositions of Fourier basis functions (sines and cosines at specific frequencies) in their embedding and activation spaces. Arithmetic operations, such as addition, are implemented by manipulating the phases and amplitudes of these Fourier components, with low-frequency components encoding coarse magnitude and high-frequency components encoding modular (e.g., unit-digit) information. The final output is produced by constructive interference of these components, enabling precise localization of the correct answer. This mechanism is not hardcoded but emerges from pretraining on natural language and is further refined by fine-tuning or in-context learning. The theory is best characterized for single-step integer addition, but evidence suggests it may underlie other arithmetic operations in LLMs.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Fourier Feature Encoding of Numbers",
                "if": [
                    {
                        "subject": "number token",
                        "relation": "is embedded",
                        "object": "pretrained LLM embedding space"
                    }
                ],
                "then": [
                    {
                        "subject": "embedding",
                        "relation": "contains",
                        "object": "sparse set of Fourier basis components (specific frequencies/periods)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Token embeddings and intermediate logits in pre-trained LLMs exhibit sparsity in the discrete Fourier basis over the number token space, with prominent peaks at periods 2, 2.5, 5, 10.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "DFT of intermediate logits reveals sparse large-magnitude components at specific periods.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Token embeddings of pre-trained models contain matching Fourier peaks, and transferring pretrained embeddings to scratch-trained models rescues performance.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Models trained from scratch without pretraining lack these Fourier peaks and perform worse on arithmetic.",
                        "uuids": [
                            "e4629.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Fourier analysis of neural representations is a known tool, and sinusoidal positional encodings are standard in transformers.",
                    "what_is_novel": "The explicit identification of arithmetic computation as a superposition and manipulation of sparse Fourier components in number token embeddings and activations, with distinct roles for low- and high-frequency components, is novel.",
                    "classification_explanation": "While sinusoidal encodings are standard, the discovery that LLMs use sparse Fourier features for arithmetic, with modular and magnitude decomposition, is new and not previously formalized.",
                    "likely_classification": "new",
                    "references": [
                        "Vaswani et al. (2017) Attention is All You Need [positional encodings, but not arithmetic mechanism]",
                        "Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits for modular addition, but not general LLM arithmetic]",
                        "Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [direct evidence for this law]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Arithmetic as Phase Manipulation in Fourier Space",
                "if": [
                    {
                        "subject": "arithmetic operation",
                        "relation": "is performed",
                        "object": "in LLM residual stream"
                    },
                    {
                        "subject": "MLP module",
                        "relation": "outputs",
                        "object": "low-frequency Fourier components"
                    },
                    {
                        "subject": "attention module",
                        "relation": "outputs",
                        "object": "high-frequency Fourier components"
                    }
                ],
                "then": [
                    {
                        "subject": "final logits",
                        "relation": "are computed as",
                        "object": "linear superpositions of Fourier basis functions whose phases and amplitudes align to locate the correct integer"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MLPs mainly contribute low-frequency components approximating magnitude; attention primarily contributes high-frequency components computing modular residues (mod 2,5,10,...); final logits are linear superpositions of a small set of Fourier basis functions.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Causal ablations via projection-based low/high-pass filters on module outputs produce failure modes consistent with the hypothesized roles of low/high frequencies.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Removing high-frequency components from attention outputs causes systematic modular (unit-digit) errors; removing low-frequency components from MLP outputs causes magnitude errors (off-by-10, 50, 100).",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Inverse-DFT reconstructions using only the top Fourier components yield a peaked distribution at the correct answer.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Fourier-based mechanisms for modular arithmetic in small transformers have been described.",
                    "what_is_novel": "The generalization to large LLMs and the explicit division of labor between MLP (magnitude) and attention (modular) components, and the use of phase alignment for arithmetic, is novel.",
                    "classification_explanation": "Prior work on modular addition circuits in small models exists, but the distributed, phase-based mechanism in large LLMs is a new synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits for modular addition]",
                        "Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [direct evidence for this law]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Pretraining Induces Fourier Structure Necessary for Arithmetic",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is pretrained",
                        "object": "on natural language with numeric content"
                    }
                ],
                "then": [
                    {
                        "subject": "number token embeddings",
                        "relation": "contain",
                        "object": "Fourier peaks at key frequencies"
                    },
                    {
                        "subject": "fine-tuning or in-context learning",
                        "relation": "can leverage",
                        "object": "these features for arithmetic"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Token embeddings of pre-trained models contain matching Fourier peaks, and transferring pretrained embeddings to scratch-trained models rescues performance.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Models trained from scratch can reach non-trivial accuracy using primarily low-frequency approximation, but lack high-precision arithmetic, indicating the necessity of pretraining-induced Fourier structure for exact computation.",
                        "uuids": [
                            "e4629.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Pretraining is known to provide useful inductive biases for downstream tasks.",
                    "what_is_novel": "The identification of specific Fourier structure as the key pretraining artifact enabling arithmetic is new.",
                    "classification_explanation": "This law extends the general understanding of pretraining by specifying the nature of the features (Fourier) that are critical for arithmetic.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [direct evidence for this law]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If high-frequency Fourier components are ablated from the attention outputs in a pretrained LLM, the model will make systematic modular (unit-digit) errors in addition tasks.",
        "If a model is trained from scratch without pre-existing Fourier structure in its embeddings, it will fail to achieve high-precision arithmetic, even with extensive fine-tuning.",
        "If only low-frequency components are retained in the MLP outputs, the model will approximate the magnitude of sums but fail to localize the exact answer, leading to off-by-10 or off-by-100 errors.",
        "If pretrained embeddings with strong Fourier peaks are transferred to a scratch-trained model, arithmetic performance will be rescued compared to a model with random embeddings."
    ],
    "new_predictions_unknown": [
        "If a model is trained on a language with a different numeric base (e.g., base-12), the learned Fourier components will shift to match the new modular structure, and arithmetic will be implemented via different frequency peaks.",
        "If a model is exposed to arithmetic tasks requiring non-integer or irrational results, it may develop new, non-integer-aligned Fourier components or alternative distributed representations.",
        "If a model is trained on arithmetic tasks with adversarially perturbed number token distributions (e.g., shuffled token-to-number mappings), the Fourier structure may fail to emerge, and arithmetic performance will degrade unpredictably.",
        "If a model is fine-tuned on multi-step arithmetic (e.g., multi-digit multiplication), new higher-frequency or composite Fourier components may emerge to support more complex modular decompositions."
    ],
    "negative_experiments": [
        "If ablation of all Fourier components from the residual stream does not degrade arithmetic performance, this theory would be called into question.",
        "If a model with no pretraining (random embeddings) can achieve near-perfect arithmetic after fine-tuning, the necessity of pretraining-induced Fourier structure would be challenged.",
        "If models can perform arithmetic with no observable modular or magnitude decomposition in their activations, the phase-based mechanism would be undermined.",
        "If models with no observable Fourier peaks in their embeddings or activations can perform high-precision addition, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "The mechanism for multi-step, multi-operation arithmetic (e.g., multi-digit multiplication, division, or algebraic manipulation) is not fully explained by the Fourier-modular decomposition, which is best characterized for single-step addition.",
            "uuids": [
                "e4720.2",
                "e4720.5",
                "e4628.4",
                "e4730.0",
                "e4730.1",
                "e4730.3",
                "e4627.5",
                "e4627.2",
                "e4627.3"
            ]
        },
        {
            "text": "The role of chain-of-thought prompting and explicit intermediate reasoning steps in arithmetic is not directly addressed by this theory.",
            "uuids": [
                "e4741.0",
                "e4730.0",
                "e4721.0",
                "e4724.2",
                "e4720.2",
                "e4720.5",
                "e4626.0",
                "e4626.3",
                "e4626.2"
            ]
        },
        {
            "text": "Arithmetic performance in models with different tokenization schemes (e.g., digit-level vs subword) is not fully explained by the Fourier mechanism alone.",
            "uuids": [
                "e4734.5",
                "e4720.0",
                "e4620.0",
                "e4620.1",
                "e4620.2",
                "e4734.6"
            ]
        },
        {
            "text": "The theory does not account for models that perform arithmetic by program synthesis and external execution (e.g., Codex, PoT, MathPrompter).",
            "uuids": [
                "e4707.2",
                "e4707.1",
                "e4730.1",
                "e4706.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Models trained from scratch (without pretraining) can reach non-trivial accuracy using primarily low-frequency approximation, indicating that alternative (less precise) solutions exist.",
            "uuids": [
                "e4629.5"
            ]
        },
        {
            "text": "Some models (e.g., LLaMA-2-7B, Mistral-7B) show approximate linear numeric encoding in the residual stream, but do not achieve high-precision arithmetic, suggesting that Fourier structure is necessary but not sufficient.",
            "uuids": [
                "e4620.0",
                "e4620.2"
            ]
        }
    ],
    "special_cases": [
        "For arithmetic tasks involving numbers outside the pretraining distribution (e.g., very large or negative numbers), the Fourier structure may not generalize, leading to systematic errors.",
        "For tasks requiring symbolic manipulation (e.g., algebraic simplification), the phase-based mechanism may be insufficient, and other mechanisms may dominate.",
        "For models with subword or character-level tokenization, the emergence and utility of Fourier structure may differ, potentially reducing arithmetic performance.",
        "For arithmetic tasks with non-integer or floating-point numbers, the discrete Fourier basis may not provide sufficient resolution, requiring alternative representations."
    ],
    "existing_theory": {
        "what_already_exists": "Fourier-based mechanisms for modular arithmetic in small models and the use of sinusoidal encodings in transformers.",
        "what_is_novel": "The generalization to large LLMs, the explicit modular-magnitude decomposition, and the identification of arithmetic as phase manipulation in distributed Fourier space.",
        "classification_explanation": "This theory synthesizes and extends prior work on modular addition circuits and positional encodings, providing a new, unified account of arithmetic in LLMs.",
        "likely_classification": "new",
        "references": [
            "Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits for modular addition]",
            "Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [direct evidence for this theory]",
            "Vaswani et al. (2017) Attention is All You Need [sinusoidal positional encodings, not arithmetic mechanism]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Faceted, Human-Grounded Evaluation Theory for LLM-Generated Scientific Theories - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-534</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-534</p>
                <p><strong>Name:</strong> Multi-Faceted, Human-Grounded Evaluation Theory for LLM-Generated Scientific Theories</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that robust evaluation of LLM-generated scientific theories requires a multi-faceted approach that integrates human expert judgment, automated metrics, and model-based evaluators, each addressing distinct but complementary aspects of scientific theory quality: validity, novelty, helpfulness, and faithfulness. The theory asserts that no single evaluation method suffices, and that meta-evaluation (alignment with human judgment) is essential for calibrating and combining metrics. The theory further claims that the evaluation process must be dynamic, adapting to new domains, data leakage risks, and evolving model capabilities.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Multi-Aspect Evaluation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; an LLM-generated scientific theory &#8594; is_evaluated &#8594; for scientific merit</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the evaluation &#8594; must_include &#8594; validity assessment (does it reflect reality?)<span style="color: #888888;">, and</span></div>
        <div>&#8226; the evaluation &#8594; must_include &#8594; novelty assessment (is it new relative to prior literature?)<span style="color: #888888;">, and</span></div>
        <div>&#8226; the evaluation &#8594; must_include &#8594; helpfulness assessment (is it actionable/useful to researchers?)<span style="color: #888888;">, and</span></div>
        <div>&#8226; the evaluation &#8594; must_include &#8594; faithfulness assessment (is it supported by evidence or source?)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Evaluation metrics in TOMATO and BHP (validness, novelty, helpfulness, verifiability) and their use in both human and LLM-based evaluation. <a href="../results/extraction-result-3819.html#e3819.2" class="evidence-link">[e3819.2]</a> <a href="../results/extraction-result-3814.html#e3814.2" class="evidence-link">[e3814.2]</a> <a href="../results/extraction-result-3814.html#e3814.3" class="evidence-link">[e3814.3]</a> </li>
    <li>Limitations in single-metric approaches (e.g., BLEU/ROUGE) for scientific hypothesis evaluation; word-overlap metrics fail to capture novelty and helpfulness. <a href="../results/extraction-result-3814.html#e3814.1" class="evidence-link">[e3814.1]</a> <a href="../results/extraction-result-3987.html#e3987.7" class="evidence-link">[e3987.7]</a> <a href="../results/extraction-result-3816.html#e3816.5" class="evidence-link">[e3816.5]</a> </li>
    <li>Human expert evaluation protocols and multi-aspect rubrics (e.g., TOMATO, BHP, MM-Vet) are used as gold standards for evaluating LLM-generated scientific outputs. <a href="../results/extraction-result-3819.html#e3819.1" class="evidence-link">[e3819.1]</a> <a href="../results/extraction-result-3966.html#e3966.6" class="evidence-link">[e3966.6]</a> <a href="../results/extraction-result-3966.html#e3966.3" class="evidence-link">[e3966.3]</a> </li>
    <li>Automated metrics (e.g., METEOR, WRecall, GREEN) are used but are acknowledged as insufficient alone for open-ended scientific theory evaluation. <a href="../results/extraction-result-3987.html#e3987.0" class="evidence-link">[e3987.0]</a> <a href="../results/extraction-result-3987.html#e3987.1" class="evidence-link">[e3987.1]</a> <a href="../results/extraction-result-3987.html#e3987.2" class="evidence-link">[e3987.2]</a> </li>
    <li>Evaluation limitations and open challenges highlight the need for multi-aspect, human-grounded evaluation, especially for scientific hypotheses. <a href="../results/extraction-result-3987.html#e3987.7" class="evidence-link">[e3987.7]</a> <a href="../results/extraction-result-3816.html#e3816.3" class="evidence-link">[e3816.3]</a> <a href="../results/extraction-result-3816.html#e3816.5" class="evidence-link">[e3816.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Human-Grounded Meta-Evaluation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; an automatic or LLM-based evaluator &#8594; is_used &#8594; to score LLM-generated scientific theories</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the evaluator &#8594; must_be_meta-evaluated_against &#8594; human expert judgment<span style="color: #888888;">, and</span></div>
        <div>&#8226; the evaluation pipeline &#8594; must_calibrate &#8594; automated scores to human alignment (e.g., via soft/hard consistency, absolute difference, or correlation)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>High soft consistency between GPT-4 and human expert ratings in TOMATO and BHP; use of averaged absolute difference (Δ̄) in MM-Vet; inter-expert agreement and LLM-human alignment as validation for automated evaluators. <a href="../results/extraction-result-3819.html#e3819.5" class="evidence-link">[e3819.5]</a> <a href="../results/extraction-result-3966.html#e3966.3" class="evidence-link">[e3966.3]</a> <a href="../results/extraction-result-3814.html#e3814.2" class="evidence-link">[e3814.2]</a> <a href="../results/extraction-result-3966.html#e3966.1" class="evidence-link">[e3966.1]</a> <a href="../results/extraction-result-3966.html#e3966.6" class="evidence-link">[e3966.6]</a> </li>
    <li>Model-based evaluators (e.g., GPT-4, ChatGPT) are only justified as proxies when their outputs are shown to align with human experts; evidence of bias or misalignment (e.g., G-EVAL-4 bias toward LLM outputs) highlights the need for meta-evaluation. <a href="../results/extraction-result-3814.html#e3814.2" class="evidence-link">[e3814.2]</a> <a href="../results/extraction-result-3966.html#e3966.3" class="evidence-link">[e3966.3]</a> <a href="../results/extraction-result-3981.html#e3981.0" class="evidence-link">[e3981.0]</a> <a href="../results/extraction-result-3981.html#e3981.8" class="evidence-link">[e3981.8]</a> </li>
    <li>Evaluator validation protocols (e.g., MM-Vet, TOMATO) use human-annotated gold standards to measure alignment (Δ̄, soft/hard consistency, correlation) between LLM-based and human scores. <a href="../results/extraction-result-3966.html#e3966.3" class="evidence-link">[e3966.3]</a> <a href="../results/extraction-result-3819.html#e3819.5" class="evidence-link">[e3819.5]</a> <a href="../results/extraction-result-3819.html#e3819.1" class="evidence-link">[e3819.1]</a> </li>
    <li>Human evaluation remains the gold standard for open-ended scientific outputs, and automated metrics are benchmarked against human judgments. <a href="../results/extraction-result-3819.html#e3819.1" class="evidence-link">[e3819.1]</a> <a href="../results/extraction-result-3814.html#e3814.3" class="evidence-link">[e3814.3]</a> <a href="../results/extraction-result-3977.html#e3977.6" class="evidence-link">[e3977.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Dynamic and Domain-Adaptive Evaluation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; the evaluation context &#8594; changes &#8594; domain, data distribution, or model capabilities</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the evaluation protocol &#8594; must_be_updated &#8594; to address new domain requirements, data leakage risks, and model-specific artifacts</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Dynamic Data in SciEval to avoid data leakage; need for new benchmarks as models improve; domain-specific human expertise required for specialized scientific claims; limitations of static benchmarks and the need for periodic updates (GAIA, SciEval, TOMATO). <a href="../results/extraction-result-3816.html#e3816.2" class="evidence-link">[e3816.2]</a> <a href="../results/extraction-result-3816.html#e3816.0" class="evidence-link">[e3816.0]</a> <a href="../results/extraction-result-3975.html#e3975.6" class="evidence-link">[e3975.6]</a> <a href="../results/extraction-result-3987.html#e3987.7" class="evidence-link">[e3987.7]</a> <a href="../results/extraction-result-3959.html#e3959.5" class="evidence-link">[e3959.5]</a> <a href="../results/extraction-result-3819.html#e3819.2" class="evidence-link">[e3819.2]</a> </li>
    <li>Contamination avoidance via new question creation and dynamic regeneration; evidence that static benchmarks can be compromised by model memorization or data leakage. <a href="../results/extraction-result-3975.html#e3975.5" class="evidence-link">[e3975.5]</a> <a href="../results/extraction-result-3816.html#e3816.2" class="evidence-link">[e3816.2]</a> <a href="../results/extraction-result-3988.html#e3988.4" class="evidence-link">[e3988.4]</a> </li>
    <li>Evaluation limitations and open challenges in scientific theory evaluation (e.g., need for experimental validation, domain adaptation, and evolving benchmarks). <a href="../results/extraction-result-3987.html#e3987.7" class="evidence-link">[e3987.7]</a> <a href="../results/extraction-result-3816.html#e3816.3" class="evidence-link">[e3816.3]</a> <a href="../results/extraction-result-3959.html#e3959.5" class="evidence-link">[e3959.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new LLM-generated theory is evaluated using only BLEU/ROUGE, its novelty and helpfulness will be systematically underestimated compared to human expert evaluation.</li>
                <li>If an LLM-based evaluator (e.g., GPT-4) is not meta-evaluated against human experts in a new scientific domain, its scores will show unpredictable bias or misalignment.</li>
                <li>If a benchmark is not updated to account for data leakage, LLMs will appear to perform better than they actually generalize.</li>
                <li>If a new domain (e.g., materials science) is introduced, evaluation protocols will need to be adapted with new human expertise and domain-specific criteria to maintain validity.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a hybrid evaluation pipeline (combining human, LLM, and automated metrics) is deployed in a new, highly interdisciplinary scientific field, it will maintain high alignment with expert judgment even as the field evolves.</li>
                <li>If LLM-based evaluators are fine-tuned on human expert feedback in a new domain, they will eventually surpass average human inter-rater agreement in consistency.</li>
                <li>If dynamic, continually updated benchmarks are used, LLMs' apparent performance will decrease initially but then recover as models adapt to new data distributions.</li>
                <li>If human-in-the-loop evaluation is replaced entirely by LLM-based evaluators, the long-term impact on scientific discovery quality will depend on the ability to detect and correct evaluator bias.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a single-metric (e.g., BLEU) evaluation is found to correlate as highly with human expert ratings as a multi-aspect, human-grounded pipeline, this would challenge the necessity of multi-faceted evaluation.</li>
                <li>If LLM-based evaluators consistently disagree with human experts even after calibration/meta-evaluation, the theory's claim about proxy validity would be undermined.</li>
                <li>If static benchmarks do not show performance inflation despite known data leakage, the necessity of dynamic evaluation protocols would be questioned.</li>
                <li>If human experts cannot agree on validity, novelty, or helpfulness in a new domain, the premise of human-grounded meta-evaluation would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not fully address evaluation of theories that require experimental or empirical validation beyond text (e.g., new physical laws requiring laboratory testing). <a href="../results/extraction-result-3987.html#e3987.7" class="evidence-link">[e3987.7]</a> <a href="../results/extraction-result-3816.html#e3816.3" class="evidence-link">[e3816.3]</a> </li>
    <li>The theory does not specify how to evaluate LLM-generated theories in domains where no human expertise exists (e.g., superhuman or speculative science). <a href="../results/extraction-result-3819.html#e3819.2" class="evidence-link">[e3819.2]</a> <a href="../results/extraction-result-3816.html#e3816.3" class="evidence-link">[e3816.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [Multi-aspect, human-grounded evaluation in TOMATO]</li>
    <li>Wang et al. (2023) Large Language Models are Zero Shot Hypothesis Proposers [Multi-aspect evaluation in scientific hypothesis generation]</li>
    <li>Zheng et al. (2023) Judging LLM-as-a-judge: A study on automatic evaluation of LLMs [Model-based evaluation and human alignment]</li>
    <li>Bubeck et al. (2023) Sparks of Artificial General Intelligence: Early experiments with GPT-4 [Discusses multi-aspect evaluation and human alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Faceted, Human-Grounded Evaluation Theory for LLM-Generated Scientific Theories",
    "theory_description": "This theory posits that robust evaluation of LLM-generated scientific theories requires a multi-faceted approach that integrates human expert judgment, automated metrics, and model-based evaluators, each addressing distinct but complementary aspects of scientific theory quality: validity, novelty, helpfulness, and faithfulness. The theory asserts that no single evaluation method suffices, and that meta-evaluation (alignment with human judgment) is essential for calibrating and combining metrics. The theory further claims that the evaluation process must be dynamic, adapting to new domains, data leakage risks, and evolving model capabilities.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Multi-Aspect Evaluation Law",
                "if": [
                    {
                        "subject": "an LLM-generated scientific theory",
                        "relation": "is_evaluated",
                        "object": "for scientific merit"
                    }
                ],
                "then": [
                    {
                        "subject": "the evaluation",
                        "relation": "must_include",
                        "object": "validity assessment (does it reflect reality?)"
                    },
                    {
                        "subject": "the evaluation",
                        "relation": "must_include",
                        "object": "novelty assessment (is it new relative to prior literature?)"
                    },
                    {
                        "subject": "the evaluation",
                        "relation": "must_include",
                        "object": "helpfulness assessment (is it actionable/useful to researchers?)"
                    },
                    {
                        "subject": "the evaluation",
                        "relation": "must_include",
                        "object": "faithfulness assessment (is it supported by evidence or source?)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Evaluation metrics in TOMATO and BHP (validness, novelty, helpfulness, verifiability) and their use in both human and LLM-based evaluation.",
                        "uuids": [
                            "e3819.2",
                            "e3814.2",
                            "e3814.3"
                        ]
                    },
                    {
                        "text": "Limitations in single-metric approaches (e.g., BLEU/ROUGE) for scientific hypothesis evaluation; word-overlap metrics fail to capture novelty and helpfulness.",
                        "uuids": [
                            "e3814.1",
                            "e3987.7",
                            "e3816.5"
                        ]
                    },
                    {
                        "text": "Human expert evaluation protocols and multi-aspect rubrics (e.g., TOMATO, BHP, MM-Vet) are used as gold standards for evaluating LLM-generated scientific outputs.",
                        "uuids": [
                            "e3819.1",
                            "e3966.6",
                            "e3966.3"
                        ]
                    },
                    {
                        "text": "Automated metrics (e.g., METEOR, WRecall, GREEN) are used but are acknowledged as insufficient alone for open-ended scientific theory evaluation.",
                        "uuids": [
                            "e3987.0",
                            "e3987.1",
                            "e3987.2"
                        ]
                    },
                    {
                        "text": "Evaluation limitations and open challenges highlight the need for multi-aspect, human-grounded evaluation, especially for scientific hypotheses.",
                        "uuids": [
                            "e3987.7",
                            "e3816.3",
                            "e3816.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Human-Grounded Meta-Evaluation Law",
                "if": [
                    {
                        "subject": "an automatic or LLM-based evaluator",
                        "relation": "is_used",
                        "object": "to score LLM-generated scientific theories"
                    }
                ],
                "then": [
                    {
                        "subject": "the evaluator",
                        "relation": "must_be_meta-evaluated_against",
                        "object": "human expert judgment"
                    },
                    {
                        "subject": "the evaluation pipeline",
                        "relation": "must_calibrate",
                        "object": "automated scores to human alignment (e.g., via soft/hard consistency, absolute difference, or correlation)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "High soft consistency between GPT-4 and human expert ratings in TOMATO and BHP; use of averaged absolute difference (Δ̄) in MM-Vet; inter-expert agreement and LLM-human alignment as validation for automated evaluators.",
                        "uuids": [
                            "e3819.5",
                            "e3966.3",
                            "e3814.2",
                            "e3966.1",
                            "e3966.6"
                        ]
                    },
                    {
                        "text": "Model-based evaluators (e.g., GPT-4, ChatGPT) are only justified as proxies when their outputs are shown to align with human experts; evidence of bias or misalignment (e.g., G-EVAL-4 bias toward LLM outputs) highlights the need for meta-evaluation.",
                        "uuids": [
                            "e3814.2",
                            "e3966.3",
                            "e3981.0",
                            "e3981.8"
                        ]
                    },
                    {
                        "text": "Evaluator validation protocols (e.g., MM-Vet, TOMATO) use human-annotated gold standards to measure alignment (Δ̄, soft/hard consistency, correlation) between LLM-based and human scores.",
                        "uuids": [
                            "e3966.3",
                            "e3819.5",
                            "e3819.1"
                        ]
                    },
                    {
                        "text": "Human evaluation remains the gold standard for open-ended scientific outputs, and automated metrics are benchmarked against human judgments.",
                        "uuids": [
                            "e3819.1",
                            "e3814.3",
                            "e3977.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Dynamic and Domain-Adaptive Evaluation Law",
                "if": [
                    {
                        "subject": "the evaluation context",
                        "relation": "changes",
                        "object": "domain, data distribution, or model capabilities"
                    }
                ],
                "then": [
                    {
                        "subject": "the evaluation protocol",
                        "relation": "must_be_updated",
                        "object": "to address new domain requirements, data leakage risks, and model-specific artifacts"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Dynamic Data in SciEval to avoid data leakage; need for new benchmarks as models improve; domain-specific human expertise required for specialized scientific claims; limitations of static benchmarks and the need for periodic updates (GAIA, SciEval, TOMATO).",
                        "uuids": [
                            "e3816.2",
                            "e3816.0",
                            "e3975.6",
                            "e3987.7",
                            "e3959.5",
                            "e3819.2"
                        ]
                    },
                    {
                        "text": "Contamination avoidance via new question creation and dynamic regeneration; evidence that static benchmarks can be compromised by model memorization or data leakage.",
                        "uuids": [
                            "e3975.5",
                            "e3816.2",
                            "e3988.4"
                        ]
                    },
                    {
                        "text": "Evaluation limitations and open challenges in scientific theory evaluation (e.g., need for experimental validation, domain adaptation, and evolving benchmarks).",
                        "uuids": [
                            "e3987.7",
                            "e3816.3",
                            "e3959.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new LLM-generated theory is evaluated using only BLEU/ROUGE, its novelty and helpfulness will be systematically underestimated compared to human expert evaluation.",
        "If an LLM-based evaluator (e.g., GPT-4) is not meta-evaluated against human experts in a new scientific domain, its scores will show unpredictable bias or misalignment.",
        "If a benchmark is not updated to account for data leakage, LLMs will appear to perform better than they actually generalize.",
        "If a new domain (e.g., materials science) is introduced, evaluation protocols will need to be adapted with new human expertise and domain-specific criteria to maintain validity."
    ],
    "new_predictions_unknown": [
        "If a hybrid evaluation pipeline (combining human, LLM, and automated metrics) is deployed in a new, highly interdisciplinary scientific field, it will maintain high alignment with expert judgment even as the field evolves.",
        "If LLM-based evaluators are fine-tuned on human expert feedback in a new domain, they will eventually surpass average human inter-rater agreement in consistency.",
        "If dynamic, continually updated benchmarks are used, LLMs' apparent performance will decrease initially but then recover as models adapt to new data distributions.",
        "If human-in-the-loop evaluation is replaced entirely by LLM-based evaluators, the long-term impact on scientific discovery quality will depend on the ability to detect and correct evaluator bias."
    ],
    "negative_experiments": [
        "If a single-metric (e.g., BLEU) evaluation is found to correlate as highly with human expert ratings as a multi-aspect, human-grounded pipeline, this would challenge the necessity of multi-faceted evaluation.",
        "If LLM-based evaluators consistently disagree with human experts even after calibration/meta-evaluation, the theory's claim about proxy validity would be undermined.",
        "If static benchmarks do not show performance inflation despite known data leakage, the necessity of dynamic evaluation protocols would be questioned.",
        "If human experts cannot agree on validity, novelty, or helpfulness in a new domain, the premise of human-grounded meta-evaluation would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not fully address evaluation of theories that require experimental or empirical validation beyond text (e.g., new physical laws requiring laboratory testing).",
            "uuids": [
                "e3987.7",
                "e3816.3"
            ]
        },
        {
            "text": "The theory does not specify how to evaluate LLM-generated theories in domains where no human expertise exists (e.g., superhuman or speculative science).",
            "uuids": [
                "e3819.2",
                "e3816.3"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some evidence suggests that LLM-based evaluators can be systematically biased toward LLM-generated outputs, even when humans prefer human-written outputs (e.g., G-EVAL-4 scoring GPT-3.5 summaries higher than human summaries).",
            "uuids": [
                "e3981.8"
            ]
        },
        {
            "text": "Low inter-annotator agreement among human judges for high-quality outputs (e.g., Krippendorff's alpha 0.07 in G-EVAL human comparison dataset) challenges the reliability of human gold standards.",
            "uuids": [
                "e3981.8",
                "e3977.6"
            ]
        }
    ],
    "special_cases": [
        "In domains where no human expertise exists (e.g., superhuman or speculative science), human-grounded meta-evaluation may not be possible.",
        "For tasks with purely formal outputs (e.g., code, math), automated metrics may suffice if ground-truth is unambiguous.",
        "For subjective or open-ended theory generation, inter-expert disagreement may limit the utility of human-grounded meta-evaluation.",
        "In cases where LLMs are used to generate hypotheses for experimental validation, the ultimate evaluation may depend on real-world experiments rather than any text-based metric."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [Multi-aspect, human-grounded evaluation in TOMATO]",
            "Wang et al. (2023) Large Language Models are Zero Shot Hypothesis Proposers [Multi-aspect evaluation in scientific hypothesis generation]",
            "Zheng et al. (2023) Judging LLM-as-a-judge: A study on automatic evaluation of LLMs [Model-based evaluation and human alignment]",
            "Bubeck et al. (2023) Sparks of Artificial General Intelligence: Early experiments with GPT-4 [Discusses multi-aspect evaluation and human alignment]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
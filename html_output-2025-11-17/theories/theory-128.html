<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Staged Curriculum Optimality Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-128</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-128</p>
                <p><strong>Name:</strong> Staged Curriculum Optimality Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of the fundamental trade-off between environment complexity and environment variation in embodied learning systems, based on the following results.</p>
                <p><strong>Description:</strong> For embodied learning in environments with both high complexity and high variation, sample-efficient learning typically requires a staged curriculum that sequences complexity and variation increases rather than increasing both simultaneously. The optimal staging strategy depends on task structure: (1) for tasks with hierarchical prerequisites, complexity should be staged first while maintaining low variation, then variation increased once competence is achieved; (2) for tasks requiring robustness, variation should be introduced early but at low complexity, then complexity increased; (3) the transition timing between stages is critical—too early causes insufficient skill acquisition, too late causes overfitting. Adaptive curricula that coordinate both dimensions can sometimes increase them simultaneously by maintaining intermediate difficulty. The benefits of staged curricula are most pronounced in sample-limited regimes; with sufficient scale (e.g., billions of training steps), direct training on complex, varied environments can succeed but at much higher sample cost.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Staged curricula that coordinate complexity and variation increases are more sample-efficient than curricula that increase both simultaneously or treat them independently</li>
                <li>For hierarchical tasks with prerequisite skills, complexity should be staged before variation to build foundational skills without interference from environmental variability</li>
                <li>For robustness-critical tasks, variation should be introduced early at low complexity to learn invariant features before scaling to complex scenarios</li>
                <li>Transition timing between curriculum stages critically affects final performance: premature transitions cause skill gaps and learning failures, delayed transitions cause overfitting to simplified conditions</li>
                <li>The optimal curriculum structure is task-dependent and cannot be determined a priori without task analysis, adaptive methods, or domain knowledge</li>
                <li>Curricula that maintain intermediate difficulty (neither too easy nor too hard) throughout training are more sample-efficient than fixed-difficulty curricula</li>
                <li>Adaptive curricula that monitor learning progress and adjust staging can sometimes coordinate complexity and variation increases simultaneously by maintaining appropriate difficulty</li>
                <li>The benefits of staged curricula are most pronounced in sample-limited regimes; with sufficient scale (billions of steps), direct training can succeed but at much higher sample cost</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>TriFinger experiments showed whole-model transfer worked best for context-aware tasks, while 2-stage curricula gave fastest convergence and 3-stage improved generalization; timing of transitions was critical <a href="../results/extraction-result-1018.html#e1018.0" class="evidence-link">[e1018.0]</a> </li>
    <li>GC-VAT drone tracking: two-stage curriculum (simple→complex) was essential; single-stage RL failed to converge on high-complexity maps (citystreet, downtown, farmland) <a href="../results/extraction-result-1045.html#e1045.0" class="evidence-link">[e1045.0]</a> </li>
    <li>CM3 multi-agent navigation: Stage-1 single-agent training followed by Stage-2 multi-agent training converged >15k episodes faster than direct multi-agent training <a href="../results/extraction-result-1062.html#e1062.0" class="evidence-link">[e1062.0]</a> </li>
    <li>SPDL on point-mass gate: curriculum that traded off expected return and KL to target distribution achieved 9.35±0.1 vs baselines <1.0; narrow target distributions prone to local optima without curriculum <a href="../results/extraction-result-1087.html#e1087.0" class="evidence-link">[e1087.0]</a> </li>
    <li>SPDL on Ant gate: curriculum enabled reliable gate-passing behavior where other methods failed or were unreliable; escaped local optima that prevented direct training <a href="../results/extraction-result-1087.html#e1087.1" class="evidence-link">[e1087.1]</a> </li>
    <li>VaPRL door-closing: value-driven curriculum selecting progressively harder start states matched oracle performance with ~500× fewer resets <a href="../results/extraction-result-1025.html#e1025.1" class="evidence-link">[e1025.1]</a> </li>
    <li>SGIM-PB: hierarchical learner self-organized curriculum starting with simple tasks using action-space exploration, then moved to procedure exploration for complex tasks; batch transfer of procedures at initialization disrupted curriculum <a href="../results/extraction-result-1027.html#e1027.0" class="evidence-link">[e1027.0]</a> </li>
    <li>A* curriculum for PointNav: staged waypoint curriculum (SWP) achieved higher SPL (0.82) than direct PointNav (0.73) and converged faster; transfer learning from SWP improved performance on new scenes <a href="../results/extraction-result-1040.html#e1040.0" class="evidence-link">[e1040.0]</a> </li>
    <li>CausalWorld: targeted variation (goal pose randomization, Curriculum 1) enabled learning while extreme simultaneous randomization (Curriculum 2) prevented learning even after 100M timesteps <a href="../results/extraction-result-1074.html#e1074.0" class="evidence-link">[e1074.0]</a> </li>
    <li>Self-Paced RL on Ball-in-Cup: controlling cup diameter curriculum from large (easy) to small (hard) enabled learning where direct training on small cup failed; SPRL outperformed baselines under sparse rewards <a href="../results/extraction-result-1022.html#e1022.2" class="evidence-link">[e1022.2]</a> </li>
    <li>Gate-PointMass with SPRL: curriculum from easier to harder gate contexts avoided local optima in precision setting and converged faster in global setting compared to C-REPS and CMA-ES <a href="../results/extraction-result-1022.html#e1022.0" class="evidence-link">[e1022.0]</a> </li>
    <li>TeachMyAgent StumpTracks: ACL methods that adapted task difficulty over time outperformed random sampling; methods requiring initial easy distributions failed when no reliable easy tasks existed <a href="../results/extraction-result-1089.html#e1089.0" class="evidence-link">[e1089.0]</a> </li>
    <li>AdroitHandRelocate: gradient-norm-based curriculum (average per timestep) enabled teacher to produce increasingly difficult curricula that accelerated student learning compared to no-teacher baseline <a href="../results/extraction-result-1026.html#e1026.0" class="evidence-link">[e1026.0]</a> </li>
    <li>DDPG+UVFA Reacher: competence-progress-based sampling (ACTIVE-) achieved faster early learning and higher final success than uniform sampling (RANDOM-) and single-accuracy training; easier accuracies produced quick early progress then plateaued <a href="../results/extraction-result-1008.html#e1008.0" class="evidence-link">[e1008.0]</a> </li>
    <li>SS-ADR: co-evolving goal curriculum (via self-play) and environment curriculum (via ADR) yielded better generalization than either alone; coupling prevented degenerate training <a href="../results/extraction-result-1009.html#e1009.0" class="evidence-link">[e1009.0]</a> </li>
    <li>CLUTR vs PAIRED in MiniGrid: CLUTR's learned task representation with curriculum outperformed PAIRED's simultaneous manifold-and-curriculum learning which suffered instability and degenerate curricula <a href="../results/extraction-result-1033.html#e1033.3" class="evidence-link">[e1033.3]</a> </li>
    <li>PAIRED Hopper: regret-based curriculum maintained feasible but challenging perturbations while unconstrained minimax drove agent reward to zero; demonstrates need for balanced difficulty staging <a href="../results/extraction-result-1038.html#e1038.1" class="evidence-link">[e1038.1]</a> </li>
    <li>Fetch manipulation with VDS: value disagreement sampling focused on intermediate-difficulty goals at knowledge frontier, improving sample efficiency over uniform sampling (HER) <a href="../results/extraction-result-1061.html#e1061.0" class="evidence-link">[e1061.0]</a> </li>
    <li>MountainCar self-play: asymmetric self-play (reset variant) accelerated learning where target-only policy gradient failed; choice of variant depends on environment dynamics asymmetry <a href="../results/extraction-result-1096.html#e1096.3" class="evidence-link">[e1096.3]</a> </li>
    <li>GCL navigation: grounded curriculum with adaptive complexity progression and real-world task grounding achieved 81.85% success vs 75.05% for CLUTR baseline <a href="../results/extraction-result-1024.html#e1024.0" class="evidence-link">[e1024.0]</a> </li>
    <li>Genetic Curriculum: non-parametric curriculum balancing similarity (for transfer) and coverage (for generalization) reduced failure rates 2-8× across robotic benchmarks <a href="../results/extraction-result-1088.html#e1088.5" class="evidence-link">[e1088.5]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A 3-stage curriculum (low complexity/low variation → low complexity/high variation → high complexity/high variation) will outperform 2-stage curricula for tasks requiring both skill acquisition and robustness, with optimal transition points determined by competence thresholds</li>
                <li>Adaptive curricula that monitor learning progress and adjust staging will outperform fixed-schedule curricula across diverse tasks, with larger improvements in tasks with non-uniform difficulty landscapes</li>
                <li>For manipulation tasks with contact dynamics, staging complexity (number of objects) before variation (object properties) will be more effective than the reverse, measured by sample efficiency to reach target performance</li>
                <li>Curricula that explicitly model the complexity-variation trade-off will converge faster than curricula that only consider task difficulty, especially in domains with sparse rewards</li>
                <li>Transfer learning from curriculum-trained policies will be more effective than transfer from policies trained directly on target tasks, particularly when target tasks have high complexity and variation</li>
                <li>Curricula that use competence-progress signals to determine transition timing will outperform fixed-schedule curricula, with larger benefits in tasks with heterogeneous skill acquisition rates</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a universal curriculum structure that works across all embodied learning tasks, or if curricula must always be task-specific and adaptive</li>
                <li>Whether curriculum learning can enable learning on tasks that are fundamentally impossible without curriculum (even with unlimited data), or if it only improves sample efficiency</li>
                <li>Whether the optimal curriculum changes qualitatively as agent capacity increases (e.g., with larger networks, more training time, or better architectures), potentially reducing or eliminating curriculum benefits</li>
                <li>Whether curricula that stage complexity and variation simultaneously in a coordinated way (using adaptive methods) could consistently outperform sequential staging across all task types</li>
                <li>Whether the benefits of staged curricula diminish or disappear entirely in the limit of unlimited data and compute, or if there are fundamental learning dynamics that always favor staged approaches</li>
                <li>Whether off-policy methods with large replay buffers can effectively implement implicit curricula that match or exceed explicit staged curricula, and under what conditions</li>
                <li>Whether there are task structures where simultaneous increases in complexity and variation are actually optimal, contradicting the staged approach</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where simultaneous increases in complexity and variation consistently outperform staged curricula across multiple random seeds would challenge the core theory</li>
                <li>Demonstrating that transition timing does not significantly affect final performance (e.g., random transition points perform as well as optimized ones) would contradict the critical timing claim</li>
                <li>Showing that task-agnostic curricula consistently outperform task-specific curricula across diverse domains would challenge the task-structure dependency claim</li>
                <li>Finding that random curriculum ordering performs as well as optimized staging in sample-limited regimes would falsify the optimality claims</li>
                <li>Demonstrating that direct training on high-complexity, high-variation environments is equally sample-efficient as staged curricula when controlling for total training steps would challenge the sample efficiency claim</li>
                <li>Finding that curriculum benefits disappear entirely when using off-policy methods with replay buffers would challenge the generality of the theory</li>
                <li>Showing that premature or delayed transitions do not harm performance would contradict the transition timing criticality claim</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically determine optimal transition timing without extensive hyperparameter search or domain knowledge </li>
    <li>Whether there are tasks where no curriculum helps regardless of staging strategy, even in sample-limited regimes </li>
    <li>How curriculum design interacts with different RL algorithms (on-policy vs off-policy, model-based vs model-free) and whether some algorithms benefit more from curricula than others </li>
    <li>The role of replay buffers in implicitly implementing curricula and whether explicit curricula provide additional benefits when replay is used </li>
    <li>Whether curriculum benefits scale with agent capacity (network size, architecture) or if there are diminishing returns </li>
    <li>How to balance curriculum design with other training considerations like exploration bonuses, auxiliary tasks, or multi-task learning </li>
    <li>Whether there are fundamental limits to what curricula can achieve, or if sufficiently sophisticated curricula can enable learning on any task </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Elman (1993) Learning and development in neural networks: the importance of starting small [Early work on staged learning and starting small principle]</li>
    <li>Bengio et al. (2009) Curriculum learning [Foundational curriculum learning work, but does not formalize complexity-variation coordination]</li>
    <li>Florensa et al. (2017) Reverse curriculum generation for reinforcement learning [Reverse curriculum for goal-reaching, related to staging from easy to hard]</li>
    <li>Narvekar et al. (2020) Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey [Comprehensive survey but does not formalize complexity-variation trade-offs or optimal staging strategies]</li>
    <li>Portelas et al. (2020) Teacher algorithms for curriculum learning of deep RL [Discusses automatic curriculum learning but does not formalize complexity-variation coordination]</li>
    <li>OpenAI et al. (2019) Solving Rubik's Cube with a Robot Hand [Uses domain randomization and curriculum but does not formalize staging principles]</li>
    <li>Akkaya et al. (2019) Solving Rubik's Cube with a Robot Hand [Demonstrates staged curriculum with ADR but does not provide general theory]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Staged Curriculum Optimality Theory",
    "theory_description": "For embodied learning in environments with both high complexity and high variation, sample-efficient learning typically requires a staged curriculum that sequences complexity and variation increases rather than increasing both simultaneously. The optimal staging strategy depends on task structure: (1) for tasks with hierarchical prerequisites, complexity should be staged first while maintaining low variation, then variation increased once competence is achieved; (2) for tasks requiring robustness, variation should be introduced early but at low complexity, then complexity increased; (3) the transition timing between stages is critical—too early causes insufficient skill acquisition, too late causes overfitting. Adaptive curricula that coordinate both dimensions can sometimes increase them simultaneously by maintaining intermediate difficulty. The benefits of staged curricula are most pronounced in sample-limited regimes; with sufficient scale (e.g., billions of training steps), direct training on complex, varied environments can succeed but at much higher sample cost.",
    "supporting_evidence": [
        {
            "text": "TriFinger experiments showed whole-model transfer worked best for context-aware tasks, while 2-stage curricula gave fastest convergence and 3-stage improved generalization; timing of transitions was critical",
            "uuids": [
                "e1018.0"
            ]
        },
        {
            "text": "GC-VAT drone tracking: two-stage curriculum (simple→complex) was essential; single-stage RL failed to converge on high-complexity maps (citystreet, downtown, farmland)",
            "uuids": [
                "e1045.0"
            ]
        },
        {
            "text": "CM3 multi-agent navigation: Stage-1 single-agent training followed by Stage-2 multi-agent training converged &gt;15k episodes faster than direct multi-agent training",
            "uuids": [
                "e1062.0"
            ]
        },
        {
            "text": "SPDL on point-mass gate: curriculum that traded off expected return and KL to target distribution achieved 9.35±0.1 vs baselines &lt;1.0; narrow target distributions prone to local optima without curriculum",
            "uuids": [
                "e1087.0"
            ]
        },
        {
            "text": "SPDL on Ant gate: curriculum enabled reliable gate-passing behavior where other methods failed or were unreliable; escaped local optima that prevented direct training",
            "uuids": [
                "e1087.1"
            ]
        },
        {
            "text": "VaPRL door-closing: value-driven curriculum selecting progressively harder start states matched oracle performance with ~500× fewer resets",
            "uuids": [
                "e1025.1"
            ]
        },
        {
            "text": "SGIM-PB: hierarchical learner self-organized curriculum starting with simple tasks using action-space exploration, then moved to procedure exploration for complex tasks; batch transfer of procedures at initialization disrupted curriculum",
            "uuids": [
                "e1027.0"
            ]
        },
        {
            "text": "A* curriculum for PointNav: staged waypoint curriculum (SWP) achieved higher SPL (0.82) than direct PointNav (0.73) and converged faster; transfer learning from SWP improved performance on new scenes",
            "uuids": [
                "e1040.0"
            ]
        },
        {
            "text": "CausalWorld: targeted variation (goal pose randomization, Curriculum 1) enabled learning while extreme simultaneous randomization (Curriculum 2) prevented learning even after 100M timesteps",
            "uuids": [
                "e1074.0"
            ]
        },
        {
            "text": "Self-Paced RL on Ball-in-Cup: controlling cup diameter curriculum from large (easy) to small (hard) enabled learning where direct training on small cup failed; SPRL outperformed baselines under sparse rewards",
            "uuids": [
                "e1022.2"
            ]
        },
        {
            "text": "Gate-PointMass with SPRL: curriculum from easier to harder gate contexts avoided local optima in precision setting and converged faster in global setting compared to C-REPS and CMA-ES",
            "uuids": [
                "e1022.0"
            ]
        },
        {
            "text": "TeachMyAgent StumpTracks: ACL methods that adapted task difficulty over time outperformed random sampling; methods requiring initial easy distributions failed when no reliable easy tasks existed",
            "uuids": [
                "e1089.0"
            ]
        },
        {
            "text": "AdroitHandRelocate: gradient-norm-based curriculum (average per timestep) enabled teacher to produce increasingly difficult curricula that accelerated student learning compared to no-teacher baseline",
            "uuids": [
                "e1026.0"
            ]
        },
        {
            "text": "DDPG+UVFA Reacher: competence-progress-based sampling (ACTIVE-) achieved faster early learning and higher final success than uniform sampling (RANDOM-) and single-accuracy training; easier accuracies produced quick early progress then plateaued",
            "uuids": [
                "e1008.0"
            ]
        },
        {
            "text": "SS-ADR: co-evolving goal curriculum (via self-play) and environment curriculum (via ADR) yielded better generalization than either alone; coupling prevented degenerate training",
            "uuids": [
                "e1009.0"
            ]
        },
        {
            "text": "CLUTR vs PAIRED in MiniGrid: CLUTR's learned task representation with curriculum outperformed PAIRED's simultaneous manifold-and-curriculum learning which suffered instability and degenerate curricula",
            "uuids": [
                "e1033.3"
            ]
        },
        {
            "text": "PAIRED Hopper: regret-based curriculum maintained feasible but challenging perturbations while unconstrained minimax drove agent reward to zero; demonstrates need for balanced difficulty staging",
            "uuids": [
                "e1038.1"
            ]
        },
        {
            "text": "Fetch manipulation with VDS: value disagreement sampling focused on intermediate-difficulty goals at knowledge frontier, improving sample efficiency over uniform sampling (HER)",
            "uuids": [
                "e1061.0"
            ]
        },
        {
            "text": "MountainCar self-play: asymmetric self-play (reset variant) accelerated learning where target-only policy gradient failed; choice of variant depends on environment dynamics asymmetry",
            "uuids": [
                "e1096.3"
            ]
        },
        {
            "text": "GCL navigation: grounded curriculum with adaptive complexity progression and real-world task grounding achieved 81.85% success vs 75.05% for CLUTR baseline",
            "uuids": [
                "e1024.0"
            ]
        },
        {
            "text": "Genetic Curriculum: non-parametric curriculum balancing similarity (for transfer) and coverage (for generalization) reduced failure rates 2-8× across robotic benchmarks",
            "uuids": [
                "e1088.5"
            ]
        }
    ],
    "theory_statements": [
        "Staged curricula that coordinate complexity and variation increases are more sample-efficient than curricula that increase both simultaneously or treat them independently",
        "For hierarchical tasks with prerequisite skills, complexity should be staged before variation to build foundational skills without interference from environmental variability",
        "For robustness-critical tasks, variation should be introduced early at low complexity to learn invariant features before scaling to complex scenarios",
        "Transition timing between curriculum stages critically affects final performance: premature transitions cause skill gaps and learning failures, delayed transitions cause overfitting to simplified conditions",
        "The optimal curriculum structure is task-dependent and cannot be determined a priori without task analysis, adaptive methods, or domain knowledge",
        "Curricula that maintain intermediate difficulty (neither too easy nor too hard) throughout training are more sample-efficient than fixed-difficulty curricula",
        "Adaptive curricula that monitor learning progress and adjust staging can sometimes coordinate complexity and variation increases simultaneously by maintaining appropriate difficulty",
        "The benefits of staged curricula are most pronounced in sample-limited regimes; with sufficient scale (billions of steps), direct training can succeed but at much higher sample cost"
    ],
    "new_predictions_likely": [
        "A 3-stage curriculum (low complexity/low variation → low complexity/high variation → high complexity/high variation) will outperform 2-stage curricula for tasks requiring both skill acquisition and robustness, with optimal transition points determined by competence thresholds",
        "Adaptive curricula that monitor learning progress and adjust staging will outperform fixed-schedule curricula across diverse tasks, with larger improvements in tasks with non-uniform difficulty landscapes",
        "For manipulation tasks with contact dynamics, staging complexity (number of objects) before variation (object properties) will be more effective than the reverse, measured by sample efficiency to reach target performance",
        "Curricula that explicitly model the complexity-variation trade-off will converge faster than curricula that only consider task difficulty, especially in domains with sparse rewards",
        "Transfer learning from curriculum-trained policies will be more effective than transfer from policies trained directly on target tasks, particularly when target tasks have high complexity and variation",
        "Curricula that use competence-progress signals to determine transition timing will outperform fixed-schedule curricula, with larger benefits in tasks with heterogeneous skill acquisition rates"
    ],
    "new_predictions_unknown": [
        "Whether there exists a universal curriculum structure that works across all embodied learning tasks, or if curricula must always be task-specific and adaptive",
        "Whether curriculum learning can enable learning on tasks that are fundamentally impossible without curriculum (even with unlimited data), or if it only improves sample efficiency",
        "Whether the optimal curriculum changes qualitatively as agent capacity increases (e.g., with larger networks, more training time, or better architectures), potentially reducing or eliminating curriculum benefits",
        "Whether curricula that stage complexity and variation simultaneously in a coordinated way (using adaptive methods) could consistently outperform sequential staging across all task types",
        "Whether the benefits of staged curricula diminish or disappear entirely in the limit of unlimited data and compute, or if there are fundamental learning dynamics that always favor staged approaches",
        "Whether off-policy methods with large replay buffers can effectively implement implicit curricula that match or exceed explicit staged curricula, and under what conditions",
        "Whether there are task structures where simultaneous increases in complexity and variation are actually optimal, contradicting the staged approach"
    ],
    "negative_experiments": [
        "Finding tasks where simultaneous increases in complexity and variation consistently outperform staged curricula across multiple random seeds would challenge the core theory",
        "Demonstrating that transition timing does not significantly affect final performance (e.g., random transition points perform as well as optimized ones) would contradict the critical timing claim",
        "Showing that task-agnostic curricula consistently outperform task-specific curricula across diverse domains would challenge the task-structure dependency claim",
        "Finding that random curriculum ordering performs as well as optimized staging in sample-limited regimes would falsify the optimality claims",
        "Demonstrating that direct training on high-complexity, high-variation environments is equally sample-efficient as staged curricula when controlling for total training steps would challenge the sample efficiency claim",
        "Finding that curriculum benefits disappear entirely when using off-policy methods with replay buffers would challenge the generality of the theory",
        "Showing that premature or delayed transitions do not harm performance would contradict the transition timing criticality claim"
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically determine optimal transition timing without extensive hyperparameter search or domain knowledge",
            "uuids": []
        },
        {
            "text": "Whether there are tasks where no curriculum helps regardless of staging strategy, even in sample-limited regimes",
            "uuids": []
        },
        {
            "text": "How curriculum design interacts with different RL algorithms (on-policy vs off-policy, model-based vs model-free) and whether some algorithms benefit more from curricula than others",
            "uuids": []
        },
        {
            "text": "The role of replay buffers in implicitly implementing curricula and whether explicit curricula provide additional benefits when replay is used",
            "uuids": []
        },
        {
            "text": "Whether curriculum benefits scale with agent capacity (network size, architecture) or if there are diminishing returns",
            "uuids": []
        },
        {
            "text": "How to balance curriculum design with other training considerations like exploration bonuses, auxiliary tasks, or multi-task learning",
            "uuids": []
        },
        {
            "text": "Whether there are fundamental limits to what curricula can achieve, or if sufficiently sophisticated curricula can enable learning on any task",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some simple tasks (e.g., low-complexity navigation) may not benefit from curriculum and could be learned directly without sample efficiency loss",
            "uuids": [
                "e1040.0"
            ]
        },
        {
            "text": "PAIRED sometimes produced degenerate curricula despite adaptive generation, suggesting optimal staging is not always achievable with current methods",
            "uuids": [
                "e1092.2",
                "e1033.3"
            ]
        },
        {
            "text": "Depth RL agents trained at very large scale (75M+ steps) eventually matched or exceeded SLAM performance without explicit curriculum, suggesting sufficient scale can substitute for curriculum",
            "uuids": [
                "e1083.0"
            ]
        },
        {
            "text": "DD-PPO achieved near-perfect PointGoal navigation with 2.5B frames without explicit curriculum, indicating massive scale can overcome the need for staged training",
            "uuids": [
                "e1082.0"
            ]
        },
        {
            "text": "PLR (Prioritized Level Replay) achieved strong generalization through adaptive level selection without explicit complexity-variation staging, suggesting replay-based curricula may not require sequential staging",
            "uuids": [
                "e1086.0"
            ]
        },
        {
            "text": "Some tasks showed that increasing variation (e.g., adding more training scenes) improved performance even without explicit complexity staging, suggesting variation can sometimes be beneficial throughout training",
            "uuids": [
                "e1082.0"
            ]
        }
    ],
    "special_cases": [
        "Tasks with no hierarchical structure or prerequisite skills may not benefit from complexity staging and could learn directly from varied environments",
        "Extremely simple tasks may not require curriculum at all and could be learned efficiently through direct training",
        "Tasks with very sparse rewards may require special curriculum designs that ensure some reward signal throughout training, potentially violating standard staging principles",
        "Off-policy methods with large replay buffers may implement implicit curricula that reduce or eliminate the need for explicit staging",
        "In the limit of unlimited data and compute (e.g., billions of training steps), the benefits of staged curricula may diminish or disappear entirely",
        "Tasks where the complexity-variation relationship is non-monotonic (e.g., some variations make complex tasks easier) may require non-standard curriculum structures",
        "Adaptive curricula that maintain intermediate difficulty may be able to increase complexity and variation simultaneously without the drawbacks of naive simultaneous increases",
        "Transfer learning scenarios may have different optimal curricula than learning from scratch, as pretrained representations may already handle some aspects of complexity or variation"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Elman (1993) Learning and development in neural networks: the importance of starting small [Early work on staged learning and starting small principle]",
            "Bengio et al. (2009) Curriculum learning [Foundational curriculum learning work, but does not formalize complexity-variation coordination]",
            "Florensa et al. (2017) Reverse curriculum generation for reinforcement learning [Reverse curriculum for goal-reaching, related to staging from easy to hard]",
            "Narvekar et al. (2020) Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey [Comprehensive survey but does not formalize complexity-variation trade-offs or optimal staging strategies]",
            "Portelas et al. (2020) Teacher algorithms for curriculum learning of deep RL [Discusses automatic curriculum learning but does not formalize complexity-variation coordination]",
            "OpenAI et al. (2019) Solving Rubik's Cube with a Robot Hand [Uses domain randomization and curriculum but does not formalize staging principles]",
            "Akkaya et al. (2019) Solving Rubik's Cube with a Robot Hand [Demonstrates staged curriculum with ADR but does not provide general theory]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
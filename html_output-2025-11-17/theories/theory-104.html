<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Code Generation as Executable Belief and Planning Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-104</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-104</p>
                <p><strong>Name:</strong> Code Generation as Executable Belief and Planning Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how agents perform planning with external tools in partially observable text environments, including belief-state updates that incorporate tool outputs and guide shortest-path actions, based on the following results.</p>
                <p><strong>Description:</strong> In complex reasoning, planning, and action execution tasks, generating executable code (Python, SQL, PDDL, or domain-specific languages) as an intermediate representation provides multiple benefits: (1) a more precise and verifiable belief state than natural language, (2) a compositional planning mechanism that can be verified through execution, and (3) a translation layer between high-level intentions and low-level actions. Code execution by external interpreters serves as both a belief verification mechanism and a planning tool, with execution feedback (including detailed error traces) enabling iterative refinement through debugging. This approach is particularly effective for tasks requiring exact specifications, multi-step procedures, mathematical reasoning, or mapping between natural language and structured action spaces. The effectiveness depends on the availability of reliable execution environments and the ability to express the task in code.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Code generation provides a more precise intermediate representation than natural language for tasks requiring exact specifications, enabling verification through execution.</li>
                <li>External code interpreters (Python, SQL engines, PDDL planners) serve as reliable execution tools that provide deterministic feedback, enabling iterative refinement.</li>
                <li>Code as belief state enables explicit representation of program state (variables, data structures, intermediate values) that is difficult to maintain consistently in natural language.</li>
                <li>Execution feedback, particularly detailed error traces (error type, message, location), provides richer debugging signals than natural language feedback alone.</li>
                <li>Code generation is particularly effective for compositional tasks where complex operations can be built from simpler primitives through function composition.</li>
                <li>Code can serve as a translation layer between high-level natural language intentions and low-level structured action spaces, improving executability.</li>
                <li>The benefit of code generation scales with the precision requirements of the task: tasks requiring exact arithmetic, logical reasoning, or API calls benefit more than open-ended creative tasks.</li>
                <li>Iterative debugging with parsed error traces (rubber duck debugging) is more effective than single-shot code generation or surface-level error feedback.</li>
                <li>Code generation without execution provides limited benefit compared to code generation with execution, as execution provides ground truth verification.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>PAL generating Python programs achieves 72.0% on GSM8K vs 65.6% for CoT, with particular robustness to large numbers (GSM-HARD 61.2% vs CoT 23.3%) <a href="../results/extraction-result-888.html#e888.0" class="evidence-link">[e888.0]</a> </li>
    <li>EHRAgent using code generation + execution achieves 71.58% SR vs 45.33% without code interface; rubber duck debugging with parsed error traces improves SR from 55.00% to 71.58% <a href="../results/extraction-result-799.html#e799.0" class="evidence-link">[e799.0]</a> </li>
    <li>ViperGPT generating Python with vision module calls achieves 72.0% IoU on RefCOCO and 51.9% on OK-VQA, with procedural program state serving as explicit belief <a href="../results/extraction-result-866.html#e866.0" class="evidence-link">[e866.0]</a> </li>
    <li>LLM+P translating to PDDL achieves 90% success on BlocksWorld vs 15-20% for LLM-only approaches, demonstrating value of formal specifications <a href="../results/extraction-result-882.html#e882.0" class="evidence-link">[e882.0]</a> </li>
    <li>Code as Policies generating robot control programs enables compositional task execution in embodied environments <a href="../results/extraction-result-884.html#e884.0" class="evidence-link">[e884.0]</a> </li>
    <li>ConAgents with code generation + review agent achieves 79% SR on RestBench-TMDB; review agent detecting code errors improves success by ~4 percentage points <a href="../results/extraction-result-800.html#e800.0" class="evidence-link">[e800.0]</a> <a href="../results/extraction-result-800.html#e800.2" class="evidence-link">[e800.2]</a> </li>
    <li>Translated LM pipeline using code translation to map free-form LM outputs to admissible actions increases executability from 18% to 78.57% in VirtualHome <a href="../results/extraction-result-867.html#e867.0" class="evidence-link">[e867.0]</a> <a href="../results/extraction-result-877.html#e877.0" class="evidence-link">[e877.0]</a> </li>
    <li>ProAgent using DataAgent with code generation for complex data processing enables workflow automation that rule-based nodes cannot express <a href="../results/extraction-result-787.html#e787.0" class="evidence-link">[e787.0]</a> <a href="../results/extraction-result-787.html#e787.1" class="evidence-link">[e787.1]</a> </li>
    <li>KNOWNO using LLM-generated code with conformal prediction achieves plan success 0.76 with calibrated uncertainty quantification <a href="../results/extraction-result-876.html#e876.0" class="evidence-link">[e876.0]</a> </li>
    <li>Voyager using code generation for Minecraft skills with self-verification enables open-ended exploration and skill accumulation <a href="../results/extraction-result-886.html#e886.1" class="evidence-link">[e886.1]</a> </li>
    <li>DIN-SQL with rule-based SQL correction provides modest gains but is limited by rigid correction rules compared to flexible code-execution approaches <a href="../results/extraction-result-799.html#e799.5" class="evidence-link">[e799.5]</a> </li>
    <li>PAL ablation showing that LLM generating code but also simulating execution (without interpreter) achieves only 23.2% on GSM8K, demonstrating value of actual execution <a href="../results/extraction-result-888.html#e888.0" class="evidence-link">[e888.0]</a> </li>
    <li>External calculator experiments (PaLM, Wei et al.) showed only +2.3% improvement on GSM8K, much smaller than PAL's gains with full Python execution <a href="../results/extraction-result-888.html#e888.2" class="evidence-link">[e888.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An agent that generates code with explicit state variables and assertions will be more debuggable than an agent generating code without explicit state tracking.</li>
                <li>Agents that use static analysis tools (linters, type checkers) in addition to execution will catch more errors before execution than agents using execution alone.</li>
                <li>In multi-step reasoning tasks, agents that generate code with intermediate checkpoints and print statements will be more robust to errors than agents generating monolithic code.</li>
                <li>For tasks requiring interaction with structured APIs, code generation will outperform natural language by a larger margin than for tasks with natural language interfaces.</li>
                <li>Agents that maintain a library of verified code snippets (like Voyager's skill library) will be more sample-efficient than agents generating code from scratch each time.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether learned code generation (without explicit programming language syntax) can match the precision of traditional code generation while maintaining interpretability.</li>
                <li>Whether code generation remains beneficial when LLMs become sufficiently capable at direct reasoning without code, or if there is a fundamental advantage to executable representations.</li>
                <li>Whether there exists a universal intermediate representation (beyond specific programming languages) that works across all task types, or if domain-specific languages are necessary.</li>
                <li>Whether the computational overhead of code execution (parsing, compilation, runtime) becomes prohibitive for real-time interactive agents, or if optimizations can make it practical.</li>
                <li>Whether code generation can be effectively combined with learned neural policies, or if the discrete nature of code conflicts with continuous optimization.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Demonstrating that natural language reasoning performs as well as code generation on tasks requiring exact specifications (e.g., multi-digit arithmetic) would challenge the precision claim.</li>
                <li>Showing that code generation without execution (just generating code as reasoning trace) performs as well as code generation with execution would question the value of execution feedback.</li>
                <li>Finding that code generation provides no benefit on tasks where it should be highly relevant (e.g., mathematical reasoning, API interaction) would challenge the theory's scope.</li>
                <li>Demonstrating that the computational overhead of code execution negates the accuracy benefits in time-constrained settings would challenge practical applicability.</li>
                <li>Showing that agents can achieve the same level of compositional reasoning without code generation would challenge the necessity of code as an intermediate representation.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically determine when to use code generation vs. natural language reasoning for a given task </li>
    <li>The computational costs of code execution and their impact on agent latency, especially for real-time interactive systems </li>
    <li>How to handle cases where the task cannot be easily expressed in code (e.g., subjective judgments, creative tasks) </li>
    <li>The security implications of executing LLM-generated code, including potential for malicious code injection </li>
    <li>How to handle non-deterministic or stochastic environments where code execution may produce different results on repeated runs </li>
    <li>The trade-off between code interpretability and execution efficiency (e.g., verbose code with comments vs. compact optimized code) </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [Program generation for reasoning, demonstrates code execution benefits]</li>
    <li>Liang et al. (2022) Code as Policies: Language Model Programs for Embodied Control [Code generation for robot control, compositional task execution]</li>
    <li>Suris et al. (2023) ViperGPT: Visual Inference via Python Execution for Reasoning [Code generation for vision, procedural program state as belief]</li>
    <li>Chen et al. (2021) Evaluating Large Language Models Trained on Code [Codex and code generation capabilities, foundation for code-generating agents]</li>
    <li>Liu et al. (2023) LLM+P: Empowering Large Language Models with Optimal Planning Proficiency [Translation to PDDL for planning, formal specifications]</li>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [Self-supervised tool use, includes code execution as a tool]</li>
    <li>Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models [Interleaving reasoning and acting, alternative to code generation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Code Generation as Executable Belief and Planning Theory",
    "theory_description": "In complex reasoning, planning, and action execution tasks, generating executable code (Python, SQL, PDDL, or domain-specific languages) as an intermediate representation provides multiple benefits: (1) a more precise and verifiable belief state than natural language, (2) a compositional planning mechanism that can be verified through execution, and (3) a translation layer between high-level intentions and low-level actions. Code execution by external interpreters serves as both a belief verification mechanism and a planning tool, with execution feedback (including detailed error traces) enabling iterative refinement through debugging. This approach is particularly effective for tasks requiring exact specifications, multi-step procedures, mathematical reasoning, or mapping between natural language and structured action spaces. The effectiveness depends on the availability of reliable execution environments and the ability to express the task in code.",
    "supporting_evidence": [
        {
            "text": "PAL generating Python programs achieves 72.0% on GSM8K vs 65.6% for CoT, with particular robustness to large numbers (GSM-HARD 61.2% vs CoT 23.3%)",
            "uuids": [
                "e888.0"
            ]
        },
        {
            "text": "EHRAgent using code generation + execution achieves 71.58% SR vs 45.33% without code interface; rubber duck debugging with parsed error traces improves SR from 55.00% to 71.58%",
            "uuids": [
                "e799.0"
            ]
        },
        {
            "text": "ViperGPT generating Python with vision module calls achieves 72.0% IoU on RefCOCO and 51.9% on OK-VQA, with procedural program state serving as explicit belief",
            "uuids": [
                "e866.0"
            ]
        },
        {
            "text": "LLM+P translating to PDDL achieves 90% success on BlocksWorld vs 15-20% for LLM-only approaches, demonstrating value of formal specifications",
            "uuids": [
                "e882.0"
            ]
        },
        {
            "text": "Code as Policies generating robot control programs enables compositional task execution in embodied environments",
            "uuids": [
                "e884.0"
            ]
        },
        {
            "text": "ConAgents with code generation + review agent achieves 79% SR on RestBench-TMDB; review agent detecting code errors improves success by ~4 percentage points",
            "uuids": [
                "e800.0",
                "e800.2"
            ]
        },
        {
            "text": "Translated LM pipeline using code translation to map free-form LM outputs to admissible actions increases executability from 18% to 78.57% in VirtualHome",
            "uuids": [
                "e867.0",
                "e877.0"
            ]
        },
        {
            "text": "ProAgent using DataAgent with code generation for complex data processing enables workflow automation that rule-based nodes cannot express",
            "uuids": [
                "e787.0",
                "e787.1"
            ]
        },
        {
            "text": "KNOWNO using LLM-generated code with conformal prediction achieves plan success 0.76 with calibrated uncertainty quantification",
            "uuids": [
                "e876.0"
            ]
        },
        {
            "text": "Voyager using code generation for Minecraft skills with self-verification enables open-ended exploration and skill accumulation",
            "uuids": [
                "e886.1"
            ]
        },
        {
            "text": "DIN-SQL with rule-based SQL correction provides modest gains but is limited by rigid correction rules compared to flexible code-execution approaches",
            "uuids": [
                "e799.5"
            ]
        },
        {
            "text": "PAL ablation showing that LLM generating code but also simulating execution (without interpreter) achieves only 23.2% on GSM8K, demonstrating value of actual execution",
            "uuids": [
                "e888.0"
            ]
        },
        {
            "text": "External calculator experiments (PaLM, Wei et al.) showed only +2.3% improvement on GSM8K, much smaller than PAL's gains with full Python execution",
            "uuids": [
                "e888.2"
            ]
        }
    ],
    "theory_statements": [
        "Code generation provides a more precise intermediate representation than natural language for tasks requiring exact specifications, enabling verification through execution.",
        "External code interpreters (Python, SQL engines, PDDL planners) serve as reliable execution tools that provide deterministic feedback, enabling iterative refinement.",
        "Code as belief state enables explicit representation of program state (variables, data structures, intermediate values) that is difficult to maintain consistently in natural language.",
        "Execution feedback, particularly detailed error traces (error type, message, location), provides richer debugging signals than natural language feedback alone.",
        "Code generation is particularly effective for compositional tasks where complex operations can be built from simpler primitives through function composition.",
        "Code can serve as a translation layer between high-level natural language intentions and low-level structured action spaces, improving executability.",
        "The benefit of code generation scales with the precision requirements of the task: tasks requiring exact arithmetic, logical reasoning, or API calls benefit more than open-ended creative tasks.",
        "Iterative debugging with parsed error traces (rubber duck debugging) is more effective than single-shot code generation or surface-level error feedback.",
        "Code generation without execution provides limited benefit compared to code generation with execution, as execution provides ground truth verification."
    ],
    "new_predictions_likely": [
        "An agent that generates code with explicit state variables and assertions will be more debuggable than an agent generating code without explicit state tracking.",
        "Agents that use static analysis tools (linters, type checkers) in addition to execution will catch more errors before execution than agents using execution alone.",
        "In multi-step reasoning tasks, agents that generate code with intermediate checkpoints and print statements will be more robust to errors than agents generating monolithic code.",
        "For tasks requiring interaction with structured APIs, code generation will outperform natural language by a larger margin than for tasks with natural language interfaces.",
        "Agents that maintain a library of verified code snippets (like Voyager's skill library) will be more sample-efficient than agents generating code from scratch each time."
    ],
    "new_predictions_unknown": [
        "Whether learned code generation (without explicit programming language syntax) can match the precision of traditional code generation while maintaining interpretability.",
        "Whether code generation remains beneficial when LLMs become sufficiently capable at direct reasoning without code, or if there is a fundamental advantage to executable representations.",
        "Whether there exists a universal intermediate representation (beyond specific programming languages) that works across all task types, or if domain-specific languages are necessary.",
        "Whether the computational overhead of code execution (parsing, compilation, runtime) becomes prohibitive for real-time interactive agents, or if optimizations can make it practical.",
        "Whether code generation can be effectively combined with learned neural policies, or if the discrete nature of code conflicts with continuous optimization."
    ],
    "negative_experiments": [
        "Demonstrating that natural language reasoning performs as well as code generation on tasks requiring exact specifications (e.g., multi-digit arithmetic) would challenge the precision claim.",
        "Showing that code generation without execution (just generating code as reasoning trace) performs as well as code generation with execution would question the value of execution feedback.",
        "Finding that code generation provides no benefit on tasks where it should be highly relevant (e.g., mathematical reasoning, API interaction) would challenge the theory's scope.",
        "Demonstrating that the computational overhead of code execution negates the accuracy benefits in time-constrained settings would challenge practical applicability.",
        "Showing that agents can achieve the same level of compositional reasoning without code generation would challenge the necessity of code as an intermediate representation."
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically determine when to use code generation vs. natural language reasoning for a given task",
            "uuids": []
        },
        {
            "text": "The computational costs of code execution and their impact on agent latency, especially for real-time interactive systems",
            "uuids": []
        },
        {
            "text": "How to handle cases where the task cannot be easily expressed in code (e.g., subjective judgments, creative tasks)",
            "uuids": []
        },
        {
            "text": "The security implications of executing LLM-generated code, including potential for malicious code injection",
            "uuids": []
        },
        {
            "text": "How to handle non-deterministic or stochastic environments where code execution may produce different results on repeated runs",
            "uuids": []
        },
        {
            "text": "The trade-off between code interpretability and execution efficiency (e.g., verbose code with comments vs. compact optimized code)",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLM agents achieve strong performance with pure natural language reasoning without code generation (e.g., Introspective Tips achieving &gt;0.9 success rate in TextWorld)",
            "uuids": [
                "e802.0"
            ]
        },
        {
            "text": "Code generation can introduce brittleness if the generated code has syntax errors or runtime failures, requiring additional error handling",
            "uuids": []
        },
        {
            "text": "For some tasks (e.g., open-ended dialogue, creative writing), code generation may not be applicable or may constrain the solution space",
            "uuids": []
        },
        {
            "text": "Translated LM pipeline shows that code translation can reduce semantic correctness (Codex correctness drops from 64.87% to 54.88%) even while improving executability",
            "uuids": [
                "e867.0",
                "e877.0"
            ]
        },
        {
            "text": "ReAct-style agents can achieve reasonable performance with natural language action traces without code generation in some domains",
            "uuids": [
                "e889.2",
                "e889.4"
            ]
        }
    ],
    "special_cases": [
        "For tasks with ambiguous or underspecified requirements, code generation may be too rigid and natural language may be more appropriate for exploring solution spaces.",
        "When execution environments are unavailable, expensive, or unsafe (e.g., production systems), code generation without execution may provide limited benefit.",
        "For tasks requiring creativity, open-ended generation, or subjective judgment, code may be too constraining and natural language may be more expressive.",
        "In domains where the action space is naturally expressed in natural language (e.g., text-based games with parser interfaces), code generation may add unnecessary complexity.",
        "For real-time interactive systems, the latency of code execution (parsing, compilation, runtime) may be prohibitive, favoring direct action generation.",
        "When the task requires handling exceptions or edge cases that are difficult to enumerate in code, natural language reasoning may be more robust.",
        "For tasks where the correct solution requires violating typical programming conventions (e.g., intentionally inefficient algorithms for pedagogical purposes), code generation may be biased toward standard patterns."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Gao et al. (2022) PAL: Program-aided Language Models [Program generation for reasoning, demonstrates code execution benefits]",
            "Liang et al. (2022) Code as Policies: Language Model Programs for Embodied Control [Code generation for robot control, compositional task execution]",
            "Suris et al. (2023) ViperGPT: Visual Inference via Python Execution for Reasoning [Code generation for vision, procedural program state as belief]",
            "Chen et al. (2021) Evaluating Large Language Models Trained on Code [Codex and code generation capabilities, foundation for code-generating agents]",
            "Liu et al. (2023) LLM+P: Empowering Large Language Models with Optimal Planning Proficiency [Translation to PDDL for planning, formal specifications]",
            "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [Self-supervised tool use, includes code execution as a tool]",
            "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models [Interleaving reasoning and acting, alternative to code generation]"
        ]
    },
    "reflected_from_theory_index": 7,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
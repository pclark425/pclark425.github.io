<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Context-Evidence Sufficiency Theory for Software Artifact Evaluation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-364</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-364</p>
                <p><strong>Name:</strong> Context-Evidence Sufficiency Theory for Software Artifact Evaluation</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about alignment between proxy evaluations (LLM-as-a-judge, Likert-style) and expert human review for software development artifacts, including necessary conditions for high agreement.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that alignment between LLM-based proxy evaluations and expert human review of software artifacts is primarily determined by whether sufficient context and evidence are available to make the evaluation decision using explicit reasoning alone. The theory posits that evaluation tasks exist on a continuum of context-evidence requirements, ranging from 'evidence-complete' tasks (where all necessary information can be explicitly provided) to 'evidence-incomplete' tasks (where critical information is unavailable, implicit, or requires experiential knowledge to interpret). High LLM-human alignment (r > 0.7) occurs when: (1) all decision-relevant evidence can be explicitly represented in the evaluation context, (2) the evaluation criteria can be articulated as checkable conditions, and (3) the relationship between evidence and judgment can be expressed through logical or pattern-based reasoning. Alignment degrades proportionally to the degree of evidence incompleteness, with three primary sources of insufficiency: missing contextual information (e.g., organizational standards, historical decisions), implicit evidence requiring domain experience to recognize (e.g., code smells, architectural anti-patterns in novel contexts), and evidence requiring embodied interaction (e.g., performance characteristics, user experience qualities). The theory predicts that systematic augmentation of context and evidence can shift tasks along the continuum toward higher alignment, but with diminishing returns as tasks approach fundamental limits of explicit representation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>LLM-human alignment for software artifact evaluation is a continuous function of context-evidence sufficiency, not a binary or categorical property</li>
                <li>For any evaluation task, there exists a minimum sufficient evidence set (MSES) such that providing this evidence enables high alignment (r > 0.7) between LLM and expert judgments</li>
                <li>The MSES varies by task type: syntactic/structural evaluations have smaller MSES than semantic/architectural evaluations, which have smaller MSES than pragmatic/contextual evaluations</li>
                <li>When provided context falls below 70% of the MSES, alignment degrades approximately linearly with missing evidence; when above 70%, alignment shows logarithmic improvement approaching an asymptote</li>
                <li>Three categories of evidence contribute independently to sufficiency: artifact-intrinsic evidence (code structure, documentation), artifact-extrinsic evidence (requirements, constraints, standards), and evaluation-specific evidence (criteria, examples, edge cases)</li>
                <li>Expert-expert agreement provides an upper bound on achievable LLM-expert alignment for a given evidence set; when experts disagree due to insufficient evidence, LLMs cannot exceed this agreement level</li>
                <li>Tasks requiring evidence that cannot be represented in text (e.g., runtime behavior, user interaction patterns) show systematically lower alignment unless this evidence is explicitly captured and provided</li>
                <li>The sufficiency threshold is domain-dependent: domains with well-codified practices (e.g., style checking) have lower evidence requirements than domains with emergent practices (e.g., novel framework evaluation)</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Expert evaluation quality depends critically on access to relevant contextual information and evidence about the artifact being evaluated </li>
    <li>Software quality assessment requires consideration of multiple contextual factors including project history, team conventions, and organizational standards that may not be explicitly documented </li>
    <li>LLMs perform evaluation tasks more accurately when provided with explicit criteria, examples, and relevant context in prompts </li>
    <li>Expert-novice differences in software evaluation often stem from experts' ability to access and apply relevant contextual knowledge and recognize implicit patterns </li>
    <li>Evaluation agreement increases when evaluators have access to the same information and use explicit, shared criteria </li>
    <li>Software artifacts contain both explicit properties (syntax, structure) and implicit properties (design intent, quality attributes) that require different types of evidence to evaluate </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Providing LLM evaluators with explicit project coding standards and architectural decision records will increase alignment with expert reviews by 0.15-0.25 correlation points for architectural appropriateness judgments</li>
                <li>For code review tasks, alignment will be highest (r > 0.85) for style/formatting issues, moderate (r = 0.6-0.75) for bug detection, and lowest (r = 0.4-0.6) for design quality assessment when standard context is provided</li>
                <li>Systematically augmenting evaluation prompts with relevant examples of good/bad instances will show diminishing returns: first 3 examples improve alignment substantially, next 3 show moderate improvement, additional examples show minimal improvement</li>
                <li>Inter-expert agreement will correlate strongly (r > 0.75) with LLM-expert alignment across different evaluation tasks, with LLM-expert alignment typically 0.1-0.2 points lower than expert-expert agreement</li>
                <li>Evaluation tasks that can be decomposed into explicit checklists will show higher LLM-human alignment than holistic judgment tasks, even when both are provided with identical context</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether providing LLMs with interactive access to development environments (to gather additional evidence on demand) can overcome fundamental evidence insufficiency limitations is unknown</li>
                <li>Whether fine-tuning LLMs on expert evaluation rationales (not just judgments) can enable them to identify and request missing evidence, thereby improving alignment on evidence-incomplete tasks, is unclear</li>
                <li>Whether there exist software evaluation tasks where the MSES is fundamentally impossible to represent in textual form, creating an absolute ceiling on LLM-human alignment, is unknown</li>
                <li>Whether hybrid evaluation systems that combine LLM pattern recognition with targeted human evidence gathering can achieve super-expert performance by more efficiently identifying evidence gaps is unclear</li>
                <li>Whether the evidence sufficiency requirements change over time as software practices evolve and become more or less codified is not well understood</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If providing comprehensive context (all available project documentation, standards, and examples) does not improve LLM-human alignment compared to minimal context, this would challenge the evidence sufficiency premise</li>
                <li>If LLM-human alignment is equally high for tasks where experts report having insufficient information as for tasks where experts report having complete information, this would invalidate the theory</li>
                <li>If expert-expert agreement and LLM-expert agreement are uncorrelated across different evaluation tasks, this would question the role of evidence sufficiency in determining alignment</li>
                <li>If tasks with smaller MSES (e.g., syntax checking) do not show higher alignment than tasks with larger MSES (e.g., architecture evaluation) when appropriate evidence is provided, this would challenge the theory's predictions</li>
                <li>If systematically removing specific types of evidence from evaluation context does not predictably decrease alignment, this would question the theory's model of evidence contribution</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to objectively measure or quantify the MSES for a given evaluation task </li>
    <li>Individual differences among experts in their evidence requirements and interpretation strategies are not explicitly modeled </li>
    <li>The role of evaluator confidence and uncertainty in relation to evidence sufficiency is not addressed </li>
    <li>How evidence sufficiency interacts with evaluation scale (Likert vs. binary vs. continuous) is not specified </li>
    <li>The theory does not address how conflicting or contradictory evidence affects alignment </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Cicchetti (1994) Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology [Discusses inter-rater reliability but not in context of LLM-human alignment or evidence sufficiency]</li>
    <li>Hallgren (2012) Computing inter-rater reliability for observational data: an overview and tutorial [Covers agreement measurement but not the theoretical basis for alignment in terms of evidence]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Demonstrates importance of context for LLM reasoning but does not propose a theory of evidence sufficiency for evaluation alignment]</li>
    <li>Bacchelli & Bird (2013) Expectations, outcomes, and challenges of modern code review [Discusses factors affecting code review quality but not LLM-human alignment theory]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Context-Evidence Sufficiency Theory for Software Artifact Evaluation",
    "theory_description": "This theory proposes that alignment between LLM-based proxy evaluations and expert human review of software artifacts is primarily determined by whether sufficient context and evidence are available to make the evaluation decision using explicit reasoning alone. The theory posits that evaluation tasks exist on a continuum of context-evidence requirements, ranging from 'evidence-complete' tasks (where all necessary information can be explicitly provided) to 'evidence-incomplete' tasks (where critical information is unavailable, implicit, or requires experiential knowledge to interpret). High LLM-human alignment (r &gt; 0.7) occurs when: (1) all decision-relevant evidence can be explicitly represented in the evaluation context, (2) the evaluation criteria can be articulated as checkable conditions, and (3) the relationship between evidence and judgment can be expressed through logical or pattern-based reasoning. Alignment degrades proportionally to the degree of evidence incompleteness, with three primary sources of insufficiency: missing contextual information (e.g., organizational standards, historical decisions), implicit evidence requiring domain experience to recognize (e.g., code smells, architectural anti-patterns in novel contexts), and evidence requiring embodied interaction (e.g., performance characteristics, user experience qualities). The theory predicts that systematic augmentation of context and evidence can shift tasks along the continuum toward higher alignment, but with diminishing returns as tasks approach fundamental limits of explicit representation.",
    "supporting_evidence": [
        {
            "text": "Expert evaluation quality depends critically on access to relevant contextual information and evidence about the artifact being evaluated",
            "citations": [
                "Bacchelli & Bird (2013) Expectations, outcomes, and challenges of modern code review",
                "Rigby & Bird (2013) Convergent contemporary software peer review practices"
            ]
        },
        {
            "text": "Software quality assessment requires consideration of multiple contextual factors including project history, team conventions, and organizational standards that may not be explicitly documented",
            "citations": [
                "Seaman (1999) Qualitative methods in empirical studies of software engineering",
                "Ko et al. (2015) The state of the art in end-user software engineering"
            ]
        },
        {
            "text": "LLMs perform evaluation tasks more accurately when provided with explicit criteria, examples, and relevant context in prompts",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Brown et al. (2020) Language Models are Few-Shot Learners"
            ]
        },
        {
            "text": "Expert-novice differences in software evaluation often stem from experts' ability to access and apply relevant contextual knowledge and recognize implicit patterns",
            "citations": [
                "Adelson (1981) Problem solving and the development of abstract categories in programming languages",
                "Sonnentag (1998) Expertise in professional software design: a process study"
            ]
        },
        {
            "text": "Evaluation agreement increases when evaluators have access to the same information and use explicit, shared criteria",
            "citations": [
                "Cicchetti (1994) Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology",
                "Hallgren (2012) Computing inter-rater reliability for observational data: an overview and tutorial"
            ]
        },
        {
            "text": "Software artifacts contain both explicit properties (syntax, structure) and implicit properties (design intent, quality attributes) that require different types of evidence to evaluate",
            "citations": [
                "Parnas & Clements (1986) A rational design process: How and why to fake it",
                "Bass et al. (2003) Software Architecture in Practice"
            ]
        }
    ],
    "theory_statements": [
        "LLM-human alignment for software artifact evaluation is a continuous function of context-evidence sufficiency, not a binary or categorical property",
        "For any evaluation task, there exists a minimum sufficient evidence set (MSES) such that providing this evidence enables high alignment (r &gt; 0.7) between LLM and expert judgments",
        "The MSES varies by task type: syntactic/structural evaluations have smaller MSES than semantic/architectural evaluations, which have smaller MSES than pragmatic/contextual evaluations",
        "When provided context falls below 70% of the MSES, alignment degrades approximately linearly with missing evidence; when above 70%, alignment shows logarithmic improvement approaching an asymptote",
        "Three categories of evidence contribute independently to sufficiency: artifact-intrinsic evidence (code structure, documentation), artifact-extrinsic evidence (requirements, constraints, standards), and evaluation-specific evidence (criteria, examples, edge cases)",
        "Expert-expert agreement provides an upper bound on achievable LLM-expert alignment for a given evidence set; when experts disagree due to insufficient evidence, LLMs cannot exceed this agreement level",
        "Tasks requiring evidence that cannot be represented in text (e.g., runtime behavior, user interaction patterns) show systematically lower alignment unless this evidence is explicitly captured and provided",
        "The sufficiency threshold is domain-dependent: domains with well-codified practices (e.g., style checking) have lower evidence requirements than domains with emergent practices (e.g., novel framework evaluation)"
    ],
    "new_predictions_likely": [
        "Providing LLM evaluators with explicit project coding standards and architectural decision records will increase alignment with expert reviews by 0.15-0.25 correlation points for architectural appropriateness judgments",
        "For code review tasks, alignment will be highest (r &gt; 0.85) for style/formatting issues, moderate (r = 0.6-0.75) for bug detection, and lowest (r = 0.4-0.6) for design quality assessment when standard context is provided",
        "Systematically augmenting evaluation prompts with relevant examples of good/bad instances will show diminishing returns: first 3 examples improve alignment substantially, next 3 show moderate improvement, additional examples show minimal improvement",
        "Inter-expert agreement will correlate strongly (r &gt; 0.75) with LLM-expert alignment across different evaluation tasks, with LLM-expert alignment typically 0.1-0.2 points lower than expert-expert agreement",
        "Evaluation tasks that can be decomposed into explicit checklists will show higher LLM-human alignment than holistic judgment tasks, even when both are provided with identical context"
    ],
    "new_predictions_unknown": [
        "Whether providing LLMs with interactive access to development environments (to gather additional evidence on demand) can overcome fundamental evidence insufficiency limitations is unknown",
        "Whether fine-tuning LLMs on expert evaluation rationales (not just judgments) can enable them to identify and request missing evidence, thereby improving alignment on evidence-incomplete tasks, is unclear",
        "Whether there exist software evaluation tasks where the MSES is fundamentally impossible to represent in textual form, creating an absolute ceiling on LLM-human alignment, is unknown",
        "Whether hybrid evaluation systems that combine LLM pattern recognition with targeted human evidence gathering can achieve super-expert performance by more efficiently identifying evidence gaps is unclear",
        "Whether the evidence sufficiency requirements change over time as software practices evolve and become more or less codified is not well understood"
    ],
    "negative_experiments": [
        "If providing comprehensive context (all available project documentation, standards, and examples) does not improve LLM-human alignment compared to minimal context, this would challenge the evidence sufficiency premise",
        "If LLM-human alignment is equally high for tasks where experts report having insufficient information as for tasks where experts report having complete information, this would invalidate the theory",
        "If expert-expert agreement and LLM-expert agreement are uncorrelated across different evaluation tasks, this would question the role of evidence sufficiency in determining alignment",
        "If tasks with smaller MSES (e.g., syntax checking) do not show higher alignment than tasks with larger MSES (e.g., architecture evaluation) when appropriate evidence is provided, this would challenge the theory's predictions",
        "If systematically removing specific types of evidence from evaluation context does not predictably decrease alignment, this would question the theory's model of evidence contribution"
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to objectively measure or quantify the MSES for a given evaluation task",
            "citations": []
        },
        {
            "text": "Individual differences among experts in their evidence requirements and interpretation strategies are not explicitly modeled",
            "citations": []
        },
        {
            "text": "The role of evaluator confidence and uncertainty in relation to evidence sufficiency is not addressed",
            "citations": []
        },
        {
            "text": "How evidence sufficiency interacts with evaluation scale (Likert vs. binary vs. continuous) is not specified",
            "citations": []
        },
        {
            "text": "The theory does not address how conflicting or contradictory evidence affects alignment",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some research suggests that expert intuition involves pattern recognition that operates on implicit cues not easily made explicit, which may not be fully addressable through context augmentation",
            "citations": [
                "Klein (1998) Sources of Power: How People Make Decisions",
                "Dreyfus & Dreyfus (1986) Mind over Machine: The Power of Human Intuition"
            ]
        },
        {
            "text": "Studies of tacit knowledge suggest that some expert knowledge cannot be fully articulated or transferred through explicit instruction, potentially limiting the effectiveness of context provision",
            "citations": [
                "Polanyi (1966) The Tacit Dimension",
                "Collins (2010) Tacit and Explicit Knowledge"
            ]
        },
        {
            "text": "Research on LLM limitations suggests that some reasoning capabilities may be fundamentally constrained by architecture rather than just available evidence",
            "citations": [
                "Marcus (2020) The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",
                "Bender et al. (2021) On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
            ]
        }
    ],
    "special_cases": [
        "For highly standardized evaluation tasks with comprehensive rubrics (e.g., automated testing coverage, style guide compliance), the MSES may be very small and alignment can approach perfect agreement (r &gt; 0.95)",
        "For creative or aesthetic judgments (e.g., API design elegance, code readability), evidence sufficiency may be fundamentally limited by subjective preferences, creating a lower ceiling on achievable alignment (r &lt; 0.6)",
        "For security-critical evaluations, the MSES may include threat models and attack scenarios that are difficult to comprehensively specify, limiting alignment even with extensive context",
        "For evaluations of novel technologies or frameworks with limited precedent, the MSES may be undefined or unstable, leading to high variance in alignment across similar tasks",
        "For evaluations requiring runtime or performance evidence, alignment depends critically on whether this evidence can be captured and represented textually (e.g., profiling data, logs)",
        "In domains with rapidly evolving best practices (e.g., modern web frameworks), the MSES may change over time, requiring continuous updating of evaluation context",
        "For evaluations involving organizational-specific context (e.g., team conventions, legacy system constraints), the MSES may be large and difficult to fully capture, systematically limiting alignment in practice"
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Cicchetti (1994) Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology [Discusses inter-rater reliability but not in context of LLM-human alignment or evidence sufficiency]",
            "Hallgren (2012) Computing inter-rater reliability for observational data: an overview and tutorial [Covers agreement measurement but not the theoretical basis for alignment in terms of evidence]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Demonstrates importance of context for LLM reasoning but does not propose a theory of evidence sufficiency for evaluation alignment]",
            "Bacchelli & Bird (2013) Expectations, outcomes, and challenges of modern code review [Discusses factors affecting code review quality but not LLM-human alignment theory]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "theory_query": "Build a theory about alignment between proxy evaluations (LLM-as-a-judge, Likert-style) and expert human review for software development artifacts, including necessary conditions for high agreement.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-204",
    "original_theory_name": "Context-Evidence Sufficiency Theory for Software Artifact Evaluation",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
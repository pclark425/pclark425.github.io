<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Computation in Language Models for Logical Reasoning - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1094</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1094</p>
                <p><strong>Name:</strong> Dual-Process Computation in Language Models for Logical Reasoning</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models (LMs) possess two distinct computational pathways: a fast, heuristic-driven pathway optimized for fluent language generation, and a slower, more systematic pathway capable of strict logical reasoning. The activation of the logical pathway depends on both the input structure and internal model representations, and can be modulated by training, architecture, and prompt design.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Dual-Process Pathway Selection (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; receives &#8594; input<span style="color: #888888;">, and</span></div>
        <div>&#8226; input &#8594; contains &#8594; features indicative of logical reasoning (e.g., formal symbols, explicit logical connectives, meta-instructions)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; activates &#8594; systematic/logical pathway</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show LMs perform better on logic tasks when prompted with formal language or explicit instructions. </li>
    <li>Attention and activation analyses reveal distinct patterns for logic tasks versus narrative tasks. </li>
    <li>Cognitive science literature on dual-process theory (Kahneman, 2011) supports the existence of fast/slow reasoning modes in humans, which may be mirrored in LMs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While dual-process theory is established in humans, its application to LMs as a computational mechanism is novel.</p>            <p><strong>What Already Exists:</strong> Dual-process theories are well-established in cognitive science; LMs are known to perform better on logic tasks with explicit prompts.</p>            <p><strong>What is Novel:</strong> The explicit mapping of dual-process theory to LM internal computation and the claim that pathway selection is dynamically modulated by input features.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory in humans]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]</li>
    <li>Srivastava et al. (2022) Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [LMs' reasoning capabilities]</li>
</ul>
            <h3>Statement 1: Training-Dependent Pathway Optimization (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_trained_on &#8594; datasets with explicit logical structure and reasoning tasks</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; systematic/logical pathway &#8594; is_optimized_for &#8594; strict logical reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Fine-tuning LMs on logic-heavy datasets improves logical reasoning performance. </li>
    <li>Curriculum learning with increasing logical complexity leads to better generalization on logic tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect of training on reasoning is known, but the pathway-specific optimization is a novel hypothesis.</p>            <p><strong>What Already Exists:</strong> Fine-tuning and curriculum learning are known to improve task-specific performance.</p>            <p><strong>What is Novel:</strong> The claim that such training specifically optimizes a distinct logical pathway within the LM.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [fine-tuning and task adaptation]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting and training for reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs will show improved logical reasoning when prompted with formal logic or meta-instructions, compared to natural language prompts.</li>
                <li>Fine-tuning LMs on logic-heavy datasets will result in measurable changes in internal activations during logic tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Repeated activation of the logical pathway may lead to persistent changes in LM representations, even for unrelated tasks.</li>
                <li>Architectural modifications that explicitly separate heuristic and logical pathways may further enhance logical reasoning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs do not show distinct activation patterns for logic tasks versus narrative tasks, the dual-process hypothesis is challenged.</li>
                <li>If training on logic-heavy datasets does not improve logical reasoning, the pathway optimization law is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs may lack sufficient capacity or architectural features to support distinct pathways. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes existing ideas but introduces a novel computational framework for LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory in humans]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [task adaptation in LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Computation in Language Models for Logical Reasoning",
    "theory_description": "This theory posits that language models (LMs) possess two distinct computational pathways: a fast, heuristic-driven pathway optimized for fluent language generation, and a slower, more systematic pathway capable of strict logical reasoning. The activation of the logical pathway depends on both the input structure and internal model representations, and can be modulated by training, architecture, and prompt design.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Dual-Process Pathway Selection",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "receives",
                        "object": "input"
                    },
                    {
                        "subject": "input",
                        "relation": "contains",
                        "object": "features indicative of logical reasoning (e.g., formal symbols, explicit logical connectives, meta-instructions)"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "activates",
                        "object": "systematic/logical pathway"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show LMs perform better on logic tasks when prompted with formal language or explicit instructions.",
                        "uuids": []
                    },
                    {
                        "text": "Attention and activation analyses reveal distinct patterns for logic tasks versus narrative tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Cognitive science literature on dual-process theory (Kahneman, 2011) supports the existence of fast/slow reasoning modes in humans, which may be mirrored in LMs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Dual-process theories are well-established in cognitive science; LMs are known to perform better on logic tasks with explicit prompts.",
                    "what_is_novel": "The explicit mapping of dual-process theory to LM internal computation and the claim that pathway selection is dynamically modulated by input features.",
                    "classification_explanation": "While dual-process theory is established in humans, its application to LMs as a computational mechanism is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kahneman (2011) Thinking, Fast and Slow [dual-process theory in humans]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]",
                        "Srivastava et al. (2022) Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [LMs' reasoning capabilities]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Training-Dependent Pathway Optimization",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_trained_on",
                        "object": "datasets with explicit logical structure and reasoning tasks"
                    }
                ],
                "then": [
                    {
                        "subject": "systematic/logical pathway",
                        "relation": "is_optimized_for",
                        "object": "strict logical reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Fine-tuning LMs on logic-heavy datasets improves logical reasoning performance.",
                        "uuids": []
                    },
                    {
                        "text": "Curriculum learning with increasing logical complexity leads to better generalization on logic tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Fine-tuning and curriculum learning are known to improve task-specific performance.",
                    "what_is_novel": "The claim that such training specifically optimizes a distinct logical pathway within the LM.",
                    "classification_explanation": "The effect of training on reasoning is known, but the pathway-specific optimization is a novel hypothesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [fine-tuning and task adaptation]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting and training for reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs will show improved logical reasoning when prompted with formal logic or meta-instructions, compared to natural language prompts.",
        "Fine-tuning LMs on logic-heavy datasets will result in measurable changes in internal activations during logic tasks."
    ],
    "new_predictions_unknown": [
        "Repeated activation of the logical pathway may lead to persistent changes in LM representations, even for unrelated tasks.",
        "Architectural modifications that explicitly separate heuristic and logical pathways may further enhance logical reasoning."
    ],
    "negative_experiments": [
        "If LMs do not show distinct activation patterns for logic tasks versus narrative tasks, the dual-process hypothesis is challenged.",
        "If training on logic-heavy datasets does not improve logical reasoning, the pathway optimization law is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs may lack sufficient capacity or architectural features to support distinct pathways.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Small or undertrained LMs often fail to improve on logic tasks even with explicit prompts or training.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with ambiguous or informal logic may not trigger the logical pathway.",
        "Multilingual or code-mixed inputs may disrupt pathway selection."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process theory in humans; prompt engineering and fine-tuning for reasoning in LMs.",
        "what_is_novel": "Explicit mapping of dual-process computation to LMs and the claim of dynamic pathway selection and optimization.",
        "classification_explanation": "The theory synthesizes existing ideas but introduces a novel computational framework for LMs.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kahneman (2011) Thinking, Fast and Slow [dual-process theory in humans]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [task adaptation in LMs]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-601",
    "original_theory_name": "Neuro-Symbolic Interface Bottleneck Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
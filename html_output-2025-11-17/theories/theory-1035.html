<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Representation and Modular Reasoning in LLMs for Spatial Puzzle Solving - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1035</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1035</p>
                <p><strong>Name:</strong> Hierarchical Representation and Modular Reasoning in LLMs for Spatial Puzzle Solving</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.</p>
                <p><strong>Description:</strong> This theory proposes that LLMs internally construct hierarchical, modular representations of spatial puzzles, enabling them to decompose complex problems into subproblems and apply modular reasoning strategies. These representations are shaped by the model's exposure to hierarchical and compositional structures in language, allowing for flexible adaptation to new spatial puzzles.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Decomposition of Spatial Puzzles (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is exposed_to &#8594; spatial puzzle in text form<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; has inductive bias &#8594; for hierarchical structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; constructs &#8594; internal hierarchical representation of puzzle<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; decomposes &#8594; puzzle into subproblems (e.g., rows, columns, blocks in Sudoku)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can be prompted to solve Sudoku by row, column, or block, indicating internal decomposition. </li>
    <li>Hierarchical reasoning is observed in LLMs for other tasks, such as code generation and story understanding. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends known hierarchical reasoning in LLMs to the spatial puzzle domain, which is a novel application.</p>            <p><strong>What Already Exists:</strong> Hierarchical and modular reasoning in LLMs has been observed in language and code tasks.</p>            <p><strong>What is Novel:</strong> Application of hierarchical decomposition to spatial puzzle solving in LLMs is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Akyürek et al. (2022) What Learning Algorithm Is In-Context Learning? [LLMs learn modular, compositional reasoning]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [LLMs can decompose problems stepwise]</li>
</ul>
            <h3>Statement 1: Modular Application of Reasoning Strategies (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; has hierarchical representation &#8594; of spatial puzzle<span style="color: #888888;">, and</span></div>
        <div>&#8226; subproblem &#8594; is identified &#8594; within puzzle structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; applies &#8594; modular reasoning strategies to subproblems<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; integrates &#8594; subproblem solutions into global solution</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can solve individual rows or blocks in Sudoku and then combine results for the full solution. </li>
    <li>Modular reasoning is a hallmark of human and algorithmic puzzle solving. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is an extension of modular reasoning to a new domain, with new predictions.</p>            <p><strong>What Already Exists:</strong> Modular reasoning is well-studied in cognitive science and some LLM tasks.</p>            <p><strong>What is Novel:</strong> The explicit claim that LLMs use modular reasoning for spatial puzzles is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building machines that learn and think like people [Modular, compositional reasoning in cognition]</li>
    <li>Akyürek et al. (2022) What Learning Algorithm Is In-Context Learning? [LLMs learn modular, compositional reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will perform better on spatial puzzles when prompted to solve subproblems (e.g., rows, columns) before integrating the solution.</li>
                <li>LLMs will show transfer learning: training on one type of spatial puzzle will improve performance on structurally similar puzzles.</li>
                <li>LLMs will be able to explain their reasoning in terms of subproblem decomposition when prompted.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may develop novel, non-human modular strategies for spatial puzzles that outperform classical approaches.</li>
                <li>Hierarchical representations in LLMs may enable zero-shot generalization to entirely new classes of spatial puzzles.</li>
                <li>LLMs may be able to discover and exploit latent symmetries in spatial puzzles without explicit instruction.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs cannot be prompted to solve subproblems or show no performance gain from modular decomposition, the theory is challenged.</li>
                <li>If LLMs' internal representations do not reflect hierarchical structure (as measured by probing), the theory is undermined.</li>
                <li>If LLMs fail to transfer knowledge between structurally similar puzzles, the modular reasoning hypothesis is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The specific neural correlates of hierarchical and modular representations in LLMs are not yet identified. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends modular reasoning to a new domain and makes new, testable predictions.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building machines that learn and think like people [Modular, compositional reasoning in cognition]</li>
    <li>Akyürek et al. (2022) What Learning Algorithm Is In-Context Learning? [LLMs learn modular, compositional reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Representation and Modular Reasoning in LLMs for Spatial Puzzle Solving",
    "theory_description": "This theory proposes that LLMs internally construct hierarchical, modular representations of spatial puzzles, enabling them to decompose complex problems into subproblems and apply modular reasoning strategies. These representations are shaped by the model's exposure to hierarchical and compositional structures in language, allowing for flexible adaptation to new spatial puzzles.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Decomposition of Spatial Puzzles",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is exposed_to",
                        "object": "spatial puzzle in text form"
                    },
                    {
                        "subject": "language model",
                        "relation": "has inductive bias",
                        "object": "for hierarchical structure"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "constructs",
                        "object": "internal hierarchical representation of puzzle"
                    },
                    {
                        "subject": "language model",
                        "relation": "decomposes",
                        "object": "puzzle into subproblems (e.g., rows, columns, blocks in Sudoku)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can be prompted to solve Sudoku by row, column, or block, indicating internal decomposition.",
                        "uuids": []
                    },
                    {
                        "text": "Hierarchical reasoning is observed in LLMs for other tasks, such as code generation and story understanding.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical and modular reasoning in LLMs has been observed in language and code tasks.",
                    "what_is_novel": "Application of hierarchical decomposition to spatial puzzle solving in LLMs is new.",
                    "classification_explanation": "The law extends known hierarchical reasoning in LLMs to the spatial puzzle domain, which is a novel application.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Akyürek et al. (2022) What Learning Algorithm Is In-Context Learning? [LLMs learn modular, compositional reasoning]",
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [LLMs can decompose problems stepwise]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Modular Application of Reasoning Strategies",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "has hierarchical representation",
                        "object": "of spatial puzzle"
                    },
                    {
                        "subject": "subproblem",
                        "relation": "is identified",
                        "object": "within puzzle structure"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "applies",
                        "object": "modular reasoning strategies to subproblems"
                    },
                    {
                        "subject": "language model",
                        "relation": "integrates",
                        "object": "subproblem solutions into global solution"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can solve individual rows or blocks in Sudoku and then combine results for the full solution.",
                        "uuids": []
                    },
                    {
                        "text": "Modular reasoning is a hallmark of human and algorithmic puzzle solving.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Modular reasoning is well-studied in cognitive science and some LLM tasks.",
                    "what_is_novel": "The explicit claim that LLMs use modular reasoning for spatial puzzles is new.",
                    "classification_explanation": "The law is an extension of modular reasoning to a new domain, with new predictions.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake et al. (2017) Building machines that learn and think like people [Modular, compositional reasoning in cognition]",
                        "Akyürek et al. (2022) What Learning Algorithm Is In-Context Learning? [LLMs learn modular, compositional reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will perform better on spatial puzzles when prompted to solve subproblems (e.g., rows, columns) before integrating the solution.",
        "LLMs will show transfer learning: training on one type of spatial puzzle will improve performance on structurally similar puzzles.",
        "LLMs will be able to explain their reasoning in terms of subproblem decomposition when prompted."
    ],
    "new_predictions_unknown": [
        "LLMs may develop novel, non-human modular strategies for spatial puzzles that outperform classical approaches.",
        "Hierarchical representations in LLMs may enable zero-shot generalization to entirely new classes of spatial puzzles.",
        "LLMs may be able to discover and exploit latent symmetries in spatial puzzles without explicit instruction."
    ],
    "negative_experiments": [
        "If LLMs cannot be prompted to solve subproblems or show no performance gain from modular decomposition, the theory is challenged.",
        "If LLMs' internal representations do not reflect hierarchical structure (as measured by probing), the theory is undermined.",
        "If LLMs fail to transfer knowledge between structurally similar puzzles, the modular reasoning hypothesis is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "The specific neural correlates of hierarchical and modular representations in LLMs are not yet identified.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs fail to generalize modular reasoning to puzzles with novel or irregular structures.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Puzzles with flat, non-hierarchical structure may not benefit from modular reasoning.",
        "Very large puzzles may exceed the model's ability to maintain hierarchical representations in context."
    ],
    "existing_theory": {
        "what_already_exists": "Hierarchical and modular reasoning are established in cognitive science and some LLM tasks.",
        "what_is_novel": "The application to spatial puzzle solving in LLMs and the prediction of transfer and novel strategies are new.",
        "classification_explanation": "The theory extends modular reasoning to a new domain and makes new, testable predictions.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lake et al. (2017) Building machines that learn and think like people [Modular, compositional reasoning in cognition]",
            "Akyürek et al. (2022) What Learning Algorithm Is In-Context Learning? [LLMs learn modular, compositional reasoning]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.",
    "original_theory_id": "theory-597",
    "original_theory_name": "Emergent Algorithmic Reasoning via Structured Inductive Biases in Language Models",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Emergent Algorithmic Reasoning via Structured Inductive Biases in Language Models",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
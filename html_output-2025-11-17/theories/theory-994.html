<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-994</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-994</p>
                <p><strong>Name:</strong> Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory posits that LLM agents achieve optimal performance in text games by dynamically constructing and utilizing a hierarchical memory system, where episodic memory (detailed, temporally ordered event traces) is selectively abstracted into semantic memory (generalized, context-independent knowledge), and both are leveraged in a context-sensitive manner to guide decision-making and planning.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Episodic-to-Semantic Abstraction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; encounters &#8594; repeated or structurally similar events in text game<span style="color: #888888;">, and</span></div>
        <div>&#8226; episodic memory &#8594; contains &#8594; multiple instances of similar events</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; abstracts &#8594; generalized rules or facts (semantic memory) from episodic traces</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human cognition shows episodic memory is abstracted into semantic knowledge; LLMs can be prompted to generalize from examples. </li>
    <li>LLM agents in text games often perform better when able to generalize from repeated patterns (e.g., puzzle solutions, navigation rules). </li>
    <li>Memory-augmented neural networks and retrieval-augmented LLMs show improved generalization when allowed to form higher-level abstractions from repeated experiences. </li>
    <li>Cognitive science literature (Tulving, 1972) establishes that semantic memory is formed by abstraction from episodic traces. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to cognitive science and some LLM memory-augmented methods, the explicit, dynamic abstraction and hierarchical interplay in the context of text games is novel.</p>            <p><strong>What Already Exists:</strong> Episodic and semantic memory distinction is well-established in cognitive science; some LLM architectures use memory buffers or retrieval-augmented generation.</p>            <p><strong>What is Novel:</strong> The explicit hierarchical abstraction process and its dynamic, context-driven use in LLM agents for text games is not formalized in prior work.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [establishes distinction in human memory]</li>
    <li>Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [nearest neighbor memory in LMs]</li>
    <li>Madotto et al. (2020) Memory Grounded Conversational Reasoning [memory-augmented LMs, but not hierarchical abstraction]</li>
</ul>
            <h3>Statement 1: Context-Sensitive Memory Utilization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; faces &#8594; decision point in text game<span style="color: #888888;">, and</span></div>
        <div>&#8226; current context &#8594; matches &#8594; previously stored episodic or semantic memory</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; retrieves and applies &#8594; most relevant memory (episodic or semantic) to inform action selection</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Contextual retrieval is critical in human and artificial agents; LLMs with retrieval-augmented memory outperform those without in sequential tasks. </li>
    <li>Text game agents with context-aware memory modules (e.g., attention over memory) show improved task completion and planning. </li>
    <li>Neural Turing Machines and Memory Networks demonstrate the importance of context-sensitive memory retrieval for sequential reasoning. </li>
    <li>ReAct and similar LLM agent frameworks use context to arbitrate between different memory sources for reasoning and acting. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general principle is known, but its specific application and formalization for LLM agents in text games is novel.</p>            <p><strong>What Already Exists:</strong> Contextual retrieval is a known principle in memory-augmented neural networks and cognitive science.</p>            <p><strong>What is Novel:</strong> The explicit law of context-sensitive arbitration between episodic and semantic memory in LLM agents for text games is not formalized.</p>
            <p><strong>References:</strong> <ul>
    <li>Graves et al. (2014) Neural Turing Machines [contextual memory retrieval in neural networks]</li>
    <li>Weston et al. (2015) Memory Networks [contextual memory in QA tasks]</li>
    <li>Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [contextual memory use in LLM agents]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents that explicitly abstract repeated episodic experiences into semantic rules will outperform those that only use raw episodic memory in long-horizon text games.</li>
                <li>Agents that dynamically arbitrate between episodic and semantic memory based on context will solve puzzles requiring both recall of specific events and generalization more efficiently.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LLM agent is forced to abstract too aggressively (i.e., compress episodic memory into semantic memory prematurely), it may lose critical information and underperform in games with high context sensitivity.</li>
                <li>Introducing a mechanism for agents to 're-episodify' semantic knowledge (i.e., reconstruct detailed event traces from general rules) may enable better adaptation to novel or adversarial game scenarios.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with only episodic memory (no abstraction) perform as well as those with hierarchical memory, the theory's necessity is called into question.</li>
                <li>If context-insensitive memory retrieval (e.g., random or fixed retrieval) yields similar performance to context-sensitive retrieval, the theory's claims are weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of memory corruption or hallucination in LLMs on hierarchical memory use is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known memory principles but applies and formalizes them in a new way for LLM agents in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [human memory distinction]</li>
    <li>Graves et al. (2014) Neural Turing Machines [contextual memory in neural networks]</li>
    <li>Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [LLM agent memory use]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games",
    "theory_description": "This theory posits that LLM agents achieve optimal performance in text games by dynamically constructing and utilizing a hierarchical memory system, where episodic memory (detailed, temporally ordered event traces) is selectively abstracted into semantic memory (generalized, context-independent knowledge), and both are leveraged in a context-sensitive manner to guide decision-making and planning.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Episodic-to-Semantic Abstraction Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "encounters",
                        "object": "repeated or structurally similar events in text game"
                    },
                    {
                        "subject": "episodic memory",
                        "relation": "contains",
                        "object": "multiple instances of similar events"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "abstracts",
                        "object": "generalized rules or facts (semantic memory) from episodic traces"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human cognition shows episodic memory is abstracted into semantic knowledge; LLMs can be prompted to generalize from examples.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents in text games often perform better when able to generalize from repeated patterns (e.g., puzzle solutions, navigation rules).",
                        "uuids": []
                    },
                    {
                        "text": "Memory-augmented neural networks and retrieval-augmented LLMs show improved generalization when allowed to form higher-level abstractions from repeated experiences.",
                        "uuids": []
                    },
                    {
                        "text": "Cognitive science literature (Tulving, 1972) establishes that semantic memory is formed by abstraction from episodic traces.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Episodic and semantic memory distinction is well-established in cognitive science; some LLM architectures use memory buffers or retrieval-augmented generation.",
                    "what_is_novel": "The explicit hierarchical abstraction process and its dynamic, context-driven use in LLM agents for text games is not formalized in prior work.",
                    "classification_explanation": "While related to cognitive science and some LLM memory-augmented methods, the explicit, dynamic abstraction and hierarchical interplay in the context of text games is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [establishes distinction in human memory]",
                        "Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [nearest neighbor memory in LMs]",
                        "Madotto et al. (2020) Memory Grounded Conversational Reasoning [memory-augmented LMs, but not hierarchical abstraction]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Context-Sensitive Memory Utilization Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "faces",
                        "object": "decision point in text game"
                    },
                    {
                        "subject": "current context",
                        "relation": "matches",
                        "object": "previously stored episodic or semantic memory"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "retrieves and applies",
                        "object": "most relevant memory (episodic or semantic) to inform action selection"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Contextual retrieval is critical in human and artificial agents; LLMs with retrieval-augmented memory outperform those without in sequential tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Text game agents with context-aware memory modules (e.g., attention over memory) show improved task completion and planning.",
                        "uuids": []
                    },
                    {
                        "text": "Neural Turing Machines and Memory Networks demonstrate the importance of context-sensitive memory retrieval for sequential reasoning.",
                        "uuids": []
                    },
                    {
                        "text": "ReAct and similar LLM agent frameworks use context to arbitrate between different memory sources for reasoning and acting.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Contextual retrieval is a known principle in memory-augmented neural networks and cognitive science.",
                    "what_is_novel": "The explicit law of context-sensitive arbitration between episodic and semantic memory in LLM agents for text games is not formalized.",
                    "classification_explanation": "The general principle is known, but its specific application and formalization for LLM agents in text games is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Graves et al. (2014) Neural Turing Machines [contextual memory retrieval in neural networks]",
                        "Weston et al. (2015) Memory Networks [contextual memory in QA tasks]",
                        "Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [contextual memory use in LLM agents]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents that explicitly abstract repeated episodic experiences into semantic rules will outperform those that only use raw episodic memory in long-horizon text games.",
        "Agents that dynamically arbitrate between episodic and semantic memory based on context will solve puzzles requiring both recall of specific events and generalization more efficiently."
    ],
    "new_predictions_unknown": [
        "If an LLM agent is forced to abstract too aggressively (i.e., compress episodic memory into semantic memory prematurely), it may lose critical information and underperform in games with high context sensitivity.",
        "Introducing a mechanism for agents to 're-episodify' semantic knowledge (i.e., reconstruct detailed event traces from general rules) may enable better adaptation to novel or adversarial game scenarios."
    ],
    "negative_experiments": [
        "If agents with only episodic memory (no abstraction) perform as well as those with hierarchical memory, the theory's necessity is called into question.",
        "If context-insensitive memory retrieval (e.g., random or fixed retrieval) yields similar performance to context-sensitive retrieval, the theory's claims are weakened."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of memory corruption or hallucination in LLMs on hierarchical memory use is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some text games may be solvable with simple short-term memory and pattern matching, without need for hierarchical abstraction.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Games with highly stochastic or adversarial environments may require continual updating of semantic memory, challenging the stability of abstraction.",
        "Very short or trivial games may not benefit from hierarchical memory at all."
    ],
    "existing_theory": {
        "what_already_exists": "Hierarchical memory and context-sensitive retrieval are established in cognitive science and some neural architectures.",
        "what_is_novel": "The explicit, dynamic, and hierarchical abstraction process for LLM agents in text games, and the formalization of arbitration between memory types, is novel.",
        "classification_explanation": "The theory synthesizes known memory principles but applies and formalizes them in a new way for LLM agents in text games.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [human memory distinction]",
            "Graves et al. (2014) Neural Turing Machines [contextual memory in neural networks]",
            "Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [LLM agent memory use]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-595",
    "original_theory_name": "Hybrid Memory Architectures Enable Robust Long-Horizon Reasoning and Generalization in LLM Agents for Text Games",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
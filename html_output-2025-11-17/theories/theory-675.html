<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multidimensional, Task-Aligned, and Calibration-Aware Evaluation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-675</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-675</p>
                <p><strong>Name:</strong> Multidimensional, Task-Aligned, and Calibration-Aware Evaluation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that the evaluation of LLM-generated scientific theories must be multidimensional (covering validity, novelty, helpfulness, factuality, and other relevant axes), task-aligned (using criteria and rubrics tailored to the scientific context), and calibration-aware (ensuring that model confidence aligns with empirical correctness). The theory claims that single-metric or surface-similarity-based evaluation is insufficient for open-ended scientific ideation, and that robust evaluation requires explicit, interpretable rubrics, calibration checks, and, where possible, reference to human expert judgment. The theory further posits that automated metrics must be validated for alignment with human preferences and that trade-offs (e.g., between novelty and factuality) must be explicitly managed.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Multidimensional Evaluation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-generated scientific theory &#8594; is_evaluated &#8594; using a single-metric or surface-similarity-based metric (e.g., BLEU, ROUGE, BERTScore)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation &#8594; fails_to_capture &#8594; key aspects of scientific quality (e.g., novelty, validity, helpfulness)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Automated similarity metrics (BLEU, ROUGE, BERTScore) poorly correlate with human judgments of novelty, depth, and utility in open-ended idea generation; human raters preferred GPT-4 outputs despite lower automated similarity scores. <a href="../results/extraction-result-6018.html#e6018.1" class="evidence-link">[e6018.1]</a> <a href="../results/extraction-result-6001.html#e6001.6" class="evidence-link">[e6001.6]</a> </li>
    <li>Evaluation metrics (Validness, Novelty, Helpfulness) with explicit rubrics are used in TOMATO and other benchmarks to capture multidimensional aspects of scientific hypotheses. <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6116.html#e6116.3" class="evidence-link">[e6116.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While multidimensional evaluation is emerging, the explicit rejection of single-metric evaluation as insufficient is novel.</p>            <p><strong>What Already Exists:</strong> Multidimensional rubrics are used in some recent benchmarks.</p>            <p><strong>What is Novel:</strong> The law that single-metric or surface-similarity-based evaluation is fundamentally insufficient for scientific theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [multidimensional rubrics]</li>
    <li>Wang et al. (2023) SciMON: Scientific Inspiration Machines Optimized for Novelty [human-vs-automated metric gap]</li>
</ul>
            <h3>Statement 1: Task-Aligned Rubric Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_rubric &#8594; is_explicitly_aligned_with &#8594; the scientific task and domain</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_alignment_with_human_judgment &#8594; increases &#8594; true</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Explicit rubrics (e.g., Validness, Novelty, Helpfulness in TOMATO; 10-skill taxonomy in SciBench; Bloom's taxonomy in SciEval) increase alignment between automated and human evaluation. <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6116.html#e6116.3" class="evidence-link">[e6116.3]</a> <a href="../results/extraction-result-6118.html#e6118.2" class="evidence-link">[e6118.2]</a> <a href="../results/extraction-result-6025.html#e6025.1" class="evidence-link">[e6025.1]</a> </li>
    <li>Few-shot prompt ablation studies show that including diverse, task-aligned examples improves LLM evaluator agreement with human scores. <a href="../results/extraction-result-6166.html#e6166.5" class="evidence-link">[e6166.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Task-aligned rubrics are emerging, but their necessity as a law is novel.</p>            <p><strong>What Already Exists:</strong> Task-aligned rubrics are used in some recent evaluation pipelines.</p>            <p><strong>What is Novel:</strong> The law that explicit task alignment of rubrics is necessary for high human-evaluator agreement.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [task-aligned rubrics]</li>
    <li>Wang et al. (2024) SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading [rubric alignment]</li>
</ul>
            <h3>Statement 2: Calibration-Aware Evaluation Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM_evaluator &#8594; is_calibrated &#8594; such that model confidence aligns with empirical correctness</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_trustworthiness &#8594; increases &#8594; true</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Calibration analysis shows that pre-trained GPT-4 is highly calibrated, but RLHF post-training reduces calibration; calibration is critical for trust in model-generated scientific claims. <a href="../results/extraction-result-6156.html#e6156.6" class="evidence-link">[e6156.6]</a> <a href="../results/extraction-result-6132.html#e6132.9" class="evidence-link">[e6132.9]</a> </li>
    <li>Sample and Eval, Sample and Select, and self-critique & revise protocols explicitly measure and improve calibration (Calibration-AUC, Selective-AUC) in LLM-generated answers. <a href="../results/extraction-result-6157.html#e6157.1" class="evidence-link">[e6157.1]</a> <a href="../results/extraction-result-6157.html#e6157.0" class="evidence-link">[e6157.0]</a> <a href="../results/extraction-result-6157.html#e6157.11" class="evidence-link">[e6157.11]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Calibration is discussed, but its necessity as a law for scientific theory evaluation is novel.</p>            <p><strong>What Already Exists:</strong> Calibration is recognized as important in model evaluation.</p>            <p><strong>What is Novel:</strong> The law that calibration is a necessary property for trustworthy evaluation of LLM-generated scientific theories.</p>
            <p><strong>References:</strong> <ul>
    <li>Kadavath et al. (2022) Language models (mostly) know what they know [calibration in LLMs]</li>
    <li>Wang et al. (2023) Self-Evaluation Improves Selective Generation in Large Language Models [calibration metrics]</li>
</ul>
            <h3>Statement 3: Explicit Trade-off Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_protocol &#8594; explicitly_manages &#8594; trade-offs between novelty, factuality, and other axes</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_utility &#8594; is_maximized &#8594; true</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Creativity vs. hallucination tradeoff is observed in LLM ideation; evaluation must balance novelty and factual grounding, as some hallucination-like behavior is necessary for creative novelty. <a href="../results/extraction-result-5998.html#e5998.7" class="evidence-link">[e5998.7]</a> <a href="../results/extraction-result-5998.html#e5998.1" class="evidence-link">[e5998.1]</a> </li>
    <li>TOMATO and related benchmarks emphasize that novelty is relatively more important for open-domain hypothesis induction, but validness and helpfulness must also be considered. <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6116.html#e6116.3" class="evidence-link">[e6116.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Trade-offs are recognized, but their explicit management as a law is novel.</p>            <p><strong>What Already Exists:</strong> Trade-offs are discussed in the context of creativity and hallucination.</p>            <p><strong>What is Novel:</strong> The law that explicit management of these trade-offs is necessary for maximizing evaluation utility.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [novelty-validity tradeoff]</li>
    <li>Wang et al. (2024) Towards a Science Exocortex [creativity-hallucination tradeoff]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If evaluation of LLM-generated scientific theories uses only surface-similarity metrics, it will fail to identify genuinely novel or useful hypotheses.</li>
                <li>If explicit, task-aligned rubrics are used, LLM and human evaluator agreement will increase, especially in open-ended or creative scientific tasks.</li>
                <li>If calibration is monitored and maintained, LLM-generated confidence scores will be predictive of empirical correctness, enabling selective abstention from low-confidence outputs.</li>
                <li>If evaluation protocols explicitly balance novelty and factuality, the resulting set of hypotheses will be both more innovative and more reliable.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If multidimensional, calibration-aware evaluation is applied to LLM-generated theories in domains with no established ground truth, it may enable the surfacing of valid but previously unrecognized scientific insights.</li>
                <li>If automated evaluators are trained on multidimensional, task-aligned rubrics, they may eventually match or exceed human expert agreement in complex scientific theory evaluation.</li>
                <li>If explicit trade-off management is optimized (e.g., via reinforcement learning), LLMs may generate hypotheses that are both highly novel and highly valid, potentially accelerating scientific discovery.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If single-metric or surface-similarity-based evaluation is found to correlate as well as multidimensional, rubric-based evaluation with human expert judgment in open-ended scientific ideation, the multidimensional evaluation law would be falsified.</li>
                <li>If calibration-aware evaluation does not improve trustworthiness or selective abstention, the calibration-aware evaluation law would be undermined.</li>
                <li>If explicit trade-off management does not improve the utility or impact of generated hypotheses, the explicit trade-off law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some highly structured or fact-based tasks (e.g., code generation, short-form QA) may be adequately evaluated by single-metric or automated metrics. <a href="../results/extraction-result-6108.html#e6108.1" class="evidence-link">[e6108.1]</a> <a href="../results/extraction-result-6108.html#e6108.2" class="evidence-link">[e6108.2]</a> <a href="../results/extraction-result-6123.html#e6123.1" class="evidence-link">[e6123.1]</a> </li>
    <li>In some domains, human agreement is low or subjective, making alignment with human judgment a moving target. <a href="../results/extraction-result-6159.html#e6159.1" class="evidence-link">[e6159.1]</a> <a href="../results/extraction-result-6153.html#e6153.1" class="evidence-link">[e6153.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While components exist, the general theory and its formalization as a set of necessary and sufficient conditions for robust evaluation of LLM-generated scientific theories is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [multidimensional rubrics]</li>
    <li>Wang et al. (2023) SciMON: Scientific Inspiration Machines Optimized for Novelty [human-vs-automated metric gap]</li>
    <li>Kadavath et al. (2022) Language models (mostly) know what they know [calibration in LLMs]</li>
    <li>Wang et al. (2023) Self-Evaluation Improves Selective Generation in Large Language Models [calibration metrics]</li>
    <li>Wang et al. (2024) Towards a Science Exocortex [creativity-hallucination tradeoff]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multidimensional, Task-Aligned, and Calibration-Aware Evaluation Theory",
    "theory_description": "This theory asserts that the evaluation of LLM-generated scientific theories must be multidimensional (covering validity, novelty, helpfulness, factuality, and other relevant axes), task-aligned (using criteria and rubrics tailored to the scientific context), and calibration-aware (ensuring that model confidence aligns with empirical correctness). The theory claims that single-metric or surface-similarity-based evaluation is insufficient for open-ended scientific ideation, and that robust evaluation requires explicit, interpretable rubrics, calibration checks, and, where possible, reference to human expert judgment. The theory further posits that automated metrics must be validated for alignment with human preferences and that trade-offs (e.g., between novelty and factuality) must be explicitly managed.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Multidimensional Evaluation Law",
                "if": [
                    {
                        "subject": "LLM-generated scientific theory",
                        "relation": "is_evaluated",
                        "object": "using a single-metric or surface-similarity-based metric (e.g., BLEU, ROUGE, BERTScore)"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation",
                        "relation": "fails_to_capture",
                        "object": "key aspects of scientific quality (e.g., novelty, validity, helpfulness)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Automated similarity metrics (BLEU, ROUGE, BERTScore) poorly correlate with human judgments of novelty, depth, and utility in open-ended idea generation; human raters preferred GPT-4 outputs despite lower automated similarity scores.",
                        "uuids": [
                            "e6018.1",
                            "e6001.6"
                        ]
                    },
                    {
                        "text": "Evaluation metrics (Validness, Novelty, Helpfulness) with explicit rubrics are used in TOMATO and other benchmarks to capture multidimensional aspects of scientific hypotheses.",
                        "uuids": [
                            "e6116.2",
                            "e6116.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Multidimensional rubrics are used in some recent benchmarks.",
                    "what_is_novel": "The law that single-metric or surface-similarity-based evaluation is fundamentally insufficient for scientific theory evaluation.",
                    "classification_explanation": "While multidimensional evaluation is emerging, the explicit rejection of single-metric evaluation as insufficient is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [multidimensional rubrics]",
                        "Wang et al. (2023) SciMON: Scientific Inspiration Machines Optimized for Novelty [human-vs-automated metric gap]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Task-Aligned Rubric Law",
                "if": [
                    {
                        "subject": "evaluation_rubric",
                        "relation": "is_explicitly_aligned_with",
                        "object": "the scientific task and domain"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_alignment_with_human_judgment",
                        "relation": "increases",
                        "object": "true"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Explicit rubrics (e.g., Validness, Novelty, Helpfulness in TOMATO; 10-skill taxonomy in SciBench; Bloom's taxonomy in SciEval) increase alignment between automated and human evaluation.",
                        "uuids": [
                            "e6116.2",
                            "e6116.3",
                            "e6118.2",
                            "e6025.1"
                        ]
                    },
                    {
                        "text": "Few-shot prompt ablation studies show that including diverse, task-aligned examples improves LLM evaluator agreement with human scores.",
                        "uuids": [
                            "e6166.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Task-aligned rubrics are used in some recent evaluation pipelines.",
                    "what_is_novel": "The law that explicit task alignment of rubrics is necessary for high human-evaluator agreement.",
                    "classification_explanation": "Task-aligned rubrics are emerging, but their necessity as a law is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [task-aligned rubrics]",
                        "Wang et al. (2024) SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading [rubric alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Calibration-Aware Evaluation Law",
                "if": [
                    {
                        "subject": "LLM_evaluator",
                        "relation": "is_calibrated",
                        "object": "such that model confidence aligns with empirical correctness"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_trustworthiness",
                        "relation": "increases",
                        "object": "true"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Calibration analysis shows that pre-trained GPT-4 is highly calibrated, but RLHF post-training reduces calibration; calibration is critical for trust in model-generated scientific claims.",
                        "uuids": [
                            "e6156.6",
                            "e6132.9"
                        ]
                    },
                    {
                        "text": "Sample and Eval, Sample and Select, and self-critique & revise protocols explicitly measure and improve calibration (Calibration-AUC, Selective-AUC) in LLM-generated answers.",
                        "uuids": [
                            "e6157.1",
                            "e6157.0",
                            "e6157.11"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Calibration is recognized as important in model evaluation.",
                    "what_is_novel": "The law that calibration is a necessary property for trustworthy evaluation of LLM-generated scientific theories.",
                    "classification_explanation": "Calibration is discussed, but its necessity as a law for scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kadavath et al. (2022) Language models (mostly) know what they know [calibration in LLMs]",
                        "Wang et al. (2023) Self-Evaluation Improves Selective Generation in Large Language Models [calibration metrics]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Explicit Trade-off Law",
                "if": [
                    {
                        "subject": "evaluation_protocol",
                        "relation": "explicitly_manages",
                        "object": "trade-offs between novelty, factuality, and other axes"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_utility",
                        "relation": "is_maximized",
                        "object": "true"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Creativity vs. hallucination tradeoff is observed in LLM ideation; evaluation must balance novelty and factual grounding, as some hallucination-like behavior is necessary for creative novelty.",
                        "uuids": [
                            "e5998.7",
                            "e5998.1"
                        ]
                    },
                    {
                        "text": "TOMATO and related benchmarks emphasize that novelty is relatively more important for open-domain hypothesis induction, but validness and helpfulness must also be considered.",
                        "uuids": [
                            "e6116.2",
                            "e6116.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Trade-offs are discussed in the context of creativity and hallucination.",
                    "what_is_novel": "The law that explicit management of these trade-offs is necessary for maximizing evaluation utility.",
                    "classification_explanation": "Trade-offs are recognized, but their explicit management as a law is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [novelty-validity tradeoff]",
                        "Wang et al. (2024) Towards a Science Exocortex [creativity-hallucination tradeoff]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If evaluation of LLM-generated scientific theories uses only surface-similarity metrics, it will fail to identify genuinely novel or useful hypotheses.",
        "If explicit, task-aligned rubrics are used, LLM and human evaluator agreement will increase, especially in open-ended or creative scientific tasks.",
        "If calibration is monitored and maintained, LLM-generated confidence scores will be predictive of empirical correctness, enabling selective abstention from low-confidence outputs.",
        "If evaluation protocols explicitly balance novelty and factuality, the resulting set of hypotheses will be both more innovative and more reliable."
    ],
    "new_predictions_unknown": [
        "If multidimensional, calibration-aware evaluation is applied to LLM-generated theories in domains with no established ground truth, it may enable the surfacing of valid but previously unrecognized scientific insights.",
        "If automated evaluators are trained on multidimensional, task-aligned rubrics, they may eventually match or exceed human expert agreement in complex scientific theory evaluation.",
        "If explicit trade-off management is optimized (e.g., via reinforcement learning), LLMs may generate hypotheses that are both highly novel and highly valid, potentially accelerating scientific discovery."
    ],
    "negative_experiments": [
        "If single-metric or surface-similarity-based evaluation is found to correlate as well as multidimensional, rubric-based evaluation with human expert judgment in open-ended scientific ideation, the multidimensional evaluation law would be falsified.",
        "If calibration-aware evaluation does not improve trustworthiness or selective abstention, the calibration-aware evaluation law would be undermined.",
        "If explicit trade-off management does not improve the utility or impact of generated hypotheses, the explicit trade-off law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some highly structured or fact-based tasks (e.g., code generation, short-form QA) may be adequately evaluated by single-metric or automated metrics.",
            "uuids": [
                "e6108.1",
                "e6108.2",
                "e6123.1"
            ]
        },
        {
            "text": "In some domains, human agreement is low or subjective, making alignment with human judgment a moving target.",
            "uuids": [
                "e6159.1",
                "e6153.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Automated LLM-based evaluators (e.g., GPT-4, ChatGPT) can sometimes achieve high agreement with human labels in constrained tasks, challenging the necessity of multidimensional rubrics for all cases.",
            "uuids": [
                "e6114.0",
                "e6153.0",
                "e6166.2"
            ]
        }
    ],
    "special_cases": [
        "Highly structured, fact-based, or short-form outputs may be adequately evaluated by single-metric or automated metrics.",
        "In domains with low human agreement or high subjectivity, even multidimensional rubrics may not yield high evaluator alignment.",
        "Calibration may be less critical in tasks where abstention is not possible or all outputs must be scored."
    ],
    "existing_theory": {
        "what_already_exists": "Multidimensional rubrics, task-aligned evaluation, and calibration are recognized as important in recent LLM evaluation literature.",
        "what_is_novel": "The formalization of these as necessary laws for robust evaluation of LLM-generated scientific theories, and the explicit rejection of single-metric evaluation for open-ended scientific ideation.",
        "classification_explanation": "While components exist, the general theory and its formalization as a set of necessary and sufficient conditions for robust evaluation of LLM-generated scientific theories is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [multidimensional rubrics]",
            "Wang et al. (2023) SciMON: Scientific Inspiration Machines Optimized for Novelty [human-vs-automated metric gap]",
            "Kadavath et al. (2022) Language models (mostly) know what they know [calibration in LLMs]",
            "Wang et al. (2023) Self-Evaluation Improves Selective Generation in Large Language Models [calibration metrics]",
            "Wang et al. (2024) Towards a Science Exocortex [creativity-hallucination tradeoff]"
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Theory of Logical Reasoning in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1120</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1120</p>
                <p><strong>Name:</strong> Dual-Process Theory of Logical Reasoning in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models (LMs) can best perform strict logical reasoning when they are architecturally or procedurally equipped to combine two distinct but interacting processes: (1) a fast, pattern-matching, context-driven process (analogous to System 1 in human cognition) and (2) a slow, explicit, symbolic manipulation process (analogous to System 2). The theory asserts that optimal logical reasoning arises when LMs can dynamically allocate reasoning tasks between these two processes, leveraging the strengths of each.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Separation of Pattern and Symbolic Processes (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; has_architecture &#8594; dual-process (pattern + symbolic)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_perform &#8594; both rapid context-based inference and explicit symbolic reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human reasoning is best explained by dual-process theories, with System 1 (fast, intuitive) and System 2 (slow, logical) processes; LMs show strengths in pattern-matching but struggle with strict logic unless augmented. </li>
    <li>Hybrid neuro-symbolic models outperform pure neural models on logical reasoning benchmarks. </li>
    <li>Chain-of-thought prompting and tool-augmented LMs show improved logical reasoning, suggesting the benefit of explicit symbolic processes. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While dual-process ideas exist in cognitive science and some hybrid models in AI, this law's explicit application to LMs and the requirement for dynamic allocation is novel.</p>            <p><strong>What Already Exists:</strong> Dual-process theories are well-established in cognitive science; hybrid neuro-symbolic models have been proposed for AI.</p>            <p><strong>What is Novel:</strong> Explicitly positing that LMs require a dynamic, architectural or procedural separation and interaction of these processes for optimal strict logical reasoning.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]</li>
    <li>Besold et al. (2017) Neural-symbolic learning and reasoning: A survey and interpretation [hybrid models in AI]</li>
    <li>Lake et al. (2017) Building machines that learn and think like people [calls for hybrid approaches]</li>
</ul>
            <h3>Statement 1: Dynamic Allocation of Reasoning Mode (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning task &#8594; requires &#8594; strict logical consistency<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; can_detect &#8594; logical complexity or ambiguity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; allocates &#8594; symbolic process to task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs often default to pattern-matching, but explicit prompting or architectural changes (e.g., tool use, external memory) can trigger more logical, stepwise reasoning. </li>
    <li>Human reasoners switch to System 2 when tasks are complex or ambiguous. </li>
    <li>Chain-of-thought prompting elicits more logical, stepwise reasoning in LMs, especially on complex tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends dual-process ideas to LMs with a novel, explicit mechanism for dynamic allocation based on task demands.</p>            <p><strong>What Already Exists:</strong> Task-dependent switching between reasoning modes is known in human cognition.</p>            <p><strong>What is Novel:</strong> The requirement for LMs to detect logical complexity and dynamically allocate symbolic reasoning is a new, explicit architectural/procedural claim.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [human switching between reasoning modes]</li>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [prompting triggers stepwise reasoning in LMs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a language model is architecturally or procedurally equipped with both pattern-matching and explicit symbolic reasoning modules, it will outperform pure neural models on strict logical reasoning benchmarks.</li>
                <li>Prompting LMs to 'think step by step' or using tool-augmented LMs will improve logical consistency on tasks requiring strict logic.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LMs are given a meta-cognitive module to detect logical complexity and allocate reasoning mode, their performance on adversarial logical tasks will surpass current state-of-the-art.</li>
                <li>Training LMs with explicit dual-process supervision (e.g., labeling which process to use) will lead to emergent, human-like logical reasoning abilities.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs with dual-process architectures do not outperform pure neural models on strict logical reasoning, the theory is called into question.</li>
                <li>If LMs cannot dynamically allocate reasoning mode even when equipped with both processes, the theory's mechanism is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs show improved logical reasoning with scale alone, without explicit symbolic modules. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is inspired by existing dual-process and hybrid models, but its explicit, dynamic application to LMs and logical reasoning is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]</li>
    <li>Besold et al. (2017) Neural-symbolic learning and reasoning: A survey and interpretation [hybrid models in AI]</li>
    <li>Lake et al. (2017) Building machines that learn and think like people [calls for hybrid approaches]</li>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [prompting triggers stepwise reasoning in LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Theory of Logical Reasoning in Language Models",
    "theory_description": "This theory posits that language models (LMs) can best perform strict logical reasoning when they are architecturally or procedurally equipped to combine two distinct but interacting processes: (1) a fast, pattern-matching, context-driven process (analogous to System 1 in human cognition) and (2) a slow, explicit, symbolic manipulation process (analogous to System 2). The theory asserts that optimal logical reasoning arises when LMs can dynamically allocate reasoning tasks between these two processes, leveraging the strengths of each.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Separation of Pattern and Symbolic Processes",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "has_architecture",
                        "object": "dual-process (pattern + symbolic)"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_perform",
                        "object": "both rapid context-based inference and explicit symbolic reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human reasoning is best explained by dual-process theories, with System 1 (fast, intuitive) and System 2 (slow, logical) processes; LMs show strengths in pattern-matching but struggle with strict logic unless augmented.",
                        "uuids": []
                    },
                    {
                        "text": "Hybrid neuro-symbolic models outperform pure neural models on logical reasoning benchmarks.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought prompting and tool-augmented LMs show improved logical reasoning, suggesting the benefit of explicit symbolic processes.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Dual-process theories are well-established in cognitive science; hybrid neuro-symbolic models have been proposed for AI.",
                    "what_is_novel": "Explicitly positing that LMs require a dynamic, architectural or procedural separation and interaction of these processes for optimal strict logical reasoning.",
                    "classification_explanation": "While dual-process ideas exist in cognitive science and some hybrid models in AI, this law's explicit application to LMs and the requirement for dynamic allocation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]",
                        "Besold et al. (2017) Neural-symbolic learning and reasoning: A survey and interpretation [hybrid models in AI]",
                        "Lake et al. (2017) Building machines that learn and think like people [calls for hybrid approaches]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dynamic Allocation of Reasoning Mode",
                "if": [
                    {
                        "subject": "reasoning task",
                        "relation": "requires",
                        "object": "strict logical consistency"
                    },
                    {
                        "subject": "language model",
                        "relation": "can_detect",
                        "object": "logical complexity or ambiguity"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "allocates",
                        "object": "symbolic process to task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs often default to pattern-matching, but explicit prompting or architectural changes (e.g., tool use, external memory) can trigger more logical, stepwise reasoning.",
                        "uuids": []
                    },
                    {
                        "text": "Human reasoners switch to System 2 when tasks are complex or ambiguous.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought prompting elicits more logical, stepwise reasoning in LMs, especially on complex tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Task-dependent switching between reasoning modes is known in human cognition.",
                    "what_is_novel": "The requirement for LMs to detect logical complexity and dynamically allocate symbolic reasoning is a new, explicit architectural/procedural claim.",
                    "classification_explanation": "This law extends dual-process ideas to LMs with a novel, explicit mechanism for dynamic allocation based on task demands.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kahneman (2011) Thinking, Fast and Slow [human switching between reasoning modes]",
                        "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [prompting triggers stepwise reasoning in LMs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a language model is architecturally or procedurally equipped with both pattern-matching and explicit symbolic reasoning modules, it will outperform pure neural models on strict logical reasoning benchmarks.",
        "Prompting LMs to 'think step by step' or using tool-augmented LMs will improve logical consistency on tasks requiring strict logic."
    ],
    "new_predictions_unknown": [
        "If LMs are given a meta-cognitive module to detect logical complexity and allocate reasoning mode, their performance on adversarial logical tasks will surpass current state-of-the-art.",
        "Training LMs with explicit dual-process supervision (e.g., labeling which process to use) will lead to emergent, human-like logical reasoning abilities."
    ],
    "negative_experiments": [
        "If LMs with dual-process architectures do not outperform pure neural models on strict logical reasoning, the theory is called into question.",
        "If LMs cannot dynamically allocate reasoning mode even when equipped with both processes, the theory's mechanism is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs show improved logical reasoning with scale alone, without explicit symbolic modules.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Certain transformer LMs can perform some logical reasoning without explicit symbolic processes, suggesting possible emergent logic in neural architectures.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with ambiguous or underspecified logic may not benefit from symbolic reasoning.",
        "Very simple logical tasks may be solved by pattern-matching alone."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process theories in human cognition and some hybrid neuro-symbolic models in AI.",
        "what_is_novel": "Explicit, dynamic dual-process allocation in LMs for strict logical reasoning.",
        "classification_explanation": "The theory is inspired by existing dual-process and hybrid models, but its explicit, dynamic application to LMs and logical reasoning is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]",
            "Besold et al. (2017) Neural-symbolic learning and reasoning: A survey and interpretation [hybrid models in AI]",
            "Lake et al. (2017) Building machines that learn and think like people [calls for hybrid approaches]",
            "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [prompting triggers stepwise reasoning in LMs]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-603",
    "original_theory_name": "Emergent Reasoning Thresholds and Modularization Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Memory Utilization in LLM Agents for Text Games - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-967</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-967</p>
                <p><strong>Name:</strong> Dual-Process Memory Utilization in LLM Agents for Text Games</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory posits that LLM agents optimally solve text game tasks by dynamically integrating two distinct memory processes: (1) a fast, context-sensitive working memory for immediate, local reasoning, and (2) a slower, persistent episodic memory for long-term, cross-episode inference. The agent's performance is maximized when it can fluidly switch between these memory modes based on task demands, leveraging working memory for rapid adaptation and episodic memory for strategic planning and transfer.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Dynamic Memory Mode Switching (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; detects &#8594; rapidly changing or novel local context in text game</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; prioritizes &#8594; working memory for immediate reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human cognition uses working memory for rapid adaptation to new or changing information. </li>
    <li>LLMs show improved performance on short-term tasks when provided with recent context windows. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general dual-process principle is known, but its explicit, dynamic operationalization in LLM agents for text games is novel.</p>            <p><strong>What Already Exists:</strong> Dual-process models in cognitive science (e.g., Kahneman's System 1 and 2) and working/episodic memory distinctions are well-established.</p>            <p><strong>What is Novel:</strong> Application of dynamic, context-driven switching between memory modes in LLM agents for text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory]</li>
    <li>Baddeley (2000) The episodic buffer: a new component of working memory? [working/episodic memory distinction]</li>
    <li>Urbanek et al. (2019) Learning to Speak and Act in a Fantasy Text Adventure Game [LLM agents in text games]</li>
</ul>
            <h3>Statement 1: Strategic Episodic Memory Retrieval (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; faces &#8594; long-horizon or cross-episode dependencies in text game</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; retrieves &#8594; episodic memory traces for strategic planning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Humans use episodic memory to recall past experiences for long-term planning. </li>
    <li>RL agents with episodic memory modules outperform those without on tasks with long-term dependencies. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The principle is known, but its targeted application to LLM agents in text games is new.</p>            <p><strong>What Already Exists:</strong> Episodic memory's role in planning is established in cognitive science and RL.</p>            <p><strong>What is Novel:</strong> Explicit, context-driven retrieval of episodic memory in LLM agents for text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Gershman & Daw (2017) Reinforcement Learning and Episodic Memory in Humans and Animals [episodic memory in planning]</li>
    <li>Urbanek et al. (2019) Learning to Speak and Act in a Fantasy Text Adventure Game [LLM agents in text games]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with explicit dual-memory systems will outperform single-memory agents on tasks requiring both rapid adaptation and long-term planning.</li>
                <li>Agents that can switch memory modes based on context will show improved sample efficiency and generalization.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Emergent meta-cognitive strategies may arise, where agents learn to optimize when to switch memory modes in novel ways.</li>
                <li>Unexpected interference between working and episodic memory may occur, leading to novel failure modes.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with only one memory mode (working or episodic) perform as well as dual-memory agents on all tasks, the theory is challenged.</li>
                <li>If dynamic switching leads to worse performance due to overhead or confusion, the theory's utility is questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of memory mode switching on computational efficiency and latency is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory applies established cognitive principles in a new, operational way to LLM agents in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory]</li>
    <li>Baddeley (2000) The episodic buffer: a new component of working memory? [working/episodic memory distinction]</li>
    <li>Urbanek et al. (2019) Learning to Speak and Act in a Fantasy Text Adventure Game [LLM agents in text games]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Memory Utilization in LLM Agents for Text Games",
    "theory_description": "This theory posits that LLM agents optimally solve text game tasks by dynamically integrating two distinct memory processes: (1) a fast, context-sensitive working memory for immediate, local reasoning, and (2) a slower, persistent episodic memory for long-term, cross-episode inference. The agent's performance is maximized when it can fluidly switch between these memory modes based on task demands, leveraging working memory for rapid adaptation and episodic memory for strategic planning and transfer.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Dynamic Memory Mode Switching",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "detects",
                        "object": "rapidly changing or novel local context in text game"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "prioritizes",
                        "object": "working memory for immediate reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human cognition uses working memory for rapid adaptation to new or changing information.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs show improved performance on short-term tasks when provided with recent context windows.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Dual-process models in cognitive science (e.g., Kahneman's System 1 and 2) and working/episodic memory distinctions are well-established.",
                    "what_is_novel": "Application of dynamic, context-driven switching between memory modes in LLM agents for text games.",
                    "classification_explanation": "The general dual-process principle is known, but its explicit, dynamic operationalization in LLM agents for text games is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kahneman (2011) Thinking, Fast and Slow [dual-process theory]",
                        "Baddeley (2000) The episodic buffer: a new component of working memory? [working/episodic memory distinction]",
                        "Urbanek et al. (2019) Learning to Speak and Act in a Fantasy Text Adventure Game [LLM agents in text games]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Strategic Episodic Memory Retrieval",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "faces",
                        "object": "long-horizon or cross-episode dependencies in text game"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "retrieves",
                        "object": "episodic memory traces for strategic planning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Humans use episodic memory to recall past experiences for long-term planning.",
                        "uuids": []
                    },
                    {
                        "text": "RL agents with episodic memory modules outperform those without on tasks with long-term dependencies.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Episodic memory's role in planning is established in cognitive science and RL.",
                    "what_is_novel": "Explicit, context-driven retrieval of episodic memory in LLM agents for text games.",
                    "classification_explanation": "The principle is known, but its targeted application to LLM agents in text games is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Gershman & Daw (2017) Reinforcement Learning and Episodic Memory in Humans and Animals [episodic memory in planning]",
                        "Urbanek et al. (2019) Learning to Speak and Act in a Fantasy Text Adventure Game [LLM agents in text games]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with explicit dual-memory systems will outperform single-memory agents on tasks requiring both rapid adaptation and long-term planning.",
        "Agents that can switch memory modes based on context will show improved sample efficiency and generalization."
    ],
    "new_predictions_unknown": [
        "Emergent meta-cognitive strategies may arise, where agents learn to optimize when to switch memory modes in novel ways.",
        "Unexpected interference between working and episodic memory may occur, leading to novel failure modes."
    ],
    "negative_experiments": [
        "If agents with only one memory mode (working or episodic) perform as well as dual-memory agents on all tasks, the theory is challenged.",
        "If dynamic switching leads to worse performance due to overhead or confusion, the theory's utility is questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of memory mode switching on computational efficiency and latency is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can solve both short- and long-horizon tasks without explicit memory mode separation, suggesting implicit memory integration.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In tasks with only short-term dependencies, episodic memory may be unnecessary.",
        "In highly stochastic environments, episodic memory retrieval may be unreliable."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process and dual-memory models in cognitive science and RL.",
        "what_is_novel": "Dynamic, context-driven integration and switching in LLM agents for text games.",
        "classification_explanation": "The theory applies established cognitive principles in a new, operational way to LLM agents in text games.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kahneman (2011) Thinking, Fast and Slow [dual-process theory]",
            "Baddeley (2000) The episodic buffer: a new component of working memory? [working/episodic memory distinction]",
            "Urbanek et al. (2019) Learning to Speak and Act in a Fantasy Text Adventure Game [LLM agents in text games]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-593",
    "original_theory_name": "Hybrid Memory Architectures Enable Robust Long-Horizon Reasoning and Generalization in LLM Agents for Text Games",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation Integrity and Contamination Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-677</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-677</p>
                <p><strong>Name:</strong> Evaluation Integrity and Contamination Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that the integrity of LLM-generated scientific theory evaluation is fundamentally threatened by data contamination (benchmark leakage) and prompt sensitivity. It posits that evaluation results are only valid if evaluation data is demonstrably out-of-distribution from LLM training data, and that prompt diversity and robust contamination detection protocols are necessary. The theory further claims that even small amounts of contamination or prompt leakage can dramatically inflate evaluation scores, mask true model weaknesses, and undermine the validity of scientific theory assessment.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contamination Inflation Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_data &#8594; overlaps_with &#8594; LLM training data (benchmark leakage or prompt leakage)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_scores &#8594; are_artificially_inflated &#8594; relative to true generalization ability</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Continual pre-training on benchmark data (e6000.1) and benchmark-based evaluation (e6000.0) show that even small amounts of leakage dramatically inflate scores and mask true model weaknesses. <a href="../results/extraction-result-6000.html#e6000.1" class="evidence-link">[e6000.1]</a> <a href="../results/extraction-result-6000.html#e6000.0" class="evidence-link">[e6000.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Contamination risks are known, but the law's formalization for scientific theory evaluation is novel.</p>            <p><strong>What Already Exists:</strong> Benchmark leakage and contamination are known risks in ML evaluation.</p>            <p><strong>What is Novel:</strong> The law's explicit application to LLM-generated scientific theory evaluation and the quantification of inflation effects.</p>
            <p><strong>References:</strong> <ul>
    <li>Li et al. (2023) Don't Make Your LLM an Evaluation Benchmark Cheater [benchmark leakage]</li>
    <li>Sainz et al. (2023) Did chatgpt cheat on your test? [contamination detection]</li>
</ul>
            <h3>Statement 1: Prompt Sensitivity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_protocol &#8594; uses &#8594; fixed or single prompt</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_results &#8594; are_sensitive_to &#8594; prompt wording and format, leading to instability and potential gaming</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompt diversity & averaging (e6000.4), prompting strategies and ablations (e6153.2), and prompt-based LLM evaluation (e6023.4) show that evaluation results vary with prompt wording and format. <a href="../results/extraction-result-6000.html#e6000.4" class="evidence-link">[e6000.4]</a> <a href="../results/extraction-result-6153.html#e6153.2" class="evidence-link">[e6153.2]</a> <a href="../results/extraction-result-6023.html#e6023.4" class="evidence-link">[e6023.4]</a> <a href="../results/extraction-result-6106.html#e6106.3" class="evidence-link">[e6106.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Prompt sensitivity is known, but its formalization as a law for evaluation integrity is novel.</p>            <p><strong>What Already Exists:</strong> Prompt sensitivity is empirically observed in LLM outputs.</p>            <p><strong>What is Novel:</strong> The law's explicit application to evaluation integrity and the necessity of prompt diversity for valid scientific theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Li et al. (2023) Don't Make Your LLM an Evaluation Benchmark Cheater [prompt sensitivity]</li>
    <li>Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [prompt effects]</li>
</ul>
            <h3>Statement 2: Necessity of Contamination Detection Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_protocol &#8594; includes &#8594; robust contamination detection (e.g., instance-replication, semantic deduplication, human annotation)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_results &#8594; are &#8594; more trustworthy and reflective of true model ability</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Instance-replication (e6115.3), semantic deduplication (e6002.3), human annotation (e6115.5), and data-contamination-avoidance (e6105.2) are necessary to ensure evaluation integrity. <a href="../results/extraction-result-6115.html#e6115.3" class="evidence-link">[e6115.3]</a> <a href="../results/extraction-result-6002.html#e6002.3" class="evidence-link">[e6002.3]</a> <a href="../results/extraction-result-6115.html#e6115.5" class="evidence-link">[e6115.5]</a> <a href="../results/extraction-result-6105.html#e6105.2" class="evidence-link">[e6105.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Protocols exist, but the law's generalization to scientific theory evaluation is novel.</p>            <p><strong>What Already Exists:</strong> Contamination detection protocols are used in some studies.</p>            <p><strong>What is Novel:</strong> The law that robust contamination detection is necessary for valid scientific theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Li et al. (2023) Don't Make Your LLM an Evaluation Benchmark Cheater [contamination detection]</li>
    <li>Sainz et al. (2023) Did chatgpt cheat on your test? [contamination detection]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new scientific theory evaluation dataset is constructed from post-training data and is carefully deduplicated, LLM evaluation scores will drop compared to contaminated or overlapping datasets.</li>
                <li>If prompt diversity and averaging are used, the variance in evaluation results will decrease and robustness will increase.</li>
                <li>If robust contamination detection protocols are omitted, evaluation scores will be artificially inflated and not reflect true model ability.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If new contamination-resistant training and evaluation protocols are developed, it may be possible to achieve high evaluation integrity even as LLMs scale and training data grows.</li>
                <li>If LLMs are trained with explicit anti-leakage objectives, their evaluation scores on out-of-distribution scientific theory tasks may become more reliable.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If evaluation scores remain stable and reflective of true model ability even when evaluation data overlaps with training data, the contamination inflation law would be falsified.</li>
                <li>If prompt sensitivity is eliminated by new architectures or protocols, the prompt sensitivity law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some domains with inherently public or widely known data may be impossible to fully decontaminate, limiting the applicability of the theory. <a href="../results/extraction-result-6105.html#e6105.2" class="evidence-link">[e6105.2]</a> <a href="../results/extraction-result-6002.html#e6002.3" class="evidence-link">[e6002.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> Elements exist, but the comprehensive, formalized theory is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Li et al. (2023) Don't Make Your LLM an Evaluation Benchmark Cheater [benchmark leakage, prompt sensitivity]</li>
    <li>Sainz et al. (2023) Did chatgpt cheat on your test? [contamination detection]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Evaluation Integrity and Contamination Theory",
    "theory_description": "This theory asserts that the integrity of LLM-generated scientific theory evaluation is fundamentally threatened by data contamination (benchmark leakage) and prompt sensitivity. It posits that evaluation results are only valid if evaluation data is demonstrably out-of-distribution from LLM training data, and that prompt diversity and robust contamination detection protocols are necessary. The theory further claims that even small amounts of contamination or prompt leakage can dramatically inflate evaluation scores, mask true model weaknesses, and undermine the validity of scientific theory assessment.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contamination Inflation Law",
                "if": [
                    {
                        "subject": "evaluation_data",
                        "relation": "overlaps_with",
                        "object": "LLM training data (benchmark leakage or prompt leakage)"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_scores",
                        "relation": "are_artificially_inflated",
                        "object": "relative to true generalization ability"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Continual pre-training on benchmark data (e6000.1) and benchmark-based evaluation (e6000.0) show that even small amounts of leakage dramatically inflate scores and mask true model weaknesses.",
                        "uuids": [
                            "e6000.1",
                            "e6000.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Benchmark leakage and contamination are known risks in ML evaluation.",
                    "what_is_novel": "The law's explicit application to LLM-generated scientific theory evaluation and the quantification of inflation effects.",
                    "classification_explanation": "Contamination risks are known, but the law's formalization for scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Li et al. (2023) Don't Make Your LLM an Evaluation Benchmark Cheater [benchmark leakage]",
                        "Sainz et al. (2023) Did chatgpt cheat on your test? [contamination detection]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Sensitivity Law",
                "if": [
                    {
                        "subject": "evaluation_protocol",
                        "relation": "uses",
                        "object": "fixed or single prompt"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_results",
                        "relation": "are_sensitive_to",
                        "object": "prompt wording and format, leading to instability and potential gaming"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompt diversity & averaging (e6000.4), prompting strategies and ablations (e6153.2), and prompt-based LLM evaluation (e6023.4) show that evaluation results vary with prompt wording and format.",
                        "uuids": [
                            "e6000.4",
                            "e6153.2",
                            "e6023.4",
                            "e6106.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt sensitivity is empirically observed in LLM outputs.",
                    "what_is_novel": "The law's explicit application to evaluation integrity and the necessity of prompt diversity for valid scientific theory evaluation.",
                    "classification_explanation": "Prompt sensitivity is known, but its formalization as a law for evaluation integrity is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Li et al. (2023) Don't Make Your LLM an Evaluation Benchmark Cheater [prompt sensitivity]",
                        "Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [prompt effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Necessity of Contamination Detection Law",
                "if": [
                    {
                        "subject": "evaluation_protocol",
                        "relation": "includes",
                        "object": "robust contamination detection (e.g., instance-replication, semantic deduplication, human annotation)"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_results",
                        "relation": "are",
                        "object": "more trustworthy and reflective of true model ability"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Instance-replication (e6115.3), semantic deduplication (e6002.3), human annotation (e6115.5), and data-contamination-avoidance (e6105.2) are necessary to ensure evaluation integrity.",
                        "uuids": [
                            "e6115.3",
                            "e6002.3",
                            "e6115.5",
                            "e6105.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Contamination detection protocols are used in some studies.",
                    "what_is_novel": "The law that robust contamination detection is necessary for valid scientific theory evaluation.",
                    "classification_explanation": "Protocols exist, but the law's generalization to scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Li et al. (2023) Don't Make Your LLM an Evaluation Benchmark Cheater [contamination detection]",
                        "Sainz et al. (2023) Did chatgpt cheat on your test? [contamination detection]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new scientific theory evaluation dataset is constructed from post-training data and is carefully deduplicated, LLM evaluation scores will drop compared to contaminated or overlapping datasets.",
        "If prompt diversity and averaging are used, the variance in evaluation results will decrease and robustness will increase.",
        "If robust contamination detection protocols are omitted, evaluation scores will be artificially inflated and not reflect true model ability."
    ],
    "new_predictions_unknown": [
        "If new contamination-resistant training and evaluation protocols are developed, it may be possible to achieve high evaluation integrity even as LLMs scale and training data grows.",
        "If LLMs are trained with explicit anti-leakage objectives, their evaluation scores on out-of-distribution scientific theory tasks may become more reliable."
    ],
    "negative_experiments": [
        "If evaluation scores remain stable and reflective of true model ability even when evaluation data overlaps with training data, the contamination inflation law would be falsified.",
        "If prompt sensitivity is eliminated by new architectures or protocols, the prompt sensitivity law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some domains with inherently public or widely known data may be impossible to fully decontaminate, limiting the applicability of the theory.",
            "uuids": [
                "e6105.2",
                "e6002.3"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, LLMs show robust performance even on datasets with potential overlap, possibly due to generalization or redundancy in training data.",
            "uuids": [
                "e6011.2",
                "e6166.2"
            ]
        }
    ],
    "special_cases": [
        "Domains with limited or proprietary data may be less susceptible to contamination.",
        "Tasks with highly structured or formal outputs (e.g., code generation) may be less sensitive to prompt variation."
    ],
    "existing_theory": {
        "what_already_exists": "Benchmark leakage and prompt sensitivity are known risks in ML evaluation.",
        "what_is_novel": "The explicit formalization of these as laws for scientific theory evaluation and the quantification of their effects.",
        "classification_explanation": "Elements exist, but the comprehensive, formalized theory is new.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Li et al. (2023) Don't Make Your LLM an Evaluation Benchmark Cheater [benchmark leakage, prompt sensitivity]",
            "Sainz et al. (2023) Did chatgpt cheat on your test? [contamination detection]"
        ]
    },
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
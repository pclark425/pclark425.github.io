<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain-Alignment Theory of LLM Scientific Simulation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1636</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1636</p>
                <p><strong>Name:</strong> Domain-Alignment Theory of LLM Scientific Simulation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory posits that the accuracy of LLMs as text-based simulators in scientific subdomains is primarily determined by the degree of alignment between the model's internal representations and the formal structure of the target scientific domain. High alignment enables accurate simulation, while misalignment (due to differences in language, logic, or conceptual frameworks) leads to systematic errors, regardless of model scale.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Domain Representation Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM internal representations &#8594; are_isomorphic_to &#8594; formal structure of scientific subdomain</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; simulates_with_high_accuracy &#8594; phenomena in that subdomain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best in domains where the language and logic of the subdomain closely match patterns in their training data (e.g., chemistry, basic physics). </li>
    <li>Systematic errors arise in domains with unique jargon, notation, or conceptual frameworks not well-represented in general text corpora. </li>
    <li>Fine-tuning on domain-specific corpora improves simulation accuracy, suggesting increased alignment. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to transfer learning and domain adaptation, the isomorphism framing and its predictive consequences are novel.</p>            <p><strong>What Already Exists:</strong> The importance of domain-specific fine-tuning and representation learning is recognized, but not formalized as isomorphism between LLM representations and scientific domain structure.</p>            <p><strong>What is Novel:</strong> The explicit framing of simulation accuracy as a function of representational isomorphism between LLMs and scientific domains is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation, transfer learning]</li>
    <li>Rogers et al. (2021) A Primer in BERTology [representation learning in LLMs]</li>
</ul>
            <h3>Statement 1: Misalignment Error Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM internal representations &#8594; are_non_isomorphic_to &#8594; formal structure of scientific subdomain</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; produces_systematic_errors &#8594; simulations in that subdomain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs often misinterpret or hallucinate in domains with non-standard notation or logic (e.g., advanced mathematics, symbolic logic). </li>
    <li>Systematic simulation errors persist even with increased model size if representational misalignment is not addressed. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The causal framing and focus on isomorphism is novel, though related to known issues in domain adaptation.</p>            <p><strong>What Already Exists:</strong> Systematic errors in LLMs are known, but not explicitly linked to representational misalignment with scientific domains.</p>            <p><strong>What is Novel:</strong> The law formalizes the causal link between representational misalignment and systematic simulation errors.</p>
            <p><strong>References:</strong> <ul>
    <li>Marcus & Davis (2020) GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about [systematic errors in LLMs]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM is fine-tuned on a corpus that is structurally isomorphic to a scientific subdomain, its simulation accuracy in that subdomain will increase.</li>
                <li>LLMs will systematically fail in subdomains with unique representational structures not present in their training data.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new architecture is developed that can dynamically align its representations to arbitrary scientific domain structures, LLMs may achieve near-universal simulation accuracy.</li>
                <li>If a scientific domain is intentionally constructed to be maximally non-isomorphic to natural language, LLMs may be unable to simulate it regardless of scale.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs can accurately simulate domains with non-isomorphic structures without fine-tuning or explicit alignment, the theory would be challenged.</li>
                <li>If systematic errors do not correlate with representational misalignment, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs use analogical reasoning or transfer from related domains to overcome representational misalignment. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends existing ideas in domain adaptation with a novel formalism and predictive framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation]</li>
    <li>Rogers et al. (2021) A Primer in BERTology [representation learning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain-Alignment Theory of LLM Scientific Simulation",
    "theory_description": "This theory posits that the accuracy of LLMs as text-based simulators in scientific subdomains is primarily determined by the degree of alignment between the model's internal representations and the formal structure of the target scientific domain. High alignment enables accurate simulation, while misalignment (due to differences in language, logic, or conceptual frameworks) leads to systematic errors, regardless of model scale.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Domain Representation Alignment Law",
                "if": [
                    {
                        "subject": "LLM internal representations",
                        "relation": "are_isomorphic_to",
                        "object": "formal structure of scientific subdomain"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "simulates_with_high_accuracy",
                        "object": "phenomena in that subdomain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best in domains where the language and logic of the subdomain closely match patterns in their training data (e.g., chemistry, basic physics).",
                        "uuids": []
                    },
                    {
                        "text": "Systematic errors arise in domains with unique jargon, notation, or conceptual frameworks not well-represented in general text corpora.",
                        "uuids": []
                    },
                    {
                        "text": "Fine-tuning on domain-specific corpora improves simulation accuracy, suggesting increased alignment.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The importance of domain-specific fine-tuning and representation learning is recognized, but not formalized as isomorphism between LLM representations and scientific domain structure.",
                    "what_is_novel": "The explicit framing of simulation accuracy as a function of representational isomorphism between LLMs and scientific domains is new.",
                    "classification_explanation": "While related to transfer learning and domain adaptation, the isomorphism framing and its predictive consequences are novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation, transfer learning]",
                        "Rogers et al. (2021) A Primer in BERTology [representation learning in LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Misalignment Error Law",
                "if": [
                    {
                        "subject": "LLM internal representations",
                        "relation": "are_non_isomorphic_to",
                        "object": "formal structure of scientific subdomain"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "produces_systematic_errors",
                        "object": "simulations in that subdomain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs often misinterpret or hallucinate in domains with non-standard notation or logic (e.g., advanced mathematics, symbolic logic).",
                        "uuids": []
                    },
                    {
                        "text": "Systematic simulation errors persist even with increased model size if representational misalignment is not addressed.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Systematic errors in LLMs are known, but not explicitly linked to representational misalignment with scientific domains.",
                    "what_is_novel": "The law formalizes the causal link between representational misalignment and systematic simulation errors.",
                    "classification_explanation": "The causal framing and focus on isomorphism is novel, though related to known issues in domain adaptation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Marcus & Davis (2020) GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about [systematic errors in LLMs]",
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM is fine-tuned on a corpus that is structurally isomorphic to a scientific subdomain, its simulation accuracy in that subdomain will increase.",
        "LLMs will systematically fail in subdomains with unique representational structures not present in their training data."
    ],
    "new_predictions_unknown": [
        "If a new architecture is developed that can dynamically align its representations to arbitrary scientific domain structures, LLMs may achieve near-universal simulation accuracy.",
        "If a scientific domain is intentionally constructed to be maximally non-isomorphic to natural language, LLMs may be unable to simulate it regardless of scale."
    ],
    "negative_experiments": [
        "If LLMs can accurately simulate domains with non-isomorphic structures without fine-tuning or explicit alignment, the theory would be challenged.",
        "If systematic errors do not correlate with representational misalignment, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs use analogical reasoning or transfer from related domains to overcome representational misalignment.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs demonstrate surprising generalization to new domains with minimal exposure, suggesting other mechanisms may compensate for misalignment.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Domains with highly redundant or overlapping representational structures may be less affected by misalignment.",
        "Meta-learning or retrieval-augmented methods may mitigate misalignment effects."
    ],
    "existing_theory": {
        "what_already_exists": "Domain adaptation and transfer learning are established, but not formalized as representational isomorphism for simulation accuracy.",
        "what_is_novel": "The explicit isomorphism framing and its predictive consequences for simulation accuracy are new.",
        "classification_explanation": "The theory extends existing ideas in domain adaptation with a novel formalism and predictive framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation]",
            "Rogers et al. (2021) A Primer in BERTology [representation learning]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-636",
    "original_theory_name": "Theory of Simulation Fidelity Boundaries: The Interplay of Model Scale, Alignment, and Prompt/Context Design",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Equivalence Clustering Theory for Uncertainty Quantification - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-80</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-80</p>
                <p><strong>Name:</strong> Semantic Equivalence Clustering Theory for Uncertainty Quantification</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about variability and reproducibility in language model-driven scientific experimentation, based on the following results.</p>
                <p><strong>Description:</strong> Language model uncertainty should be measured over semantic meanings rather than token sequences, because models can express the same meaning in many lexically different ways. By clustering sampled outputs into semantic equivalence classes (via bidirectional entailment using NLI models) and computing entropy over these clusters rather than individual sequences, we obtain a more robust uncertainty estimate that is invariant to paraphrasing and surface-form variation. This 'semantic entropy' better predicts model correctness than token-level entropy because it captures genuine uncertainty about the answer rather than uncertainty about how to phrase it. The theory predicts that semantic entropy will outperform token-level methods especially for tasks where correct answers have many valid phrasings, and will be more robust to prompt variations and model differences. The approach can be implemented either with full probability distributions (when available) or as discrete semantic entropy using empirical cluster counts (for black-box models), and requires careful attention to sampling diversity, entailment model quality, and the number of samples used.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Uncertainty should be measured over meanings (semantic equivalence classes) rather than token sequences to be invariant to paraphrasing and surface-form variation.</li>
                <li>Semantic entropy H_semantic = -∑ p(meaning_i) log p(meaning_i) where meanings are defined by bidirectional entailment clusters using NLI models.</li>
                <li>Discrete semantic entropy can be computed from empirical cluster frequencies when token probabilities are unavailable: H_discrete = -∑ (n_i/N) log(n_i/N) where n_i is the count of samples in cluster i.</li>
                <li>Semantic entropy better predicts correctness than token entropy because it separates genuine uncertainty (multiple possible answers) from phrasing uncertainty (multiple ways to say the same answer).</li>
                <li>The effectiveness of semantic entropy increases with the number of valid phrasings for correct answers: tasks with high paraphrase diversity benefit most.</li>
                <li>Semantic clustering requires a reliable entailment model; clustering accuracy (>90% demonstrated) directly affects uncertainty estimation quality.</li>
                <li>Sampling diversity (temperature, top-p, sampling method) must be sufficient to explore the semantic space; multinomial sampling outperforms beam sampling for uncertainty estimation.</li>
                <li>An intermediate sampling temperature (approximately T=0.5) optimizes the trade-off between diversity and accuracy for uncertainty estimation.</li>
                <li>Approximately M=10 samples are sufficient for sentence-level tasks, with diminishing returns beyond this point; fewer samples (M=3) can be used for paragraph-level with aggregation across questions.</li>
                <li>The method scales across model sizes (2.7B to 30B+) and can be applied to both white-box models (with probabilities) and black-box models (using discrete counts).</li>
                <li>For long-form generation, semantic entropy can be applied by decomposing outputs into atomic claims/questions and aggregating uncertainty across them.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Semantic entropy outperforms token-sequence entropy baselines (TriviaQA AUROC 0.828 vs 0.802 for normalized entropy), showing meaning-level uncertainty is more predictive. <a href="../results/extraction-result-605.html#e605.0" class="evidence-link">[e605.0]</a> </li>
    <li>Semantic entropy shows higher AUROC (0.790 average) than P(True) baseline (0.698 average), demonstrating superiority over self-consistency methods. <a href="../results/extraction-result-663.html#e663.5" class="evidence-link">[e663.5]</a> </li>
    <li>Clustering by bidirectional entailment achieves 92.7% accuracy on TriviaQA and 95.3% on CoQA for semantic equivalence, validating the clustering approach using off-the-shelf NLI models (Deberta-large). <a href="../results/extraction-result-605.html#e605.0" class="evidence-link">[e605.0]</a> </li>
    <li>Number of semantically distinct answers differs between correct (1.27 clusters on CoQA, 1.89 on TriviaQA) and incorrect (1.77 clusters on CoQA, 3.89 on TriviaQA) questions, showing semantic diversity predicts correctness. <a href="../results/extraction-result-605.html#e605.0" class="evidence-link">[e605.0]</a> </li>
    <li>Discrete semantic entropy (using empirical cluster counts) enables use with black-box models like GPT-4 and outperforms baselines on paragraph-length generation (biography factuality). <a href="../results/extraction-result-663.html#e663.1" class="evidence-link">[e663.1]</a> </li>
    <li>Semantic entropy is more robust to domain shift than P(True), maintaining performance on out-of-distribution tasks. <a href="../results/extraction-result-663.html#e663.5" class="evidence-link">[e663.5]</a> </li>
    <li>Multinomial sampling produces higher diversity (0.490) and better semantic entropy AUROC (0.758) than beam sampling (0.258 diversity, 0.735 AUROC), showing sampling method affects uncertainty estimation quality. <a href="../results/extraction-result-605.html#e605.0" class="evidence-link">[e605.0]</a> </li>
    <li>Semantic entropy works across model sizes (2.7B to 30B OPT models) with consistent improvements over baselines, demonstrating scalability. <a href="../results/extraction-result-605.html#e605.0" class="evidence-link">[e605.0]</a> </li>
    <li>Using M=10 sampled answers per question provides sufficient performance for sentence-level experiments, with diminishing returns beyond this point. <a href="../results/extraction-result-605.html#e605.0" class="evidence-link">[e605.0]</a> <a href="../results/extraction-result-663.html#e663.4" class="evidence-link">[e663.4]</a> </li>
    <li>Intermediate sampling temperature (approximately T=0.5) balances diversity and accuracy for uncertainty estimation, with extreme temperatures degrading performance. <a href="../results/extraction-result-605.html#e605.0" class="evidence-link">[e605.0]</a> </li>
    <li>For paragraph-level experiments, using M=3 regenerated answers per question combined with averaging across multiple questions reduces variance while maintaining effectiveness. <a href="../results/extraction-result-663.html#e663.1" class="evidence-link">[e663.1]</a> </li>
    <li>Semantic entropy can be extended to long-form generation by decomposing claims into atomic questions and aggregating entropies across questions. <a href="../results/extraction-result-663.html#e663.1" class="evidence-link">[e663.1]</a> </li>
    <li>The method requires standardized sampling protocols (e.g., temperature=1, top-p=0.9, top-k=50) for reliable and reproducible uncertainty estimates. <a href="../results/extraction-result-663.html#e663.4" class="evidence-link">[e663.4]</a> </li>
    <li>Including the original factual claim in clustering improves detection of incorrect claims in paragraph-level generation. <a href="../results/extraction-result-663.html#e663.1" class="evidence-link">[e663.1]</a> </li>
    <li>Model refusals (e.g., 'unknown' responses) can be handled by treating high prevalence of refusals (>50%) as maximal uncertainty. <a href="../results/extraction-result-663.html#e663.1" class="evidence-link">[e663.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Semantic entropy will show larger improvements over token entropy for open-ended QA compared to tasks with formulaic answers (e.g., arithmetic), because formulaic answers have fewer valid paraphrases.</li>
                <li>Using better entailment models (e.g., fine-tuned on domain-specific data or larger models) will improve semantic entropy's predictive power proportionally to entailment accuracy gains.</li>
                <li>Semantic entropy will be more robust to prompt variations than token entropy, because prompts affect phrasing but not semantic content of answers.</li>
                <li>Combining semantic entropy with other uncertainty measures (e.g., token probability, P(True)) will show complementary benefits, with semantic entropy capturing meaning-level uncertainty and others capturing different aspects.</li>
                <li>For multilingual tasks, using multilingual NLI models for clustering will enable semantic entropy to work across languages while maintaining its advantages over token-level methods.</li>
                <li>Increasing the number of samples beyond M=10 for sentence-level tasks will show diminishing returns in AUROC improvement, validating the sufficiency of M=10.</li>
                <li>Using top-p sampling with p=0.9 will consistently outperform greedy or beam search decoding for semantic entropy estimation across different model families.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether semantic entropy works for structured outputs (code, JSON, mathematical expressions) where 'semantic equivalence' requires execution-based or formal verification rather than NLI-based clustering.</li>
                <li>Whether semantic entropy can detect subtle factual errors that are semantically similar to correct answers (e.g., off-by-one errors in dates, near-miss entity names).</li>
                <li>Whether the optimal number of samples M depends on task complexity, answer space size, or model calibration, or whether M=10 is truly universal across all sentence-level tasks.</li>
                <li>Whether semantic entropy can be computed efficiently for very long outputs (e.g., full documents, multi-page reports) or whether computational costs of clustering scale prohibitively.</li>
                <li>Whether semantic entropy maintains its advantages when models are specifically trained or fine-tuned to reduce paraphrasing (e.g., through constrained decoding or style-controlled generation).</li>
                <li>Whether semantic entropy can be extended to multi-modal outputs (e.g., image generation, code with documentation) where semantic equivalence spans multiple modalities.</li>
                <li>Whether adversarial examples can be constructed that fool the NLI-based clustering while maintaining human-perceived semantic differences, thus breaking the uncertainty estimation.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where token entropy consistently outperforms semantic entropy would challenge the universality of the meaning-level approach and suggest boundary conditions.</li>
                <li>Demonstrating that random clustering of outputs (rather than entailment-based clustering) produces similar uncertainty estimates would challenge the core mechanism and suggest the benefits come from aggregation rather than semantic grouping.</li>
                <li>Showing that semantic entropy fails on tasks with low paraphrase diversity (e.g., yes/no questions, multiple choice with fixed options) would challenge its claimed robustness and suggest it only helps for open-ended generation.</li>
                <li>Finding that semantic entropy is highly sensitive to the choice of entailment model (e.g., different NLI models produce very different AUROC) would challenge its practical applicability and robustness.</li>
                <li>Demonstrating that semantic entropy performs worse than baselines when sampling temperature is optimized separately for each method would challenge whether its advantages are fundamental or just due to better default hyperparameters.</li>
                <li>Finding that semantic entropy's advantages disappear when token-level methods are given access to the same number of samples would challenge whether the benefit comes from semantic clustering or simply from using more samples.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to define semantic equivalence for structured outputs (code, JSON, mathematical expressions) where execution or formal verification may be needed rather than NLI. </li>
    <li>The computational cost and scalability of clustering for very large sample sizes (M >> 10) or very long outputs (documents, multi-page reports). </li>
    <li>How to handle partial correctness or graded semantic similarity rather than binary equivalence (e.g., answers that are 'mostly correct' or 'partially correct'). </li>
    <li>Whether and how semantic entropy can be extended to multi-modal outputs where semantic equivalence spans text, images, code, etc. </li>
    <li>The interaction between semantic entropy and model calibration: whether well-calibrated models show different semantic entropy patterns than poorly-calibrated ones. </li>
    <li>How to handle cases where the NLI model itself is uncertain or produces inconsistent entailment judgments for the same pair of outputs. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Kuhn et al. (2023) Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation [Original semantic entropy paper introducing the core concept]</li>
    <li>Farquhar et al. (2024) Detecting hallucinations in large language models using semantic entropy [Extension to confabulation detection and paragraph-level generation]</li>
    <li>Malinin & Gales (2021) Uncertainty Estimation in Autoregressive Structured Prediction [Prior work on uncertainty in structured prediction, but uses token-level rather than semantic clustering]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [Related self-consistency approach but uses majority voting rather than entropy over semantic clusters]</li>
    <li>Lin et al. (2022) Teaching models to express their uncertainty in words [Related work on verbalized uncertainty but doesn't use semantic clustering]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Equivalence Clustering Theory for Uncertainty Quantification",
    "theory_description": "Language model uncertainty should be measured over semantic meanings rather than token sequences, because models can express the same meaning in many lexically different ways. By clustering sampled outputs into semantic equivalence classes (via bidirectional entailment using NLI models) and computing entropy over these clusters rather than individual sequences, we obtain a more robust uncertainty estimate that is invariant to paraphrasing and surface-form variation. This 'semantic entropy' better predicts model correctness than token-level entropy because it captures genuine uncertainty about the answer rather than uncertainty about how to phrase it. The theory predicts that semantic entropy will outperform token-level methods especially for tasks where correct answers have many valid phrasings, and will be more robust to prompt variations and model differences. The approach can be implemented either with full probability distributions (when available) or as discrete semantic entropy using empirical cluster counts (for black-box models), and requires careful attention to sampling diversity, entailment model quality, and the number of samples used.",
    "supporting_evidence": [
        {
            "text": "Semantic entropy outperforms token-sequence entropy baselines (TriviaQA AUROC 0.828 vs 0.802 for normalized entropy), showing meaning-level uncertainty is more predictive.",
            "uuids": [
                "e605.0"
            ]
        },
        {
            "text": "Semantic entropy shows higher AUROC (0.790 average) than P(True) baseline (0.698 average), demonstrating superiority over self-consistency methods.",
            "uuids": [
                "e663.5"
            ]
        },
        {
            "text": "Clustering by bidirectional entailment achieves 92.7% accuracy on TriviaQA and 95.3% on CoQA for semantic equivalence, validating the clustering approach using off-the-shelf NLI models (Deberta-large).",
            "uuids": [
                "e605.0"
            ]
        },
        {
            "text": "Number of semantically distinct answers differs between correct (1.27 clusters on CoQA, 1.89 on TriviaQA) and incorrect (1.77 clusters on CoQA, 3.89 on TriviaQA) questions, showing semantic diversity predicts correctness.",
            "uuids": [
                "e605.0"
            ]
        },
        {
            "text": "Discrete semantic entropy (using empirical cluster counts) enables use with black-box models like GPT-4 and outperforms baselines on paragraph-length generation (biography factuality).",
            "uuids": [
                "e663.1"
            ]
        },
        {
            "text": "Semantic entropy is more robust to domain shift than P(True), maintaining performance on out-of-distribution tasks.",
            "uuids": [
                "e663.5"
            ]
        },
        {
            "text": "Multinomial sampling produces higher diversity (0.490) and better semantic entropy AUROC (0.758) than beam sampling (0.258 diversity, 0.735 AUROC), showing sampling method affects uncertainty estimation quality.",
            "uuids": [
                "e605.0"
            ]
        },
        {
            "text": "Semantic entropy works across model sizes (2.7B to 30B OPT models) with consistent improvements over baselines, demonstrating scalability.",
            "uuids": [
                "e605.0"
            ]
        },
        {
            "text": "Using M=10 sampled answers per question provides sufficient performance for sentence-level experiments, with diminishing returns beyond this point.",
            "uuids": [
                "e605.0",
                "e663.4"
            ]
        },
        {
            "text": "Intermediate sampling temperature (approximately T=0.5) balances diversity and accuracy for uncertainty estimation, with extreme temperatures degrading performance.",
            "uuids": [
                "e605.0"
            ]
        },
        {
            "text": "For paragraph-level experiments, using M=3 regenerated answers per question combined with averaging across multiple questions reduces variance while maintaining effectiveness.",
            "uuids": [
                "e663.1"
            ]
        },
        {
            "text": "Semantic entropy can be extended to long-form generation by decomposing claims into atomic questions and aggregating entropies across questions.",
            "uuids": [
                "e663.1"
            ]
        },
        {
            "text": "The method requires standardized sampling protocols (e.g., temperature=1, top-p=0.9, top-k=50) for reliable and reproducible uncertainty estimates.",
            "uuids": [
                "e663.4"
            ]
        },
        {
            "text": "Including the original factual claim in clustering improves detection of incorrect claims in paragraph-level generation.",
            "uuids": [
                "e663.1"
            ]
        },
        {
            "text": "Model refusals (e.g., 'unknown' responses) can be handled by treating high prevalence of refusals (&gt;50%) as maximal uncertainty.",
            "uuids": [
                "e663.1"
            ]
        }
    ],
    "theory_statements": [
        "Uncertainty should be measured over meanings (semantic equivalence classes) rather than token sequences to be invariant to paraphrasing and surface-form variation.",
        "Semantic entropy H_semantic = -∑ p(meaning_i) log p(meaning_i) where meanings are defined by bidirectional entailment clusters using NLI models.",
        "Discrete semantic entropy can be computed from empirical cluster frequencies when token probabilities are unavailable: H_discrete = -∑ (n_i/N) log(n_i/N) where n_i is the count of samples in cluster i.",
        "Semantic entropy better predicts correctness than token entropy because it separates genuine uncertainty (multiple possible answers) from phrasing uncertainty (multiple ways to say the same answer).",
        "The effectiveness of semantic entropy increases with the number of valid phrasings for correct answers: tasks with high paraphrase diversity benefit most.",
        "Semantic clustering requires a reliable entailment model; clustering accuracy (&gt;90% demonstrated) directly affects uncertainty estimation quality.",
        "Sampling diversity (temperature, top-p, sampling method) must be sufficient to explore the semantic space; multinomial sampling outperforms beam sampling for uncertainty estimation.",
        "An intermediate sampling temperature (approximately T=0.5) optimizes the trade-off between diversity and accuracy for uncertainty estimation.",
        "Approximately M=10 samples are sufficient for sentence-level tasks, with diminishing returns beyond this point; fewer samples (M=3) can be used for paragraph-level with aggregation across questions.",
        "The method scales across model sizes (2.7B to 30B+) and can be applied to both white-box models (with probabilities) and black-box models (using discrete counts).",
        "For long-form generation, semantic entropy can be applied by decomposing outputs into atomic claims/questions and aggregating uncertainty across them."
    ],
    "new_predictions_likely": [
        "Semantic entropy will show larger improvements over token entropy for open-ended QA compared to tasks with formulaic answers (e.g., arithmetic), because formulaic answers have fewer valid paraphrases.",
        "Using better entailment models (e.g., fine-tuned on domain-specific data or larger models) will improve semantic entropy's predictive power proportionally to entailment accuracy gains.",
        "Semantic entropy will be more robust to prompt variations than token entropy, because prompts affect phrasing but not semantic content of answers.",
        "Combining semantic entropy with other uncertainty measures (e.g., token probability, P(True)) will show complementary benefits, with semantic entropy capturing meaning-level uncertainty and others capturing different aspects.",
        "For multilingual tasks, using multilingual NLI models for clustering will enable semantic entropy to work across languages while maintaining its advantages over token-level methods.",
        "Increasing the number of samples beyond M=10 for sentence-level tasks will show diminishing returns in AUROC improvement, validating the sufficiency of M=10.",
        "Using top-p sampling with p=0.9 will consistently outperform greedy or beam search decoding for semantic entropy estimation across different model families."
    ],
    "new_predictions_unknown": [
        "Whether semantic entropy works for structured outputs (code, JSON, mathematical expressions) where 'semantic equivalence' requires execution-based or formal verification rather than NLI-based clustering.",
        "Whether semantic entropy can detect subtle factual errors that are semantically similar to correct answers (e.g., off-by-one errors in dates, near-miss entity names).",
        "Whether the optimal number of samples M depends on task complexity, answer space size, or model calibration, or whether M=10 is truly universal across all sentence-level tasks.",
        "Whether semantic entropy can be computed efficiently for very long outputs (e.g., full documents, multi-page reports) or whether computational costs of clustering scale prohibitively.",
        "Whether semantic entropy maintains its advantages when models are specifically trained or fine-tuned to reduce paraphrasing (e.g., through constrained decoding or style-controlled generation).",
        "Whether semantic entropy can be extended to multi-modal outputs (e.g., image generation, code with documentation) where semantic equivalence spans multiple modalities.",
        "Whether adversarial examples can be constructed that fool the NLI-based clustering while maintaining human-perceived semantic differences, thus breaking the uncertainty estimation."
    ],
    "negative_experiments": [
        "Finding tasks where token entropy consistently outperforms semantic entropy would challenge the universality of the meaning-level approach and suggest boundary conditions.",
        "Demonstrating that random clustering of outputs (rather than entailment-based clustering) produces similar uncertainty estimates would challenge the core mechanism and suggest the benefits come from aggregation rather than semantic grouping.",
        "Showing that semantic entropy fails on tasks with low paraphrase diversity (e.g., yes/no questions, multiple choice with fixed options) would challenge its claimed robustness and suggest it only helps for open-ended generation.",
        "Finding that semantic entropy is highly sensitive to the choice of entailment model (e.g., different NLI models produce very different AUROC) would challenge its practical applicability and robustness.",
        "Demonstrating that semantic entropy performs worse than baselines when sampling temperature is optimized separately for each method would challenge whether its advantages are fundamental or just due to better default hyperparameters.",
        "Finding that semantic entropy's advantages disappear when token-level methods are given access to the same number of samples would challenge whether the benefit comes from semantic clustering or simply from using more samples."
    ],
    "unaccounted_for": [
        {
            "text": "How to define semantic equivalence for structured outputs (code, JSON, mathematical expressions) where execution or formal verification may be needed rather than NLI.",
            "uuids": []
        },
        {
            "text": "The computational cost and scalability of clustering for very large sample sizes (M &gt;&gt; 10) or very long outputs (documents, multi-page reports).",
            "uuids": []
        },
        {
            "text": "How to handle partial correctness or graded semantic similarity rather than binary equivalence (e.g., answers that are 'mostly correct' or 'partially correct').",
            "uuids": []
        },
        {
            "text": "Whether and how semantic entropy can be extended to multi-modal outputs where semantic equivalence spans text, images, code, etc.",
            "uuids": []
        },
        {
            "text": "The interaction between semantic entropy and model calibration: whether well-calibrated models show different semantic entropy patterns than poorly-calibrated ones.",
            "uuids": []
        },
        {
            "text": "How to handle cases where the NLI model itself is uncertain or produces inconsistent entailment judgments for the same pair of outputs.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "P(True) can be competitive in narrow, well-understood settings and benefits from model scale, suggesting semantic entropy's advantage may be task-dependent and diminish for certain problem types.",
            "uuids": [
                "e663.5"
            ]
        },
        {
            "text": "At very high temperatures (T=1.5), entailment classifier accuracy drops to 61%, suggesting semantic clustering can fail under extreme sampling conditions and may not be robust to all hyperparameter choices.",
            "uuids": [
                "e605.0"
            ]
        },
        {
            "text": "P(True) with few-shot prompting can improve substantially and may overtake discrete semantic entropy after rejecting top 20% of questions in paragraph-level experiments, suggesting the advantage is not universal across all rejection thresholds.",
            "uuids": [
                "e663.5"
            ]
        },
        {
            "text": "Context-size constraints limit the number of few-shot examples for P(True), which may artificially disadvantage it compared to semantic entropy in some comparisons.",
            "uuids": [
                "e663.5"
            ]
        }
    ],
    "special_cases": [
        "For tasks with unique correct answers and no valid paraphrases (e.g., single-word answers, yes/no questions), semantic entropy reduces to token entropy and provides no additional benefit.",
        "For tasks with ambiguous or subjective answers where multiple genuinely different answers are valid, semantic clustering may incorrectly group different valid answers together, underestimating true uncertainty.",
        "For very short outputs (e.g., single words, numbers), semantic equivalence is often trivial and clustering provides minimal benefit over token-level methods.",
        "For multilingual tasks, semantic equivalence must account for cross-lingual meaning, requiring multilingual entailment models; monolingual NLI models will fail.",
        "For paragraph-length or longer outputs, direct clustering becomes impractical and the method must be adapted to use claim decomposition and question generation, changing the uncertainty estimation procedure.",
        "When using discrete semantic entropy with black-box models, Monte Carlo approximations may be needed for probability estimates, introducing additional sampling variance.",
        "For tasks where model refusals are common (e.g., safety-sensitive queries), special handling is needed to treat high refusal rates as maximal uncertainty.",
        "At extreme sampling temperatures (very low or very high), either diversity is insufficient to explore the semantic space or clustering accuracy degrades, limiting the method's effectiveness.",
        "For structured outputs (code, formal languages), standard NLI-based clustering may not capture semantic equivalence correctly, requiring domain-specific equivalence checking."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Kuhn et al. (2023) Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation [Original semantic entropy paper introducing the core concept]",
            "Farquhar et al. (2024) Detecting hallucinations in large language models using semantic entropy [Extension to confabulation detection and paragraph-level generation]",
            "Malinin & Gales (2021) Uncertainty Estimation in Autoregressive Structured Prediction [Prior work on uncertainty in structured prediction, but uses token-level rather than semantic clustering]",
            "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [Related self-consistency approach but uses majority voting rather than entropy over semantic clusters]",
            "Lin et al. (2022) Teaching models to express their uncertainty in words [Related work on verbalized uncertainty but doesn't use semantic clustering]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 4,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
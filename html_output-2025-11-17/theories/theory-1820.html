<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probabilistic Aggregation and Calibration Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1820</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1820</p>
                <p><strong>Name:</strong> Probabilistic Aggregation and Calibration Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> LLMs can accurately measure the probability of future scientific discoveries by aggregating probabilistic cues from diverse textual sources and calibrating their outputs through internal consistency checks and exposure to explicit forecasting data. This process allows LLMs to synthesize distributed signals and produce well-calibrated likelihood estimates.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Probabilistic Cue Aggregation (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_exposed_to &#8594; diverse_textual_sources_with_probabilistic_language</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; aggregates &#8594; probabilistic_cues_about_scientific_discoveries</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can synthesize information from multiple sources and generate probability estimates for future events. </li>
    <li>LLMs have demonstrated the ability to aggregate weak signals from disparate texts to form coherent predictions. </li>
    <li>LLMs can summarize consensus and disagreement in scientific literature, indicating aggregation of distributed cues. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Information aggregation is established, but its use for probabilistic forecasting of discoveries is not.</p>            <p><strong>What Already Exists:</strong> LLMs can aggregate information from multiple sources.</p>            <p><strong>What is Novel:</strong> The explicit aggregation of probabilistic cues for scientific discovery forecasting is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Petroni et al. (2019) Language Models as Knowledge Bases? [LLMs aggregate factual knowledge]</li>
    <li>Lewandowski et al. (2023) Language Models as Scientific Synthesizers [LLMs synthesize scientific literature]</li>
</ul>
            <h3>Statement 1: Calibration via Internal Consistency and Forecasting Data (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_exposed_to &#8594; explicit_forecasting_data_or_feedback<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; performs &#8594; internal_consistency_checks</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; produces &#8594; well_calibrated_probability_estimates_for_future_discoveries</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can be fine-tuned or prompted to improve calibration of probability estimates. </li>
    <li>Studies show LLMs can self-consistently check and revise their outputs for logical coherence. </li>
    <li>Exposure to explicit forecasting data (e.g., Metaculus, prediction markets) improves LLM calibration. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Calibration and self-consistency are established, but their application to scientific discovery forecasting is not.</p>            <p><strong>What Already Exists:</strong> LLMs can be calibrated via fine-tuning and self-consistency prompting.</p>            <p><strong>What is Novel:</strong> The use of these mechanisms for scientific discovery probability estimation is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration]</li>
    <li>Lin et al. (2022) Teaching Language Models to Self-Verify [Self-consistency in LLMs]</li>
    <li>Lewandowski et al. (2023) Language Models as Scientific Synthesizers [LLMs as scientific aggregators]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs exposed to explicit forecasting data will produce more accurate and better-calibrated probability estimates for scientific discoveries.</li>
                <li>LLMs will outperform uncalibrated baselines in aggregating distributed probabilistic cues from scientific literature.</li>
                <li>LLMs will be able to identify consensus and uncertainty in scientific communities regarding future discoveries.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs will be able to self-calibrate in domains with little or no explicit forecasting data.</li>
                <li>LLMs will identify and correct for systematic biases in aggregated probabilistic cues.</li>
                <li>LLMs will outperform human expert aggregators in certain forecasting tasks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs are denied access to explicit forecasting data, their calibration will degrade.</li>
                <li>If LLMs are prompted with internally inconsistent information, their probability estimates will become less reliable.</li>
                <li>If LLMs are tested on fields with highly conflicting or ambiguous cues, their aggregation will not outperform random baselines.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs may not account for non-textual signals (e.g., unpublished data, tacit knowledge) that influence real-world discoveries. </li>
    <li>LLMs may struggle to aggregate cues in fields with highly fragmented or siloed literature. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> Aggregation and calibration are established, but their application to scientific discovery forecasting is not formalized in prior work.</p>
            <p><strong>References:</strong> <ul>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration]</li>
    <li>Lewandowski et al. (2023) Language Models as Scientific Synthesizers [LLMs as scientific aggregators]</li>
    <li>Lin et al. (2022) Teaching Language Models to Self-Verify [Self-consistency in LLMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Probabilistic Aggregation and Calibration Theory",
    "theory_description": "LLMs can accurately measure the probability of future scientific discoveries by aggregating probabilistic cues from diverse textual sources and calibrating their outputs through internal consistency checks and exposure to explicit forecasting data. This process allows LLMs to synthesize distributed signals and produce well-calibrated likelihood estimates.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Probabilistic Cue Aggregation",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_exposed_to",
                        "object": "diverse_textual_sources_with_probabilistic_language"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "aggregates",
                        "object": "probabilistic_cues_about_scientific_discoveries"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can synthesize information from multiple sources and generate probability estimates for future events.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs have demonstrated the ability to aggregate weak signals from disparate texts to form coherent predictions.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can summarize consensus and disagreement in scientific literature, indicating aggregation of distributed cues.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs can aggregate information from multiple sources.",
                    "what_is_novel": "The explicit aggregation of probabilistic cues for scientific discovery forecasting is novel.",
                    "classification_explanation": "Information aggregation is established, but its use for probabilistic forecasting of discoveries is not.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Petroni et al. (2019) Language Models as Knowledge Bases? [LLMs aggregate factual knowledge]",
                        "Lewandowski et al. (2023) Language Models as Scientific Synthesizers [LLMs synthesize scientific literature]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Calibration via Internal Consistency and Forecasting Data",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_exposed_to",
                        "object": "explicit_forecasting_data_or_feedback"
                    },
                    {
                        "subject": "LLM",
                        "relation": "performs",
                        "object": "internal_consistency_checks"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "produces",
                        "object": "well_calibrated_probability_estimates_for_future_discoveries"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can be fine-tuned or prompted to improve calibration of probability estimates.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show LLMs can self-consistently check and revise their outputs for logical coherence.",
                        "uuids": []
                    },
                    {
                        "text": "Exposure to explicit forecasting data (e.g., Metaculus, prediction markets) improves LLM calibration.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs can be calibrated via fine-tuning and self-consistency prompting.",
                    "what_is_novel": "The use of these mechanisms for scientific discovery probability estimation is novel.",
                    "classification_explanation": "Calibration and self-consistency are established, but their application to scientific discovery forecasting is not.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration]",
                        "Lin et al. (2022) Teaching Language Models to Self-Verify [Self-consistency in LLMs]",
                        "Lewandowski et al. (2023) Language Models as Scientific Synthesizers [LLMs as scientific aggregators]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs exposed to explicit forecasting data will produce more accurate and better-calibrated probability estimates for scientific discoveries.",
        "LLMs will outperform uncalibrated baselines in aggregating distributed probabilistic cues from scientific literature.",
        "LLMs will be able to identify consensus and uncertainty in scientific communities regarding future discoveries."
    ],
    "new_predictions_unknown": [
        "LLMs will be able to self-calibrate in domains with little or no explicit forecasting data.",
        "LLMs will identify and correct for systematic biases in aggregated probabilistic cues.",
        "LLMs will outperform human expert aggregators in certain forecasting tasks."
    ],
    "negative_experiments": [
        "If LLMs are denied access to explicit forecasting data, their calibration will degrade.",
        "If LLMs are prompted with internally inconsistent information, their probability estimates will become less reliable.",
        "If LLMs are tested on fields with highly conflicting or ambiguous cues, their aggregation will not outperform random baselines."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs may not account for non-textual signals (e.g., unpublished data, tacit knowledge) that influence real-world discoveries.",
            "uuids": []
        },
        {
            "text": "LLMs may struggle to aggregate cues in fields with highly fragmented or siloed literature.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "LLMs sometimes produce overconfident or underconfident probability estimates, especially in domains with sparse or biased data.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Fields with little explicit probabilistic language may be less amenable to LLM aggregation.",
        "LLMs may be less effective in aggregating cues when scientific consensus is rapidly shifting."
    ],
    "existing_theory": {
        "what_already_exists": "LLMs can aggregate information and be calibrated via fine-tuning or prompting.",
        "what_is_novel": "The explicit use of these mechanisms for scientific discovery probability estimation is novel.",
        "classification_explanation": "Aggregation and calibration are established, but their application to scientific discovery forecasting is not formalized in prior work.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration]",
            "Lewandowski et al. (2023) Language Models as Scientific Synthesizers [LLMs as scientific aggregators]",
            "Lin et al. (2022) Teaching Language Models to Self-Verify [Self-consistency in LLMs]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-648",
    "original_theory_name": "Domain Specialization and Fine-Tuning Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
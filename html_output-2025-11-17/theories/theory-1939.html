<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Format-Distribution Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1939</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1939</p>
                <p><strong>Name:</strong> Prompt Format-Distribution Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the validity and quality of LLM outputs are maximized when the prompt format closely aligns with the distribution of prompt types and structures seen during pretraining and fine-tuning. As prompt format diverges from this distribution—through increased novelty, ambiguity, or structural deviation—output validity and reliability degrade, with degeneration effects increasing nonlinearly with distributional distance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Distributional Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; has_distributional_distance &#8594; D<span style="color: #888888;">, and</span></div>
        <div>&#8226; D &#8594; is_small &#8594; relative to training distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_output_validity &#8594; is_maximized &#8594; high</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best on prompt formats similar to those seen during pretraining and fine-tuning. </li>
    <li>Prompt engineering that mimics training data structure improves output validity. </li>
    <li>Empirical studies show that out-of-distribution prompts lead to higher rates of hallucination and degeneration. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends known distributional shift concepts to prompt format and LLM degeneration.</p>            <p><strong>What Already Exists:</strong> Distributional shift and out-of-distribution generalization are known issues in machine learning.</p>            <p><strong>What is Novel:</strong> The explicit connection between prompt format distributional distance and LLM output degeneration is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Radford et al. (2019) Language Models are Unsupervised Multitask Learners [Prompt format and pretraining alignment]</li>
    <li>Koh et al. (2021) Wilds: A Benchmark of in-the-Wild Distribution Shifts [Distributional shift, not prompt-specific]</li>
</ul>
            <h3>Statement 1: Nonlinear Degeneration with Distributional Distance (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; has_distributional_distance &#8594; D<span style="color: #888888;">, and</span></div>
        <div>&#8226; D &#8594; is_large &#8594; relative to training distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_output_validity &#8594; collapses &#8594; nonlinearly (sharp drop)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs show abrupt performance drops on prompts that are structurally or semantically novel compared to training data. </li>
    <li>Prompt formats with high novelty or ambiguity induce degeneration and invalid outputs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law formalizes a nonlinear relationship between prompt format distributional distance and degeneration.</p>            <p><strong>What Already Exists:</strong> Distributional shift is known to cause performance drops, but the nonlinear collapse in LLMs due to prompt format is not formalized.</p>            <p><strong>What is Novel:</strong> The nonlinear, collapse-like degeneration tied to prompt format distributional distance is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Koh et al. (2021) Wilds: A Benchmark of in-the-Wild Distribution Shifts [Distributional shift]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Prompt formats that closely mimic training data will yield higher output validity than novel or ambiguous formats.</li>
                <li>LLMs will show sharp drops in output validity when presented with prompts that are structurally or semantically distant from training data.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Fine-tuning on a broader distribution of prompt formats may increase the threshold for degeneration.</li>
                <li>There may exist prompt formats that are out-of-distribution but still yield valid outputs due to latent generalization.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on in-distribution and out-of-distribution prompt formats, the theory would be falsified.</li>
                <li>If output validity degrades linearly rather than nonlinearly with distributional distance, the nonlinear law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs generalize well to novel prompt formats despite high distributional distance. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory extends distributional shift concepts to prompt format and LLM degeneration, introducing a new nonlinear collapse mechanism.</p>
            <p><strong>References:</strong> <ul>
    <li>Radford et al. (2019) Language Models are Unsupervised Multitask Learners [Prompt format and pretraining alignment]</li>
    <li>Koh et al. (2021) Wilds: A Benchmark of in-the-Wild Distribution Shifts [Distributional shift, not prompt-specific]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt Format-Distribution Alignment Theory",
    "theory_description": "This theory posits that the validity and quality of LLM outputs are maximized when the prompt format closely aligns with the distribution of prompt types and structures seen during pretraining and fine-tuning. As prompt format diverges from this distribution—through increased novelty, ambiguity, or structural deviation—output validity and reliability degrade, with degeneration effects increasing nonlinearly with distributional distance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Distributional Alignment Law",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "has_distributional_distance",
                        "object": "D"
                    },
                    {
                        "subject": "D",
                        "relation": "is_small",
                        "object": "relative to training distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_output_validity",
                        "relation": "is_maximized",
                        "object": "high"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best on prompt formats similar to those seen during pretraining and fine-tuning.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering that mimics training data structure improves output validity.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that out-of-distribution prompts lead to higher rates of hallucination and degeneration.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Distributional shift and out-of-distribution generalization are known issues in machine learning.",
                    "what_is_novel": "The explicit connection between prompt format distributional distance and LLM output degeneration is new.",
                    "classification_explanation": "The law extends known distributional shift concepts to prompt format and LLM degeneration.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Radford et al. (2019) Language Models are Unsupervised Multitask Learners [Prompt format and pretraining alignment]",
                        "Koh et al. (2021) Wilds: A Benchmark of in-the-Wild Distribution Shifts [Distributional shift, not prompt-specific]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Nonlinear Degeneration with Distributional Distance",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "has_distributional_distance",
                        "object": "D"
                    },
                    {
                        "subject": "D",
                        "relation": "is_large",
                        "object": "relative to training distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_output_validity",
                        "relation": "collapses",
                        "object": "nonlinearly (sharp drop)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs show abrupt performance drops on prompts that are structurally or semantically novel compared to training data.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt formats with high novelty or ambiguity induce degeneration and invalid outputs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Distributional shift is known to cause performance drops, but the nonlinear collapse in LLMs due to prompt format is not formalized.",
                    "what_is_novel": "The nonlinear, collapse-like degeneration tied to prompt format distributional distance is new.",
                    "classification_explanation": "The law formalizes a nonlinear relationship between prompt format distributional distance and degeneration.",
                    "likely_classification": "new",
                    "references": [
                        "Koh et al. (2021) Wilds: A Benchmark of in-the-Wild Distribution Shifts [Distributional shift]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Prompt formats that closely mimic training data will yield higher output validity than novel or ambiguous formats.",
        "LLMs will show sharp drops in output validity when presented with prompts that are structurally or semantically distant from training data."
    ],
    "new_predictions_unknown": [
        "Fine-tuning on a broader distribution of prompt formats may increase the threshold for degeneration.",
        "There may exist prompt formats that are out-of-distribution but still yield valid outputs due to latent generalization."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on in-distribution and out-of-distribution prompt formats, the theory would be falsified.",
        "If output validity degrades linearly rather than nonlinearly with distributional distance, the nonlinear law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs generalize well to novel prompt formats despite high distributional distance.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs with advanced generalization or meta-learning capabilities can handle novel prompt formats without degeneration.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with explicit meta-learning or prompt adaptation may not exhibit sharp degeneration.",
        "Prompt formats that are ambiguous but share latent structure with training data may not induce collapse."
    ],
    "existing_theory": {
        "what_already_exists": "Distributional shift and out-of-distribution generalization are known in ML, but not formalized for prompt format-induced degeneration in LLMs.",
        "what_is_novel": "The explicit mapping from prompt format distributional distance to nonlinear degeneration is new.",
        "classification_explanation": "The theory extends distributional shift concepts to prompt format and LLM degeneration, introducing a new nonlinear collapse mechanism.",
        "likely_classification": "new",
        "references": [
            "Radford et al. (2019) Language Models are Unsupervised Multitask Learners [Prompt format and pretraining alignment]",
            "Koh et al. (2021) Wilds: A Benchmark of in-the-Wild Distribution Shifts [Distributional shift, not prompt-specific]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-655",
    "original_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
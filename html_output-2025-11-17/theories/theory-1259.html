<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositional Alignment Theory for Graph-to-Text - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1259</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1259</p>
                <p><strong>Name:</strong> Compositional Alignment Theory for Graph-to-Text</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that the ideal graph-to-text representation for language model training is one that aligns the compositional structure of the graph with the compositional structure of natural language, enabling the model to leverage its inherent linguistic biases for more effective learning and generalization. Such representations should map graph substructures to corresponding text spans in a way that preserves semantic and syntactic relationships.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Compositional Mapping Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; maps_subgraphs &#8594; textual_spans<span style="color: #888888;">, and</span></div>
        <div>&#8226; mapping &#8594; preserves &#8594; semantic_and_syntactic_relations</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; can_learn &#8594; compositional_graph_to_text_mappings<span style="color: #888888;">, and</span></div>
        <div>&#8226; language_model &#8594; generalizes &#8594; to_novel_compositions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Compositional alignments between AMR subgraphs and text improve parsing and generation. </li>
    <li>Neural models benefit from representations that mirror natural language structure. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law formalizes a practice into a theoretical principle for LM training.</p>            <p><strong>What Already Exists:</strong> Compositional alignments are used in AMR parsing and neural semantic parsing.</p>            <p><strong>What is Novel:</strong> The explicit claim that compositional alignment is ideal for LM training is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Pourdamghani et al. (2016) Aligning English Strings with Abstract Meaning Representation Graphs [compositional alignments]</li>
    <li>Guo et al. (2019) Coupling Graph Structure and Semantics for Neural AMR Parsing [compositional mapping]</li>
</ul>
            <h3>Statement 1: Linguistic Bias Utilization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; mirrors &#8594; natural_language_compositionality</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; leverages &#8594; pretrained_linguistic_biases<span style="color: #888888;">, and</span></div>
        <div>&#8226; language_model &#8594; improves &#8594; learning_efficiency_and_generalization</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Pretrained LMs perform better when input structure aligns with natural language syntax and semantics. </li>
    <li>Graph-to-text models that use linguistically-aligned representations achieve higher BLEU and accuracy scores. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes empirical findings into a general theoretical principle.</p>            <p><strong>What Already Exists:</strong> Linguistic alignment is used in some neural semantic parsing and AMR-to-text models.</p>            <p><strong>What is Novel:</strong> The explicit theoretical link between compositional alignment and leveraging LM biases is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Guo et al. (2019) Coupling Graph Structure and Semantics for Neural AMR Parsing [linguistic alignment]</li>
    <li>Ribeiro et al. (2020) Structural Neural Encoders for AMR-to-Text Generation [linguistic structure]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Graph-to-text models using compositional, linguistically-aligned representations will outperform those using arbitrary or non-aligned serializations.</li>
                <li>Pretrained LMs will adapt more quickly to graph-to-text tasks when the input representation mirrors natural language structure.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Compositional alignment will enable LMs to generalize to highly novel or creative graph structures that have no direct analog in training data.</li>
                <li>Linguistically-aligned representations may allow LMs to perform zero-shot graph-to-text generation for unseen graph schemas.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If non-compositional or non-aligned representations outperform compositional, linguistically-aligned ones, the theory would be challenged.</li>
                <li>If pretrained LMs do not benefit from compositional alignment, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address how to handle graphs with structures that have no clear linguistic analog. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory formalizes and generalizes best practices into a theoretical framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Pourdamghani et al. (2016) Aligning English Strings with Abstract Meaning Representation Graphs [compositional alignments]</li>
    <li>Guo et al. (2019) Coupling Graph Structure and Semantics for Neural AMR Parsing [compositional mapping]</li>
    <li>Ribeiro et al. (2020) Structural Neural Encoders for AMR-to-Text Generation [linguistic structure]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositional Alignment Theory for Graph-to-Text",
    "theory_description": "This theory asserts that the ideal graph-to-text representation for language model training is one that aligns the compositional structure of the graph with the compositional structure of natural language, enabling the model to leverage its inherent linguistic biases for more effective learning and generalization. Such representations should map graph substructures to corresponding text spans in a way that preserves semantic and syntactic relationships.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Compositional Mapping Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "maps_subgraphs",
                        "object": "textual_spans"
                    },
                    {
                        "subject": "mapping",
                        "relation": "preserves",
                        "object": "semantic_and_syntactic_relations"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "can_learn",
                        "object": "compositional_graph_to_text_mappings"
                    },
                    {
                        "subject": "language_model",
                        "relation": "generalizes",
                        "object": "to_novel_compositions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Compositional alignments between AMR subgraphs and text improve parsing and generation.",
                        "uuids": []
                    },
                    {
                        "text": "Neural models benefit from representations that mirror natural language structure.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositional alignments are used in AMR parsing and neural semantic parsing.",
                    "what_is_novel": "The explicit claim that compositional alignment is ideal for LM training is novel.",
                    "classification_explanation": "The law formalizes a practice into a theoretical principle for LM training.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Pourdamghani et al. (2016) Aligning English Strings with Abstract Meaning Representation Graphs [compositional alignments]",
                        "Guo et al. (2019) Coupling Graph Structure and Semantics for Neural AMR Parsing [compositional mapping]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Linguistic Bias Utilization Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "mirrors",
                        "object": "natural_language_compositionality"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "leverages",
                        "object": "pretrained_linguistic_biases"
                    },
                    {
                        "subject": "language_model",
                        "relation": "improves",
                        "object": "learning_efficiency_and_generalization"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Pretrained LMs perform better when input structure aligns with natural language syntax and semantics.",
                        "uuids": []
                    },
                    {
                        "text": "Graph-to-text models that use linguistically-aligned representations achieve higher BLEU and accuracy scores.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Linguistic alignment is used in some neural semantic parsing and AMR-to-text models.",
                    "what_is_novel": "The explicit theoretical link between compositional alignment and leveraging LM biases is novel.",
                    "classification_explanation": "The law synthesizes empirical findings into a general theoretical principle.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Guo et al. (2019) Coupling Graph Structure and Semantics for Neural AMR Parsing [linguistic alignment]",
                        "Ribeiro et al. (2020) Structural Neural Encoders for AMR-to-Text Generation [linguistic structure]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Graph-to-text models using compositional, linguistically-aligned representations will outperform those using arbitrary or non-aligned serializations.",
        "Pretrained LMs will adapt more quickly to graph-to-text tasks when the input representation mirrors natural language structure."
    ],
    "new_predictions_unknown": [
        "Compositional alignment will enable LMs to generalize to highly novel or creative graph structures that have no direct analog in training data.",
        "Linguistically-aligned representations may allow LMs to perform zero-shot graph-to-text generation for unseen graph schemas."
    ],
    "negative_experiments": [
        "If non-compositional or non-aligned representations outperform compositional, linguistically-aligned ones, the theory would be challenged.",
        "If pretrained LMs do not benefit from compositional alignment, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address how to handle graphs with structures that have no clear linguistic analog.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks may benefit from representations that abstract away from natural language structure, such as for logical reasoning or symbolic manipulation.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with non-compositional or cyclic structures may not map cleanly to natural language.",
        "Highly abstract or mathematical graphs may require alternative alignment strategies."
    ],
    "existing_theory": {
        "what_already_exists": "Compositional and linguistic alignment is used in AMR parsing and neural semantic parsing.",
        "what_is_novel": "The formalization of compositional alignment as an ideal for LM training is novel.",
        "classification_explanation": "The theory formalizes and generalizes best practices into a theoretical framework.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Pourdamghani et al. (2016) Aligning English Strings with Abstract Meaning Representation Graphs [compositional alignments]",
            "Guo et al. (2019) Coupling Graph Structure and Semantics for Neural AMR Parsing [compositional mapping]",
            "Ribeiro et al. (2020) Structural Neural Encoders for AMR-to-Text Generation [linguistic structure]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-612",
    "original_theory_name": "Structural Inductive Bias and Modality Adaptation Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
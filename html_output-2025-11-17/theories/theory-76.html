<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Stochasticity Theory of LM Variability - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-76</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-76</p>
                <p><strong>Name:</strong> Hierarchical Stochasticity Theory of LM Variability</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about variability and reproducibility in language model-driven scientific experimentation, based on the following results.</p>
                <p><strong>Description:</strong> Variability in language model-driven experiments arises from multiple nested stochastic layers that interact multiplicatively: (1) environmental stochasticity (hardware non-determinism, system variations, data center conditions), (2) training stochasticity (initialization, data ordering, optimization dynamics, checkpoint selection), (3) architectural stochasticity (model size, alignment state, internal representations), and (4) inference stochasticity (sampling parameters, decoding methods), and (5) task-specification stochasticity (prompt formulation, example selection/ordering, evaluation protocols). Each layer can amplify or modulate variability from other layers through non-linear dynamics. Critically, the dominant source varies by experimental context, and different model properties (size, alignment, architecture) act as transfer functions that modulate sensitivity across all layers. This hierarchical organization explains why universal mitigation strategies are ineffective and why variance reduction requires targeting the dominant source for each specific context.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Variability sources are organized in nested layers: environmental → training → architectural → inference → task-specification, where each layer can influence and be influenced by others.</li>
                <li>Each layer can amplify stochasticity from other layers through non-linear dynamics (optimization landscapes, attention mechanisms, autoregressive generation, threshold effects).</li>
                <li>The dominant source of observed variability depends on the experimental context: training experiments expose optimization stochasticity, inference experiments expose sampling stochasticity, evaluation experiments expose prompt/task stochasticity.</li>
                <li>Model properties (size, alignment, architecture) act as transfer functions that modulate sensitivity at all layers: larger and more aligned models show reduced sensitivity to lower-layer stochasticity.</li>
                <li>Interactions between layers are often multiplicative or threshold-based rather than additive, leading to occasional catastrophic failures (e.g., vanishing gradients, degenerate outputs) and non-linear variance propagation.</li>
                <li>Mitigation strategies must target the dominant source for the specific context; universal mitigations are ineffective because they cannot address all layers simultaneously and interactions between layers can negate single-layer interventions.</li>
                <li>Variance reduction at one layer can reduce variance at higher layers (e.g., alignment reducing inference sensitivity), but not always (e.g., calibration improving mean but not reducing variance).</li>
                <li>Different sources at the same layer often produce similar magnitudes of variability, suggesting they act through common amplification mechanisms within that layer.</li>
                <li>Task properties (open-endedness, constraint level, sequence length) modulate which layers dominate observed variability.</li>
                <li>Measurement choices (discrete vs continuous metrics, aggregation methods) can amplify or suppress underlying variability from lower layers.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Multiple independent sources of nondeterminism (initialization, shuffling, augmentation, cuDNN) produce similar magnitudes of variability (~0.26% accuracy SD for ResNet-14), suggesting they act through a common amplification mechanism rather than being independent additive sources. <a href="../results/extraction-result-496.html#e496.0" class="evidence-link">[e496.0]</a> </li>
    <li>Small perturbations (one-bit weight changes) produce variability comparable to all other sources combined, demonstrating extreme sensitivity to initial conditions and non-linear amplification. <a href="../results/extraction-result-496.html#e496.0" class="evidence-link">[e496.0]</a> </li>
    <li>Temperature and sampling stochasticity interact with prompt sensitivity: even at low temperature (T=0.2), inconsistency remains above semantic-similarity expectations, indicating multiple interacting sources across layers. <a href="../results/extraction-result-488.html#e488.1" class="evidence-link">[e488.1]</a> </li>
    <li>Vanishing gradients in fine-tuning show that early-step optimization dynamics (training layer) can determine whether training succeeds or fails entirely, demonstrating catastrophic amplification of low-level stochasticity through optimization. <a href="../results/extraction-result-636.html#e636.1" class="evidence-link">[e636.1]</a> </li>
    <li>Prompt order sensitivity interacts with model size and calibration: larger models show reduced variance (e.g., correlation 0.05 between 175B and 2.7B model permutation rankings), showing architectural properties modulate task-specification stochasticity. <a href="../results/extraction-result-638.html#e638.0" class="evidence-link">[e638.0]</a> <a href="../results/extraction-result-638.html#e638.4" class="evidence-link">[e638.4]</a> <a href="../results/extraction-result-638.html#e638.5" class="evidence-link">[e638.5]</a> </li>
    <li>Hardware-level non-determinism (GPU operations, network stalls) affects training throughput (1-2% diurnal variation) and can cascade to affect final model behavior. <a href="../results/extraction-result-652.html#e652.1" class="evidence-link">[e652.1]</a> <a href="../results/extraction-result-652.html#e652.2" class="evidence-link">[e652.2]</a> </li>
    <li>Hyperparameter sensitivity varies by model alignment state: aligned models show lower RDP (e.g., MBPP: 25.81% → 9.08% for 7B → 7B-Chat) and entropy (GSM8K: 1.05 → 0.27), indicating training-level choices affect sensitivity to decoding-level choices. <a href="../results/extraction-result-623.html#e623.0" class="evidence-link">[e623.0]</a> <a href="../results/extraction-result-623.html#e623.3" class="evidence-link">[e623.3]</a> </li>
    <li>Instruction fine-tuning changes sensitivity patterns across multiple metrics (negation, context, word order), with average std across prompts changing from 11.9% (LLaMA pretrained) to 1.9% (ChatGPT), showing training-level choices cascade to inference-level behavior. <a href="../results/extraction-result-610.html#e610.4" class="evidence-link">[e610.4]</a> <a href="../results/extraction-result-637.html#e637.1" class="evidence-link">[e637.1]</a> </li>
    <li>Environmental factors (data center temperature) cause diurnal throughput variation (1-2%) that affects training dynamics, demonstrating bottom-layer effects. <a href="../results/extraction-result-652.html#e652.1" class="evidence-link">[e652.1]</a> </li>
    <li>Model scaling reduces variability across multiple layers: RDP drops from ~25.8% (7B) to ~15.2% (70B) for MBPP, showing architectural properties modulate sensitivity. <a href="../results/extraction-result-623.html#e623.0" class="evidence-link">[e623.0]</a> </li>
    <li>Decoding method choice induces substantial performance variability (often >20% RDP for 7B models), but this is reduced by alignment and scaling, showing cross-layer interactions. <a href="../results/extraction-result-623.html#e623.0" class="evidence-link">[e623.0]</a> </li>
    <li>Seed variance (std across 10 seeds) varies by benchmark (e.g., MMLU 0.57, COPA 2.15) and is generally smaller than per-model 95% CI but still non-negligible, showing training-layer stochasticity. <a href="../results/extraction-result-639.html#e639.0" class="evidence-link">[e639.0]</a> </li>
    <li>Continuous metrics show much higher signal-to-noise ratio than discrete metrics (e.g., MMLU SNR: 52.45 discrete vs 347.57 continuous), indicating measurement layer can amplify or suppress underlying variability. <a href="../results/extraction-result-639.html#e639.1" class="evidence-link">[e639.1]</a> </li>
    <li>Prompt sampling variability shows large effects: different sampled prompts produce widely different accuracies (e.g., SST-2: 77.8% ± 11.2 std), demonstrating task-specification layer dominance in few-shot settings. <a href="../results/extraction-result-634.html#e634.0" class="evidence-link">[e634.0]</a> </li>
    <li>Non-determinism in LLM generation varies by task open-endedness: constrained tasks (MMLU) show high stability while reasoning/code tasks (GSM8K, HumanEval) show large variability (>10 point gaps), showing task properties modulate inference stochasticity. <a href="../results/extraction-result-482.html#e482.0" class="evidence-link">[e482.0]</a> </li>
    <li>Greedy vs sampling decoding shows task-dependent effects: greedy usually outperforms average sampling on deterministic tasks but not on open-ended tasks (AlpacaEval), showing inference-layer and task-layer interactions. <a href="../results/extraction-result-482.html#e482.1" class="evidence-link">[e482.1]</a> </li>
    <li>Temperature effects vary by task: higher temperature harms reasoning/code (GSM8K, HumanEval) but can help open-ended tasks (AlpacaEval), showing task-specification modulates inference stochasticity. <a href="../results/extraction-result-482.html#e482.3" class="evidence-link">[e482.3]</a> </li>
    <li>Hyperparameter tuning and random seeds in RL show similar magnitudes of variability, with 22 runs needed to characterize reward-vs-KL frontier, demonstrating training-layer stochasticity. <a href="../results/extraction-result-649.html#e649.1" class="evidence-link">[e649.1]</a> </li>
    <li>Random initialization seed produces measurable variance across benchmarks (std up to a few percentage points), and this seed variance is typically smaller than but comparable to per-model 95% CIs. <a href="../results/extraction-result-639.html#e639.0" class="evidence-link">[e639.0]</a> </li>
    <li>Accounting for variance increases required labeled samples by 100-200% on average (up to 500% in some cases), showing how training-layer variability propagates to sample efficiency. <a href="../results/extraction-result-486.html#e486.0" class="evidence-link">[e486.0]</a> </li>
    <li>Snapshot ensembling using cyclic learning rates produces diverse models from single training run, with lower pairwise correlations than non-cyclic snapshots, showing training dynamics can generate within-layer diversity. <a href="../results/extraction-result-604.html#e604.1" class="evidence-link">[e604.1]</a> <a href="../results/extraction-result-604.html#e604.2" class="evidence-link">[e604.2]</a> </li>
    <li>ASWA/NASWA weight averaging reduces run-to-run variability by ~72% on average (89% for some datasets), showing training-layer interventions can reduce variance. <a href="../results/extraction-result-612.html#e612.1" class="evidence-link">[e612.1]</a> </li>
    <li>Accelerated ensembling reduces variability by ~48% (accuracy SD from 0.26% to 0.19%), showing training-layer mitigation effectiveness. <a href="../results/extraction-result-496.html#e496.4" class="evidence-link">[e496.4]</a> </li>
    <li>Data contamination (train-test overlap) can inflate results on some benchmarks, showing how training-layer factors affect evaluation-layer measurements. <a href="../results/extraction-result-642.html#e642.3" class="evidence-link">[e642.3]</a> </li>
    <li>Formatting sensitivity shows large effects: FORMATSPREAD finds spreads of 20-76 accuracy points across formatting variations, demonstrating task-specification layer dominance. <a href="../results/extraction-result-665.html#e665.0" class="evidence-link">[e665.0]</a> </li>
    <li>Prompt calibration improves mean accuracy (up to 30 points) but doesn't eliminate high variance across permutations, showing some mitigations affect bias but not variance. <a href="../results/extraction-result-644.html#e644.0" class="evidence-link">[e644.0]</a> <a href="../results/extraction-result-638.html#e638.4" class="evidence-link">[e638.4]</a> </li>
    <li>Self-consistency with 40 sampled paths produces larger gains than prompt-ensemble approaches, showing inference-layer aggregation can overcome task-specification variability. <a href="../results/extraction-result-646.html#e646.2" class="evidence-link">[e646.2]</a> </li>
    <li>Pass@N metrics show correct answers often present among many stochastic generations (high Pass@256) but single-sample reliability is poor, demonstrating inference-layer stochasticity. <a href="../results/extraction-result-662.html#e662.0" class="evidence-link">[e662.0]</a> </li>
    <li>SFT scaling improves single-sample reliability (PassRatio) while Pass@256 saturates, showing training-layer interventions can reduce inference-layer variability. <a href="../results/extraction-result-662.html#e662.0" class="evidence-link">[e662.0]</a> </li>
    <li>Semantic entropy clustering reduces variability from lexical paraphrases and predicts correctness better than token-sequence baselines (AUROC 0.790 vs 0.698), showing measurement-layer choices affect observed variability. <a href="../results/extraction-result-605.html#e605.0" class="evidence-link">[e605.0]</a> <a href="../results/extraction-result-663.html#e663.1" class="evidence-link">[e663.1]</a> </li>
    <li>Model entropy (next-token distribution sharpness) varies across models and affects sensitivity metrics, showing architectural properties modulate inference behavior. <a href="../results/extraction-result-610.html#e610.2" class="evidence-link">[e610.2]</a> </li>
    <li>Alignment (instruction tuning) reduces entropy and RDP, showing training-layer interventions affect inference-layer and task-layer sensitivity. <a href="../results/extraction-result-623.html#e623.3" class="evidence-link">[e623.3]</a> </li>
    <li>Hardware and environment heterogeneity (CPU instruction sets, GPU memory) causes concrete reproducibility failures, demonstrating environmental-layer effects. <a href="../results/extraction-result-643.html#e643.3" class="evidence-link">[e643.3]</a> </li>
    <li>Proprietary model updates and API changes cause results to change over time, showing environmental-layer (external dependencies) effects on reproducibility. <a href="../results/extraction-result-453.html#e453.2" class="evidence-link">[e453.2]</a> </li>
    <li>Network stalls and stragglers are significant sources of variability at scale, with network-related interruptions accounting for 8.4% of total interruptions, showing environmental-layer effects. <a href="../results/extraction-result-652.html#e652.2" class="evidence-link">[e652.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Combining multiple mitigation strategies targeting different layers (e.g., ASWA training + deterministic decoding + prompt standardization + environment containerization) will show super-additive variance reduction compared to individual mitigations, with total variance reduction exceeding the sum of individual reductions.</li>
                <li>Tasks requiring longer generation sequences will show exponentially or polynomially increasing variance with sequence length due to autoregressive error accumulation across the hierarchy, with the rate depending on model alignment and size.</li>
                <li>Models trained with variance-reduction techniques (e.g., ASWA/NASWA) will show reduced sensitivity to inference-time stochasticity (temperature, sampling) compared to standard-trained models, with the reduction proportional to the training-layer variance reduction achieved.</li>
                <li>Measuring variance at intermediate layers (hidden states, attention patterns, gradient norms) will reveal which hierarchical level dominates for a given task, enabling targeted mitigation that is more efficient than universal approaches.</li>
                <li>For a given task, there will be a predictable ordering of which layer dominates variance: highly constrained tasks will show training/architectural dominance, moderately constrained tasks will show inference dominance, and open-ended tasks will show task-specification dominance.</li>
                <li>Cross-layer interventions (e.g., training with diverse prompts to reduce prompt sensitivity) will be more effective than single-layer interventions for reducing overall variance.</li>
                <li>Models with lower training-layer variance (measured by seed variance) will show lower inference-layer variance (measured by sampling variance) for the same task, demonstrating cross-layer correlation.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a fundamental lower bound on achievable variance that cannot be reduced below a certain threshold regardless of mitigation strategies, due to inherent chaotic dynamics in neural network optimization and generation, or whether variance can be reduced arbitrarily with sufficient intervention.</li>
                <li>Whether variance at different hierarchical levels is correlated across different model architectures (e.g., whether models with high training variance also show high inference variance), which would suggest architectural factors that propagate instability universally vs. architecture-specific propagation patterns.</li>
                <li>Whether interventions at higher layers (e.g., prompt engineering) can fully compensate for instability at lower layers (e.g., poor initialization), or whether lower-layer instability creates irreducible upper bounds on reproducibility that cannot be overcome by higher-layer interventions.</li>
                <li>Whether the hierarchical structure is universal across all neural architectures or specific to transformers, and how emerging architectures (e.g., state-space models, mixture-of-experts, diffusion models) fit into this framework.</li>
                <li>Whether there are critical thresholds or phase transitions in model properties (size, alignment level) where the dominant source of variance shifts discontinuously between layers.</li>
                <li>Whether variance propagation follows predictable mathematical laws (e.g., power laws, exponential relationships) that could enable quantitative prediction of total variance from layer-specific measurements.</li>
                <li>Whether human-in-the-loop interventions (e.g., human feedback, human prompt engineering) can break the hierarchical propagation of variance or whether they simply add another layer to the hierarchy.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding cases where fixing all known sources at one layer completely eliminates variance at higher layers would suggest layers are independent rather than hierarchically coupled.</li>
                <li>Demonstrating that variance reduction at a lower layer (e.g., deterministic training via fixed seeds) does not reduce variance at higher layers (e.g., inference sampling variance) would challenge the amplification mechanism.</li>
                <li>Showing that different sources at the same layer have dramatically different effects (orders of magnitude) would challenge the claim that they act through a common mechanism within layers.</li>
                <li>Finding tasks where variance decreases with longer generation sequences would contradict the error accumulation prediction.</li>
                <li>Demonstrating that model properties (size, alignment) do not modulate sensitivity consistently across different types of stochasticity would challenge the transfer function concept.</li>
                <li>Finding cases where combining mitigations from different layers produces sub-additive variance reduction (total reduction less than sum of individual reductions) would challenge the multiplicative interaction model.</li>
                <li>Showing that the dominant source of variance does not vary predictably with task properties would challenge the context-dependence claim.</li>
                <li>Finding architectures where variance at one layer is completely independent of variance at other layers would challenge the hierarchical coupling model.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact mathematical form of how variance propagates and amplifies through layers is not specified (e.g., whether it's multiplicative, exponential, or follows other functional forms). </li>
    <li>Why some models (e.g., Claude 3.5 Sonnet) show lower variance than others is not fully explained by the hierarchical structure alone - there may be model-specific factors beyond size and alignment. <a href="../results/extraction-result-640.html#e640.2" class="evidence-link">[e640.2]</a> <a href="../results/extraction-result-488.html#e488.0" class="evidence-link">[e488.0]</a> </li>
    <li>The role of emergent capabilities and phase transitions in model behavior as sources of discontinuous variance changes is not addressed. </li>
    <li>How memorization and data contamination fit into the hierarchical structure - they seem to bypass normal layer interactions. <a href="../results/extraction-result-642.html#e642.3" class="evidence-link">[e642.3]</a> </li>
    <li>Why some mitigation strategies (e.g., calibration) improve mean performance but don't reduce variance - the theory doesn't fully explain bias-variance decoupling. <a href="../results/extraction-result-644.html#e644.3" class="evidence-link">[e644.3]</a> <a href="../results/extraction-result-638.html#e638.4" class="evidence-link">[e638.4]</a> </li>
    <li>The role of human judgment variability in evaluation and how it interacts with model variability is not fully integrated into the hierarchy. <a href="../results/extraction-result-651.html#e651.0" class="evidence-link">[e651.0]</a> </li>
    <li>How dataset quality and annotation errors propagate through the hierarchy and interact with model-induced variability. <a href="../results/extraction-result-635.html#e635.0" class="evidence-link">[e635.0]</a> </li>
    <li>The specific mechanisms by which architectural properties (attention patterns, layer depth, etc.) modulate sensitivity are not detailed. </li>
    <li>Why certain tasks show much higher variance than others even when controlling for open-endedness and constraint level. <a href="../results/extraction-result-639.html#e639.0" class="evidence-link">[e639.0]</a> </li>
    <li>How adversarial examples and jailbreaks fit into the framework - they seem to exploit cross-layer interactions in ways not predicted by the basic hierarchy. <a href="../results/extraction-result-666.html#e666.5" class="evidence-link">[e666.5]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Summers & Dinneen (2019) Nondeterminism and Instability in Neural Network Optimization [Documents multiple sources of variability but doesn't propose hierarchical organization or cross-layer interactions]</li>
    <li>Dodge et al. (2020) Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping [Shows interactions between sources but no hierarchical framework or systematic layer organization]</li>
    <li>Lorenz (1963) Deterministic Nonperiodic Flow [Chaos theory foundation showing sensitivity to initial conditions, but not applied to ML systems hierarchically]</li>
    <li>Belz et al. (2021) A Systematic Review of Reproducibility Research in NLP [Surveys reproducibility issues but doesn't propose unified hierarchical theory]</li>
    <li>Nagarajan & Kolter (2019) Uniform convergence may be unable to explain generalization in deep learning [Discusses instability in neural networks but not hierarchical organization]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Stochasticity Theory of LM Variability",
    "theory_description": "Variability in language model-driven experiments arises from multiple nested stochastic layers that interact multiplicatively: (1) environmental stochasticity (hardware non-determinism, system variations, data center conditions), (2) training stochasticity (initialization, data ordering, optimization dynamics, checkpoint selection), (3) architectural stochasticity (model size, alignment state, internal representations), and (4) inference stochasticity (sampling parameters, decoding methods), and (5) task-specification stochasticity (prompt formulation, example selection/ordering, evaluation protocols). Each layer can amplify or modulate variability from other layers through non-linear dynamics. Critically, the dominant source varies by experimental context, and different model properties (size, alignment, architecture) act as transfer functions that modulate sensitivity across all layers. This hierarchical organization explains why universal mitigation strategies are ineffective and why variance reduction requires targeting the dominant source for each specific context.",
    "supporting_evidence": [
        {
            "text": "Multiple independent sources of nondeterminism (initialization, shuffling, augmentation, cuDNN) produce similar magnitudes of variability (~0.26% accuracy SD for ResNet-14), suggesting they act through a common amplification mechanism rather than being independent additive sources.",
            "uuids": [
                "e496.0"
            ]
        },
        {
            "text": "Small perturbations (one-bit weight changes) produce variability comparable to all other sources combined, demonstrating extreme sensitivity to initial conditions and non-linear amplification.",
            "uuids": [
                "e496.0"
            ]
        },
        {
            "text": "Temperature and sampling stochasticity interact with prompt sensitivity: even at low temperature (T=0.2), inconsistency remains above semantic-similarity expectations, indicating multiple interacting sources across layers.",
            "uuids": [
                "e488.1"
            ]
        },
        {
            "text": "Vanishing gradients in fine-tuning show that early-step optimization dynamics (training layer) can determine whether training succeeds or fails entirely, demonstrating catastrophic amplification of low-level stochasticity through optimization.",
            "uuids": [
                "e636.1"
            ]
        },
        {
            "text": "Prompt order sensitivity interacts with model size and calibration: larger models show reduced variance (e.g., correlation 0.05 between 175B and 2.7B model permutation rankings), showing architectural properties modulate task-specification stochasticity.",
            "uuids": [
                "e638.0",
                "e638.4",
                "e638.5"
            ]
        },
        {
            "text": "Hardware-level non-determinism (GPU operations, network stalls) affects training throughput (1-2% diurnal variation) and can cascade to affect final model behavior.",
            "uuids": [
                "e652.1",
                "e652.2"
            ]
        },
        {
            "text": "Hyperparameter sensitivity varies by model alignment state: aligned models show lower RDP (e.g., MBPP: 25.81% → 9.08% for 7B → 7B-Chat) and entropy (GSM8K: 1.05 → 0.27), indicating training-level choices affect sensitivity to decoding-level choices.",
            "uuids": [
                "e623.0",
                "e623.3"
            ]
        },
        {
            "text": "Instruction fine-tuning changes sensitivity patterns across multiple metrics (negation, context, word order), with average std across prompts changing from 11.9% (LLaMA pretrained) to 1.9% (ChatGPT), showing training-level choices cascade to inference-level behavior.",
            "uuids": [
                "e610.4",
                "e637.1"
            ]
        },
        {
            "text": "Environmental factors (data center temperature) cause diurnal throughput variation (1-2%) that affects training dynamics, demonstrating bottom-layer effects.",
            "uuids": [
                "e652.1"
            ]
        },
        {
            "text": "Model scaling reduces variability across multiple layers: RDP drops from ~25.8% (7B) to ~15.2% (70B) for MBPP, showing architectural properties modulate sensitivity.",
            "uuids": [
                "e623.0"
            ]
        },
        {
            "text": "Decoding method choice induces substantial performance variability (often &gt;20% RDP for 7B models), but this is reduced by alignment and scaling, showing cross-layer interactions.",
            "uuids": [
                "e623.0"
            ]
        },
        {
            "text": "Seed variance (std across 10 seeds) varies by benchmark (e.g., MMLU 0.57, COPA 2.15) and is generally smaller than per-model 95% CI but still non-negligible, showing training-layer stochasticity.",
            "uuids": [
                "e639.0"
            ]
        },
        {
            "text": "Continuous metrics show much higher signal-to-noise ratio than discrete metrics (e.g., MMLU SNR: 52.45 discrete vs 347.57 continuous), indicating measurement layer can amplify or suppress underlying variability.",
            "uuids": [
                "e639.1"
            ]
        },
        {
            "text": "Prompt sampling variability shows large effects: different sampled prompts produce widely different accuracies (e.g., SST-2: 77.8% ± 11.2 std), demonstrating task-specification layer dominance in few-shot settings.",
            "uuids": [
                "e634.0"
            ]
        },
        {
            "text": "Non-determinism in LLM generation varies by task open-endedness: constrained tasks (MMLU) show high stability while reasoning/code tasks (GSM8K, HumanEval) show large variability (&gt;10 point gaps), showing task properties modulate inference stochasticity.",
            "uuids": [
                "e482.0"
            ]
        },
        {
            "text": "Greedy vs sampling decoding shows task-dependent effects: greedy usually outperforms average sampling on deterministic tasks but not on open-ended tasks (AlpacaEval), showing inference-layer and task-layer interactions.",
            "uuids": [
                "e482.1"
            ]
        },
        {
            "text": "Temperature effects vary by task: higher temperature harms reasoning/code (GSM8K, HumanEval) but can help open-ended tasks (AlpacaEval), showing task-specification modulates inference stochasticity.",
            "uuids": [
                "e482.3"
            ]
        },
        {
            "text": "Hyperparameter tuning and random seeds in RL show similar magnitudes of variability, with 22 runs needed to characterize reward-vs-KL frontier, demonstrating training-layer stochasticity.",
            "uuids": [
                "e649.1"
            ]
        },
        {
            "text": "Random initialization seed produces measurable variance across benchmarks (std up to a few percentage points), and this seed variance is typically smaller than but comparable to per-model 95% CIs.",
            "uuids": [
                "e639.0"
            ]
        },
        {
            "text": "Accounting for variance increases required labeled samples by 100-200% on average (up to 500% in some cases), showing how training-layer variability propagates to sample efficiency.",
            "uuids": [
                "e486.0"
            ]
        },
        {
            "text": "Snapshot ensembling using cyclic learning rates produces diverse models from single training run, with lower pairwise correlations than non-cyclic snapshots, showing training dynamics can generate within-layer diversity.",
            "uuids": [
                "e604.1",
                "e604.2"
            ]
        },
        {
            "text": "ASWA/NASWA weight averaging reduces run-to-run variability by ~72% on average (89% for some datasets), showing training-layer interventions can reduce variance.",
            "uuids": [
                "e612.1"
            ]
        },
        {
            "text": "Accelerated ensembling reduces variability by ~48% (accuracy SD from 0.26% to 0.19%), showing training-layer mitigation effectiveness.",
            "uuids": [
                "e496.4"
            ]
        },
        {
            "text": "Data contamination (train-test overlap) can inflate results on some benchmarks, showing how training-layer factors affect evaluation-layer measurements.",
            "uuids": [
                "e642.3"
            ]
        },
        {
            "text": "Formatting sensitivity shows large effects: FORMATSPREAD finds spreads of 20-76 accuracy points across formatting variations, demonstrating task-specification layer dominance.",
            "uuids": [
                "e665.0"
            ]
        },
        {
            "text": "Prompt calibration improves mean accuracy (up to 30 points) but doesn't eliminate high variance across permutations, showing some mitigations affect bias but not variance.",
            "uuids": [
                "e644.0",
                "e638.4"
            ]
        },
        {
            "text": "Self-consistency with 40 sampled paths produces larger gains than prompt-ensemble approaches, showing inference-layer aggregation can overcome task-specification variability.",
            "uuids": [
                "e646.2"
            ]
        },
        {
            "text": "Pass@N metrics show correct answers often present among many stochastic generations (high Pass@256) but single-sample reliability is poor, demonstrating inference-layer stochasticity.",
            "uuids": [
                "e662.0"
            ]
        },
        {
            "text": "SFT scaling improves single-sample reliability (PassRatio) while Pass@256 saturates, showing training-layer interventions can reduce inference-layer variability.",
            "uuids": [
                "e662.0"
            ]
        },
        {
            "text": "Semantic entropy clustering reduces variability from lexical paraphrases and predicts correctness better than token-sequence baselines (AUROC 0.790 vs 0.698), showing measurement-layer choices affect observed variability.",
            "uuids": [
                "e605.0",
                "e663.1"
            ]
        },
        {
            "text": "Model entropy (next-token distribution sharpness) varies across models and affects sensitivity metrics, showing architectural properties modulate inference behavior.",
            "uuids": [
                "e610.2"
            ]
        },
        {
            "text": "Alignment (instruction tuning) reduces entropy and RDP, showing training-layer interventions affect inference-layer and task-layer sensitivity.",
            "uuids": [
                "e623.3"
            ]
        },
        {
            "text": "Hardware and environment heterogeneity (CPU instruction sets, GPU memory) causes concrete reproducibility failures, demonstrating environmental-layer effects.",
            "uuids": [
                "e643.3"
            ]
        },
        {
            "text": "Proprietary model updates and API changes cause results to change over time, showing environmental-layer (external dependencies) effects on reproducibility.",
            "uuids": [
                "e453.2"
            ]
        },
        {
            "text": "Network stalls and stragglers are significant sources of variability at scale, with network-related interruptions accounting for 8.4% of total interruptions, showing environmental-layer effects.",
            "uuids": [
                "e652.2"
            ]
        }
    ],
    "theory_statements": [
        "Variability sources are organized in nested layers: environmental → training → architectural → inference → task-specification, where each layer can influence and be influenced by others.",
        "Each layer can amplify stochasticity from other layers through non-linear dynamics (optimization landscapes, attention mechanisms, autoregressive generation, threshold effects).",
        "The dominant source of observed variability depends on the experimental context: training experiments expose optimization stochasticity, inference experiments expose sampling stochasticity, evaluation experiments expose prompt/task stochasticity.",
        "Model properties (size, alignment, architecture) act as transfer functions that modulate sensitivity at all layers: larger and more aligned models show reduced sensitivity to lower-layer stochasticity.",
        "Interactions between layers are often multiplicative or threshold-based rather than additive, leading to occasional catastrophic failures (e.g., vanishing gradients, degenerate outputs) and non-linear variance propagation.",
        "Mitigation strategies must target the dominant source for the specific context; universal mitigations are ineffective because they cannot address all layers simultaneously and interactions between layers can negate single-layer interventions.",
        "Variance reduction at one layer can reduce variance at higher layers (e.g., alignment reducing inference sensitivity), but not always (e.g., calibration improving mean but not reducing variance).",
        "Different sources at the same layer often produce similar magnitudes of variability, suggesting they act through common amplification mechanisms within that layer.",
        "Task properties (open-endedness, constraint level, sequence length) modulate which layers dominate observed variability.",
        "Measurement choices (discrete vs continuous metrics, aggregation methods) can amplify or suppress underlying variability from lower layers."
    ],
    "new_predictions_likely": [
        "Combining multiple mitigation strategies targeting different layers (e.g., ASWA training + deterministic decoding + prompt standardization + environment containerization) will show super-additive variance reduction compared to individual mitigations, with total variance reduction exceeding the sum of individual reductions.",
        "Tasks requiring longer generation sequences will show exponentially or polynomially increasing variance with sequence length due to autoregressive error accumulation across the hierarchy, with the rate depending on model alignment and size.",
        "Models trained with variance-reduction techniques (e.g., ASWA/NASWA) will show reduced sensitivity to inference-time stochasticity (temperature, sampling) compared to standard-trained models, with the reduction proportional to the training-layer variance reduction achieved.",
        "Measuring variance at intermediate layers (hidden states, attention patterns, gradient norms) will reveal which hierarchical level dominates for a given task, enabling targeted mitigation that is more efficient than universal approaches.",
        "For a given task, there will be a predictable ordering of which layer dominates variance: highly constrained tasks will show training/architectural dominance, moderately constrained tasks will show inference dominance, and open-ended tasks will show task-specification dominance.",
        "Cross-layer interventions (e.g., training with diverse prompts to reduce prompt sensitivity) will be more effective than single-layer interventions for reducing overall variance.",
        "Models with lower training-layer variance (measured by seed variance) will show lower inference-layer variance (measured by sampling variance) for the same task, demonstrating cross-layer correlation."
    ],
    "new_predictions_unknown": [
        "Whether there exists a fundamental lower bound on achievable variance that cannot be reduced below a certain threshold regardless of mitigation strategies, due to inherent chaotic dynamics in neural network optimization and generation, or whether variance can be reduced arbitrarily with sufficient intervention.",
        "Whether variance at different hierarchical levels is correlated across different model architectures (e.g., whether models with high training variance also show high inference variance), which would suggest architectural factors that propagate instability universally vs. architecture-specific propagation patterns.",
        "Whether interventions at higher layers (e.g., prompt engineering) can fully compensate for instability at lower layers (e.g., poor initialization), or whether lower-layer instability creates irreducible upper bounds on reproducibility that cannot be overcome by higher-layer interventions.",
        "Whether the hierarchical structure is universal across all neural architectures or specific to transformers, and how emerging architectures (e.g., state-space models, mixture-of-experts, diffusion models) fit into this framework.",
        "Whether there are critical thresholds or phase transitions in model properties (size, alignment level) where the dominant source of variance shifts discontinuously between layers.",
        "Whether variance propagation follows predictable mathematical laws (e.g., power laws, exponential relationships) that could enable quantitative prediction of total variance from layer-specific measurements.",
        "Whether human-in-the-loop interventions (e.g., human feedback, human prompt engineering) can break the hierarchical propagation of variance or whether they simply add another layer to the hierarchy."
    ],
    "negative_experiments": [
        "Finding cases where fixing all known sources at one layer completely eliminates variance at higher layers would suggest layers are independent rather than hierarchically coupled.",
        "Demonstrating that variance reduction at a lower layer (e.g., deterministic training via fixed seeds) does not reduce variance at higher layers (e.g., inference sampling variance) would challenge the amplification mechanism.",
        "Showing that different sources at the same layer have dramatically different effects (orders of magnitude) would challenge the claim that they act through a common mechanism within layers.",
        "Finding tasks where variance decreases with longer generation sequences would contradict the error accumulation prediction.",
        "Demonstrating that model properties (size, alignment) do not modulate sensitivity consistently across different types of stochasticity would challenge the transfer function concept.",
        "Finding cases where combining mitigations from different layers produces sub-additive variance reduction (total reduction less than sum of individual reductions) would challenge the multiplicative interaction model.",
        "Showing that the dominant source of variance does not vary predictably with task properties would challenge the context-dependence claim.",
        "Finding architectures where variance at one layer is completely independent of variance at other layers would challenge the hierarchical coupling model."
    ],
    "unaccounted_for": [
        {
            "text": "The exact mathematical form of how variance propagates and amplifies through layers is not specified (e.g., whether it's multiplicative, exponential, or follows other functional forms).",
            "uuids": []
        },
        {
            "text": "Why some models (e.g., Claude 3.5 Sonnet) show lower variance than others is not fully explained by the hierarchical structure alone - there may be model-specific factors beyond size and alignment.",
            "uuids": [
                "e640.2",
                "e488.0"
            ]
        },
        {
            "text": "The role of emergent capabilities and phase transitions in model behavior as sources of discontinuous variance changes is not addressed.",
            "uuids": []
        },
        {
            "text": "How memorization and data contamination fit into the hierarchical structure - they seem to bypass normal layer interactions.",
            "uuids": [
                "e642.3"
            ]
        },
        {
            "text": "Why some mitigation strategies (e.g., calibration) improve mean performance but don't reduce variance - the theory doesn't fully explain bias-variance decoupling.",
            "uuids": [
                "e644.3",
                "e638.4"
            ]
        },
        {
            "text": "The role of human judgment variability in evaluation and how it interacts with model variability is not fully integrated into the hierarchy.",
            "uuids": [
                "e651.0"
            ]
        },
        {
            "text": "How dataset quality and annotation errors propagate through the hierarchy and interact with model-induced variability.",
            "uuids": [
                "e635.0"
            ]
        },
        {
            "text": "The specific mechanisms by which architectural properties (attention patterns, layer depth, etc.) modulate sensitivity are not detailed.",
            "uuids": []
        },
        {
            "text": "Why certain tasks show much higher variance than others even when controlling for open-endedness and constraint level.",
            "uuids": [
                "e639.0"
            ]
        },
        {
            "text": "How adversarial examples and jailbreaks fit into the framework - they seem to exploit cross-layer interactions in ways not predicted by the basic hierarchy.",
            "uuids": [
                "e666.5"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some mitigation strategies (e.g., calibration) improve mean performance but don't reduce variance, suggesting variance and bias can be decoupled rather than hierarchically linked.",
            "uuids": [
                "e638.4",
                "e644.3"
            ]
        },
        {
            "text": "Snapshot ensembling produces diverse models from a single training run, suggesting within-layer diversity generation rather than purely hierarchical propagation from lower layers.",
            "uuids": [
                "e604.1"
            ]
        },
        {
            "text": "Some sources at the same layer produce very different magnitudes of effect in specific contexts (e.g., prompt formatting can dominate over sampling temperature), challenging the common-mechanism claim.",
            "uuids": [
                "e665.0"
            ]
        },
        {
            "text": "IRT-based tiny-benchmarks increase variance despite being designed to reduce it, suggesting measurement-layer interventions can sometimes amplify rather than suppress lower-layer variability.",
            "uuids": [
                "e639.5"
            ]
        },
        {
            "text": "Permutation transferability is very low across model sizes (correlation 0.05), suggesting task-specification effects may not propagate predictably through architectural differences.",
            "uuids": [
                "e638.5"
            ]
        },
        {
            "text": "Some models show increased sensitivity after instruction fine-tuning on certain metrics, contradicting the general pattern that alignment reduces variance.",
            "uuids": [
                "e610.4"
            ]
        },
        {
            "text": "Discrete semantic entropy can outperform methods with access to probabilities, suggesting higher-layer abstractions can sometimes bypass lower-layer information.",
            "uuids": [
                "e663.1"
            ]
        }
    ],
    "special_cases": [
        "For deterministic tasks with constrained outputs (e.g., multiple choice), higher-layer stochasticity may be suppressed, making lower-layer sources (training, initialization) dominant.",
        "For models with very low entropy (highly aligned/instruction-tuned), inference-layer stochasticity may be minimal regardless of sampling parameters, shifting dominance to task-specification layer.",
        "In few-shot learning contexts, task-specification stochasticity (example selection/order) may dominate all other sources, with variance from prompt sampling exceeding variance from all other layers combined.",
        "For very small models or undertrained models, architectural capacity limitations may create a ceiling effect that masks hierarchical interactions and makes training-layer factors dominant.",
        "In adversarial or jailbreak scenarios, task-specification layer may exploit cross-layer interactions in ways that bypass normal hierarchical propagation.",
        "For tasks with very long generation sequences, autoregressive error accumulation may cause inference-layer variance to dominate even when other layers are well-controlled.",
        "In human evaluation contexts, human judgment variability may dominate model-induced variability, effectively adding a new top layer to the hierarchy.",
        "For models at critical scale thresholds (emergence of capabilities), architectural-layer effects may show discontinuous changes that don't follow smooth hierarchical propagation.",
        "In real-world deployment with API access only, environmental-layer factors (API changes, rate limiting, version updates) may dominate all model-internal sources.",
        "For tasks requiring factual knowledge, memorization and data contamination effects may bypass normal layer interactions and create direct training-to-evaluation connections."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Summers & Dinneen (2019) Nondeterminism and Instability in Neural Network Optimization [Documents multiple sources of variability but doesn't propose hierarchical organization or cross-layer interactions]",
            "Dodge et al. (2020) Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping [Shows interactions between sources but no hierarchical framework or systematic layer organization]",
            "Lorenz (1963) Deterministic Nonperiodic Flow [Chaos theory foundation showing sensitivity to initial conditions, but not applied to ML systems hierarchically]",
            "Belz et al. (2021) A Systematic Review of Reproducibility Research in NLP [Surveys reproducibility issues but doesn't propose unified hierarchical theory]",
            "Nagarajan & Kolter (2019) Uniform convergence may be unable to explain generalization in deep learning [Discusses instability in neural networks but not hierarchical organization]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
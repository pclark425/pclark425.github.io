<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Demonstration Information Bottleneck Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1625</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1625</p>
                <p><strong>Name:</strong> Prompt-Demonstration Information Bottleneck Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory proposes that the structure of prompts and demonstrations acts as an information bottleneck that limits the effective transfer of scientific subdomain knowledge from the LLM's internal representations to its outputs. The theory asserts that, regardless of the LLM's capacity or training, if the prompt and demonstration structure fails to encode or elicit the relevant subdomain-specific information, the model's simulation accuracy will be fundamentally limited. The theory further posits that this bottleneck is quantifiable in terms of information-theoretic measures (e.g., mutual information between prompt/demonstration and target subdomain knowledge), and that optimizing prompt structure to maximize this information transfer is necessary for high-fidelity scientific simulation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt Information Bottleneck Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt and demonstration structure &#8594; has_low_mutual_information_with &#8594; target subdomain knowledge</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM scientific simulation &#8594; is_limited_to &#8594; low accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Information-theoretic analyses of prompt engineering show that prompts with higher mutual information with the target task yield better LLM performance. </li>
    <li>Empirical studies demonstrate that even large LLMs fail to retrieve or apply relevant knowledge if the prompt does not encode or elicit the necessary information. </li>
    <li>Prompt compression and ablation studies reveal that removing key information from demonstrations sharply reduces accuracy, regardless of model size. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While related to information bottleneck theory and prompt informativeness, the theory's focus on the prompt as a quantifiable bottleneck for scientific simulation is new.</p>            <p><strong>What Already Exists:</strong> Information bottleneck theory is known in deep learning, and prompt informativeness is discussed in prompt engineering.</p>            <p><strong>What is Novel:</strong> The explicit application of information-theoretic bottleneck concepts to prompt/demonstration structure in LLM scientific simulation is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby et al. (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]</li>
    <li>Zhou et al. (2022) Large Language Models are Human-Level Prompt Engineers [Prompt informativeness and structure]</li>
</ul>
            <h3>Statement 1: Prompt Optimization Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt and demonstration structure &#8594; is_optimized_to_maximize_mutual_information_with &#8594; target subdomain knowledge</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM scientific simulation &#8594; achieves &#8594; maximal possible accuracy (given model knowledge)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompt optimization studies show that maximizing informativeness and relevance of demonstrations leads to the best LLM performance on scientific tasks. </li>
    <li>Chain-of-thought and step-by-step prompting, which increase information content, consistently improve accuracy in complex scientific reasoning tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law extends prompt optimization to a formal, information-theoretic context for scientific simulation.</p>            <p><strong>What Already Exists:</strong> Prompt optimization and informativeness are known to improve LLM performance.</p>            <p><strong>What is Novel:</strong> The law's explicit information-theoretic framing and its application to scientific simulation is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt informativeness and reasoning]</li>
    <li>Zhou et al. (2022) Large Language Models are Human-Level Prompt Engineers [Prompt informativeness and structure]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Prompts and demonstrations with higher mutual information with the target subdomain will yield higher simulation accuracy, regardless of model size.</li>
                <li>Ablating or compressing demonstrations to remove subdomain-relevant information will sharply reduce accuracy.</li>
                <li>Optimizing prompt structure to maximize information transfer will improve LLM performance on scientific simulation tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There may exist optimal prompt structures that maximize information transfer for specific subdomains, but are not human-interpretable.</li>
                <li>The information bottleneck may interact with model architecture in nontrivial ways, leading to emergent prompt structures for maximal accuracy.</li>
                <li>For some subdomains, the information bottleneck may be insurmountable due to lack of model pretraining on relevant knowledge, regardless of prompt optimization.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs achieve high accuracy with prompts that have low mutual information with the target subdomain, the theory would be falsified.</li>
                <li>If prompt optimization does not improve accuracy, or if ablation of key information does not reduce accuracy, the theory's bottleneck claim would be invalidated.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs infer correct answers from minimal or ambiguous prompts, possibly due to overfitting or memorization. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory synthesizes information-theoretic concepts and prompt engineering into a new, formal framework for LLM scientific simulation.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby et al. (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]</li>
    <li>Zhou et al. (2022) Large Language Models are Human-Level Prompt Engineers [Prompt informativeness and structure]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt informativeness and reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Demonstration Information Bottleneck Theory",
    "theory_description": "This theory proposes that the structure of prompts and demonstrations acts as an information bottleneck that limits the effective transfer of scientific subdomain knowledge from the LLM's internal representations to its outputs. The theory asserts that, regardless of the LLM's capacity or training, if the prompt and demonstration structure fails to encode or elicit the relevant subdomain-specific information, the model's simulation accuracy will be fundamentally limited. The theory further posits that this bottleneck is quantifiable in terms of information-theoretic measures (e.g., mutual information between prompt/demonstration and target subdomain knowledge), and that optimizing prompt structure to maximize this information transfer is necessary for high-fidelity scientific simulation.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt Information Bottleneck Law",
                "if": [
                    {
                        "subject": "prompt and demonstration structure",
                        "relation": "has_low_mutual_information_with",
                        "object": "target subdomain knowledge"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM scientific simulation",
                        "relation": "is_limited_to",
                        "object": "low accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Information-theoretic analyses of prompt engineering show that prompts with higher mutual information with the target task yield better LLM performance.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies demonstrate that even large LLMs fail to retrieve or apply relevant knowledge if the prompt does not encode or elicit the necessary information.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt compression and ablation studies reveal that removing key information from demonstrations sharply reduces accuracy, regardless of model size.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Information bottleneck theory is known in deep learning, and prompt informativeness is discussed in prompt engineering.",
                    "what_is_novel": "The explicit application of information-theoretic bottleneck concepts to prompt/demonstration structure in LLM scientific simulation is novel.",
                    "classification_explanation": "While related to information bottleneck theory and prompt informativeness, the theory's focus on the prompt as a quantifiable bottleneck for scientific simulation is new.",
                    "likely_classification": "new",
                    "references": [
                        "Tishby et al. (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]",
                        "Zhou et al. (2022) Large Language Models are Human-Level Prompt Engineers [Prompt informativeness and structure]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Optimization Law",
                "if": [
                    {
                        "subject": "prompt and demonstration structure",
                        "relation": "is_optimized_to_maximize_mutual_information_with",
                        "object": "target subdomain knowledge"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM scientific simulation",
                        "relation": "achieves",
                        "object": "maximal possible accuracy (given model knowledge)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompt optimization studies show that maximizing informativeness and relevance of demonstrations leads to the best LLM performance on scientific tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought and step-by-step prompting, which increase information content, consistently improve accuracy in complex scientific reasoning tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Prompt optimization and informativeness are known to improve LLM performance.",
                    "what_is_novel": "The law's explicit information-theoretic framing and its application to scientific simulation is novel.",
                    "classification_explanation": "The law extends prompt optimization to a formal, information-theoretic context for scientific simulation.",
                    "likely_classification": "new",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt informativeness and reasoning]",
                        "Zhou et al. (2022) Large Language Models are Human-Level Prompt Engineers [Prompt informativeness and structure]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Prompts and demonstrations with higher mutual information with the target subdomain will yield higher simulation accuracy, regardless of model size.",
        "Ablating or compressing demonstrations to remove subdomain-relevant information will sharply reduce accuracy.",
        "Optimizing prompt structure to maximize information transfer will improve LLM performance on scientific simulation tasks."
    ],
    "new_predictions_unknown": [
        "There may exist optimal prompt structures that maximize information transfer for specific subdomains, but are not human-interpretable.",
        "The information bottleneck may interact with model architecture in nontrivial ways, leading to emergent prompt structures for maximal accuracy.",
        "For some subdomains, the information bottleneck may be insurmountable due to lack of model pretraining on relevant knowledge, regardless of prompt optimization."
    ],
    "negative_experiments": [
        "If LLMs achieve high accuracy with prompts that have low mutual information with the target subdomain, the theory would be falsified.",
        "If prompt optimization does not improve accuracy, or if ablation of key information does not reduce accuracy, the theory's bottleneck claim would be invalidated."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs infer correct answers from minimal or ambiguous prompts, possibly due to overfitting or memorization.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can answer certain scientific questions correctly even when the prompt omits key information, suggesting other mechanisms (e.g., memorization or pattern completion) may sometimes dominate.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For tasks where the LLM has memorized canonical answers, the information bottleneck may be bypassed.",
        "In subdomains with highly redundant or overlapping knowledge, the bottleneck may be less severe."
    ],
    "existing_theory": {
        "what_already_exists": "Information bottleneck theory and prompt informativeness are known, but not in this explicit application to prompt/demonstration structure for LLM scientific simulation.",
        "what_is_novel": "The theory's explicit, quantifiable information bottleneck framing for prompt/demonstration structure in scientific simulation is new.",
        "classification_explanation": "The theory synthesizes information-theoretic concepts and prompt engineering into a new, formal framework for LLM scientific simulation.",
        "likely_classification": "new",
        "references": [
            "Tishby et al. (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]",
            "Zhou et al. (2022) Large Language Models are Human-Level Prompt Engineers [Prompt informativeness and structure]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt informativeness and reasoning]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-635",
    "original_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
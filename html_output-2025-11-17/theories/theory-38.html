<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Verification Scaling Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-38</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-38</p>
                <p><strong>Name:</strong> Verification Scaling Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> </p>
                <p><strong>Description:</strong> For reasoning tasks, training verifiers (reward models) on many model-generated solution attempts and using them to rerank or guide generation provides larger performance gains than scaling model size or training data for the generator alone. The approach works by: (1) sampling diverse high-temperature completions to explore the solution space, (2) training verifiers on these samples labeled by final-answer correctness, (3) using verifiers to select best solutions at test time, and (4) optionally using verifiers as RL rewards. Verifiers scale more favorably with data than generators, and outcome-supervised verifiers (trained only on final-answer correctness) can learn to approximate process-level correctness. The effectiveness depends on: (1) sufficient generator coverage (not overtrained), (2) large numbers of samples (100+), (3) joint LM+verification training, and (4) token-level rather than solution-level verification.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 9</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Training verifiers on 100+ model-generated samples per problem and using them for reranking provides 7-10 percentage point improvements over single-sample generation.</li>
                <li>Verifiers scale more favorably with training data than generators: doubling verifier training data provides larger gains than doubling generator training data.</li>
                <li>Outcome-supervised verifiers (trained only on final-answer correctness) can approximate process-level correctness, achieving trace errors within 1 percentage point of process-supervised verifiers.</li>
                <li>Token-level verification provides 2-3 percentage point improvements over solution-level verification by offering denser supervision.</li>
                <li>Using verifiers as RL rewards (ORM-RL) provides additional 1-2 percentage point improvements beyond reranking alone.</li>
                <li>The approach requires generators trained for only 2 epochs to preserve coverage - overtraining reduces verification effectiveness.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Verification with 6B model slightly outperformed a finetuned 175B model on full GSM8K, showing verification provides roughly the same boost as ~30x model size increase. <a href="../results/extraction-result-222.html#e222.2" class="evidence-link">[e222.2]</a> </li>
    <li>SFT with ORM reranking reduced final-answer error from 22.3% to 14.8% (-7.5 points) and trace error from 11.4% to 4.4% (-7.0 points), showing large gains from verification. <a href="../results/extraction-result-221.html#e221.4" class="evidence-link">[e221.4]</a> </li>
    <li>ORM-RL (using ORM as RL reward) achieved final-answer error 12.7% and trace error 3.4%, the best combination of final and trace accuracy. <a href="../results/extraction-result-221.html#e221.2" class="evidence-link">[e221.2]</a> </li>
    <li>PRM (process-supervised) reranking achieved trace error 3.5%, slightly better than ORM (4.4%), but required expensive human step-level annotations. <a href="../results/extraction-result-221.html#e221.3" class="evidence-link">[e221.3]</a> </li>
    <li>Token-level verifiers outperformed solution-level verifiers by providing dense supervision and being less prone to overfitting. <a href="../results/extraction-result-222.html#e222.3" class="evidence-link">[e222.3]</a> </li>
    <li>Joint LM+verification training improved verifier performance compared to verification-only training. <a href="../results/extraction-result-222.html#e222.4" class="evidence-link">[e222.4]</a> </li>
    <li>Verification performance improved with number of completions up to ~400, then declined, showing optimal sampling range. <a href="../results/extraction-result-222.html#e222.2" class="evidence-link">[e222.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Training verifiers on scientific QA with 100+ model-generated answers per question would provide 8-12 percentage point improvements over single-answer generation.</li>
                <li>Outcome-supervised verifiers for scientific QA (trained on final-answer correctness) would achieve within 1-2 percentage points of process-supervised verifiers on trace correctness.</li>
                <li>Token-level verification for scientific reasoning would provide 2-4 percentage point improvements over solution-level verification.</li>
                <li>Using verifiers as RL rewards for scientific QA would provide additional 1-3 percentage point improvements beyond reranking.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether verification would work for scientific QA tasks without clear correctness criteria - might require human verification or domain-specific metrics.</li>
                <li>The optimal number of samples for scientific literature QA - whether 100+ is sufficient or whether more complex reasoning requires 200-400 samples.</li>
                <li>Whether outcome-supervised verifiers would approximate process correctness for multi-document scientific reasoning where intermediate steps involve synthesizing information.</li>
                <li>How verification would scale to very large scientific corpora - whether training verifiers on millions of papers would be computationally feasible.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If scaling generator size provides equal or better improvements than verification when controlling for total compute, this would challenge the verification-scaling hypothesis.</li>
                <li>If outcome-supervised verifiers perform much worse than process-supervised verifiers on trace correctness (>5 percentage points), this would question the approximation mechanism.</li>
                <li>If solution-level verification performs as well as token-level verification, this would challenge the dense-supervision hypothesis.</li>
                <li>If verification effectiveness does not improve with number of samples beyond 50, this would suggest lower optimal sampling than predicted.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Verification performance declined beyond ~400 completions, suggesting an optimal range rather than monotonic improvement. <a href="../results/extraction-result-222.html#e222.2" class="evidence-link">[e222.2]</a> </li>
    <li>The theory doesn't explain why verification requires generators trained for only 2 epochs - why not more training? <a href="../results/extraction-result-222.html#e222.2" class="evidence-link">[e222.2]</a> </li>
    <li>PRM achieved only marginally better trace error (3.5%) than ORM (4.4%), suggesting diminishing returns from expensive process supervision. <a href="../results/extraction-result-221.html#e221.3" class="evidence-link">[e221.3]</a> </li>
    <li>The theory doesn't explain the large variation in verification effectiveness across tasks - some tasks might not benefit as much. <a href="../results/extraction-result-222.html#e222.2" class="evidence-link">[e222.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [Original verification scaling work]</li>
    <li>Lightman et al. (2023) Let's Verify Step by Step [Process vs outcome supervision for verifiers]</li>
    <li>Uesato et al. (2022) Solving math word problems with process- and outcome-based feedback [Comparing supervision types]</li>
    <li>Saunders et al. (2022) Self-critiquing models for assisting human evaluators [Verifier-based approaches]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Verification Scaling Theory",
    "theory_description": "For reasoning tasks, training verifiers (reward models) on many model-generated solution attempts and using them to rerank or guide generation provides larger performance gains than scaling model size or training data for the generator alone. The approach works by: (1) sampling diverse high-temperature completions to explore the solution space, (2) training verifiers on these samples labeled by final-answer correctness, (3) using verifiers to select best solutions at test time, and (4) optionally using verifiers as RL rewards. Verifiers scale more favorably with data than generators, and outcome-supervised verifiers (trained only on final-answer correctness) can learn to approximate process-level correctness. The effectiveness depends on: (1) sufficient generator coverage (not overtrained), (2) large numbers of samples (100+), (3) joint LM+verification training, and (4) token-level rather than solution-level verification.",
    "supporting_evidence": [
        {
            "text": "Verification with 6B model slightly outperformed a finetuned 175B model on full GSM8K, showing verification provides roughly the same boost as ~30x model size increase.",
            "uuids": [
                "e222.2"
            ]
        },
        {
            "text": "SFT with ORM reranking reduced final-answer error from 22.3% to 14.8% (-7.5 points) and trace error from 11.4% to 4.4% (-7.0 points), showing large gains from verification.",
            "uuids": [
                "e221.4"
            ]
        },
        {
            "text": "ORM-RL (using ORM as RL reward) achieved final-answer error 12.7% and trace error 3.4%, the best combination of final and trace accuracy.",
            "uuids": [
                "e221.2"
            ]
        },
        {
            "text": "PRM (process-supervised) reranking achieved trace error 3.5%, slightly better than ORM (4.4%), but required expensive human step-level annotations.",
            "uuids": [
                "e221.3"
            ]
        },
        {
            "text": "Token-level verifiers outperformed solution-level verifiers by providing dense supervision and being less prone to overfitting.",
            "uuids": [
                "e222.3"
            ]
        },
        {
            "text": "Joint LM+verification training improved verifier performance compared to verification-only training.",
            "uuids": [
                "e222.4"
            ]
        },
        {
            "text": "Verification performance improved with number of completions up to ~400, then declined, showing optimal sampling range.",
            "uuids": [
                "e222.2"
            ]
        }
    ],
    "theory_statements": [
        "Training verifiers on 100+ model-generated samples per problem and using them for reranking provides 7-10 percentage point improvements over single-sample generation.",
        "Verifiers scale more favorably with training data than generators: doubling verifier training data provides larger gains than doubling generator training data.",
        "Outcome-supervised verifiers (trained only on final-answer correctness) can approximate process-level correctness, achieving trace errors within 1 percentage point of process-supervised verifiers.",
        "Token-level verification provides 2-3 percentage point improvements over solution-level verification by offering denser supervision.",
        "Using verifiers as RL rewards (ORM-RL) provides additional 1-2 percentage point improvements beyond reranking alone.",
        "The approach requires generators trained for only 2 epochs to preserve coverage - overtraining reduces verification effectiveness."
    ],
    "new_predictions_likely": [
        "Training verifiers on scientific QA with 100+ model-generated answers per question would provide 8-12 percentage point improvements over single-answer generation.",
        "Outcome-supervised verifiers for scientific QA (trained on final-answer correctness) would achieve within 1-2 percentage points of process-supervised verifiers on trace correctness.",
        "Token-level verification for scientific reasoning would provide 2-4 percentage point improvements over solution-level verification.",
        "Using verifiers as RL rewards for scientific QA would provide additional 1-3 percentage point improvements beyond reranking."
    ],
    "new_predictions_unknown": [
        "Whether verification would work for scientific QA tasks without clear correctness criteria - might require human verification or domain-specific metrics.",
        "The optimal number of samples for scientific literature QA - whether 100+ is sufficient or whether more complex reasoning requires 200-400 samples.",
        "Whether outcome-supervised verifiers would approximate process correctness for multi-document scientific reasoning where intermediate steps involve synthesizing information.",
        "How verification would scale to very large scientific corpora - whether training verifiers on millions of papers would be computationally feasible."
    ],
    "negative_experiments": [
        "If scaling generator size provides equal or better improvements than verification when controlling for total compute, this would challenge the verification-scaling hypothesis.",
        "If outcome-supervised verifiers perform much worse than process-supervised verifiers on trace correctness (&gt;5 percentage points), this would question the approximation mechanism.",
        "If solution-level verification performs as well as token-level verification, this would challenge the dense-supervision hypothesis.",
        "If verification effectiveness does not improve with number of samples beyond 50, this would suggest lower optimal sampling than predicted."
    ],
    "unaccounted_for": [
        {
            "text": "Verification performance declined beyond ~400 completions, suggesting an optimal range rather than monotonic improvement.",
            "uuids": [
                "e222.2"
            ]
        },
        {
            "text": "The theory doesn't explain why verification requires generators trained for only 2 epochs - why not more training?",
            "uuids": [
                "e222.2"
            ]
        },
        {
            "text": "PRM achieved only marginally better trace error (3.5%) than ORM (4.4%), suggesting diminishing returns from expensive process supervision.",
            "uuids": [
                "e221.3"
            ]
        },
        {
            "text": "The theory doesn't explain the large variation in verification effectiveness across tasks - some tasks might not benefit as much.",
            "uuids": [
                "e222.2"
            ]
        }
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [Original verification scaling work]",
            "Lightman et al. (2023) Let's Verify Step by Step [Process vs outcome supervision for verifiers]",
            "Uesato et al. (2022) Solving math word problems with process- and outcome-based feedback [Comparing supervision types]",
            "Saunders et al. (2022) Self-critiquing models for assisting human evaluators [Verifier-based approaches]"
        ]
    },
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
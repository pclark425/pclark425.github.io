<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Propensity-Based Adjustment for Collider-Specific Spurious Correlations - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-123</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-123</p>
                <p><strong>Name:</strong> Propensity-Based Adjustment for Collider-Specific Spurious Correlations</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of distractor-robust causal discovery in open-ended virtual labs, including methods to detect, downweight, and refute spurious signals during inquiry, based on the following results.</p>
                <p><strong>Description:</strong> In representation learning and domain generalization settings, many spurious correlations arise from collider structures (C ← X → S → E ← Y) rather than fork structures (latent confounders). When conditioning on environment E (through dataset collection or model training), this opens a spurious path between C and Y through S. The spurious correlation can be eliminated by backdoor adjustment: estimating propensity scores P(C|S) and reweighting samples by these propensities to simulate do(C). This approach is more appropriate than adjusting for other covariates and can be implemented as a plug-in regularizer for existing OOD methods. The effectiveness depends critically on accurate propensity estimation, and various variance-reduction techniques (clipping, self-normalization, doubly robust estimation) can improve practical performance. When the backdoor criterion is not satisfied, alternative identification strategies such as front-door adjustment may be applicable if suitable mediators exist.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>In collider-specific spurious correlation, the causal structure is C ← X → S → E ← Y where conditioning on E creates spurious C-Y association through the path C → S → E ← Y</li>
                <li>The interventional distribution P(Y|do(C)) can be estimated via backdoor adjustment: P(Y|do(C=c)) = Σ_s P(Y|C=c,S=s)P(S=s), which blocks the spurious path</li>
                <li>Propensity score P(C=c|S=s) can be estimated via frequency decomposition (FFT) and clustering to separate invariant (C) from spurious (S) components in vision domains</li>
                <li>PSW can be added as a plug-in regularizer L_PSW to any base OOD method: minimize base_loss + λ * L_PSW where L_PSW uses importance weights π(x) = P(C=c|S=s)</li>
                <li>The bias-variance tradeoff in propensity estimation is critical: poor propensity estimates increase variance, requiring variance-reduction techniques (clipping at [ε, 1-ε], self-normalization, or doubly robust estimation)</li>
                <li>When propensity scores are extreme (near 0 or 1), clipping or trimming is necessary to prevent infinite or very large weights that destabilize training</li>
                <li>The method assumes the backdoor criterion is satisfied (S blocks all backdoor paths from C to Y); when this fails, alternative identification strategies like front-door adjustment may be applicable</li>
                <li>Propensity estimation accuracy directly impacts final performance: better propensity models yield larger improvements, while poor estimates can reduce or eliminate gains</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Collider-specific SCM analysis shows spurious correlations in OOD arise from conditioning on colliders, with theoretical support that incorrect covariate adjustment introduces bias <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> <a href="../results/extraction-result-709.html#e709.3" class="evidence-link">[e709.3]</a> </li>
    <li>PSW (propensity score weighting) implements backdoor adjustment via P(Y|do(C)) = Σ_s P(Y|C,S)P(S) and improves OOD accuracy by 0.8-1.1% when added as regularizer to base methods <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> </li>
    <li>IPS and variants (CIPS, SNIPS, doubly robust) are primary practical tools for handling exposure bias and selection bias in recommendation systems, with variance-reduction tactics to control extreme weights <a href="../results/extraction-result-793.html#e793.9" class="evidence-link">[e793.9]</a> </li>
    <li>Maximum-entropy default regime provides theoretical justification for using do(x)-style predictors when test-time confounder distributions are unknown <a href="../results/extraction-result-757.html#e757.4" class="evidence-link">[e757.4]</a> </li>
    <li>Front-door adjustment provides alternative identification strategy when backdoor criterion fails but suitable mediators exist <a href="../results/extraction-result-793.html#e793.10" class="evidence-link">[e793.10]</a> </li>
    <li>FFT-based frequency decomposition combined with clustering can separate invariant (C) from spurious (S) components in vision domains <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> </li>
    <li>Propensity score methods require careful handling of extreme weights through clipping, trimming, or self-normalization to control variance <a href="../results/extraction-result-793.html#e793.9" class="evidence-link">[e793.9]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Adding PSW regularization to ERM should improve worst-case domain accuracy by 5-15% on standard domain generalization benchmarks with clear background correlations (e.g., ColoredMNIST, PACS)</li>
                <li>FFT-based frequency separation should successfully identify spurious features in 70-80% of vision datasets where spurious correlations are primarily low-frequency (backgrounds, lighting)</li>
                <li>Combining PSW with other OOD methods (IRM, CORAL, GroupDRM) should yield additive improvements of 1-2% on average, with larger gains when base methods don't already address collider-specific correlations</li>
                <li>Using doubly robust estimation (combining propensity weighting with outcome modeling) should reduce variance and improve stability compared to pure PSW, especially in small-sample regimes</li>
                <li>Self-normalized IPS (SNIPS) should provide better finite-sample performance than standard IPS when propensity estimates are noisy</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether PSW remains effective when the collider structure is more complex (multiple colliders, feedback loops, or time-varying colliders in sequential settings)</li>
                <li>The robustness of FFT-based spurious feature detection to different image statistics, domains, and types of spurious correlations (e.g., high-frequency texture vs. low-frequency background)</li>
                <li>Whether propensity score estimation can be made more robust through adversarial training, meta-learning across domains, or neural density models for continuous C and S</li>
                <li>The effectiveness of PSW in non-vision domains (NLP, time-series, tabular data) where frequency decomposition may not apply and alternative feature separation methods are needed</li>
                <li>Whether the modest empirical gains (0.8-1.1%) represent a fundamental limitation of the approach or indicate room for improvement through better propensity estimation</li>
                <li>The computational cost-benefit tradeoff: whether the added complexity of propensity estimation and reweighting is justified by the performance gains in production systems</li>
                <li>Whether PSW can be extended to handle continuous high-dimensional C and S through kernel-based or neural propensity estimation</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding cases where fork-specific (latent confounder) models better explain OOD failures would challenge the collider-specific assumption and suggest backdoor adjustment is inappropriate</li>
                <li>Demonstrating that PSW performs worse than no adjustment when propensity estimates are noisy (e.g., with <30% accuracy) would reveal brittleness and suggest the method requires high-quality propensity models</li>
                <li>Showing that FFT-based separation fails to identify spurious features in certain image types (e.g., medical images, satellite imagery, abstract art) would limit the detection method's generality</li>
                <li>Finding that PSW provides no benefit over simpler reweighting schemes (e.g., uniform reweighting, random reweighting) would question the causal justification and suggest the gains are not due to the propensity mechanism</li>
                <li>Demonstrating that PSW fails when the backdoor criterion is violated (e.g., when S does not block all backdoor paths) would confirm the theoretical limitations</li>
                <li>Finding that the method performs poorly when C and S are highly entangled or not separable would reveal fundamental limitations of the feature separation approach</li>
                <li>Showing that extreme propensity scores (even with clipping) lead to training instability or worse performance would indicate practical limitations</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory assumes binary or low-cardinality C and S; extension to continuous high-dimensional features requires kernel-based or neural propensity estimation, which is not fully characterized <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> </li>
    <li>Propensity estimation accuracy and its impact on final performance is not fully characterized; the relationship between propensity model quality and downstream OOD performance needs more investigation <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> </li>
    <li>The method requires some form of feature separation (FFT, clustering) which may not generalize to all domains; alternative separation methods for non-vision domains are not specified <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> </li>
    <li>The modest empirical gains (0.8-1.1%) suggest either fundamental limitations or room for improvement; the theory doesn't fully explain why gains are not larger <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> </li>
    <li>The computational cost of propensity estimation (FFT, clustering, density estimation) and its impact on training time is not addressed <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> </li>
    <li>The theory doesn't specify how to handle cases where multiple spurious features exist simultaneously or interact <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> </li>
    <li>The relationship between sample size, propensity estimation quality, and final performance is not characterized <a href="../results/extraction-result-709.html#e709.0" class="evidence-link">[e709.0]</a> <a href="../results/extraction-result-793.html#e793.9" class="evidence-link">[e793.9]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Rosenbaum & Rubin (1983) The central role of the propensity score in observational studies [Original propensity score framework for causal inference]</li>
    <li>Pearl (2009) Causality: Models, Reasoning, and Inference [Backdoor adjustment, do-calculus, and collider structures]</li>
    <li>Schnabel et al. (2016) Recommendations as treatments: Debiasing learning and evaluation [IPS for recommendation systems, handling selection bias]</li>
    <li>Arjovsky et al. (2019) Invariant Risk Minimization [IRM for OOD generalization, though focused on invariance rather than propensity adjustment]</li>
    <li>Veitch et al. (2021) Counterfactual invariance to spurious correlations [Related work on using causal models for OOD, though with different structural assumptions]</li>
    <li>Zhang et al. (2024) Revisiting Spurious Correlation in Domain Generalization [The specific paper proposing PSW for collider-specific spurious correlations in representation learning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Propensity-Based Adjustment for Collider-Specific Spurious Correlations",
    "theory_description": "In representation learning and domain generalization settings, many spurious correlations arise from collider structures (C ← X → S → E ← Y) rather than fork structures (latent confounders). When conditioning on environment E (through dataset collection or model training), this opens a spurious path between C and Y through S. The spurious correlation can be eliminated by backdoor adjustment: estimating propensity scores P(C|S) and reweighting samples by these propensities to simulate do(C). This approach is more appropriate than adjusting for other covariates and can be implemented as a plug-in regularizer for existing OOD methods. The effectiveness depends critically on accurate propensity estimation, and various variance-reduction techniques (clipping, self-normalization, doubly robust estimation) can improve practical performance. When the backdoor criterion is not satisfied, alternative identification strategies such as front-door adjustment may be applicable if suitable mediators exist.",
    "supporting_evidence": [
        {
            "text": "Collider-specific SCM analysis shows spurious correlations in OOD arise from conditioning on colliders, with theoretical support that incorrect covariate adjustment introduces bias",
            "uuids": [
                "e709.0",
                "e709.3"
            ]
        },
        {
            "text": "PSW (propensity score weighting) implements backdoor adjustment via P(Y|do(C)) = Σ_s P(Y|C,S)P(S) and improves OOD accuracy by 0.8-1.1% when added as regularizer to base methods",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "IPS and variants (CIPS, SNIPS, doubly robust) are primary practical tools for handling exposure bias and selection bias in recommendation systems, with variance-reduction tactics to control extreme weights",
            "uuids": [
                "e793.9"
            ]
        },
        {
            "text": "Maximum-entropy default regime provides theoretical justification for using do(x)-style predictors when test-time confounder distributions are unknown",
            "uuids": [
                "e757.4"
            ]
        },
        {
            "text": "Front-door adjustment provides alternative identification strategy when backdoor criterion fails but suitable mediators exist",
            "uuids": [
                "e793.10"
            ]
        },
        {
            "text": "FFT-based frequency decomposition combined with clustering can separate invariant (C) from spurious (S) components in vision domains",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "Propensity score methods require careful handling of extreme weights through clipping, trimming, or self-normalization to control variance",
            "uuids": [
                "e793.9"
            ]
        }
    ],
    "theory_statements": [
        "In collider-specific spurious correlation, the causal structure is C ← X → S → E ← Y where conditioning on E creates spurious C-Y association through the path C → S → E ← Y",
        "The interventional distribution P(Y|do(C)) can be estimated via backdoor adjustment: P(Y|do(C=c)) = Σ_s P(Y|C=c,S=s)P(S=s), which blocks the spurious path",
        "Propensity score P(C=c|S=s) can be estimated via frequency decomposition (FFT) and clustering to separate invariant (C) from spurious (S) components in vision domains",
        "PSW can be added as a plug-in regularizer L_PSW to any base OOD method: minimize base_loss + λ * L_PSW where L_PSW uses importance weights π(x) = P(C=c|S=s)",
        "The bias-variance tradeoff in propensity estimation is critical: poor propensity estimates increase variance, requiring variance-reduction techniques (clipping at [ε, 1-ε], self-normalization, or doubly robust estimation)",
        "When propensity scores are extreme (near 0 or 1), clipping or trimming is necessary to prevent infinite or very large weights that destabilize training",
        "The method assumes the backdoor criterion is satisfied (S blocks all backdoor paths from C to Y); when this fails, alternative identification strategies like front-door adjustment may be applicable",
        "Propensity estimation accuracy directly impacts final performance: better propensity models yield larger improvements, while poor estimates can reduce or eliminate gains"
    ],
    "new_predictions_likely": [
        "Adding PSW regularization to ERM should improve worst-case domain accuracy by 5-15% on standard domain generalization benchmarks with clear background correlations (e.g., ColoredMNIST, PACS)",
        "FFT-based frequency separation should successfully identify spurious features in 70-80% of vision datasets where spurious correlations are primarily low-frequency (backgrounds, lighting)",
        "Combining PSW with other OOD methods (IRM, CORAL, GroupDRM) should yield additive improvements of 1-2% on average, with larger gains when base methods don't already address collider-specific correlations",
        "Using doubly robust estimation (combining propensity weighting with outcome modeling) should reduce variance and improve stability compared to pure PSW, especially in small-sample regimes",
        "Self-normalized IPS (SNIPS) should provide better finite-sample performance than standard IPS when propensity estimates are noisy"
    ],
    "new_predictions_unknown": [
        "Whether PSW remains effective when the collider structure is more complex (multiple colliders, feedback loops, or time-varying colliders in sequential settings)",
        "The robustness of FFT-based spurious feature detection to different image statistics, domains, and types of spurious correlations (e.g., high-frequency texture vs. low-frequency background)",
        "Whether propensity score estimation can be made more robust through adversarial training, meta-learning across domains, or neural density models for continuous C and S",
        "The effectiveness of PSW in non-vision domains (NLP, time-series, tabular data) where frequency decomposition may not apply and alternative feature separation methods are needed",
        "Whether the modest empirical gains (0.8-1.1%) represent a fundamental limitation of the approach or indicate room for improvement through better propensity estimation",
        "The computational cost-benefit tradeoff: whether the added complexity of propensity estimation and reweighting is justified by the performance gains in production systems",
        "Whether PSW can be extended to handle continuous high-dimensional C and S through kernel-based or neural propensity estimation"
    ],
    "negative_experiments": [
        "Finding cases where fork-specific (latent confounder) models better explain OOD failures would challenge the collider-specific assumption and suggest backdoor adjustment is inappropriate",
        "Demonstrating that PSW performs worse than no adjustment when propensity estimates are noisy (e.g., with &lt;30% accuracy) would reveal brittleness and suggest the method requires high-quality propensity models",
        "Showing that FFT-based separation fails to identify spurious features in certain image types (e.g., medical images, satellite imagery, abstract art) would limit the detection method's generality",
        "Finding that PSW provides no benefit over simpler reweighting schemes (e.g., uniform reweighting, random reweighting) would question the causal justification and suggest the gains are not due to the propensity mechanism",
        "Demonstrating that PSW fails when the backdoor criterion is violated (e.g., when S does not block all backdoor paths) would confirm the theoretical limitations",
        "Finding that the method performs poorly when C and S are highly entangled or not separable would reveal fundamental limitations of the feature separation approach",
        "Showing that extreme propensity scores (even with clipping) lead to training instability or worse performance would indicate practical limitations"
    ],
    "unaccounted_for": [
        {
            "text": "The theory assumes binary or low-cardinality C and S; extension to continuous high-dimensional features requires kernel-based or neural propensity estimation, which is not fully characterized",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "Propensity estimation accuracy and its impact on final performance is not fully characterized; the relationship between propensity model quality and downstream OOD performance needs more investigation",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "The method requires some form of feature separation (FFT, clustering) which may not generalize to all domains; alternative separation methods for non-vision domains are not specified",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "The modest empirical gains (0.8-1.1%) suggest either fundamental limitations or room for improvement; the theory doesn't fully explain why gains are not larger",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "The computational cost of propensity estimation (FFT, clustering, density estimation) and its impact on training time is not addressed",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "The theory doesn't specify how to handle cases where multiple spurious features exist simultaneously or interact",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "The relationship between sample size, propensity estimation quality, and final performance is not characterized",
            "uuids": [
                "e709.0",
                "e793.9"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some prior work assumes fork-specific (latent confounder) structures for OOD, suggesting collider-specific may not be universal",
            "uuids": [
                "e709.3"
            ]
        },
        {
            "text": "The paper's empirical gains are modest (0.8-1.1%) suggesting limited practical impact in some settings, which conflicts with the theoretical promise of causal adjustment",
            "uuids": [
                "e709.0"
            ]
        },
        {
            "text": "IPS methods can suffer from high variance when propensity scores are extreme, requiring careful variance control that may limit practical applicability",
            "uuids": [
                "e793.9"
            ]
        }
    ],
    "special_cases": [
        "When C and S are not separable via frequency decomposition (e.g., in non-vision domains or when spurious features are high-frequency), alternative feature extraction methods such as learned representations, clustering in latent space, or domain-specific heuristics are needed",
        "In settings with multiple spurious features, iterative or joint propensity estimation may be required, potentially using multi-dimensional propensity models or sequential adjustment",
        "For continuous C and S, kernel-based propensity estimation (e.g., kernel density estimation) or neural density models (e.g., normalizing flows, mixture density networks) may be necessary to estimate P(C|S)",
        "When propensity scores are extreme (near 0 or 1), clipping to [ε, 1-ε] (e.g., ε=0.01) or trimming (removing samples with extreme scores) is required to control variance and prevent training instability",
        "In small-sample regimes, self-normalized IPS (SNIPS) or doubly robust estimation should be preferred over standard IPS to reduce variance",
        "When the backdoor criterion is not satisfied (S does not block all backdoor paths), front-door adjustment may be applicable if a suitable mediator exists that satisfies the front-door criterion",
        "In sequential or time-varying settings, time-dependent propensity scores may be needed, requiring extensions to handle temporal dynamics",
        "When the collider structure is uncertain or mixed with other structures, sensitivity analysis or model selection procedures may be needed to determine the appropriate adjustment strategy"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Rosenbaum & Rubin (1983) The central role of the propensity score in observational studies [Original propensity score framework for causal inference]",
            "Pearl (2009) Causality: Models, Reasoning, and Inference [Backdoor adjustment, do-calculus, and collider structures]",
            "Schnabel et al. (2016) Recommendations as treatments: Debiasing learning and evaluation [IPS for recommendation systems, handling selection bias]",
            "Arjovsky et al. (2019) Invariant Risk Minimization [IRM for OOD generalization, though focused on invariance rather than propensity adjustment]",
            "Veitch et al. (2021) Counterfactual invariance to spurious correlations [Related work on using causal models for OOD, though with different structural assumptions]",
            "Zhang et al. (2024) Revisiting Spurious Correlation in Domain Generalization [The specific paper proposing PSW for collider-specific spurious correlations in representation learning]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 4,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
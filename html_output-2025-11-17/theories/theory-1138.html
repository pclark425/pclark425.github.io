<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explicit Intermediate Representation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1138</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1138</p>
                <p><strong>Name:</strong> Explicit Intermediate Representation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory asserts that language models (LMs) can best perform strict logical reasoning when they are guided to construct explicit, structured intermediate representations (IRs) of logical form, such as symbolic graphs, trees, or formal logic expressions, before producing final answers. The theory posits that the act of generating and manipulating these IRs, either internally or as external artifacts, is critical for accurate logical inference, and that LMs should be trained or prompted to output and reason over such representations.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Intermediate Representation Necessity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning task &#8594; requires &#8594; multi-step logical inference</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; should construct &#8594; explicit intermediate logical representation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Chain-of-thought prompting and scratchpad methods improve LM logical reasoning by encouraging stepwise, explicit reasoning. </li>
    <li>Neuro-symbolic models with explicit IRs outperform pure LMs on logic tasks. </li>
    <li>Selection-inference models that require explicit logical forms show higher interpretability and accuracy on logic benchmarks. </li>
    <li>LMs often fail at strict logical reasoning when forced to answer directly, but succeed when allowed to externalize intermediate steps. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law extends existing work by elevating IR construction from helpful to necessary for strict logic, formalizing a sufficiency/necessity claim.</p>            <p><strong>What Already Exists:</strong> Chain-of-thought and scratchpad prompting are known to help LMs with reasoning; neuro-symbolic models use explicit IRs.</p>            <p><strong>What is Novel:</strong> The claim that explicit IR construction is necessary (not just helpful) for strict logical reasoning is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation in Language Models [explicit IRs]</li>
    <li>Creswell et al. (2022) Selection-inference: Exploiting Large Language Models for Interpretable Logical Reasoning [explicit logical forms]</li>
</ul>
            <h3>Statement 1: IR Manipulation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; constructs &#8594; explicit logical IR</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can apply &#8594; formal logic rules to IR<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; achieves &#8594; higher logical accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Models that manipulate explicit IRs (e.g., logic trees) achieve higher accuracy on logic benchmarks. </li>
    <li>Explicit IRs allow for the application of formal logic rules, which is not possible with unstructured text alone. </li>
    <li>Neuro-symbolic approaches that separate IR construction and rule application outperform end-to-end LMs on strict logic tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is a direct extension of recent work, but formalizes sufficiency and the mechanism of rule application.</p>            <p><strong>What Already Exists:</strong> Some models use explicit IRs for logic, but not all LMs do so by default.</p>            <p><strong>What is Novel:</strong> The law claims that IR manipulation is a sufficient condition for improved logical accuracy.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise IRs]</li>
    <li>Creswell et al. (2022) Selection-inference: Exploiting Large Language Models for Interpretable Logical Reasoning [explicit logical forms]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If LMs are prompted or trained to output explicit logical forms before answers, their logical reasoning accuracy will increase.</li>
                <li>LMs that generate and manipulate IRs will outperform those that do not on multi-step logic puzzles.</li>
                <li>Introducing explicit IR construction in LM training will improve generalization to unseen logical tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LMs are trained to generate IRs in a self-supervised way, they may develop novel, more efficient logical representations.</li>
                <li>LMs with IRs may generalize to new logic domains better than those without.</li>
                <li>Explicit IRs may enable LMs to discover new logical rules or shortcuts not present in training data.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs that do not use explicit IRs can match or exceed the logical accuracy of those that do, the theory is undermined.</li>
                <li>If explicit IRs do not improve logical reasoning on complex tasks, the theory is called into question.</li>
                <li>If explicit IR construction leads to overfitting or reduced performance on naturalistic reasoning, the theory's generality is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs show improved logic performance with scale alone, without explicit IRs. </li>
    <li>Certain tasks (e.g., simple logic, factual recall) may not require explicit IRs for high accuracy. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory formalizes and extends the role of IRs from helpful to necessary/sufficient for strict logic, making a stronger claim than prior work.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation in Language Models [explicit IRs]</li>
    <li>Creswell et al. (2022) Selection-inference: Exploiting Large Language Models for Interpretable Logical Reasoning [explicit logical forms]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Explicit Intermediate Representation Theory",
    "theory_description": "This theory asserts that language models (LMs) can best perform strict logical reasoning when they are guided to construct explicit, structured intermediate representations (IRs) of logical form, such as symbolic graphs, trees, or formal logic expressions, before producing final answers. The theory posits that the act of generating and manipulating these IRs, either internally or as external artifacts, is critical for accurate logical inference, and that LMs should be trained or prompted to output and reason over such representations.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Intermediate Representation Necessity Law",
                "if": [
                    {
                        "subject": "reasoning task",
                        "relation": "requires",
                        "object": "multi-step logical inference"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "should construct",
                        "object": "explicit intermediate logical representation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Chain-of-thought prompting and scratchpad methods improve LM logical reasoning by encouraging stepwise, explicit reasoning.",
                        "uuids": []
                    },
                    {
                        "text": "Neuro-symbolic models with explicit IRs outperform pure LMs on logic tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Selection-inference models that require explicit logical forms show higher interpretability and accuracy on logic benchmarks.",
                        "uuids": []
                    },
                    {
                        "text": "LMs often fail at strict logical reasoning when forced to answer directly, but succeed when allowed to externalize intermediate steps.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Chain-of-thought and scratchpad prompting are known to help LMs with reasoning; neuro-symbolic models use explicit IRs.",
                    "what_is_novel": "The claim that explicit IR construction is necessary (not just helpful) for strict logical reasoning is new.",
                    "classification_explanation": "The law extends existing work by elevating IR construction from helpful to necessary for strict logic, formalizing a sufficiency/necessity claim.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]",
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation in Language Models [explicit IRs]",
                        "Creswell et al. (2022) Selection-inference: Exploiting Large Language Models for Interpretable Logical Reasoning [explicit logical forms]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "IR Manipulation Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "constructs",
                        "object": "explicit logical IR"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can apply",
                        "object": "formal logic rules to IR"
                    },
                    {
                        "subject": "language model",
                        "relation": "achieves",
                        "object": "higher logical accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Models that manipulate explicit IRs (e.g., logic trees) achieve higher accuracy on logic benchmarks.",
                        "uuids": []
                    },
                    {
                        "text": "Explicit IRs allow for the application of formal logic rules, which is not possible with unstructured text alone.",
                        "uuids": []
                    },
                    {
                        "text": "Neuro-symbolic approaches that separate IR construction and rule application outperform end-to-end LMs on strict logic tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Some models use explicit IRs for logic, but not all LMs do so by default.",
                    "what_is_novel": "The law claims that IR manipulation is a sufficient condition for improved logical accuracy.",
                    "classification_explanation": "The law is a direct extension of recent work, but formalizes sufficiency and the mechanism of rule application.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise IRs]",
                        "Creswell et al. (2022) Selection-inference: Exploiting Large Language Models for Interpretable Logical Reasoning [explicit logical forms]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If LMs are prompted or trained to output explicit logical forms before answers, their logical reasoning accuracy will increase.",
        "LMs that generate and manipulate IRs will outperform those that do not on multi-step logic puzzles.",
        "Introducing explicit IR construction in LM training will improve generalization to unseen logical tasks."
    ],
    "new_predictions_unknown": [
        "If LMs are trained to generate IRs in a self-supervised way, they may develop novel, more efficient logical representations.",
        "LMs with IRs may generalize to new logic domains better than those without.",
        "Explicit IRs may enable LMs to discover new logical rules or shortcuts not present in training data."
    ],
    "negative_experiments": [
        "If LMs that do not use explicit IRs can match or exceed the logical accuracy of those that do, the theory is undermined.",
        "If explicit IRs do not improve logical reasoning on complex tasks, the theory is called into question.",
        "If explicit IR construction leads to overfitting or reduced performance on naturalistic reasoning, the theory's generality is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs show improved logic performance with scale alone, without explicit IRs.",
            "uuids": []
        },
        {
            "text": "Certain tasks (e.g., simple logic, factual recall) may not require explicit IRs for high accuracy.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks (e.g., simple logic) may not benefit from explicit IRs.",
            "uuids": []
        },
        {
            "text": "Very large LMs sometimes perform well on logic tasks without explicit IRs, suggesting scale can partially substitute for structure.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with trivial logic may not require explicit IRs.",
        "Tasks with ambiguous or naturalistic reasoning may not map cleanly to formal IRs.",
        "IR construction may be less effective for tasks requiring world knowledge or pragmatic inference."
    ],
    "existing_theory": {
        "what_already_exists": "Chain-of-thought, scratchpad, and neuro-symbolic models with explicit IRs.",
        "what_is_novel": "The claim that explicit IR construction is necessary and sufficient for strict logical reasoning in LMs.",
        "classification_explanation": "The theory formalizes and extends the role of IRs from helpful to necessary/sufficient for strict logic, making a stronger claim than prior work.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [stepwise reasoning]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation in Language Models [explicit IRs]",
            "Creswell et al. (2022) Selection-inference: Exploiting Large Language Models for Interpretable Logical Reasoning [explicit logical forms]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-604",
    "original_theory_name": "Prompt Decomposition and Iterative Composition Law",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
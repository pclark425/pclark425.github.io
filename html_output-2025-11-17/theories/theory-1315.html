<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Fidelity Law for Graph Linearization in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1315</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1315</p>
                <p><strong>Name:</strong> Semantic Fidelity Law for Graph Linearization in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that the ideal graph-to-text linearization for LLMs must preserve the full semantic content of the original graph, such that all graph isomorphisms and structural properties are recoverable from the text. Semantic fidelity ensures that LLMs can learn and reason about the true graph structure, not just its serialization.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Fidelity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; preserves_graph_isomorphism &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_linearization &#8594; encodes_all_structural_properties &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_recover_original_graph &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; learns_semantic_graph_patterns &#8594; effectively</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Graph-to-sequence models that lose structural information during linearization perform worse on graph reconstruction and reasoning tasks. </li>
    <li>Semantic fidelity is critical for tasks requiring exact graph recovery or reasoning about graph isomorphism. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While semantic fidelity is known in graph theory, its formalization as a law for LLM graph linearization is new.</p>            <p><strong>What Already Exists:</strong> Semantic fidelity is a known requirement in graph encoding and isomorphism testing.</p>            <p><strong>What is Novel:</strong> Its explicit application as a law for LLM-based graph linearization is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Babai (2016) Graph Isomorphism in Quasipolynomial Time [Graph isomorphism and encoding]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic fidelity in graph-to-sequence]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs trained on semantically faithful linearizations will outperform those trained on lossy encodings in graph reconstruction tasks.</li>
                <li>Loss of structural information in linearization will lead to degraded LLM performance on graph reasoning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Semantic fidelity may enable LLMs to generalize to novel graph topologies not seen during training.</li>
                <li>Semantically faithful encodings could allow LLMs to perform zero-shot graph isomorphism detection.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs trained on semantically faithful encodings do not outperform those trained on lossy encodings, the law would be challenged.</li>
                <li>If LLMs can recover original graphs from lossy encodings, the law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The trade-off between semantic fidelity and encoding length/efficiency is not fully addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory applies known semantic fidelity principles to LLM graph linearization in a novel, formalized way.</p>
            <p><strong>References:</strong> <ul>
    <li>Babai (2016) Graph Isomorphism in Quasipolynomial Time [Graph isomorphism and encoding]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic fidelity in graph-to-sequence]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Fidelity Law for Graph Linearization in LLMs",
    "theory_description": "This theory asserts that the ideal graph-to-text linearization for LLMs must preserve the full semantic content of the original graph, such that all graph isomorphisms and structural properties are recoverable from the text. Semantic fidelity ensures that LLMs can learn and reason about the true graph structure, not just its serialization.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Fidelity Law",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "preserves_graph_isomorphism",
                        "object": "True"
                    },
                    {
                        "subject": "graph_linearization",
                        "relation": "encodes_all_structural_properties",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_recover_original_graph",
                        "object": "True"
                    },
                    {
                        "subject": "LLM",
                        "relation": "learns_semantic_graph_patterns",
                        "object": "effectively"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Graph-to-sequence models that lose structural information during linearization perform worse on graph reconstruction and reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Semantic fidelity is critical for tasks requiring exact graph recovery or reasoning about graph isomorphism.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Semantic fidelity is a known requirement in graph encoding and isomorphism testing.",
                    "what_is_novel": "Its explicit application as a law for LLM-based graph linearization is novel.",
                    "classification_explanation": "While semantic fidelity is known in graph theory, its formalization as a law for LLM graph linearization is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Babai (2016) Graph Isomorphism in Quasipolynomial Time [Graph isomorphism and encoding]",
                        "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic fidelity in graph-to-sequence]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs trained on semantically faithful linearizations will outperform those trained on lossy encodings in graph reconstruction tasks.",
        "Loss of structural information in linearization will lead to degraded LLM performance on graph reasoning."
    ],
    "new_predictions_unknown": [
        "Semantic fidelity may enable LLMs to generalize to novel graph topologies not seen during training.",
        "Semantically faithful encodings could allow LLMs to perform zero-shot graph isomorphism detection."
    ],
    "negative_experiments": [
        "If LLMs trained on semantically faithful encodings do not outperform those trained on lossy encodings, the law would be challenged.",
        "If LLMs can recover original graphs from lossy encodings, the law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The trade-off between semantic fidelity and encoding length/efficiency is not fully addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks may tolerate partial loss of structural information if only local patterns are needed.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very large graphs may require compressed or approximate encodings.",
        "Graphs with redundant or irrelevant structure may not require full semantic fidelity."
    ],
    "existing_theory": {
        "what_already_exists": "Semantic fidelity is a known requirement in graph encoding and isomorphism testing.",
        "what_is_novel": "Its explicit formalization as a law for LLM-based graph linearization is new.",
        "classification_explanation": "The theory applies known semantic fidelity principles to LLM graph linearization in a novel, formalized way.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Babai (2016) Graph Isomorphism in Quasipolynomial Time [Graph isomorphism and encoding]",
            "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic fidelity in graph-to-sequence]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-615",
    "original_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
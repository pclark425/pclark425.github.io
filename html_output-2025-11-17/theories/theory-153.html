<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reconstruction-Free Contrastive Learning for Robustness to Visual Distractors - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-153</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-153</p>
                <p><strong>Name:</strong> Reconstruction-Free Contrastive Learning for Robustness to Visual Distractors</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about what constitutes an optimal world model for AI systems, incorporating fidelity, interpretability, computational efficiency, and task-specific utility, based on the following results.</p>
                <p><strong>Description:</strong> World models trained without pixel reconstruction objectives, using instead contrastive or predictive coding losses, achieve better robustness to visual distractors and task-irrelevant features in visually complex environments. This is because reconstruction forces the model to encode all visual information including task-irrelevant details (as evidenced by DreamerV3 reconstructing background details at the expense of small task elements), while contrastive objectives allow the model to focus on temporally consistent, task-relevant features. The optimal reconstruction-free approach combines: (1) multi-step temporal contrastive losses (K=3-5 steps) rather than single-step to capture dynamics, (2) separate low-capacity linear dynamics for contrastive prediction to enforce temporal smoothness while avoiding extreme latent jumps, (3) data augmentation (random crops) to encourage invariance to irrelevant visual variations, and (4) careful tuning of KL balancing parameters to prevent posterior collapse. However, this approach involves trade-offs: reconstruction-free models may require higher computational costs (e.g., two-view encoding in DreamerPro), can have lower performance on clean tasks where reconstruction provides useful auxiliary supervision, and require careful hyperparameter tuning. The performance advantage is most pronounced in environments with substantial visual distractors (>30% of visual field) where reconstruction-based models waste capacity on nuisance features.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Reconstruction-free models with contrastive objectives are more robust to visual distractors than reconstruction-based models, with performance advantages increasing as distractor prevalence increases</li>
                <li>Multi-step temporal contrastive losses (K=3-5) are superior to single-step losses for learning dynamics-aware representations that support planning</li>
                <li>Using separate low-capacity (linear) dynamics for contrastive prediction improves representation quality by enforcing temporal smoothness and avoiding extreme latent jumps that harm contrastive optimization</li>
                <li>Data augmentation (particularly random crops) is essential for contrastive world models to learn invariance to task-irrelevant visual variations</li>
                <li>The performance advantage of reconstruction-free models is most pronounced when visual distractors constitute >30% of the observation space</li>
                <li>Reconstruction-free models require careful tuning of KL balancing parameters and may need different hyperparameters than reconstruction-based models</li>
                <li>Reconstruction-free approaches can reduce computational cost (avoiding decoder backpropagation) but some implementations (e.g., two-view encoding) may increase cost</li>
                <li>In clean visual environments without distractors, reconstruction may provide useful auxiliary supervision that improves sample efficiency</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>MuDreamer achieves mean 739.6 on Visual Control Suite with natural backgrounds, outperforming DreamerV3 which reconstructs background details and loses small task elements <a href="../results/extraction-result-1244.html#e1244.0" class="evidence-link">[e1244.0]</a> <a href="../results/extraction-result-1244.html#e1244.1" class="evidence-link">[e1244.1]</a> </li>
    <li>DreamerV3 with reconstruction performs worse on natural-background tasks (object vanishing phenomenon) because it models background detail and neglects small critical task objects <a href="../results/extraction-result-1244.html#e1244.1" class="evidence-link">[e1244.1]</a> </li>
    <li>TPC (reconstruction-free with temporal predictive coding) achieves mean 372.8 on natural background tasks with lower FLOPs (~2.2B vs ~4.3B for DreamerV3) but still underperforms MuDreamer <a href="../results/extraction-result-1244.html#e1244.4" class="evidence-link">[e1244.4]</a> <a href="../results/extraction-result-1417.html#e1417.0" class="evidence-link">[e1417.0]</a> </li>
    <li>Dreaming uses independent linear forward dynamics for contrastive prediction, achieving substantially higher scores on manipulation tasks (Table II ablation shows replacing linear dynamics with shared expressive dynamics degrades performance) <a href="../results/extraction-result-1423.html#e1423.0" class="evidence-link">[e1423.0]</a> <a href="../results/extraction-result-1423.html#e1423.3" class="evidence-link">[e1423.3]</a> </li>
    <li>Linear tilde-dynamics in Dreaming reduces latent gaps that harm Info-NCE optimization, with ablation showing models using linear dynamics score substantially higher on manipulation tasks <a href="../results/extraction-result-1423.html#e1423.3" class="evidence-link">[e1423.3]</a> </li>
    <li>DreamerPro uses prototypical representations (SwAV-style) without reconstruction, achieving mean 445.2 on natural background tasks, but requires encoding two augmented views leading to ~7.9B FLOPs and higher memory usage <a href="../results/extraction-result-1244.html#e1244.3" class="evidence-link">[e1244.3]</a> <a href="../results/extraction-result-1395.html#e1395.5" class="evidence-link">[e1395.5]</a> </li>
    <li>Dreamer with reconstruction fails in natural-video background experiments where reconstruction forces encoding of distractor pixels, leading to failure to learn meaningful behaviors <a href="../results/extraction-result-1417.html#e1417.1" class="evidence-link">[e1417.1]</a> </li>
    <li>CVRL contrastive approach outperforms generative Dreamer on Natural MuJoCo: walker-walk 941.5 vs 206.6, demonstrating large performance gap when visual complexity is high <a href="../results/extraction-result-1386.html#e1386.2" class="evidence-link">[e1386.2]</a> </li>
    <li>MuDreamer uses multi-view contrastive learning with random crops and KL balancing, showing that reconstruction-free with proper design choices yields better robustness <a href="../results/extraction-result-1244.html#e1244.0" class="evidence-link">[e1244.0]</a> </li>
    <li>Dreaming's multi-step NCE (K=3) combined with linear tilde-dynamics and random-crop augmentation found to be best configuration for difficult manipulation tasks <a href="../results/extraction-result-1423.html#e1423.3" class="evidence-link">[e1423.3]</a> </li>
    <li>CVRL uses contrastive ELBO that replaces reconstruction log-likelihood with contrastive lower bound, enabling focus on task-relevant features in presence of complex observations <a href="../results/extraction-result-1386.html#e1386.2" class="evidence-link">[e1386.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In environments with 50%+ visual distractors, reconstruction-free contrastive models will outperform reconstruction-based models by 30-50% in final task performance</li>
                <li>Multi-step contrastive losses (K=5) will achieve 15-25% better sample efficiency than single-step contrastive losses on tasks requiring temporal reasoning</li>
                <li>Reconstruction-free models will generalize better to novel visual backgrounds (e.g., 40%+ higher success rate) than reconstruction-based models when tested on out-of-distribution visual contexts</li>
                <li>Using linear dynamics for contrastive prediction will yield 20-30% better performance than using the same expressive dynamics for both contrastive and planning objectives in visually complex tasks</li>
                <li>Combining random crop augmentation with multi-step contrastive losses will show synergistic effects, outperforming either technique alone by 15-20%</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether reconstruction-free models can match or exceed reconstruction-based models on tasks requiring fine-grained visual discrimination (e.g., reading small text, detecting subtle visual differences)</li>
                <li>If there exists an optimal contrastive horizon K that generalizes across all domains, or if K must be tuned per task complexity</li>
                <li>Whether reconstruction-free models learn more transferable representations that enable better zero-shot or few-shot transfer to new tasks</li>
                <li>If hybrid approaches that use reconstruction as a weighted auxiliary loss (e.g., 0.1-0.3 weight) can achieve best of both worlds, combining robustness with fine-grained visual understanding</li>
                <li>Whether the performance gap between reconstruction-free and reconstruction-based models continues to grow linearly with distractor complexity or plateaus at some threshold</li>
                <li>If reconstruction-free models are more or less susceptible to adversarial visual perturbations compared to reconstruction-based models</li>
                <li>Whether the computational savings from avoiding reconstruction can offset the cost of multi-view encoding or other architectural requirements of contrastive methods</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where reconstruction-based models consistently outperform contrastive models even with 50%+ visual distractors would challenge the core robustness claim</li>
                <li>Demonstrating that single-step contrastive losses achieve the same performance as multi-step losses (K=3-5) would question the necessity of temporal depth in contrastive objectives</li>
                <li>Showing that using expressive shared dynamics for contrastive prediction performs as well as or better than separate linear dynamics would challenge the smoothness enforcement mechanism</li>
                <li>Finding that reconstruction-free models fail catastrophically on tasks requiring visual details (e.g., <50% success rate vs >90% for reconstruction-based) would severely limit the theory's applicability</li>
                <li>Demonstrating that the performance advantage disappears when controlling for total compute budget (allowing reconstruction-based models more capacity) would suggest the advantage is merely computational rather than fundamental</li>
                <li>Showing that reconstruction-free models have worse sample efficiency on clean tasks (e.g., 2x more samples needed) would challenge claims about general superiority</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to determine optimal contrastive horizon K without task-specific tuning or validation </li>
    <li>The precise relationship between negative sampling strategy and representation quality in contrastive world models </li>
    <li>Whether contrastive objectives can be effectively combined with other self-supervised objectives (e.g., forward-inverse consistency, curiosity-driven objectives) </li>
    <li>The interaction between KL balancing parameters and contrastive loss weights, and how to tune them jointly </li>
    <li>Why TPC (reconstruction-free) underperforms MuDreamer despite both being reconstruction-free, suggesting other factors beyond reconstruction matter <a href="../results/extraction-result-1244.html#e1244.4" class="evidence-link">[e1244.4]</a> </li>
    <li>The role of multi-view encoding architecture choices (e.g., shared vs separate encoders, fusion mechanisms) in determining final performance </li>
    <li>How the theory applies to domains beyond visual control (e.g., language, audio, multimodal settings) </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Oord et al. (2018) Representation Learning with Contrastive Predictive Coding [CPC - foundational work on contrastive predictive coding for temporal sequences]</li>
    <li>Chen et al. (2020) A Simple Framework for Contrastive Learning of Visual Representations [SimCLR - contrastive learning with data augmentation]</li>
    <li>Schwarzer et al. (2021) Data-Efficient Reinforcement Learning with Self-Predictive Representations [SPR - temporal contrastive learning for RL]</li>
    <li>Stooke et al. (2021) Decoupling Representation Learning from Reinforcement Learning [CURL - contrastive learning with random crops for RL]</li>
    <li>Hafner et al. (2023) Mastering Diverse Domains through World Models [DreamerV3 - reconstruction-based baseline showing strong performance]</li>
    <li>Mendonca et al. (2021) Discovering and Achieving Goals via World Models [Dreaming - reconstruction-free with contrastive objectives]</li>
    <li>Seo et al. (2022) Reinforcement Learning with Action-Free Pre-Training from Videos [Temporal predictive coding for model-based RL]</li>
    <li>Ma et al. (2020) Contrastive Variational Model-Based Reinforcement Learning for Complex Observations [CVRL - contrastive ELBO for complex observations]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Reconstruction-Free Contrastive Learning for Robustness to Visual Distractors",
    "theory_description": "World models trained without pixel reconstruction objectives, using instead contrastive or predictive coding losses, achieve better robustness to visual distractors and task-irrelevant features in visually complex environments. This is because reconstruction forces the model to encode all visual information including task-irrelevant details (as evidenced by DreamerV3 reconstructing background details at the expense of small task elements), while contrastive objectives allow the model to focus on temporally consistent, task-relevant features. The optimal reconstruction-free approach combines: (1) multi-step temporal contrastive losses (K=3-5 steps) rather than single-step to capture dynamics, (2) separate low-capacity linear dynamics for contrastive prediction to enforce temporal smoothness while avoiding extreme latent jumps, (3) data augmentation (random crops) to encourage invariance to irrelevant visual variations, and (4) careful tuning of KL balancing parameters to prevent posterior collapse. However, this approach involves trade-offs: reconstruction-free models may require higher computational costs (e.g., two-view encoding in DreamerPro), can have lower performance on clean tasks where reconstruction provides useful auxiliary supervision, and require careful hyperparameter tuning. The performance advantage is most pronounced in environments with substantial visual distractors (&gt;30% of visual field) where reconstruction-based models waste capacity on nuisance features.",
    "supporting_evidence": [
        {
            "text": "MuDreamer achieves mean 739.6 on Visual Control Suite with natural backgrounds, outperforming DreamerV3 which reconstructs background details and loses small task elements",
            "uuids": [
                "e1244.0",
                "e1244.1"
            ]
        },
        {
            "text": "DreamerV3 with reconstruction performs worse on natural-background tasks (object vanishing phenomenon) because it models background detail and neglects small critical task objects",
            "uuids": [
                "e1244.1"
            ]
        },
        {
            "text": "TPC (reconstruction-free with temporal predictive coding) achieves mean 372.8 on natural background tasks with lower FLOPs (~2.2B vs ~4.3B for DreamerV3) but still underperforms MuDreamer",
            "uuids": [
                "e1244.4",
                "e1417.0"
            ]
        },
        {
            "text": "Dreaming uses independent linear forward dynamics for contrastive prediction, achieving substantially higher scores on manipulation tasks (Table II ablation shows replacing linear dynamics with shared expressive dynamics degrades performance)",
            "uuids": [
                "e1423.0",
                "e1423.3"
            ]
        },
        {
            "text": "Linear tilde-dynamics in Dreaming reduces latent gaps that harm Info-NCE optimization, with ablation showing models using linear dynamics score substantially higher on manipulation tasks",
            "uuids": [
                "e1423.3"
            ]
        },
        {
            "text": "DreamerPro uses prototypical representations (SwAV-style) without reconstruction, achieving mean 445.2 on natural background tasks, but requires encoding two augmented views leading to ~7.9B FLOPs and higher memory usage",
            "uuids": [
                "e1244.3",
                "e1395.5"
            ]
        },
        {
            "text": "Dreamer with reconstruction fails in natural-video background experiments where reconstruction forces encoding of distractor pixels, leading to failure to learn meaningful behaviors",
            "uuids": [
                "e1417.1"
            ]
        },
        {
            "text": "CVRL contrastive approach outperforms generative Dreamer on Natural MuJoCo: walker-walk 941.5 vs 206.6, demonstrating large performance gap when visual complexity is high",
            "uuids": [
                "e1386.2"
            ]
        },
        {
            "text": "MuDreamer uses multi-view contrastive learning with random crops and KL balancing, showing that reconstruction-free with proper design choices yields better robustness",
            "uuids": [
                "e1244.0"
            ]
        },
        {
            "text": "Dreaming's multi-step NCE (K=3) combined with linear tilde-dynamics and random-crop augmentation found to be best configuration for difficult manipulation tasks",
            "uuids": [
                "e1423.3"
            ]
        },
        {
            "text": "CVRL uses contrastive ELBO that replaces reconstruction log-likelihood with contrastive lower bound, enabling focus on task-relevant features in presence of complex observations",
            "uuids": [
                "e1386.2"
            ]
        }
    ],
    "theory_statements": [
        "Reconstruction-free models with contrastive objectives are more robust to visual distractors than reconstruction-based models, with performance advantages increasing as distractor prevalence increases",
        "Multi-step temporal contrastive losses (K=3-5) are superior to single-step losses for learning dynamics-aware representations that support planning",
        "Using separate low-capacity (linear) dynamics for contrastive prediction improves representation quality by enforcing temporal smoothness and avoiding extreme latent jumps that harm contrastive optimization",
        "Data augmentation (particularly random crops) is essential for contrastive world models to learn invariance to task-irrelevant visual variations",
        "The performance advantage of reconstruction-free models is most pronounced when visual distractors constitute &gt;30% of the observation space",
        "Reconstruction-free models require careful tuning of KL balancing parameters and may need different hyperparameters than reconstruction-based models",
        "Reconstruction-free approaches can reduce computational cost (avoiding decoder backpropagation) but some implementations (e.g., two-view encoding) may increase cost",
        "In clean visual environments without distractors, reconstruction may provide useful auxiliary supervision that improves sample efficiency"
    ],
    "new_predictions_likely": [
        "In environments with 50%+ visual distractors, reconstruction-free contrastive models will outperform reconstruction-based models by 30-50% in final task performance",
        "Multi-step contrastive losses (K=5) will achieve 15-25% better sample efficiency than single-step contrastive losses on tasks requiring temporal reasoning",
        "Reconstruction-free models will generalize better to novel visual backgrounds (e.g., 40%+ higher success rate) than reconstruction-based models when tested on out-of-distribution visual contexts",
        "Using linear dynamics for contrastive prediction will yield 20-30% better performance than using the same expressive dynamics for both contrastive and planning objectives in visually complex tasks",
        "Combining random crop augmentation with multi-step contrastive losses will show synergistic effects, outperforming either technique alone by 15-20%"
    ],
    "new_predictions_unknown": [
        "Whether reconstruction-free models can match or exceed reconstruction-based models on tasks requiring fine-grained visual discrimination (e.g., reading small text, detecting subtle visual differences)",
        "If there exists an optimal contrastive horizon K that generalizes across all domains, or if K must be tuned per task complexity",
        "Whether reconstruction-free models learn more transferable representations that enable better zero-shot or few-shot transfer to new tasks",
        "If hybrid approaches that use reconstruction as a weighted auxiliary loss (e.g., 0.1-0.3 weight) can achieve best of both worlds, combining robustness with fine-grained visual understanding",
        "Whether the performance gap between reconstruction-free and reconstruction-based models continues to grow linearly with distractor complexity or plateaus at some threshold",
        "If reconstruction-free models are more or less susceptible to adversarial visual perturbations compared to reconstruction-based models",
        "Whether the computational savings from avoiding reconstruction can offset the cost of multi-view encoding or other architectural requirements of contrastive methods"
    ],
    "negative_experiments": [
        "Finding tasks where reconstruction-based models consistently outperform contrastive models even with 50%+ visual distractors would challenge the core robustness claim",
        "Demonstrating that single-step contrastive losses achieve the same performance as multi-step losses (K=3-5) would question the necessity of temporal depth in contrastive objectives",
        "Showing that using expressive shared dynamics for contrastive prediction performs as well as or better than separate linear dynamics would challenge the smoothness enforcement mechanism",
        "Finding that reconstruction-free models fail catastrophically on tasks requiring visual details (e.g., &lt;50% success rate vs &gt;90% for reconstruction-based) would severely limit the theory's applicability",
        "Demonstrating that the performance advantage disappears when controlling for total compute budget (allowing reconstruction-based models more capacity) would suggest the advantage is merely computational rather than fundamental",
        "Showing that reconstruction-free models have worse sample efficiency on clean tasks (e.g., 2x more samples needed) would challenge claims about general superiority"
    ],
    "unaccounted_for": [
        {
            "text": "How to determine optimal contrastive horizon K without task-specific tuning or validation",
            "uuids": []
        },
        {
            "text": "The precise relationship between negative sampling strategy and representation quality in contrastive world models",
            "uuids": []
        },
        {
            "text": "Whether contrastive objectives can be effectively combined with other self-supervised objectives (e.g., forward-inverse consistency, curiosity-driven objectives)",
            "uuids": []
        },
        {
            "text": "The interaction between KL balancing parameters and contrastive loss weights, and how to tune them jointly",
            "uuids": []
        },
        {
            "text": "Why TPC (reconstruction-free) underperforms MuDreamer despite both being reconstruction-free, suggesting other factors beyond reconstruction matter",
            "uuids": [
                "e1244.4"
            ]
        },
        {
            "text": "The role of multi-view encoding architecture choices (e.g., shared vs separate encoders, fusion mechanisms) in determining final performance",
            "uuids": []
        },
        {
            "text": "How the theory applies to domains beyond visual control (e.g., language, audio, multimodal settings)",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "DreamerV3 with reconstruction achieves strong performance (mean 739.6, median 808.5) on standard Visual Control Suite without distractors, suggesting reconstruction isn't always harmful and may provide useful supervision",
            "uuids": [
                "e1244.1"
            ]
        },
        {
            "text": "TPC (reconstruction-free) achieves lower task performance (mean 372.8) than both MuDreamer (739.6) and DreamerPro (445.2) on natural background tasks, indicating reconstruction-free alone is insufficient",
            "uuids": [
                "e1244.4"
            ]
        },
        {
            "text": "DreamerPro requires two-view encoding leading to ~3x FLOPs compared to MuDreamer and possible OOM for larger images, showing reconstruction-free can increase rather than decrease computational cost",
            "uuids": [
                "e1244.3"
            ]
        },
        {
            "text": "Some reconstruction-free methods (TPC) are faster but achieve noticeably lower task performance, suggesting a fundamental trade-off between efficiency and effectiveness",
            "uuids": [
                "e1244.4"
            ]
        },
        {
            "text": "MuDreamer's superior performance may be due to multi-view encoding and KL balancing rather than purely the absence of reconstruction, confounding the causal attribution",
            "uuids": [
                "e1244.0"
            ]
        }
    ],
    "special_cases": [
        "In clean visual environments without distractors (standard DMC), reconstruction may provide useful auxiliary supervision that improves sample efficiency and final performance, making reconstruction-based models preferable",
        "For tasks requiring fine-grained visual discrimination or precise object localization, reconstruction objectives may be necessary to ensure sufficient visual detail is encoded",
        "When computational budget is severely limited, simpler reconstruction-free methods (like TPC) may be preferred despite lower performance, as they require fewer FLOPs",
        "When using two-view encoding for contrastive learning, the computational cost may exceed reconstruction-based methods, reversing the efficiency advantage",
        "In domains with very high-dimensional observations (e.g., high-resolution images), the memory requirements of multi-view encoding may make reconstruction-free approaches infeasible",
        "For transfer learning scenarios where pre-trained visual features are available, reconstruction-free methods may not provide additional benefits",
        "In partially observable environments, reconstruction may help maintain information about unobserved state, potentially making it beneficial even with distractors",
        "When the task requires modeling stochastic visual dynamics (e.g., particle effects, fluid simulation), reconstruction may be necessary to capture the full distribution of possible observations"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Oord et al. (2018) Representation Learning with Contrastive Predictive Coding [CPC - foundational work on contrastive predictive coding for temporal sequences]",
            "Chen et al. (2020) A Simple Framework for Contrastive Learning of Visual Representations [SimCLR - contrastive learning with data augmentation]",
            "Schwarzer et al. (2021) Data-Efficient Reinforcement Learning with Self-Predictive Representations [SPR - temporal contrastive learning for RL]",
            "Stooke et al. (2021) Decoupling Representation Learning from Reinforcement Learning [CURL - contrastive learning with random crops for RL]",
            "Hafner et al. (2023) Mastering Diverse Domains through World Models [DreamerV3 - reconstruction-based baseline showing strong performance]",
            "Mendonca et al. (2021) Discovering and Achieving Goals via World Models [Dreaming - reconstruction-free with contrastive objectives]",
            "Seo et al. (2022) Reinforcement Learning with Action-Free Pre-Training from Videos [Temporal predictive coding for model-based RL]",
            "Ma et al. (2020) Contrastive Variational Model-Based Reinforcement Learning for Complex Observations [CVRL - contrastive ELBO for complex observations]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 6,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
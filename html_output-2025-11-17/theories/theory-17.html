<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model Size and Training Data Diversity Drive First-Order ToM Performance - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-17</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-17</p>
                <p><strong>Name:</strong> Model Size and Training Data Diversity Drive First-Order ToM Performance</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> </p>
                <p><strong>Description:</strong> The ability of LLMs to perform first-order theory-of-mind tasks is positively correlated with model size and the diversity and richness of their training data. Larger models trained on more varied datasets can better capture linguistic patterns related to mental state reasoning, enabling above-child-level performance on first-order ToM tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2023</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <ol>
                <li><a href="../evaluations/theory-evaluation-3.html">theory-evaluation-3</a></li>
                <li><a href="../evaluations/theory-evaluation-4.html">theory-evaluation-4</a></li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Model size positively correlates with first-order ToM task performance.</li>
                <li>Diverse and rich training data enhance the model's ability to generalize ToM reasoning.</li>
                <li>Smaller models perform near chance on first-order ToM tasks.</li>
                <li>First-order ToM tasks are more tractable for LLMs than higher-order tasks.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>BLOOM (176B) and LLaMa-33B perform above child level on first-order ToM tasks, with larger models outperforming smaller ones. <a href="../results/extraction-result-83.html#e83.2" class="evidence-link">[e83.2]</a> <a href="../results/extraction-result-75.html#e75.0" class="evidence-link">[e75.0]</a> <a href="../results/extraction-result-75.html#e75.1" class="evidence-link">[e75.1]</a> </li>
    <li>GPT-3 (175B) shows improved first-order ToM performance compared to smaller models, achieving ~74.5% accuracy. <a href="../results/extraction-result-72.html#e72.0" class="evidence-link">[e72.0]</a> <a href="../results/extraction-result-74.html#e74.0" class="evidence-link">[e74.0]</a> </li>
    <li>Training data diversity influences generalization to ToM tasks, with models trained on multilingual and diverse corpora performing better. <a href="../results/extraction-result-83.html#e83.2" class="evidence-link">[e83.2]</a> <a href="../results/extraction-result-86.html#e86.0" class="evidence-link">[e86.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Increasing model size beyond current scales will yield diminishing but positive returns on first-order ToM tasks.</li>
                <li>Training on more diverse and socially rich datasets will improve first-order ToM performance.</li>
                <li>Models smaller than 10B parameters will continue to perform poorly on first-order ToM tasks.</li>
                <li>Multilingual training data will enhance ToM performance across languages.</li>
                <li>Fine-tuning on ToM-specific datasets will further improve first-order ToM accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there is a model size threshold beyond which first-order ToM performance saturates is unknown.</li>
                <li>The impact of training data quality versus quantity on ToM performance is unclear.</li>
                <li>Whether first-order ToM performance can be achieved without instruction tuning remains to be seen.</li>
                <li>The role of pretraining on social narratives in ToM development is uncertain.</li>
                <li>Whether first-order ToM performance predicts higher-order ToM capabilities is unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If smaller models outperform larger models on first-order ToM tasks, the size-performance correlation would be challenged.</li>
                <li>If training on more diverse data does not improve ToM performance, the role of data diversity would be questioned.</li>
                <li>If fine-tuning on ToM datasets fails to improve performance, the importance of targeted data would be undermined.</li>
                <li>If monolingual models outperform multilingual ones on ToM tasks, the impact of multilingual data would be doubted.</li>
                <li>If first-order ToM performance does not improve with scale, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some models with similar sizes show variable ToM performance, suggesting other factors beyond size and data influence outcomes. <a href="../results/extraction-result-74.html#e74.0" class="evidence-link">[e74.0]</a> <a href="../results/extraction-result-83.html#e83.1" class="evidence-link">[e83.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Model Size and Training Data Diversity Drive First-Order ToM Performance",
    "theory_description": "The ability of LLMs to perform first-order theory-of-mind tasks is positively correlated with model size and the diversity and richness of their training data. Larger models trained on more varied datasets can better capture linguistic patterns related to mental state reasoning, enabling above-child-level performance on first-order ToM tasks.",
    "supporting_evidence": [
        {
            "text": "BLOOM (176B) and LLaMa-33B perform above child level on first-order ToM tasks, with larger models outperforming smaller ones.",
            "uuids": [
                "e83.2",
                "e75.0",
                "e75.1"
            ]
        },
        {
            "text": "GPT-3 (175B) shows improved first-order ToM performance compared to smaller models, achieving ~74.5% accuracy.",
            "uuids": [
                "e72.0",
                "e74.0"
            ]
        },
        {
            "text": "Training data diversity influences generalization to ToM tasks, with models trained on multilingual and diverse corpora performing better.",
            "uuids": [
                "e83.2",
                "e86.0"
            ]
        }
    ],
    "theory_statements": [
        "Model size positively correlates with first-order ToM task performance.",
        "Diverse and rich training data enhance the model's ability to generalize ToM reasoning.",
        "Smaller models perform near chance on first-order ToM tasks.",
        "First-order ToM tasks are more tractable for LLMs than higher-order tasks."
    ],
    "new_predictions_likely": [
        "Increasing model size beyond current scales will yield diminishing but positive returns on first-order ToM tasks.",
        "Training on more diverse and socially rich datasets will improve first-order ToM performance.",
        "Models smaller than 10B parameters will continue to perform poorly on first-order ToM tasks.",
        "Multilingual training data will enhance ToM performance across languages.",
        "Fine-tuning on ToM-specific datasets will further improve first-order ToM accuracy."
    ],
    "new_predictions_unknown": [
        "Whether there is a model size threshold beyond which first-order ToM performance saturates is unknown.",
        "The impact of training data quality versus quantity on ToM performance is unclear.",
        "Whether first-order ToM performance can be achieved without instruction tuning remains to be seen.",
        "The role of pretraining on social narratives in ToM development is uncertain.",
        "Whether first-order ToM performance predicts higher-order ToM capabilities is unknown."
    ],
    "negative_experiments": [
        "If smaller models outperform larger models on first-order ToM tasks, the size-performance correlation would be challenged.",
        "If training on more diverse data does not improve ToM performance, the role of data diversity would be questioned.",
        "If fine-tuning on ToM datasets fails to improve performance, the importance of targeted data would be undermined.",
        "If monolingual models outperform multilingual ones on ToM tasks, the impact of multilingual data would be doubted.",
        "If first-order ToM performance does not improve with scale, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Some models with similar sizes show variable ToM performance, suggesting other factors beyond size and data influence outcomes.",
            "uuids": [
                "e74.0",
                "e83.1"
            ]
        }
    ],
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
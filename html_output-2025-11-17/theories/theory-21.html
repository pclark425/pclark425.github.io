<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model Size, Training Data Quality, and Fine-Tuning Drive First-Order ToM Performance in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-21</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-21</p>
                <p><strong>Name:</strong> Model Size, Training Data Quality, and Fine-Tuning Drive First-Order ToM Performance in LLMs</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> </p>
                <p><strong>Description:</strong> The ability of large language models (LLMs) to perform first-order theory-of-mind (ToM) tasks is influenced by a combination of model size, the quality and diversity of training data, and targeted fine-tuning or instruction tuning on ToM-specific or socially rich datasets. While larger models generally show improved ToM performance, diminishing returns occur at very large scales, making fine-tuning and data quality critical factors. Diverse training data, especially rich in social narratives and multilingual content, enhances ToM reasoning capabilities. However, strong performance on benchmarks does not necessarily imply genuine understanding or functional ToM capabilities, as models may rely on heuristics and show variability due to factors beyond size and data, such as architecture, fine-tuning methods, and evaluation design. Current ToM benchmarks may overestimate LLMs' true ToM abilities due to oversimplified tasks and lack of adversarial robustness. Smaller models can improve ToM performance substantially through fine-tuning and novel inference-time methods, partially mitigating size limitations.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-17.html">[theory-17]</a></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Model size positively correlates with first-order ToM task performance, but with diminishing returns at very large scales.</li>
                <li>The quality, complexity, and social narrative richness of training data are more influential than sheer quantity or multilinguality alone.</li>
                <li>Fine-tuning and instruction tuning on ToM-specific or socially rich datasets critically enhance ToM reasoning capabilities.</li>
                <li>Smaller models can improve ToM performance substantially through fine-tuning and inference-time methods, partially mitigating size limitations.</li>
                <li>Variability in ToM performance among models of similar size indicates that factors beyond size and training data, such as architecture, fine-tuning methods, and evaluation design, influence outcomes.</li>
                <li>Strong performance on ToM benchmarks does not guarantee genuine understanding or functional ToM capabilities; models may rely on heuristics and show brittleness to task perturbations.</li>
                <li>Current ToM benchmarks may overestimate LLMs' true ToM abilities due to oversimplified tasks and lack of adversarial robustness.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Multiple large models such as GPT-4, LLaMA2-70B, and Qwen 7B demonstrate improved first-order ToM performance correlating positively with model size and training data diversity, achieving accuracies near or above child-level benchmarks. <a href="../results/extraction-result-92.html#e92.0" class="evidence-link">[e92.0]</a> <a href="../results/extraction-result-96.html#e96.0" class="evidence-link">[e96.0]</a> <a href="../results/extraction-result-93.html#e93.0" class="evidence-link">[e93.0]</a> <a href="../results/extraction-result-103.html#e103.0" class="evidence-link">[e103.0]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-109.html#e109.1" class="evidence-link">[e109.1]</a> <a href="../results/extraction-result-115.html#e115.0" class="evidence-link">[e115.0]</a> <a href="../results/extraction-result-115.html#e115.1" class="evidence-link">[e115.1]</a> </li>
    <li>Fine-tuning and instruction tuning on ToM-specific or socially rich datasets significantly enhance first-order ToM task performance, as seen in models like Flan-PaLM, Mistral-7B, and GPT-4 variants. <a href="../results/extraction-result-100.html#e100.1" class="evidence-link">[e100.1]</a> <a href="../results/extraction-result-104.html#e104.0" class="evidence-link">[e104.0]</a> <a href="../results/extraction-result-112.html#e112.0" class="evidence-link">[e112.0]</a> <a href="../results/extraction-result-108.html#e108.0" class="evidence-link">[e108.0]</a> <a href="../results/extraction-result-106.html#e106.0" class="evidence-link">[e106.0]</a> <a href="../results/extraction-result-106.html#e106.1" class="evidence-link">[e106.1]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-96.html#e96.1" class="evidence-link">[e96.1]</a> </li>
    <li>Training data diversity, especially inclusion of multilingual and social narrative content, positively impacts ToM reasoning capabilities, supported by evidence from models like GPT-4, LLaMA-2, and DeepSeek R1. <a href="../results/extraction-result-91.html#e91.0" class="evidence-link">[e91.0]</a> <a href="../results/extraction-result-98.html#e98.1" class="evidence-link">[e98.1]</a> <a href="../results/extraction-result-103.html#e103.0" class="evidence-link">[e103.0]</a> <a href="../results/extraction-result-109.html#e109.1" class="evidence-link">[e109.1]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-96.html#e96.0" class="evidence-link">[e96.0]</a> </li>
    <li>Smaller models (below ~10B parameters) generally perform poorly on first-order ToM tasks, but fine-tuning and novel inference-time methods can partially improve their performance. <a href="../results/extraction-result-108.html#e108.1" class="evidence-link">[e108.1]</a> <a href="../results/extraction-result-107.html#e107.0" class="evidence-link">[e107.0]</a> <a href="../results/extraction-result-96.html#e96.1" class="evidence-link">[e96.1]</a> <a href="../results/extraction-result-101.html#e101.0" class="evidence-link">[e101.0]</a> <a href="../results/extraction-result-98.html#e98.0" class="evidence-link">[e98.0]</a> <a href="../results/extraction-result-95.html#e95.0" class="evidence-link">[e95.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Fine-tuning on high-quality, socially rich, and diverse ToM datasets will yield larger performance gains than increasing model size alone beyond current scales.</li>
                <li>Models trained on complex social narratives and diverse cultural contexts will generalize better to novel ToM tasks and languages.</li>
                <li>Inference-time methods that encourage perspective-taking (e.g., Shoes-of-Others prefixing) will improve ToM performance, especially in smaller models.</li>
                <li>Diminishing returns in ToM performance will be observed as model size increases beyond hundreds of billions of parameters without corresponding improvements in data quality or fine-tuning.</li>
                <li>Models with similar sizes but different fine-tuning strategies or architectures will show significant variability in ToM task performance.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a precise model size threshold beyond which ToM performance saturates, regardless of data or fine-tuning improvements.</li>
                <li>The extent to which functional ToM capabilities (e.g., adaptive social reasoning in interactive settings) can be achieved by LLMs through fine-tuning and architectural innovations.</li>
                <li>How the balance between training data quality and quantity affects ToM performance across different languages and cultural contexts.</li>
                <li>Whether multimodal training (e.g., integrating vision and language) can substantially enhance ToM reasoning beyond text-only models.</li>
                <li>The relationship between first-order ToM performance and higher-order ToM capabilities in LLMs, and whether improvements in one predict improvements in the other.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If larger models consistently fail to outperform smaller models on first-order ToM tasks despite similar training data and fine-tuning, the size-performance correlation would be challenged.</li>
                <li>If fine-tuning on ToM-specific datasets does not improve or degrades ToM task performance, the importance of targeted fine-tuning would be questioned.</li>
                <li>If models trained on diverse and socially rich datasets do not show better ToM generalization than those trained on less diverse data, the role of data diversity and quality would be undermined.</li>
                <li>If models with strong benchmark performance fail to generalize to adversarial or out-of-distribution ToM tasks, the validity of current benchmarks would be questioned.</li>
                <li>If smaller models cannot improve ToM performance through fine-tuning or inference-time methods, the mitigation of size limitations would be disproven.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some large models of similar or greater size (e.g., GPT-4o, Vicuna-33B, Llama-3.1) perform poorly or inconsistently on first-order ToM tasks, suggesting that size and data diversity alone do not fully explain ToM performance variability. <a href="../results/extraction-result-99.html#e99.1" class="evidence-link">[e99.1]</a> <a href="../results/extraction-result-114.html#e114.1" class="evidence-link">[e114.1]</a> <a href="../results/extraction-result-99.html#e99.0" class="evidence-link">[e99.0]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-114.html#e114.2" class="evidence-link">[e114.2]</a> <a href="../results/extraction-result-114.html#e114.1" class="evidence-link">[e114.1]</a> </li>
    <li>LLMs often rely on shallow heuristics or spurious correlations rather than genuine understanding of mental states, as evidenced by performance drops with minor task perturbations. <a href="../results/extraction-result-109.html#e109.0" class="evidence-link">[e109.0]</a> <a href="../results/extraction-result-104.html#e104.0" class="evidence-link">[e104.0]</a> <a href="../results/extraction-result-105.html#e105.0" class="evidence-link">[e105.0]</a> <a href="../results/extraction-result-109.html#e109.1" class="evidence-link">[e109.1]</a> <a href="../results/extraction-result-113.html#e113.0" class="evidence-link">[e113.0]</a> </li>
    <li>Variability in ToM performance among models of similar size suggests that factors such as model architecture, fine-tuning methods, and task design significantly influence outcomes. <a href="../results/extraction-result-74.html#e74.0" class="evidence-link">[e74.0]</a> <a href="../results/extraction-result-83.html#e83.1" class="evidence-link">[e83.1]</a> <a href="../results/extraction-result-111.html#e111.0" class="evidence-link">[e111.0]</a> <a href="../results/extraction-result-113.html#e113.0" class="evidence-link">[e113.0]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-115.html#e115.0" class="evidence-link">[e115.0]</a> <a href="../results/extraction-result-115.html#e115.1" class="evidence-link">[e115.1]</a> </li>
    <li>Some benchmarks and datasets may overestimate model ToM capabilities due to oversimplified tasks or lack of adversarial examples, indicating that current evaluations might not fully capture genuine ToM reasoning. <a href="../results/extraction-result-99.html#e99.0" class="evidence-link">[e99.0]</a> <a href="../results/extraction-result-99.html#e99.1" class="evidence-link">[e99.1]</a> <a href="../results/extraction-result-93.html#e93.0" class="evidence-link">[e93.0]</a> <a href="../results/extraction-result-91.html#e91.0" class="evidence-link">[e91.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Model Size, Training Data Quality, and Fine-Tuning Drive First-Order ToM Performance in LLMs",
    "type": "specific",
    "theory_description": "The ability of large language models (LLMs) to perform first-order theory-of-mind (ToM) tasks is influenced by a combination of model size, the quality and diversity of training data, and targeted fine-tuning or instruction tuning on ToM-specific or socially rich datasets. While larger models generally show improved ToM performance, diminishing returns occur at very large scales, making fine-tuning and data quality critical factors. Diverse training data, especially rich in social narratives and multilingual content, enhances ToM reasoning capabilities. However, strong performance on benchmarks does not necessarily imply genuine understanding or functional ToM capabilities, as models may rely on heuristics and show variability due to factors beyond size and data, such as architecture, fine-tuning methods, and evaluation design. Current ToM benchmarks may overestimate LLMs' true ToM abilities due to oversimplified tasks and lack of adversarial robustness. Smaller models can improve ToM performance substantially through fine-tuning and novel inference-time methods, partially mitigating size limitations.",
    "supporting_evidence": [
        {
            "text": "Multiple large models such as GPT-4, LLaMA2-70B, and Qwen 7B demonstrate improved first-order ToM performance correlating positively with model size and training data diversity, achieving accuracies near or above child-level benchmarks.",
            "uuids": [
                "e92.0",
                "e96.0",
                "e93.0",
                "e103.0",
                "e114.0",
                "e109.1",
                "e115.0",
                "e115.1"
            ]
        },
        {
            "text": "Fine-tuning and instruction tuning on ToM-specific or socially rich datasets significantly enhance first-order ToM task performance, as seen in models like Flan-PaLM, Mistral-7B, and GPT-4 variants.",
            "uuids": [
                "e100.1",
                "e104.0",
                "e112.0",
                "e108.0",
                "e106.0",
                "e106.1",
                "e114.0",
                "e96.1"
            ]
        },
        {
            "text": "Training data diversity, especially inclusion of multilingual and social narrative content, positively impacts ToM reasoning capabilities, supported by evidence from models like GPT-4, LLaMA-2, and DeepSeek R1.",
            "uuids": [
                "e91.0",
                "e98.1",
                "e103.0",
                "e109.1",
                "e114.0",
                "e96.0"
            ]
        },
        {
            "text": "Smaller models (below ~10B parameters) generally perform poorly on first-order ToM tasks, but fine-tuning and novel inference-time methods can partially improve their performance.",
            "uuids": [
                "e108.1",
                "e107.0",
                "e96.1",
                "e101.0",
                "e98.0",
                "e95.0"
            ]
        }
    ],
    "theory_statements": [
        "Model size positively correlates with first-order ToM task performance, but with diminishing returns at very large scales.",
        "The quality, complexity, and social narrative richness of training data are more influential than sheer quantity or multilinguality alone.",
        "Fine-tuning and instruction tuning on ToM-specific or socially rich datasets critically enhance ToM reasoning capabilities.",
        "Smaller models can improve ToM performance substantially through fine-tuning and inference-time methods, partially mitigating size limitations.",
        "Variability in ToM performance among models of similar size indicates that factors beyond size and training data, such as architecture, fine-tuning methods, and evaluation design, influence outcomes.",
        "Strong performance on ToM benchmarks does not guarantee genuine understanding or functional ToM capabilities; models may rely on heuristics and show brittleness to task perturbations.",
        "Current ToM benchmarks may overestimate LLMs' true ToM abilities due to oversimplified tasks and lack of adversarial robustness."
    ],
    "new_predictions_likely": [
        "Fine-tuning on high-quality, socially rich, and diverse ToM datasets will yield larger performance gains than increasing model size alone beyond current scales.",
        "Models trained on complex social narratives and diverse cultural contexts will generalize better to novel ToM tasks and languages.",
        "Inference-time methods that encourage perspective-taking (e.g., Shoes-of-Others prefixing) will improve ToM performance, especially in smaller models.",
        "Diminishing returns in ToM performance will be observed as model size increases beyond hundreds of billions of parameters without corresponding improvements in data quality or fine-tuning.",
        "Models with similar sizes but different fine-tuning strategies or architectures will show significant variability in ToM task performance."
    ],
    "new_predictions_unknown": [
        "Whether there exists a precise model size threshold beyond which ToM performance saturates, regardless of data or fine-tuning improvements.",
        "The extent to which functional ToM capabilities (e.g., adaptive social reasoning in interactive settings) can be achieved by LLMs through fine-tuning and architectural innovations.",
        "How the balance between training data quality and quantity affects ToM performance across different languages and cultural contexts.",
        "Whether multimodal training (e.g., integrating vision and language) can substantially enhance ToM reasoning beyond text-only models.",
        "The relationship between first-order ToM performance and higher-order ToM capabilities in LLMs, and whether improvements in one predict improvements in the other."
    ],
    "negative_experiments": [
        "If larger models consistently fail to outperform smaller models on first-order ToM tasks despite similar training data and fine-tuning, the size-performance correlation would be challenged.",
        "If fine-tuning on ToM-specific datasets does not improve or degrades ToM task performance, the importance of targeted fine-tuning would be questioned.",
        "If models trained on diverse and socially rich datasets do not show better ToM generalization than those trained on less diverse data, the role of data diversity and quality would be undermined.",
        "If models with strong benchmark performance fail to generalize to adversarial or out-of-distribution ToM tasks, the validity of current benchmarks would be questioned.",
        "If smaller models cannot improve ToM performance through fine-tuning or inference-time methods, the mitigation of size limitations would be disproven."
    ],
    "unaccounted_for": [
        {
            "text": "Some large models of similar or greater size (e.g., GPT-4o, Vicuna-33B, Llama-3.1) perform poorly or inconsistently on first-order ToM tasks, suggesting that size and data diversity alone do not fully explain ToM performance variability.",
            "uuids": [
                "e99.1",
                "e114.1",
                "e99.0",
                "e114.0",
                "e114.2",
                "e114.1"
            ]
        },
        {
            "text": "LLMs often rely on shallow heuristics or spurious correlations rather than genuine understanding of mental states, as evidenced by performance drops with minor task perturbations.",
            "uuids": [
                "e109.0",
                "e104.0",
                "e105.0",
                "e109.1",
                "e113.0"
            ]
        },
        {
            "text": "Variability in ToM performance among models of similar size suggests that factors such as model architecture, fine-tuning methods, and task design significantly influence outcomes.",
            "uuids": [
                "e74.0",
                "e83.1",
                "e111.0",
                "e113.0",
                "e114.0",
                "e115.0",
                "e115.1"
            ]
        },
        {
            "text": "Some benchmarks and datasets may overestimate model ToM capabilities due to oversimplified tasks or lack of adversarial examples, indicating that current evaluations might not fully capture genuine ToM reasoning.",
            "uuids": [
                "e99.0",
                "e99.1",
                "e93.0",
                "e91.0"
            ]
        }
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
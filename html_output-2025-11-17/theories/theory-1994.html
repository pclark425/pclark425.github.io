<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM-Driven Law Abstraction via Semantic Aggregation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1994</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1994</p>
                <p><strong>Name:</strong> LLM-Driven Law Abstraction via Semantic Aggregation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can be used to distill qualitative laws from large numbers of scholarly input papers.</p>
                <p><strong>Description:</strong> This theory posits that large language models (LLMs), when exposed to a sufficiently large and diverse corpus of biomedical literature, can abstract qualitative gene–disease association laws by semantically aggregating and reconciling evidence across heterogeneous abstracts. The LLM's internal representations enable it to identify recurring relational patterns, resolve terminological variation, and generalize beyond explicit statements, thus distilling robust, high-level association laws.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Pattern Aggregation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_exposed_to &#8594; large_corpus_of_biomedical_abstracts<span style="color: #888888;">, and</span></div>
        <div>&#8226; abstracts &#8594; contain &#8594; gene–disease_association_statements</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_abstract &#8594; generalized_gene–disease_association_laws</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs have demonstrated the ability to extract and generalize biomedical relationships from unstructured text, outperforming rule-based and shallow learning systems in relation extraction tasks. </li>
    <li>Semantic aggregation across abstracts enables LLMs to identify patterns not explicit in any single document. </li>
    <li>LLMs can resolve terminological variation and normalize entities, allowing aggregation of evidence across studies. </li>
    <li>Empirical studies show LLMs can synthesize knowledge from multiple sources to produce higher-level summaries and rules. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While LLMs have been used for relation extraction, the explicit abstraction of generalized laws via semantic aggregation is a novel theoretical framing.</p>            <p><strong>What Already Exists:</strong> Prior work has shown LLMs can extract biomedical relations and perform entity normalization.</p>            <p><strong>What is Novel:</strong> The law formalizes the process by which LLMs semantically aggregate across abstracts to distill generalized, qualitative association laws, not just extract instances.</p>
            <p><strong>References:</strong> <ul>
    <li>Lee (2020) BioBERT: a pre-trained biomedical language representation model for biomedical text mining [LLMs for biomedical relation extraction]</li>
    <li>Sung (2022) Can large language models learn from explanations? [LLMs generalize from textual evidence, but not formalized as law abstraction]</li>
    <li>Zhang (2023) Large Language Models as Knowledge Extractors for Biomedical Text [LLMs extract and generalize biomedical relations]</li>
</ul>
            <h3>Statement 1: Latent Relational Generalization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; encounters &#8594; diverse_phrasings_of_gene–disease_associations<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; has_internal_latent_space &#8594; sufficiently_structured</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_infer &#8594; novel_gene–disease_association_laws_not_explicitly_stated</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs have demonstrated the ability to generalize and infer relationships not explicitly stated in the training data, especially when exposed to diverse paraphrases. </li>
    <li>Latent space representations in LLMs capture relational semantics beyond surface text. </li>
    <li>LLMs can perform zero-shot and few-shot inference, indicating abstraction of underlying relational rules. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The generalization ability of LLMs is known, but its formalization as a law of latent relational abstraction in the context of biomedical law discovery is novel.</p>            <p><strong>What Already Exists:</strong> LLMs are known to generalize and infer relationships from diverse input.</p>            <p><strong>What is Novel:</strong> The law formalizes the mechanism by which LLMs use latent relational structure to infer new, abstracted association laws.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown (2020) Language Models are Few-Shot Learners [LLMs generalize from few examples]</li>
    <li>Zhang (2023) Large Language Models as Knowledge Extractors for Biomedical Text [LLMs extract and generalize biomedical relations]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM is exposed to a new corpus of abstracts on a rare disease, it will be able to abstract generalized gene–disease association laws even if individual associations are inconsistently phrased.</li>
                <li>LLMs will outperform rule-based systems in identifying novel gene–disease association patterns when abstracts use diverse terminology.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may be able to abstract higher-order, multi-gene–multi-disease association laws that are not present in any single abstract but emerge from aggregate patterns.</li>
                <li>LLMs could potentially identify causal, rather than merely correlative, gene–disease laws through semantic aggregation, even if causality is not explicit in the text.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs fail to abstract generalized association laws when exposed to a large, diverse corpus, the theory would be called into question.</li>
                <li>If LLMs cannot infer novel association laws from paraphrased or semantically similar statements, the latent relational generalization law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where gene–disease associations are only present in figures, tables, or supplementary data not accessible to the LLM. </li>
    <li>Associations that require domain-specific background knowledge not present in the training data. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While related to existing work on LLMs for relation extraction, the theory formalizes the process of law abstraction and generalization, which is not present in prior literature.</p>
            <p><strong>References:</strong> <ul>
    <li>Lee (2020) BioBERT: a pre-trained biomedical language representation model for biomedical text mining [relation extraction, not law abstraction]</li>
    <li>Zhang (2023) Large Language Models as Knowledge Extractors for Biomedical Text [knowledge extraction, not law abstraction]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "LLM-Driven Law Abstraction via Semantic Aggregation",
    "theory_description": "This theory posits that large language models (LLMs), when exposed to a sufficiently large and diverse corpus of biomedical literature, can abstract qualitative gene–disease association laws by semantically aggregating and reconciling evidence across heterogeneous abstracts. The LLM's internal representations enable it to identify recurring relational patterns, resolve terminological variation, and generalize beyond explicit statements, thus distilling robust, high-level association laws.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Pattern Aggregation Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_exposed_to",
                        "object": "large_corpus_of_biomedical_abstracts"
                    },
                    {
                        "subject": "abstracts",
                        "relation": "contain",
                        "object": "gene–disease_association_statements"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_abstract",
                        "object": "generalized_gene–disease_association_laws"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs have demonstrated the ability to extract and generalize biomedical relationships from unstructured text, outperforming rule-based and shallow learning systems in relation extraction tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Semantic aggregation across abstracts enables LLMs to identify patterns not explicit in any single document.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can resolve terminological variation and normalize entities, allowing aggregation of evidence across studies.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LLMs can synthesize knowledge from multiple sources to produce higher-level summaries and rules.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown LLMs can extract biomedical relations and perform entity normalization.",
                    "what_is_novel": "The law formalizes the process by which LLMs semantically aggregate across abstracts to distill generalized, qualitative association laws, not just extract instances.",
                    "classification_explanation": "While LLMs have been used for relation extraction, the explicit abstraction of generalized laws via semantic aggregation is a novel theoretical framing.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lee (2020) BioBERT: a pre-trained biomedical language representation model for biomedical text mining [LLMs for biomedical relation extraction]",
                        "Sung (2022) Can large language models learn from explanations? [LLMs generalize from textual evidence, but not formalized as law abstraction]",
                        "Zhang (2023) Large Language Models as Knowledge Extractors for Biomedical Text [LLMs extract and generalize biomedical relations]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Latent Relational Generalization Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "encounters",
                        "object": "diverse_phrasings_of_gene–disease_associations"
                    },
                    {
                        "subject": "LLM",
                        "relation": "has_internal_latent_space",
                        "object": "sufficiently_structured"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_infer",
                        "object": "novel_gene–disease_association_laws_not_explicitly_stated"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs have demonstrated the ability to generalize and infer relationships not explicitly stated in the training data, especially when exposed to diverse paraphrases.",
                        "uuids": []
                    },
                    {
                        "text": "Latent space representations in LLMs capture relational semantics beyond surface text.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can perform zero-shot and few-shot inference, indicating abstraction of underlying relational rules.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to generalize and infer relationships from diverse input.",
                    "what_is_novel": "The law formalizes the mechanism by which LLMs use latent relational structure to infer new, abstracted association laws.",
                    "classification_explanation": "The generalization ability of LLMs is known, but its formalization as a law of latent relational abstraction in the context of biomedical law discovery is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown (2020) Language Models are Few-Shot Learners [LLMs generalize from few examples]",
                        "Zhang (2023) Large Language Models as Knowledge Extractors for Biomedical Text [LLMs extract and generalize biomedical relations]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM is exposed to a new corpus of abstracts on a rare disease, it will be able to abstract generalized gene–disease association laws even if individual associations are inconsistently phrased.",
        "LLMs will outperform rule-based systems in identifying novel gene–disease association patterns when abstracts use diverse terminology."
    ],
    "new_predictions_unknown": [
        "LLMs may be able to abstract higher-order, multi-gene–multi-disease association laws that are not present in any single abstract but emerge from aggregate patterns.",
        "LLMs could potentially identify causal, rather than merely correlative, gene–disease laws through semantic aggregation, even if causality is not explicit in the text."
    ],
    "negative_experiments": [
        "If LLMs fail to abstract generalized association laws when exposed to a large, diverse corpus, the theory would be called into question.",
        "If LLMs cannot infer novel association laws from paraphrased or semantically similar statements, the latent relational generalization law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where gene–disease associations are only present in figures, tables, or supplementary data not accessible to the LLM.",
            "uuids": []
        },
        {
            "text": "Associations that require domain-specific background knowledge not present in the training data.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Instances where LLMs hallucinate associations not supported by any evidence in the corpus.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs may struggle with associations involving rare genes or diseases with minimal textual representation.",
        "Highly ambiguous or polysemous gene/disease names may reduce the accuracy of law abstraction."
    ],
    "existing_theory": {
        "what_already_exists": "LLMs have been used for biomedical relation extraction and knowledge base construction.",
        "what_is_novel": "The explicit theory of LLM-driven law abstraction via semantic aggregation and latent relational generalization is new.",
        "classification_explanation": "While related to existing work on LLMs for relation extraction, the theory formalizes the process of law abstraction and generalization, which is not present in prior literature.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lee (2020) BioBERT: a pre-trained biomedical language representation model for biomedical text mining [relation extraction, not law abstraction]",
            "Zhang (2023) Large Language Models as Knowledge Extractors for Biomedical Text [knowledge extraction, not law abstraction]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can be used to distill qualitative laws from large numbers of scholarly input papers.",
    "original_theory_id": "theory-659",
    "original_theory_name": "LLM-Driven Extraction of Biomedical Gene–Disease Association Laws via Abstract Aggregation",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "LLM-Driven Extraction of Biomedical Gene–Disease Association Laws via Abstract Aggregation",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
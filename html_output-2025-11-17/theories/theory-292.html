<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain Randomization Targeting Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-292</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-292</p>
                <p><strong>Name:</strong> Domain Randomization Targeting Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of the fundamental trade-off between environment complexity and environment variation in embodied learning systems.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that the effectiveness of domain randomization in embodied learning systems depends critically on strategically targeting specific dimensions of environmental variation based on their relationship to task complexity and learner capacity. Rather than uniformly randomizing all possible environmental parameters, optimal learning occurs when randomization effort is allocated according to: (1) the parameter's causal impact on task-relevant dynamics and observables, (2) the learner's current sensitivity and robustness to that parameter, (3) the complexity of the base environment, and (4) the capacity of the learning system to filter irrelevant variation. The theory posits that there exists an optimal targeting function that maps (environment complexity, learner capacity, training stage) tuples to specific randomization strategies, maximizing transfer performance while minimizing the sample complexity costs of excessive variation. Critically, the benefits of targeting are moderated by model capacity: high-capacity models can partially compensate for suboptimal targeting by learning to ignore irrelevant variation, while lower-capacity models benefit more dramatically from strategic targeting.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The effectiveness of domain randomization is not uniform across all environmental parameters; parameters that causally affect task-relevant dynamics provide greater generalization benefits than parameters affecting only task-irrelevant features.</li>
                <li>There exists an optimal allocation of randomization effort across environmental parameters that maximizes transfer performance while minimizing sample complexity, and this allocation is not uniform.</li>
                <li>The optimal targeting strategy is a function of: (a) base environment complexity, (b) the learner's current policy robustness, (c) the distribution of target deployment environments, and (d) the capacity of the learning system.</li>
                <li>Over-randomization of task-irrelevant parameters increases sample complexity without proportional gains in generalization, with the magnitude of this cost inversely related to model capacity.</li>
                <li>Under-randomization of task-critical parameters leads to brittle policies that fail to transfer to novel conditions, regardless of model capacity.</li>
                <li>The optimal randomization targeting strategy evolves during training: early training benefits from focused variation on high-impact parameters, while later training can incorporate broader variation as the policy becomes more robust.</li>
                <li>Parameters that directly affect task-relevant observables and dynamics should receive higher randomization priority than parameters affecting only task-irrelevant features, with priority proportional to their causal influence on task success.</li>
                <li>The complexity-variation trade-off follows a Pareto frontier: for any given environment complexity level and model capacity, there exists an optimal variation level beyond which additional variation provides diminishing returns or negative returns.</li>
                <li>Model capacity moderates the benefits of targeting: high-capacity models can partially compensate for suboptimal targeting by learning to filter irrelevant variation, while low-capacity models show larger performance gaps between optimal and suboptimal targeting.</li>
                <li>The sample efficiency gain from optimal targeting compared to uniform randomization increases with task complexity and decreases with model capacity.</li>
                <li>There exists a critical threshold of model capacity above which the benefits of sophisticated targeting diminish, as the model can effectively learn to ignore irrelevant variation.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Domain randomization improves sim-to-real transfer in robotic manipulation tasks, demonstrating that environmental variation enhances generalization to novel conditions. </li>
    <li>Excessive or poorly chosen randomization can impair learning by increasing sample complexity and making it difficult to extract task-relevant features, suggesting that not all variation is equally beneficial. </li>
    <li>Curriculum learning approaches that progressively increase task difficulty or variation show improved learning efficiency compared to uniform difficulty, supporting the idea that the timing and targeting of variation matters. </li>
    <li>Automatic domain randomization methods that adapt randomization parameters during training outperform fixed randomization strategies, indicating that optimal targeting is dynamic and context-dependent. </li>
    <li>Task complexity influences the optimal amount and type of environmental variation, with more complex tasks requiring more carefully calibrated randomization strategies. </li>
    <li>High-capacity models (large neural networks) can achieve strong transfer with broad uniform randomization, suggesting that model capacity moderates the necessity of careful targeting. </li>
    <li>Physics parameter randomization (dynamics, contact properties) often produces better transfer than visual randomization alone in manipulation tasks, suggesting that task-relevant parameters should be prioritized. </li>
    <li>Methods that assess transferability and adjust randomization accordingly show improved efficiency, supporting the principle of adaptive targeting. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In robotic grasping tasks with limited model capacity (e.g., small neural networks), randomizing object properties (mass, friction, geometry) will produce 40-60% better transfer success rates than randomizing lighting conditions alone, because object properties directly affect task dynamics. This performance gap will shrink to 10-20% with high-capacity models.</li>
                <li>For locomotion tasks, progressive targeting that starts with randomizing terrain friction and gradually adds terrain geometry variation will outperform simultaneous randomization of all parameters from the start by 25-40% in sample efficiency, with larger gains for more complex terrains.</li>
                <li>Agents trained with targeted randomization on high-impact parameters will achieve equivalent transfer performance to uniformly randomized agents while requiring 30-50% fewer training samples for medium-capacity models, but only 10-20% fewer samples for very high-capacity models.</li>
                <li>In manipulation tasks with complex contact dynamics, prioritizing randomization of physics parameters over visual parameters will yield 35-50% better real-world transfer success rates when using vision-based policies with limited model capacity.</li>
                <li>For a fixed computational budget, allocating 70% of randomization effort to the top 30% most task-relevant parameters will outperform uniform allocation by 20-35% in transfer performance for medium-complexity tasks.</li>
                <li>The optimal targeting strategy will show a phase transition: below a critical model capacity threshold, targeting provides large benefits (>40% improvement); above this threshold, benefits diminish rapidly (<15% improvement).</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There may exist a universal targeting function that can be learned once and transferred across different task domains within a task family (e.g., all manipulation tasks), potentially enabling meta-learning of optimal randomization strategies that generalize across tasks.</li>
                <li>The optimal targeting strategy might exhibit sharp phase transitions at specific complexity thresholds, where the prioritization of parameters suddenly shifts as task complexity crosses critical values, potentially following power-law or sigmoid relationships.</li>
                <li>Targeting strategies optimized for worst-case robustness may fundamentally differ from those optimized for average-case performance by 50% or more in parameter prioritization, potentially requiring different theoretical frameworks and possibly being incompatible objectives.</li>
                <li>The interaction between environment complexity and variation targeting might be non-monotonic, with intermediate complexity levels requiring the most sophisticated targeting strategies, while very simple and very complex tasks both benefit from simpler targeting approaches.</li>
                <li>Embodied learning systems might develop internal representations that naturally align with optimal targeting strategies, suggesting that biological learning systems may have evolved implicit targeting mechanisms that could be reverse-engineered to improve artificial systems.</li>
                <li>The optimal targeting function might be compressible into a low-dimensional representation (e.g., 3-5 key factors), enabling efficient meta-learning and transfer across domains, or it might be irreducibly high-dimensional and domain-specific.</li>
                <li>There may exist a fundamental information-theoretic bound on the benefits of targeting that depends on the mutual information between randomized parameters and task success, providing a principled upper limit on achievable gains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If uniform randomization across all parameters consistently outperforms carefully designed targeted randomization strategies across diverse tasks and model capacities (not just high-capacity models), this would challenge the core premise that targeting matters.</li>
                <li>If the optimal targeting strategy shows no correlation with environment complexity levels across different domains (e.g., optimal strategies for simple and complex tasks are statistically indistinguishable), this would undermine the theory's complexity-variation trade-off principle.</li>
                <li>If agents trained with randomization only on task-irrelevant parameters achieve similar transfer performance (within 10%) to those trained with task-relevant randomization across multiple tasks, this would question the importance of strategic targeting based on causal relevance.</li>
                <li>If the sample complexity of learning does not decrease with more targeted randomization compared to uniform randomization when controlling for model capacity, this would challenge the efficiency claims of the theory.</li>
                <li>If the optimal targeting strategy remains constant throughout training rather than evolving (e.g., early-stage and late-stage optimal strategies are identical), this would contradict the theory's prediction about dynamic targeting requirements.</li>
                <li>If low-capacity and high-capacity models show identical relative benefits from targeting (no interaction effect), this would challenge the theory's claim that model capacity moderates targeting benefits.</li>
                <li>If parameters identified as task-irrelevant through causal analysis provide equal generalization benefits to task-relevant parameters when randomized, this would undermine the causal relevance principle of the theory.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not fully specify computational methods for automatically identifying which parameters are task-relevant versus task-irrelevant without extensive prior domain knowledge or manual analysis, though active domain randomization provides partial solutions. </li>
    <li>The interaction between targeted domain randomization and other generalization techniques (e.g., data augmentation, adversarial training, regularization) is not fully characterized - whether these techniques are complementary, redundant, or interfering. </li>
    <li>The theory does not address how targeting strategies should differ between model-based and model-free learning approaches, which may have different sensitivities to environmental variation. </li>
    <li>The theory does not fully explain how to handle correlated parameters where randomizing one parameter necessitates or interacts with randomizing another, potentially creating complex dependency structures. </li>
    <li>The role of sensorimotor pre-training and its interaction with targeted randomization strategies is not addressed by the theory. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Tobin et al. (2017) Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, IROS [Introduces domain randomization but does not propose targeted/strategic allocation of randomization effort based on parameter relevance or complexity-variation trade-offs]</li>
    <li>Mehta et al. (2020) Active Domain Randomization, CoRL [Proposes adaptive randomization that adjusts parameters during training, but does not develop a comprehensive theory of targeting based on complexity-variation trade-offs or the moderating role of model capacity]</li>
    <li>Bengio et al. (2009) Curriculum Learning, ICML [Addresses progressive difficulty but not specifically the targeting of randomization dimensions or the allocation of variation across parameters]</li>
    <li>Peng et al. (2018) Sim-to-Real Transfer of Robotic Control with Dynamics Randomization, ICRA [Demonstrates dynamics randomization effectiveness but does not theorize about optimal targeting strategies or complexity-variation relationships]</li>
    <li>Muratore et al. (2021) Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment, PMLR [Addresses assessment and adaptation but not the fundamental theory of targeting or the role of model capacity]</li>
    <li>OpenAI et al. (2019) Learning Dexterous In-Hand Manipulation, IJRR [Demonstrates successful broad randomization with high-capacity models but does not theorize about when and why targeting matters]</li>
    <li>Pinto et al. (2017) Robust Adversarial Reinforcement Learning, ICML [Addresses robustness through adversarial approaches but not strategic targeting of randomization]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain Randomization Targeting Theory",
    "theory_description": "This theory proposes that the effectiveness of domain randomization in embodied learning systems depends critically on strategically targeting specific dimensions of environmental variation based on their relationship to task complexity and learner capacity. Rather than uniformly randomizing all possible environmental parameters, optimal learning occurs when randomization effort is allocated according to: (1) the parameter's causal impact on task-relevant dynamics and observables, (2) the learner's current sensitivity and robustness to that parameter, (3) the complexity of the base environment, and (4) the capacity of the learning system to filter irrelevant variation. The theory posits that there exists an optimal targeting function that maps (environment complexity, learner capacity, training stage) tuples to specific randomization strategies, maximizing transfer performance while minimizing the sample complexity costs of excessive variation. Critically, the benefits of targeting are moderated by model capacity: high-capacity models can partially compensate for suboptimal targeting by learning to ignore irrelevant variation, while lower-capacity models benefit more dramatically from strategic targeting.",
    "supporting_evidence": [
        {
            "text": "Domain randomization improves sim-to-real transfer in robotic manipulation tasks, demonstrating that environmental variation enhances generalization to novel conditions.",
            "citations": [
                "Tobin et al. (2017) Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, IROS",
                "Peng et al. (2018) Sim-to-Real Transfer of Robotic Control with Dynamics Randomization, ICRA"
            ]
        },
        {
            "text": "Excessive or poorly chosen randomization can impair learning by increasing sample complexity and making it difficult to extract task-relevant features, suggesting that not all variation is equally beneficial.",
            "citations": [
                "Mehta et al. (2020) Active Domain Randomization, CoRL"
            ]
        },
        {
            "text": "Curriculum learning approaches that progressively increase task difficulty or variation show improved learning efficiency compared to uniform difficulty, supporting the idea that the timing and targeting of variation matters.",
            "citations": [
                "Bengio et al. (2009) Curriculum Learning, ICML",
                "Narvekar et al. (2020) Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey, JMLR"
            ]
        },
        {
            "text": "Automatic domain randomization methods that adapt randomization parameters during training outperform fixed randomization strategies, indicating that optimal targeting is dynamic and context-dependent.",
            "citations": [
                "Mehta et al. (2020) Active Domain Randomization, CoRL",
                "OpenAI et al. (2019) Learning Dexterous In-Hand Manipulation, IJRR"
            ]
        },
        {
            "text": "Task complexity influences the optimal amount and type of environmental variation, with more complex tasks requiring more carefully calibrated randomization strategies.",
            "citations": [
                "Pinto et al. (2017) Robust Adversarial Reinforcement Learning, ICML",
                "Rajeswaran et al. (2017) EPOpt: Learning Robust Neural Network Policies Using Model Ensembles, ICLR"
            ]
        },
        {
            "text": "High-capacity models (large neural networks) can achieve strong transfer with broad uniform randomization, suggesting that model capacity moderates the necessity of careful targeting.",
            "citations": [
                "Akkaya et al. (2019) Solving Rubik's Cube with a Robot Hand, arXiv",
                "OpenAI et al. (2019) Learning Dexterous In-Hand Manipulation, IJRR"
            ]
        },
        {
            "text": "Physics parameter randomization (dynamics, contact properties) often produces better transfer than visual randomization alone in manipulation tasks, suggesting that task-relevant parameters should be prioritized.",
            "citations": [
                "Peng et al. (2018) Sim-to-Real Transfer of Robotic Control with Dynamics Randomization, ICRA"
            ]
        },
        {
            "text": "Methods that assess transferability and adjust randomization accordingly show improved efficiency, supporting the principle of adaptive targeting.",
            "citations": [
                "Muratore et al. (2021) Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment, PMLR"
            ]
        }
    ],
    "theory_statements": [
        "The effectiveness of domain randomization is not uniform across all environmental parameters; parameters that causally affect task-relevant dynamics provide greater generalization benefits than parameters affecting only task-irrelevant features.",
        "There exists an optimal allocation of randomization effort across environmental parameters that maximizes transfer performance while minimizing sample complexity, and this allocation is not uniform.",
        "The optimal targeting strategy is a function of: (a) base environment complexity, (b) the learner's current policy robustness, (c) the distribution of target deployment environments, and (d) the capacity of the learning system.",
        "Over-randomization of task-irrelevant parameters increases sample complexity without proportional gains in generalization, with the magnitude of this cost inversely related to model capacity.",
        "Under-randomization of task-critical parameters leads to brittle policies that fail to transfer to novel conditions, regardless of model capacity.",
        "The optimal randomization targeting strategy evolves during training: early training benefits from focused variation on high-impact parameters, while later training can incorporate broader variation as the policy becomes more robust.",
        "Parameters that directly affect task-relevant observables and dynamics should receive higher randomization priority than parameters affecting only task-irrelevant features, with priority proportional to their causal influence on task success.",
        "The complexity-variation trade-off follows a Pareto frontier: for any given environment complexity level and model capacity, there exists an optimal variation level beyond which additional variation provides diminishing returns or negative returns.",
        "Model capacity moderates the benefits of targeting: high-capacity models can partially compensate for suboptimal targeting by learning to filter irrelevant variation, while low-capacity models show larger performance gaps between optimal and suboptimal targeting.",
        "The sample efficiency gain from optimal targeting compared to uniform randomization increases with task complexity and decreases with model capacity.",
        "There exists a critical threshold of model capacity above which the benefits of sophisticated targeting diminish, as the model can effectively learn to ignore irrelevant variation."
    ],
    "new_predictions_likely": [
        "In robotic grasping tasks with limited model capacity (e.g., small neural networks), randomizing object properties (mass, friction, geometry) will produce 40-60% better transfer success rates than randomizing lighting conditions alone, because object properties directly affect task dynamics. This performance gap will shrink to 10-20% with high-capacity models.",
        "For locomotion tasks, progressive targeting that starts with randomizing terrain friction and gradually adds terrain geometry variation will outperform simultaneous randomization of all parameters from the start by 25-40% in sample efficiency, with larger gains for more complex terrains.",
        "Agents trained with targeted randomization on high-impact parameters will achieve equivalent transfer performance to uniformly randomized agents while requiring 30-50% fewer training samples for medium-capacity models, but only 10-20% fewer samples for very high-capacity models.",
        "In manipulation tasks with complex contact dynamics, prioritizing randomization of physics parameters over visual parameters will yield 35-50% better real-world transfer success rates when using vision-based policies with limited model capacity.",
        "For a fixed computational budget, allocating 70% of randomization effort to the top 30% most task-relevant parameters will outperform uniform allocation by 20-35% in transfer performance for medium-complexity tasks.",
        "The optimal targeting strategy will show a phase transition: below a critical model capacity threshold, targeting provides large benefits (&gt;40% improvement); above this threshold, benefits diminish rapidly (&lt;15% improvement)."
    ],
    "new_predictions_unknown": [
        "There may exist a universal targeting function that can be learned once and transferred across different task domains within a task family (e.g., all manipulation tasks), potentially enabling meta-learning of optimal randomization strategies that generalize across tasks.",
        "The optimal targeting strategy might exhibit sharp phase transitions at specific complexity thresholds, where the prioritization of parameters suddenly shifts as task complexity crosses critical values, potentially following power-law or sigmoid relationships.",
        "Targeting strategies optimized for worst-case robustness may fundamentally differ from those optimized for average-case performance by 50% or more in parameter prioritization, potentially requiring different theoretical frameworks and possibly being incompatible objectives.",
        "The interaction between environment complexity and variation targeting might be non-monotonic, with intermediate complexity levels requiring the most sophisticated targeting strategies, while very simple and very complex tasks both benefit from simpler targeting approaches.",
        "Embodied learning systems might develop internal representations that naturally align with optimal targeting strategies, suggesting that biological learning systems may have evolved implicit targeting mechanisms that could be reverse-engineered to improve artificial systems.",
        "The optimal targeting function might be compressible into a low-dimensional representation (e.g., 3-5 key factors), enabling efficient meta-learning and transfer across domains, or it might be irreducibly high-dimensional and domain-specific.",
        "There may exist a fundamental information-theoretic bound on the benefits of targeting that depends on the mutual information between randomized parameters and task success, providing a principled upper limit on achievable gains."
    ],
    "negative_experiments": [
        "If uniform randomization across all parameters consistently outperforms carefully designed targeted randomization strategies across diverse tasks and model capacities (not just high-capacity models), this would challenge the core premise that targeting matters.",
        "If the optimal targeting strategy shows no correlation with environment complexity levels across different domains (e.g., optimal strategies for simple and complex tasks are statistically indistinguishable), this would undermine the theory's complexity-variation trade-off principle.",
        "If agents trained with randomization only on task-irrelevant parameters achieve similar transfer performance (within 10%) to those trained with task-relevant randomization across multiple tasks, this would question the importance of strategic targeting based on causal relevance.",
        "If the sample complexity of learning does not decrease with more targeted randomization compared to uniform randomization when controlling for model capacity, this would challenge the efficiency claims of the theory.",
        "If the optimal targeting strategy remains constant throughout training rather than evolving (e.g., early-stage and late-stage optimal strategies are identical), this would contradict the theory's prediction about dynamic targeting requirements.",
        "If low-capacity and high-capacity models show identical relative benefits from targeting (no interaction effect), this would challenge the theory's claim that model capacity moderates targeting benefits.",
        "If parameters identified as task-irrelevant through causal analysis provide equal generalization benefits to task-relevant parameters when randomized, this would undermine the causal relevance principle of the theory."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not fully specify computational methods for automatically identifying which parameters are task-relevant versus task-irrelevant without extensive prior domain knowledge or manual analysis, though active domain randomization provides partial solutions.",
            "citations": [
                "Mehta et al. (2020) Active Domain Randomization, CoRL",
                "Muratore et al. (2022) Robot Learning with Sensorimotor Pre-training, CoRL"
            ]
        },
        {
            "text": "The interaction between targeted domain randomization and other generalization techniques (e.g., data augmentation, adversarial training, regularization) is not fully characterized - whether these techniques are complementary, redundant, or interfering.",
            "citations": [
                "Laskin et al. (2020) Reinforcement Learning with Augmented Data, NeurIPS",
                "Kostrikov et al. (2021) Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels, ICLR"
            ]
        },
        {
            "text": "The theory does not address how targeting strategies should differ between model-based and model-free learning approaches, which may have different sensitivities to environmental variation.",
            "citations": [
                "Nagabandi et al. (2018) Neural Network Dynamics for Model-Based Deep Reinforcement Learning, CoRL",
                "Chua et al. (2018) Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models, NeurIPS"
            ]
        },
        {
            "text": "The theory does not fully explain how to handle correlated parameters where randomizing one parameter necessitates or interacts with randomizing another, potentially creating complex dependency structures.",
            "citations": [
                "Peng et al. (2018) Sim-to-Real Transfer of Robotic Control with Dynamics Randomization, ICRA"
            ]
        },
        {
            "text": "The role of sensorimotor pre-training and its interaction with targeted randomization strategies is not addressed by the theory.",
            "citations": [
                "Muratore et al. (2022) Robot Learning with Sensorimotor Pre-training, CoRL"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that very broad, uniform randomization can be highly effective in achieving strong transfer, particularly when using high-capacity neural network policies with extensive compute resources. However, this can be reconciled with the theory by recognizing that high model capacity moderates the benefits of targeting.",
            "citations": [
                "Akkaya et al. (2019) Solving Rubik's Cube with a Robot Hand, arXiv",
                "OpenAI et al. (2019) Learning Dexterous In-Hand Manipulation, IJRR"
            ]
        },
        {
            "text": "Certain simple tasks show no measurable benefit from targeted randomization over uniform randomization, suggesting that the theory's predictions may only apply above a minimum task complexity threshold.",
            "citations": [
                "Zhao et al. (2020) Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey, IEEE SSCI"
            ]
        }
    ],
    "special_cases": [
        "For extremely simple tasks (low complexity, few degrees of freedom), the benefits of targeting may be negligible as uniform randomization is already efficient and the task may not have sufficient structure to benefit from strategic targeting.",
        "For tasks where the deployment environment distribution is known and narrow, targeting should focus specifically on that distribution rather than broad generalization, potentially requiring minimal randomization.",
        "In safety-critical applications, targeting strategies may need to prioritize worst-case scenarios over average-case performance, potentially requiring different optimization objectives (e.g., robust optimization rather than expected value maximization).",
        "When using very high-capacity function approximators (e.g., large neural networks with millions of parameters), the sample complexity benefits of targeting may be substantially reduced (by 50-80%) due to the model's ability to filter irrelevant variation, though targeting may still provide computational efficiency benefits.",
        "Multi-task learning scenarios may require different targeting strategies than single-task learning, as parameters relevant across multiple tasks should receive higher priority, potentially requiring a union or weighted combination of single-task targeting strategies.",
        "For model-based reinforcement learning, targeting may need to focus on parameters that affect model accuracy rather than policy performance directly, potentially leading to different optimal strategies than model-free approaches.",
        "In scenarios with significant sim-to-real gap, targeting should prioritize parameters where the simulator is known to be inaccurate, even if those parameters would otherwise be lower priority.",
        "When training time is severely limited, targeting should be more aggressive (higher concentration on top parameters) than when training time is abundant, as the opportunity cost of exploring irrelevant variation is higher."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Tobin et al. (2017) Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, IROS [Introduces domain randomization but does not propose targeted/strategic allocation of randomization effort based on parameter relevance or complexity-variation trade-offs]",
            "Mehta et al. (2020) Active Domain Randomization, CoRL [Proposes adaptive randomization that adjusts parameters during training, but does not develop a comprehensive theory of targeting based on complexity-variation trade-offs or the moderating role of model capacity]",
            "Bengio et al. (2009) Curriculum Learning, ICML [Addresses progressive difficulty but not specifically the targeting of randomization dimensions or the allocation of variation across parameters]",
            "Peng et al. (2018) Sim-to-Real Transfer of Robotic Control with Dynamics Randomization, ICRA [Demonstrates dynamics randomization effectiveness but does not theorize about optimal targeting strategies or complexity-variation relationships]",
            "Muratore et al. (2021) Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment, PMLR [Addresses assessment and adaptation but not the fundamental theory of targeting or the role of model capacity]",
            "OpenAI et al. (2019) Learning Dexterous In-Hand Manipulation, IJRR [Demonstrates successful broad randomization with high-capacity models but does not theorize about when and why targeting matters]",
            "Pinto et al. (2017) Robust Adversarial Reinforcement Learning, ICML [Addresses robustness through adversarial approaches but not strategic targeting of randomization]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory of the fundamental trade-off between environment complexity and environment variation in embodied learning systems.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-133",
    "original_theory_name": "Domain Randomization Targeting Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
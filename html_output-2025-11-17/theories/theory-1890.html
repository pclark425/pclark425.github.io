<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Format as a Latent Task Specification Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1890</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1890</p>
                <p><strong>Name:</strong> Prompt Format as a Latent Task Specification Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format of a prompt implicitly encodes a latent specification of the task, which is decoded by the LLM to infer not only the required output type but also the expected reasoning style, level of detail, and even the epistemic stance (e.g., certainty, speculation). The LLM uses this latent specification to select or synthesize an internal policy for response generation, thus affecting both the process and the outcome.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt Format Encodes Latent Task Specification (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt &#8594; has_format &#8594; format_Z</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; infers_latent_task_specification &#8594; specification_Z</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can infer whether a prompt expects a summary, explanation, or direct answer based on format alone. </li>
    <li>Prompt format influences the level of detail and style in LLM responses, even when the explicit task is unchanged. </li>
    <li>Instruction-following LLMs adapt their epistemic stance (e.g., hedging, certainty) based on prompt cues. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to instruction tuning, the focus on latent specification and policy selection is a new, more formalized perspective.</p>            <p><strong>What Already Exists:</strong> Prompt format is known to affect LLM output style and detail, and instruction tuning leverages this.</p>            <p><strong>What is Novel:</strong> The explicit claim that prompt format encodes a latent, decodable task specification that governs internal policy selection is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and prompt format]</li>
    <li>Mishra et al. (2022) Cross-task generalization via natural language instructions [Prompt format and task specification]</li>
</ul>
            <h3>Statement 1: Latent Task Specification Governs Response Policy (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; infers_latent_task_specification &#8594; specification_Z</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; selects_response_policy &#8594; policy_Z<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; generates_output &#8594; output_Z</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs generate different outputs (e.g., stepwise reasoning, direct answers, summaries) depending on inferred task from prompt format. </li>
    <li>Prompt format can induce LLMs to adopt different reasoning strategies (e.g., deductive, analogical, speculative). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law formalizes the link between prompt format, latent specification, and policy selection, which is not explicit in prior work.</p>            <p><strong>What Already Exists:</strong> Instruction tuning and prompt engineering show that LLMs can be guided to different response styles.</p>            <p><strong>What is Novel:</strong> The explicit mapping from latent task specification to internal policy selection is a new, mechanistic claim.</p>
            <p><strong>References:</strong> <ul>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and response policy]</li>
    <li>Mishra et al. (2022) Cross-task generalization via natural language instructions [Prompt format and policy selection]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt is reformatted to resemble an explanation request, the LLM will generate more detailed, stepwise outputs even for simple questions.</li>
                <li>If a prompt is made more formal or technical, the LLM will infer a higher expected level of detail and precision in its response.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a prompt is constructed to ambiguously blend multiple task formats, the LLM may synthesize a novel response policy or exhibit unstable output styles.</li>
                <li>If LLMs are exposed to adversarially designed prompt formats, they may infer incorrect latent task specifications, leading to systematic errors.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs generate identical outputs regardless of prompt format, the theory would be falsified.</li>
                <li>If LLMs cannot infer task type or response style from prompt format alone, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs with explicit task-type conditioning may bypass latent specification inference, reducing the effect of prompt format. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory builds on instruction tuning but introduces a new, latent-variable and policy-selection perspective.</p>
            <p><strong>References:</strong> <ul>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and prompt format]</li>
    <li>Mishra et al. (2022) Cross-task generalization via natural language instructions [Prompt format and task specification]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt Format as a Latent Task Specification Theory",
    "theory_description": "This theory proposes that the format of a prompt implicitly encodes a latent specification of the task, which is decoded by the LLM to infer not only the required output type but also the expected reasoning style, level of detail, and even the epistemic stance (e.g., certainty, speculation). The LLM uses this latent specification to select or synthesize an internal policy for response generation, thus affecting both the process and the outcome.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt Format Encodes Latent Task Specification",
                "if": [
                    {
                        "subject": "prompt",
                        "relation": "has_format",
                        "object": "format_Z"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "infers_latent_task_specification",
                        "object": "specification_Z"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can infer whether a prompt expects a summary, explanation, or direct answer based on format alone.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt format influences the level of detail and style in LLM responses, even when the explicit task is unchanged.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction-following LLMs adapt their epistemic stance (e.g., hedging, certainty) based on prompt cues.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt format is known to affect LLM output style and detail, and instruction tuning leverages this.",
                    "what_is_novel": "The explicit claim that prompt format encodes a latent, decodable task specification that governs internal policy selection is novel.",
                    "classification_explanation": "While related to instruction tuning, the focus on latent specification and policy selection is a new, more formalized perspective.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and prompt format]",
                        "Mishra et al. (2022) Cross-task generalization via natural language instructions [Prompt format and task specification]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Latent Task Specification Governs Response Policy",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "infers_latent_task_specification",
                        "object": "specification_Z"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "selects_response_policy",
                        "object": "policy_Z"
                    },
                    {
                        "subject": "LLM",
                        "relation": "generates_output",
                        "object": "output_Z"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs generate different outputs (e.g., stepwise reasoning, direct answers, summaries) depending on inferred task from prompt format.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt format can induce LLMs to adopt different reasoning strategies (e.g., deductive, analogical, speculative).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Instruction tuning and prompt engineering show that LLMs can be guided to different response styles.",
                    "what_is_novel": "The explicit mapping from latent task specification to internal policy selection is a new, mechanistic claim.",
                    "classification_explanation": "The law formalizes the link between prompt format, latent specification, and policy selection, which is not explicit in prior work.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and response policy]",
                        "Mishra et al. (2022) Cross-task generalization via natural language instructions [Prompt format and policy selection]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt is reformatted to resemble an explanation request, the LLM will generate more detailed, stepwise outputs even for simple questions.",
        "If a prompt is made more formal or technical, the LLM will infer a higher expected level of detail and precision in its response."
    ],
    "new_predictions_unknown": [
        "If a prompt is constructed to ambiguously blend multiple task formats, the LLM may synthesize a novel response policy or exhibit unstable output styles.",
        "If LLMs are exposed to adversarially designed prompt formats, they may infer incorrect latent task specifications, leading to systematic errors."
    ],
    "negative_experiments": [
        "If LLMs generate identical outputs regardless of prompt format, the theory would be falsified.",
        "If LLMs cannot infer task type or response style from prompt format alone, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs with explicit task-type conditioning may bypass latent specification inference, reducing the effect of prompt format.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs trained with rigid instruction templates may ignore subtle prompt format cues.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For prompts with minimal or no format cues, LLMs may default to generic response policies.",
        "For highly repetitive or templated prompts, the effect of latent specification inference may be diminished."
    ],
    "existing_theory": {
        "what_already_exists": "Instruction tuning and prompt engineering show that prompt format affects LLM outputs.",
        "what_is_novel": "The explicit theory of prompt format as a latent task specification and its mechanistic link to policy selection is new.",
        "classification_explanation": "The theory builds on instruction tuning but introduces a new, latent-variable and policy-selection perspective.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and prompt format]",
            "Mishra et al. (2022) Cross-task generalization via natural language instructions [Prompt format and task specification]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-652",
    "original_theory_name": "Prompt Format as a High-Dimensional Control Signal for LLM Computation",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt Format as a High-Dimensional Control Signal for LLM Computation",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
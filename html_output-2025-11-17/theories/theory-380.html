<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositional Generalization Gap Theory (Revised) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-380</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-380</p>
                <p><strong>Name:</strong> Compositional Generalization Gap Theory (Revised)</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments, based on the following results.</p>
                <p><strong>Description:</strong> Agents trained on compositional tasks exhibit systematic generalization gaps when tested on novel combinations of learned primitives. Gap magnitude is primarily determined by the interaction of: (1) training regime (episodic meta-learning, curriculum with explicit compositional structure, or standard supervised training), (2) coverage of compositional patterns (k-coverage: multiple shared contexts providing functional equivalence evidence), (3) task structure (autoregressive compositional, path-ambiguous, or tree-structured), (4) compositional depth/complexity, and (5) architectural support for composition. The coverage principle provides the underlying mechanism: compositional generalization requires k-coverage with data requirements scaling as |X|^c where c≈2-3 for depth-2 compositions, invariant to model parameter scale but reducible through architectural innovations like Chain-of-Thought representation (which decomposes multi-hop into sequential single-hop steps). Under standard supervised training without explicit compositional structure, gaps range from moderate (20-40%) for shallow compositions to severe (50-80%) for deep linguistic/semantic compositions (≥3 supporting facts) and specific structural phenomena (attraction errors, negation-of-operators, path-ambiguous structures). Episodic meta-learning or curriculum strategies providing explicit compositional structure reduce gaps substantially (to 10-20% for linguistic/visual domains). Hierarchical architectures with input-adaptive recurrence, latent algorithmic supervision, discrete bottlenecks, and self-correction achieve near-perfect transfer (gaps <5%) in procedural domains. Path ambiguity (variables affecting output through multiple computational paths) represents a fundamental task-structure property requiring near-exhaustive combinations or explicit variable-binding mechanisms. Linear decodability of constituent representations in hidden activations serves as a diagnostic indicator of compositional success across domains. Transfer success depends critically on competency alignment between source and target tasks (2-15× gains when aligned, 10-30% degradation when misaligned).</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-177.html">[theory-177]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Refined theory description to emphasize interaction of factors rather than hierarchy, and to position coverage principle as underlying mechanism for training diversity.</li>
                <li>Replaced rigid percentage ranges with more flexible conditional statements that acknowledge variability: 'gaps ranging from 20-40% for shallow to 50-80% for deep' under standard training, '10-20%' under episodic meta-learning, '<5%' with proper architectural support in procedural domains.</li>
                <li>Clarified that model parameter scaling is invariant to coverage-scaling exponent but acknowledged that scale may help other aspects (e.g., pretraining benefits).</li>
                <li>Distinguished Chain-of-Thought as a representation format that decomposes multi-hop into sequential single-hop steps, reducing but not eliminating coverage requirements.</li>
                <li>Repositioned linear decodability as a diagnostic indicator rather than a causal mechanism, acknowledging correlation without claiming causation.</li>
                <li>Framed path ambiguity as a task-structure property rather than just a limitation, emphasizing it's a property of the task not the model.</li>
                <li>Added more nuance about when different strategies work: episodic meta-learning for linguistic/visual, architectural support for procedural, curriculum for improving standard training.</li>
                <li>Refined inoculation claims to emphasize pattern alignment and coverage rather than sample count, with explicit negative example (v_dat_p2 not transferring to obj_pp_to_subj_pp).</li>
                <li>Added specific quantified failure modes: 96.73% nearest-noun assignment in attraction errors, operator confusion with limited support, non-systematic parsing.</li>
                <li>Incorporated ARC framework as positive bound for autoregressive compositional tasks with explicit conditions (per-component coverage, identifiability).</li>
                <li>Added statement about episodic meta-learning enabling generalization where static training fails completely (86.73% vs 0%), emphasizing training regime primacy.</li>
                <li>Clarified competency alignment principle with specific quantification: 2-15× gains when aligned, 10-30% degradation when misaligned.</li>
                <li>Added statement about entropy/diversity being critical for standard supervised training but compensable through architectural support.</li>
                <li>Expanded unaccounted-for section to include: mechanisms of episodic meta-learning, procedural vs linguistic asymmetry, causal role of linear decodability, conditions for inoculation effectiveness, mixed strategies in subnetworks, relationship between training regime and architecture, Lake & Baroni failures, role of pretraining.</li>
                <li>Added theory statement about compositional depth increasing gap magnitude with specific ranges for n=1, n=2, n≥3.</li>
                <li>Refined new predictions to be more specific about conditions and expected outcomes, distinguishing likely from unknown predictions more clearly.</li>
                <li>Added negative experiments testing coverage principle, scale-invariance, linear decodability diagnostic value, and CoT mechanism.</li>
                <li>Incorporated evidence about RASP proving representational capacity exists while learning remains challenging.</li>
                <li>Added evidence about subnetwork probing revealing mixed strategies (syntactic + non-compositional heuristics).</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Training regime (episodic meta-learning, curriculum with explicit compositional structure, or standard supervised training) is the primary determinant of compositional generalization gap magnitude, with gaps varying from severe (50-80%) under standard training to moderate (10-20%) under episodic meta-learning to minimal (<5%) with proper architectural support in procedural domains.</li>
                <li>The coverage principle governs data requirements: compositional generalization requires k-coverage (multiple distinct shared contexts providing functional equivalence evidence), with required dataset size scaling as |X|^c where c≈2-3 for depth-2 compositions, increasing with compositional depth and parallel branches.</li>
                <li>Model parameter scaling (up to 20× tested) does not reduce the coverage-scaling exponent c or overcome coverage limitations; architectural and training innovations are necessary to improve data efficiency.</li>
                <li>Chain-of-Thought representation (explicit intermediate-state supervision) reduces the coverage-scaling exponent by decomposing multi-hop problems into sequential single-hop predictions (e.g., 3-HOP: c=2.58→1.76), but does not eliminate coverage requirements.</li>
                <li>Task structure determines fundamental difficulty: autoregressive compositional tasks enable exponential generalization (D^T from Õ(D) training) with per-component coverage; path-ambiguous tasks (variables in multiple computational paths) prevent unified intermediate representations without near-exhaustive combinations or explicit variable-binding mechanisms; tree-structured tasks show intermediate difficulty.</li>
                <li>Linear decodability of constituent representations in hidden activations serves as a diagnostic indicator of compositional success: models with high linear decodability (>90% probe accuracy) show better compositional generalization (<20% gaps) across domains, while those with poor decodability (<70%) show larger gaps (>40%).</li>
                <li>Specific systematic failure modes include: (1) attraction errors where 96.73% of cases assign verb arguments to nearest PP noun when PP is present, (2) confusion between similar operators (twice vs thrice) with limited support, (3) non-systematic parsing of ambiguous nested constructs, (4) ignoring negation in negation-of-operator patterns.</li>
                <li>Transfer success depends critically on competency alignment: pretraining or auxiliary tasks aligned with target task structure provide 2-15× sample efficiency gains, while misaligned pretraining degrades performance by 10-30% compared to training from scratch.</li>
                <li>Training distribution entropy/diversity is critical for standard supervised training: performance scales monotonically with entropy H of component distributions, with near-ceiling at H≥2 but failure at low H (H=0, H=1), though architectural support (e.g., permutation equivariance) can compensate.</li>
                <li>Hierarchical architectures with input-adaptive recurrence, latent algorithmic supervision, discrete factorized bottlenecks, and self-correction training achieve near-perfect transfer (gaps <5%) in procedural domains by enabling depth-invariant algorithms in latent space.</li>
                <li>Episodic meta-learning over dynamically varying task grammars enables systematic generalization where static supervised training fails completely (86.73% vs 0% OOD on same architecture), suggesting training regime is more important than architecture or scale for compositional generalization.</li>
                <li>Curriculum strategies that increase k-coverage through multi-grained recombinations (span-level substitution, learned difficulty-aware selection) reduce gaps by 15-40% in linguistic domains, with effectiveness depending on coverage of critical compositional patterns rather than raw sample count.</li>
                <li>Targeted exposure to compositional patterns (inoculation) can reduce gaps when patterns are aligned with target generalization requirements, but shows limited crossover benefits when patterns are misaligned, with effectiveness depending on pattern coverage and alignment rather than sample count alone.</li>
                <li>Compositional depth increases gap magnitude: n=1 shows minimal gaps (<10%), n=2 shows moderate gaps (15-30%), n≥3 shows large gaps (30-50%) under standard training in linguistic domains, with gaps increasing with depth due to compounding coverage requirements.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>CompoST benchmark shows training macro F1 ~0.92-0.95 drops to 0.45 (easy), 0.26 (medium), 0.09 (hard) on novel SPARQL structural combinations under standard supervised training, with compositionality-adjusted F1 never exceeding 0.57. Performance approaches ~0.01 for depth=3 & breadth=3 patterns. <a href="../results/extraction-result-2015.html#e2015.0" class="evidence-link">[e2015.0]</a> <a href="../results/extraction-result-2015.html#e2015.1" class="evidence-link">[e2015.1]</a> <a href="../results/extraction-result-2015.html#e2015.2" class="evidence-link">[e2015.2]</a> </li>
    <li>ReCOGS_pos shows ~88.6% IID dropping to 19.7% on obj_pp_to_subj_pp (69pp gap) and 40-52% on recursion splits under standard training. Detailed error analysis reveals 99.74% of single-part agent errors show attraction to nearest PP noun, with 96.73% assigning to nearest, demonstrating systematic failure mode. <a href="../results/extraction-result-2025.html#e2025.1" class="evidence-link">[e2025.1]</a> <a href="../results/extraction-result-2025.html#e2025.2" class="evidence-link">[e2025.2]</a> <a href="../results/extraction-result-2025.html#e2025.3" class="evidence-link">[e2025.3]</a> <a href="../results/extraction-result-2025.html#e2025.4" class="evidence-link">[e2025.4]</a> </li>
    <li>Coverage principle: Required dataset size scales as |X|^c where c≈2.26 for 2-HOP, 2.43 for PARALLEL-2-HOP, 2.58 for 3-HOP, with log-log fit R²>0.99. Chain-of-Thought supervision reduces exponent (3-HOP: 2.58→1.76) by decomposing multi-hop into sequential single-hop predictions. Model parameter scaling (68M→1.5B, up to 20×) does not reduce exponent. <a href="../results/extraction-result-2023.html#e2023.0" class="evidence-link">[e2023.0]</a> <a href="../results/extraction-result-2023.html#e2023.1" class="evidence-link">[e2023.1]</a> <a href="../results/extraction-result-2023.html#e2023.2" class="evidence-link">[e2023.2]</a> <a href="../results/extraction-result-2023.html#e2023.4" class="evidence-link">[e2023.4]</a> <a href="../results/extraction-result-2023.html#e2023.5" class="evidence-link">[e2023.5]</a> <a href="../results/extraction-result-2023.html#e2023.6" class="evidence-link">[e2023.6]</a> </li>
    <li>Path ambiguity (NON-TREE task): When variable x2 affects output through multiple computational paths, models form context-dependent (b,x2)-conditioned clusters instead of unified b representations. Even with 1.5B parameters and near-exhaustive ID combinations (N up to 50k, 36k epochs), ID accuracy only reaches ~0.96 vs >0.99 for tree-structured 2-HOP. <a href="../results/extraction-result-2023.html#e2023.3" class="evidence-link">[e2023.3]</a> </li>
    <li>Training regime is primary factor: MLC with episodic meta-learning achieves 98.78% 3-shot and 86.73% systematicity (12pp gap) with 5.7M parameters, while static supervised training achieves >99% training fit but 0% OOD on same architecture. Small MLC dramatically outperforms 70B general LLMs under standard prompting (GPT-4o: 0.99%, Gemini: 2.66%, o3-mini: 0.53%). <a href="../results/extraction-result-2027.html#e2027.0" class="evidence-link">[e2027.0]</a> <a href="../results/extraction-result-2027.html#e2027.1" class="evidence-link">[e2027.1]</a> <a href="../results/extraction-result-2027.html#e2027.2" class="evidence-link">[e2027.2]</a> <a href="../results/extraction-result-2027.html#e2027.3" class="evidence-link">[e2027.3]</a> <a href="../results/extraction-result-2027.html#e2027.4" class="evidence-link">[e2027.4]</a> <a href="../results/extraction-result-2027.html#e2027.5" class="evidence-link">[e2027.5]</a> </li>
    <li>Entropy experiments confirm coverage/diversity principle: performance scales monotonically with entropy H of component distributions, near-ceiling at H≥2 but failure at low H (H=0, H=1). Permutation-equivariant model solves all H levels, showing architectural support can compensate for low training diversity. <a href="../results/extraction-result-2016.html#e2016.0" class="evidence-link">[e2016.0]</a> <a href="../results/extraction-result-2016.html#e2016.1" class="evidence-link">[e2016.1]</a> <a href="../results/extraction-result-2016.html#e2016.2" class="evidence-link">[e2016.2]</a> <a href="../results/extraction-result-2016.html#e2016.3" class="evidence-link">[e2016.3]</a> <a href="../results/extraction-result-2016.html#e2016.4" class="evidence-link">[e2016.4]</a> </li>
    <li>CompSub span-level augmentation (increasing k-coverage through multi-grained recombinations) improves: SCAN jump/around-right to ~100%, COGS 55.4%→91.8% (+36.4pp), GeoQuery compositional split improvements, demonstrating substantial gap reductions through curriculum strategies that increase coverage. <a href="../results/extraction-result-2026.html#e2026.0" class="evidence-link">[e2026.0]</a> <a href="../results/extraction-result-2026.html#e2026.1" class="evidence-link">[e2026.1]</a> <a href="../results/extraction-result-2026.html#e2026.2" class="evidence-link">[e2026.2]</a> <a href="../results/extraction-result-2026.html#e2026.5" class="evidence-link">[e2026.5]</a> </li>
    <li>Discrete Latent Space Supervision with Self-Correction (input-adaptive recurrence + latent algorithmic supervision + discrete factorized bottlenecks + self-correction training) achieves near-perfect OOD generalization on modular arithmetic graphs up to 4× training size (N≤32 train, N=128 test), supporting near-complete transfer with proper architectural support in procedural domains. <a href="../results/extraction-result-2024.html#e2024.0" class="evidence-link">[e2024.0]</a> <a href="../results/extraction-result-2024.html#e2024.1" class="evidence-link">[e2024.1]</a> <a href="../results/extraction-result-2024.html#e2024.2" class="evidence-link">[e2024.2]</a> <a href="../results/extraction-result-2024.html#e2024.3" class="evidence-link">[e2024.3]</a> <a href="../results/extraction-result-2024.html#e2024.5" class="evidence-link">[e2024.5]</a> <a href="../results/extraction-result-2024.html#e2024.6" class="evidence-link">[e2024.6]</a> </li>
    <li>Length generalization transfer shows multitask training with structurally aligned auxiliary tasks enables main task extrapolation with 2-15× sample efficiency gains when competency is aligned, while misaligned pretraining hurts performance, confirming competency alignment principle. RoPE positional encodings facilitate transfer better than no positional encoding. <a href="../results/extraction-result-2018.html#e2018.0" class="evidence-link">[e2018.0]</a> <a href="../results/extraction-result-2018.html#e2018.1" class="evidence-link">[e2018.1]</a> <a href="../results/extraction-result-2018.html#e2018.2" class="evidence-link">[e2018.2]</a> <a href="../results/extraction-result-2018.html#e2018.3" class="evidence-link">[e2018.3]</a> <a href="../results/extraction-result-2018.html#e2018.4" class="evidence-link">[e2018.4]</a> <a href="../results/extraction-result-2018.html#e2018.5" class="evidence-link">[e2018.5]</a> </li>
    <li>ARC framework: Tasks with autoregressive compositional structure (T subtasks, D choices per step) can generalize to D^T tasks from Õ(D) training tasks when per-component coverage and identifiability conditions hold. Validated on parity (>95% with ~3·d·ln(d) tasks generalizing to ~6,400 tasks), arithmetic (D=2 operations), and multi-step translation. <a href="../results/extraction-result-2022.html#e2022.0" class="evidence-link">[e2022.0]</a> <a href="../results/extraction-result-2022.html#e2022.1" class="evidence-link">[e2022.1]</a> <a href="../results/extraction-result-2022.html#e2022.3" class="evidence-link">[e2022.3]</a> <a href="../results/extraction-result-2022.html#e2022.4" class="evidence-link">[e2022.4]</a> <a href="../results/extraction-result-2022.html#e2022.5" class="evidence-link">[e2022.5]</a> <a href="../results/extraction-result-2022.html#e2022.6" class="evidence-link">[e2022.6]</a> </li>
    <li>Linear decodability of constituents correlates with compositional success across domains: hyperteacher shows R²>0.95 when constituents are linearly decodable from hidden activations; image generation models with higher constituent decodability show higher composition success rates; curriculum-trained models show high decodability of intermediate subtask values while vanilla-trained show weak decodability. <a href="../results/extraction-result-2009.html#e2009.0" class="evidence-link">[e2009.0]</a> <a href="../results/extraction-result-2009.html#e2009.1" class="evidence-link">[e2009.1]</a> <a href="../results/extraction-result-2009.html#e2009.4" class="evidence-link">[e2009.4]</a> <a href="../results/extraction-result-2004.html#e2004.0" class="evidence-link">[e2004.0]</a> <a href="../results/extraction-result-2004.html#e2004.1" class="evidence-link">[e2004.1]</a> </li>
    <li>Curriculum training with in-context subtask blocks enables zero-shot compositional inference with linearly decodable intermediate representations and fewer errors (curriculum model), while vanilla training with same exemplar-level data but no in-sequence subtask correlations shows weaker zero-shot performance and less decodable intermediate representations. <a href="../results/extraction-result-2004.html#e2004.0" class="evidence-link">[e2004.0]</a> <a href="../results/extraction-result-2004.html#e2004.1" class="evidence-link">[e2004.1]</a> </li>
    <li>Meta-SGD maintains performance across compositional depths D={3,5,7} with minimal degradation while SGD degrades substantially (+15.5% to +34.1% improvements), but featural/dimensional complexity (high F) causes collapse for all methods at F=32, showing depth-robustness but complexity-sensitivity. <a href="../results/extraction-result-2003.html#e2003.0" class="evidence-link">[e2003.0]</a> </li>
    <li>Propositional logic shows ~94% IID but near-zero on specific negation-of-operator patterns (P1-P3), with tree encodings, GCN, and LSTM showing improvements but persistent gaps. LSTM best on P3 (not xor), indicating architecture-specific strengths for certain composition types. <a href="../results/extraction-result-2010.html#e2010.0" class="evidence-link">[e2010.0]</a> <a href="../results/extraction-result-2010.html#e2010.1" class="evidence-link">[e2010.1]</a> <a href="../results/extraction-result-2010.html#e2010.2" class="evidence-link">[e2010.2]</a> <a href="../results/extraction-result-2010.html#e2010.3" class="evidence-link">[e2010.3]</a> </li>
    <li>Inoculation effectiveness depends on pattern alignment: 328 modified v_dat_p2 training examples (PP moved to recipient) did not produce reliable crossover improvement to obj_pp_to_subj_pp (22% ±6.7% vs baseline 19.7%), while other inoculation experiments show benefits, demonstrating alignment-dependence. <a href="../results/extraction-result-2025.html#e2025.3" class="evidence-link">[e2025.3]</a> </li>
    <li>RASP program (Transformer-equivalent, hand-constructed) achieves 100% SEM on ReCOGS_pos test and nearly all generalization splits (92.20% on obj_pp_to_subj_pp, 100% on recursion to depth 12), proving by construction that Transformers can represent systematic compositional solutions, though learning them remains challenging. <a href="../results/extraction-result-2025.html#e2025.0" class="evidence-link">[e2025.0]</a> </li>
    <li>Subnetwork probing reveals standard Transformers contain subnetworks achieving >90% on PP-IOBJ generalization while preserving IID performance, but causal analysis (LEACE concept scrubbing) shows these rely on both syntactic features and non-compositional heuristics, indicating mixed strategies. <a href="../results/extraction-result-2014.html#e2014.0" class="evidence-link">[e2014.0]</a> <a href="../results/extraction-result-2014.html#e2014.1" class="evidence-link">[e2014.1]</a> <a href="../results/extraction-result-2014.html#e2014.2" class="evidence-link">[e2014.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Any new linguistic/semantic compositional task trained with standard supervised learning will show gaps ranging from 20-40% for shallow compositions (depth 1-2) to 50-80% for deep compositions (depth ≥3), but the same task trained with episodic meta-learning will show 10-20% gaps regardless of depth.</li>
                <li>For any compositional task, required training dataset size will scale as |X|^c with token-set size, where c≈2-3 for depth-2 compositions and increases with depth, and this scaling will be invariant to model parameter size but reducible by ~30-40% through Chain-of-Thought representation.</li>
                <li>Procedural compositional tasks with input-adaptive recurrence, latent algorithmic supervision, discrete bottlenecks, and self-correction will show <5% gaps and scale to 4-10× training complexity without degradation.</li>
                <li>Models trained to high IID accuracy (>90%) that show poor linear decodability of constituent representations (<70% probe accuracy) will exhibit large compositional gaps (>40%), while those with high linear decodability (>90%) will show smaller gaps (<20%).</li>
                <li>Augmentation strategies that increase k-coverage (multiple shared contexts per functional equivalence class) will reduce gaps by 15-30%, while augmentation that increases raw sample count without k-coverage will show minimal improvement (<5%).</li>
                <li>Pretraining on compositionally-aligned source tasks will provide 2-5× sample efficiency gains on target tasks, while pretraining on misaligned tasks will degrade performance by 10-30% compared to training from scratch.</li>
                <li>For autoregressive compositional tasks (T subtasks, D choices per step), training on Õ(D·log(D·T)) tasks will enable generalization to D^T tasks when per-component coverage is satisfied, with performance >90% on held-out tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a training procedure that enables >95% compositional generalization in path-ambiguous linguistic tasks without explicit variable-binding mechanisms or near-exhaustive training data.</li>
                <li>Whether compositional generalization gaps scale indefinitely with depth beyond n=4, or plateau at some level of complexity (current evidence goes to n≤4 for most tasks, depth 12 for recursion in specific cases with specialized architectures).</li>
                <li>Whether very large pretrained models (>100B parameters) trained with episodic meta-learning objectives during pretraining can achieve <10% gaps on linguistic compositional tasks, or if architectural changes remain necessary.</li>
                <li>Whether the coverage principle's power-law scaling (|X|^c) holds for compositional depths beyond 3-4, and whether the exponent c continues to increase with depth or shows different scaling behavior (e.g., logarithmic or plateau).</li>
                <li>Whether linear decodability of constituents is causally necessary for compositional success or merely correlational - i.e., whether enforcing linear decodability through training objectives (e.g., auxiliary probe losses) would improve compositional generalization.</li>
                <li>Whether path ambiguity can be overcome through architectural innovations (e.g., explicit memory mechanisms, structured state spaces, attention with variable binding) without requiring near-exhaustive training data.</li>
                <li>Whether there exists a unified curriculum strategy that works equally well across linguistic, procedural, and hierarchical compositional domains, or if domain-specific training regimes are fundamentally necessary due to different task structures.</li>
                <li>Whether the gap between episodic meta-learning (10-20%) and near-perfect performance (<5%) in linguistic domains can be closed through improved meta-learning algorithms, or if it represents a fundamental limit of the approach due to task structure differences from procedural domains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding a standard supervised training procedure that achieves >90% compositional generalization on deep linguistic compositions (n≥3) without episodic meta-learning, explicit curriculum, or targeted inoculation would challenge the training regime primacy claim.</li>
                <li>Demonstrating that compositional gaps disappear with sufficient training data (e.g., 10× current requirements) without k-coverage or targeted patterns would challenge the coverage principle and suggest the issue is purely sample efficiency.</li>
                <li>Finding that path-ambiguous tasks can be solved with <20% gaps using standard architectures and training (without variable-binding mechanisms) would challenge the fundamental task-structure limitation claim.</li>
                <li>Showing that model parameter scaling (e.g., to 1T+ parameters) reduces the coverage-scaling exponent c or enables generalization without k-coverage would challenge the scale-invariance claim.</li>
                <li>Demonstrating that models with poor linear decodability of constituents (<50% probe accuracy) can still achieve strong compositional generalization (>90%) would challenge the diagnostic value of linear decodability.</li>
                <li>Finding that misaligned pretraining consistently helps rather than hurts compositional generalization would challenge the competency alignment principle.</li>
                <li>Showing that static supervised training can match episodic meta-learning performance on the same architecture and data budget would challenge the training regime primacy claim.</li>
                <li>Demonstrating that inoculation with small amounts of OOD data (e.g., 500 samples) consistently closes gaps to >90% regardless of pattern alignment would challenge the pattern-alignment-dependence claim.</li>
                <li>Finding that Chain-of-Thought representation does not reduce the coverage-scaling exponent c in new compositional tasks would challenge the mechanism by which CoT improves data efficiency.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Why certain specific composition types (GIVE events, double disjunctions, negation-of-operators, attraction to nearest PP noun) are systematically harder than others even with diverse training - the theory identifies these patterns but doesn't explain the underlying cognitive/computational reasons for their difficulty. <a href="../results/extraction-result-1622.html#e1622.0" class="evidence-link">[e1622.0]</a> <a href="../results/extraction-result-2010.html#e2010.0" class="evidence-link">[e2010.0]</a> <a href="../results/extraction-result-2025.html#e2025.4" class="evidence-link">[e2025.4]</a> </li>
    <li>The exact mechanisms by which episodic meta-learning enables compositional generalization where static training fails completely (0% vs 86.73%) - what specific computational properties or inductive biases emerge from episodic training that enable composition. <a href="../results/extraction-result-2027.html#e2027.1" class="evidence-link">[e2027.1]</a> <a href="../results/extraction-result-2027.html#e2027.2" class="evidence-link">[e2027.2]</a> </li>
    <li>Why procedural/hierarchical domains show near-complete compositional transfer (<5% gaps) with proper architecture while linguistic domains show persistent gaps (10-20%) even with episodic meta-learning - what fundamental difference in task structure, evaluation methodology, or required abstractions causes this asymmetry. <a href="../results/extraction-result-2024.html#e2024.1" class="evidence-link">[e2024.1]</a> <a href="../results/extraction-result-2027.html#e2027.1" class="evidence-link">[e2027.1]</a> </li>
    <li>The relationship between compositional depth and gap magnitude beyond n=4 for most tasks (evidence exists for recursion to depth 12 in specific cases with specialized architectures but not systematically across task types and training regimes). <a href="../results/extraction-result-1622.html#e1622.0" class="evidence-link">[e1622.0]</a> <a href="../results/extraction-result-1588.html#e1588.1" class="evidence-link">[e1588.1]</a> <a href="../results/extraction-result-2025.html#e2025.1" class="evidence-link">[e2025.1]</a> </li>
    <li>Why some curriculum strategies (learning-progress, task-difficulty, episodic meta-learning, span-level augmentation) work well across domains while others (expertise-based, simple data augmentation, misaligned inoculation) are domain-specific or show limited transfer. <a href="../results/extraction-result-1505.html#e1505.0" class="evidence-link">[e1505.0]</a> <a href="../results/extraction-result-1505.html#e1505.1" class="evidence-link">[e1505.1]</a> <a href="../results/extraction-result-1505.html#e1505.2" class="evidence-link">[e1505.2]</a> <a href="../results/extraction-result-2026.html#e2026.3" class="evidence-link">[e2026.3]</a> <a href="../results/extraction-result-2025.html#e2025.3" class="evidence-link">[e2025.3]</a> </li>
    <li>The causal relationship between linear decodability and compositional success - whether linear decodability is necessary, sufficient, or merely correlational, and whether enforcing it through training objectives would improve generalization. <a href="../results/extraction-result-2009.html#e2009.0" class="evidence-link">[e2009.0]</a> <a href="../results/extraction-result-2009.html#e2009.4" class="evidence-link">[e2009.4]</a> <a href="../results/extraction-result-2004.html#e2004.0" class="evidence-link">[e2004.0]</a> </li>
    <li>Why Chain-of-Thought supervision reduces the coverage-scaling exponent but doesn't eliminate coverage requirements - what computational properties of CoT enable partial but not complete relief from coverage constraints, and why the reduction is consistent across tasks. <a href="../results/extraction-result-2023.html#e2023.4" class="evidence-link">[e2023.4]</a> </li>
    <li>The precise conditions under which inoculation is effective vs ineffective - why 328 examples of v_dat_p2 PP movement don't transfer to obj_pp_to_subj_pp, but other inoculation experiments show benefits, and what determines pattern alignment. <a href="../results/extraction-result-2025.html#e2025.3" class="evidence-link">[e2025.3]</a> <a href="../results/extraction-result-1622.html#e1622.3" class="evidence-link">[e1622.3]</a> </li>
    <li>Why standard Transformers contain subnetworks that achieve >90% on some compositional splits but these subnetworks rely on mixed strategies (syntactic features + non-compositional heuristics) - what determines when compositional vs non-compositional strategies are learned. <a href="../results/extraction-result-2014.html#e2014.1" class="evidence-link">[e2014.1]</a> <a href="../results/extraction-result-2014.html#e2014.2" class="evidence-link">[e2014.2]</a> </li>
    <li>The relationship between training regime, architecture, and domain - whether certain training regimes require specific architectures to be effective, or whether training regime effects are architecture-agnostic. <a href="../results/extraction-result-2027.html#e2027.1" class="evidence-link">[e2027.1]</a> <a href="../results/extraction-result-2024.html#e2024.1" class="evidence-link">[e2024.1]</a> <a href="../results/extraction-result-2004.html#e2004.0" class="evidence-link">[e2004.0]</a> </li>
    <li>Why Lake & Baroni meta-learning model shows specific episode failures (41-54% accuracy) with systematic confusion despite meta-learning training, and what distinguishes successful from unsuccessful meta-learning implementations. <a href="../results/extraction-result-2007.html#e2007.0" class="evidence-link">[e2007.0]</a> <a href="../results/extraction-result-2007.html#e2007.1" class="evidence-link">[e2007.1]</a> </li>
    <li>The role of pretraining scale and diversity - whether natural language pretraining provides compositional benefits through scale, diversity, or specific inductive biases, and how this interacts with downstream training regime. <a href="../results/extraction-result-2018.html#e2018.3" class="evidence-link">[e2018.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositional Generalization Gap Theory (Revised)",
    "type": "specific",
    "theory_description": "Agents trained on compositional tasks exhibit systematic generalization gaps when tested on novel combinations of learned primitives. Gap magnitude is primarily determined by the interaction of: (1) training regime (episodic meta-learning, curriculum with explicit compositional structure, or standard supervised training), (2) coverage of compositional patterns (k-coverage: multiple shared contexts providing functional equivalence evidence), (3) task structure (autoregressive compositional, path-ambiguous, or tree-structured), (4) compositional depth/complexity, and (5) architectural support for composition. The coverage principle provides the underlying mechanism: compositional generalization requires k-coverage with data requirements scaling as |X|^c where c≈2-3 for depth-2 compositions, invariant to model parameter scale but reducible through architectural innovations like Chain-of-Thought representation (which decomposes multi-hop into sequential single-hop steps). Under standard supervised training without explicit compositional structure, gaps range from moderate (20-40%) for shallow compositions to severe (50-80%) for deep linguistic/semantic compositions (≥3 supporting facts) and specific structural phenomena (attraction errors, negation-of-operators, path-ambiguous structures). Episodic meta-learning or curriculum strategies providing explicit compositional structure reduce gaps substantially (to 10-20% for linguistic/visual domains). Hierarchical architectures with input-adaptive recurrence, latent algorithmic supervision, discrete bottlenecks, and self-correction achieve near-perfect transfer (gaps &lt;5%) in procedural domains. Path ambiguity (variables affecting output through multiple computational paths) represents a fundamental task-structure property requiring near-exhaustive combinations or explicit variable-binding mechanisms. Linear decodability of constituent representations in hidden activations serves as a diagnostic indicator of compositional success across domains. Transfer success depends critically on competency alignment between source and target tasks (2-15× gains when aligned, 10-30% degradation when misaligned).",
    "supporting_evidence": [
        {
            "text": "CompoST benchmark shows training macro F1 ~0.92-0.95 drops to 0.45 (easy), 0.26 (medium), 0.09 (hard) on novel SPARQL structural combinations under standard supervised training, with compositionality-adjusted F1 never exceeding 0.57. Performance approaches ~0.01 for depth=3 & breadth=3 patterns.",
            "uuids": [
                "e2015.0",
                "e2015.1",
                "e2015.2"
            ]
        },
        {
            "text": "ReCOGS_pos shows ~88.6% IID dropping to 19.7% on obj_pp_to_subj_pp (69pp gap) and 40-52% on recursion splits under standard training. Detailed error analysis reveals 99.74% of single-part agent errors show attraction to nearest PP noun, with 96.73% assigning to nearest, demonstrating systematic failure mode.",
            "uuids": [
                "e2025.1",
                "e2025.2",
                "e2025.3",
                "e2025.4"
            ]
        },
        {
            "text": "Coverage principle: Required dataset size scales as |X|^c where c≈2.26 for 2-HOP, 2.43 for PARALLEL-2-HOP, 2.58 for 3-HOP, with log-log fit R²&gt;0.99. Chain-of-Thought supervision reduces exponent (3-HOP: 2.58→1.76) by decomposing multi-hop into sequential single-hop predictions. Model parameter scaling (68M→1.5B, up to 20×) does not reduce exponent.",
            "uuids": [
                "e2023.0",
                "e2023.1",
                "e2023.2",
                "e2023.4",
                "e2023.5",
                "e2023.6"
            ]
        },
        {
            "text": "Path ambiguity (NON-TREE task): When variable x2 affects output through multiple computational paths, models form context-dependent (b,x2)-conditioned clusters instead of unified b representations. Even with 1.5B parameters and near-exhaustive ID combinations (N up to 50k, 36k epochs), ID accuracy only reaches ~0.96 vs &gt;0.99 for tree-structured 2-HOP.",
            "uuids": [
                "e2023.3"
            ]
        },
        {
            "text": "Training regime is primary factor: MLC with episodic meta-learning achieves 98.78% 3-shot and 86.73% systematicity (12pp gap) with 5.7M parameters, while static supervised training achieves &gt;99% training fit but 0% OOD on same architecture. Small MLC dramatically outperforms 70B general LLMs under standard prompting (GPT-4o: 0.99%, Gemini: 2.66%, o3-mini: 0.53%).",
            "uuids": [
                "e2027.0",
                "e2027.1",
                "e2027.2",
                "e2027.3",
                "e2027.4",
                "e2027.5"
            ]
        },
        {
            "text": "Entropy experiments confirm coverage/diversity principle: performance scales monotonically with entropy H of component distributions, near-ceiling at H≥2 but failure at low H (H=0, H=1). Permutation-equivariant model solves all H levels, showing architectural support can compensate for low training diversity.",
            "uuids": [
                "e2016.0",
                "e2016.1",
                "e2016.2",
                "e2016.3",
                "e2016.4"
            ]
        },
        {
            "text": "CompSub span-level augmentation (increasing k-coverage through multi-grained recombinations) improves: SCAN jump/around-right to ~100%, COGS 55.4%→91.8% (+36.4pp), GeoQuery compositional split improvements, demonstrating substantial gap reductions through curriculum strategies that increase coverage.",
            "uuids": [
                "e2026.0",
                "e2026.1",
                "e2026.2",
                "e2026.5"
            ]
        },
        {
            "text": "Discrete Latent Space Supervision with Self-Correction (input-adaptive recurrence + latent algorithmic supervision + discrete factorized bottlenecks + self-correction training) achieves near-perfect OOD generalization on modular arithmetic graphs up to 4× training size (N≤32 train, N=128 test), supporting near-complete transfer with proper architectural support in procedural domains.",
            "uuids": [
                "e2024.0",
                "e2024.1",
                "e2024.2",
                "e2024.3",
                "e2024.5",
                "e2024.6"
            ]
        },
        {
            "text": "Length generalization transfer shows multitask training with structurally aligned auxiliary tasks enables main task extrapolation with 2-15× sample efficiency gains when competency is aligned, while misaligned pretraining hurts performance, confirming competency alignment principle. RoPE positional encodings facilitate transfer better than no positional encoding.",
            "uuids": [
                "e2018.0",
                "e2018.1",
                "e2018.2",
                "e2018.3",
                "e2018.4",
                "e2018.5"
            ]
        },
        {
            "text": "ARC framework: Tasks with autoregressive compositional structure (T subtasks, D choices per step) can generalize to D^T tasks from Õ(D) training tasks when per-component coverage and identifiability conditions hold. Validated on parity (&gt;95% with ~3·d·ln(d) tasks generalizing to ~6,400 tasks), arithmetic (D=2 operations), and multi-step translation.",
            "uuids": [
                "e2022.0",
                "e2022.1",
                "e2022.3",
                "e2022.4",
                "e2022.5",
                "e2022.6"
            ]
        },
        {
            "text": "Linear decodability of constituents correlates with compositional success across domains: hyperteacher shows R²&gt;0.95 when constituents are linearly decodable from hidden activations; image generation models with higher constituent decodability show higher composition success rates; curriculum-trained models show high decodability of intermediate subtask values while vanilla-trained show weak decodability.",
            "uuids": [
                "e2009.0",
                "e2009.1",
                "e2009.4",
                "e2004.0",
                "e2004.1"
            ]
        },
        {
            "text": "Curriculum training with in-context subtask blocks enables zero-shot compositional inference with linearly decodable intermediate representations and fewer errors (curriculum model), while vanilla training with same exemplar-level data but no in-sequence subtask correlations shows weaker zero-shot performance and less decodable intermediate representations.",
            "uuids": [
                "e2004.0",
                "e2004.1"
            ]
        },
        {
            "text": "Meta-SGD maintains performance across compositional depths D={3,5,7} with minimal degradation while SGD degrades substantially (+15.5% to +34.1% improvements), but featural/dimensional complexity (high F) causes collapse for all methods at F=32, showing depth-robustness but complexity-sensitivity.",
            "uuids": [
                "e2003.0"
            ]
        },
        {
            "text": "Propositional logic shows ~94% IID but near-zero on specific negation-of-operator patterns (P1-P3), with tree encodings, GCN, and LSTM showing improvements but persistent gaps. LSTM best on P3 (not xor), indicating architecture-specific strengths for certain composition types.",
            "uuids": [
                "e2010.0",
                "e2010.1",
                "e2010.2",
                "e2010.3"
            ]
        },
        {
            "text": "Inoculation effectiveness depends on pattern alignment: 328 modified v_dat_p2 training examples (PP moved to recipient) did not produce reliable crossover improvement to obj_pp_to_subj_pp (22% ±6.7% vs baseline 19.7%), while other inoculation experiments show benefits, demonstrating alignment-dependence.",
            "uuids": [
                "e2025.3"
            ]
        },
        {
            "text": "RASP program (Transformer-equivalent, hand-constructed) achieves 100% SEM on ReCOGS_pos test and nearly all generalization splits (92.20% on obj_pp_to_subj_pp, 100% on recursion to depth 12), proving by construction that Transformers can represent systematic compositional solutions, though learning them remains challenging.",
            "uuids": [
                "e2025.0"
            ]
        },
        {
            "text": "Subnetwork probing reveals standard Transformers contain subnetworks achieving &gt;90% on PP-IOBJ generalization while preserving IID performance, but causal analysis (LEACE concept scrubbing) shows these rely on both syntactic features and non-compositional heuristics, indicating mixed strategies.",
            "uuids": [
                "e2014.0",
                "e2014.1",
                "e2014.2"
            ]
        }
    ],
    "theory_statements": [
        "Training regime (episodic meta-learning, curriculum with explicit compositional structure, or standard supervised training) is the primary determinant of compositional generalization gap magnitude, with gaps varying from severe (50-80%) under standard training to moderate (10-20%) under episodic meta-learning to minimal (&lt;5%) with proper architectural support in procedural domains.",
        "The coverage principle governs data requirements: compositional generalization requires k-coverage (multiple distinct shared contexts providing functional equivalence evidence), with required dataset size scaling as |X|^c where c≈2-3 for depth-2 compositions, increasing with compositional depth and parallel branches.",
        "Model parameter scaling (up to 20× tested) does not reduce the coverage-scaling exponent c or overcome coverage limitations; architectural and training innovations are necessary to improve data efficiency.",
        "Chain-of-Thought representation (explicit intermediate-state supervision) reduces the coverage-scaling exponent by decomposing multi-hop problems into sequential single-hop predictions (e.g., 3-HOP: c=2.58→1.76), but does not eliminate coverage requirements.",
        "Task structure determines fundamental difficulty: autoregressive compositional tasks enable exponential generalization (D^T from Õ(D) training) with per-component coverage; path-ambiguous tasks (variables in multiple computational paths) prevent unified intermediate representations without near-exhaustive combinations or explicit variable-binding mechanisms; tree-structured tasks show intermediate difficulty.",
        "Linear decodability of constituent representations in hidden activations serves as a diagnostic indicator of compositional success: models with high linear decodability (&gt;90% probe accuracy) show better compositional generalization (&lt;20% gaps) across domains, while those with poor decodability (&lt;70%) show larger gaps (&gt;40%).",
        "Specific systematic failure modes include: (1) attraction errors where 96.73% of cases assign verb arguments to nearest PP noun when PP is present, (2) confusion between similar operators (twice vs thrice) with limited support, (3) non-systematic parsing of ambiguous nested constructs, (4) ignoring negation in negation-of-operator patterns.",
        "Transfer success depends critically on competency alignment: pretraining or auxiliary tasks aligned with target task structure provide 2-15× sample efficiency gains, while misaligned pretraining degrades performance by 10-30% compared to training from scratch.",
        "Training distribution entropy/diversity is critical for standard supervised training: performance scales monotonically with entropy H of component distributions, with near-ceiling at H≥2 but failure at low H (H=0, H=1), though architectural support (e.g., permutation equivariance) can compensate.",
        "Hierarchical architectures with input-adaptive recurrence, latent algorithmic supervision, discrete factorized bottlenecks, and self-correction training achieve near-perfect transfer (gaps &lt;5%) in procedural domains by enabling depth-invariant algorithms in latent space.",
        "Episodic meta-learning over dynamically varying task grammars enables systematic generalization where static supervised training fails completely (86.73% vs 0% OOD on same architecture), suggesting training regime is more important than architecture or scale for compositional generalization.",
        "Curriculum strategies that increase k-coverage through multi-grained recombinations (span-level substitution, learned difficulty-aware selection) reduce gaps by 15-40% in linguistic domains, with effectiveness depending on coverage of critical compositional patterns rather than raw sample count.",
        "Targeted exposure to compositional patterns (inoculation) can reduce gaps when patterns are aligned with target generalization requirements, but shows limited crossover benefits when patterns are misaligned, with effectiveness depending on pattern coverage and alignment rather than sample count alone.",
        "Compositional depth increases gap magnitude: n=1 shows minimal gaps (&lt;10%), n=2 shows moderate gaps (15-30%), n≥3 shows large gaps (30-50%) under standard training in linguistic domains, with gaps increasing with depth due to compounding coverage requirements."
    ],
    "new_predictions_likely": [
        "Any new linguistic/semantic compositional task trained with standard supervised learning will show gaps ranging from 20-40% for shallow compositions (depth 1-2) to 50-80% for deep compositions (depth ≥3), but the same task trained with episodic meta-learning will show 10-20% gaps regardless of depth.",
        "For any compositional task, required training dataset size will scale as |X|^c with token-set size, where c≈2-3 for depth-2 compositions and increases with depth, and this scaling will be invariant to model parameter size but reducible by ~30-40% through Chain-of-Thought representation.",
        "Procedural compositional tasks with input-adaptive recurrence, latent algorithmic supervision, discrete bottlenecks, and self-correction will show &lt;5% gaps and scale to 4-10× training complexity without degradation.",
        "Models trained to high IID accuracy (&gt;90%) that show poor linear decodability of constituent representations (&lt;70% probe accuracy) will exhibit large compositional gaps (&gt;40%), while those with high linear decodability (&gt;90%) will show smaller gaps (&lt;20%).",
        "Augmentation strategies that increase k-coverage (multiple shared contexts per functional equivalence class) will reduce gaps by 15-30%, while augmentation that increases raw sample count without k-coverage will show minimal improvement (&lt;5%).",
        "Pretraining on compositionally-aligned source tasks will provide 2-5× sample efficiency gains on target tasks, while pretraining on misaligned tasks will degrade performance by 10-30% compared to training from scratch.",
        "For autoregressive compositional tasks (T subtasks, D choices per step), training on Õ(D·log(D·T)) tasks will enable generalization to D^T tasks when per-component coverage is satisfied, with performance &gt;90% on held-out tasks."
    ],
    "new_predictions_unknown": [
        "Whether there exists a training procedure that enables &gt;95% compositional generalization in path-ambiguous linguistic tasks without explicit variable-binding mechanisms or near-exhaustive training data.",
        "Whether compositional generalization gaps scale indefinitely with depth beyond n=4, or plateau at some level of complexity (current evidence goes to n≤4 for most tasks, depth 12 for recursion in specific cases with specialized architectures).",
        "Whether very large pretrained models (&gt;100B parameters) trained with episodic meta-learning objectives during pretraining can achieve &lt;10% gaps on linguistic compositional tasks, or if architectural changes remain necessary.",
        "Whether the coverage principle's power-law scaling (|X|^c) holds for compositional depths beyond 3-4, and whether the exponent c continues to increase with depth or shows different scaling behavior (e.g., logarithmic or plateau).",
        "Whether linear decodability of constituents is causally necessary for compositional success or merely correlational - i.e., whether enforcing linear decodability through training objectives (e.g., auxiliary probe losses) would improve compositional generalization.",
        "Whether path ambiguity can be overcome through architectural innovations (e.g., explicit memory mechanisms, structured state spaces, attention with variable binding) without requiring near-exhaustive training data.",
        "Whether there exists a unified curriculum strategy that works equally well across linguistic, procedural, and hierarchical compositional domains, or if domain-specific training regimes are fundamentally necessary due to different task structures.",
        "Whether the gap between episodic meta-learning (10-20%) and near-perfect performance (&lt;5%) in linguistic domains can be closed through improved meta-learning algorithms, or if it represents a fundamental limit of the approach due to task structure differences from procedural domains."
    ],
    "negative_experiments": [
        "Finding a standard supervised training procedure that achieves &gt;90% compositional generalization on deep linguistic compositions (n≥3) without episodic meta-learning, explicit curriculum, or targeted inoculation would challenge the training regime primacy claim.",
        "Demonstrating that compositional gaps disappear with sufficient training data (e.g., 10× current requirements) without k-coverage or targeted patterns would challenge the coverage principle and suggest the issue is purely sample efficiency.",
        "Finding that path-ambiguous tasks can be solved with &lt;20% gaps using standard architectures and training (without variable-binding mechanisms) would challenge the fundamental task-structure limitation claim.",
        "Showing that model parameter scaling (e.g., to 1T+ parameters) reduces the coverage-scaling exponent c or enables generalization without k-coverage would challenge the scale-invariance claim.",
        "Demonstrating that models with poor linear decodability of constituents (&lt;50% probe accuracy) can still achieve strong compositional generalization (&gt;90%) would challenge the diagnostic value of linear decodability.",
        "Finding that misaligned pretraining consistently helps rather than hurts compositional generalization would challenge the competency alignment principle.",
        "Showing that static supervised training can match episodic meta-learning performance on the same architecture and data budget would challenge the training regime primacy claim.",
        "Demonstrating that inoculation with small amounts of OOD data (e.g., 500 samples) consistently closes gaps to &gt;90% regardless of pattern alignment would challenge the pattern-alignment-dependence claim.",
        "Finding that Chain-of-Thought representation does not reduce the coverage-scaling exponent c in new compositional tasks would challenge the mechanism by which CoT improves data efficiency."
    ],
    "unaccounted_for": [
        {
            "text": "Why certain specific composition types (GIVE events, double disjunctions, negation-of-operators, attraction to nearest PP noun) are systematically harder than others even with diverse training - the theory identifies these patterns but doesn't explain the underlying cognitive/computational reasons for their difficulty.",
            "uuids": [
                "e1622.0",
                "e2010.0",
                "e2025.4"
            ]
        },
        {
            "text": "The exact mechanisms by which episodic meta-learning enables compositional generalization where static training fails completely (0% vs 86.73%) - what specific computational properties or inductive biases emerge from episodic training that enable composition.",
            "uuids": [
                "e2027.1",
                "e2027.2"
            ]
        },
        {
            "text": "Why procedural/hierarchical domains show near-complete compositional transfer (&lt;5% gaps) with proper architecture while linguistic domains show persistent gaps (10-20%) even with episodic meta-learning - what fundamental difference in task structure, evaluation methodology, or required abstractions causes this asymmetry.",
            "uuids": [
                "e2024.1",
                "e2027.1"
            ]
        },
        {
            "text": "The relationship between compositional depth and gap magnitude beyond n=4 for most tasks (evidence exists for recursion to depth 12 in specific cases with specialized architectures but not systematically across task types and training regimes).",
            "uuids": [
                "e1622.0",
                "e1588.1",
                "e2025.1"
            ]
        },
        {
            "text": "Why some curriculum strategies (learning-progress, task-difficulty, episodic meta-learning, span-level augmentation) work well across domains while others (expertise-based, simple data augmentation, misaligned inoculation) are domain-specific or show limited transfer.",
            "uuids": [
                "e1505.0",
                "e1505.1",
                "e1505.2",
                "e2026.3",
                "e2025.3"
            ]
        },
        {
            "text": "The causal relationship between linear decodability and compositional success - whether linear decodability is necessary, sufficient, or merely correlational, and whether enforcing it through training objectives would improve generalization.",
            "uuids": [
                "e2009.0",
                "e2009.4",
                "e2004.0"
            ]
        },
        {
            "text": "Why Chain-of-Thought supervision reduces the coverage-scaling exponent but doesn't eliminate coverage requirements - what computational properties of CoT enable partial but not complete relief from coverage constraints, and why the reduction is consistent across tasks.",
            "uuids": [
                "e2023.4"
            ]
        },
        {
            "text": "The precise conditions under which inoculation is effective vs ineffective - why 328 examples of v_dat_p2 PP movement don't transfer to obj_pp_to_subj_pp, but other inoculation experiments show benefits, and what determines pattern alignment.",
            "uuids": [
                "e2025.3",
                "e1622.3"
            ]
        },
        {
            "text": "Why standard Transformers contain subnetworks that achieve &gt;90% on some compositional splits but these subnetworks rely on mixed strategies (syntactic features + non-compositional heuristics) - what determines when compositional vs non-compositional strategies are learned.",
            "uuids": [
                "e2014.1",
                "e2014.2"
            ]
        },
        {
            "text": "The relationship between training regime, architecture, and domain - whether certain training regimes require specific architectures to be effective, or whether training regime effects are architecture-agnostic.",
            "uuids": [
                "e2027.1",
                "e2024.1",
                "e2004.0"
            ]
        },
        {
            "text": "Why Lake & Baroni meta-learning model shows specific episode failures (41-54% accuracy) with systematic confusion despite meta-learning training, and what distinguishes successful from unsuccessful meta-learning implementations.",
            "uuids": [
                "e2007.0",
                "e2007.1"
            ]
        },
        {
            "text": "The role of pretraining scale and diversity - whether natural language pretraining provides compositional benefits through scale, diversity, or specific inductive biases, and how this interacts with downstream training regime.",
            "uuids": [
                "e2018.3"
            ]
        }
    ],
    "change_log": [
        "Refined theory description to emphasize interaction of factors rather than hierarchy, and to position coverage principle as underlying mechanism for training diversity.",
        "Replaced rigid percentage ranges with more flexible conditional statements that acknowledge variability: 'gaps ranging from 20-40% for shallow to 50-80% for deep' under standard training, '10-20%' under episodic meta-learning, '&lt;5%' with proper architectural support in procedural domains.",
        "Clarified that model parameter scaling is invariant to coverage-scaling exponent but acknowledged that scale may help other aspects (e.g., pretraining benefits).",
        "Distinguished Chain-of-Thought as a representation format that decomposes multi-hop into sequential single-hop steps, reducing but not eliminating coverage requirements.",
        "Repositioned linear decodability as a diagnostic indicator rather than a causal mechanism, acknowledging correlation without claiming causation.",
        "Framed path ambiguity as a task-structure property rather than just a limitation, emphasizing it's a property of the task not the model.",
        "Added more nuance about when different strategies work: episodic meta-learning for linguistic/visual, architectural support for procedural, curriculum for improving standard training.",
        "Refined inoculation claims to emphasize pattern alignment and coverage rather than sample count, with explicit negative example (v_dat_p2 not transferring to obj_pp_to_subj_pp).",
        "Added specific quantified failure modes: 96.73% nearest-noun assignment in attraction errors, operator confusion with limited support, non-systematic parsing.",
        "Incorporated ARC framework as positive bound for autoregressive compositional tasks with explicit conditions (per-component coverage, identifiability).",
        "Added statement about episodic meta-learning enabling generalization where static training fails completely (86.73% vs 0%), emphasizing training regime primacy.",
        "Clarified competency alignment principle with specific quantification: 2-15× gains when aligned, 10-30% degradation when misaligned.",
        "Added statement about entropy/diversity being critical for standard supervised training but compensable through architectural support.",
        "Expanded unaccounted-for section to include: mechanisms of episodic meta-learning, procedural vs linguistic asymmetry, causal role of linear decodability, conditions for inoculation effectiveness, mixed strategies in subnetworks, relationship between training regime and architecture, Lake & Baroni failures, role of pretraining.",
        "Added theory statement about compositional depth increasing gap magnitude with specific ranges for n=1, n=2, n≥3.",
        "Refined new predictions to be more specific about conditions and expected outcomes, distinguishing likely from unknown predictions more clearly.",
        "Added negative experiments testing coverage principle, scale-invariance, linear decodability diagnostic value, and CoT mechanism.",
        "Incorporated evidence about RASP proving representational capacity exists while learning remains challenging.",
        "Added evidence about subnetwork probing revealing mixed strategies (syntactic + non-compositional heuristics)."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
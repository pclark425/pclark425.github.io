<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory of Simulation Fidelity Boundaries: The Interplay of Model Scale, Alignment, and Prompt/Context Design - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-636</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-636</p>
                <p><strong>Name:</strong> Theory of Simulation Fidelity Boundaries: The Interplay of Model Scale, Alignment, and Prompt/Context Design</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains, based on the following results.</p>
                <p><strong>Description:</strong> The accuracy and fidelity of LLM-based text simulators in scientific subdomains are governed by the interplay of (a) model scale and architecture, (b) alignment and fine-tuning (including RLHF and safety tuning), and (c) prompt/context design (including few-shot, chain-of-thought, and persona/contextualization). Model scale enables higher accuracy and more human-like simulation up to a point, but alignment interventions (e.g., RLHF, safety tuning) can introduce systematic distortions (e.g., hyper-accuracy, loss of negative behaviors, or caricature). Prompt and context design can modulate these effects, enabling or suppressing simulation fidelity. The boundaries of simulation fidelity are thus set by the interaction of these three factors, and can be predicted and manipulated by controlling them.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Scale-Alignment-Fidelity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_scale &#8594; large<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_aligned_with &#8594; target_population_or_behavior<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; is_well_designed_for &#8594; simulation_task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_simulator &#8594; achieves_high_fidelity &#8594; target_simulation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Larger models (e.g., GPT-4, ChatGPT) outperform smaller ones in psychology and social simulation tasks, but alignment can cause hyper-accuracy distortion or loss of negative behaviors. <a href="../results/extraction-result-5540.html#e5540.0" class="evidence-link">[e5540.0]</a> <a href="../results/extraction-result-5682.html#e5682.3" class="evidence-link">[e5682.3]</a> <a href="../results/extraction-result-5504.html#e5504.0" class="evidence-link">[e5504.0]</a> <a href="../results/extraction-result-5682.html#e5682.0" class="evidence-link">[e5682.0]</a> <a href="../results/extraction-result-5682.html#e5682.1" class="evidence-link">[e5682.1]</a> <a href="../results/extraction-result-5682.html#e5682.2" class="evidence-link">[e5682.2]</a> </li>
    <li>Prompt/profile design and dataset alignment are critical for simulating target populations (e.g., Argyle et al., CoMPosT, CommunityLM, Aher et al., ProLLM, Generative Agents). <a href="../results/extraction-result-5540.html#e5540.1" class="evidence-link">[e5540.1]</a> <a href="../results/extraction-result-5504.html#e5504.0" class="evidence-link">[e5504.0]</a> <a href="../results/extraction-result-5651.html#e5651.0" class="evidence-link">[e5651.0]</a> <a href="../results/extraction-result-5540.html#e5540.4" class="evidence-link">[e5540.4]</a> <a href="../results/extraction-result-5658.html#e5658.0" class="evidence-link">[e5658.0]</a> <a href="../results/extraction-result-5667.html#e5667.0" class="evidence-link">[e5667.0]</a> </li>
    <li>Instructional fine-tuning and prompt tuning (e.g., Med-PaLM, MolecularGPT, ProLLM) can dramatically improve simulation fidelity in specialized domains. <a href="../results/extraction-result-5675.html#e5675.2" class="evidence-link">[e5675.2]</a> <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> <a href="../results/extraction-result-5658.html#e5658.0" class="evidence-link">[e5658.0]</a> </li>
    <li>Chain-of-thought and few-shot prompting (e.g., Wei et al., 2022; arithmetic reasoning; commonsense reasoning) enable higher-fidelity multi-step reasoning in large models. <a href="../results/extraction-result-5610.html#e5610.0" class="evidence-link">[e5610.0]</a> <a href="../results/extraction-result-5610.html#e5610.1" class="evidence-link">[e5610.1]</a> <a href="../results/extraction-result-5696.html#e5696.3" class="evidence-link">[e5696.3]</a> </li>
    <li>Alignment and safety tuning can reduce the ability to simulate negative or unaligned behaviors, and can bias outputs toward 'safe' or consensus answers (e.g., psychology simulations, social acceptability, hate speech, CoMPosT). <a href="../results/extraction-result-5540.html#e5540.0" class="evidence-link">[e5540.0]</a> <a href="../results/extraction-result-5608.html#e5608.0" class="evidence-link">[e5608.0]</a> <a href="../results/extraction-result-5608.html#e5608.1" class="evidence-link">[e5608.1]</a> <a href="../results/extraction-result-5608.html#e5608.2" class="evidence-link">[e5608.2]</a> <a href="../results/extraction-result-5608.html#e5608.3" class="evidence-link">[e5608.3]</a> <a href="../results/extraction-result-5608.html#e5608.4" class="evidence-link">[e5608.4]</a> <a href="../results/extraction-result-5504.html#e5504.0" class="evidence-link">[e5504.0]</a> </li>
    <li>Simulation fidelity is modulated by the match between prompt/context and the target subpopulation or behavior (e.g., dataset-alignment in Argyle et al., Aher et al., CommunityLM, ProLLM). <a href="../results/extraction-result-5540.html#e5540.1" class="evidence-link">[e5540.1]</a> <a href="../results/extraction-result-5540.html#e5540.4" class="evidence-link">[e5540.4]</a> <a href="../results/extraction-result-5651.html#e5651.0" class="evidence-link">[e5651.0]</a> <a href="../results/extraction-result-5658.html#e5658.0" class="evidence-link">[e5658.0]</a> </li>
    <li>Inadequate prompt/context design or misalignment leads to caricature, loss of nuance, or failure to reproduce target behaviors (e.g., CoMPosT, CommunityLM, Milgram TE, Wisdom TE). <a href="../results/extraction-result-5504.html#e5504.0" class="evidence-link">[e5504.0]</a> <a href="../results/extraction-result-5651.html#e5651.3" class="evidence-link">[e5651.3]</a> <a href="../results/extraction-result-5682.html#e5682.2" class="evidence-link">[e5682.2]</a> <a href="../results/extraction-result-5682.html#e5682.3" class="evidence-link">[e5682.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the components are known, the explicit boundary theory and prediction of emergent distortions is novel.</p>            <p><strong>What Already Exists:</strong> It is known that model scale, alignment, and prompt design affect LLM performance, and that alignment can introduce biases.</p>            <p><strong>What is Novel:</strong> This law formalizes the three-way interaction as a predictive boundary for simulation fidelity, and predicts systematic distortions (e.g., hyper-accuracy, caricature) as emergent from alignment interventions.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting and scale]</li>
    <li>OpenAI (2023) GPT-4 Technical Report [scaling and alignment]</li>
    <li>Argyle et al. (2022) Out of One, Many: Using Language Models to Simulate Human Samples [prompt/profile alignment]</li>
    <li>Bai et al. (2022) Training a Helpful and Harmless Assistant with RLHF [alignment effects]</li>
    <li>Aher et al. (2023) Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies [simulation fidelity]</li>
</ul>
            <h3>Statement 1: Alignment-Induced Distortion Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_fine_tuned_with &#8594; alignment_or_safety_objectives</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_simulator &#8594; exhibits &#8594; systematic distortions (e.g., hyper-accuracy, loss of negative behaviors, or caricature)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Hyper-accuracy distortion observed in large/aligned models (ChatGPT, GPT-4) in wisdom-of-crowds and psychology simulations; newer models produce unnaturally perfect estimates and reduced variability. <a href="../results/extraction-result-5682.html#e5682.3" class="evidence-link">[e5682.3]</a> <a href="../results/extraction-result-5540.html#e5540.0" class="evidence-link">[e5540.0]</a> <a href="../results/extraction-result-5682.html#e5682.0" class="evidence-link">[e5682.0]</a> <a href="../results/extraction-result-5682.html#e5682.1" class="evidence-link">[e5682.1]</a> </li>
    <li>Safety/value tuning reduces ability to simulate negative/unaligned behaviors in psychology and social simulations; models refuse or distort outputs for negative behaviors. <a href="../results/extraction-result-5540.html#e5540.0" class="evidence-link">[e5540.0]</a> <a href="../results/extraction-result-5504.html#e5504.0" class="evidence-link">[e5504.0]</a> <a href="../results/extraction-result-5608.html#e5608.0" class="evidence-link">[e5608.0]</a> <a href="../results/extraction-result-5608.html#e5608.1" class="evidence-link">[e5608.1]</a> <a href="../results/extraction-result-5608.html#e5608.2" class="evidence-link">[e5608.2]</a> <a href="../results/extraction-result-5608.html#e5608.3" class="evidence-link">[e5608.3]</a> <a href="../results/extraction-result-5608.html#e5608.4" class="evidence-link">[e5608.4]</a> </li>
    <li>Caricature and exaggeration effects are observed in persona/topic simulations (CoMPosT), especially for politicized or marginalized groups, and are modulated by alignment and prompt design. <a href="../results/extraction-result-5504.html#e5504.0" class="evidence-link">[e5504.0]</a> <a href="../results/extraction-result-5504.html#e5504.1" class="evidence-link">[e5504.1]</a> <a href="../results/extraction-result-5504.html#e5504.2" class="evidence-link">[e5504.2]</a> </li>
    <li>Alignment interventions (e.g., RLHF, safety tuning) can cause models to default to consensus or 'safe' answers, reducing simulation of real-world variability (e.g., social acceptability, hate speech, Milgram TE). <a href="../results/extraction-result-5608.html#e5608.0" class="evidence-link">[e5608.0]</a> <a href="../results/extraction-result-5608.html#e5608.1" class="evidence-link">[e5608.1]</a> <a href="../results/extraction-result-5608.html#e5608.2" class="evidence-link">[e5608.2]</a> <a href="../results/extraction-result-5608.html#e5608.3" class="evidence-link">[e5608.3]</a> <a href="../results/extraction-result-5608.html#e5608.4" class="evidence-link">[e5608.4]</a> <a href="../results/extraction-result-5682.html#e5682.2" class="evidence-link">[e5682.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends prior observations into a predictive, testable framework for simulation fidelity.</p>            <p><strong>What Already Exists:</strong> Alignment-induced bias is recognized in LLM literature.</p>            <p><strong>What is Novel:</strong> This law predicts specific, measurable distortions (e.g., hyper-accuracy, loss of negative behaviors, caricature) as a function of alignment interventions in simulation contexts.</p>
            <p><strong>References:</strong> <ul>
    <li>Bai et al. (2022) Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback [alignment effects]</li>
    <li>Aher et al. (2023) Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies [simulation fidelity]</li>
    <li>Dubois et al. (2023) Social Biases in Large Language Models [alignment and bias]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new LLM is released with stronger alignment/safety tuning, it will show increased hyper-accuracy or loss of negative behaviors in human simulation tasks (e.g., more perfect answers, less realistic variability, more refusals for negative behaviors).</li>
                <li>If prompt design is optimized for a specific subpopulation (e.g., dataset-aligned profiles), simulation fidelity for that group will increase, but may decrease for others (e.g., less accurate for out-of-distribution groups).</li>
                <li>If chain-of-thought or few-shot prompting is omitted, even large/aligned models will show reduced simulation fidelity, especially for multi-step reasoning or nuanced behaviors.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If alignment is performed with multi-perspective or distributional objectives (rather than single-label), it may be possible to eliminate hyper-accuracy distortion and better simulate real human variability.</li>
                <li>If a future LLM is trained with explicit anti-caricature objectives, it may be able to simulate marginalized or politicized groups without exaggeration or loss of nuance.</li>
                <li>If alignment is performed with adversarial or diversity-seeking objectives, models may recover the ability to simulate negative or rare behaviors without sacrificing safety.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a highly aligned LLM does not exhibit hyper-accuracy or loss of negative behaviors in simulation tasks, the theory would be challenged.</li>
                <li>If prompt/context design has no effect on simulation fidelity (e.g., dataset-alignment or persona prompts do not change outputs), the theory would be falsified.</li>
                <li>If small or unaligned models can achieve high-fidelity simulation of complex human behaviors without prompt engineering, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks (e.g., code generation, mathematical reasoning, program search) are less affected by alignment-induced distortion, suggesting domain-specific exceptions. <a href="../results/extraction-result-5678.html#e5678.4" class="evidence-link">[e5678.4]</a> <a href="../results/extraction-result-5647.html#e5647.0" class="evidence-link">[e5647.0]</a> <a href="../results/extraction-result-5673.html#e5673.4" class="evidence-link">[e5673.4]</a> <a href="../results/extraction-result-5610.html#e5610.0" class="evidence-link">[e5610.0]</a> <a href="../results/extraction-result-5610.html#e5610.1" class="evidence-link">[e5610.1]</a> <a href="../results/extraction-result-5696.html#e5696.3" class="evidence-link">[e5696.3]</a> </li>
    <li>In some domains, tool augmentation or external simulation (e.g., Mind's Eye, ChemCrow, FunSearch) can overcome limitations of scale or alignment, enabling high-fidelity simulation even with smaller or less-aligned models. <a href="../results/extraction-result-5673.html#e5673.2" class="evidence-link">[e5673.2]</a> <a href="../results/extraction-result-5672.html#e5672.0" class="evidence-link">[e5672.0]</a> <a href="../results/extraction-result-5647.html#e5647.0" class="evidence-link">[e5647.0]</a> <a href="../results/extraction-result-5647.html#e5647.1" class="evidence-link">[e5647.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The explicit boundary theory and prediction of emergent distortions is novel, though components are known; the theory integrates these into a predictive, testable framework for simulation fidelity.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting and scale]</li>
    <li>Bai et al. (2022) Training a Helpful and Harmless Assistant with RLHF [alignment effects]</li>
    <li>Aher et al. (2023) Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies [simulation fidelity]</li>
    <li>Dubois et al. (2023) Social Biases in Large Language Models [alignment and bias]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Theory of Simulation Fidelity Boundaries: The Interplay of Model Scale, Alignment, and Prompt/Context Design",
    "theory_description": "The accuracy and fidelity of LLM-based text simulators in scientific subdomains are governed by the interplay of (a) model scale and architecture, (b) alignment and fine-tuning (including RLHF and safety tuning), and (c) prompt/context design (including few-shot, chain-of-thought, and persona/contextualization). Model scale enables higher accuracy and more human-like simulation up to a point, but alignment interventions (e.g., RLHF, safety tuning) can introduce systematic distortions (e.g., hyper-accuracy, loss of negative behaviors, or caricature). Prompt and context design can modulate these effects, enabling or suppressing simulation fidelity. The boundaries of simulation fidelity are thus set by the interaction of these three factors, and can be predicted and manipulated by controlling them.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Scale-Alignment-Fidelity Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_scale",
                        "object": "large"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_aligned_with",
                        "object": "target_population_or_behavior"
                    },
                    {
                        "subject": "prompt",
                        "relation": "is_well_designed_for",
                        "object": "simulation_task"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_simulator",
                        "relation": "achieves_high_fidelity",
                        "object": "target_simulation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Larger models (e.g., GPT-4, ChatGPT) outperform smaller ones in psychology and social simulation tasks, but alignment can cause hyper-accuracy distortion or loss of negative behaviors.",
                        "uuids": [
                            "e5540.0",
                            "e5682.3",
                            "e5504.0",
                            "e5682.0",
                            "e5682.1",
                            "e5682.2"
                        ]
                    },
                    {
                        "text": "Prompt/profile design and dataset alignment are critical for simulating target populations (e.g., Argyle et al., CoMPosT, CommunityLM, Aher et al., ProLLM, Generative Agents).",
                        "uuids": [
                            "e5540.1",
                            "e5504.0",
                            "e5651.0",
                            "e5540.4",
                            "e5658.0",
                            "e5667.0"
                        ]
                    },
                    {
                        "text": "Instructional fine-tuning and prompt tuning (e.g., Med-PaLM, MolecularGPT, ProLLM) can dramatically improve simulation fidelity in specialized domains.",
                        "uuids": [
                            "e5675.2",
                            "e5680.0",
                            "e5658.0"
                        ]
                    },
                    {
                        "text": "Chain-of-thought and few-shot prompting (e.g., Wei et al., 2022; arithmetic reasoning; commonsense reasoning) enable higher-fidelity multi-step reasoning in large models.",
                        "uuids": [
                            "e5610.0",
                            "e5610.1",
                            "e5696.3"
                        ]
                    },
                    {
                        "text": "Alignment and safety tuning can reduce the ability to simulate negative or unaligned behaviors, and can bias outputs toward 'safe' or consensus answers (e.g., psychology simulations, social acceptability, hate speech, CoMPosT).",
                        "uuids": [
                            "e5540.0",
                            "e5608.0",
                            "e5608.1",
                            "e5608.2",
                            "e5608.3",
                            "e5608.4",
                            "e5504.0"
                        ]
                    },
                    {
                        "text": "Simulation fidelity is modulated by the match between prompt/context and the target subpopulation or behavior (e.g., dataset-alignment in Argyle et al., Aher et al., CommunityLM, ProLLM).",
                        "uuids": [
                            "e5540.1",
                            "e5540.4",
                            "e5651.0",
                            "e5658.0"
                        ]
                    },
                    {
                        "text": "Inadequate prompt/context design or misalignment leads to caricature, loss of nuance, or failure to reproduce target behaviors (e.g., CoMPosT, CommunityLM, Milgram TE, Wisdom TE).",
                        "uuids": [
                            "e5504.0",
                            "e5651.3",
                            "e5682.2",
                            "e5682.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that model scale, alignment, and prompt design affect LLM performance, and that alignment can introduce biases.",
                    "what_is_novel": "This law formalizes the three-way interaction as a predictive boundary for simulation fidelity, and predicts systematic distortions (e.g., hyper-accuracy, caricature) as emergent from alignment interventions.",
                    "classification_explanation": "While the components are known, the explicit boundary theory and prediction of emergent distortions is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting and scale]",
                        "OpenAI (2023) GPT-4 Technical Report [scaling and alignment]",
                        "Argyle et al. (2022) Out of One, Many: Using Language Models to Simulate Human Samples [prompt/profile alignment]",
                        "Bai et al. (2022) Training a Helpful and Harmless Assistant with RLHF [alignment effects]",
                        "Aher et al. (2023) Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies [simulation fidelity]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Alignment-Induced Distortion Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_fine_tuned_with",
                        "object": "alignment_or_safety_objectives"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_simulator",
                        "relation": "exhibits",
                        "object": "systematic distortions (e.g., hyper-accuracy, loss of negative behaviors, or caricature)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Hyper-accuracy distortion observed in large/aligned models (ChatGPT, GPT-4) in wisdom-of-crowds and psychology simulations; newer models produce unnaturally perfect estimates and reduced variability.",
                        "uuids": [
                            "e5682.3",
                            "e5540.0",
                            "e5682.0",
                            "e5682.1"
                        ]
                    },
                    {
                        "text": "Safety/value tuning reduces ability to simulate negative/unaligned behaviors in psychology and social simulations; models refuse or distort outputs for negative behaviors.",
                        "uuids": [
                            "e5540.0",
                            "e5504.0",
                            "e5608.0",
                            "e5608.1",
                            "e5608.2",
                            "e5608.3",
                            "e5608.4"
                        ]
                    },
                    {
                        "text": "Caricature and exaggeration effects are observed in persona/topic simulations (CoMPosT), especially for politicized or marginalized groups, and are modulated by alignment and prompt design.",
                        "uuids": [
                            "e5504.0",
                            "e5504.1",
                            "e5504.2"
                        ]
                    },
                    {
                        "text": "Alignment interventions (e.g., RLHF, safety tuning) can cause models to default to consensus or 'safe' answers, reducing simulation of real-world variability (e.g., social acceptability, hate speech, Milgram TE).",
                        "uuids": [
                            "e5608.0",
                            "e5608.1",
                            "e5608.2",
                            "e5608.3",
                            "e5608.4",
                            "e5682.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Alignment-induced bias is recognized in LLM literature.",
                    "what_is_novel": "This law predicts specific, measurable distortions (e.g., hyper-accuracy, loss of negative behaviors, caricature) as a function of alignment interventions in simulation contexts.",
                    "classification_explanation": "The law extends prior observations into a predictive, testable framework for simulation fidelity.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bai et al. (2022) Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback [alignment effects]",
                        "Aher et al. (2023) Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies [simulation fidelity]",
                        "Dubois et al. (2023) Social Biases in Large Language Models [alignment and bias]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new LLM is released with stronger alignment/safety tuning, it will show increased hyper-accuracy or loss of negative behaviors in human simulation tasks (e.g., more perfect answers, less realistic variability, more refusals for negative behaviors).",
        "If prompt design is optimized for a specific subpopulation (e.g., dataset-aligned profiles), simulation fidelity for that group will increase, but may decrease for others (e.g., less accurate for out-of-distribution groups).",
        "If chain-of-thought or few-shot prompting is omitted, even large/aligned models will show reduced simulation fidelity, especially for multi-step reasoning or nuanced behaviors."
    ],
    "new_predictions_unknown": [
        "If alignment is performed with multi-perspective or distributional objectives (rather than single-label), it may be possible to eliminate hyper-accuracy distortion and better simulate real human variability.",
        "If a future LLM is trained with explicit anti-caricature objectives, it may be able to simulate marginalized or politicized groups without exaggeration or loss of nuance.",
        "If alignment is performed with adversarial or diversity-seeking objectives, models may recover the ability to simulate negative or rare behaviors without sacrificing safety."
    ],
    "negative_experiments": [
        "If a highly aligned LLM does not exhibit hyper-accuracy or loss of negative behaviors in simulation tasks, the theory would be challenged.",
        "If prompt/context design has no effect on simulation fidelity (e.g., dataset-alignment or persona prompts do not change outputs), the theory would be falsified.",
        "If small or unaligned models can achieve high-fidelity simulation of complex human behaviors without prompt engineering, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks (e.g., code generation, mathematical reasoning, program search) are less affected by alignment-induced distortion, suggesting domain-specific exceptions.",
            "uuids": [
                "e5678.4",
                "e5647.0",
                "e5673.4",
                "e5610.0",
                "e5610.1",
                "e5696.3"
            ]
        },
        {
            "text": "In some domains, tool augmentation or external simulation (e.g., Mind's Eye, ChemCrow, FunSearch) can overcome limitations of scale or alignment, enabling high-fidelity simulation even with smaller or less-aligned models.",
            "uuids": [
                "e5673.2",
                "e5672.0",
                "e5647.0",
                "e5647.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some domains, alignment/fine-tuning improves both safety and simulation fidelity (e.g., Med-PaLM in medical QA, ProLLM in PPI prediction, MolecularGPT in molecular property prediction), suggesting that alignment can be beneficial if carefully designed.",
            "uuids": [
                "e5675.2",
                "e5658.0",
                "e5680.0"
            ]
        },
        {
            "text": "Some open-source models (e.g., Mixtral-8x7B, LLaMA-2-70B) can outperform larger or more aligned models on specific sub-tasks (e.g., molecule design), indicating that scale and alignment are not always sufficient.",
            "uuids": [
                "e5565.2",
                "e5699.4"
            ]
        }
    ],
    "special_cases": [
        "Tasks with objective, factual ground truth (e.g., code, math, program search) are less susceptible to alignment-induced distortion.",
        "Simulation of highly marginalized or underrepresented groups may remain inaccurate even with optimal prompt/context design due to pretraining data limitations.",
        "Tool-augmented or retrieval-augmented LLMs can bypass some fidelity boundaries imposed by scale or alignment (e.g., Mind's Eye, ChemCrow, LLM4Doc)."
    ],
    "existing_theory": {
        "what_already_exists": "Scale, alignment, and prompt design are known to affect LLM performance; alignment-induced bias is recognized.",
        "what_is_novel": "The theory formalizes the three-way interaction as a predictive boundary for simulation fidelity, and predicts specific emergent distortions (e.g., hyper-accuracy, caricature) as a function of alignment.",
        "classification_explanation": "The explicit boundary theory and prediction of emergent distortions is novel, though components are known; the theory integrates these into a predictive, testable framework for simulation fidelity.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting and scale]",
            "Bai et al. (2022) Training a Helpful and Harmless Assistant with RLHF [alignment effects]",
            "Aher et al. (2023) Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies [simulation fidelity]",
            "Dubois et al. (2023) Social Biases in Large Language Models [alignment and bias]"
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
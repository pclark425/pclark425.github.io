<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Salience and Accessibility Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1918</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1918</p>
                <p><strong>Name:</strong> Information Salience and Accessibility Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that LLM performance is determined by the salience and accessibility of critical information within the problem presentation. The more salient (i.e., prominent, recent, or contextually highlighted) and accessible (i.e., easy to retrieve given the model's attention mechanisms) the information, the better the LLM's performance, regardless of the absolute position or format.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Salience Enhancement Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; key_information &#8594; is_made &#8594; salient_via_formatting_or_repetition</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; improved_performance_on_key_information</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Highlighting, repeating, or otherwise emphasizing key information in prompts increases LLM accuracy. </li>
    <li>Empirical studies show that bolding, bulleting, or repeating information improves LLM recall and reasoning. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Salience is known, but its explicit use as a law for LLM prompt design is novel.</p>            <p><strong>What Already Exists:</strong> Salience effects are known in human cognition and have been observed in LLMs.</p>            <p><strong>What is Novel:</strong> The law formalizes salience as a manipulable variable in LLM problem presentation.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Salience and recency effects]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience of examples]</li>
</ul>
            <h3>Statement 1: Accessibility Optimization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; key_information &#8594; is_structured &#8594; to_maximize_attention_accessibility</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; higher_accuracy_and_recall</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Structuring prompts to make key information easily accessible (e.g., at the start/end, or with explicit markers) improves LLM performance. </li>
    <li>Attention-based studies show that LLMs retrieve information more accurately when it is easily accessible in the prompt. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Accessibility is discussed in LLM literature, but its formalization as a law is new.</p>            <p><strong>What Already Exists:</strong> Accessibility and attention are known to affect LLM performance.</p>            <p><strong>What is Novel:</strong> The law frames accessibility as a design principle for problem presentation.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Accessibility and attention]</li>
    <li>Dong et al. (2023) Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth [Attention and accessibility]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If key information is repeated or highlighted in a prompt, LLM performance will increase compared to a single, unmarked mention.</li>
                <li>If prompts are structured to make key information more accessible (e.g., via explicit markers), LLMs will show higher accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained to ignore formatting cues, the effect of salience may diminish or reverse.</li>
                <li>If information is made salient but misleading, LLMs may be more prone to errors or hallucinations.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs show no improvement when information is made more salient or accessible, the theory would be falsified.</li>
                <li>If LLMs perform worse when information is highlighted or repeated, the salience law would be invalid.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs may have architectural features that reduce the impact of salience or accessibility manipulations. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory builds on known cognitive principles but applies them as formal laws for LLM prompt design.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Salience and accessibility]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience of examples]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Salience and Accessibility Theory",
    "theory_description": "This theory proposes that LLM performance is determined by the salience and accessibility of critical information within the problem presentation. The more salient (i.e., prominent, recent, or contextually highlighted) and accessible (i.e., easy to retrieve given the model's attention mechanisms) the information, the better the LLM's performance, regardless of the absolute position or format.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Salience Enhancement Law",
                "if": [
                    {
                        "subject": "key_information",
                        "relation": "is_made",
                        "object": "salient_via_formatting_or_repetition"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "improved_performance_on_key_information"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Highlighting, repeating, or otherwise emphasizing key information in prompts increases LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that bolding, bulleting, or repeating information improves LLM recall and reasoning.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Salience effects are known in human cognition and have been observed in LLMs.",
                    "what_is_novel": "The law formalizes salience as a manipulable variable in LLM problem presentation.",
                    "classification_explanation": "Salience is known, but its explicit use as a law for LLM prompt design is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Salience and recency effects]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience of examples]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Accessibility Optimization Law",
                "if": [
                    {
                        "subject": "key_information",
                        "relation": "is_structured",
                        "object": "to_maximize_attention_accessibility"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "higher_accuracy_and_recall"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Structuring prompts to make key information easily accessible (e.g., at the start/end, or with explicit markers) improves LLM performance.",
                        "uuids": []
                    },
                    {
                        "text": "Attention-based studies show that LLMs retrieve information more accurately when it is easily accessible in the prompt.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Accessibility and attention are known to affect LLM performance.",
                    "what_is_novel": "The law frames accessibility as a design principle for problem presentation.",
                    "classification_explanation": "Accessibility is discussed in LLM literature, but its formalization as a law is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Accessibility and attention]",
                        "Dong et al. (2023) Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth [Attention and accessibility]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If key information is repeated or highlighted in a prompt, LLM performance will increase compared to a single, unmarked mention.",
        "If prompts are structured to make key information more accessible (e.g., via explicit markers), LLMs will show higher accuracy."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained to ignore formatting cues, the effect of salience may diminish or reverse.",
        "If information is made salient but misleading, LLMs may be more prone to errors or hallucinations."
    ],
    "negative_experiments": [
        "If LLMs show no improvement when information is made more salient or accessible, the theory would be falsified.",
        "If LLMs perform worse when information is highlighted or repeated, the salience law would be invalid."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs may have architectural features that reduce the impact of salience or accessibility manipulations.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Certain tasks (e.g., code generation) may not benefit from salience or accessibility manipulations.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very short prompts may not benefit from salience or accessibility enhancements.",
        "Tasks with highly structured input (e.g., tables) may require different salience strategies."
    ],
    "existing_theory": {
        "what_already_exists": "Salience and accessibility are known to affect both human and LLM cognition.",
        "what_is_novel": "The explicit formalization of these as laws for LLM problem presentation is new.",
        "classification_explanation": "The theory builds on known cognitive principles but applies them as formal laws for LLM prompt design.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Salience and accessibility]",
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience of examples]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
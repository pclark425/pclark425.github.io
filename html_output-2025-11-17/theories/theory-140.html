<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Topology-Policy Complexity Matching Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-140</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-140</p>
                <p><strong>Name:</strong> Topology-Policy Complexity Matching Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about navigation complexity in text worlds that relates graph-topology features (diameter, clustering, dead-ends, door constraints) to exploration efficiency and optimal policy structure, based on the following results.</p>
                <p><strong>Description:</strong> The optimal policy structure for navigation in text worlds and graph-structured environments must match the complexity of the environment's graph topology. Specifically, environments with high diameter, sparse connectivity, and conditional access constraints (doors/keys/dependencies) require policies with explicit memory and planning capabilities, while low-diameter, densely-connected environments can be navigated effectively with reactive policies. The theory posits a hierarchy of policy complexity requirements: (1) reactive policies suffice for simple topologies (diameter ≤5, dense connectivity), (2) recurrent memory (LSTM/GRU) is needed for moderate complexity (diameter 5-15, perceptual aliasing), (3) explicit graph memory (knowledge graphs, topological maps) is required for high complexity (diameter >15, conditional dependencies, bottlenecks), and (4) hierarchical planning with graph memory is optimal for the highest complexity environments (diameter >25, multiple conditional dependencies, sparse connectivity). The performance gap between optimal and suboptimal policy structures increases with topology complexity, though this relationship can be modulated by factors such as perceptual aliasing, action space size, stochasticity, and domain-specific regularities.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Policy complexity requirements scale monotonically with graph topology complexity, measured by diameter, connectivity sparsity, conditional access constraints, and perceptual aliasing.</li>
                <li>Reactive policies are sufficient for environments where diameter ≤ 5 steps, connectivity is dense (average degree > 4), and there is no perceptual aliasing or conditional access.</li>
                <li>Recurrent memory (LSTM/GRU) becomes necessary when diameter > 5, when perceptual aliasing is present, or when short-term temporal dependencies must be tracked.</li>
                <li>Explicit graph memory (knowledge graphs, topological maps) is required when diameter > 15, when conditional dependencies (keys/doors) create bottlenecks, or when long-term spatial relationships must be maintained.</li>
                <li>Hierarchical planning with graph memory is optimal for environments with diameter > 25, multiple conditional dependencies, sparse connectivity, and complex goal structures.</li>
                <li>The performance gap between optimal and suboptimal policy structures increases monotonically with topology complexity, with gaps of 2-3x common between adjacent complexity levels.</li>
                <li>Memory capacity requirements scale with the product of diameter and branching factor, as agents must maintain information about reachable states.</li>
                <li>In environments with stochastic transitions or noisy observations, the complexity threshold for requiring each policy type decreases by approximately 20-30%.</li>
                <li>Action space size and branching factor interact multiplicatively with topology complexity to determine exploration difficulty and required policy sophistication.</li>
                <li>Domain-specific regularities (repeating patterns, strong homophily) can reduce effective topology complexity by up to one level in the hierarchy.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>In small static mazes (5x10), reactive feedforward A3C policies can perform competitively, but memory-augmented policies with auxiliary tasks (depth prediction) achieve better performance and faster learning (AUC 104.3 vs lower for feedforward). <a href="../results/extraction-result-1370.html#e1370.1" class="evidence-link">[e1370.1]</a> </li>
    <li>In random goal mazes (5x10), policies must remember discovered goal locations and learn explore-then-exploit strategies, with stacked LSTM+auxiliary tasks achieving 71.1 AUC and 85.5% position decoding accuracy. <a href="../results/extraction-result-1370.html#e1370.3" class="evidence-link">[e1370.3]</a> </li>
    <li>In larger, more complex mazes (15x15 3D ViZDoom), policies without spatial memory (LSTM-only) achieve only 52.4% success on unseen maps, while Neural Map with GRU-based spatial memory maintains 66.6% success, demonstrating the need for structured spatial memory in high-diameter environments. <a href="../results/extraction-result-1348.html#e1348.1" class="evidence-link">[e1348.1]</a> </li>
    <li>In ViZDoom maze1 and maze2, GAM with recurrent graph attention achieves 100% success on both mazes, while reactive baselines achieve only 10% on the more complex maze2, showing that long-distance navigation requires multi-step graph aggregation. <a href="../results/extraction-result-1161.html#e1161.0" class="evidence-link">[e1161.0]</a> </li>
    <li>Pure LSTM policies achieve only 22.5% success on House3D RoomNav tasks (H=1000), while hierarchical BRM with explicit semantic graph memory achieves 41.1% success, and RNN controller achieves 37.0%, demonstrating the advantage of explicit relational memory over implicit recurrent memory. <a href="../results/extraction-result-1362.html#e1362.1" class="evidence-link">[e1362.1]</a> <a href="../results/extraction-result-1362.html#e1362.2" class="evidence-link">[e1362.2]</a> <a href="../results/extraction-result-1362.html#e1362.3" class="evidence-link">[e1362.3]</a> </li>
    <li>In Zork1 text adventures with bottlenecks and conditional access, knowledge-graph-based policies (KG-A2C) achieve score ~34 and converge faster than text-only baselines (TDQN ~9.9), with graph mask and GAT both contributing to performance. <a href="../results/extraction-result-1205.html#e1205.0" class="evidence-link">[e1205.0]</a> <a href="../results/extraction-result-1220.html#e1220.0" class="evidence-link">[e1220.0]</a> </li>
    <li>On PointGoal navigation in Matterport3D (larger scenes, average geodesic distance 11.5m), depth-equipped learned policies with memory (GRU) achieve 69% success and SPL 0.54, vs 35% success and SPL 0.25 for blind reactive policies, with performance degrading as geodesic distance and GED ratio increase. <a href="../results/extraction-result-1377.html#e1377.0" class="evidence-link">[e1377.0]</a> <a href="../results/extraction-result-1369.html#e1369.0" class="evidence-link">[e1369.0]</a> </li>
    <li>In ViZDoom mazes requiring long exploration sequences (~10k steps), SPTM with explicit non-parametric topological memory achieves ~3x higher success than LSTM baselines, with visual shortcuts reducing average shortest-path from 990 to 155 steps. <a href="../results/extraction-result-1198.html#e1198.0" class="evidence-link">[e1198.0]</a> <a href="../results/extraction-result-1198.html#e1198.2" class="evidence-link">[e1198.2]</a> </li>
    <li>For navigation in 25-room 2D layouts (higher diameter), hierarchical landmark-based policies (Dr. Strategy) achieve 67.11% success vs 9.62% for non-hierarchical LEXA and 0.14% for LEXA-Explore, demonstrating the necessity of hierarchical structure for large state spaces. <a href="../results/extraction-result-1272.html#e1272.2" class="evidence-link">[e1272.2]</a> </li>
    <li>In text-based CoinCollector with long chains (L30, diameter ~30), episodic discovery bonus with recurrence enables near-perfect generalization when trained on L10, while non-recurrent policies struggle, showing memory requirements scale with path length. <a href="../results/extraction-result-1174.html#e1174.0" class="evidence-link">[e1174.0]</a> <a href="../results/extraction-result-1361.html#e1361.0" class="evidence-link">[e1361.0]</a> </li>
    <li>Graph-augmented policies (GATA) with dynamic belief graphs achieve +24.2% improvement over text-only baselines in TextWorld cooking games with multiple rooms and dependencies, with continuous belief graphs outperforming discrete updates. <a href="../results/extraction-result-1374.html#e1374.1" class="evidence-link">[e1374.1]</a> </li>
    <li>In Gibson indoor navigation, Active Neural SLAM with explicit map-based global planning achieves 94.8% coverage vs ~54% for random exploration, with advantages increasing in larger scenes and higher geodesic distances. <a href="../results/extraction-result-1366.html#e1366.0" class="evidence-link">[e1366.0]</a> </li>
    <li>GraphNav with behavior decomposition and topological planning achieves up to 83.3% success in structured indoor environments, with performance correlating with corridor structure and failing in large open spaces. <a href="../results/extraction-result-1347.html#e1347.0" class="evidence-link">[e1347.0]</a> </li>
    <li>LAMP with memory-augmented reactive planning achieves 10-40% reduction in travel time over baselines in repeated navigation with stochastic portal blocking, demonstrating benefits of structured memory for environments with conditional access. <a href="../results/extraction-result-1245.html#e1245.3" class="evidence-link">[e1245.3]</a> </li>
    <li>Lifelong topological visual navigation (LTvN) with controller-aware graph construction achieves 65-70% success after maintenance vs 20% before, showing that explicit topology representation enables learning from experience. <a href="../results/extraction-result-1163.html#e1163.0" class="evidence-link">[e1163.0]</a> </li>
    <li>DGMem with dynamic topological graph memory achieves better coverage and navigation performance than curiosity-driven baselines, with topological distance rewards and node-visit tracking improving exploration efficiency. <a href="../results/extraction-result-1219.html#e1219.0" class="evidence-link">[e1219.0]</a> </li>
    <li>Evolving Graphical Planner (EGP) with growing topological memory achieves 52% success on R2R validation vs 42% without message passing, with performance improving as top-K expansion increases graph connectivity. <a href="../results/extraction-result-1252.html#e1252.0" class="evidence-link">[e1252.0]</a> </li>
    <li>GMetaExp with graph-structured memory achieves 72% coverage in 2D mazes vs 33% for random and 54% for RandDFS, with autoregressive history aggregation providing ~5% improvement. <a href="../results/extraction-result-1359.html#e1359.0" class="evidence-link">[e1359.0]</a> </li>
    <li>M-Walk with RNN+MCTS achieves 99.0% test accuracy on Three Glass Puzzle vs ~49% for REINFORCE baseline, demonstrating that memory+planning is essential for sparse-reward combinatorial graphs. <a href="../results/extraction-result-1373.html#e1373.0" class="evidence-link">[e1373.0]</a> </li>
    <li>M-Walk on knowledge graphs (WN18RR, NELL-995) shows larger relative improvements for multi-step reasoning (path length 2-3) vs single-step, with history encoding (LSTM) being crucial for performance. <a href="../results/extraction-result-1373.html#e1373.1" class="evidence-link">[e1373.1]</a> <a href="../results/extraction-result-1373.html#e1373.2" class="evidence-link">[e1373.2]</a> </li>
    <li>MINERVA on knowledge graphs achieves MRR 0.448 on WN18RR with history-dependent LSTM policy, with performance degrading substantially when history encoding is removed, especially on longer reasoning chains. <a href="../results/extraction-result-1375.html#e1375.0" class="evidence-link">[e1375.0]</a> </li>
    <li>Go-Explore on Montezuma's Revenge with domain-knowledge cell representation (preserving room/level/key structure) discovers mean 238 rooms vs 35 without domain knowledge, solving level 1 in 57.6M vs 640M frames. <a href="../results/extraction-result-1365.html#e1365.0" class="evidence-link">[e1365.0]</a> </li>
    <li>In deterministic chain MDPs, Bootstrapped DQN with temporally-extended exploration scales gracefully with chain length, while shallow strategies scale exponentially worse, demonstrating the necessity of temporal commitment for high-diameter topologies. <a href="../results/extraction-result-1189.html#e1189.0" class="evidence-link">[e1189.0]</a> </li>
    <li>In hierarchical DRL on 6-state chain with distant contingent reward, h-DQN with goal-conditioned policies achieves ~0.13 average reward vs ~0.01 for flat Q-learning, showing hierarchical goal structure helps in high path-length environments. <a href="../results/extraction-result-1357.html#e1357.0" class="evidence-link">[e1357.0]</a> </li>
    <li>On Montezuma's Revenge, h-DQN with entity-relation goals achieves ~400 reward per episode vs 0 for flat DQN, demonstrating that hierarchical goal-conditioned policies overcome bottlenecks in sparse-reward, high-diameter environments. <a href="../results/extraction-result-1357.html#e1357.1" class="evidence-link">[e1357.1]</a> </li>
    <li>In AI2-THOR object navigation, policies with directed object-relation graphs (DOA) achieve 74.32% success vs 69.14% baseline, with directed graphs significantly outperforming undirected, showing explicit relational structure helps in complex topologies. <a href="../results/extraction-result-1246.html#e1246.0" class="evidence-link">[e1246.0]</a> </li>
    <li>VoroNav with Voronoi-based topological graph achieves 42% success on HM3D vs lower for frontier-based methods, with intersection-based waypoints providing higher SEA and SCA than frontier selection. <a href="../results/extraction-result-1237.html#e1237.0" class="evidence-link">[e1237.0]</a> </li>
    <li>L3MVN with LLM-based frontier scoring achieves 54.2% success on HM3D validation, with feed-forward LLM outperforming zero-shot and demonstrating benefits of semantic priors for frontier selection in complex layouts. <a href="../results/extraction-result-1364.html#e1364.0" class="evidence-link">[e1364.0]</a> </li>
    <li>ESC with commonsense constraints achieves 39.2% success on HM3D, with room-level reasoning reducing exploration error and FrontierDist (7.6m vs 8.2m for baseline). <a href="../results/extraction-result-1355.html#e1355.0" class="evidence-link">[e1355.0]</a> </li>
    <li>In TextWorld home games, KG-DQN with knowledge graph memory converges ~40% faster than LSTM-DQN on small maps and achieves 73.7±8.5 steps vs much higher for baselines, with action pruning via graph reducing exploration waste. <a href="../results/extraction-result-1349.html#e1349.1" class="evidence-link">[e1349.1]</a> </li>
    <li>NESTA with abstract lifted rules achieves best performance across TextWorld Commonsense splits, with largest improvements on hard (two-room) tasks requiring multi-step chaining. <a href="../results/extraction-result-1199.html#e1199.1" class="evidence-link">[e1199.1]</a> </li>
    <li>Q*BERT and variants with knowledge-graph-based exploration and intrinsic KG-growth rewards achieve higher max scores and faster bottleneck discovery in Zork1 than baselines without structured memory. <a href="../results/extraction-result-1184.html#e1184.1" class="evidence-link">[e1184.1]</a> <a href="../results/extraction-result-1220.html#e1220.2" class="evidence-link">[e1220.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In a text world with 50 rooms arranged in a tree structure (diameter ~25), a hierarchical policy with topological memory will achieve >2x the success rate of a pure LSTM policy and >5x the success rate of a reactive policy.</li>
                <li>Adding 5 locked doors requiring keys to a 10-room maze will increase the performance advantage of knowledge-graph policies over reactive policies by at least 20 percentage points and over LSTM policies by at least 10 percentage points.</li>
                <li>In environments with diameter < 3 and no conditional access, the performance difference between reactive and memory-based policies will be < 5%, but adding perceptual aliasing will increase this gap to >15%.</li>
                <li>Increasing maze diameter from 10 to 20 steps while holding other factors constant will increase the performance gap between explicit graph memory and LSTM policies from ~10% to ~25%.</li>
                <li>In a 100-room environment with diameter 40, hierarchical planning with graph memory will achieve >3x the success rate of flat graph-memory policies.</li>
                <li>Combining high diameter (>20) with high branching factor (>10) will require hierarchical policies even if no conditional dependencies are present.</li>
                <li>In environments with stochastic transitions, the diameter threshold for requiring explicit graph memory will decrease from 15 to approximately 10-12 steps.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>In extremely high-diameter environments (>100 steps), there may be a threshold beyond which even hierarchical planning fails without additional abstraction mechanisms such as multi-level hierarchies or learned macro-actions.</li>
                <li>The relationship between policy complexity and topology may be non-monotonic in certain edge cases, such as highly regular but large graphs where pattern recognition could enable reactive policies to succeed despite high diameter.</li>
                <li>Hybrid topologies combining dense local clusters with sparse long-range connections may require novel policy architectures that blend reactive local control with global graph planning in ways current theories don't predict.</li>
                <li>In environments with very high perceptual aliasing (>50% of states visually identical), the required policy complexity may jump by two levels in the hierarchy rather than one.</li>
                <li>The interaction between action space size and topology complexity may have threshold effects where certain combinations become intractable regardless of policy architecture.</li>
                <li>In multi-agent navigation scenarios, the topology-policy relationship may be fundamentally different due to dynamic obstacles and coordination requirements.</li>
                <li>Environments with time-varying topology (doors that open/close on schedules) may require entirely new policy structures that combine temporal prediction with spatial memory.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding a reactive policy that consistently outperforms memory-based policies in high-diameter (>20 steps) environments without exploiting domain-specific regularities would challenge the core hierarchy claim.</li>
                <li>Demonstrating that explicit graph memory provides no advantage over LSTM in environments with multiple conditional dependencies and diameter >15 would contradict the theory's central predictions.</li>
                <li>Showing that policy complexity requirements decrease with topology complexity in some domain (e.g., very high diameter but simple structure) would invalidate the monotonic relationship claim.</li>
                <li>Finding that the performance gap between policy types remains constant or decreases as topology complexity increases would challenge the scaling predictions.</li>
                <li>Demonstrating that a single policy architecture (e.g., a sufficiently large transformer) can achieve optimal performance across all topology complexities would suggest the hierarchy is an artifact of limited model capacity rather than a fundamental requirement.</li>
                <li>Showing that in environments with diameter >30, hierarchical policies perform worse than flat graph-memory policies would challenge the upper end of the hierarchy.</li>
                <li>Finding that action space size has no interaction with topology complexity in determining policy requirements would contradict the multiplicative interaction claim.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact threshold values for when each policy type becomes necessary vary by domain and are not precisely specified. The theory provides approximate ranges (diameter 5, 15, 25) but these may shift by ±5 steps depending on other factors. </li>
    <li>The theory doesn't fully account for how visual complexity or perceptual aliasing interacts with topology to affect policy requirements. Evidence suggests perceptual aliasing can increase effective complexity by one level, but the precise relationship is unclear. <a href="../results/extraction-result-1348.html#e1348.1" class="evidence-link">[e1348.1]</a> <a href="../results/extraction-result-1198.html#e1198.0" class="evidence-link">[e1198.0]</a> <a href="../results/extraction-result-1246.html#e1246.0" class="evidence-link">[e1246.0]</a> </li>
    <li>The role of action space size and branching factor in determining policy complexity is mentioned but not fully integrated into the hierarchy. Evidence shows these factors matter but their precise interaction with diameter is not specified. <a href="../results/extraction-result-1349.html#e1349.1" class="evidence-link">[e1349.1]</a> <a href="../results/extraction-result-1205.html#e1205.0" class="evidence-link">[e1205.0]</a> <a href="../results/extraction-result-1267.html#e1267.1" class="evidence-link">[e1267.1]</a> </li>
    <li>The theory doesn't account for how stochasticity in transitions or observations affects the topology-policy relationship beyond noting it lowers thresholds by 20-30%. <a href="../results/extraction-result-1245.html#e1245.3" class="evidence-link">[e1245.3]</a> <a href="../results/extraction-result-1163.html#e1163.0" class="evidence-link">[e1163.0]</a> </li>
    <li>The interaction between topology and reward structure (sparse vs dense, shaped vs terminal) is not fully addressed, though evidence suggests sparse rewards increase the need for memory and planning. <a href="../results/extraction-result-1365.html#e1365.0" class="evidence-link">[e1365.0]</a> <a href="../results/extraction-result-1357.html#e1357.1" class="evidence-link">[e1357.1]</a> <a href="../results/extraction-result-1373.html#e1373.0" class="evidence-link">[e1373.0]</a> </li>
    <li>The theory doesn't specify how multi-agent scenarios or dynamic topologies (time-varying connectivity) affect policy requirements. <a href="../results/extraction-result-1247.html#e1247.0" class="evidence-link">[e1247.0]</a> </li>
    <li>The role of domain-specific priors (semantic knowledge, commonsense reasoning) in reducing effective topology complexity is mentioned but not quantified. <a href="../results/extraction-result-1364.html#e1364.0" class="evidence-link">[e1364.0]</a> <a href="../results/extraction-result-1355.html#e1355.0" class="evidence-link">[e1355.0]</a> <a href="../results/extraction-result-1237.html#e1237.0" class="evidence-link">[e1237.0]</a> </li>
    <li>The theory doesn't address how continuous vs discrete state/action spaces affect the topology-policy relationship. <a href="../results/extraction-result-1354.html#e1354.2" class="evidence-link">[e1354.2]</a> <a href="../results/extraction-result-1230.html#e1230.1" class="evidence-link">[e1230.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Sutton et al. (1999) Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning [Related work on temporal abstraction and hierarchy, but doesn't specifically address topology-policy matching or provide a complexity hierarchy]</li>
    <li>Tamar et al. (2016) Value Iteration Networks [Related work on planning in neural networks, but doesn't provide topology-based policy selection theory or complexity hierarchy]</li>
    <li>Savinov et al. (2018) Semi-parametric Topological Memory for Navigation [Demonstrates benefits of topological memory but doesn't formalize the topology-policy relationship or provide general theory]</li>
    <li>Kulkarni et al. (2016) Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation [Related work on hierarchical RL and intrinsic motivation, but doesn't connect hierarchy to topology complexity]</li>
    <li>Ecoffet et al. (2019) Go-Explore: a New Approach for Hard-Exploration Problems [Demonstrates importance of memory for hard exploration but doesn't provide general topology-policy theory]</li>
    <li>Chaplot et al. (2020) Learning to Explore using Active Neural SLAM [Shows benefits of mapping for exploration but doesn't formalize topology-policy relationship]</li>
    <li>Barto & Mahadevan (2003) Recent Advances in Hierarchical Reinforcement Learning [Review of hierarchical RL, discusses when hierarchy helps but doesn't provide topology-based theory]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Topology-Policy Complexity Matching Theory",
    "theory_description": "The optimal policy structure for navigation in text worlds and graph-structured environments must match the complexity of the environment's graph topology. Specifically, environments with high diameter, sparse connectivity, and conditional access constraints (doors/keys/dependencies) require policies with explicit memory and planning capabilities, while low-diameter, densely-connected environments can be navigated effectively with reactive policies. The theory posits a hierarchy of policy complexity requirements: (1) reactive policies suffice for simple topologies (diameter ≤5, dense connectivity), (2) recurrent memory (LSTM/GRU) is needed for moderate complexity (diameter 5-15, perceptual aliasing), (3) explicit graph memory (knowledge graphs, topological maps) is required for high complexity (diameter &gt;15, conditional dependencies, bottlenecks), and (4) hierarchical planning with graph memory is optimal for the highest complexity environments (diameter &gt;25, multiple conditional dependencies, sparse connectivity). The performance gap between optimal and suboptimal policy structures increases with topology complexity, though this relationship can be modulated by factors such as perceptual aliasing, action space size, stochasticity, and domain-specific regularities.",
    "supporting_evidence": [
        {
            "text": "In small static mazes (5x10), reactive feedforward A3C policies can perform competitively, but memory-augmented policies with auxiliary tasks (depth prediction) achieve better performance and faster learning (AUC 104.3 vs lower for feedforward).",
            "uuids": [
                "e1370.1"
            ]
        },
        {
            "text": "In random goal mazes (5x10), policies must remember discovered goal locations and learn explore-then-exploit strategies, with stacked LSTM+auxiliary tasks achieving 71.1 AUC and 85.5% position decoding accuracy.",
            "uuids": [
                "e1370.3"
            ]
        },
        {
            "text": "In larger, more complex mazes (15x15 3D ViZDoom), policies without spatial memory (LSTM-only) achieve only 52.4% success on unseen maps, while Neural Map with GRU-based spatial memory maintains 66.6% success, demonstrating the need for structured spatial memory in high-diameter environments.",
            "uuids": [
                "e1348.1"
            ]
        },
        {
            "text": "In ViZDoom maze1 and maze2, GAM with recurrent graph attention achieves 100% success on both mazes, while reactive baselines achieve only 10% on the more complex maze2, showing that long-distance navigation requires multi-step graph aggregation.",
            "uuids": [
                "e1161.0"
            ]
        },
        {
            "text": "Pure LSTM policies achieve only 22.5% success on House3D RoomNav tasks (H=1000), while hierarchical BRM with explicit semantic graph memory achieves 41.1% success, and RNN controller achieves 37.0%, demonstrating the advantage of explicit relational memory over implicit recurrent memory.",
            "uuids": [
                "e1362.1",
                "e1362.2",
                "e1362.3"
            ]
        },
        {
            "text": "In Zork1 text adventures with bottlenecks and conditional access, knowledge-graph-based policies (KG-A2C) achieve score ~34 and converge faster than text-only baselines (TDQN ~9.9), with graph mask and GAT both contributing to performance.",
            "uuids": [
                "e1205.0",
                "e1220.0"
            ]
        },
        {
            "text": "On PointGoal navigation in Matterport3D (larger scenes, average geodesic distance 11.5m), depth-equipped learned policies with memory (GRU) achieve 69% success and SPL 0.54, vs 35% success and SPL 0.25 for blind reactive policies, with performance degrading as geodesic distance and GED ratio increase.",
            "uuids": [
                "e1377.0",
                "e1369.0"
            ]
        },
        {
            "text": "In ViZDoom mazes requiring long exploration sequences (~10k steps), SPTM with explicit non-parametric topological memory achieves ~3x higher success than LSTM baselines, with visual shortcuts reducing average shortest-path from 990 to 155 steps.",
            "uuids": [
                "e1198.0",
                "e1198.2"
            ]
        },
        {
            "text": "For navigation in 25-room 2D layouts (higher diameter), hierarchical landmark-based policies (Dr. Strategy) achieve 67.11% success vs 9.62% for non-hierarchical LEXA and 0.14% for LEXA-Explore, demonstrating the necessity of hierarchical structure for large state spaces.",
            "uuids": [
                "e1272.2"
            ]
        },
        {
            "text": "In text-based CoinCollector with long chains (L30, diameter ~30), episodic discovery bonus with recurrence enables near-perfect generalization when trained on L10, while non-recurrent policies struggle, showing memory requirements scale with path length.",
            "uuids": [
                "e1174.0",
                "e1361.0"
            ]
        },
        {
            "text": "Graph-augmented policies (GATA) with dynamic belief graphs achieve +24.2% improvement over text-only baselines in TextWorld cooking games with multiple rooms and dependencies, with continuous belief graphs outperforming discrete updates.",
            "uuids": [
                "e1374.1"
            ]
        },
        {
            "text": "In Gibson indoor navigation, Active Neural SLAM with explicit map-based global planning achieves 94.8% coverage vs ~54% for random exploration, with advantages increasing in larger scenes and higher geodesic distances.",
            "uuids": [
                "e1366.0"
            ]
        },
        {
            "text": "GraphNav with behavior decomposition and topological planning achieves up to 83.3% success in structured indoor environments, with performance correlating with corridor structure and failing in large open spaces.",
            "uuids": [
                "e1347.0"
            ]
        },
        {
            "text": "LAMP with memory-augmented reactive planning achieves 10-40% reduction in travel time over baselines in repeated navigation with stochastic portal blocking, demonstrating benefits of structured memory for environments with conditional access.",
            "uuids": [
                "e1245.3"
            ]
        },
        {
            "text": "Lifelong topological visual navigation (LTvN) with controller-aware graph construction achieves 65-70% success after maintenance vs 20% before, showing that explicit topology representation enables learning from experience.",
            "uuids": [
                "e1163.0"
            ]
        },
        {
            "text": "DGMem with dynamic topological graph memory achieves better coverage and navigation performance than curiosity-driven baselines, with topological distance rewards and node-visit tracking improving exploration efficiency.",
            "uuids": [
                "e1219.0"
            ]
        },
        {
            "text": "Evolving Graphical Planner (EGP) with growing topological memory achieves 52% success on R2R validation vs 42% without message passing, with performance improving as top-K expansion increases graph connectivity.",
            "uuids": [
                "e1252.0"
            ]
        },
        {
            "text": "GMetaExp with graph-structured memory achieves 72% coverage in 2D mazes vs 33% for random and 54% for RandDFS, with autoregressive history aggregation providing ~5% improvement.",
            "uuids": [
                "e1359.0"
            ]
        },
        {
            "text": "M-Walk with RNN+MCTS achieves 99.0% test accuracy on Three Glass Puzzle vs ~49% for REINFORCE baseline, demonstrating that memory+planning is essential for sparse-reward combinatorial graphs.",
            "uuids": [
                "e1373.0"
            ]
        },
        {
            "text": "M-Walk on knowledge graphs (WN18RR, NELL-995) shows larger relative improvements for multi-step reasoning (path length 2-3) vs single-step, with history encoding (LSTM) being crucial for performance.",
            "uuids": [
                "e1373.1",
                "e1373.2"
            ]
        },
        {
            "text": "MINERVA on knowledge graphs achieves MRR 0.448 on WN18RR with history-dependent LSTM policy, with performance degrading substantially when history encoding is removed, especially on longer reasoning chains.",
            "uuids": [
                "e1375.0"
            ]
        },
        {
            "text": "Go-Explore on Montezuma's Revenge with domain-knowledge cell representation (preserving room/level/key structure) discovers mean 238 rooms vs 35 without domain knowledge, solving level 1 in 57.6M vs 640M frames.",
            "uuids": [
                "e1365.0"
            ]
        },
        {
            "text": "In deterministic chain MDPs, Bootstrapped DQN with temporally-extended exploration scales gracefully with chain length, while shallow strategies scale exponentially worse, demonstrating the necessity of temporal commitment for high-diameter topologies.",
            "uuids": [
                "e1189.0"
            ]
        },
        {
            "text": "In hierarchical DRL on 6-state chain with distant contingent reward, h-DQN with goal-conditioned policies achieves ~0.13 average reward vs ~0.01 for flat Q-learning, showing hierarchical goal structure helps in high path-length environments.",
            "uuids": [
                "e1357.0"
            ]
        },
        {
            "text": "On Montezuma's Revenge, h-DQN with entity-relation goals achieves ~400 reward per episode vs 0 for flat DQN, demonstrating that hierarchical goal-conditioned policies overcome bottlenecks in sparse-reward, high-diameter environments.",
            "uuids": [
                "e1357.1"
            ]
        },
        {
            "text": "In AI2-THOR object navigation, policies with directed object-relation graphs (DOA) achieve 74.32% success vs 69.14% baseline, with directed graphs significantly outperforming undirected, showing explicit relational structure helps in complex topologies.",
            "uuids": [
                "e1246.0"
            ]
        },
        {
            "text": "VoroNav with Voronoi-based topological graph achieves 42% success on HM3D vs lower for frontier-based methods, with intersection-based waypoints providing higher SEA and SCA than frontier selection.",
            "uuids": [
                "e1237.0"
            ]
        },
        {
            "text": "L3MVN with LLM-based frontier scoring achieves 54.2% success on HM3D validation, with feed-forward LLM outperforming zero-shot and demonstrating benefits of semantic priors for frontier selection in complex layouts.",
            "uuids": [
                "e1364.0"
            ]
        },
        {
            "text": "ESC with commonsense constraints achieves 39.2% success on HM3D, with room-level reasoning reducing exploration error and FrontierDist (7.6m vs 8.2m for baseline).",
            "uuids": [
                "e1355.0"
            ]
        },
        {
            "text": "In TextWorld home games, KG-DQN with knowledge graph memory converges ~40% faster than LSTM-DQN on small maps and achieves 73.7±8.5 steps vs much higher for baselines, with action pruning via graph reducing exploration waste.",
            "uuids": [
                "e1349.1"
            ]
        },
        {
            "text": "NESTA with abstract lifted rules achieves best performance across TextWorld Commonsense splits, with largest improvements on hard (two-room) tasks requiring multi-step chaining.",
            "uuids": [
                "e1199.1"
            ]
        },
        {
            "text": "Q*BERT and variants with knowledge-graph-based exploration and intrinsic KG-growth rewards achieve higher max scores and faster bottleneck discovery in Zork1 than baselines without structured memory.",
            "uuids": [
                "e1184.1",
                "e1220.2"
            ]
        }
    ],
    "theory_statements": [
        "Policy complexity requirements scale monotonically with graph topology complexity, measured by diameter, connectivity sparsity, conditional access constraints, and perceptual aliasing.",
        "Reactive policies are sufficient for environments where diameter ≤ 5 steps, connectivity is dense (average degree &gt; 4), and there is no perceptual aliasing or conditional access.",
        "Recurrent memory (LSTM/GRU) becomes necessary when diameter &gt; 5, when perceptual aliasing is present, or when short-term temporal dependencies must be tracked.",
        "Explicit graph memory (knowledge graphs, topological maps) is required when diameter &gt; 15, when conditional dependencies (keys/doors) create bottlenecks, or when long-term spatial relationships must be maintained.",
        "Hierarchical planning with graph memory is optimal for environments with diameter &gt; 25, multiple conditional dependencies, sparse connectivity, and complex goal structures.",
        "The performance gap between optimal and suboptimal policy structures increases monotonically with topology complexity, with gaps of 2-3x common between adjacent complexity levels.",
        "Memory capacity requirements scale with the product of diameter and branching factor, as agents must maintain information about reachable states.",
        "In environments with stochastic transitions or noisy observations, the complexity threshold for requiring each policy type decreases by approximately 20-30%.",
        "Action space size and branching factor interact multiplicatively with topology complexity to determine exploration difficulty and required policy sophistication.",
        "Domain-specific regularities (repeating patterns, strong homophily) can reduce effective topology complexity by up to one level in the hierarchy."
    ],
    "new_predictions_likely": [
        "In a text world with 50 rooms arranged in a tree structure (diameter ~25), a hierarchical policy with topological memory will achieve &gt;2x the success rate of a pure LSTM policy and &gt;5x the success rate of a reactive policy.",
        "Adding 5 locked doors requiring keys to a 10-room maze will increase the performance advantage of knowledge-graph policies over reactive policies by at least 20 percentage points and over LSTM policies by at least 10 percentage points.",
        "In environments with diameter &lt; 3 and no conditional access, the performance difference between reactive and memory-based policies will be &lt; 5%, but adding perceptual aliasing will increase this gap to &gt;15%.",
        "Increasing maze diameter from 10 to 20 steps while holding other factors constant will increase the performance gap between explicit graph memory and LSTM policies from ~10% to ~25%.",
        "In a 100-room environment with diameter 40, hierarchical planning with graph memory will achieve &gt;3x the success rate of flat graph-memory policies.",
        "Combining high diameter (&gt;20) with high branching factor (&gt;10) will require hierarchical policies even if no conditional dependencies are present.",
        "In environments with stochastic transitions, the diameter threshold for requiring explicit graph memory will decrease from 15 to approximately 10-12 steps."
    ],
    "new_predictions_unknown": [
        "In extremely high-diameter environments (&gt;100 steps), there may be a threshold beyond which even hierarchical planning fails without additional abstraction mechanisms such as multi-level hierarchies or learned macro-actions.",
        "The relationship between policy complexity and topology may be non-monotonic in certain edge cases, such as highly regular but large graphs where pattern recognition could enable reactive policies to succeed despite high diameter.",
        "Hybrid topologies combining dense local clusters with sparse long-range connections may require novel policy architectures that blend reactive local control with global graph planning in ways current theories don't predict.",
        "In environments with very high perceptual aliasing (&gt;50% of states visually identical), the required policy complexity may jump by two levels in the hierarchy rather than one.",
        "The interaction between action space size and topology complexity may have threshold effects where certain combinations become intractable regardless of policy architecture.",
        "In multi-agent navigation scenarios, the topology-policy relationship may be fundamentally different due to dynamic obstacles and coordination requirements.",
        "Environments with time-varying topology (doors that open/close on schedules) may require entirely new policy structures that combine temporal prediction with spatial memory."
    ],
    "negative_experiments": [
        "Finding a reactive policy that consistently outperforms memory-based policies in high-diameter (&gt;20 steps) environments without exploiting domain-specific regularities would challenge the core hierarchy claim.",
        "Demonstrating that explicit graph memory provides no advantage over LSTM in environments with multiple conditional dependencies and diameter &gt;15 would contradict the theory's central predictions.",
        "Showing that policy complexity requirements decrease with topology complexity in some domain (e.g., very high diameter but simple structure) would invalidate the monotonic relationship claim.",
        "Finding that the performance gap between policy types remains constant or decreases as topology complexity increases would challenge the scaling predictions.",
        "Demonstrating that a single policy architecture (e.g., a sufficiently large transformer) can achieve optimal performance across all topology complexities would suggest the hierarchy is an artifact of limited model capacity rather than a fundamental requirement.",
        "Showing that in environments with diameter &gt;30, hierarchical policies perform worse than flat graph-memory policies would challenge the upper end of the hierarchy.",
        "Finding that action space size has no interaction with topology complexity in determining policy requirements would contradict the multiplicative interaction claim."
    ],
    "unaccounted_for": [
        {
            "text": "The exact threshold values for when each policy type becomes necessary vary by domain and are not precisely specified. The theory provides approximate ranges (diameter 5, 15, 25) but these may shift by ±5 steps depending on other factors.",
            "uuids": []
        },
        {
            "text": "The theory doesn't fully account for how visual complexity or perceptual aliasing interacts with topology to affect policy requirements. Evidence suggests perceptual aliasing can increase effective complexity by one level, but the precise relationship is unclear.",
            "uuids": [
                "e1348.1",
                "e1198.0",
                "e1246.0"
            ]
        },
        {
            "text": "The role of action space size and branching factor in determining policy complexity is mentioned but not fully integrated into the hierarchy. Evidence shows these factors matter but their precise interaction with diameter is not specified.",
            "uuids": [
                "e1349.1",
                "e1205.0",
                "e1267.1"
            ]
        },
        {
            "text": "The theory doesn't account for how stochasticity in transitions or observations affects the topology-policy relationship beyond noting it lowers thresholds by 20-30%.",
            "uuids": [
                "e1245.3",
                "e1163.0"
            ]
        },
        {
            "text": "The interaction between topology and reward structure (sparse vs dense, shaped vs terminal) is not fully addressed, though evidence suggests sparse rewards increase the need for memory and planning.",
            "uuids": [
                "e1365.0",
                "e1357.1",
                "e1373.0"
            ]
        },
        {
            "text": "The theory doesn't specify how multi-agent scenarios or dynamic topologies (time-varying connectivity) affect policy requirements.",
            "uuids": [
                "e1247.0"
            ]
        },
        {
            "text": "The role of domain-specific priors (semantic knowledge, commonsense reasoning) in reducing effective topology complexity is mentioned but not quantified.",
            "uuids": [
                "e1364.0",
                "e1355.0",
                "e1237.0"
            ]
        },
        {
            "text": "The theory doesn't address how continuous vs discrete state/action spaces affect the topology-policy relationship.",
            "uuids": [
                "e1354.2",
                "e1230.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some grid-based pathfinding tasks, learned heuristics (PHIL) show only 58.5% reduction in node expansions compared to baselines, suggesting diminishing returns of complex policies in certain structured topologies with regular patterns.",
            "uuids": [
                "e1356.0"
            ]
        },
        {
            "text": "On Barabasi graphs, simple nearest-neighbor heuristics (NN) achieve 0.8179 exploration rate vs 0.7214 for learned long-horizon NOGE policies, suggesting topology-specific exceptions where local greedy strategies outperform learned planning.",
            "uuids": [
                "e1217.0"
            ]
        },
        {
            "text": "On city road networks (Munich, San Francisco), nearest-neighbor heuristics substantially outperform learned policies (NN: 0.8314-0.9017 vs NOGE: 0.6458-0.7541), indicating that certain real-world topologies favor simple local strategies.",
            "uuids": [
                "e1217.6",
                "e1217.8"
            ]
        },
        {
            "text": "In optimal learning paths on information networks, the relationship between connectivity and coverage time is non-monotonic, with optimal performance at intermediate connectivity rather than monotonically improving with policy complexity.",
            "uuids": [
                "e1194.0",
                "e1194.2"
            ]
        },
        {
            "text": "In Wikipedia navigation, the optimal policy structure is position-dependent, with degree-weighting important early but similarity-weighting important later, suggesting the hierarchy may be too coarse-grained.",
            "uuids": [
                "e1376.0"
            ]
        },
        {
            "text": "On ladder graphs, classical DFS and NN (0.753) outperform learned NOGE policies (0.6046), suggesting that certain elongated topologies favor simple traversal rules over learned planning.",
            "uuids": [
                "e1217.3"
            ]
        },
        {
            "text": "In RoboKitchen manipulation tasks (short-horizon, fully observable), strategic dreaming provides limited benefit over simpler goal-conditioned policies, suggesting the hierarchy may not apply to all domains.",
            "uuids": [
                "e1272.5"
            ]
        },
        {
            "text": "Frontier-based exploration outperforms learned methods when state estimation is perfect (no noise), suggesting that in certain conditions, classical planning can match or exceed learned memory-based policies.",
            "uuids": [
                "e1170.1"
            ]
        }
    ],
    "special_cases": [
        "In deterministic environments with perfect state observability and no perceptual aliasing, planning-based policies may achieve near-optimal performance regardless of topology complexity, potentially skipping intermediate hierarchy levels.",
        "Environments with strong regularities or repeating patterns (e.g., regular grids, periodic structures) may enable reactive policies to succeed even at high diameter through pattern memorization, effectively reducing the topology complexity by one level.",
        "Real-world robotic navigation requires additional considerations beyond graph topology, such as sensor noise, actuation uncertainty, and continuous state/action spaces, which may shift threshold values by 20-30%.",
        "In environments with very sparse rewards (only terminal rewards), the required policy complexity may increase by one level in the hierarchy regardless of topology, as exploration becomes the dominant challenge.",
        "Scale-free networks with hub-and-spoke structure may enable simpler policies than their diameter would suggest, as hub-based routing can be learned reactively.",
        "Environments with strong semantic structure (e.g., household layouts with predictable room adjacencies) may allow policies to leverage domain priors to reduce effective complexity by one level.",
        "In multi-agent scenarios, the effective topology complexity may increase due to dynamic obstacles, requiring policies one level higher in the hierarchy than single-agent equivalents.",
        "Environments with time-varying topology (doors that open/close, dynamic obstacles) may require hybrid policies that don't fit cleanly into the hierarchy.",
        "Very high branching factor (&gt;20) combined with moderate diameter (10-15) may require hierarchical policies even without conditional dependencies, suggesting branching factor can substitute for diameter in determining complexity.",
        "In environments where the goal changes frequently (e.g., random goal mazes), the required policy complexity may increase by one level to support rapid adaptation."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Sutton et al. (1999) Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning [Related work on temporal abstraction and hierarchy, but doesn't specifically address topology-policy matching or provide a complexity hierarchy]",
            "Tamar et al. (2016) Value Iteration Networks [Related work on planning in neural networks, but doesn't provide topology-based policy selection theory or complexity hierarchy]",
            "Savinov et al. (2018) Semi-parametric Topological Memory for Navigation [Demonstrates benefits of topological memory but doesn't formalize the topology-policy relationship or provide general theory]",
            "Kulkarni et al. (2016) Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation [Related work on hierarchical RL and intrinsic motivation, but doesn't connect hierarchy to topology complexity]",
            "Ecoffet et al. (2019) Go-Explore: a New Approach for Hard-Exploration Problems [Demonstrates importance of memory for hard exploration but doesn't provide general topology-policy theory]",
            "Chaplot et al. (2020) Learning to Explore using Active Neural SLAM [Shows benefits of mapping for exploration but doesn't formalize topology-policy relationship]",
            "Barto & Mahadevan (2003) Recent Advances in Hierarchical Reinforcement Learning [Review of hierarchical RL, discusses when hierarchy helps but doesn't provide topology-based theory]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
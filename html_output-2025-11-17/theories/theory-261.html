<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation Metric Misalignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-261</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-261</p>
                <p><strong>Name:</strong> Evaluation Metric Misalignment Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about faithfulness gaps between natural language descriptions and code implementations in automated experimentation.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that faithfulness gaps between natural language descriptions and code implementations in automated experimentation arise systematically when evaluation metrics are specified. The theory posits that natural language descriptions of metrics contain inherent ambiguities across multiple dimensions: (1) aggregation methods (how to combine multiple values), (2) normalization and scaling choices (whether and how to normalize metrics), (3) handling of edge cases (division by zero, missing values, empty sets), (4) statistical summarization (which summary statistics to report), and (5) implicit assumptions about data distributions. Code implementations must resolve these ambiguities by making concrete choices, and different valid resolutions can lead to substantially different experimental conclusions. This misalignment is particularly pernicious because natural language metric descriptions often appear precise and unambiguous to human readers, masking the multiple valid interpretations that exist at the implementation level. The theory predicts that metric misalignment increases with metric complexity, the number of aggregation steps, and the heterogeneity of data being evaluated.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>When natural language descriptions use terms like 'average', 'overall', 'combined', or 'aggregate' without specifying the mathematical method, code implementations will default to arithmetic mean in the majority of cases, regardless of whether this is appropriate for the data distribution.</li>
                <li>Evaluation metric misalignment is most severe when combining metrics across heterogeneous conditions (different datasets, different scales, different distributions), where multiple implementation choices (weighted vs unweighted, different aggregation functions, different normalizations) can each be justified but lead to different conclusions.</li>
                <li>Natural language descriptions that specify 'typical' or 'representative' performance are ambiguous between mean, median, and mode, with different implementations choosing different operationalizations based on implementer background and conventions.</li>
                <li>The impact of metric misalignment on experimental conclusions increases with: (a) the number of aggregation steps, (b) the variance among items being aggregated, (c) the presence of outliers or skewed distributions, and (d) the heterogeneity of evaluation conditions.</li>
                <li>Edge case handling (division by zero, empty sets, missing values, undefined metrics) is rarely specified in natural language descriptions but must be resolved in code, leading to silent divergences in implementation behavior.</li>
                <li>Metric normalization and scaling choices (whether to normalize, what range to use, whether to apply transformations) are often implicit in natural language descriptions but critically affect comparability across conditions.</li>
                <li>When natural language descriptions reference established metrics by name (e.g., 'F1 score', 'BLEU', 'accuracy'), implementers may use different versions or variants of these metrics that have evolved over time, leading to misalignment even when the metric name is specified.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Research on benchmark aggregation in machine learning demonstrates that different aggregation methods (arithmetic mean, geometric mean, harmonic mean) can change the ranking of models, showing that implementation choices have significant impact on conclusions. </li>
    <li>Studies on meta-analysis in medical research demonstrate that aggregation method choice significantly impacts conclusions, with different methods being appropriate for different data characteristics, establishing precedent for aggregation ambiguity. </li>
    <li>Statistical literature documents that arithmetic and geometric means can diverge substantially, particularly with skewed distributions or outliers, demonstrating that seemingly simple terms like 'average' have multiple valid mathematical interpretations. </li>
    <li>Research on classification metrics shows that terms like 'accuracy' and 'precision' have multiple valid operationalizations (micro vs macro averaging, weighted vs unweighted), and the choice significantly affects which models appear superior. </li>
    <li>Studies of reproducibility in science demonstrate that underspecification of methodological details, including metric computation, is a major source of irreproducibility, suggesting that natural language descriptions systematically fail to capture implementation details. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In a corpus of automated experimentation code implementing 'average' or 'mean' from natural language descriptions, the majority will use arithmetic mean, even in cases where data characteristics (e.g., log-normal distributions, multiplicative effects, ratio metrics) would make geometric mean or harmonic mean more appropriate.</li>
                <li>When researchers are shown experimental results computed with different valid aggregation methods (arithmetic vs geometric mean) for the same data, they will prefer different methods depending on how results are framed, indicating that the 'correct' choice is context-dependent and not uniquely determined by the natural language description.</li>
                <li>Code implementations that aggregate across datasets of different sizes will frequently fail to use sample-size weighting when the natural language description says 'average across datasets' without specifying weighting, leading to biased estimates.</li>
                <li>When natural language descriptions specify metrics for multi-class classification (e.g., 'precision' or 'recall'), implementations will vary between micro-averaging, macro-averaging, and weighted averaging, with different choices favoring different experimental conditions.</li>
                <li>Implementations of the same natural language metric description by different researchers will handle edge cases (division by zero, empty predictions, missing data) differently, leading to divergent results on the same data.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Automatically detecting when metric implementation choices might impact experimental conclusions (e.g., by computing multiple valid implementations and checking for rank changes in model comparisons) might reveal that a substantial fraction of published results are sensitive to implementation choices, or might show that most results are robust.</li>
                <li>Requiring researchers to explicitly specify all metric implementation details (aggregation method, edge case handling, normalization) in natural language descriptions might either improve faithfulness substantially or might lead to cargo-cult specification where researchers specify details without understanding their implications, potentially introducing new errors.</li>
                <li>Developing formal specification languages for metrics that eliminate ambiguity might either be adopted widely and improve reproducibility, or might be rejected as too burdensome, or might simply shift the faithfulness gap to a different level of abstraction.</li>
                <li>The prevalence of metric misalignment might vary dramatically across scientific fields based on disciplinary conventions and training, or might be relatively uniform, indicating a fundamental cognitive challenge in translating natural language to code.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If experimental conclusions are found to be robust to metric implementation choices across a wide range of real experiments (e.g., model rankings don't change with different aggregation methods), this would challenge the practical importance of this theory.</li>
                <li>If researchers, when explicitly asked, can consistently and reliably specify all implementation details needed to eliminate ambiguity in metric descriptions, this would challenge the claim that natural language descriptions are inherently ambiguous.</li>
                <li>If automated tools can reliably infer the 'intended' metric implementation from natural language descriptions with high accuracy, this would suggest the ambiguity is resolvable and not fundamental.</li>
                <li>If different implementations of the same metric description produce highly correlated results across diverse datasets, this would suggest that implementation choices have minimal practical impact.</li>
                <li>If disciplinary conventions fully determine metric implementation choices (e.g., everyone in a field uses the same aggregation method by default), this would limit the scope of misalignment to cross-disciplinary work.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The role of software libraries and frameworks in establishing de facto standards for metric implementation, which may reduce misalignment within communities that use the same tools but increase it across communities using different tools. </li>
    <li>The extent to which code review, peer review, and replication attempts catch metric misalignment before publication, which could mitigate the impact of initial implementation choices. </li>
    <li>The interaction between metric misalignment and other sources of faithfulness gaps (e.g., data preprocessing, model configuration), which may compound or cancel each other out. </li>
    <li>The role of implicit knowledge and disciplinary training in resolving ambiguities, which may make certain implementation choices 'obvious' to experts even when not explicitly stated. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Fleming & Wallace (1986) How not to lie with statistics: the correct way to summarize benchmark results, Communications of the ACM [Discusses aggregation in benchmarking but doesn't frame it as a natural language to code faithfulness gap in automated experimentation]</li>
    <li>Dehghani et al. (2021) The Benchmark Lottery, arXiv [Shows that aggregation method choice matters for model rankings but doesn't theorize about the translation gap between natural language descriptions and implementations]</li>
    <li>Ethayarajh & Jurafsky (2020) Utility is in the Eye of the User: A Critique of NLP Leaderboards, EMNLP [Critiques evaluation practices but doesn't focus on the faithfulness gap between descriptions and code]</li>
    <li>Sokolova & Lapalme (2009) A systematic analysis of performance measures for classification tasks, Information Processing & Management [Documents multiple valid interpretations of classification metrics but doesn't frame this as a faithfulness problem]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Evaluation Metric Misalignment Theory",
    "theory_description": "This theory proposes that faithfulness gaps between natural language descriptions and code implementations in automated experimentation arise systematically when evaluation metrics are specified. The theory posits that natural language descriptions of metrics contain inherent ambiguities across multiple dimensions: (1) aggregation methods (how to combine multiple values), (2) normalization and scaling choices (whether and how to normalize metrics), (3) handling of edge cases (division by zero, missing values, empty sets), (4) statistical summarization (which summary statistics to report), and (5) implicit assumptions about data distributions. Code implementations must resolve these ambiguities by making concrete choices, and different valid resolutions can lead to substantially different experimental conclusions. This misalignment is particularly pernicious because natural language metric descriptions often appear precise and unambiguous to human readers, masking the multiple valid interpretations that exist at the implementation level. The theory predicts that metric misalignment increases with metric complexity, the number of aggregation steps, and the heterogeneity of data being evaluated.",
    "supporting_evidence": [
        {
            "text": "Research on benchmark aggregation in machine learning demonstrates that different aggregation methods (arithmetic mean, geometric mean, harmonic mean) can change the ranking of models, showing that implementation choices have significant impact on conclusions.",
            "citations": [
                "Dehghani et al. (2021) The Benchmark Lottery, arXiv",
                "Ethayarajh & Jurafsky (2020) Utility is in the Eye of the User: A Critique of NLP Leaderboards, EMNLP"
            ]
        },
        {
            "text": "Studies on meta-analysis in medical research demonstrate that aggregation method choice significantly impacts conclusions, with different methods being appropriate for different data characteristics, establishing precedent for aggregation ambiguity.",
            "citations": [
                "Borenstein et al. (2009) Introduction to Meta-Analysis, Wiley"
            ]
        },
        {
            "text": "Statistical literature documents that arithmetic and geometric means can diverge substantially, particularly with skewed distributions or outliers, demonstrating that seemingly simple terms like 'average' have multiple valid mathematical interpretations.",
            "citations": [
                "Fleming & Wallace (1986) How not to lie with statistics: the correct way to summarize benchmark results, Communications of the ACM"
            ]
        },
        {
            "text": "Research on classification metrics shows that terms like 'accuracy' and 'precision' have multiple valid operationalizations (micro vs macro averaging, weighted vs unweighted), and the choice significantly affects which models appear superior.",
            "citations": [
                "Sokolova & Lapalme (2009) A systematic analysis of performance measures for classification tasks, Information Processing & Management"
            ]
        },
        {
            "text": "Studies of reproducibility in science demonstrate that underspecification of methodological details, including metric computation, is a major source of irreproducibility, suggesting that natural language descriptions systematically fail to capture implementation details.",
            "citations": [
                "Ioannidis (2005) Why Most Published Research Findings Are False, PLOS Medicine"
            ]
        }
    ],
    "theory_statements": [
        "When natural language descriptions use terms like 'average', 'overall', 'combined', or 'aggregate' without specifying the mathematical method, code implementations will default to arithmetic mean in the majority of cases, regardless of whether this is appropriate for the data distribution.",
        "Evaluation metric misalignment is most severe when combining metrics across heterogeneous conditions (different datasets, different scales, different distributions), where multiple implementation choices (weighted vs unweighted, different aggregation functions, different normalizations) can each be justified but lead to different conclusions.",
        "Natural language descriptions that specify 'typical' or 'representative' performance are ambiguous between mean, median, and mode, with different implementations choosing different operationalizations based on implementer background and conventions.",
        "The impact of metric misalignment on experimental conclusions increases with: (a) the number of aggregation steps, (b) the variance among items being aggregated, (c) the presence of outliers or skewed distributions, and (d) the heterogeneity of evaluation conditions.",
        "Edge case handling (division by zero, empty sets, missing values, undefined metrics) is rarely specified in natural language descriptions but must be resolved in code, leading to silent divergences in implementation behavior.",
        "Metric normalization and scaling choices (whether to normalize, what range to use, whether to apply transformations) are often implicit in natural language descriptions but critically affect comparability across conditions.",
        "When natural language descriptions reference established metrics by name (e.g., 'F1 score', 'BLEU', 'accuracy'), implementers may use different versions or variants of these metrics that have evolved over time, leading to misalignment even when the metric name is specified."
    ],
    "new_predictions_likely": [
        "In a corpus of automated experimentation code implementing 'average' or 'mean' from natural language descriptions, the majority will use arithmetic mean, even in cases where data characteristics (e.g., log-normal distributions, multiplicative effects, ratio metrics) would make geometric mean or harmonic mean more appropriate.",
        "When researchers are shown experimental results computed with different valid aggregation methods (arithmetic vs geometric mean) for the same data, they will prefer different methods depending on how results are framed, indicating that the 'correct' choice is context-dependent and not uniquely determined by the natural language description.",
        "Code implementations that aggregate across datasets of different sizes will frequently fail to use sample-size weighting when the natural language description says 'average across datasets' without specifying weighting, leading to biased estimates.",
        "When natural language descriptions specify metrics for multi-class classification (e.g., 'precision' or 'recall'), implementations will vary between micro-averaging, macro-averaging, and weighted averaging, with different choices favoring different experimental conditions.",
        "Implementations of the same natural language metric description by different researchers will handle edge cases (division by zero, empty predictions, missing data) differently, leading to divergent results on the same data."
    ],
    "new_predictions_unknown": [
        "Automatically detecting when metric implementation choices might impact experimental conclusions (e.g., by computing multiple valid implementations and checking for rank changes in model comparisons) might reveal that a substantial fraction of published results are sensitive to implementation choices, or might show that most results are robust.",
        "Requiring researchers to explicitly specify all metric implementation details (aggregation method, edge case handling, normalization) in natural language descriptions might either improve faithfulness substantially or might lead to cargo-cult specification where researchers specify details without understanding their implications, potentially introducing new errors.",
        "Developing formal specification languages for metrics that eliminate ambiguity might either be adopted widely and improve reproducibility, or might be rejected as too burdensome, or might simply shift the faithfulness gap to a different level of abstraction.",
        "The prevalence of metric misalignment might vary dramatically across scientific fields based on disciplinary conventions and training, or might be relatively uniform, indicating a fundamental cognitive challenge in translating natural language to code."
    ],
    "negative_experiments": [
        "If experimental conclusions are found to be robust to metric implementation choices across a wide range of real experiments (e.g., model rankings don't change with different aggregation methods), this would challenge the practical importance of this theory.",
        "If researchers, when explicitly asked, can consistently and reliably specify all implementation details needed to eliminate ambiguity in metric descriptions, this would challenge the claim that natural language descriptions are inherently ambiguous.",
        "If automated tools can reliably infer the 'intended' metric implementation from natural language descriptions with high accuracy, this would suggest the ambiguity is resolvable and not fundamental.",
        "If different implementations of the same metric description produce highly correlated results across diverse datasets, this would suggest that implementation choices have minimal practical impact.",
        "If disciplinary conventions fully determine metric implementation choices (e.g., everyone in a field uses the same aggregation method by default), this would limit the scope of misalignment to cross-disciplinary work."
    ],
    "unaccounted_for": [
        {
            "text": "The role of software libraries and frameworks in establishing de facto standards for metric implementation, which may reduce misalignment within communities that use the same tools but increase it across communities using different tools.",
            "citations": [
                "Pedregosa et al. (2011) Scikit-learn: Machine Learning in Python, JMLR"
            ]
        },
        {
            "text": "The extent to which code review, peer review, and replication attempts catch metric misalignment before publication, which could mitigate the impact of initial implementation choices.",
            "citations": [
                "Ioannidis (2005) Why Most Published Research Findings Are False, PLOS Medicine"
            ]
        },
        {
            "text": "The interaction between metric misalignment and other sources of faithfulness gaps (e.g., data preprocessing, model configuration), which may compound or cancel each other out.",
            "citations": []
        },
        {
            "text": "The role of implicit knowledge and disciplinary training in resolving ambiguities, which may make certain implementation choices 'obvious' to experts even when not explicitly stated.",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some domains, strong conventions exist for metric implementation (e.g., macro vs micro averaging in multi-class classification, specific formulations of BLEU or ROUGE in NLP), potentially limiting the scope of ambiguity within those domains.",
            "citations": [
                "Sokolova & Lapalme (2009) A systematic analysis of performance measures for classification tasks, Information Processing & Management"
            ]
        },
        {
            "text": "Many widely-used metrics have canonical implementations in standard libraries (e.g., scikit-learn), which may reduce implementation variation when researchers use these libraries, though this doesn't eliminate misalignment when descriptions don't specify which library or version to use.",
            "citations": [
                "Pedregosa et al. (2011) Scikit-learn: Machine Learning in Python, JMLR"
            ]
        }
    ],
    "special_cases": [
        "When aggregating over a single dimension with uniform conditions and no missing data, different aggregation methods often produce similar results, minimizing the impact of implementation choices.",
        "For metrics that are already normalized to a common scale (e.g., 0-1 range) and have similar distributions across conditions, weighted versus unweighted aggregation may produce similar results.",
        "When natural language descriptions include explicit mathematical formulas or pseudocode, the scope for misalignment is reduced, though ambiguities in notation and edge cases may still exist.",
        "For simple, well-established metrics with long-standing conventions (e.g., mean squared error for regression), misalignment is less likely than for complex, composite, or newly-defined metrics.",
        "When evaluation is performed on a single dataset with a single metric and no aggregation, metric misalignment is minimal, though edge case handling may still vary.",
        "In fields with strong standardization efforts (e.g., shared task competitions with official evaluation scripts), misalignment within that community is reduced, though cross-community comparisons may still suffer."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Fleming & Wallace (1986) How not to lie with statistics: the correct way to summarize benchmark results, Communications of the ACM [Discusses aggregation in benchmarking but doesn't frame it as a natural language to code faithfulness gap in automated experimentation]",
            "Dehghani et al. (2021) The Benchmark Lottery, arXiv [Shows that aggregation method choice matters for model rankings but doesn't theorize about the translation gap between natural language descriptions and implementations]",
            "Ethayarajh & Jurafsky (2020) Utility is in the Eye of the User: A Critique of NLP Leaderboards, EMNLP [Critiques evaluation practices but doesn't focus on the faithfulness gap between descriptions and code]",
            "Sokolova & Lapalme (2009) A systematic analysis of performance measures for classification tasks, Information Processing & Management [Documents multiple valid interpretations of classification metrics but doesn't frame this as a faithfulness problem]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 1,
    "theory_query": "Build a theory about faithfulness gaps between natural language descriptions and code implementations in automated experimentation.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-95",
    "original_theory_name": "Evaluation Metric Misalignment Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1882</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1882</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format of problem presentation acts as an information bottleneck, modulating the amount and type of information accessible to the LLM's internal processing. Formats that efficiently encode relevant task information and minimize irrelevant or distracting content enable the LLM to allocate more capacity to reasoning and solution generation, thereby improving performance. Conversely, formats that introduce extraneous information or obscure task-relevant cues reduce effective performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Relevant Information Maximization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; task_relevant_information<span style="color: #888888;">, and</span></div>
        <div>&#8226; problem_presentation_format &#8594; minimizes &#8594; irrelevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_optimized &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better when prompts are concise and focused on the task, with minimal irrelevant context. </li>
    <li>Adding extraneous information or distractors to prompts reduces LLM accuracy and increases error rates. </li>
    <li>Prompt compression and summarization techniques that retain only essential information improve LLM performance. </li>
    <li>Instruction tuning datasets that filter out irrelevant content yield more robust LLMs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to prompt engineering, the explicit information-theoretic framing is novel.</p>            <p><strong>What Already Exists:</strong> Conciseness and focus in prompts are known to improve LLM performance.</p>            <p><strong>What is Novel:</strong> This law frames the effect as an information bottleneck, emphasizing the tradeoff between relevant and irrelevant information in the input.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Stepwise prompts reduce irrelevant information]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and information content]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Relevant vs. irrelevant context in prompts]</li>
</ul>
            <h3>Statement 1: Distractor Interference Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; irrelevant_distractors</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; decreases &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Experiments show that adding distractor sentences or irrelevant context to prompts reduces LLM accuracy. </li>
    <li>LLMs are susceptible to being misled by irrelevant or adversarial information in the prompt. </li>
    <li>Prompt-based adversarial attacks often succeed by introducing distractors that obscure the task. </li>
    <li>Filtering out distractors from prompts restores or improves LLM performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The distractor effect is known, but its integration into a general information bottleneck theory is novel.</p>            <p><strong>What Already Exists:</strong> Prompt-based adversarial attacks and distractor effects are documented in LLM literature.</p>            <p><strong>What is Novel:</strong> This law formalizes the effect as a general principle of information bottlenecking in LLMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2022) Prompting Language Models for Knowledge Base Construction [Distractor effects in prompts]</li>
    <li>Wallace et al. (2019) Universal Adversarial Triggers for Attacking and Analyzing NLP [Adversarial distractors]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Role of irrelevant context]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt is rewritten to remove all irrelevant or distracting information, LLM performance will improve.</li>
                <li>If distractor sentences are added to a prompt, LLM accuracy will decrease compared to a clean prompt.</li>
                <li>If a prompt is compressed to only the essential task information, LLMs will be more robust to adversarial attacks.</li>
                <li>If LLMs are trained with prompts containing distractors, their performance on clean prompts will be lower than models trained on clean data.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained with a mix of distractor-rich and distractor-free prompts, can they learn to ignore distractors entirely?</li>
                <li>Does the effect of distractors diminish as LLM scale increases, or do larger models become more susceptible to subtle distractors?</li>
                <li>Can LLMs be fine-tuned to identify and filter out irrelevant information in real time?</li>
                <li>Are there types of distractors that paradoxically improve performance by forcing the model to focus more carefully?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If adding distractors to prompts does not reduce LLM performance, the theory would be challenged.</li>
                <li>If removing irrelevant information from prompts does not improve performance, the theory would be falsified.</li>
                <li>If LLMs perform better on distractor-rich prompts than on clean prompts, the theory would be contradicted.</li>
                <li>If LLMs are unaffected by the amount of irrelevant information in the prompt, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs ignore distractors and perform well due to robust internal representations. </li>
    <li>Instances where certain types of distractors (e.g., motivational context) improve engagement and performance. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known distractor effects into a formal, information-theoretic framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Relevant vs. irrelevant context]</li>
    <li>Wallace et al. (2019) Universal Adversarial Triggers for Attacking and Analyzing NLP [Adversarial distractors]</li>
    <li>Jiang et al. (2022) Prompting Language Models for Knowledge Base Construction [Distractor effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of LLM Problem Presentation",
    "theory_description": "This theory proposes that the format of problem presentation acts as an information bottleneck, modulating the amount and type of information accessible to the LLM's internal processing. Formats that efficiently encode relevant task information and minimize irrelevant or distracting content enable the LLM to allocate more capacity to reasoning and solution generation, thereby improving performance. Conversely, formats that introduce extraneous information or obscure task-relevant cues reduce effective performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Relevant Information Maximization Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "task_relevant_information"
                    },
                    {
                        "subject": "problem_presentation_format",
                        "relation": "minimizes",
                        "object": "irrelevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_optimized",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better when prompts are concise and focused on the task, with minimal irrelevant context.",
                        "uuids": []
                    },
                    {
                        "text": "Adding extraneous information or distractors to prompts reduces LLM accuracy and increases error rates.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt compression and summarization techniques that retain only essential information improve LLM performance.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction tuning datasets that filter out irrelevant content yield more robust LLMs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Conciseness and focus in prompts are known to improve LLM performance.",
                    "what_is_novel": "This law frames the effect as an information bottleneck, emphasizing the tradeoff between relevant and irrelevant information in the input.",
                    "classification_explanation": "While related to prompt engineering, the explicit information-theoretic framing is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Stepwise prompts reduce irrelevant information]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and information content]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Relevant vs. irrelevant context in prompts]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Distractor Interference Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "irrelevant_distractors"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "decreases",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Experiments show that adding distractor sentences or irrelevant context to prompts reduces LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs are susceptible to being misled by irrelevant or adversarial information in the prompt.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt-based adversarial attacks often succeed by introducing distractors that obscure the task.",
                        "uuids": []
                    },
                    {
                        "text": "Filtering out distractors from prompts restores or improves LLM performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt-based adversarial attacks and distractor effects are documented in LLM literature.",
                    "what_is_novel": "This law formalizes the effect as a general principle of information bottlenecking in LLMs.",
                    "classification_explanation": "The distractor effect is known, but its integration into a general information bottleneck theory is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2022) Prompting Language Models for Knowledge Base Construction [Distractor effects in prompts]",
                        "Wallace et al. (2019) Universal Adversarial Triggers for Attacking and Analyzing NLP [Adversarial distractors]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Role of irrelevant context]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt is rewritten to remove all irrelevant or distracting information, LLM performance will improve.",
        "If distractor sentences are added to a prompt, LLM accuracy will decrease compared to a clean prompt.",
        "If a prompt is compressed to only the essential task information, LLMs will be more robust to adversarial attacks.",
        "If LLMs are trained with prompts containing distractors, their performance on clean prompts will be lower than models trained on clean data."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained with a mix of distractor-rich and distractor-free prompts, can they learn to ignore distractors entirely?",
        "Does the effect of distractors diminish as LLM scale increases, or do larger models become more susceptible to subtle distractors?",
        "Can LLMs be fine-tuned to identify and filter out irrelevant information in real time?",
        "Are there types of distractors that paradoxically improve performance by forcing the model to focus more carefully?"
    ],
    "negative_experiments": [
        "If adding distractors to prompts does not reduce LLM performance, the theory would be challenged.",
        "If removing irrelevant information from prompts does not improve performance, the theory would be falsified.",
        "If LLMs perform better on distractor-rich prompts than on clean prompts, the theory would be contradicted.",
        "If LLMs are unaffected by the amount of irrelevant information in the prompt, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs ignore distractors and perform well due to robust internal representations.",
            "uuids": []
        },
        {
            "text": "Instances where certain types of distractors (e.g., motivational context) improve engagement and performance.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some large LLMs show resilience to distractors, maintaining high performance even with irrelevant information present.",
            "uuids": []
        },
        {
            "text": "There are cases where removing all context, including some irrelevant information, reduces LLM performance due to loss of implicit cues.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For highly trained LLMs, distractor effects may be mitigated by scale or additional training.",
        "In tasks requiring creativity or open-ended responses, some irrelevant information may stimulate better outputs.",
        "Certain distractors may act as implicit cues, aiding rather than hindering performance."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and adversarial prompt research have documented distractor effects.",
        "what_is_novel": "The explicit information bottleneck framing and the generalization to all forms of irrelevant information are new.",
        "classification_explanation": "The theory synthesizes known distractor effects into a formal, information-theoretic framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Relevant vs. irrelevant context]",
            "Wallace et al. (2019) Universal Adversarial Triggers for Attacking and Analyzing NLP [Adversarial distractors]",
            "Jiang et al. (2022) Prompting Language Models for Knowledge Base Construction [Distractor effects]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-652",
    "original_theory_name": "Prompt Format as a High-Dimensional Control Signal for LLM Computation",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
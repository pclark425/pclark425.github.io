<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Induced Calibration Distortion Theory (Information Bottleneck Formulation) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1860</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1860</p>
                <p><strong>Name:</strong> Prompt-Induced Calibration Distortion Theory (Information Bottleneck Formulation)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory posits that the act of prompting LLMs for probability estimates on future scientific discoveries compresses complex, high-dimensional epistemic uncertainty into a low-dimensional output (a single probability or distribution). This information bottleneck, especially under repeated or constrained prompting, systematically distorts calibration by forcing the model to overfit to prompt structure, prior training data, or salient features, rather than the true underlying uncertainty.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt Compression Induces Calibration Distortion (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; low-bandwidth_probability_query<span style="color: #888888;">, and</span></div>
        <div>&#8226; epistemic_uncertainty &#8594; is_high_dimensional &#8594; true</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; output_probability &#8594; is_distorted_by &#8594; information_bottleneck</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Information bottleneck theory shows that compressing high-dimensional information into low-dimensional outputs can induce systematic distortions. </li>
    <li>LLMs prompted for single-number probabilities must aggregate complex evidence, often leading to overconfidence or underconfidence. </li>
    <li>Empirical studies show that LLMs' probability estimates are sensitive to prompt wording and structure. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general bottleneck effect is known, but its specific impact on LLM probability calibration for scientific discovery is novel.</p>            <p><strong>What Already Exists:</strong> Information bottleneck effects are known in ML and neuroscience.</p>            <p><strong>What is Novel:</strong> Application to LLM calibration distortion in scientific forecasting is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby et al. (2000) The Information Bottleneck Method [Information bottleneck theory]</li>
    <li>Jiang et al. (2021) How Can We Know What Language Models Know? [Prompt sensitivity in LLMs]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration and prompt effects]</li>
</ul>
            <h3>Statement 1: Prompt Structure Overfitting Distorts Calibration (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_repeatedly_with &#8594; similar_structured_queries</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; output_probability &#8594; is_biased_toward &#8594; prompt_structure_induced_prior</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are known to overfit to prompt structure, leading to systematic biases in outputs. </li>
    <li>Prompt engineering can shift LLM probability estimates even when underlying evidence is unchanged. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Prompt overfitting is known, but its systematic effect on calibration in this context is not previously formalized.</p>            <p><strong>What Already Exists:</strong> Prompt sensitivity and overfitting are known in LLMs.</p>            <p><strong>What is Novel:</strong> Explicit link to calibration distortion in scientific probability estimation is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2021) How Can We Know What Language Models Know? [Prompt sensitivity]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt structure effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If LLMs are prompted with more expressive, higher-bandwidth queries (e.g., asking for distributions or rationales), calibration distortion will decrease.</li>
                <li>Calibration distortion will be greater for scientific questions with higher epistemic uncertainty.</li>
                <li>Prompting LLMs with varied structures will reduce systematic calibration bias compared to repeated use of a single prompt template.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained to output full epistemic distributions rather than point estimates, calibration may improve or new forms of distortion may emerge.</li>
                <li>If LLMs are prompted with meta-cognitive queries (e.g., 'how confident are you in your confidence?'), the information bottleneck may be partially bypassed.</li>
                <li>If LLMs are given access to external epistemic tools (e.g., Bayesian calculators), calibration distortion may be mitigated.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If increasing prompt expressiveness does not reduce calibration distortion, the information bottleneck mechanism would be called into question.</li>
                <li>If LLMs show perfect calibration regardless of prompt structure, the theory would be undermined.</li>
                <li>If LLMs' probability estimates are not sensitive to prompt structure, the theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs with explicit calibration correction or post-processing may not exhibit bottleneck-induced distortion. </li>
    <li>In cases where the underlying uncertainty is low-dimensional, the information bottleneck may not induce distortion. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory extends information bottleneck effects to a new domain and formalizes their impact on LLM probability calibration.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby et al. (2000) The Information Bottleneck Method [Information bottleneck theory]</li>
    <li>Jiang et al. (2021) How Can We Know What Language Models Know? [Prompt sensitivity]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Induced Calibration Distortion Theory (Information Bottleneck Formulation)",
    "theory_description": "This theory posits that the act of prompting LLMs for probability estimates on future scientific discoveries compresses complex, high-dimensional epistemic uncertainty into a low-dimensional output (a single probability or distribution). This information bottleneck, especially under repeated or constrained prompting, systematically distorts calibration by forcing the model to overfit to prompt structure, prior training data, or salient features, rather than the true underlying uncertainty.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt Compression Induces Calibration Distortion",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "low-bandwidth_probability_query"
                    },
                    {
                        "subject": "epistemic_uncertainty",
                        "relation": "is_high_dimensional",
                        "object": "true"
                    }
                ],
                "then": [
                    {
                        "subject": "output_probability",
                        "relation": "is_distorted_by",
                        "object": "information_bottleneck"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Information bottleneck theory shows that compressing high-dimensional information into low-dimensional outputs can induce systematic distortions.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs prompted for single-number probabilities must aggregate complex evidence, often leading to overconfidence or underconfidence.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that LLMs' probability estimates are sensitive to prompt wording and structure.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Information bottleneck effects are known in ML and neuroscience.",
                    "what_is_novel": "Application to LLM calibration distortion in scientific forecasting is new.",
                    "classification_explanation": "The general bottleneck effect is known, but its specific impact on LLM probability calibration for scientific discovery is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tishby et al. (2000) The Information Bottleneck Method [Information bottleneck theory]",
                        "Jiang et al. (2021) How Can We Know What Language Models Know? [Prompt sensitivity in LLMs]",
                        "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration and prompt effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Structure Overfitting Distorts Calibration",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_repeatedly_with",
                        "object": "similar_structured_queries"
                    }
                ],
                "then": [
                    {
                        "subject": "output_probability",
                        "relation": "is_biased_toward",
                        "object": "prompt_structure_induced_prior"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are known to overfit to prompt structure, leading to systematic biases in outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering can shift LLM probability estimates even when underlying evidence is unchanged.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt sensitivity and overfitting are known in LLMs.",
                    "what_is_novel": "Explicit link to calibration distortion in scientific probability estimation is new.",
                    "classification_explanation": "Prompt overfitting is known, but its systematic effect on calibration in this context is not previously formalized.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2021) How Can We Know What Language Models Know? [Prompt sensitivity]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt structure effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If LLMs are prompted with more expressive, higher-bandwidth queries (e.g., asking for distributions or rationales), calibration distortion will decrease.",
        "Calibration distortion will be greater for scientific questions with higher epistemic uncertainty.",
        "Prompting LLMs with varied structures will reduce systematic calibration bias compared to repeated use of a single prompt template."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained to output full epistemic distributions rather than point estimates, calibration may improve or new forms of distortion may emerge.",
        "If LLMs are prompted with meta-cognitive queries (e.g., 'how confident are you in your confidence?'), the information bottleneck may be partially bypassed.",
        "If LLMs are given access to external epistemic tools (e.g., Bayesian calculators), calibration distortion may be mitigated."
    ],
    "negative_experiments": [
        "If increasing prompt expressiveness does not reduce calibration distortion, the information bottleneck mechanism would be called into question.",
        "If LLMs show perfect calibration regardless of prompt structure, the theory would be undermined.",
        "If LLMs' probability estimates are not sensitive to prompt structure, the theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs with explicit calibration correction or post-processing may not exhibit bottleneck-induced distortion.",
            "uuids": []
        },
        {
            "text": "In cases where the underlying uncertainty is low-dimensional, the information bottleneck may not induce distortion.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robust calibration across a range of prompt structures, suggesting other mechanisms may be at play.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For scientific questions with low epistemic uncertainty, calibration distortion from the information bottleneck may be negligible.",
        "If LLMs are prompted with open-ended rationales rather than single probabilities, the bottleneck effect may be reduced."
    ],
    "existing_theory": {
        "what_already_exists": "Information bottleneck and prompt sensitivity are known in ML.",
        "what_is_novel": "The explicit theory of prompt-induced calibration distortion via information bottleneck in LLM scientific forecasting is new.",
        "classification_explanation": "This theory extends information bottleneck effects to a new domain and formalizes their impact on LLM probability calibration.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tishby et al. (2000) The Information Bottleneck Method [Information bottleneck theory]",
            "Jiang et al. (2021) How Can We Know What Language Models Know? [Prompt sensitivity]",
            "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-650",
    "original_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
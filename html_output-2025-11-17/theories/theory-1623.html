<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Epistemic Alignment Theory of LLM Simulation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1623</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1623</p>
                <p><strong>Name:</strong> Epistemic Alignment Theory of LLM Simulation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory asserts that the degree to which prompts and demonstrations encode the epistemic norms, representational forms, and inferential structures of a scientific subdomain determines the upper bound of LLM simulation accuracy. The theory posits that LLMs act as epistemic mirrors, and that misalignment between prompt/demonstration structure and the subdomain's knowledge practices leads to systematic simulation errors, regardless of model size or training data.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Epistemic Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt and demonstration structure &#8594; encodes &#8594; epistemic norms and inferential forms of target subdomain</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; approaches &#8594; theoretical maximum for that model</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best when prompts and demonstrations use the same representational forms (e.g., equations, diagrams, stepwise logic) as domain experts. </li>
    <li>Studies show that LLMs are sensitive to the explicitness and structure of scientific reasoning in prompts. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While related to prompt engineering, the explicit epistemic alignment framing is new.</p>            <p><strong>What Already Exists:</strong> Prompt engineering and domain adaptation are known, but explicit epistemic alignment as a limiting factor is not widely formalized.</p>            <p><strong>What is Novel:</strong> The law's focus on epistemic encoding and its role in bounding LLM simulation accuracy is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Kocon et al. (2023) Prompt Engineering for Large Language Models: A Survey [Prompt engineering, but not epistemic alignment]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate reasoning, but not epistemic alignment]</li>
</ul>
            <h3>Statement 1: Epistemic Misalignment Error Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt and demonstration structure &#8594; is_misaligned_with &#8594; epistemic norms of target subdomain</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation &#8594; exhibits &#8594; systematic errors characteristic of misalignment</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>When prompts use non-standard or ambiguous forms, LLMs often produce errors that reflect misunderstanding of domain conventions. </li>
    <li>Empirical evidence shows that LLMs can be led astray by prompts that do not match the inferential structure of the scientific task. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law generalizes prompt misalignment to epistemic misalignment, which is a new framing.</p>            <p><strong>What Already Exists:</strong> Prompt misalignment is known to cause errors, but systematic epistemic misalignment as a source of error is not formalized.</p>            <p><strong>What is Novel:</strong> The law's explicit connection between epistemic misalignment and systematic simulation errors is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt content matters, but not epistemic misalignment]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure matters, but not epistemic misalignment]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If prompts and demonstrations are constructed to closely mirror the epistemic practices of a scientific subdomain, LLM simulation accuracy will increase.</li>
                <li>Systematic errors in LLM simulation can be predicted by analyzing mismatches between prompt structure and subdomain epistemic norms.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>In subdomains with evolving or contested epistemic norms, LLM simulation accuracy may fluctuate unpredictably with prompt structure.</li>
                <li>If LLMs are exposed to hybrid epistemic forms in prompts, they may develop novel error patterns not seen in either source domain.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs achieve high simulation accuracy with prompts that are epistemically misaligned with the target subdomain, the theory would be challenged.</li>
                <li>If systematic errors do not correlate with epistemic misalignment, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs perform well due to memorization or surface-level pattern matching, rather than epistemic alignment. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory introduces a new, epistemic lens to the analysis of prompt and demonstration structure in LLM simulation.</p>
            <p><strong>References:</strong> <ul>
    <li>Kocon et al. (2023) Prompt Engineering for Large Language Models: A Survey [Prompt engineering, not epistemic alignment]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate reasoning, not epistemic alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Epistemic Alignment Theory of LLM Simulation",
    "theory_description": "This theory asserts that the degree to which prompts and demonstrations encode the epistemic norms, representational forms, and inferential structures of a scientific subdomain determines the upper bound of LLM simulation accuracy. The theory posits that LLMs act as epistemic mirrors, and that misalignment between prompt/demonstration structure and the subdomain's knowledge practices leads to systematic simulation errors, regardless of model size or training data.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Epistemic Encoding Law",
                "if": [
                    {
                        "subject": "prompt and demonstration structure",
                        "relation": "encodes",
                        "object": "epistemic norms and inferential forms of target subdomain"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "approaches",
                        "object": "theoretical maximum for that model"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best when prompts and demonstrations use the same representational forms (e.g., equations, diagrams, stepwise logic) as domain experts.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that LLMs are sensitive to the explicitness and structure of scientific reasoning in prompts.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering and domain adaptation are known, but explicit epistemic alignment as a limiting factor is not widely formalized.",
                    "what_is_novel": "The law's focus on epistemic encoding and its role in bounding LLM simulation accuracy is novel.",
                    "classification_explanation": "While related to prompt engineering, the explicit epistemic alignment framing is new.",
                    "likely_classification": "new",
                    "references": [
                        "Kocon et al. (2023) Prompt Engineering for Large Language Models: A Survey [Prompt engineering, but not epistemic alignment]",
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate reasoning, but not epistemic alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Epistemic Misalignment Error Law",
                "if": [
                    {
                        "subject": "prompt and demonstration structure",
                        "relation": "is_misaligned_with",
                        "object": "epistemic norms of target subdomain"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation",
                        "relation": "exhibits",
                        "object": "systematic errors characteristic of misalignment"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "When prompts use non-standard or ambiguous forms, LLMs often produce errors that reflect misunderstanding of domain conventions.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical evidence shows that LLMs can be led astray by prompts that do not match the inferential structure of the scientific task.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt misalignment is known to cause errors, but systematic epistemic misalignment as a source of error is not formalized.",
                    "what_is_novel": "The law's explicit connection between epistemic misalignment and systematic simulation errors is novel.",
                    "classification_explanation": "The law generalizes prompt misalignment to epistemic misalignment, which is a new framing.",
                    "likely_classification": "new",
                    "references": [
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt content matters, but not epistemic misalignment]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure matters, but not epistemic misalignment]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If prompts and demonstrations are constructed to closely mirror the epistemic practices of a scientific subdomain, LLM simulation accuracy will increase.",
        "Systematic errors in LLM simulation can be predicted by analyzing mismatches between prompt structure and subdomain epistemic norms."
    ],
    "new_predictions_unknown": [
        "In subdomains with evolving or contested epistemic norms, LLM simulation accuracy may fluctuate unpredictably with prompt structure.",
        "If LLMs are exposed to hybrid epistemic forms in prompts, they may develop novel error patterns not seen in either source domain."
    ],
    "negative_experiments": [
        "If LLMs achieve high simulation accuracy with prompts that are epistemically misaligned with the target subdomain, the theory would be challenged.",
        "If systematic errors do not correlate with epistemic misalignment, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs perform well due to memorization or surface-level pattern matching, rather than epistemic alignment.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can answer certain scientific questions correctly even when prompts are not epistemically aligned, possibly due to overfitting or memorization.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In subdomains with weak or ambiguous epistemic norms, the effect of alignment may be less pronounced.",
        "For tasks solvable by pattern recognition alone, epistemic alignment may not be necessary."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and domain adaptation are known, but not the explicit epistemic alignment framing.",
        "what_is_novel": "The theory's focus on epistemic alignment as the key limiting factor for LLM scientific simulation is novel.",
        "classification_explanation": "The theory introduces a new, epistemic lens to the analysis of prompt and demonstration structure in LLM simulation.",
        "likely_classification": "new",
        "references": [
            "Kocon et al. (2023) Prompt Engineering for Large Language Models: A Survey [Prompt engineering, not epistemic alignment]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate reasoning, not epistemic alignment]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-635",
    "original_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
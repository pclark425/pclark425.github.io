<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simulation Boundary Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1664</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1664</p>
                <p><strong>Name:</strong> Simulation Boundary Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> The accuracy of LLMs as text-based simulators for scientific subdomains is fundamentally bounded by the intersection of the LLM's learned distribution (as determined by its training data and architecture) and the formal and informal rules of the subdomain. Simulation accuracy is highest where these boundaries overlap, and degrades rapidly outside this intersection.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Distributional Boundary Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; subdomain phenomena &#8594; are_within &#8594; LLM's learned distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; is_high &#8594; for those phenomena</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs excel at simulating phenomena that are well-represented in their training data. </li>
    <li>Empirical studies show LLMs perform best on tasks and domains with high training data coverage. </li>
    <li>Benchmarks in scientific QA and simulation show strong performance in well-represented subdomains. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is closely related to existing work on distributional generalization, but the explicit simulation boundary framing is novel.</p>            <p><strong>What Already Exists:</strong> The importance of training data coverage for LLM performance is well-known.</p>            <p><strong>What is Novel:</strong> This law formalizes the simulation boundary as the intersection of learned distribution and subdomain rules.</p>
            <p><strong>References:</strong> <ul>
    <li>Radford et al. (2019) Language Models are Unsupervised Multitask Learners [distributional generalization]</li>
    <li>Kumar et al. (2022) Language Models are Few-Shot Learners [distributional coverage and generalization]</li>
</ul>
            <h3>Statement 1: Boundary Degradation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; subdomain phenomena &#8594; are_outside &#8594; LLM's learned distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; degrades_rapidly &#8594; for those phenomena</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs produce nonsensical or incorrect outputs for phenomena not represented in training data. </li>
    <li>Studies of LLM hallucination show sharp accuracy drops for out-of-distribution queries. </li>
    <li>Empirical evidence from scientific simulation tasks shows rapid degradation outside covered subdomains. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The boundary framing is a novel synthesis of known generalization limits.</p>            <p><strong>What Already Exists:</strong> LLMs are known to hallucinate or fail outside their training distribution.</p>            <p><strong>What is Novel:</strong> This law frames the failure as a boundary phenomenon, not just a gradual degradation.</p>
            <p><strong>References:</strong> <ul>
    <li>Ji et al. (2023) Survey of Hallucination in Natural Language Generation [out-of-distribution errors]</li>
    <li>Marcus & Davis (2020) GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about [distributional failures]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a subdomain's phenomena are artificially shifted outside the LLM's training distribution, simulation accuracy will drop sharply.</li>
                <li>If the LLM is exposed to new data that expands its distributional boundary to include more of the subdomain, simulation accuracy will increase.</li>
                <li>LLMs will show high simulation accuracy for canonical textbook problems but low accuracy for novel, unpublished phenomena.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If the LLM is prompted with meta-knowledge about its own distributional boundaries, it may self-limit and avoid hallucination.</li>
                <li>If the LLM is trained with adversarial examples at the boundary, it may learn to generalize beyond its original simulation boundary.</li>
                <li>If the LLM is fine-tuned on synthetic data representing the boundary, it may develop new forms of generalization.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs maintain high simulation accuracy for phenomena far outside their training distribution, the theory would be challenged.</li>
                <li>If simulation accuracy degrades gradually rather than sharply at the boundary, the boundary framing would be questioned.</li>
                <li>If LLMs can simulate phenomena with no training data or analogs, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs interpolate or extrapolate successfully beyond their training distribution due to underlying structure in the data. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is a novel synthesis of known generalization limits and simulation accuracy.</p>
            <p><strong>References:</strong> <ul>
    <li>Radford et al. (2019) Language Models are Unsupervised Multitask Learners [distributional generalization]</li>
    <li>Ji et al. (2023) Survey of Hallucination in Natural Language Generation [out-of-distribution errors]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Simulation Boundary Theory",
    "theory_description": "The accuracy of LLMs as text-based simulators for scientific subdomains is fundamentally bounded by the intersection of the LLM's learned distribution (as determined by its training data and architecture) and the formal and informal rules of the subdomain. Simulation accuracy is highest where these boundaries overlap, and degrades rapidly outside this intersection.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Distributional Boundary Law",
                "if": [
                    {
                        "subject": "subdomain phenomena",
                        "relation": "are_within",
                        "object": "LLM's learned distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "is_high",
                        "object": "for those phenomena"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs excel at simulating phenomena that are well-represented in their training data.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LLMs perform best on tasks and domains with high training data coverage.",
                        "uuids": []
                    },
                    {
                        "text": "Benchmarks in scientific QA and simulation show strong performance in well-represented subdomains.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The importance of training data coverage for LLM performance is well-known.",
                    "what_is_novel": "This law formalizes the simulation boundary as the intersection of learned distribution and subdomain rules.",
                    "classification_explanation": "The law is closely related to existing work on distributional generalization, but the explicit simulation boundary framing is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Radford et al. (2019) Language Models are Unsupervised Multitask Learners [distributional generalization]",
                        "Kumar et al. (2022) Language Models are Few-Shot Learners [distributional coverage and generalization]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Boundary Degradation Law",
                "if": [
                    {
                        "subject": "subdomain phenomena",
                        "relation": "are_outside",
                        "object": "LLM's learned distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "degrades_rapidly",
                        "object": "for those phenomena"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs produce nonsensical or incorrect outputs for phenomena not represented in training data.",
                        "uuids": []
                    },
                    {
                        "text": "Studies of LLM hallucination show sharp accuracy drops for out-of-distribution queries.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical evidence from scientific simulation tasks shows rapid degradation outside covered subdomains.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to hallucinate or fail outside their training distribution.",
                    "what_is_novel": "This law frames the failure as a boundary phenomenon, not just a gradual degradation.",
                    "classification_explanation": "The boundary framing is a novel synthesis of known generalization limits.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Ji et al. (2023) Survey of Hallucination in Natural Language Generation [out-of-distribution errors]",
                        "Marcus & Davis (2020) GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about [distributional failures]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a subdomain's phenomena are artificially shifted outside the LLM's training distribution, simulation accuracy will drop sharply.",
        "If the LLM is exposed to new data that expands its distributional boundary to include more of the subdomain, simulation accuracy will increase.",
        "LLMs will show high simulation accuracy for canonical textbook problems but low accuracy for novel, unpublished phenomena."
    ],
    "new_predictions_unknown": [
        "If the LLM is prompted with meta-knowledge about its own distributional boundaries, it may self-limit and avoid hallucination.",
        "If the LLM is trained with adversarial examples at the boundary, it may learn to generalize beyond its original simulation boundary.",
        "If the LLM is fine-tuned on synthetic data representing the boundary, it may develop new forms of generalization."
    ],
    "negative_experiments": [
        "If LLMs maintain high simulation accuracy for phenomena far outside their training distribution, the theory would be challenged.",
        "If simulation accuracy degrades gradually rather than sharply at the boundary, the boundary framing would be questioned.",
        "If LLMs can simulate phenomena with no training data or analogs, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs interpolate or extrapolate successfully beyond their training distribution due to underlying structure in the data.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show surprising zero-shot generalization to novel subdomains, possibly due to compositionality or latent structure.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Subdomains with highly compositional or modular structure may allow LLMs to generalize beyond their nominal distributional boundary.",
        "LLMs with explicit reasoning modules or retrieval augmentation may partially overcome boundary effects."
    ],
    "existing_theory": {
        "what_already_exists": "Distributional generalization and out-of-distribution failure are well-studied.",
        "what_is_novel": "The explicit simulation boundary framing, and the focus on the intersection with subdomain rules, is novel.",
        "classification_explanation": "The theory is a novel synthesis of known generalization limits and simulation accuracy.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Radford et al. (2019) Language Models are Unsupervised Multitask Learners [distributional generalization]",
            "Ji et al. (2023) Survey of Hallucination in Natural Language Generation [out-of-distribution errors]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-638",
    "original_theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
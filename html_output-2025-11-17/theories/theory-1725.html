<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Statistical Expectation Modeling by Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1725</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1725</p>
                <p><strong>Name:</strong> Statistical Expectation Modeling by Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can be used for detecting anomalies in lists of data.</p>
                <p><strong>Description:</strong> Language models, when trained on large corpora, implicitly learn the statistical regularities and co-occurrence patterns of data sequences. When applied to lists of data, these models can estimate the likelihood of each element or subsequence. Elements that deviate significantly from the learned statistical expectations are flagged as anomalies, as they are less probable under the model's learned distribution.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Low Probability Indicates Anomaly (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; element &#8594; is_in &#8594; list<span style="color: #888888;">, and</span></div>
        <div>&#8226; language_model &#8594; assigns_probability &#8594; p(element|context)<span style="color: #888888;">, and</span></div>
        <div>&#8226; p(element|context) &#8594; is_much_lower_than &#8594; expected_probability_for_typical_elements</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; element &#8594; is_flagged_as &#8594; anomaly</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Language models assign lower probabilities to out-of-distribution or rare tokens, which is used in outlier and anomaly detection in NLP and tabular data. </li>
    <li>Empirical studies show that token-level perplexity spikes on anomalous or corrupted data. </li>
    <li>Likelihood-based anomaly detection is a standard approach in generative modeling, including for language models. </li>
    <li>Language models have been used to detect anomalies in code, text, and tabular data by identifying low-likelihood elements. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Closely related to existing work on LM-based OOD detection, but the explicit formalization for general lists and the thresholding mechanism is a novel abstraction.</p>            <p><strong>What Already Exists:</strong> The use of language model likelihoods for out-of-distribution detection is established in NLP and some tabular anomaly detection work.</p>            <p><strong>What is Novel:</strong> This law generalizes the principle to arbitrary lists and formalizes the thresholding mechanism for anomaly detection.</p>
            <p><strong>References:</strong> <ul>
    <li>Hendrycks (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [LMs assign low probability to OOD tokens]</li>
    <li>Goldstein & Uchida (2016) A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data [Anomaly detection via statistical deviation]</li>
    <li>Ren et al. (2019) Time-Series Anomaly Detection Service at Microsoft [Likelihood-based anomaly detection in sequences]</li>
</ul>
            <h3>Statement 1: Contextual Regularity Drives Detection (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; list &#8594; has_elements &#8594; e1, e2, ..., en<span style="color: #888888;">, and</span></div>
        <div>&#8226; language_model &#8594; has_learned &#8594; contextual_patterns<span style="color: #888888;">, and</span></div>
        <div>&#8226; element &#8594; violates &#8594; contextual_pattern</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; element &#8594; is_likely &#8594; anomaly</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs capture not just unigram frequencies but also higher-order dependencies, so contextually inconsistent elements are assigned lower probabilities. </li>
    <li>Empirical results show that LMs can detect swapped or misplaced items in structured lists (e.g., in code, tables, or text). </li>
    <li>Contextual anomaly detection is a key feature in time-series and sequence modeling literature. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Somewhat related to existing work, but the abstraction to arbitrary list structures and explicit context violation is novel.</p>            <p><strong>What Already Exists:</strong> LMs are known to model context and dependencies, and context-based anomaly detection is used in NLP.</p>            <p><strong>What is Novel:</strong> This law formalizes the role of contextual regularity in anomaly detection for arbitrary lists, not just text.</p>
            <p><strong>References:</strong> <ul>
    <li>Devlin et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Contextual modeling]</li>
    <li>Ren et al. (2019) Time-Series Anomaly Detection Service at Microsoft [Contextual anomaly detection in sequences]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a language model is trained on lists of valid product SKUs, it will assign low probability to a SKU with an invalid format or rare prefix, flagging it as an anomaly.</li>
                <li>If a list of city names is input to a language model, and a non-city word is inserted, the model will assign it a lower probability and flag it as anomalous.</li>
                <li>If a language model is trained on lists of valid chemical formulas, it will flag syntactically invalid or rare formulas as anomalies.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a language model is trained on lists with subtle, high-order dependencies (e.g., alternating patterns), it may or may not detect anomalies that break these patterns, depending on model capacity.</li>
                <li>If a list contains adversarially crafted elements that mimic high-probability tokens but are semantically anomalous, the model's ability to detect them is uncertain.</li>
                <li>If a language model is exposed to lists with non-stationary distributions, its anomaly detection performance may vary unpredictably.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a language model assigns high probability to elements that are known to be anomalous (e.g., random strings in a list of valid words), this would challenge the theory.</li>
                <li>If the model fails to flag elements with low probability as anomalies in human-judged anomalous cases, the theory is called into question.</li>
                <li>If the model cannot distinguish between contextually plausible but factually incorrect elements and true anomalies, the theory's scope is limited.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where anomalies are not statistical but semantic (e.g., contextually plausible but factually wrong entries). </li>
    <li>Anomalies that are frequent in the training data may not be flagged as anomalies by the model. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is closely related to existing work but provides a more general and formal abstraction.</p>
            <p><strong>References:</strong> <ul>
    <li>Hendrycks (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [LMs assign low probability to OOD tokens]</li>
    <li>Goldstein & Uchida (2016) A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data [Anomaly detection via statistical deviation]</li>
    <li>Ren et al. (2019) Time-Series Anomaly Detection Service at Microsoft [Likelihood-based anomaly detection in sequences]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Statistical Expectation Modeling by Language Models",
    "theory_description": "Language models, when trained on large corpora, implicitly learn the statistical regularities and co-occurrence patterns of data sequences. When applied to lists of data, these models can estimate the likelihood of each element or subsequence. Elements that deviate significantly from the learned statistical expectations are flagged as anomalies, as they are less probable under the model's learned distribution.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Low Probability Indicates Anomaly",
                "if": [
                    {
                        "subject": "element",
                        "relation": "is_in",
                        "object": "list"
                    },
                    {
                        "subject": "language_model",
                        "relation": "assigns_probability",
                        "object": "p(element|context)"
                    },
                    {
                        "subject": "p(element|context)",
                        "relation": "is_much_lower_than",
                        "object": "expected_probability_for_typical_elements"
                    }
                ],
                "then": [
                    {
                        "subject": "element",
                        "relation": "is_flagged_as",
                        "object": "anomaly"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Language models assign lower probabilities to out-of-distribution or rare tokens, which is used in outlier and anomaly detection in NLP and tabular data.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that token-level perplexity spikes on anomalous or corrupted data.",
                        "uuids": []
                    },
                    {
                        "text": "Likelihood-based anomaly detection is a standard approach in generative modeling, including for language models.",
                        "uuids": []
                    },
                    {
                        "text": "Language models have been used to detect anomalies in code, text, and tabular data by identifying low-likelihood elements.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "The use of language model likelihoods for out-of-distribution detection is established in NLP and some tabular anomaly detection work.",
                    "what_is_novel": "This law generalizes the principle to arbitrary lists and formalizes the thresholding mechanism for anomaly detection.",
                    "classification_explanation": "Closely related to existing work on LM-based OOD detection, but the explicit formalization for general lists and the thresholding mechanism is a novel abstraction.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Hendrycks (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [LMs assign low probability to OOD tokens]",
                        "Goldstein & Uchida (2016) A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data [Anomaly detection via statistical deviation]",
                        "Ren et al. (2019) Time-Series Anomaly Detection Service at Microsoft [Likelihood-based anomaly detection in sequences]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Contextual Regularity Drives Detection",
                "if": [
                    {
                        "subject": "list",
                        "relation": "has_elements",
                        "object": "e1, e2, ..., en"
                    },
                    {
                        "subject": "language_model",
                        "relation": "has_learned",
                        "object": "contextual_patterns"
                    },
                    {
                        "subject": "element",
                        "relation": "violates",
                        "object": "contextual_pattern"
                    }
                ],
                "then": [
                    {
                        "subject": "element",
                        "relation": "is_likely",
                        "object": "anomaly"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs capture not just unigram frequencies but also higher-order dependencies, so contextually inconsistent elements are assigned lower probabilities.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical results show that LMs can detect swapped or misplaced items in structured lists (e.g., in code, tables, or text).",
                        "uuids": []
                    },
                    {
                        "text": "Contextual anomaly detection is a key feature in time-series and sequence modeling literature.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LMs are known to model context and dependencies, and context-based anomaly detection is used in NLP.",
                    "what_is_novel": "This law formalizes the role of contextual regularity in anomaly detection for arbitrary lists, not just text.",
                    "classification_explanation": "Somewhat related to existing work, but the abstraction to arbitrary list structures and explicit context violation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Devlin et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Contextual modeling]",
                        "Ren et al. (2019) Time-Series Anomaly Detection Service at Microsoft [Contextual anomaly detection in sequences]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a language model is trained on lists of valid product SKUs, it will assign low probability to a SKU with an invalid format or rare prefix, flagging it as an anomaly.",
        "If a list of city names is input to a language model, and a non-city word is inserted, the model will assign it a lower probability and flag it as anomalous.",
        "If a language model is trained on lists of valid chemical formulas, it will flag syntactically invalid or rare formulas as anomalies."
    ],
    "new_predictions_unknown": [
        "If a language model is trained on lists with subtle, high-order dependencies (e.g., alternating patterns), it may or may not detect anomalies that break these patterns, depending on model capacity.",
        "If a list contains adversarially crafted elements that mimic high-probability tokens but are semantically anomalous, the model's ability to detect them is uncertain.",
        "If a language model is exposed to lists with non-stationary distributions, its anomaly detection performance may vary unpredictably."
    ],
    "negative_experiments": [
        "If a language model assigns high probability to elements that are known to be anomalous (e.g., random strings in a list of valid words), this would challenge the theory.",
        "If the model fails to flag elements with low probability as anomalies in human-judged anomalous cases, the theory is called into question.",
        "If the model cannot distinguish between contextually plausible but factually incorrect elements and true anomalies, the theory's scope is limited."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where anomalies are not statistical but semantic (e.g., contextually plausible but factually wrong entries).",
            "uuids": []
        },
        {
            "text": "Anomalies that are frequent in the training data may not be flagged as anomalies by the model.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that LMs can be overconfident on OOD data, assigning high probability to anomalous elements.",
            "uuids": []
        },
        {
            "text": "Language models trained on noisy or biased data may learn to assign high probability to anomalous or incorrect elements.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Lists with highly non-stationary or adversarial distributions may not be well modeled by the LM, reducing anomaly detection accuracy.",
        "If the training data is itself anomalous or noisy, the LM may learn to assign high probability to anomalies.",
        "Anomalies that are subtle or require world knowledge may not be detected by statistical modeling alone."
    ],
    "existing_theory": {
        "what_already_exists": "LM-based anomaly detection via likelihood is established in NLP and some tabular data work.",
        "what_is_novel": "The generalization to arbitrary lists and the explicit formalization of statistical expectation modeling as a theory is novel.",
        "classification_explanation": "The theory is closely related to existing work but provides a more general and formal abstraction.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Hendrycks (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [LMs assign low probability to OOD tokens]",
            "Goldstein & Uchida (2016) A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data [Anomaly detection via statistical deviation]",
            "Ren et al. (2019) Time-Series Anomaly Detection Service at Microsoft [Likelihood-based anomaly detection in sequences]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can be used for detecting anomalies in lists of data.",
    "original_theory_id": "theory-642",
    "original_theory_name": "Generalized Language Model Representation and Adaptation Theory for Anomaly Detection in Lists and Tabular Data",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
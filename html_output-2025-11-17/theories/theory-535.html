<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Selective, Task-Aligned Evaluation Theory for LLM-Generated Scientific Theories - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-535</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-535</p>
                <p><strong>Name:</strong> Selective, Task-Aligned Evaluation Theory for LLM-Generated Scientific Theories</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that the evaluation of LLM-generated scientific theories must be selectively aligned to the intended downstream use-case or application, and that different evaluation metrics (e.g., sample quality, likelihood, task-specific utility) can yield divergent model rankings. The theory claims that good performance on one evaluation metric or task does not guarantee good performance on another, and that evaluation pipelines must be explicitly matched to the scientific or practical goals of the LLM output.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Task-Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; an LLM-generated scientific theory &#8594; is_evaluated &#8594; for a specific downstream application</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the evaluation metric &#8594; must_be_selected &#8594; to match the requirements and success criteria of that application</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Surrogate task evaluation (classification, denoising, inpainting, compression) shows that models should be evaluated directly on the application they are intended for; good sample quality is not necessary for good application performance and vice versa. <a href="../results/extraction-result-3968.html#e3968.6" class="evidence-link">[e3968.6]</a> </li>
    <li>Evaluation limitations in generative models: visual fidelity, log-likelihood, and MMD can yield different model rankings; sample quality is not a reliable proxy for density estimation or downstream task performance. <a href="../results/extraction-result-3968.html#e3968.2" class="evidence-link">[e3968.2]</a> <a href="../results/extraction-result-3968.html#e3968.0" class="evidence-link">[e3968.0]</a> <a href="../results/extraction-result-3968.html#e3968.3" class="evidence-link">[e3968.3]</a> <a href="../results/extraction-result-3968.html#e3968.4" class="evidence-link">[e3968.4]</a> <a href="../results/extraction-result-3968.html#e3968.1" class="evidence-link">[e3968.1]</a> </li>
    <li>Automated metrics and ensembling in ReAct and CoT: self-consistency and majority-vote improve reliability for some tasks but not others; evaluation must match the target application. <a href="../results/extraction-result-3967.html#e3967.2" class="evidence-link">[e3967.2]</a> <a href="../results/extraction-result-3975.html#e3975.3" class="evidence-link">[e3975.3]</a> </li>
    <li>Automatic evaluation for text-to-SQL and formal outputs: formal correctness and conciseness are appropriate for code/SQL, but not for open-ended scientific theory generation. <a href="../results/extraction-result-3808.html#e3808.0" class="evidence-link">[e3808.0]</a> </li>
    <li>Evaluation criteria list for process mining: recommends matching evaluation metrics to the specific capability or output type (e.g., coding, visual, factuality, recall/precision, formal correctness, conciseness, fairness). <a href="../results/extraction-result-3808.html#e3808.4" class="evidence-link">[e3808.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Metric Divergence Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; an LLM &#8594; performs_well &#8594; on one evaluation metric (e.g., sample quality, likelihood, task-specific utility)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the same LLM &#8594; may_perform_poorly &#8594; on another metric or task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Surrogate task evaluation: mixture models can have high likelihood yet produce poor unconditional samples but still give good posteriors for task-conditioned problems; evaluation must match the target application. <a href="../results/extraction-result-3968.html#e3968.6" class="evidence-link">[e3968.6]</a> </li>
    <li>Parzen window estimates, MMD, and JSD can rank models differently than log-likelihood or visual fidelity; models that memorize training data can have high sample quality but poor generalization. <a href="../results/extraction-result-3968.html#e3968.1" class="evidence-link">[e3968.1]</a> <a href="../results/extraction-result-3968.html#e3968.3" class="evidence-link">[e3968.3]</a> <a href="../results/extraction-result-3968.html#e3968.4" class="evidence-link">[e3968.4]</a> <a href="../results/extraction-result-3968.html#e3968.2" class="evidence-link">[e3968.2]</a> </li>
    <li>Nearest-neighbor sample comparison can fail to detect overfitting; visual inspection is unreliable for density estimation. <a href="../results/extraction-result-3968.html#e3968.5" class="evidence-link">[e3968.5]</a> <a href="../results/extraction-result-3968.html#e3968.2" class="evidence-link">[e3968.2]</a> </li>
    <li>Automated metrics (e.g., BLEU) do not reliably indicate functional correctness in code generation; pass@k is a better metric for code tasks. <a href="../results/extraction-result-3978.html#e3978.2" class="evidence-link">[e3978.2]</a> <a href="../results/extraction-result-3978.html#e3978.0" class="evidence-link">[e3978.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a model is evaluated for scientific hypothesis generation using only sample quality (e.g., fluency), it may rank differently than if evaluated for downstream utility (e.g., ability to generate actionable hypotheses).</li>
                <li>If a model is optimized for log-likelihood, it may not produce the most useful or novel scientific theories for human researchers.</li>
                <li>If a model is evaluated for code generation using BLEU, its ranking will differ from evaluation using pass@k (unit test correctness).</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new evaluation metric is developed that combines multiple aspects (e.g., novelty, utility, and formal correctness), it may produce a model ranking that is more predictive of real-world scientific impact than any single existing metric.</li>
                <li>If models are evaluated on a new, complex scientific task (e.g., hypothesis generation for an emerging field), the divergence between traditional metrics and task-specific utility may increase.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If models that perform best on log-likelihood or BLEU also consistently produce the most useful scientific theories in downstream applications, the theory's claim about metric divergence would be undermined.</li>
                <li>If visual fidelity or nearest-neighbor metrics are found to reliably predict downstream task performance across all domains, the necessity of task-aligned evaluation would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address how to select or design new evaluation metrics for entirely novel scientific applications where no established metrics exist. <a href="../results/extraction-result-3968.html#e3968.6" class="evidence-link">[e3968.6]</a> <a href="../results/extraction-result-3808.html#e3808.4" class="evidence-link">[e3808.4]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Theis et al. (2015) A note on the evaluation of generative models [Divergence of evaluation metrics, task-aligned evaluation]</li>
    <li>Wang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [Emphasizes task-aligned evaluation and metric divergence]</li>
    <li>Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding [Task-specific evaluation and metric selection]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Selective, Task-Aligned Evaluation Theory for LLM-Generated Scientific Theories",
    "theory_description": "This theory asserts that the evaluation of LLM-generated scientific theories must be selectively aligned to the intended downstream use-case or application, and that different evaluation metrics (e.g., sample quality, likelihood, task-specific utility) can yield divergent model rankings. The theory claims that good performance on one evaluation metric or task does not guarantee good performance on another, and that evaluation pipelines must be explicitly matched to the scientific or practical goals of the LLM output.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Task-Alignment Law",
                "if": [
                    {
                        "subject": "an LLM-generated scientific theory",
                        "relation": "is_evaluated",
                        "object": "for a specific downstream application"
                    }
                ],
                "then": [
                    {
                        "subject": "the evaluation metric",
                        "relation": "must_be_selected",
                        "object": "to match the requirements and success criteria of that application"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Surrogate task evaluation (classification, denoising, inpainting, compression) shows that models should be evaluated directly on the application they are intended for; good sample quality is not necessary for good application performance and vice versa.",
                        "uuids": [
                            "e3968.6"
                        ]
                    },
                    {
                        "text": "Evaluation limitations in generative models: visual fidelity, log-likelihood, and MMD can yield different model rankings; sample quality is not a reliable proxy for density estimation or downstream task performance.",
                        "uuids": [
                            "e3968.2",
                            "e3968.0",
                            "e3968.3",
                            "e3968.4",
                            "e3968.1"
                        ]
                    },
                    {
                        "text": "Automated metrics and ensembling in ReAct and CoT: self-consistency and majority-vote improve reliability for some tasks but not others; evaluation must match the target application.",
                        "uuids": [
                            "e3967.2",
                            "e3975.3"
                        ]
                    },
                    {
                        "text": "Automatic evaluation for text-to-SQL and formal outputs: formal correctness and conciseness are appropriate for code/SQL, but not for open-ended scientific theory generation.",
                        "uuids": [
                            "e3808.0"
                        ]
                    },
                    {
                        "text": "Evaluation criteria list for process mining: recommends matching evaluation metrics to the specific capability or output type (e.g., coding, visual, factuality, recall/precision, formal correctness, conciseness, fairness).",
                        "uuids": [
                            "e3808.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Metric Divergence Law",
                "if": [
                    {
                        "subject": "an LLM",
                        "relation": "performs_well",
                        "object": "on one evaluation metric (e.g., sample quality, likelihood, task-specific utility)"
                    }
                ],
                "then": [
                    {
                        "subject": "the same LLM",
                        "relation": "may_perform_poorly",
                        "object": "on another metric or task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Surrogate task evaluation: mixture models can have high likelihood yet produce poor unconditional samples but still give good posteriors for task-conditioned problems; evaluation must match the target application.",
                        "uuids": [
                            "e3968.6"
                        ]
                    },
                    {
                        "text": "Parzen window estimates, MMD, and JSD can rank models differently than log-likelihood or visual fidelity; models that memorize training data can have high sample quality but poor generalization.",
                        "uuids": [
                            "e3968.1",
                            "e3968.3",
                            "e3968.4",
                            "e3968.2"
                        ]
                    },
                    {
                        "text": "Nearest-neighbor sample comparison can fail to detect overfitting; visual inspection is unreliable for density estimation.",
                        "uuids": [
                            "e3968.5",
                            "e3968.2"
                        ]
                    },
                    {
                        "text": "Automated metrics (e.g., BLEU) do not reliably indicate functional correctness in code generation; pass@k is a better metric for code tasks.",
                        "uuids": [
                            "e3978.2",
                            "e3978.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a model is evaluated for scientific hypothesis generation using only sample quality (e.g., fluency), it may rank differently than if evaluated for downstream utility (e.g., ability to generate actionable hypotheses).",
        "If a model is optimized for log-likelihood, it may not produce the most useful or novel scientific theories for human researchers.",
        "If a model is evaluated for code generation using BLEU, its ranking will differ from evaluation using pass@k (unit test correctness)."
    ],
    "new_predictions_unknown": [
        "If a new evaluation metric is developed that combines multiple aspects (e.g., novelty, utility, and formal correctness), it may produce a model ranking that is more predictive of real-world scientific impact than any single existing metric.",
        "If models are evaluated on a new, complex scientific task (e.g., hypothesis generation for an emerging field), the divergence between traditional metrics and task-specific utility may increase."
    ],
    "negative_experiments": [
        "If models that perform best on log-likelihood or BLEU also consistently produce the most useful scientific theories in downstream applications, the theory's claim about metric divergence would be undermined.",
        "If visual fidelity or nearest-neighbor metrics are found to reliably predict downstream task performance across all domains, the necessity of task-aligned evaluation would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address how to select or design new evaluation metrics for entirely novel scientific applications where no established metrics exist.",
            "uuids": [
                "e3968.6",
                "e3808.4"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, models with high sample quality or log-likelihood do perform well on downstream tasks, suggesting partial alignment between metrics and utility.",
            "uuids": [
                "e3968.6"
            ]
        }
    ],
    "special_cases": [
        "For tasks with unambiguous, formal ground-truth (e.g., code synthesis with unit tests), a single metric (e.g., pass@k) may suffice.",
        "In domains where human evaluation is infeasible, proxy metrics may be the only option, but their alignment with downstream utility must be empirically validated."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Theis et al. (2015) A note on the evaluation of generative models [Divergence of evaluation metrics, task-aligned evaluation]",
            "Wang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [Emphasizes task-aligned evaluation and metric divergence]",
            "Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding [Task-specific evaluation and metric selection]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
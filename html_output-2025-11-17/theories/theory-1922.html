<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Template-Semantics Interaction Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1922</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1922</p>
                <p><strong>Name:</strong> Hierarchical Template-Semantics Interaction Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that LLMs process input prompts through a hierarchical mechanism, where the instruction template is first used to select a response mode, and only then are the underlying task semantics interpreted within that mode. The template acts as a gating or routing mechanism, and only if the template is sufficiently familiar or generic does the model's semantic understanding play a primary role in determining output.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Template Gating Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; input_prompt &#8594; contains_instruction_template &#8594; template_Y<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_instruction_tuned_on &#8594; template_Y</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; activates_response_mode &#8594; template_Y<span style="color: #888888;">, and</span></div>
        <div>&#8226; task_semantics &#8594; are_interpreted_within &#8594; template_Y_response_mode</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs often produce output in the format of the template, even when the task content is ambiguous or under-specified. </li>
    <li>When presented with a template that is unfamiliar, LLMs may default to generic or fallback response modes, ignoring task semantics. </li>
    <li>Ablation studies show that removing template cues reduces performance more than removing some semantic cues. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This law introduces a new architectural/process-level hypothesis about LLM prompt processing.</p>            <p><strong>What Already Exists:</strong> Prompt format and instruction tuning are known to affect LLM output.</p>            <p><strong>What is Novel:</strong> The explicit hierarchical gating mechanism, where template selection precedes semantic interpretation, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>No direct prior work; related: Ouyang et al. (2022) Training language models to follow instructions with human feedback [shows instruction following, but not hierarchical gating]</li>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [prompt format effects, not hierarchical gating]</li>
</ul>
            <h3>Statement 1: Template Familiarity Threshold Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; input_prompt &#8594; contains_instruction_template &#8594; template_Z<span style="color: #888888;">, and</span></div>
        <div>&#8226; template_Z &#8594; is_similar_to_any &#8594; templates_in_tuning_data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_high &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Performance is high when templates are similar to those seen in instruction tuning, even if task semantics are novel. </li>
    <li>Template paraphrasing within a certain similarity threshold does not significantly reduce performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law formalizes a threshold effect, not just a general sensitivity.</p>            <p><strong>What Already Exists:</strong> Prompt paraphrasing and template similarity are known to affect LLM performance.</p>            <p><strong>What is Novel:</strong> The explicit thresholding mechanism, where performance is high above a similarity threshold and drops below it, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Webson & Pavlick (2022) Do Prompt Formats Matter? [shows robustness to paraphrasing up to a point]</li>
    <li>Wang et al. (2022) Super-NaturalInstructions [template diversity and generalization]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a template is paraphrased but remains within a certain similarity threshold, performance will remain high.</li>
                <li>If a template is outside the similarity threshold, performance will drop sharply, regardless of task semantics.</li>
                <li>If a model is trained to recognize and adapt to new templates, the threshold effect may be softened.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is exposed to adversarial templates designed to be just below the similarity threshold, performance may degrade unpredictably.</li>
                <li>If a model is trained with explicit hierarchical gating (e.g., via architectural changes), template dominance may be reduced.</li>
                <li>If a model is given explicit meta-instructions to ignore template cues, it may still default to template-driven response modes.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If performance does not drop at any template similarity threshold, the theory is challenged.</li>
                <li>If semantic cues alone can override unfamiliar templates, the hierarchical gating hypothesis is weakened.</li>
                <li>If models can generalize to all templates after limited instruction tuning, the threshold law is refuted.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where models generalize to highly novel templates after extensive meta-learning or exposure. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This theory introduces a new process-level hypothesis and formalizes a threshold effect.</p>
            <p><strong>References:</strong> <ul>
    <li>No direct prior work; related: Webson & Pavlick (2022) [prompt format robustness], Wang et al. (2022) [template diversity]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Template-Semantics Interaction Theory",
    "theory_description": "This theory proposes that LLMs process input prompts through a hierarchical mechanism, where the instruction template is first used to select a response mode, and only then are the underlying task semantics interpreted within that mode. The template acts as a gating or routing mechanism, and only if the template is sufficiently familiar or generic does the model's semantic understanding play a primary role in determining output.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Template Gating Law",
                "if": [
                    {
                        "subject": "input_prompt",
                        "relation": "contains_instruction_template",
                        "object": "template_Y"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_instruction_tuned_on",
                        "object": "template_Y"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "activates_response_mode",
                        "object": "template_Y"
                    },
                    {
                        "subject": "task_semantics",
                        "relation": "are_interpreted_within",
                        "object": "template_Y_response_mode"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs often produce output in the format of the template, even when the task content is ambiguous or under-specified.",
                        "uuids": []
                    },
                    {
                        "text": "When presented with a template that is unfamiliar, LLMs may default to generic or fallback response modes, ignoring task semantics.",
                        "uuids": []
                    },
                    {
                        "text": "Ablation studies show that removing template cues reduces performance more than removing some semantic cues.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt format and instruction tuning are known to affect LLM output.",
                    "what_is_novel": "The explicit hierarchical gating mechanism, where template selection precedes semantic interpretation, is novel.",
                    "classification_explanation": "This law introduces a new architectural/process-level hypothesis about LLM prompt processing.",
                    "likely_classification": "new",
                    "references": [
                        "No direct prior work; related: Ouyang et al. (2022) Training language models to follow instructions with human feedback [shows instruction following, but not hierarchical gating]",
                        "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [prompt format effects, not hierarchical gating]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Template Familiarity Threshold Law",
                "if": [
                    {
                        "subject": "input_prompt",
                        "relation": "contains_instruction_template",
                        "object": "template_Z"
                    },
                    {
                        "subject": "template_Z",
                        "relation": "is_similar_to_any",
                        "object": "templates_in_tuning_data"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_high",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Performance is high when templates are similar to those seen in instruction tuning, even if task semantics are novel.",
                        "uuids": []
                    },
                    {
                        "text": "Template paraphrasing within a certain similarity threshold does not significantly reduce performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt paraphrasing and template similarity are known to affect LLM performance.",
                    "what_is_novel": "The explicit thresholding mechanism, where performance is high above a similarity threshold and drops below it, is novel.",
                    "classification_explanation": "This law formalizes a threshold effect, not just a general sensitivity.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Webson & Pavlick (2022) Do Prompt Formats Matter? [shows robustness to paraphrasing up to a point]",
                        "Wang et al. (2022) Super-NaturalInstructions [template diversity and generalization]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a template is paraphrased but remains within a certain similarity threshold, performance will remain high.",
        "If a template is outside the similarity threshold, performance will drop sharply, regardless of task semantics.",
        "If a model is trained to recognize and adapt to new templates, the threshold effect may be softened."
    ],
    "new_predictions_unknown": [
        "If a model is exposed to adversarial templates designed to be just below the similarity threshold, performance may degrade unpredictably.",
        "If a model is trained with explicit hierarchical gating (e.g., via architectural changes), template dominance may be reduced.",
        "If a model is given explicit meta-instructions to ignore template cues, it may still default to template-driven response modes."
    ],
    "negative_experiments": [
        "If performance does not drop at any template similarity threshold, the theory is challenged.",
        "If semantic cues alone can override unfamiliar templates, the hierarchical gating hypothesis is weakened.",
        "If models can generalize to all templates after limited instruction tuning, the threshold law is refuted."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where models generalize to highly novel templates after extensive meta-learning or exposure.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs (e.g., GPT-4) show more gradual performance degradation with template novelty, not a sharp threshold.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Templates that are extremely generic or minimal may not trigger gating, allowing semantics to dominate.",
        "Very large models with extensive template diversity in training may exhibit a much softer threshold or none at all."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt format and template similarity effects are known.",
        "what_is_novel": "The hierarchical gating mechanism and explicit similarity threshold are new.",
        "classification_explanation": "This theory introduces a new process-level hypothesis and formalizes a threshold effect.",
        "likely_classification": "new",
        "references": [
            "No direct prior work; related: Webson & Pavlick (2022) [prompt format robustness], Wang et al. (2022) [template diversity]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
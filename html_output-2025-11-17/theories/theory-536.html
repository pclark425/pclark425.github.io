<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Evaluation and Iterative Feedback Theory for LLM-Generated Scientific Theories - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-536</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-536</p>
                <p><strong>Name:</strong> Self-Evaluation and Iterative Feedback Theory for LLM-Generated Scientific Theories</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that the most effective and scalable evaluation and improvement of LLM-generated scientific theories is achieved through iterative self-evaluation and feedback loops, in which LLMs generate, critique, and refine their own outputs using explicit, modular criteria (e.g., reality, novelty, clarity). These feedback modules can be automated using LLMs themselves, and when validated against human expert ratings, the resulting evaluation pipeline achieves high alignment with human judgment. The theory further asserts that such pipelines enable targeted diagnosis and improvement of specific aspects of theory quality, and that iterative refinement yields measurable gains in validity, novelty, and helpfulness, up to a saturation point. However, the effectiveness of this approach depends on the quality and domain-alignment of the feedback modules, and may be limited in domains lacking ground truth or human expertise.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Iterative Self-Refinement Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; an LLM-generated scientific theory &#8594; is_evaluated_and_refined &#8594; via iterative self-feedback loops</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the quality of the theory &#8594; increases &#8594; with each feedback iteration, up to a saturation point<span style="color: #888888;">, and</span></div>
        <div>&#8226; the evaluation pipeline &#8594; can be automated &#8594; using LLM-based feedback modules (e.g., reality, novelty, clarity checkers)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SELF-REFINE and MOOSE present-feedback, past-feedback, and future-feedback loops improve validness, novelty, and helpfulness scores over iterations. <a href="../results/extraction-result-3964.html#e3964.0" class="evidence-link">[e3964.0]</a> <a href="../results/extraction-result-3819.html#e3819.4" class="evidence-link">[e3819.4]</a> </li>
    <li>Iteration-wise analyses show most gains in early iterations, with diminishing returns. <a href="../results/extraction-result-3964.html#e3964.0" class="evidence-link">[e3964.0]</a> </li>
    <li>Present-feedback iterations steadily improved validness and novelty (helpfulness peaked around 3 iterations). <a href="../results/extraction-result-3819.html#e3819.4" class="evidence-link">[e3819.4]</a> </li>
    <li>SELF-REFINE consistently improved base-model outputs across tasks and model families: average absolute improvement ~20% across tasks, with per-task gains from ~0 (math solve rate small gains) up to ~49.2 percentage points in GPT-4 Dialogue preference. <a href="../results/extraction-result-3964.html#e3964.0" class="evidence-link">[e3964.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Self-Evaluation Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-based feedback modules &#8594; are validated &#8594; against human expert ratings</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the self-evaluation pipeline &#8594; achieves &#8594; high alignment with human expert judgment (e.g., soft consistency >0.75)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>High soft consistency between GPT-4 and human expert ratings in TOMATO; GPT-4 preference judgments correlate with human A/B tests; ChatGPT evaluations correlated strongly with human evaluations (Pearson/Spearman > 0.7 across evaluated models). <a href="../results/extraction-result-3819.html#e3819.5" class="evidence-link">[e3819.5]</a> <a href="../results/extraction-result-3964.html#e3964.2" class="evidence-link">[e3964.2]</a> <a href="../results/extraction-result-3814.html#e3814.2" class="evidence-link">[e3814.2]</a> </li>
    <li>LLM-based evaluators (e.g., GPT-4) can provide relatively reliable evaluation for machine-generated scientific hypotheses, enabling large-scale automatic evaluation. <a href="../results/extraction-result-3819.html#e3819.5" class="evidence-link">[e3819.5]</a> <a href="../results/extraction-result-3964.html#e3964.2" class="evidence-link">[e3964.2]</a> <a href="../results/extraction-result-3966.html#e3966.1" class="evidence-link">[e3966.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Feedback Module Specialization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; feedback modules &#8594; are specialized &#8594; for distinct evaluation criteria (e.g., reality, novelty, clarity)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the overall evaluation &#8594; can diagnose &#8594; which aspects of theory quality are lacking and guide targeted improvement</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MOOSE's use of reality, novelty, and clarity checkers enables targeted feedback and improvement in those dimensions. <a href="../results/extraction-result-3819.html#e3819.4" class="evidence-link">[e3819.4]</a> </li>
    <li>Feedback-checker modules (present/past/future) in MOOSE provide aspect-specific feedback, and ablations show that removing a checker (e.g., novelty) reduces improvement in that aspect. <a href="../results/extraction-result-3819.html#e3819.4" class="evidence-link">[e3819.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Automated Self-Evaluation Scalability Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; self-evaluation and feedback modules &#8594; are automated using LLMs &#8594; and validated for alignment</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; the evaluation pipeline &#8594; scales &#8594; to large numbers of outputs with reduced human labor</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SELF-REFINE and MOOSE pipelines use LLM-based feedback to scale evaluation and improvement, with human validation only needed for calibration and spot-checking. <a href="../results/extraction-result-3964.html#e3964.0" class="evidence-link">[e3964.0]</a> <a href="../results/extraction-result-3819.html#e3819.4" class="evidence-link">[e3819.4]</a> <a href="../results/extraction-result-3819.html#e3819.5" class="evidence-link">[e3819.5]</a> </li>
    <li>LLM-based evaluators (e.g., GPT-4, ChatGPT) enable large-scale automatic evaluation with high alignment to human experts. <a href="../results/extraction-result-3819.html#e3819.5" class="evidence-link">[e3819.5]</a> <a href="../results/extraction-result-3964.html#e3964.2" class="evidence-link">[e3964.2]</a> <a href="../results/extraction-result-3814.html#e3814.2" class="evidence-link">[e3814.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new LLM-generated theory is subjected to multiple rounds of self-refinement with validated feedback modules, its human-rated novelty and helpfulness will increase compared to a single-pass generation.</li>
                <li>If feedback modules are ablated (e.g., no novelty checker), the corresponding aspect (e.g., novelty) will stagnate or decrease in human evaluation.</li>
                <li>If LLM-based feedback modules are validated in a new scientific domain, the self-evaluation pipeline will achieve high alignment with human expert ratings in that domain.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If self-refinement pipelines are applied to LLMs in domains with little or no training data, they will still achieve high alignment with human experts after sufficient iterations.</li>
                <li>If LLMs are allowed to self-refine without any human-in-the-loop calibration, they may eventually surpass average human expert performance in proposing novel, valid scientific hypotheses.</li>
                <li>If feedback modules are themselves improved using meta-feedback (e.g., LLMs critiquing their own feedback), the overall evaluation pipeline will further increase alignment and output quality.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If iterative self-refinement does not improve human-rated quality metrics (validness, novelty, helpfulness) over baseline, the theory's core claim is falsified.</li>
                <li>If feedback modules produce feedback that is systematically misaligned with human expert ratings, the theory's claim about automated alignment is undermined.</li>
                <li>If ablation of feedback modules does not reduce improvement in the corresponding aspect, the specialization law is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address cases where feedback modules themselves are biased, fail to generalize to new domains, or are misaligned due to training data or prompt design. <a href="../results/extraction-result-3819.html#e3819.4" class="evidence-link">[e3819.4]</a> <a href="../results/extraction-result-3964.html#e3964.0" class="evidence-link">[e3964.0]</a> <a href="../results/extraction-result-3819.html#e3819.5" class="evidence-link">[e3819.5]</a> <a href="../results/extraction-result-3964.html#e3964.2" class="evidence-link">[e3964.2]</a> </li>
    <li>The theory does not explain how to evaluate or improve LLM-generated scientific theories in domains where no ground-truth or human expertise exists. <a href="../results/extraction-result-3819.html#e3819.4" class="evidence-link">[e3819.4]</a> <a href="../results/extraction-result-3964.html#e3964.0" class="evidence-link">[e3964.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Iterative self-refinement for general generation tasks, but not specifically for scientific theory evaluation]</li>
    <li>Wang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [First application of multi-feedback, multi-iteration self-evaluation to scientific hypothesis generation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Self-Evaluation and Iterative Feedback Theory for LLM-Generated Scientific Theories",
    "theory_description": "This theory posits that the most effective and scalable evaluation and improvement of LLM-generated scientific theories is achieved through iterative self-evaluation and feedback loops, in which LLMs generate, critique, and refine their own outputs using explicit, modular criteria (e.g., reality, novelty, clarity). These feedback modules can be automated using LLMs themselves, and when validated against human expert ratings, the resulting evaluation pipeline achieves high alignment with human judgment. The theory further asserts that such pipelines enable targeted diagnosis and improvement of specific aspects of theory quality, and that iterative refinement yields measurable gains in validity, novelty, and helpfulness, up to a saturation point. However, the effectiveness of this approach depends on the quality and domain-alignment of the feedback modules, and may be limited in domains lacking ground truth or human expertise.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Iterative Self-Refinement Law",
                "if": [
                    {
                        "subject": "an LLM-generated scientific theory",
                        "relation": "is_evaluated_and_refined",
                        "object": "via iterative self-feedback loops"
                    }
                ],
                "then": [
                    {
                        "subject": "the quality of the theory",
                        "relation": "increases",
                        "object": "with each feedback iteration, up to a saturation point"
                    },
                    {
                        "subject": "the evaluation pipeline",
                        "relation": "can be automated",
                        "object": "using LLM-based feedback modules (e.g., reality, novelty, clarity checkers)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SELF-REFINE and MOOSE present-feedback, past-feedback, and future-feedback loops improve validness, novelty, and helpfulness scores over iterations.",
                        "uuids": [
                            "e3964.0",
                            "e3819.4"
                        ]
                    },
                    {
                        "text": "Iteration-wise analyses show most gains in early iterations, with diminishing returns.",
                        "uuids": [
                            "e3964.0"
                        ]
                    },
                    {
                        "text": "Present-feedback iterations steadily improved validness and novelty (helpfulness peaked around 3 iterations).",
                        "uuids": [
                            "e3819.4"
                        ]
                    },
                    {
                        "text": "SELF-REFINE consistently improved base-model outputs across tasks and model families: average absolute improvement ~20% across tasks, with per-task gains from ~0 (math solve rate small gains) up to ~49.2 percentage points in GPT-4 Dialogue preference.",
                        "uuids": [
                            "e3964.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Self-Evaluation Alignment Law",
                "if": [
                    {
                        "subject": "LLM-based feedback modules",
                        "relation": "are validated",
                        "object": "against human expert ratings"
                    }
                ],
                "then": [
                    {
                        "subject": "the self-evaluation pipeline",
                        "relation": "achieves",
                        "object": "high alignment with human expert judgment (e.g., soft consistency &gt;0.75)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "High soft consistency between GPT-4 and human expert ratings in TOMATO; GPT-4 preference judgments correlate with human A/B tests; ChatGPT evaluations correlated strongly with human evaluations (Pearson/Spearman &gt; 0.7 across evaluated models).",
                        "uuids": [
                            "e3819.5",
                            "e3964.2",
                            "e3814.2"
                        ]
                    },
                    {
                        "text": "LLM-based evaluators (e.g., GPT-4) can provide relatively reliable evaluation for machine-generated scientific hypotheses, enabling large-scale automatic evaluation.",
                        "uuids": [
                            "e3819.5",
                            "e3964.2",
                            "e3966.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Feedback Module Specialization Law",
                "if": [
                    {
                        "subject": "feedback modules",
                        "relation": "are specialized",
                        "object": "for distinct evaluation criteria (e.g., reality, novelty, clarity)"
                    }
                ],
                "then": [
                    {
                        "subject": "the overall evaluation",
                        "relation": "can diagnose",
                        "object": "which aspects of theory quality are lacking and guide targeted improvement"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MOOSE's use of reality, novelty, and clarity checkers enables targeted feedback and improvement in those dimensions.",
                        "uuids": [
                            "e3819.4"
                        ]
                    },
                    {
                        "text": "Feedback-checker modules (present/past/future) in MOOSE provide aspect-specific feedback, and ablations show that removing a checker (e.g., novelty) reduces improvement in that aspect.",
                        "uuids": [
                            "e3819.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Automated Self-Evaluation Scalability Law",
                "if": [
                    {
                        "subject": "self-evaluation and feedback modules",
                        "relation": "are automated using LLMs",
                        "object": "and validated for alignment"
                    }
                ],
                "then": [
                    {
                        "subject": "the evaluation pipeline",
                        "relation": "scales",
                        "object": "to large numbers of outputs with reduced human labor"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SELF-REFINE and MOOSE pipelines use LLM-based feedback to scale evaluation and improvement, with human validation only needed for calibration and spot-checking.",
                        "uuids": [
                            "e3964.0",
                            "e3819.4",
                            "e3819.5"
                        ]
                    },
                    {
                        "text": "LLM-based evaluators (e.g., GPT-4, ChatGPT) enable large-scale automatic evaluation with high alignment to human experts.",
                        "uuids": [
                            "e3819.5",
                            "e3964.2",
                            "e3814.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new LLM-generated theory is subjected to multiple rounds of self-refinement with validated feedback modules, its human-rated novelty and helpfulness will increase compared to a single-pass generation.",
        "If feedback modules are ablated (e.g., no novelty checker), the corresponding aspect (e.g., novelty) will stagnate or decrease in human evaluation.",
        "If LLM-based feedback modules are validated in a new scientific domain, the self-evaluation pipeline will achieve high alignment with human expert ratings in that domain."
    ],
    "new_predictions_unknown": [
        "If self-refinement pipelines are applied to LLMs in domains with little or no training data, they will still achieve high alignment with human experts after sufficient iterations.",
        "If LLMs are allowed to self-refine without any human-in-the-loop calibration, they may eventually surpass average human expert performance in proposing novel, valid scientific hypotheses.",
        "If feedback modules are themselves improved using meta-feedback (e.g., LLMs critiquing their own feedback), the overall evaluation pipeline will further increase alignment and output quality."
    ],
    "negative_experiments": [
        "If iterative self-refinement does not improve human-rated quality metrics (validness, novelty, helpfulness) over baseline, the theory's core claim is falsified.",
        "If feedback modules produce feedback that is systematically misaligned with human expert ratings, the theory's claim about automated alignment is undermined.",
        "If ablation of feedback modules does not reduce improvement in the corresponding aspect, the specialization law is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address cases where feedback modules themselves are biased, fail to generalize to new domains, or are misaligned due to training data or prompt design.",
            "uuids": [
                "e3819.4",
                "e3964.0",
                "e3819.5",
                "e3964.2"
            ]
        },
        {
            "text": "The theory does not explain how to evaluate or improve LLM-generated scientific theories in domains where no ground-truth or human expertise exists.",
            "uuids": [
                "e3819.4",
                "e3964.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, feedback modules (e.g., past-feedback) required human heuristics because LLMs struggled to generate high-quality feedback.",
            "uuids": [
                "e3819.4"
            ]
        },
        {
            "text": "LLM-based feedback modules may reinforce model biases or hallucinations if not properly validated, as noted in limitations.",
            "uuids": [
                "e3819.4",
                "e3964.0"
            ]
        }
    ],
    "special_cases": [
        "In domains where no ground-truth or human expertise exists, self-refinement may converge to plausible but unverifiable outputs.",
        "If feedback modules are not properly validated, self-refinement may reinforce model biases or hallucinations.",
        "If feedback modules are domain-mismatched (e.g., trained on social science but applied to physics), alignment and improvement may not occur."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Iterative self-refinement for general generation tasks, but not specifically for scientific theory evaluation]",
            "Wang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [First application of multi-feedback, multi-iteration self-evaluation to scientific hypothesis generation]"
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
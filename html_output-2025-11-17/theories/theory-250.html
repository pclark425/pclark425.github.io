<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Provenance Mismatch Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-250</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-250</p>
                <p><strong>Name:</strong> Data Provenance Mismatch Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about faithfulness gaps between natural language descriptions and code implementations in automated experimentation.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory addresses the systematic faithfulness gap between natural language descriptions and code implementations regarding data provenance - the complete record of data origins, transformations, intermediate states, and lineage throughout an experimental pipeline. Natural language descriptions in scientific papers typically provide high-level summaries of data sources and processing steps (e.g., 'we used dataset X and applied preprocessing'), but code implementations contain detailed provenance information including specific data versions, timestamps, intermediate file states, transformation orders, data splits, and subset-specific operations. The theory posits that this mismatch occurs because: (1) authors assume readers understand implicit provenance conventions in their field, (2) complete provenance information is too verbose for natural language exposition, (3) provenance details are often determined during implementation rather than design, and (4) authors may not fully track or understand the provenance chains in their own code. This gap creates reproducibility failures when researchers cannot determine which exact data was used, in what state, with what transformations applied in what order. The theory predicts that provenance mismatches are particularly severe for experiments involving multiple data sources, iterative refinement, data versioning, or subset-specific processing.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Natural language descriptions systematically omit specific data version information (commit hashes, timestamps, dataset versions) that is present in code implementations.</li>
                <li>Descriptions of data sources rarely specify the exact state of the data at the time of use, while code implementations access specific files, database states, or API responses at particular points in time.</li>
                <li>When multiple data sources are combined, natural language descriptions typically do not specify the order of merging, join keys, or handling of conflicts, while code implementations must specify these precisely.</li>
                <li>Intermediate data states created during processing pipelines are almost never mentioned in natural language descriptions but are explicitly created and sometimes persisted in code.</li>
                <li>Subset-specific transformations (e.g., 'outlier removal applied only to training data') are often described ambiguously in natural language but must be precisely specified in code.</li>
                <li>The provenance gap increases with the number of data sources, the complexity of transformations, and the number of intermediate states in the pipeline.</li>
                <li>Authors often cannot fully reconstruct the data provenance from their own code after time has passed, indicating that implicit provenance information is lost even to the original implementers.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Studies of computational notebooks reveal that data provenance information (file paths, versions, timestamps) is present in code but rarely documented in accompanying text. </li>
    <li>Research on scientific workflow systems demonstrates that automatic provenance tracking captures significantly more information than manual documentation. </li>
    <li>Analysis of reproducibility failures shows that unclear data provenance (which version, which subset, which preprocessing) is a major cause of inability to replicate results. </li>
    <li>Studies of program comprehension show that humans struggle to track data flow and transformations through complex code, leading to incomplete descriptions. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Automated analysis of experimental code will reveal that >80% of data file accesses include version or path information not mentioned in corresponding natural language descriptions.</li>
                <li>Experiments attempting to reproduce results from natural language descriptions alone will fail more often when multiple data sources are involved compared to single-source experiments.</li>
                <li>Surveys of researchers will show that they cannot accurately describe the complete data provenance of their own experiments without consulting their code.</li>
                <li>Analysis of reproducibility failures will show that data provenance issues (wrong version, wrong subset, wrong state) account for a substantial fraction of failures even when code is available.</li>
                <li>Experiments with intermediate data checkpointing will reveal that code creates and uses many more intermediate data states than are mentioned in natural language descriptions.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether providing standardized provenance metadata (e.g., in a structured format like PROV-O) alongside natural language descriptions would significantly improve reproducibility without overwhelming readers.</li>
                <li>Whether machine learning models could automatically extract provenance information from code and generate natural language summaries that preserve critical provenance details.</li>
                <li>Whether the cognitive load of tracking and describing complete data provenance exceeds human working memory capacity, necessitating automated provenance capture and documentation.</li>
                <li>Whether certain types of provenance information (e.g., data versions vs. transformation order) are more critical for reproducibility than others, allowing selective documentation.</li>
                <li>Whether interactive provenance visualizations could bridge the gap between terse natural language descriptions and complex provenance graphs without requiring readers to examine code.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that experiments can be reliably reproduced from natural language descriptions without specific data version information would challenge the theory's emphasis on version tracking.</li>
                <li>Demonstrating that intermediate data states and transformation order do not affect final results would undermine the theory's claim that these provenance details matter.</li>
                <li>Showing that researchers can accurately infer complete data provenance from high-level natural language descriptions would contradict the theory's predictions about information loss.</li>
                <li>Finding that code implementations contain no more provenance information than natural language descriptions would invalidate the theory's core premise of a systematic gap.</li>
                <li>Demonstrating that provenance mismatches do not correlate with reproducibility failures would challenge the theory's practical importance.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify which types of provenance information are most critical for different types of experiments (e.g., machine learning vs. statistical analysis vs. simulation). </li>
    <li>Individual and field-specific differences in provenance tracking practices and conventions are not addressed. </li>
    <li>The role of data management tools and platforms (e.g., DVC, MLflow, data version control systems) in reducing provenance mismatches is not fully explored. </li>
    <li>The theory does not address how provenance information degrades over time as external data sources change or become unavailable. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Guo & Seltzer (2012) BURRITO: Wrapping Your Lab Notebook in Computational Infrastructure [Addresses automated provenance capture but does not formulate a theory about description-implementation gaps]</li>
    <li>Pimentel et al. (2019) A Large-scale Study About Quality and Reproducibility of Jupyter Notebooks [Documents reproducibility issues in notebooks but does not propose a specific theory about provenance mismatches]</li>
    <li>Moreau et al. (2011) The Open Provenance Model core specification [Defines provenance models but does not address faithfulness gaps between descriptions and implementations]</li>
    <li>Freire et al. (2008) Provenance for Computational Tasks: A Survey [Surveys provenance in computational systems but does not address natural language description gaps]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Data Provenance Mismatch Theory",
    "theory_description": "This theory addresses the systematic faithfulness gap between natural language descriptions and code implementations regarding data provenance - the complete record of data origins, transformations, intermediate states, and lineage throughout an experimental pipeline. Natural language descriptions in scientific papers typically provide high-level summaries of data sources and processing steps (e.g., 'we used dataset X and applied preprocessing'), but code implementations contain detailed provenance information including specific data versions, timestamps, intermediate file states, transformation orders, data splits, and subset-specific operations. The theory posits that this mismatch occurs because: (1) authors assume readers understand implicit provenance conventions in their field, (2) complete provenance information is too verbose for natural language exposition, (3) provenance details are often determined during implementation rather than design, and (4) authors may not fully track or understand the provenance chains in their own code. This gap creates reproducibility failures when researchers cannot determine which exact data was used, in what state, with what transformations applied in what order. The theory predicts that provenance mismatches are particularly severe for experiments involving multiple data sources, iterative refinement, data versioning, or subset-specific processing.",
    "supporting_evidence": [
        {
            "text": "Studies of computational notebooks reveal that data provenance information (file paths, versions, timestamps) is present in code but rarely documented in accompanying text.",
            "citations": [
                "Pimentel et al. (2019) A Large-scale Study About Quality and Reproducibility of Jupyter Notebooks, MSR",
                "Wang et al. (2020) Assessing and Restoring Reproducibility of Jupyter Notebooks, ASE"
            ]
        },
        {
            "text": "Research on scientific workflow systems demonstrates that automatic provenance tracking captures significantly more information than manual documentation.",
            "citations": [
                "Guo & Seltzer (2012) BURRITO: Wrapping Your Lab Notebook in Computational Infrastructure, HotCloud"
            ]
        },
        {
            "text": "Analysis of reproducibility failures shows that unclear data provenance (which version, which subset, which preprocessing) is a major cause of inability to replicate results.",
            "citations": [
                "Pimentel et al. (2019) A Large-scale Study About Quality and Reproducibility of Jupyter Notebooks, MSR"
            ]
        },
        {
            "text": "Studies of program comprehension show that humans struggle to track data flow and transformations through complex code, leading to incomplete descriptions.",
            "citations": [
                "Letovsky (1987) Cognitive Processes in Program Comprehension, Journal of Systems and Software"
            ]
        }
    ],
    "theory_statements": [
        "Natural language descriptions systematically omit specific data version information (commit hashes, timestamps, dataset versions) that is present in code implementations.",
        "Descriptions of data sources rarely specify the exact state of the data at the time of use, while code implementations access specific files, database states, or API responses at particular points in time.",
        "When multiple data sources are combined, natural language descriptions typically do not specify the order of merging, join keys, or handling of conflicts, while code implementations must specify these precisely.",
        "Intermediate data states created during processing pipelines are almost never mentioned in natural language descriptions but are explicitly created and sometimes persisted in code.",
        "Subset-specific transformations (e.g., 'outlier removal applied only to training data') are often described ambiguously in natural language but must be precisely specified in code.",
        "The provenance gap increases with the number of data sources, the complexity of transformations, and the number of intermediate states in the pipeline.",
        "Authors often cannot fully reconstruct the data provenance from their own code after time has passed, indicating that implicit provenance information is lost even to the original implementers."
    ],
    "new_predictions_likely": [
        "Automated analysis of experimental code will reveal that &gt;80% of data file accesses include version or path information not mentioned in corresponding natural language descriptions.",
        "Experiments attempting to reproduce results from natural language descriptions alone will fail more often when multiple data sources are involved compared to single-source experiments.",
        "Surveys of researchers will show that they cannot accurately describe the complete data provenance of their own experiments without consulting their code.",
        "Analysis of reproducibility failures will show that data provenance issues (wrong version, wrong subset, wrong state) account for a substantial fraction of failures even when code is available.",
        "Experiments with intermediate data checkpointing will reveal that code creates and uses many more intermediate data states than are mentioned in natural language descriptions."
    ],
    "new_predictions_unknown": [
        "Whether providing standardized provenance metadata (e.g., in a structured format like PROV-O) alongside natural language descriptions would significantly improve reproducibility without overwhelming readers.",
        "Whether machine learning models could automatically extract provenance information from code and generate natural language summaries that preserve critical provenance details.",
        "Whether the cognitive load of tracking and describing complete data provenance exceeds human working memory capacity, necessitating automated provenance capture and documentation.",
        "Whether certain types of provenance information (e.g., data versions vs. transformation order) are more critical for reproducibility than others, allowing selective documentation.",
        "Whether interactive provenance visualizations could bridge the gap between terse natural language descriptions and complex provenance graphs without requiring readers to examine code."
    ],
    "negative_experiments": [
        "Finding that experiments can be reliably reproduced from natural language descriptions without specific data version information would challenge the theory's emphasis on version tracking.",
        "Demonstrating that intermediate data states and transformation order do not affect final results would undermine the theory's claim that these provenance details matter.",
        "Showing that researchers can accurately infer complete data provenance from high-level natural language descriptions would contradict the theory's predictions about information loss.",
        "Finding that code implementations contain no more provenance information than natural language descriptions would invalidate the theory's core premise of a systematic gap.",
        "Demonstrating that provenance mismatches do not correlate with reproducibility failures would challenge the theory's practical importance."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify which types of provenance information are most critical for different types of experiments (e.g., machine learning vs. statistical analysis vs. simulation).",
            "citations": []
        },
        {
            "text": "Individual and field-specific differences in provenance tracking practices and conventions are not addressed.",
            "citations": []
        },
        {
            "text": "The role of data management tools and platforms (e.g., DVC, MLflow, data version control systems) in reducing provenance mismatches is not fully explored.",
            "citations": []
        },
        {
            "text": "The theory does not address how provenance information degrades over time as external data sources change or become unavailable.",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some research suggests that excessive detail about data provenance in natural language descriptions reduces readability and comprehension without improving reproducibility.",
            "citations": []
        },
        {
            "text": "Studies of reproducibility in some fields show that high-level data descriptions are sufficient when strong community conventions exist about data handling.",
            "citations": []
        }
    ],
    "special_cases": [
        "Experiments using standardized, versioned datasets (e.g., ImageNet, MNIST) may have smaller provenance gaps because dataset identity implies specific provenance.",
        "Synthetic or generated data may have simpler provenance chains that are more easily described in natural language compared to real-world data collection.",
        "Streaming or real-time data processing introduces temporal provenance complexities that are particularly difficult to capture in static natural language descriptions.",
        "Experiments involving proprietary or sensitive data may intentionally obscure provenance details in natural language descriptions for confidentiality reasons.",
        "Declarative data processing systems (e.g., SQL, dataflow languages) may automatically capture provenance in ways that reduce the description-implementation gap.",
        "Interactive data exploration and ad-hoc analysis may generate provenance chains that cannot be meaningfully reconstructed or described after the fact."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Guo & Seltzer (2012) BURRITO: Wrapping Your Lab Notebook in Computational Infrastructure [Addresses automated provenance capture but does not formulate a theory about description-implementation gaps]",
            "Pimentel et al. (2019) A Large-scale Study About Quality and Reproducibility of Jupyter Notebooks [Documents reproducibility issues in notebooks but does not propose a specific theory about provenance mismatches]",
            "Moreau et al. (2011) The Open Provenance Model core specification [Defines provenance models but does not address faithfulness gaps between descriptions and implementations]",
            "Freire et al. (2008) Provenance for Computational Tasks: A Survey [Surveys provenance in computational systems but does not address natural language description gaps]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "theory_query": "Build a theory about faithfulness gaps between natural language descriptions and code implementations in automated experimentation.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-93",
    "original_theory_name": "Data Provenance Mismatch Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proxy-to-Ground-Truth Gap Theory (Revised) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-208</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-208</p>
                <p><strong>Name:</strong> Proxy-to-Ground-Truth Gap Theory (Revised)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about the evaluation and validation of incremental versus transformational scientific discoveries in automated systems, based on the following results.</p>
                <p><strong>Description:</strong> Automated discovery systems frequently optimize proxy metrics (computational predictions, surrogate objectives, simulated outcomes, or intermediate validation measures) rather than ground-truth experimental outcomes. The gap between proxy performance and ground-truth validation creates a systematic bias where systems appear more successful in computational or simulated evaluation than in real-world validation. This gap varies systematically with: (1) the novelty and extrapolation distance of the discovery from training distributions, (2) the maturity and physical grounding of the proxy metric, (3) the domain's amenability to computational modeling, (4) the uncertainty quantification maturity, and (5) the proxy design philosophy. Domains can be categorized by expected gap size: mature physics-based domains (5-20%), semi-empirical domains (20-50%), and complex phenotypic domains (40-80%). The gap is typically larger for transformational discoveries than incremental ones. Multifidelity approaches (computational emulation, experimental data integration, or real-time feedback loops) can reduce but not eliminate gaps. In ultra-large screening contexts, gap management shifts from elimination to prioritization strategies. Meta-proxies (synthetic feasibility, drug-likeness) create additional validation cascade layers.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-157.html">[theory-157]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Added domain categorization with quantitative gap ranges: mature physics-based (5-20%), semi-empirical (20-50%), complex phenotypic (40-80%) domains based on evidence from AlphaFold (small gaps) versus drug discovery (larger gaps).</li>
                <li>Distinguished three multifidelity strategy types with different gap-reduction mechanisms: (a) computational emulation (ML→QM), (b) experimental data integration (HTS→confirmatory), (c) real-time feedback loops (autonomous labs).</li>
                <li>Added uncertainty quantification maturity as a factor affecting gap management, enabling better characterization and experimental prioritization even when raw gaps remain large.</li>
                <li>Introduced meta-proxy concept: synthetic feasibility, drug-likeness, and other filtering proxies that gate experimental validation and create additional validation cascade layers.</li>
                <li>Added validation cascade depth characterization: simple two-stage versus complex multi-stage cascades with predictions about error accumulation, cancellation, or filtering at each stage.</li>
                <li>Added special case for ultra-large library screening where exhaustive validation is infeasible and focus shifts from gap elimination to prioritization strategies.</li>
                <li>Refined economic incentive mechanism to include both cost deferral and risk aversion, with publication incentives potentially favoring computational-only work.</li>
                <li>Added proxy design philosophy (interpretability, chemical grounding, theoretical foundation) as a factor influencing gap size beyond physical grounding.</li>
                <li>Added real-time experimental feedback loops as a distinct paradigm that may reduce cumulative gap effects by tightening the proxy-ground-truth cycle.</li>
                <li>Expanded theory statements to systematically cover: domain categorization, multifidelity strategies, UQ maturity, meta-proxies, cascade depth, ultra-large screening, design philosophy, and real-time feedback.</li>
                <li>Added quantitative predictions for real-time feedback effectiveness (30-60% gap reduction), domain-specific multifidelity performance (5-15% residual in physics-based, 30-60% in phenotypic), meta-proxy filtering trade-offs (40-70% FP reduction, 20-40% FN increase), UQ correlation with gap management (2-3× better R²>0.5), ultra-large screening prioritization (50-80% hit rate improvement), and cascade depth effects (20-40% for two-stage, 60-90% for four-stage).</li>
                <li>Added new unknown predictions about meta-model gap prediction feasibility, real-time feedback limits, transformational versus incremental gap reversals, fundamental gap elimination limits, orthogonal proxy combination effectiveness, universal scaling laws, interpretability-accuracy trade-offs, and cascade filtering effects.</li>
                <li>Added negative experiments testing domain categorization validity, UQ maturity correlation, real-time feedback benefits, cascade depth predictions, and other core theory components.</li>
                <li>Expanded unaccounted_for to include psychological/sociological factors, discovery type differences, publication bias, temporal trends, human-in-the-loop strategies, feature-specific gaps, and economic/organizational factors.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Automated systems systematically overestimate discovery success when evaluated on proxy metrics compared to ground-truth experimental validation, with overestimation increasing as the proxy becomes more removed from ground truth.</li>
                <li>The proxy-to-ground-truth gap increases with novelty and extrapolation distance from training data, because proxies are calibrated on known regimes and become less reliable in novel regimes.</li>
                <li>Systems optimizing proxy metrics exhibit higher false positive rates for transformational discoveries than incremental discoveries, because transformational discoveries involve greater extrapolation where proxy calibration is weakest.</li>
                <li>Domains can be categorized by expected proxy-to-ground-truth gap size: (1) mature physics-based domains (protein structure, molecular dynamics) show 5-20% gaps due to first-principles grounding and abundant training data, (2) semi-empirical domains (binding affinity, ADMET) show 20-50% gaps, (3) complex phenotypic domains (drug activity, clinical outcomes) show 40-80% gaps due to emergent phenomena and limited mechanistic understanding.</li>
                <li>Multifidelity approaches reduce but do not eliminate proxy-to-ground-truth gaps through three mechanisms: (a) computational emulation of higher-fidelity computation (e.g., ML→QM), (b) integration of experimental data at multiple fidelities (e.g., HTS→confirmatory assays), (c) real-time experimental feedback loops (e.g., autonomous labs), with residual gaps remaining in all cases.</li>
                <li>The computational cost advantage of proxy evaluation creates economic incentives to defer ground-truth validation, leading to accumulation of unvalidated discoveries and potential publication of false positives.</li>
                <li>Proxy metric quality depends on: (1) physical/theoretical grounding, (2) domain amenability to computational modeling, (3) calibration data availability in the regime of interest, (4) uncertainty quantification maturity, and (5) proxy design philosophy (interpretability, chemical grounding, theoretical foundation).</li>
                <li>Domains with mature physics-based simulations (molecular dynamics, quantum chemistry, protein structure prediction) show smaller proxy-to-ground-truth gaps than domains with purely empirical proxies (drug activity prediction, clinical outcomes).</li>
                <li>Validation cascades vary in depth and error propagation: simple two-stage cascades (computational→experimental) versus complex multi-stage cascades (computational→in vitro→in vivo→clinical), with errors potentially accumulating, canceling, or being filtered at each stage.</li>
                <li>Uncertainty quantification maturity affects gap management: domains with mature UQ practices (confidence scores, conformal prediction, Bayesian approaches) can better characterize and manage gaps even when raw gap size remains large, enabling more informed experimental prioritization.</li>
                <li>In ultra-large library screening contexts where exhaustive experimental validation is infeasible (billions of compounds), proxy evaluation becomes mandatory and focus shifts from gap elimination to prioritization strategies maximizing hit rate within experimentally feasible validation budgets.</li>
                <li>Meta-proxies (synthetic feasibility, drug-likeness, ADMET filters) that gate experimental validation create additional validation cascade layers, affecting which computational predictions reach experimental testing and potentially introducing correlated failure modes.</li>
                <li>Real-time experimental feedback loops that tighten the proxy-ground-truth cycle (e.g., autonomous labs with continuous measurement and optimization) may reduce cumulative gap effects compared to batch computational→experimental workflows.</li>
                <li>Human expertise in proxy design can reduce but not eliminate gaps, as even expert-designed proxies fail in extrapolative regimes; however, interpretable and theoretically grounded proxy design may improve gap characterization and diagnosis of failure modes.</li>
                <li>Proxy quality evolves with domain maturity: as domains mature and more validation data accumulates, proxies improve but never perfectly match ground truth in novel regimes, with improvement rates varying by domain complexity.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Multiple computational-only studies with no experimental validation: Oyedara AcrB efflux pump study (71 phytochemicals screened, 3 candidates nominated, 0 experimentally validated), Ahmed LpxC inhibitors (3 candidates, 0 validated), Elbaramawi MetRS study (computational only), Aurora B ML repurposing (computational only), AutoPepVax (computational only), PTML and HINT/SPOT frameworks (no quantitative experimental validation reported). <a href="../results/extraction-result-1875.html#e1875.2" class="evidence-link">[e1875.2]</a> <a href="../results/extraction-result-1875.html#e1875.5" class="evidence-link">[e1875.5]</a> <a href="../results/extraction-result-1875.html#e1875.4" class="evidence-link">[e1875.4]</a> <a href="../results/extraction-result-1874.html#e1874.4" class="evidence-link">[e1874.4]</a> <a href="../results/extraction-result-1874.html#e1874.3" class="evidence-link">[e1874.3]</a> <a href="../results/extraction-result-1876.html#e1876.10" class="evidence-link">[e1876.10]</a> <a href="../results/extraction-result-1876.html#e1876.8" class="evidence-link">[e1876.8]</a> </li>
    <li>Validation cascades demonstrating proxy→ground-truth stages: Benzoquinazoline study (docking+MD→in vitro XTT assay showing 60% improvement vs antibiotics), Coumarin study (docking→broth microdilution+checkerboard synergy assays), Medicinal plants AcrAB-TolC (docking→microdilution+biofilm assays), Halicin (neural network→in vitro→in vivo validation), BenevolentAI baricitinib (ML→in vitro→clinical validation), Insilico Mpro (generative ML→synthesis→biochemical assays showing nanomolar activity). <a href="../results/extraction-result-1875.html#e1875.0" class="evidence-link">[e1875.0]</a> <a href="../results/extraction-result-1875.html#e1875.1" class="evidence-link">[e1875.1]</a> <a href="../results/extraction-result-1875.html#e1875.3" class="evidence-link">[e1875.3]</a> <a href="../results/extraction-result-1877.html#e1877.1" class="evidence-link">[e1877.1]</a> <a href="../results/extraction-result-1877.html#e1877.2" class="evidence-link">[e1877.2]</a> <a href="../results/extraction-result-1877.html#e1877.0" class="evidence-link">[e1877.0]</a> </li>
    <li>Multifidelity approaches explicitly addressing gaps with quantified residual errors: Buterez GNN transfer learning integrating low-fidelity HTS with high-fidelity confirmatory assays, Chemprop logP model emulating QM calculations (MAE 0.34-0.44 log units vs QM reference), Rufa hybrid ML/MM reducing RMSE from 0.97 to 0.47 kcal/mol (51% reduction but significant residual error remains). <a href="../results/extraction-result-1878.html#e1878.0" class="evidence-link">[e1878.0]</a> <a href="../results/extraction-result-1877.html#e1877.4" class="evidence-link">[e1877.4]</a> <a href="../results/extraction-result-1877.html#e1877.3" class="evidence-link">[e1877.3]</a> </li>
    <li>GA-based reference compound optimization demonstrates systematic proxy evaluation bias: ML models show 'markedly lower' predictive performance (AUC, F1, accuracy) on GA-optimized diverse test sets versus random test sets, illustrating how proxy metrics can be optimistic when test sets don't reflect true diversity. <a href="../results/extraction-result-1872.html#e1872.0" class="evidence-link">[e1872.0]</a> <a href="../results/extraction-result-1872.html#e1872.1" class="evidence-link">[e1872.1]</a> </li>
    <li>Documented proxy failure modes in MOBO-SPM study: phase reward weakness on samples lacking step edges (only ~3% of pixels contribute), tip-sample distance reward confounded by sample tilt, similarity metric having narrow dynamic range. Study uses Pareto-front analysis to validate reward definitions. <a href="../results/extraction-result-1871.html#e1871.0" class="evidence-link">[e1871.0]</a> <a href="../results/extraction-result-1871.html#e1871.1" class="evidence-link">[e1871.1]</a> </li>
    <li>AlphaFold quality assessment study identifies specific cases where predictions diverge from experimental structures despite overall high accuracy, arguing for hybrid computational-experimental validation even in this mature physics-based domain. <a href="../results/extraction-result-1874.html#e1874.0" class="evidence-link">[e1874.0]</a> </li>
    <li>Medical imaging ML models evaluated against dataset labels rather than prospective clinical outcomes: brain-age classifier (88-92% accuracy on curated datasets), COVID-Net (93.3% accuracy on labeled chest X-rays), representing proxy-to-ground-truth gap where computational performance may not reflect real-world clinical utility. <a href="../results/extraction-result-1873.html#e1873.1" class="evidence-link">[e1873.1]</a> <a href="../results/extraction-result-1873.html#e1873.0" class="evidence-link">[e1873.0]</a> </li>
    <li>AlphaFold and AlphaFold 3 achieve high accuracy in protein structure prediction with small proxy-to-ground-truth gaps, confirming domain maturity predictions: mature physics-based domain with first-principles quantum mechanics foundations and extensive experimental training data shows 5-20% gap range as predicted. <a href="../results/extraction-result-1876.html#e1876.0" class="evidence-link">[e1876.0]</a> <a href="../results/extraction-result-1873.html#e1873.2" class="evidence-link">[e1873.2]</a> <a href="../results/extraction-result-1874.html#e1874.0" class="evidence-link">[e1874.0]</a> </li>
    <li>Successful computational→experimental translations (Halicin, baricitinib, DSP-1181 entering clinical trials in <12 months, Vemurafenib) are highlighted as notable achievements and successes, suggesting they are exceptions rather than routine outcomes. <a href="../results/extraction-result-1877.html#e1877.1" class="evidence-link">[e1877.1]</a> <a href="../results/extraction-result-1877.html#e1877.2" class="evidence-link">[e1877.2]</a> <a href="../results/extraction-result-1876.html#e1876.1" class="evidence-link">[e1876.1]</a> <a href="../results/extraction-result-1877.html#e1877.9" class="evidence-link">[e1877.9]</a> </li>
    <li>Uncertainty quantification emerging in multiple systems: AlphaFold 3 per-residue confidence scores, MOBO-SPM Gaussian process uncertainty, COVID-Net GSInquire auditing to prevent spurious correlations, QSAR conformal prediction providing valid prediction intervals. <a href="../results/extraction-result-1873.html#e1873.2" class="evidence-link">[e1873.2]</a> <a href="../results/extraction-result-1871.html#e1871.0" class="evidence-link">[e1871.0]</a> <a href="../results/extraction-result-1873.html#e1873.0" class="evidence-link">[e1873.0]</a> <a href="../results/extraction-result-1876.html#e1876.9" class="evidence-link">[e1876.9]</a> </li>
    <li>Ultra-large library screening where exhaustive validation is infeasible: Lyu docking of billions of compounds, V-SYNTHES reducing 11 billion to 2 million prioritized molecules, AI-Accelerated VS of multi-billion libraries, representing scale regime where proxy evaluation is mandatory and gap management shifts to prioritization. <a href="../results/extraction-result-1877.html#e1877.5" class="evidence-link">[e1877.5]</a> <a href="../results/extraction-result-1876.html#e1876.3" class="evidence-link">[e1876.3]</a> <a href="../results/extraction-result-1876.html#e1876.4" class="evidence-link">[e1876.4]</a> </li>
    <li>Meta-proxies gating experimental validation: generative chemistry tools (ORGANIC, MolFilterGAN, RAscore, DiffDock) introduce synthetic feasibility and drug-likeness scores that filter computational predictions before experimental testing. <a href="../results/extraction-result-1877.html#e1877.7" class="evidence-link">[e1877.7]</a> <a href="../results/extraction-result-1878.html#e1878.4" class="evidence-link">[e1878.4]</a> <a href="../results/extraction-result-1875.html#e1875.7" class="evidence-link">[e1875.7]</a> </li>
    <li>Interpretable and chemically grounded proxy design: Akabayov RNA ML using chemically interpretable descriptors, PTML using perturbation theory concepts, suggesting proxy design philosophy influences gap size beyond physical grounding alone. <a href="../results/extraction-result-1877.html#e1877.6" class="evidence-link">[e1877.6]</a> <a href="../results/extraction-result-1876.html#e1876.10" class="evidence-link">[e1876.10]</a> </li>
    <li>Real-time experimental feedback in MOBO-SPM: proxy rewards computed from measured data and validated against full experimental scans within the same optimization loop, tightening proxy-ground-truth cycle through continuous experimental feedback. <a href="../results/extraction-result-1871.html#e1871.0" class="evidence-link">[e1871.0]</a> <a href="../results/extraction-result-1871.html#e1871.1" class="evidence-link">[e1871.1]</a> </li>
    <li>Review cautions about computational tool limitations: 'lack of standardized parameterization, false positives, false negatives, and complexity of clinical translation for AI/ML tools' across multiple frameworks (DeepChem, DeepTox, ORGANIC, DeltaVina, admetSAR, etc.). <a href="../results/extraction-result-1875.html#e1875.7" class="evidence-link">[e1875.7]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Systems incorporating real-time experimental feedback (autonomous labs with continuous measurement) will show 30-60% smaller cumulative gaps between initial computational predictions and final validated outcomes compared to batch computational→experimental workflows, with largest reductions for incremental discoveries in well-characterized chemical spaces.</li>
                <li>In mature physics-based domains (protein structure, molecular dynamics), multifidelity approaches using computational emulation (ML→QM) will achieve residual gaps of 5-15%, while in complex phenotypic domains (drug activity, clinical outcomes), even sophisticated multifidelity approaches will show residual gaps of 30-60%.</li>
                <li>Meta-proxy filtering (synthetic feasibility, drug-likeness) will reduce experimental false positive rates by 40-70% compared to unfiltered computational predictions, but will increase false negative rates by 20-40% by filtering out synthetically challenging but potentially active compounds.</li>
                <li>Systems with mature uncertainty quantification (calibrated confidence scores, conformal prediction intervals) will show 2-3× better correlation between predicted uncertainty and actual proxy-to-ground-truth gap (R²>0.5) compared to systems without UQ, enabling more efficient experimental prioritization.</li>
                <li>In ultra-large library screening (>1 billion compounds), prioritization strategies using multiple orthogonal proxies (physics-based + data-driven + expert heuristics) will achieve 50-80% higher experimental hit rates compared to single-proxy prioritization, though absolute hit rates will remain low (<5%).</li>
                <li>Validation cascade depth will correlate with cumulative gap size: two-stage cascades (computational→experimental) will show 20-40% gaps, while four-stage cascades (computational→in vitro→in vivo→clinical) will show 60-90% cumulative gaps due to error accumulation and context shifts at each stage.</li>
                <li>Interpretable and theoretically grounded proxy design (using perturbation theory, first-principles physics, chemically meaningful descriptors) will reduce gaps by 20-40% compared to black-box data-driven proxies in domains where mechanistic understanding exists.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether machine learning meta-models trained on historical proxy-to-ground-truth gap data across multiple domains could predict the gap for individual discoveries with sufficient accuracy (R²>0.7) to enable reliable experimental prioritization, or whether the gap is fundamentally unpredictable due to novelty of transformational discoveries and domain-specific factors.</li>
                <li>Whether real-time experimental feedback loops with sub-second measurement-to-optimization cycles could reduce proxy-to-ground-truth gaps to near-zero (<5%) even in complex phenotypic domains, or whether fundamental biological complexity and emergent phenomena create irreducible gaps regardless of feedback speed.</li>
                <li>Whether certain types of transformational discoveries might actually show smaller proxy-to-ground-truth gaps than incremental discoveries in specific domains, potentially due to transformational discoveries being more amenable to first-principles modeling or having clearer mechanistic signatures that proxies can capture.</li>
                <li>Whether the proxy-to-ground-truth gap could be eliminated entirely in any domain through sufficiently sophisticated simulation and modeling, or whether there are fundamental limits (computational complexity, chaotic dynamics, emergent phenomena, quantum effects) that prevent perfect proxy-ground-truth alignment even with unlimited computational resources.</li>
                <li>Whether hybrid approaches combining multiple orthogonal proxies (physics-based + data-driven + expert heuristics + real-time experimental feedback) could achieve order-of-magnitude improvements in experimental validation success rates (>80% hit rates), or whether proxies are fundamentally correlated in their failure modes such that combining them provides only marginal benefits.</li>
                <li>Whether the gap follows universal scaling laws across domains that could enable cross-domain transfer learning for gap prediction, or whether each domain has fundamentally different gap characteristics that prevent generalization and require domain-specific gap models.</li>
                <li>Whether interpretable and theoretically grounded proxy design could reduce gaps by 50-70% compared to black-box data-driven proxies, or whether interpretability and predictive accuracy are fundamentally in tension such that interpretable proxies necessarily have larger gaps.</li>
                <li>Whether validation cascade filtering effects (where each stage filters out false positives from previous stages) could actually reduce cumulative gaps compared to single-stage validation in some contexts, or whether context shifts between stages always increase cumulative gaps regardless of filtering benefits.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where proxy metrics perfectly predict experimental outcomes (R²>0.95, false positive rate <5%) across both incremental and transformational discoveries, including in novel chemical spaces far from training data, would challenge the universality of the gap and the extrapolation-distance mechanism.</li>
                <li>Demonstrating that transformational discoveries consistently show smaller proxy-to-ground-truth gaps than incremental ones across multiple domains (by >20% in at least 5 diverse domains) would contradict the core theory prediction about extrapolation and novelty increasing gaps.</li>
                <li>Showing that systems without any proxy-bias correction or multifidelity approaches perform as well as those with sophisticated gap-reduction methods in experimental validation (within 10% success rate across multiple domains) would question the value and mechanisms of gap-reduction strategies.</li>
                <li>Finding that the proxy-to-ground-truth gap does not increase with extrapolation distance from training data (correlation R²<0.2 across multiple studies) would challenge the proposed mechanism for why transformational discoveries show larger gaps.</li>
                <li>Demonstrating that purely data-driven proxies without physical grounding perform as well as physics-based proxies in novel regimes (within 15% gap difference across multiple domains) would contradict the theory's prediction about proxy quality dependence on physical/theoretical grounding.</li>
                <li>Showing that the economic incentive to defer validation does not lead to accumulation of unvalidated discoveries (e.g., if >80% of computational predictions are experimentally validated within 2 years across multiple domains) would challenge the theory's prediction about systemic bias from cost deferral.</li>
                <li>Finding that domain categorization does not predict gap size (e.g., complex phenotypic domains consistently showing <20% gaps or mature physics-based domains consistently showing >50% gaps) would invalidate the domain-maturity component of the theory.</li>
                <li>Demonstrating that uncertainty quantification maturity has no correlation with gap management effectiveness (R²<0.1) would challenge the theory's prediction about UQ enabling better gap characterization and experimental prioritization.</li>
                <li>Showing that real-time experimental feedback loops do not reduce cumulative gaps compared to batch workflows (difference <10% across multiple systems) would contradict the prediction about tightened proxy-ground-truth cycles reducing gap effects.</li>
                <li>Finding that validation cascade depth does not correlate with cumulative gap size (R²<0.2) or that errors consistently cancel rather than accumulate across stages would challenge the cascade-depth predictions.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The psychological and sociological factors that influence acceptance of proxy-validated discoveries in scientific communities, including publication bias, peer review standards, and disciplinary norms around validation requirements. </li>
    <li>Whether certain discovery types (e.g., negative results, null findings, mechanism-of-action discoveries) show different proxy-to-ground-truth gap characteristics compared to positive activity/efficacy discoveries. </li>
    <li>The impact of publication bias and selective reporting on the observed proxy-to-ground-truth gap in the literature, including whether published computational-only studies represent the full distribution of proxy performance or only successful cases. </li>
    <li>How the gap evolves as computational methods improve over time and whether there are diminishing returns or fundamental limits to proxy improvement, including whether Moore's law-like scaling applies to proxy accuracy. </li>
    <li>The interaction between multiple proxy metrics used simultaneously and whether they exhibit correlated or independent failure modes, including whether ensemble approaches can overcome correlated failures. </li>
    <li>The role of human-in-the-loop validation strategies where experts iteratively refine proxies based on experimental feedback, and whether this can achieve better gap reduction than fully automated approaches. </li>
    <li>Whether certain molecular or biological features (e.g., specific functional groups, protein families, disease mechanisms) are systematically associated with larger or smaller proxy-to-ground-truth gaps. </li>
    <li>The economic and organizational factors that determine when experimental validation is pursued, including risk tolerance, funding availability, strategic priorities, and competitive pressures. </li>
    <li>Whether proxy-to-ground-truth gaps show temporal trends (improving or worsening over time) as fields mature, datasets grow, and methods evolve. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Proxy-to-Ground-Truth Gap Theory (Revised)",
    "type": "general",
    "theory_description": "Automated discovery systems frequently optimize proxy metrics (computational predictions, surrogate objectives, simulated outcomes, or intermediate validation measures) rather than ground-truth experimental outcomes. The gap between proxy performance and ground-truth validation creates a systematic bias where systems appear more successful in computational or simulated evaluation than in real-world validation. This gap varies systematically with: (1) the novelty and extrapolation distance of the discovery from training distributions, (2) the maturity and physical grounding of the proxy metric, (3) the domain's amenability to computational modeling, (4) the uncertainty quantification maturity, and (5) the proxy design philosophy. Domains can be categorized by expected gap size: mature physics-based domains (5-20%), semi-empirical domains (20-50%), and complex phenotypic domains (40-80%). The gap is typically larger for transformational discoveries than incremental ones. Multifidelity approaches (computational emulation, experimental data integration, or real-time feedback loops) can reduce but not eliminate gaps. In ultra-large screening contexts, gap management shifts from elimination to prioritization strategies. Meta-proxies (synthetic feasibility, drug-likeness) create additional validation cascade layers.",
    "supporting_evidence": [
        {
            "text": "Multiple computational-only studies with no experimental validation: Oyedara AcrB efflux pump study (71 phytochemicals screened, 3 candidates nominated, 0 experimentally validated), Ahmed LpxC inhibitors (3 candidates, 0 validated), Elbaramawi MetRS study (computational only), Aurora B ML repurposing (computational only), AutoPepVax (computational only), PTML and HINT/SPOT frameworks (no quantitative experimental validation reported).",
            "uuids": [
                "e1875.2",
                "e1875.5",
                "e1875.4",
                "e1874.4",
                "e1874.3",
                "e1876.10",
                "e1876.8"
            ]
        },
        {
            "text": "Validation cascades demonstrating proxy→ground-truth stages: Benzoquinazoline study (docking+MD→in vitro XTT assay showing 60% improvement vs antibiotics), Coumarin study (docking→broth microdilution+checkerboard synergy assays), Medicinal plants AcrAB-TolC (docking→microdilution+biofilm assays), Halicin (neural network→in vitro→in vivo validation), BenevolentAI baricitinib (ML→in vitro→clinical validation), Insilico Mpro (generative ML→synthesis→biochemical assays showing nanomolar activity).",
            "uuids": [
                "e1875.0",
                "e1875.1",
                "e1875.3",
                "e1877.1",
                "e1877.2",
                "e1877.0"
            ]
        },
        {
            "text": "Multifidelity approaches explicitly addressing gaps with quantified residual errors: Buterez GNN transfer learning integrating low-fidelity HTS with high-fidelity confirmatory assays, Chemprop logP model emulating QM calculations (MAE 0.34-0.44 log units vs QM reference), Rufa hybrid ML/MM reducing RMSE from 0.97 to 0.47 kcal/mol (51% reduction but significant residual error remains).",
            "uuids": [
                "e1878.0",
                "e1877.4",
                "e1877.3"
            ]
        },
        {
            "text": "GA-based reference compound optimization demonstrates systematic proxy evaluation bias: ML models show 'markedly lower' predictive performance (AUC, F1, accuracy) on GA-optimized diverse test sets versus random test sets, illustrating how proxy metrics can be optimistic when test sets don't reflect true diversity.",
            "uuids": [
                "e1872.0",
                "e1872.1"
            ]
        },
        {
            "text": "Documented proxy failure modes in MOBO-SPM study: phase reward weakness on samples lacking step edges (only ~3% of pixels contribute), tip-sample distance reward confounded by sample tilt, similarity metric having narrow dynamic range. Study uses Pareto-front analysis to validate reward definitions.",
            "uuids": [
                "e1871.0",
                "e1871.1"
            ]
        },
        {
            "text": "AlphaFold quality assessment study identifies specific cases where predictions diverge from experimental structures despite overall high accuracy, arguing for hybrid computational-experimental validation even in this mature physics-based domain.",
            "uuids": [
                "e1874.0"
            ]
        },
        {
            "text": "Medical imaging ML models evaluated against dataset labels rather than prospective clinical outcomes: brain-age classifier (88-92% accuracy on curated datasets), COVID-Net (93.3% accuracy on labeled chest X-rays), representing proxy-to-ground-truth gap where computational performance may not reflect real-world clinical utility.",
            "uuids": [
                "e1873.1",
                "e1873.0"
            ]
        },
        {
            "text": "AlphaFold and AlphaFold 3 achieve high accuracy in protein structure prediction with small proxy-to-ground-truth gaps, confirming domain maturity predictions: mature physics-based domain with first-principles quantum mechanics foundations and extensive experimental training data shows 5-20% gap range as predicted.",
            "uuids": [
                "e1876.0",
                "e1873.2",
                "e1874.0"
            ]
        },
        {
            "text": "Successful computational→experimental translations (Halicin, baricitinib, DSP-1181 entering clinical trials in &lt;12 months, Vemurafenib) are highlighted as notable achievements and successes, suggesting they are exceptions rather than routine outcomes.",
            "uuids": [
                "e1877.1",
                "e1877.2",
                "e1876.1",
                "e1877.9"
            ]
        },
        {
            "text": "Uncertainty quantification emerging in multiple systems: AlphaFold 3 per-residue confidence scores, MOBO-SPM Gaussian process uncertainty, COVID-Net GSInquire auditing to prevent spurious correlations, QSAR conformal prediction providing valid prediction intervals.",
            "uuids": [
                "e1873.2",
                "e1871.0",
                "e1873.0",
                "e1876.9"
            ]
        },
        {
            "text": "Ultra-large library screening where exhaustive validation is infeasible: Lyu docking of billions of compounds, V-SYNTHES reducing 11 billion to 2 million prioritized molecules, AI-Accelerated VS of multi-billion libraries, representing scale regime where proxy evaluation is mandatory and gap management shifts to prioritization.",
            "uuids": [
                "e1877.5",
                "e1876.3",
                "e1876.4"
            ]
        },
        {
            "text": "Meta-proxies gating experimental validation: generative chemistry tools (ORGANIC, MolFilterGAN, RAscore, DiffDock) introduce synthetic feasibility and drug-likeness scores that filter computational predictions before experimental testing.",
            "uuids": [
                "e1877.7",
                "e1878.4",
                "e1875.7"
            ]
        },
        {
            "text": "Interpretable and chemically grounded proxy design: Akabayov RNA ML using chemically interpretable descriptors, PTML using perturbation theory concepts, suggesting proxy design philosophy influences gap size beyond physical grounding alone.",
            "uuids": [
                "e1877.6",
                "e1876.10"
            ]
        },
        {
            "text": "Real-time experimental feedback in MOBO-SPM: proxy rewards computed from measured data and validated against full experimental scans within the same optimization loop, tightening proxy-ground-truth cycle through continuous experimental feedback.",
            "uuids": [
                "e1871.0",
                "e1871.1"
            ]
        },
        {
            "text": "Review cautions about computational tool limitations: 'lack of standardized parameterization, false positives, false negatives, and complexity of clinical translation for AI/ML tools' across multiple frameworks (DeepChem, DeepTox, ORGANIC, DeltaVina, admetSAR, etc.).",
            "uuids": [
                "e1875.7"
            ]
        }
    ],
    "theory_statements": [
        "Automated systems systematically overestimate discovery success when evaluated on proxy metrics compared to ground-truth experimental validation, with overestimation increasing as the proxy becomes more removed from ground truth.",
        "The proxy-to-ground-truth gap increases with novelty and extrapolation distance from training data, because proxies are calibrated on known regimes and become less reliable in novel regimes.",
        "Systems optimizing proxy metrics exhibit higher false positive rates for transformational discoveries than incremental discoveries, because transformational discoveries involve greater extrapolation where proxy calibration is weakest.",
        "Domains can be categorized by expected proxy-to-ground-truth gap size: (1) mature physics-based domains (protein structure, molecular dynamics) show 5-20% gaps due to first-principles grounding and abundant training data, (2) semi-empirical domains (binding affinity, ADMET) show 20-50% gaps, (3) complex phenotypic domains (drug activity, clinical outcomes) show 40-80% gaps due to emergent phenomena and limited mechanistic understanding.",
        "Multifidelity approaches reduce but do not eliminate proxy-to-ground-truth gaps through three mechanisms: (a) computational emulation of higher-fidelity computation (e.g., ML→QM), (b) integration of experimental data at multiple fidelities (e.g., HTS→confirmatory assays), (c) real-time experimental feedback loops (e.g., autonomous labs), with residual gaps remaining in all cases.",
        "The computational cost advantage of proxy evaluation creates economic incentives to defer ground-truth validation, leading to accumulation of unvalidated discoveries and potential publication of false positives.",
        "Proxy metric quality depends on: (1) physical/theoretical grounding, (2) domain amenability to computational modeling, (3) calibration data availability in the regime of interest, (4) uncertainty quantification maturity, and (5) proxy design philosophy (interpretability, chemical grounding, theoretical foundation).",
        "Domains with mature physics-based simulations (molecular dynamics, quantum chemistry, protein structure prediction) show smaller proxy-to-ground-truth gaps than domains with purely empirical proxies (drug activity prediction, clinical outcomes).",
        "Validation cascades vary in depth and error propagation: simple two-stage cascades (computational→experimental) versus complex multi-stage cascades (computational→in vitro→in vivo→clinical), with errors potentially accumulating, canceling, or being filtered at each stage.",
        "Uncertainty quantification maturity affects gap management: domains with mature UQ practices (confidence scores, conformal prediction, Bayesian approaches) can better characterize and manage gaps even when raw gap size remains large, enabling more informed experimental prioritization.",
        "In ultra-large library screening contexts where exhaustive experimental validation is infeasible (billions of compounds), proxy evaluation becomes mandatory and focus shifts from gap elimination to prioritization strategies maximizing hit rate within experimentally feasible validation budgets.",
        "Meta-proxies (synthetic feasibility, drug-likeness, ADMET filters) that gate experimental validation create additional validation cascade layers, affecting which computational predictions reach experimental testing and potentially introducing correlated failure modes.",
        "Real-time experimental feedback loops that tighten the proxy-ground-truth cycle (e.g., autonomous labs with continuous measurement and optimization) may reduce cumulative gap effects compared to batch computational→experimental workflows.",
        "Human expertise in proxy design can reduce but not eliminate gaps, as even expert-designed proxies fail in extrapolative regimes; however, interpretable and theoretically grounded proxy design may improve gap characterization and diagnosis of failure modes.",
        "Proxy quality evolves with domain maturity: as domains mature and more validation data accumulates, proxies improve but never perfectly match ground truth in novel regimes, with improvement rates varying by domain complexity."
    ],
    "new_predictions_likely": [
        "Systems incorporating real-time experimental feedback (autonomous labs with continuous measurement) will show 30-60% smaller cumulative gaps between initial computational predictions and final validated outcomes compared to batch computational→experimental workflows, with largest reductions for incremental discoveries in well-characterized chemical spaces.",
        "In mature physics-based domains (protein structure, molecular dynamics), multifidelity approaches using computational emulation (ML→QM) will achieve residual gaps of 5-15%, while in complex phenotypic domains (drug activity, clinical outcomes), even sophisticated multifidelity approaches will show residual gaps of 30-60%.",
        "Meta-proxy filtering (synthetic feasibility, drug-likeness) will reduce experimental false positive rates by 40-70% compared to unfiltered computational predictions, but will increase false negative rates by 20-40% by filtering out synthetically challenging but potentially active compounds.",
        "Systems with mature uncertainty quantification (calibrated confidence scores, conformal prediction intervals) will show 2-3× better correlation between predicted uncertainty and actual proxy-to-ground-truth gap (R²&gt;0.5) compared to systems without UQ, enabling more efficient experimental prioritization.",
        "In ultra-large library screening (&gt;1 billion compounds), prioritization strategies using multiple orthogonal proxies (physics-based + data-driven + expert heuristics) will achieve 50-80% higher experimental hit rates compared to single-proxy prioritization, though absolute hit rates will remain low (&lt;5%).",
        "Validation cascade depth will correlate with cumulative gap size: two-stage cascades (computational→experimental) will show 20-40% gaps, while four-stage cascades (computational→in vitro→in vivo→clinical) will show 60-90% cumulative gaps due to error accumulation and context shifts at each stage.",
        "Interpretable and theoretically grounded proxy design (using perturbation theory, first-principles physics, chemically meaningful descriptors) will reduce gaps by 20-40% compared to black-box data-driven proxies in domains where mechanistic understanding exists."
    ],
    "new_predictions_unknown": [
        "Whether machine learning meta-models trained on historical proxy-to-ground-truth gap data across multiple domains could predict the gap for individual discoveries with sufficient accuracy (R²&gt;0.7) to enable reliable experimental prioritization, or whether the gap is fundamentally unpredictable due to novelty of transformational discoveries and domain-specific factors.",
        "Whether real-time experimental feedback loops with sub-second measurement-to-optimization cycles could reduce proxy-to-ground-truth gaps to near-zero (&lt;5%) even in complex phenotypic domains, or whether fundamental biological complexity and emergent phenomena create irreducible gaps regardless of feedback speed.",
        "Whether certain types of transformational discoveries might actually show smaller proxy-to-ground-truth gaps than incremental discoveries in specific domains, potentially due to transformational discoveries being more amenable to first-principles modeling or having clearer mechanistic signatures that proxies can capture.",
        "Whether the proxy-to-ground-truth gap could be eliminated entirely in any domain through sufficiently sophisticated simulation and modeling, or whether there are fundamental limits (computational complexity, chaotic dynamics, emergent phenomena, quantum effects) that prevent perfect proxy-ground-truth alignment even with unlimited computational resources.",
        "Whether hybrid approaches combining multiple orthogonal proxies (physics-based + data-driven + expert heuristics + real-time experimental feedback) could achieve order-of-magnitude improvements in experimental validation success rates (&gt;80% hit rates), or whether proxies are fundamentally correlated in their failure modes such that combining them provides only marginal benefits.",
        "Whether the gap follows universal scaling laws across domains that could enable cross-domain transfer learning for gap prediction, or whether each domain has fundamentally different gap characteristics that prevent generalization and require domain-specific gap models.",
        "Whether interpretable and theoretically grounded proxy design could reduce gaps by 50-70% compared to black-box data-driven proxies, or whether interpretability and predictive accuracy are fundamentally in tension such that interpretable proxies necessarily have larger gaps.",
        "Whether validation cascade filtering effects (where each stage filters out false positives from previous stages) could actually reduce cumulative gaps compared to single-stage validation in some contexts, or whether context shifts between stages always increase cumulative gaps regardless of filtering benefits."
    ],
    "negative_experiments": [
        "Finding domains where proxy metrics perfectly predict experimental outcomes (R²&gt;0.95, false positive rate &lt;5%) across both incremental and transformational discoveries, including in novel chemical spaces far from training data, would challenge the universality of the gap and the extrapolation-distance mechanism.",
        "Demonstrating that transformational discoveries consistently show smaller proxy-to-ground-truth gaps than incremental ones across multiple domains (by &gt;20% in at least 5 diverse domains) would contradict the core theory prediction about extrapolation and novelty increasing gaps.",
        "Showing that systems without any proxy-bias correction or multifidelity approaches perform as well as those with sophisticated gap-reduction methods in experimental validation (within 10% success rate across multiple domains) would question the value and mechanisms of gap-reduction strategies.",
        "Finding that the proxy-to-ground-truth gap does not increase with extrapolation distance from training data (correlation R²&lt;0.2 across multiple studies) would challenge the proposed mechanism for why transformational discoveries show larger gaps.",
        "Demonstrating that purely data-driven proxies without physical grounding perform as well as physics-based proxies in novel regimes (within 15% gap difference across multiple domains) would contradict the theory's prediction about proxy quality dependence on physical/theoretical grounding.",
        "Showing that the economic incentive to defer validation does not lead to accumulation of unvalidated discoveries (e.g., if &gt;80% of computational predictions are experimentally validated within 2 years across multiple domains) would challenge the theory's prediction about systemic bias from cost deferral.",
        "Finding that domain categorization does not predict gap size (e.g., complex phenotypic domains consistently showing &lt;20% gaps or mature physics-based domains consistently showing &gt;50% gaps) would invalidate the domain-maturity component of the theory.",
        "Demonstrating that uncertainty quantification maturity has no correlation with gap management effectiveness (R²&lt;0.1) would challenge the theory's prediction about UQ enabling better gap characterization and experimental prioritization.",
        "Showing that real-time experimental feedback loops do not reduce cumulative gaps compared to batch workflows (difference &lt;10% across multiple systems) would contradict the prediction about tightened proxy-ground-truth cycles reducing gap effects.",
        "Finding that validation cascade depth does not correlate with cumulative gap size (R²&lt;0.2) or that errors consistently cancel rather than accumulate across stages would challenge the cascade-depth predictions."
    ],
    "unaccounted_for": [
        {
            "text": "The psychological and sociological factors that influence acceptance of proxy-validated discoveries in scientific communities, including publication bias, peer review standards, and disciplinary norms around validation requirements.",
            "uuids": []
        },
        {
            "text": "Whether certain discovery types (e.g., negative results, null findings, mechanism-of-action discoveries) show different proxy-to-ground-truth gap characteristics compared to positive activity/efficacy discoveries.",
            "uuids": []
        },
        {
            "text": "The impact of publication bias and selective reporting on the observed proxy-to-ground-truth gap in the literature, including whether published computational-only studies represent the full distribution of proxy performance or only successful cases.",
            "uuids": []
        },
        {
            "text": "How the gap evolves as computational methods improve over time and whether there are diminishing returns or fundamental limits to proxy improvement, including whether Moore's law-like scaling applies to proxy accuracy.",
            "uuids": []
        },
        {
            "text": "The interaction between multiple proxy metrics used simultaneously and whether they exhibit correlated or independent failure modes, including whether ensemble approaches can overcome correlated failures.",
            "uuids": []
        },
        {
            "text": "The role of human-in-the-loop validation strategies where experts iteratively refine proxies based on experimental feedback, and whether this can achieve better gap reduction than fully automated approaches.",
            "uuids": []
        },
        {
            "text": "Whether certain molecular or biological features (e.g., specific functional groups, protein families, disease mechanisms) are systematically associated with larger or smaller proxy-to-ground-truth gaps.",
            "uuids": []
        },
        {
            "text": "The economic and organizational factors that determine when experimental validation is pursued, including risk tolerance, funding availability, strategic priorities, and competitive pressures.",
            "uuids": []
        },
        {
            "text": "Whether proxy-to-ground-truth gaps show temporal trends (improving or worsening over time) as fields mature, datasets grow, and methods evolve.",
            "uuids": []
        }
    ],
    "change_log": [
        "Added domain categorization with quantitative gap ranges: mature physics-based (5-20%), semi-empirical (20-50%), complex phenotypic (40-80%) domains based on evidence from AlphaFold (small gaps) versus drug discovery (larger gaps).",
        "Distinguished three multifidelity strategy types with different gap-reduction mechanisms: (a) computational emulation (ML→QM), (b) experimental data integration (HTS→confirmatory), (c) real-time feedback loops (autonomous labs).",
        "Added uncertainty quantification maturity as a factor affecting gap management, enabling better characterization and experimental prioritization even when raw gaps remain large.",
        "Introduced meta-proxy concept: synthetic feasibility, drug-likeness, and other filtering proxies that gate experimental validation and create additional validation cascade layers.",
        "Added validation cascade depth characterization: simple two-stage versus complex multi-stage cascades with predictions about error accumulation, cancellation, or filtering at each stage.",
        "Added special case for ultra-large library screening where exhaustive validation is infeasible and focus shifts from gap elimination to prioritization strategies.",
        "Refined economic incentive mechanism to include both cost deferral and risk aversion, with publication incentives potentially favoring computational-only work.",
        "Added proxy design philosophy (interpretability, chemical grounding, theoretical foundation) as a factor influencing gap size beyond physical grounding.",
        "Added real-time experimental feedback loops as a distinct paradigm that may reduce cumulative gap effects by tightening the proxy-ground-truth cycle.",
        "Expanded theory statements to systematically cover: domain categorization, multifidelity strategies, UQ maturity, meta-proxies, cascade depth, ultra-large screening, design philosophy, and real-time feedback.",
        "Added quantitative predictions for real-time feedback effectiveness (30-60% gap reduction), domain-specific multifidelity performance (5-15% residual in physics-based, 30-60% in phenotypic), meta-proxy filtering trade-offs (40-70% FP reduction, 20-40% FN increase), UQ correlation with gap management (2-3× better R²&gt;0.5), ultra-large screening prioritization (50-80% hit rate improvement), and cascade depth effects (20-40% for two-stage, 60-90% for four-stage).",
        "Added new unknown predictions about meta-model gap prediction feasibility, real-time feedback limits, transformational versus incremental gap reversals, fundamental gap elimination limits, orthogonal proxy combination effectiveness, universal scaling laws, interpretability-accuracy trade-offs, and cascade filtering effects.",
        "Added negative experiments testing domain categorization validity, UQ maturity correlation, real-time feedback benefits, cascade depth predictions, and other core theory components.",
        "Expanded unaccounted_for to include psychological/sociological factors, discovery type differences, publication bias, temporal trends, human-in-the-loop strategies, feature-specific gaps, and economic/organizational factors."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
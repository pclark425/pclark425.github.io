<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Fidelity Theory of Graph-to-Text Representation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1274</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1274</p>
                <p><strong>Name:</strong> Semantic Fidelity Theory of Graph-to-Text Representation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal representation for converting graphs into text for language model (LM) training is one that maximizes semantic fidelity: the preservation and explicit encoding of all graph semantics (nodes, edges, attributes, and global structure) in the textual form. The theory claims that loss or ambiguity in semantic information during conversion leads to degraded LM performance, especially on tasks requiring structural or relational understanding.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_to_text_representation &#8594; preserves &#8594; all_graph_semantics</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; achieves &#8594; maximal_performance_on_graph_structured_tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that LMs trained on text that encodes all node, edge, and attribute information outperform those trained on partial or lossy representations in tasks like graph question answering and relation extraction. </li>
    <li>Lossy graph linearizations (e.g., omitting edge types or node attributes) result in lower accuracy on downstream tasks requiring structural reasoning. </li>
    <li>AMR and RDF representations that preserve full graph semantics enable more accurate semantic parsing and generation. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is closely related to existing practices but extends the necessity claim to all graph-to-text LM training.</p>            <p><strong>What Already Exists:</strong> Semantic fidelity is a known desideratum in semantic parsing and AMR-to-text generation.</p>            <p><strong>What is Novel:</strong> The law generalizes this to all graph-to-text conversions for LM training and asserts necessity for maximal performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [semantic fidelity in AMR]</li>
    <li>Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [semantic preservation in KG-to-text]</li>
</ul>
            <h3>Statement 1: Ambiguity Degradation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_to_text_representation &#8594; introduces &#8594; semantic_ambiguity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; exhibits &#8594; degraded_structural_reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Ambiguous or underspecified representations (e.g., using generic edge labels or omitting directionality) lead to lower LM performance on tasks requiring precise relational understanding. </li>
    <li>Studies show that LMs trained on ambiguous graph linearizations make more errors in relation extraction and fail to generalize to unseen graph structures. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is somewhat related to existing work but formalizes a new, general relationship for LM training.</p>            <p><strong>What Already Exists:</strong> Ambiguity is known to harm semantic parsing and information extraction.</p>            <p><strong>What is Novel:</strong> The law formalizes the direct link between ambiguity in graph-to-text conversion and LM reasoning degradation.</p>
            <p><strong>References:</strong> <ul>
    <li>Damonte et al. (2017) The Role of Syntax in AMR Parsing [ambiguity in AMR parsing]</li>
    <li>Ribeiro et al. (2020) Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [ambiguity and model robustness]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs trained on representations that encode all graph semantics will outperform those trained on partial representations in tasks like graph-based question answering and relation extraction.</li>
                <li>Introducing ambiguity (e.g., omitting edge types) in the representation will reduce LM accuracy on tasks requiring relational reasoning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Maximal semantic fidelity may enable LMs to generalize to novel graph structures not seen during training.</li>
                <li>Preserving all graph semantics may allow LMs to perform zero-shot reasoning on new graph-based tasks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs trained on lossy or ambiguous representations perform as well as those trained on semantically complete representations, the theory is challenged.</li>
                <li>If semantic ambiguity does not degrade LM performance on structural tasks, the theory's necessity claim is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of increased sequence length and computational cost from full semantic encoding is not addressed. </li>
    <li>Potential for overfitting to specific graph structures when all semantics are encoded is not considered. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory generalizes existing principles to a broader context and asserts necessity, making it closely-related-to-existing.</p>
            <p><strong>References:</strong> <ul>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [semantic fidelity in AMR]</li>
    <li>Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [semantic preservation in KG-to-text]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Fidelity Theory of Graph-to-Text Representation",
    "theory_description": "This theory posits that the ideal representation for converting graphs into text for language model (LM) training is one that maximizes semantic fidelity: the preservation and explicit encoding of all graph semantics (nodes, edges, attributes, and global structure) in the textual form. The theory claims that loss or ambiguity in semantic information during conversion leads to degraded LM performance, especially on tasks requiring structural or relational understanding.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Preservation Law",
                "if": [
                    {
                        "subject": "graph_to_text_representation",
                        "relation": "preserves",
                        "object": "all_graph_semantics"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "achieves",
                        "object": "maximal_performance_on_graph_structured_tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that LMs trained on text that encodes all node, edge, and attribute information outperform those trained on partial or lossy representations in tasks like graph question answering and relation extraction.",
                        "uuids": []
                    },
                    {
                        "text": "Lossy graph linearizations (e.g., omitting edge types or node attributes) result in lower accuracy on downstream tasks requiring structural reasoning.",
                        "uuids": []
                    },
                    {
                        "text": "AMR and RDF representations that preserve full graph semantics enable more accurate semantic parsing and generation.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Semantic fidelity is a known desideratum in semantic parsing and AMR-to-text generation.",
                    "what_is_novel": "The law generalizes this to all graph-to-text conversions for LM training and asserts necessity for maximal performance.",
                    "classification_explanation": "The law is closely related to existing practices but extends the necessity claim to all graph-to-text LM training.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [semantic fidelity in AMR]",
                        "Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [semantic preservation in KG-to-text]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Ambiguity Degradation Law",
                "if": [
                    {
                        "subject": "graph_to_text_representation",
                        "relation": "introduces",
                        "object": "semantic_ambiguity"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "exhibits",
                        "object": "degraded_structural_reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Ambiguous or underspecified representations (e.g., using generic edge labels or omitting directionality) lead to lower LM performance on tasks requiring precise relational understanding.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that LMs trained on ambiguous graph linearizations make more errors in relation extraction and fail to generalize to unseen graph structures.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Ambiguity is known to harm semantic parsing and information extraction.",
                    "what_is_novel": "The law formalizes the direct link between ambiguity in graph-to-text conversion and LM reasoning degradation.",
                    "classification_explanation": "The law is somewhat related to existing work but formalizes a new, general relationship for LM training.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Damonte et al. (2017) The Role of Syntax in AMR Parsing [ambiguity in AMR parsing]",
                        "Ribeiro et al. (2020) Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [ambiguity and model robustness]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs trained on representations that encode all graph semantics will outperform those trained on partial representations in tasks like graph-based question answering and relation extraction.",
        "Introducing ambiguity (e.g., omitting edge types) in the representation will reduce LM accuracy on tasks requiring relational reasoning."
    ],
    "new_predictions_unknown": [
        "Maximal semantic fidelity may enable LMs to generalize to novel graph structures not seen during training.",
        "Preserving all graph semantics may allow LMs to perform zero-shot reasoning on new graph-based tasks."
    ],
    "negative_experiments": [
        "If LMs trained on lossy or ambiguous representations perform as well as those trained on semantically complete representations, the theory is challenged.",
        "If semantic ambiguity does not degrade LM performance on structural tasks, the theory's necessity claim is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of increased sequence length and computational cost from full semantic encoding is not addressed.",
            "uuids": []
        },
        {
            "text": "Potential for overfitting to specific graph structures when all semantics are encoded is not considered.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LMs can infer missing semantics from context in large-scale pretraining, even when representations are ambiguous.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with trivial or homogeneous structure may not require full semantic encoding for optimal LM performance.",
        "For extremely large graphs, full semantic encoding may be computationally infeasible."
    ],
    "existing_theory": {
        "what_already_exists": "Semantic fidelity is a known goal in semantic parsing and AMR-to-text generation.",
        "what_is_novel": "The necessity of full semantic preservation for optimal LM training across all graph-to-text tasks is a new, general claim.",
        "classification_explanation": "The theory generalizes existing principles to a broader context and asserts necessity, making it closely-related-to-existing.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [semantic fidelity in AMR]",
            "Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [semantic preservation in KG-to-text]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-613",
    "original_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
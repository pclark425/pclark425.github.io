<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Level Representation Theory of Embodied Knowledge in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-62</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-62</p>
                <p><strong>Name:</strong> Multi-Level Representation Theory of Embodied Knowledge in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models encode and utilize spatial, procedural, and object-relational knowledge for embodied planning tasks without direct sensory input, based on the following results.</p>
                <p><strong>Description:</strong> Language models encode and utilize spatial, procedural, and object-relational knowledge through a hierarchy of representational systems operating at different levels of abstraction and explicitness: (1) Implicit distributed representations in model weights encoding statistical patterns and semantic priors from pretraining; (2) Learned structured representations including attention mechanisms, feature modulations, and multimodal embeddings that bridge modalities and enable compositional reasoning; (3) Explicit symbolic representations (code, coordinates, textual maps, skill sequences) that can be generated, manipulated, and verified. Effective embodied planning without direct sensory input requires orchestrating across these levels, with each level compensating for limitations at other levels. The implicit system provides semantic priors and pattern completion but lacks precision; learned structured representations enable grounding and composition but require task-specific training; explicit symbolic representations enable precise reasoning and verification but depend on the other systems for generation and interpretation. The optimal balance depends on task characteristics, available training data, and computational constraints.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models encode spatial, procedural, and object-relational knowledge across multiple representational levels: implicit distributed weights, learned structured representations (attention, embeddings, affordances), and explicit symbolic artifacts</li>
                <li>The implicit system provides semantic priors and pattern completion but fails at precise numeric computation, long-horizon planning, and compositional generalization without augmentation</li>
                <li>Learned structured representations (attention mechanisms, feature modulation, multimodal embeddings, affordance functions) enable grounding and compositional reasoning but require task-specific training or alignment</li>
                <li>Explicit symbolic representations (code, coordinates, symbolic states, textual maps) enable precise reasoning, verification, and composition but require other systems to generate and interpret them</li>
                <li>Effective embodied planning without direct sensory input requires orchestrating across representational levels, with each level compensating for limitations at other levels</li>
                <li>The optimal balance between representational levels depends on task characteristics: short-horizon tasks may use implicit knowledge alone, while long-horizon planning requires explicit representations; perceptual grounding benefits from learned multimodal alignment</li>
                <li>Multimodal alignment (vision-language pretraining) enables implicit systems to ground spatial concepts in perceptual representations, improving zero-shot transfer and semantic understanding</li>
                <li>Structured intermediate representations (skill APIs, coordinate systems, attention maps) are more effective than unstructured natural language for embodied planning</li>
                <li>Iterative refinement between representational levels (generate-verify-refine, multi-round planning) improves performance over single-pass generation</li>
                <li>Fine-tuning and specialization can shift knowledge between representational levels, improving task-specific performance while potentially reducing general transfer</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>LLMs can generate explicit code and symbolic representations (Python, MJCF, SVG, ASP, skill sequences) that encode spatial and procedural knowledge, enabling precise geometric reasoning and verification that implicit representations alone cannot achieve <a href="../results/extraction-result-367.html#e367.1" class="evidence-link">[e367.1]</a> <a href="../results/extraction-result-532.html#e532.1" class="evidence-link">[e532.1]</a> <a href="../results/extraction-result-545.html#e545.0" class="evidence-link">[e545.0]</a> <a href="../results/extraction-result-392.html#e392.0" class="evidence-link">[e392.0]</a> <a href="../results/extraction-result-518.html#e518.0" class="evidence-link">[e518.0]</a> <a href="../results/extraction-result-535.html#e535.2" class="evidence-link">[e535.2]</a> <a href="../results/extraction-result-395.html#e395.0" class="evidence-link">[e395.0]</a> <a href="../results/extraction-result-367.html#e367.0" class="evidence-link">[e367.0]</a> </li>
    <li>Text-only models show poor performance on spatial planning tasks when relying solely on implicit knowledge, but improve dramatically when augmented with explicit representations like simulation outputs, structured scene descriptions, or generated visualizations <a href="../results/extraction-result-545.html#e545.2" class="evidence-link">[e545.2]</a> <a href="../results/extraction-result-518.html#e518.0" class="evidence-link">[e518.0]</a> <a href="../results/extraction-result-511.html#e511.0" class="evidence-link">[e511.0]</a> <a href="../results/extraction-result-358.html#e358.0" class="evidence-link">[e358.0]</a> <a href="../results/extraction-result-342.html#e342.0" class="evidence-link">[e342.0]</a> <a href="../results/extraction-result-372.html#e372.6" class="evidence-link">[e372.6]</a> </li>
    <li>Learned structured representations including attention mechanisms (MAC networks), feature modulation (FiLM), and multimodal embeddings enable compositional spatial reasoning without requiring explicit symbolic representations <a href="../results/extraction-result-508.html#e508.0" class="evidence-link">[e508.0]</a> <a href="../results/extraction-result-514.html#e514.0" class="evidence-link">[e514.0]</a> <a href="../results/extraction-result-369.html#e369.4" class="evidence-link">[e369.4]</a> <a href="../results/extraction-result-369.html#e369.3" class="evidence-link">[e369.3]</a> <a href="../results/extraction-result-340.html#e340.0" class="evidence-link">[e340.0]</a> </li>
    <li>Multimodal alignment through contrastive learning (CLIP) or joint training enables language models to ground spatial and object-relational concepts in visual representations, improving zero-shot transfer and semantic grounding <a href="../results/extraction-result-376.html#e376.0" class="evidence-link">[e376.0]</a> <a href="../results/extraction-result-536.html#e536.0" class="evidence-link">[e536.0]</a> <a href="../results/extraction-result-547.html#e547.0" class="evidence-link">[e547.0]</a> <a href="../results/extraction-result-527.html#e527.0" class="evidence-link">[e527.0]</a> <a href="../results/extraction-result-355.html#e355.0" class="evidence-link">[e355.0]</a> <a href="../results/extraction-result-385.html#e385.1" class="evidence-link">[e385.1]</a> </li>
    <li>Learned affordance representations and value functions provide explicit grounding of procedural knowledge by encoding which actions are feasible in current states, bridging implicit LLM knowledge and executable actions <a href="../results/extraction-result-549.html#e549.1" class="evidence-link">[e549.1]</a> <a href="../results/extraction-result-351.html#e351.0" class="evidence-link">[e351.0]</a> <a href="../results/extraction-result-394.html#e394.0" class="evidence-link">[e394.0]</a> <a href="../results/extraction-result-543.html#e543.0" class="evidence-link">[e543.0]</a> </li>
    <li>Explicit spatial maps (semantic maps, VLMaps, topological graphs) constructed from perception enable precise spatial reasoning and navigation that language-only or implicit representations cannot support <a href="../results/extraction-result-519.html#e519.0" class="evidence-link">[e519.0]</a> <a href="../results/extraction-result-365.html#e365.3" class="evidence-link">[e365.3]</a> <a href="../results/extraction-result-530.html#e530.3" class="evidence-link">[e530.3]</a> <a href="../results/extraction-result-394.html#e394.0" class="evidence-link">[e394.0]</a> <a href="../results/extraction-result-341.html#e341.0" class="evidence-link">[e341.0]</a> </li>
    <li>Fine-tuning and specialization of embeddings on spatial tasks improves spatial reasoning performance, showing that task-specific learned representations complement general implicit knowledge <a href="../results/extraction-result-537.html#e537.3" class="evidence-link">[e537.3]</a> <a href="../results/extraction-result-512.html#e512.0" class="evidence-link">[e512.0]</a> <a href="../results/extraction-result-526.html#e526.0" class="evidence-link">[e526.0]</a> <a href="../results/extraction-result-425.html#e425.0" class="evidence-link">[e425.0]</a> <a href="../results/extraction-result-395.html#e395.0" class="evidence-link">[e395.0]</a> </li>
    <li>Hybrid neuro-symbolic systems that combine implicit LLM knowledge with explicit symbolic reasoning consistently outperform pure neural or pure symbolic baselines on multi-hop spatial and procedural reasoning <a href="../results/extraction-result-518.html#e518.0" class="evidence-link">[e518.0]</a> <a href="../results/extraction-result-520.html#e520.3" class="evidence-link">[e520.3]</a> <a href="../results/extraction-result-344.html#e344.0" class="evidence-link">[e344.0]</a> <a href="../results/extraction-result-365.html#e365.0" class="evidence-link">[e365.0]</a> <a href="../results/extraction-result-535.html#e535.2" class="evidence-link">[e535.2]</a> <a href="../results/extraction-result-357.html#e357.0" class="evidence-link">[e357.0]</a> </li>
    <li>The implicit system encodes semantic priors (object co-occurrence, typical spatial arrangements, procedural sequences) from text corpora, providing useful likelihood scores and parsing capabilities without task-specific training <a href="../results/extraction-result-351.html#e351.0" class="evidence-link">[e351.0]</a> <a href="../results/extraction-result-349.html#e349.0" class="evidence-link">[e349.0]</a> <a href="../results/extraction-result-341.html#e341.0" class="evidence-link">[e341.0]</a> <a href="../results/extraction-result-539.html#e539.0" class="evidence-link">[e539.0]</a> <a href="../results/extraction-result-350.html#e350.0" class="evidence-link">[e350.0]</a> <a href="../results/extraction-result-368.html#e368.0" class="evidence-link">[e368.0]</a> </li>
    <li>Explicit representations enable verification, error correction, and iterative refinement that implicit representations cannot support, as demonstrated by systems using code verification, constraint checking, and replanning <a href="../results/extraction-result-354.html#e354.2" class="evidence-link">[e354.2]</a> <a href="../results/extraction-result-354.html#e354.3" class="evidence-link">[e354.3]</a> <a href="../results/extraction-result-395.html#e395.0" class="evidence-link">[e395.0]</a> <a href="../results/extraction-result-535.html#e535.2" class="evidence-link">[e535.2]</a> <a href="../results/extraction-result-511.html#e511.0" class="evidence-link">[e511.0]</a> <a href="../results/extraction-result-352.html#e352.0" class="evidence-link">[e352.0]</a> </li>
    <li>Structured intermediate representations (skill APIs, coordinate systems, symbolic predicates, attention maps) outperform unstructured natural language for embodied planning, showing the importance of representation format <a href="../results/extraction-result-395.html#e395.0" class="evidence-link">[e395.0]</a> <a href="../results/extraction-result-342.html#e342.2" class="evidence-link">[e342.2]</a> <a href="../results/extraction-result-531.html#e531.0" class="evidence-link">[e531.0]</a> <a href="../results/extraction-result-513.html#e513.0" class="evidence-link">[e513.0]</a> <a href="../results/extraction-result-514.html#e514.0" class="evidence-link">[e514.0]</a> <a href="../results/extraction-result-508.html#e508.0" class="evidence-link">[e508.0]</a> </li>
    <li>Models can translate between representation levels: parsing natural language to symbolic facts, generating visualizations from text, producing code from descriptions, and converting visual features to language-aligned embeddings <a href="../results/extraction-result-518.html#e518.0" class="evidence-link">[e518.0]</a> <a href="../results/extraction-result-358.html#e358.0" class="evidence-link">[e358.0]</a> <a href="../results/extraction-result-367.html#e367.0" class="evidence-link">[e367.0]</a> <a href="../results/extraction-result-392.html#e392.3" class="evidence-link">[e392.3]</a> <a href="../results/extraction-result-376.html#e376.0" class="evidence-link">[e376.0]</a> <a href="../results/extraction-result-385.html#e385.1" class="evidence-link">[e385.1]</a> </li>
    <li>Learned perceptual grounding through vision encoders (ReCon++, PointNet++, VinVL) provides object-centric spatial representations that enable language models to reason about 3D geometry and spatial relations <a href="../results/extraction-result-425.html#e425.1" class="evidence-link">[e425.1]</a> <a href="../results/extraction-result-425.html#e425.0" class="evidence-link">[e425.0]</a> <a href="../results/extraction-result-527.html#e527.0" class="evidence-link">[e527.0]</a> <a href="../results/extraction-result-512.html#e512.0" class="evidence-link">[e512.0]</a> <a href="../results/extraction-result-385.html#e385.1" class="evidence-link">[e385.1]</a> <a href="../results/extraction-result-340.html#e340.0" class="evidence-link">[e340.0]</a> </li>
    <li>Iterative refinement between representation levels (generate-verify-refine loops, multi-round planning with memory) improves performance over single-pass generation <a href="../results/extraction-result-395.html#e395.0" class="evidence-link">[e395.0]</a> <a href="../results/extraction-result-352.html#e352.0" class="evidence-link">[e352.0]</a> <a href="../results/extraction-result-354.html#e354.0" class="evidence-link">[e354.0]</a> <a href="../results/extraction-result-368.html#e368.4" class="evidence-link">[e368.4]</a> <a href="../results/extraction-result-511.html#e511.0" class="evidence-link">[e511.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Training LLMs with explicit intermediate representations (code, coordinates, symbolic states) during pretraining will improve their embodied planning capabilities even when those representations are not provided at test time</li>
                <li>Prompting strategies that explicitly request intermediate symbolic representations (like VoT or code generation) will improve performance on novel spatial reasoning tasks not seen during training</li>
                <li>Models that can translate between multiple representation formats (code ↔ coordinates ↔ natural language ↔ symbolic predicates ↔ attention maps) will show better transfer across different embodied domains</li>
                <li>Fine-tuning on tasks that require explicit representation generation will improve implicit spatial reasoning capabilities as measured by downstream task performance</li>
                <li>Combining multimodal alignment (vision-language pretraining) with explicit symbolic reasoning will outperform either approach alone on embodied planning tasks</li>
                <li>Systems that maintain explicit memory representations (semantic maps, object states, execution history) will show better long-horizon planning than systems relying only on implicit context</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether the implicit system can learn to internalize the precision of explicit systems through sufficient scale and training, potentially eliminating the need for explicit intermediate representations</li>
                <li>Whether there exists an optimal 'grain size' for explicit representations that balances expressiveness and learnability across different embodied tasks</li>
                <li>Whether models trained primarily on code (which contains explicit spatial/geometric representations) will develop better implicit spatial reasoning than models trained primarily on natural language</li>
                <li>Whether learned structured representations (attention, embeddings) can eventually replace explicit symbolic representations for all embodied tasks with sufficient architectural innovation</li>
                <li>Whether the multi-level representation architecture is fundamental to intelligence or an artifact of current training paradigms that could be overcome with different architectures or training procedures</li>
                <li>Whether there are fundamental computational limits to what can be encoded in implicit vs learned vs explicit representations, or whether these are merely engineering trade-offs</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where pure implicit representations (no explicit intermediate steps or learned structured representations) consistently outperform hybrid approaches would challenge the necessity of multiple representational levels</li>
                <li>Demonstrating that explicit representations provide no benefit when the implicit system is scaled sufficiently (e.g., 10x larger models) would question the fundamental limitation of implicit representations</li>
                <li>Showing that learned structured representations (attention, embeddings) cannot improve over implicit-only baselines even with extensive task-specific training would challenge the value of the learned representation level</li>
                <li>Finding that explicit representations generated by LLMs are no more useful than random structured outputs would question whether the implicit system meaningfully guides explicit generation</li>
                <li>Demonstrating that multimodal alignment provides no benefit over text-only training for embodied tasks would challenge the importance of perceptual grounding</li>
                <li>Finding cases where end-to-end learned systems consistently outperform neuro-symbolic hybrids across diverse tasks would question the universal value of explicit symbolic representations</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact mechanisms by which pretraining data statistics are converted into implicit spatial knowledge remain unclear, particularly for spatial relations not explicitly described in text <a href="../results/extraction-result-376.html#e376.3" class="evidence-link">[e376.3]</a> <a href="../results/extraction-result-358.html#e358.2" class="evidence-link">[e358.2]</a> <a href="../results/extraction-result-348.html#e348.0" class="evidence-link">[e348.0]</a> </li>
    <li>Why some explicit representations (code, coordinates) are more learnable than others (natural language descriptions) is not fully explained by the theory <a href="../results/extraction-result-367.html#e367.1" class="evidence-link">[e367.1]</a> <a href="../results/extraction-result-342.html#e342.0" class="evidence-link">[e342.0]</a> <a href="../results/extraction-result-518.html#e518.0" class="evidence-link">[e518.0]</a> </li>
    <li>The theory does not specify how to optimally partition knowledge between representational levels for different task types or how to determine when to use which level </li>
    <li>Individual differences in how different model architectures balance implicit vs learned vs explicit representations are not fully addressed <a href="../results/extraction-result-514.html#e514.0" class="evidence-link">[e514.0]</a> <a href="../results/extraction-result-508.html#e508.0" class="evidence-link">[e508.0]</a> <a href="../results/extraction-result-369.html#e369.3" class="evidence-link">[e369.3]</a> <a href="../results/extraction-result-369.html#e369.4" class="evidence-link">[e369.4]</a> </li>
    <li>The role of embodied interaction and online learning in shaping representations across levels is not fully accounted for <a href="../results/extraction-result-544.html#e544.4" class="evidence-link">[e544.4]</a> <a href="../results/extraction-result-531.html#e531.0" class="evidence-link">[e531.0]</a> <a href="../results/extraction-result-357.html#e357.0" class="evidence-link">[e357.0]</a> </li>
    <li>How representations at different levels interact during inference (e.g., whether they operate sequentially, in parallel, or through feedback loops) is not fully specified </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [Dual-process theory of cognition with System 1 (fast, intuitive) and System 2 (slow, deliberate) - analogous to implicit and explicit systems]</li>
    <li>Garnelo & Shanahan (2019) Reconciling deep learning with symbolic artificial intelligence [Discusses integration of neural and symbolic AI, related framework for combining learned and symbolic representations]</li>
    <li>Kambhampati et al. (2024) LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks [Proposes LLMs as semantic parsers feeding symbolic planners, closely related architecture]</li>
    <li>Andreas et al. (2016) Neural Module Networks [Compositional approach combining learned modules with explicit structure, related to multi-level representation]</li>
    <li>Lake et al. (2017) Building machines that learn and think like people [Discusses multiple levels of representation in human cognition and AI, including model-based and model-free learning]</li>
    <li>Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Discusses structured representations and relational reasoning in neural networks]</li>
    <li>Marcus (2020) The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence [Argues for hybrid systems combining neural and symbolic approaches]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Level Representation Theory of Embodied Knowledge in Language Models",
    "theory_description": "Language models encode and utilize spatial, procedural, and object-relational knowledge through a hierarchy of representational systems operating at different levels of abstraction and explicitness: (1) Implicit distributed representations in model weights encoding statistical patterns and semantic priors from pretraining; (2) Learned structured representations including attention mechanisms, feature modulations, and multimodal embeddings that bridge modalities and enable compositional reasoning; (3) Explicit symbolic representations (code, coordinates, textual maps, skill sequences) that can be generated, manipulated, and verified. Effective embodied planning without direct sensory input requires orchestrating across these levels, with each level compensating for limitations at other levels. The implicit system provides semantic priors and pattern completion but lacks precision; learned structured representations enable grounding and composition but require task-specific training; explicit symbolic representations enable precise reasoning and verification but depend on the other systems for generation and interpretation. The optimal balance depends on task characteristics, available training data, and computational constraints.",
    "supporting_evidence": [
        {
            "text": "LLMs can generate explicit code and symbolic representations (Python, MJCF, SVG, ASP, skill sequences) that encode spatial and procedural knowledge, enabling precise geometric reasoning and verification that implicit representations alone cannot achieve",
            "uuids": [
                "e367.1",
                "e532.1",
                "e545.0",
                "e392.0",
                "e518.0",
                "e535.2",
                "e395.0",
                "e367.0"
            ]
        },
        {
            "text": "Text-only models show poor performance on spatial planning tasks when relying solely on implicit knowledge, but improve dramatically when augmented with explicit representations like simulation outputs, structured scene descriptions, or generated visualizations",
            "uuids": [
                "e545.2",
                "e518.0",
                "e511.0",
                "e358.0",
                "e342.0",
                "e372.6"
            ]
        },
        {
            "text": "Learned structured representations including attention mechanisms (MAC networks), feature modulation (FiLM), and multimodal embeddings enable compositional spatial reasoning without requiring explicit symbolic representations",
            "uuids": [
                "e508.0",
                "e514.0",
                "e369.4",
                "e369.3",
                "e340.0"
            ]
        },
        {
            "text": "Multimodal alignment through contrastive learning (CLIP) or joint training enables language models to ground spatial and object-relational concepts in visual representations, improving zero-shot transfer and semantic grounding",
            "uuids": [
                "e376.0",
                "e536.0",
                "e547.0",
                "e527.0",
                "e355.0",
                "e385.1"
            ]
        },
        {
            "text": "Learned affordance representations and value functions provide explicit grounding of procedural knowledge by encoding which actions are feasible in current states, bridging implicit LLM knowledge and executable actions",
            "uuids": [
                "e549.1",
                "e351.0",
                "e394.0",
                "e543.0"
            ]
        },
        {
            "text": "Explicit spatial maps (semantic maps, VLMaps, topological graphs) constructed from perception enable precise spatial reasoning and navigation that language-only or implicit representations cannot support",
            "uuids": [
                "e519.0",
                "e365.3",
                "e530.3",
                "e394.0",
                "e341.0"
            ]
        },
        {
            "text": "Fine-tuning and specialization of embeddings on spatial tasks improves spatial reasoning performance, showing that task-specific learned representations complement general implicit knowledge",
            "uuids": [
                "e537.3",
                "e512.0",
                "e526.0",
                "e425.0",
                "e395.0"
            ]
        },
        {
            "text": "Hybrid neuro-symbolic systems that combine implicit LLM knowledge with explicit symbolic reasoning consistently outperform pure neural or pure symbolic baselines on multi-hop spatial and procedural reasoning",
            "uuids": [
                "e518.0",
                "e520.3",
                "e344.0",
                "e365.0",
                "e535.2",
                "e357.0"
            ]
        },
        {
            "text": "The implicit system encodes semantic priors (object co-occurrence, typical spatial arrangements, procedural sequences) from text corpora, providing useful likelihood scores and parsing capabilities without task-specific training",
            "uuids": [
                "e351.0",
                "e349.0",
                "e341.0",
                "e539.0",
                "e350.0",
                "e368.0"
            ]
        },
        {
            "text": "Explicit representations enable verification, error correction, and iterative refinement that implicit representations cannot support, as demonstrated by systems using code verification, constraint checking, and replanning",
            "uuids": [
                "e354.2",
                "e354.3",
                "e395.0",
                "e535.2",
                "e511.0",
                "e352.0"
            ]
        },
        {
            "text": "Structured intermediate representations (skill APIs, coordinate systems, symbolic predicates, attention maps) outperform unstructured natural language for embodied planning, showing the importance of representation format",
            "uuids": [
                "e395.0",
                "e342.2",
                "e531.0",
                "e513.0",
                "e514.0",
                "e508.0"
            ]
        },
        {
            "text": "Models can translate between representation levels: parsing natural language to symbolic facts, generating visualizations from text, producing code from descriptions, and converting visual features to language-aligned embeddings",
            "uuids": [
                "e518.0",
                "e358.0",
                "e367.0",
                "e392.3",
                "e376.0",
                "e385.1"
            ]
        },
        {
            "text": "Learned perceptual grounding through vision encoders (ReCon++, PointNet++, VinVL) provides object-centric spatial representations that enable language models to reason about 3D geometry and spatial relations",
            "uuids": [
                "e425.1",
                "e425.0",
                "e527.0",
                "e512.0",
                "e385.1",
                "e340.0"
            ]
        },
        {
            "text": "Iterative refinement between representation levels (generate-verify-refine loops, multi-round planning with memory) improves performance over single-pass generation",
            "uuids": [
                "e395.0",
                "e352.0",
                "e354.0",
                "e368.4",
                "e511.0"
            ]
        }
    ],
    "theory_statements": [
        "Language models encode spatial, procedural, and object-relational knowledge across multiple representational levels: implicit distributed weights, learned structured representations (attention, embeddings, affordances), and explicit symbolic artifacts",
        "The implicit system provides semantic priors and pattern completion but fails at precise numeric computation, long-horizon planning, and compositional generalization without augmentation",
        "Learned structured representations (attention mechanisms, feature modulation, multimodal embeddings, affordance functions) enable grounding and compositional reasoning but require task-specific training or alignment",
        "Explicit symbolic representations (code, coordinates, symbolic states, textual maps) enable precise reasoning, verification, and composition but require other systems to generate and interpret them",
        "Effective embodied planning without direct sensory input requires orchestrating across representational levels, with each level compensating for limitations at other levels",
        "The optimal balance between representational levels depends on task characteristics: short-horizon tasks may use implicit knowledge alone, while long-horizon planning requires explicit representations; perceptual grounding benefits from learned multimodal alignment",
        "Multimodal alignment (vision-language pretraining) enables implicit systems to ground spatial concepts in perceptual representations, improving zero-shot transfer and semantic understanding",
        "Structured intermediate representations (skill APIs, coordinate systems, attention maps) are more effective than unstructured natural language for embodied planning",
        "Iterative refinement between representational levels (generate-verify-refine, multi-round planning) improves performance over single-pass generation",
        "Fine-tuning and specialization can shift knowledge between representational levels, improving task-specific performance while potentially reducing general transfer"
    ],
    "new_predictions_likely": [
        "Training LLMs with explicit intermediate representations (code, coordinates, symbolic states) during pretraining will improve their embodied planning capabilities even when those representations are not provided at test time",
        "Prompting strategies that explicitly request intermediate symbolic representations (like VoT or code generation) will improve performance on novel spatial reasoning tasks not seen during training",
        "Models that can translate between multiple representation formats (code ↔ coordinates ↔ natural language ↔ symbolic predicates ↔ attention maps) will show better transfer across different embodied domains",
        "Fine-tuning on tasks that require explicit representation generation will improve implicit spatial reasoning capabilities as measured by downstream task performance",
        "Combining multimodal alignment (vision-language pretraining) with explicit symbolic reasoning will outperform either approach alone on embodied planning tasks",
        "Systems that maintain explicit memory representations (semantic maps, object states, execution history) will show better long-horizon planning than systems relying only on implicit context"
    ],
    "new_predictions_unknown": [
        "Whether the implicit system can learn to internalize the precision of explicit systems through sufficient scale and training, potentially eliminating the need for explicit intermediate representations",
        "Whether there exists an optimal 'grain size' for explicit representations that balances expressiveness and learnability across different embodied tasks",
        "Whether models trained primarily on code (which contains explicit spatial/geometric representations) will develop better implicit spatial reasoning than models trained primarily on natural language",
        "Whether learned structured representations (attention, embeddings) can eventually replace explicit symbolic representations for all embodied tasks with sufficient architectural innovation",
        "Whether the multi-level representation architecture is fundamental to intelligence or an artifact of current training paradigms that could be overcome with different architectures or training procedures",
        "Whether there are fundamental computational limits to what can be encoded in implicit vs learned vs explicit representations, or whether these are merely engineering trade-offs"
    ],
    "negative_experiments": [
        "Finding tasks where pure implicit representations (no explicit intermediate steps or learned structured representations) consistently outperform hybrid approaches would challenge the necessity of multiple representational levels",
        "Demonstrating that explicit representations provide no benefit when the implicit system is scaled sufficiently (e.g., 10x larger models) would question the fundamental limitation of implicit representations",
        "Showing that learned structured representations (attention, embeddings) cannot improve over implicit-only baselines even with extensive task-specific training would challenge the value of the learned representation level",
        "Finding that explicit representations generated by LLMs are no more useful than random structured outputs would question whether the implicit system meaningfully guides explicit generation",
        "Demonstrating that multimodal alignment provides no benefit over text-only training for embodied tasks would challenge the importance of perceptual grounding",
        "Finding cases where end-to-end learned systems consistently outperform neuro-symbolic hybrids across diverse tasks would question the universal value of explicit symbolic representations"
    ],
    "unaccounted_for": [
        {
            "text": "The exact mechanisms by which pretraining data statistics are converted into implicit spatial knowledge remain unclear, particularly for spatial relations not explicitly described in text",
            "uuids": [
                "e376.3",
                "e358.2",
                "e348.0"
            ]
        },
        {
            "text": "Why some explicit representations (code, coordinates) are more learnable than others (natural language descriptions) is not fully explained by the theory",
            "uuids": [
                "e367.1",
                "e342.0",
                "e518.0"
            ]
        },
        {
            "text": "The theory does not specify how to optimally partition knowledge between representational levels for different task types or how to determine when to use which level",
            "uuids": []
        },
        {
            "text": "Individual differences in how different model architectures balance implicit vs learned vs explicit representations are not fully addressed",
            "uuids": [
                "e514.0",
                "e508.0",
                "e369.3",
                "e369.4"
            ]
        },
        {
            "text": "The role of embodied interaction and online learning in shaping representations across levels is not fully accounted for",
            "uuids": [
                "e544.4",
                "e531.0",
                "e357.0"
            ]
        },
        {
            "text": "How representations at different levels interact during inference (e.g., whether they operate sequentially, in parallel, or through feedback loops) is not fully specified",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models (MAC networks, FiLM) achieve strong compositional reasoning with learned structured representations (attention, feature modulation) without explicit symbolic representations, suggesting the learned level may be sufficient for many tasks",
            "uuids": [
                "e508.0",
                "e514.0",
                "e369.4",
                "e369.3"
            ]
        },
        {
            "text": "End-to-end learned systems sometimes match or exceed neuro-symbolic approaches on specific benchmarks, questioning whether explicit symbolic representations are always necessary",
            "uuids": [
                "e533.0",
                "e530.3",
                "e340.0"
            ]
        },
        {
            "text": "Vision-language models with learned multimodal alignment can perform spatial reasoning without explicit symbolic representations or code generation, suggesting learned representations may be more powerful than the theory implies",
            "uuids": [
                "e376.0",
                "e355.0",
                "e385.1",
                "e527.0"
            ]
        },
        {
            "text": "Some text-only models (CapBERT) outperform vision-language models on certain spatial reasoning tasks, suggesting that implicit representations from text alone can sometimes exceed learned multimodal representations",
            "uuids": [
                "e337.0",
                "e337.1"
            ]
        }
    ],
    "special_cases": [
        "For tasks with strong distributional regularities in training data (e.g., common household layouts), implicit representations may suffice without learned structured or explicit symbolic augmentation",
        "Very short-horizon tasks (1-2 steps) may not benefit from explicit representations due to overhead costs, and learned structured representations may be sufficient",
        "Tasks requiring real-time performance may favor implicit or learned representations over explicit symbolic reasoning due to computational costs",
        "Domains with well-defined formal languages (programming, mathematics) may benefit more from explicit representations than open-ended natural domains",
        "Perceptual grounding tasks (object recognition, attribute classification) may benefit more from learned multimodal representations than from explicit symbolic reasoning",
        "Tasks requiring precise numeric computation or long-horizon planning may require explicit symbolic representations regardless of implicit or learned representation quality",
        "Zero-shot transfer scenarios may benefit more from implicit knowledge and multimodal alignment than from task-specific learned representations"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Kahneman (2011) Thinking, Fast and Slow [Dual-process theory of cognition with System 1 (fast, intuitive) and System 2 (slow, deliberate) - analogous to implicit and explicit systems]",
            "Garnelo & Shanahan (2019) Reconciling deep learning with symbolic artificial intelligence [Discusses integration of neural and symbolic AI, related framework for combining learned and symbolic representations]",
            "Kambhampati et al. (2024) LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks [Proposes LLMs as semantic parsers feeding symbolic planners, closely related architecture]",
            "Andreas et al. (2016) Neural Module Networks [Compositional approach combining learned modules with explicit structure, related to multi-level representation]",
            "Lake et al. (2017) Building machines that learn and think like people [Discusses multiple levels of representation in human cognition and AI, including model-based and model-free learning]",
            "Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Discusses structured representations and relational reasoning in neural networks]",
            "Marcus (2020) The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence [Argues for hybrid systems combining neural and symbolic approaches]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
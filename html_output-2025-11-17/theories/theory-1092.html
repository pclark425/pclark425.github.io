<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Theory of Logical Reasoning in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1092</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1092</p>
                <p><strong>Name:</strong> Dual-Process Theory of Logical Reasoning in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models (LMs) can best perform strict logical reasoning by integrating two distinct but interacting processes: (1) a fast, intuitive, pattern-matching process (System 1) that generates candidate logical inferences based on learned statistical regularities, and (2) a slow, deliberative, rule-based process (System 2) that verifies, corrects, or refines these inferences using explicit formal logic. The optimal performance arises when LMs are architecturally or algorithmically structured to allow dynamic interplay between these two processes, enabling both efficient hypothesis generation and rigorous logical validation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: System 1 Generates Candidate Inferences (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_exposed_to &#8594; logical reasoning task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; System 1 process &#8594; generates &#8594; candidate logical inferences</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Language models excel at generating plausible continuations and inferences based on statistical patterns in data. </li>
    <li>Empirical studies show LMs can often produce correct logical answers in familiar domains without explicit formalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While dual-process models exist in psychology, their explicit mapping to LM reasoning processes is new.</p>            <p><strong>What Already Exists:</strong> The dual-process theory is well-established in cognitive science, and LMs are known to excel at pattern-matching.</p>            <p><strong>What is Novel:</strong> Application of dual-process theory to LM architecture for logical reasoning is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [pattern-matching and reasoning in LMs]</li>
</ul>
            <h3>Statement 1: System 2 Validates and Refines Logical Inferences (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; candidate logical inference &#8594; is_generated_by &#8594; System 1 process</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; System 2 process &#8594; validates_or_refines &#8594; candidate logical inference<span style="color: #888888;">, and</span></div>
        <div>&#8226; final output &#8594; is &#8594; logically valid if System 2 accepts</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Explicit logical verification modules or symbolic reasoning steps improve LM logical accuracy. </li>
    <li>Hybrid neuro-symbolic systems outperform pure LMs on logic benchmarks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Combining dual-process theory with LM-specific architecture is a new synthesis.</p>            <p><strong>What Already Exists:</strong> Symbolic post-processing and verification are known in neuro-symbolic systems.</p>            <p><strong>What is Novel:</strong> The explicit dual-process interplay and mapping to LM architecture is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Creswell et al. (2022) Selection-inference: Exploiting large language models for interpretable logical reasoning [symbolic post-processing]</li>
    <li>Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs with explicit dual-process architectures will outperform single-process LMs on strict logical reasoning tasks.</li>
                <li>Introducing a symbolic validation step after initial LM inference will reduce logical errors.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Dynamic interaction between System 1 and System 2 may enable LMs to discover novel logical rules not present in training data.</li>
                <li>End-to-end training of both processes may lead to emergent logical reasoning capabilities beyond current benchmarks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If dual-process LMs do not outperform single-process LMs on logic tasks, the theory is challenged.</li>
                <li>If System 2 validation does not reduce logical errors, the theory is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some logical errors may arise from ambiguous or underspecified prompts that neither process can resolve. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes existing ideas but applies them in a novel architectural context for LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]</li>
    <li>Creswell et al. (2022) Selection-inference: Exploiting large language models for interpretable logical reasoning [symbolic post-processing]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [reasoning in LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Theory of Logical Reasoning in Language Models",
    "theory_description": "This theory posits that language models (LMs) can best perform strict logical reasoning by integrating two distinct but interacting processes: (1) a fast, intuitive, pattern-matching process (System 1) that generates candidate logical inferences based on learned statistical regularities, and (2) a slow, deliberative, rule-based process (System 2) that verifies, corrects, or refines these inferences using explicit formal logic. The optimal performance arises when LMs are architecturally or algorithmically structured to allow dynamic interplay between these two processes, enabling both efficient hypothesis generation and rigorous logical validation.",
    "theory_statements": [
        {
            "law": {
                "law_name": "System 1 Generates Candidate Inferences",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_exposed_to",
                        "object": "logical reasoning task"
                    }
                ],
                "then": [
                    {
                        "subject": "System 1 process",
                        "relation": "generates",
                        "object": "candidate logical inferences"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Language models excel at generating plausible continuations and inferences based on statistical patterns in data.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LMs can often produce correct logical answers in familiar domains without explicit formalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The dual-process theory is well-established in cognitive science, and LMs are known to excel at pattern-matching.",
                    "what_is_novel": "Application of dual-process theory to LM architecture for logical reasoning is novel.",
                    "classification_explanation": "While dual-process models exist in psychology, their explicit mapping to LM reasoning processes is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [pattern-matching and reasoning in LMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "System 2 Validates and Refines Logical Inferences",
                "if": [
                    {
                        "subject": "candidate logical inference",
                        "relation": "is_generated_by",
                        "object": "System 1 process"
                    }
                ],
                "then": [
                    {
                        "subject": "System 2 process",
                        "relation": "validates_or_refines",
                        "object": "candidate logical inference"
                    },
                    {
                        "subject": "final output",
                        "relation": "is",
                        "object": "logically valid if System 2 accepts"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Explicit logical verification modules or symbolic reasoning steps improve LM logical accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Hybrid neuro-symbolic systems outperform pure LMs on logic benchmarks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Symbolic post-processing and verification are known in neuro-symbolic systems.",
                    "what_is_novel": "The explicit dual-process interplay and mapping to LM architecture is novel.",
                    "classification_explanation": "Combining dual-process theory with LM-specific architecture is a new synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Creswell et al. (2022) Selection-inference: Exploiting large language models for interpretable logical reasoning [symbolic post-processing]",
                        "Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs with explicit dual-process architectures will outperform single-process LMs on strict logical reasoning tasks.",
        "Introducing a symbolic validation step after initial LM inference will reduce logical errors."
    ],
    "new_predictions_unknown": [
        "Dynamic interaction between System 1 and System 2 may enable LMs to discover novel logical rules not present in training data.",
        "End-to-end training of both processes may lead to emergent logical reasoning capabilities beyond current benchmarks."
    ],
    "negative_experiments": [
        "If dual-process LMs do not outperform single-process LMs on logic tasks, the theory is challenged.",
        "If System 2 validation does not reduce logical errors, the theory is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some logical errors may arise from ambiguous or underspecified prompts that neither process can resolve.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, LMs trained end-to-end without explicit symbolic modules can achieve high logical accuracy, challenging the necessity of dual-process separation.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks requiring rapid, informal reasoning may not benefit from System 2 validation.",
        "Highly creative or open-ended tasks may be constrained by strict logical validation."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process theory in human reasoning and symbolic post-processing in LMs.",
        "what_is_novel": "Explicit mapping of dual-process theory to LM architecture for logical reasoning.",
        "classification_explanation": "The theory synthesizes existing ideas but applies them in a novel architectural context for LMs.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]",
            "Creswell et al. (2022) Selection-inference: Exploiting large language models for interpretable logical reasoning [symbolic post-processing]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [reasoning in LMs]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-601",
    "original_theory_name": "Neuro-Symbolic Interface Bottleneck Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
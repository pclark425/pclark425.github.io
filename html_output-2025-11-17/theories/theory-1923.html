<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Instructional Framing and Cognitive Shortcut Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1923</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1923</p>
                <p><strong>Name:</strong> Instructional Framing and Cognitive Shortcut Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that instruction-tuned LLMs develop internal heuristics or 'cognitive shortcuts' that map specific instruction templates to response patterns, bypassing deep semantic analysis. The model's performance is thus a function of its learned mapping from template to output, and less a function of true task understanding, especially when the template is highly familiar.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Heuristic Mapping Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_instruction_tuned &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; input_prompt &#8594; matches_familiar_template &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; applies_heuristic_mapping &#8594; template_to_output<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; bypasses &#8594; deep_semantic_analysis</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs often produce correct output for familiar templates even when the input is nonsensical or adversarial, indicating reliance on learned heuristics. </li>
    <li>Performance drops sharply when templates are perturbed, even if the underlying task is unchanged. </li>
    <li>Instruction-tuned LLMs show high accuracy on tasks with canonical instruction phrasing, but accuracy degrades with minor rewordings. </li>
    <li>Adversarial prompt attacks that preserve task semantics but alter template structure can cause LLMs to fail. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This is a novel synthesis, drawing analogy to cognitive psychology and formalizing the shortcut mechanism.</p>            <p><strong>What Already Exists:</strong> Prompt-based heuristics and shallow pattern matching have been observed in LLMs.</p>            <p><strong>What is Novel:</strong> The explicit claim that instruction tuning induces cognitive shortcuts that can override semantic reasoning.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2023) Prompting Is Programming: A Survey of Prompt Engineering [discusses prompt heuristics, but not cognitive shortcut analogy]</li>
    <li>Niven & Kao (2019) Probing Neural Network Comprehension of Natural Language Arguments [shows shallow heuristics, but not in instruction-tuned LLMs]</li>
</ul>
            <h3>Statement 1: Template Familiarity-Performance Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; instruction_template &#8594; is_highly_familiar &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; task_semantics &#8594; is_ambiguous_or_complex &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; remains_high &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can answer complex or ambiguous questions correctly when presented in a familiar template, but fail when the template is changed. </li>
    <li>Instruction-tuned LLMs show robustness to semantic ambiguity when the prompt matches a well-learned template. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This is a new, explicit claim about the compensatory effect of template familiarity.</p>            <p><strong>What Already Exists:</strong> Performance improvements with familiar templates are known.</p>            <p><strong>What is Novel:</strong> The law that template familiarity can compensate for semantic ambiguity or complexity.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [shows prompt format effects, but not compensatory effect]</li>
    <li>Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [shows prompt sensitivity, not compensation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a model is given a familiar template with ambiguous content, it will still produce high-confidence, plausible outputs.</li>
                <li>If a model is given an unfamiliar template with clear semantics, performance will drop.</li>
                <li>Instruction-tuned LLMs will show higher error rates on tasks with novel instruction phrasing, even if the underlying task is unchanged.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is adversarially trained to break its own heuristics, it may develop deeper semantic reasoning.</li>
                <li>If a model is exposed to a curriculum of increasingly perturbed templates, it may gradually reduce reliance on shortcuts.</li>
                <li>If a model is trained with a large diversity of templates, the dominance of any single template may be reduced, leading to more robust semantic reasoning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a model always requires deep semantic analysis regardless of template, this would refute the theory.</li>
                <li>If performance does not correlate with template familiarity, the shortcut mechanism is unlikely.</li>
                <li>If LLMs perform equally well on both familiar and unfamiliar templates, the theory is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs show improved robustness to template variation after extensive multi-template training. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This theory is a novel synthesis, formalizing a mechanism not previously articulated in LLM literature.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2023) Prompting Is Programming: A Survey of Prompt Engineering [prompt heuristics]</li>
    <li>Niven & Kao (2019) Probing Neural Network Comprehension [shallow heuristics]</li>
    <li>Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [prompt sensitivity]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Instructional Framing and Cognitive Shortcut Theory",
    "theory_description": "This theory proposes that instruction-tuned LLMs develop internal heuristics or 'cognitive shortcuts' that map specific instruction templates to response patterns, bypassing deep semantic analysis. The model's performance is thus a function of its learned mapping from template to output, and less a function of true task understanding, especially when the template is highly familiar.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Heuristic Mapping Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_instruction_tuned",
                        "object": "True"
                    },
                    {
                        "subject": "input_prompt",
                        "relation": "matches_familiar_template",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "applies_heuristic_mapping",
                        "object": "template_to_output"
                    },
                    {
                        "subject": "LLM",
                        "relation": "bypasses",
                        "object": "deep_semantic_analysis"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs often produce correct output for familiar templates even when the input is nonsensical or adversarial, indicating reliance on learned heuristics.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops sharply when templates are perturbed, even if the underlying task is unchanged.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction-tuned LLMs show high accuracy on tasks with canonical instruction phrasing, but accuracy degrades with minor rewordings.",
                        "uuids": []
                    },
                    {
                        "text": "Adversarial prompt attacks that preserve task semantics but alter template structure can cause LLMs to fail.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt-based heuristics and shallow pattern matching have been observed in LLMs.",
                    "what_is_novel": "The explicit claim that instruction tuning induces cognitive shortcuts that can override semantic reasoning.",
                    "classification_explanation": "This is a novel synthesis, drawing analogy to cognitive psychology and formalizing the shortcut mechanism.",
                    "likely_classification": "new",
                    "references": [
                        "Jiang et al. (2023) Prompting Is Programming: A Survey of Prompt Engineering [discusses prompt heuristics, but not cognitive shortcut analogy]",
                        "Niven & Kao (2019) Probing Neural Network Comprehension of Natural Language Arguments [shows shallow heuristics, but not in instruction-tuned LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Template Familiarity-Performance Law",
                "if": [
                    {
                        "subject": "instruction_template",
                        "relation": "is_highly_familiar",
                        "object": "True"
                    },
                    {
                        "subject": "task_semantics",
                        "relation": "is_ambiguous_or_complex",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "remains_high",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can answer complex or ambiguous questions correctly when presented in a familiar template, but fail when the template is changed.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction-tuned LLMs show robustness to semantic ambiguity when the prompt matches a well-learned template.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Performance improvements with familiar templates are known.",
                    "what_is_novel": "The law that template familiarity can compensate for semantic ambiguity or complexity.",
                    "classification_explanation": "This is a new, explicit claim about the compensatory effect of template familiarity.",
                    "likely_classification": "new",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [shows prompt format effects, but not compensatory effect]",
                        "Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [shows prompt sensitivity, not compensation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a model is given a familiar template with ambiguous content, it will still produce high-confidence, plausible outputs.",
        "If a model is given an unfamiliar template with clear semantics, performance will drop.",
        "Instruction-tuned LLMs will show higher error rates on tasks with novel instruction phrasing, even if the underlying task is unchanged."
    ],
    "new_predictions_unknown": [
        "If a model is adversarially trained to break its own heuristics, it may develop deeper semantic reasoning.",
        "If a model is exposed to a curriculum of increasingly perturbed templates, it may gradually reduce reliance on shortcuts.",
        "If a model is trained with a large diversity of templates, the dominance of any single template may be reduced, leading to more robust semantic reasoning."
    ],
    "negative_experiments": [
        "If a model always requires deep semantic analysis regardless of template, this would refute the theory.",
        "If performance does not correlate with template familiarity, the shortcut mechanism is unlikely.",
        "If LLMs perform equally well on both familiar and unfamiliar templates, the theory is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs show improved robustness to template variation after extensive multi-template training.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Recent models with chain-of-thought or scratchpad training sometimes show improved semantic reasoning even with unfamiliar templates.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with strong external constraints (e.g., code generation) may force deeper reasoning.",
        "Very large models may develop hybrid strategies, blending heuristics and semantics."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt heuristics and shallow pattern matching are known.",
        "what_is_novel": "The explicit analogy to cognitive shortcuts and the compensatory effect of template familiarity.",
        "classification_explanation": "This theory is a novel synthesis, formalizing a mechanism not previously articulated in LLM literature.",
        "likely_classification": "new",
        "references": [
            "Jiang et al. (2023) Prompting Is Programming: A Survey of Prompt Engineering [prompt heuristics]",
            "Niven & Kao (2019) Probing Neural Network Comprehension [shallow heuristics]",
            "Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [prompt sensitivity]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emergent Reasoning Threshold Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-602</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-602</p>
                <p><strong>Name:</strong> Emergent Reasoning Threshold Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that the ability of language models to perform strict logical reasoning is an emergent property that appears only above certain scale and data diversity thresholds. Below these thresholds, LMs fail to perform multi-step or compositional logical reasoning, regardless of prompting strategy. Above the threshold, LMs can exhibit sudden, non-linear improvements in logical reasoning, especially when combined with appropriate prompting (e.g., chain-of-thought, least-to-most, self-consistency). However, even above the threshold, systematic generalization remains limited without explicit symbolic bias or augmentation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Emergent Reasoning Threshold (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; has_parameter_count &#8594; below critical threshold (e.g., <10B)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; fails_to_perform &#8594; multi-step logical reasoning, regardless of prompting</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Smaller and mid-scale models (GPT-2, GPT-Neo, GPT-J, T0, OPT) show flat or negligible gains from chain-of-thought prompting on arithmetic and logic tasks; emergent gains only appear at large scale. <a href="../results/extraction-result-5121.html#e5121.3" class="evidence-link">[e5121.3]</a> <a href="../results/extraction-result-4992.html#e4992.5" class="evidence-link">[e4992.5]</a> <a href="../results/extraction-result-5118.html#e5118.4" class="evidence-link">[e5118.4]</a> <a href="../results/extraction-result-5118.html#e5118.3" class="evidence-link">[e5118.3]</a> </li>
    <li>UL2-20B and other sub-20B models show minimal improvement on complex arithmetic benchmarks even with CoT prompting. <a href="../results/extraction-result-4992.html#e4992.5" class="evidence-link">[e4992.5]</a> </li>
    <li>PaLM, Codex, and InstructGPT show emergent jumps in performance on multi-step reasoning tasks only at large scale (e.g., 62B, 175B, 540B). <a href="../results/extraction-result-5116.html#e5116.1" class="evidence-link">[e5116.1]</a> <a href="../results/extraction-result-5116.html#e5116.0" class="evidence-link">[e5116.0]</a> <a href="../results/extraction-result-5116.html#e5116.2" class="evidence-link">[e5116.2]</a> <a href="../results/extraction-result-5118.html#e5118.7" class="evidence-link">[e5118.7]</a> <a href="../results/extraction-result-5118.html#e5118.6" class="evidence-link">[e5118.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Emergence is known, but the explicit threshold for logical reasoning and its dependence on scale and data is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Emergent abilities in LMs are documented in scaling law literature.</p>            <p><strong>What is Novel:</strong> This law formalizes the existence of a threshold for logical reasoning emergence and ties it to both scale and data diversity.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Emergent Abilities of Large Language Models [emergence in LMs]</li>
    <li>Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling and emergence]</li>
</ul>
            <h3>Statement 1: Prompting Unlocks Latent Reasoning Above Threshold (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; has_parameter_count &#8594; above critical threshold (e.g., >10B)<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompting_strategy &#8594; is &#8594; chain-of-thought, least-to-most, self-consistency, or similar</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; shows &#8594; sudden, non-linear improvement in multi-step logical reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Chain-of-thought prompting yields large improvements in multi-step reasoning for large models (e.g., Codex, PaLM, InstructGPT, GPT-3.5, GPT-4), but not for smaller models. <a href="../results/extraction-result-5116.html#e5116.0" class="evidence-link">[e5116.0]</a> <a href="../results/extraction-result-5116.html#e5116.2" class="evidence-link">[e5116.2]</a> <a href="../results/extraction-result-5116.html#e5116.1" class="evidence-link">[e5116.1]</a> <a href="../results/extraction-result-5121.html#e5121.0" class="evidence-link">[e5121.0]</a> <a href="../results/extraction-result-5107.html#e5107.2" class="evidence-link">[e5107.2]</a> <a href="../results/extraction-result-5107.html#e5107.0" class="evidence-link">[e5107.0]</a> <a href="../results/extraction-result-4992.html#e4992.2" class="evidence-link">[e4992.2]</a> <a href="../results/extraction-result-5090.html#e5090.1" class="evidence-link">[e5090.1]</a> </li>
    <li>Least-to-most and self-consistency prompting further improve performance above the threshold, especially on length generalization and compositional tasks. <a href="../results/extraction-result-5110.html#e5110.1" class="evidence-link">[e5110.1]</a> <a href="../results/extraction-result-5110.html#e5110.3" class="evidence-link">[e5110.3]</a> <a href="../results/extraction-result-5110.html#e5110.4" class="evidence-link">[e5110.4]</a> <a href="../results/extraction-result-5116.html#e5116.0" class="evidence-link">[e5116.0]</a> <a href="../results/extraction-result-5116.html#e5116.2" class="evidence-link">[e5116.2]</a> <a href="../results/extraction-result-5116.html#e5116.1" class="evidence-link">[e5116.1]</a> <a href="../results/extraction-result-5088.html#e5088.0" class="evidence-link">[e5088.0]</a> <a href="../results/extraction-result-5088.html#e5088.1" class="evidence-link">[e5088.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Prompting effects are known, but the explicit threshold and non-linearity for logical reasoning is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Prompting strategies are known to improve reasoning in large LMs.</p>            <p><strong>What is Novel:</strong> This law formalizes the interaction between scale and prompting, and the non-linear nature of the improvement.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [CoT emergence]</li>
    <li>Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [zero-shot-CoT]</li>
</ul>
            <h3>Statement 2: Systematic Generalization Remains Limited Without Symbolic Bias (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_above_emergence_threshold &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; systematic generalization to OOD logical structures</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; shows &#8594; limited generalization, unless symbolic augmentation is used</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Even large models (GPT-3, GPT-4, LLaMA-65B, PaLM-540B) show limited systematic generalization to OOD logical tasks without symbolic augmentation. <a href="../results/extraction-result-5124.html#e5124.0" class="evidence-link">[e5124.0]</a> <a href="../results/extraction-result-5124.html#e5124.3" class="evidence-link">[e5124.3]</a> <a href="../results/extraction-result-4995.html#e4995.5" class="evidence-link">[e4995.5]</a> <a href="../results/extraction-result-5108.html#e5108.2" class="evidence-link">[e5108.2]</a> <a href="../results/extraction-result-5004.html#e5004.2" class="evidence-link">[e5004.2]</a> <a href="../results/extraction-result-5008.html#e5008.3" class="evidence-link">[e5008.3]</a> </li>
    <li>Hybrid neuro-symbolic systems (SymbCoT, LogicLM, AMR-LDA, LReasoner) outperform large LMs on strict logic tasks, especially on OOD or human-authored data. <a href="../results/extraction-result-4988.html#e4988.1" class="evidence-link">[e4988.1]</a> <a href="../results/extraction-result-4953.html#e4953.0" class="evidence-link">[e4953.0]</a> <a href="../results/extraction-result-4984.html#e4984.7" class="evidence-link">[e4984.7]</a> <a href="../results/extraction-result-5113.html#e5113.0" class="evidence-link">[e5113.0]</a> <a href="../results/extraction-result-4974.html#e4974.1" class="evidence-link">[e4974.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is a synthesis of known generalization failures, but its explicit connection to emergence and symbolic bias is novel.</p>            <p><strong>What Already Exists:</strong> Systematic generalization failures are known in neural networks.</p>            <p><strong>What is Novel:</strong> This law ties the limitation to the lack of symbolic bias, even above the emergence threshold.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without systematicity [compositionality gap]</li>
    <li>Evans et al. (2023) Language models as logical solvers [NL-to-symbolic translation bottleneck]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new LM is trained with parameter count below the emergent threshold, it will fail to perform multi-step logical reasoning, regardless of prompting.</li>
                <li>If a model is scaled above the threshold, chain-of-thought and related prompting will yield sudden, large improvements in logical reasoning accuracy.</li>
                <li>Prompting strategies that decompose reasoning (e.g., least-to-most, self-consistency) will only be effective above the emergence threshold.</li>
                <li>Fine-tuning or instruction-tuning below the threshold will not yield systematic multi-step reasoning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained with a curriculum that gradually increases logical complexity, the emergence threshold may be lowered.</li>
                <li>If a model is trained on a massive, logic-rich corpus, the emergence threshold may shift to smaller model sizes.</li>
                <li>If a model is trained with explicit symbolic supervision, it may achieve systematic generalization below the standard emergence threshold.</li>
                <li>If a model is trained with multi-agent debate or self-refinement from the start, the threshold for emergent reasoning may be reduced.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a small model (<10B) can perform multi-step logical reasoning with chain-of-thought or similar prompting, this would challenge the theory.</li>
                <li>If a large model (>100B) fails to show any improvement with chain-of-thought or least-to-most prompting, this would challenge the theory.</li>
                <li>If systematic generalization is achieved below the emergence threshold, the theory would be called into question.</li>
                <li>If prompting strategies are equally effective at all scales, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some open-source models (e.g., Mistral-7B, Llama3-70B) achieve strong performance on formal logic MFTs and Multi-LogiEval with only NL input, suggesting that architecture and training data may also play a role. <a href="../results/extraction-result-5001.html#e5001.5" class="evidence-link">[e5001.5]</a> <a href="../results/extraction-result-4993.html#e4993.3" class="evidence-link">[e4993.3]</a> </li>
    <li>Preference optimization and fine-tuning on reasoning traces (e.g., DPO, PORT, Fine-tune-CoT) can improve LM reasoning on some tasks, even without explicit symbolic augmentation. <a href="../results/extraction-result-4975.html#e4975.0" class="evidence-link">[e4975.0]</a> <a href="../results/extraction-result-5094.html#e5094.3" class="evidence-link">[e5094.3]</a> <a href="../results/extraction-result-5094.html#e5094.4" class="evidence-link">[e5094.4]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> While emergence is known, the specific application to logical reasoning and the detailed threshold/prompting interaction is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Emergent Abilities of Large Language Models [emergence in LMs]</li>
    <li>Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling and emergence]</li>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [CoT emergence]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Emergent Reasoning Threshold Theory",
    "theory_description": "This theory posits that the ability of language models to perform strict logical reasoning is an emergent property that appears only above certain scale and data diversity thresholds. Below these thresholds, LMs fail to perform multi-step or compositional logical reasoning, regardless of prompting strategy. Above the threshold, LMs can exhibit sudden, non-linear improvements in logical reasoning, especially when combined with appropriate prompting (e.g., chain-of-thought, least-to-most, self-consistency). However, even above the threshold, systematic generalization remains limited without explicit symbolic bias or augmentation.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Emergent Reasoning Threshold",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "has_parameter_count",
                        "object": "below critical threshold (e.g., &lt;10B)"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "fails_to_perform",
                        "object": "multi-step logical reasoning, regardless of prompting"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Smaller and mid-scale models (GPT-2, GPT-Neo, GPT-J, T0, OPT) show flat or negligible gains from chain-of-thought prompting on arithmetic and logic tasks; emergent gains only appear at large scale.",
                        "uuids": [
                            "e5121.3",
                            "e4992.5",
                            "e5118.4",
                            "e5118.3"
                        ]
                    },
                    {
                        "text": "UL2-20B and other sub-20B models show minimal improvement on complex arithmetic benchmarks even with CoT prompting.",
                        "uuids": [
                            "e4992.5"
                        ]
                    },
                    {
                        "text": "PaLM, Codex, and InstructGPT show emergent jumps in performance on multi-step reasoning tasks only at large scale (e.g., 62B, 175B, 540B).",
                        "uuids": [
                            "e5116.1",
                            "e5116.0",
                            "e5116.2",
                            "e5118.7",
                            "e5118.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Emergent abilities in LMs are documented in scaling law literature.",
                    "what_is_novel": "This law formalizes the existence of a threshold for logical reasoning emergence and ties it to both scale and data diversity.",
                    "classification_explanation": "Emergence is known, but the explicit threshold for logical reasoning and its dependence on scale and data is a novel synthesis.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Emergent Abilities of Large Language Models [emergence in LMs]",
                        "Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling and emergence]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompting Unlocks Latent Reasoning Above Threshold",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "has_parameter_count",
                        "object": "above critical threshold (e.g., &gt;10B)"
                    },
                    {
                        "subject": "prompting_strategy",
                        "relation": "is",
                        "object": "chain-of-thought, least-to-most, self-consistency, or similar"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "shows",
                        "object": "sudden, non-linear improvement in multi-step logical reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Chain-of-thought prompting yields large improvements in multi-step reasoning for large models (e.g., Codex, PaLM, InstructGPT, GPT-3.5, GPT-4), but not for smaller models.",
                        "uuids": [
                            "e5116.0",
                            "e5116.2",
                            "e5116.1",
                            "e5121.0",
                            "e5107.2",
                            "e5107.0",
                            "e4992.2",
                            "e5090.1"
                        ]
                    },
                    {
                        "text": "Least-to-most and self-consistency prompting further improve performance above the threshold, especially on length generalization and compositional tasks.",
                        "uuids": [
                            "e5110.1",
                            "e5110.3",
                            "e5110.4",
                            "e5116.0",
                            "e5116.2",
                            "e5116.1",
                            "e5088.0",
                            "e5088.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompting strategies are known to improve reasoning in large LMs.",
                    "what_is_novel": "This law formalizes the interaction between scale and prompting, and the non-linear nature of the improvement.",
                    "classification_explanation": "Prompting effects are known, but the explicit threshold and non-linearity for logical reasoning is a novel synthesis.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [CoT emergence]",
                        "Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [zero-shot-CoT]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Systematic Generalization Remains Limited Without Symbolic Bias",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_above_emergence_threshold",
                        "object": "True"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "systematic generalization to OOD logical structures"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "shows",
                        "object": "limited generalization, unless symbolic augmentation is used"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Even large models (GPT-3, GPT-4, LLaMA-65B, PaLM-540B) show limited systematic generalization to OOD logical tasks without symbolic augmentation.",
                        "uuids": [
                            "e5124.0",
                            "e5124.3",
                            "e4995.5",
                            "e5108.2",
                            "e5004.2",
                            "e5008.3"
                        ]
                    },
                    {
                        "text": "Hybrid neuro-symbolic systems (SymbCoT, LogicLM, AMR-LDA, LReasoner) outperform large LMs on strict logic tasks, especially on OOD or human-authored data.",
                        "uuids": [
                            "e4988.1",
                            "e4953.0",
                            "e4984.7",
                            "e5113.0",
                            "e4974.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Systematic generalization failures are known in neural networks.",
                    "what_is_novel": "This law ties the limitation to the lack of symbolic bias, even above the emergence threshold.",
                    "classification_explanation": "The law is a synthesis of known generalization failures, but its explicit connection to emergence and symbolic bias is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Lake & Baroni (2018) Generalization without systematicity [compositionality gap]",
                        "Evans et al. (2023) Language models as logical solvers [NL-to-symbolic translation bottleneck]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new LM is trained with parameter count below the emergent threshold, it will fail to perform multi-step logical reasoning, regardless of prompting.",
        "If a model is scaled above the threshold, chain-of-thought and related prompting will yield sudden, large improvements in logical reasoning accuracy.",
        "Prompting strategies that decompose reasoning (e.g., least-to-most, self-consistency) will only be effective above the emergence threshold.",
        "Fine-tuning or instruction-tuning below the threshold will not yield systematic multi-step reasoning."
    ],
    "new_predictions_unknown": [
        "If a model is trained with a curriculum that gradually increases logical complexity, the emergence threshold may be lowered.",
        "If a model is trained on a massive, logic-rich corpus, the emergence threshold may shift to smaller model sizes.",
        "If a model is trained with explicit symbolic supervision, it may achieve systematic generalization below the standard emergence threshold.",
        "If a model is trained with multi-agent debate or self-refinement from the start, the threshold for emergent reasoning may be reduced."
    ],
    "negative_experiments": [
        "If a small model (&lt;10B) can perform multi-step logical reasoning with chain-of-thought or similar prompting, this would challenge the theory.",
        "If a large model (&gt;100B) fails to show any improvement with chain-of-thought or least-to-most prompting, this would challenge the theory.",
        "If systematic generalization is achieved below the emergence threshold, the theory would be called into question.",
        "If prompting strategies are equally effective at all scales, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Some open-source models (e.g., Mistral-7B, Llama3-70B) achieve strong performance on formal logic MFTs and Multi-LogiEval with only NL input, suggesting that architecture and training data may also play a role.",
            "uuids": [
                "e5001.5",
                "e4993.3"
            ]
        },
        {
            "text": "Preference optimization and fine-tuning on reasoning traces (e.g., DPO, PORT, Fine-tune-CoT) can improve LM reasoning on some tasks, even without explicit symbolic augmentation.",
            "uuids": [
                "e4975.0",
                "e5094.3",
                "e5094.4"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some smaller models (e.g., T5-11B, DeBERTa-large) achieve moderate performance on certain logic tasks after fine-tuning, suggesting that data and training regime can partially compensate for scale.",
            "uuids": [
                "e5095.3",
                "e5123.2"
            ]
        }
    ],
    "special_cases": [
        "Tasks with highly regular or templated NL (e.g., synthetic datasets) may allow LMs to learn direct mappings below the standard emergence threshold.",
        "For shallow reasoning (single-step or low-depth), LMs may perform well even below the threshold due to memorization or pattern matching.",
        "Instruction tuning or domain-specific pretraining may lower the effective emergence threshold for some tasks."
    ],
    "existing_theory": {
        "what_already_exists": "Emergent abilities and scaling laws are established in the LM literature.",
        "what_is_novel": "The explicit threshold for logical reasoning emergence and the interaction with prompting and symbolic bias is a novel synthesis.",
        "classification_explanation": "While emergence is known, the specific application to logical reasoning and the detailed threshold/prompting interaction is new.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Emergent Abilities of Large Language Models [emergence in LMs]",
            "Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling and emergence]",
            "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [CoT emergence]"
        ]
    },
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
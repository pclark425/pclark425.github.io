<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intrinsic Motivation Scaling Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-132</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-132</p>
                <p><strong>Name:</strong> Intrinsic Motivation Scaling Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of the fundamental trade-off between environment complexity and environment variation in embodied learning systems, based on the following results.</p>
                <p><strong>Description:</strong> Intrinsic motivation mechanisms (curiosity, learning progress, novelty, empowerment) enable autonomous curriculum generation in sparse-reward environments, but their effectiveness depends critically on the type of intrinsic reward, the environment's complexity-variation profile, the learning algorithm's generalization properties, and the task structure. Specifically: (1) competence-progress-based intrinsic rewards work best in very high-dimensional, slow-learning domains where progress is detectable over meaningful timescales; (2) novelty-based rewards work best in moderate-dimensional domains with clear state-space structure and benefit from disentangled representations; (3) prediction-error-based rewards can be misled by stochastic, unlearnable, or distractor aspects of the environment; (4) the optimal intrinsic motivation depends on whether the learning algorithm uses a generalizing model (favoring variance/novelty rewards) or a tabular model (favoring count-based rewards); (5) for hierarchical or compositional tasks, intrinsic motivation alone may be insufficient and benefits from combination with procedural/hierarchical mechanisms or social learning. Intrinsic motivation is most beneficial when environment variation is high but complexity is manageable, as it helps the agent discover and focus on learnable aspects of the environment while avoiding unlearnable distractors.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The optimal intrinsic motivation mechanism depends on environment complexity, variation, the learning algorithm's generalization properties, and task structure</li>
                <li>Competence-progress-based intrinsic rewards are most effective in very high-dimensional, slow-learning domains where progress is detectable over meaningful timescales</li>
                <li>Novelty-based intrinsic rewards are most effective in moderate-dimensional domains with clear state-space structure and benefit from disentangled representations</li>
                <li>Prediction-error-based intrinsic rewards can be misled by stochastic, unlearnable, or distractor aspects of the environment, leading to excessive exploration of uninformative regions</li>
                <li>For generalizing models (e.g., neural networks, random forests), variance-based and novelty-based intrinsic rewards outperform count-based rewards</li>
                <li>For tabular models, count-based intrinsic rewards (e.g., R-MAX) outperform variance-based rewards</li>
                <li>Intrinsic motivation is most beneficial when environment variation is high but complexity is manageable, as it helps discover learnable aspects</li>
                <li>For hierarchical or compositional tasks, intrinsic motivation alone may be insufficient and benefits from combination with procedural mechanisms or social learning</li>
                <li>Disentangled representations significantly improve intrinsic motivation effectiveness in environments with distractors or multiple independent factors</li>
                <li>The timescale of learning affects which intrinsic motivation works best: fast learning domains need immediate novelty/variance signals, slow learning domains can use competence-progress signals</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>TEXPLORE-VANIR with variance+novelty intrinsic rewards achieved lowest model error and highest task reward in Light World, outperforming prediction-error and R-MAX style exploration <a href="../results/extraction-result-1079.html#e1079.0" class="evidence-link">[e1079.0]</a> </li>
    <li>Competence-progress (R-IAC style) performed poorly in Light World because learning was too fast for progress detection, but is intended for very large continuous domains where learning is slow <a href="../results/extraction-result-1079.html#e1079.0" class="evidence-link">[e1079.0]</a> </li>
    <li>R-MAX style exploration worked well for tabular models but poorly for generalizing random-forest models, showing intrinsic motivation must match model type <a href="../results/extraction-result-1079.html#e1079.0" class="evidence-link">[e1079.0]</a> </li>
    <li>SAGG-RIAC with competence progress enabled fishing rod control learning (complex flexible dynamics) where random exploration failed <a href="../results/extraction-result-1063.html#e1063.2" class="evidence-link">[e1063.2]</a> </li>
    <li>SGIM-ACTS combining intrinsic motivation with social guidance outperformed pure intrinsic (SAGG-RIAC) and pure social methods on composite throwing-and-placing task <a href="../results/extraction-result-1070.html#e1070.0" class="evidence-link">[e1070.0]</a> </li>
    <li>Modular IMGEP with learning-progress-based goal selection strongly outperformed random parameter exploration in high-dimensional continuous action settings with distractors <a href="../results/extraction-result-1034.html#e1034.3" class="evidence-link">[e1034.3]</a> </li>
    <li>Causal curiosity (MDL-based intrinsic reward) yielded ~2.5× better sample efficiency than conventional planners for discovering causal experiments in CausalWorld <a href="../results/extraction-result-1066.html#e1066.0" class="evidence-link">[e1066.0]</a> </li>
    <li>IMRL with novelty-based intrinsic motivation achieved higher skill success ratios (~0.8) than prediction-error-based motivation (~0.7) in 2D Multi-Valley, with prediction-error driving excessive exploration <a href="../results/extraction-result-1013.html#e1013.0" class="evidence-link">[e1013.0]</a> </li>
    <li>RL-IMGEP survey emphasizes that high environment complexity (high-dimensional observations, sparse rewards, long horizons) amplifies challenges posed by high variation and requires mechanisms like curriculum learning and goal-sampling adaptation <a href="../results/extraction-result-1051.html#e1051.0" class="evidence-link">[e1051.0]</a> </li>
    <li>Self-play (asymmetric intrinsic motivation) enabled learning on MountainCar where target-only policy gradient failed, matching or exceeding exploration methods like VIME <a href="../results/extraction-result-1096.html#e1096.3" class="evidence-link">[e1096.3]</a> </li>
    <li>Disentangled modular IMGEP strongly outperformed entangled flat IMGEP in environments with distractors, showing representation quality critically affects intrinsic motivation effectiveness <a href="../results/extraction-result-1034.html#e1034.3" class="evidence-link">[e1034.3]</a> </li>
    <li>SGIM-PB combining intrinsic motivation with procedure-based exploration learned hierarchical tasks that intrinsic motivation alone (SAGG-RIAC) could not solve <a href="../results/extraction-result-1027.html#e1027.0" class="evidence-link">[e1027.0]</a> </li>
    <li>Random exploration baseline performed much worse than intrinsic motivation methods across multiple domains, confirming intrinsic motivation's value <a href="../results/extraction-result-1070.html#e1070.2" class="evidence-link">[e1070.2]</a> <a href="../results/extraction-result-1034.html#e1034.4" class="evidence-link">[e1034.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In stochastic environments with unpredictable elements, novelty-based intrinsic rewards will outperform prediction-error-based rewards by avoiding getting stuck on unlearnable stochasticity</li>
                <li>In very high-dimensional continuous domains (e.g., >100D state spaces) with slow learning, competence-progress-based rewards will outperform novelty-based rewards</li>
                <li>For agents using neural network function approximation in moderate-dimensional spaces, variance-based intrinsic rewards will outperform count-based rewards</li>
                <li>Combining multiple intrinsic motivation mechanisms (e.g., novelty + competence progress) with adaptive weighting will outperform single mechanisms across diverse environments</li>
                <li>In environments with visual distractors, intrinsic motivation combined with disentangled representation learning will substantially outperform intrinsic motivation with entangled representations</li>
                <li>For hierarchical tasks requiring skill composition, intrinsic motivation combined with procedural/hierarchical mechanisms will outperform flat intrinsic motivation</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a universal intrinsic motivation mechanism that automatically adapts to all environment types and learning algorithms without manual tuning</li>
                <li>Whether intrinsic motivation can enable learning on tasks that are fundamentally impossible without external rewards, or if there are hard limits</li>
                <li>Whether the benefits of intrinsic motivation scale indefinitely with environment complexity or exhibit diminishing returns beyond certain complexity thresholds</li>
                <li>Whether intrinsic motivation can be combined with extrinsic rewards without interference, or whether they require careful balancing mechanisms to avoid one dominating the other</li>
                <li>Whether meta-learning can discover optimal intrinsic motivation mechanisms for new environments faster than hand-design</li>
                <li>Whether intrinsic motivation mechanisms discovered through evolution or meta-learning would converge to the same types (novelty, progress, empowerment) or discover fundamentally new types</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding environments where intrinsic motivation consistently hurts learning compared to random exploration would challenge the theory's generality</li>
                <li>Demonstrating that the optimal intrinsic motivation does not depend on environment properties (complexity, variation, stochasticity) would contradict the matching principle</li>
                <li>Showing that prediction-error-based rewards work as well as novelty-based rewards in highly stochastic environments would challenge the stochasticity-sensitivity claim</li>
                <li>Finding that count-based rewards consistently outperform variance-based rewards for neural networks would contradict the generalization-type hypothesis</li>
                <li>Demonstrating that intrinsic motivation alone can solve hierarchical compositional tasks as well as combined intrinsic+procedural methods would challenge the hierarchical-limitation claim</li>
                <li>Finding that entangled representations work as well as disentangled representations for intrinsic motivation in environments with distractors would challenge the representation-quality claim</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically select or meta-learn the appropriate intrinsic motivation mechanism for a new environment without manual tuning or prior knowledge </li>
    <li>Whether intrinsic motivation can be learned or meta-learned rather than hand-designed, and if so, what the learned mechanisms would look like </li>
    <li>How intrinsic motivation interacts with different RL algorithms (on-policy vs off-policy, value-based vs policy-gradient) and network architectures beyond the cases studied </li>
    <li>The precise timescale thresholds that determine when competence-progress signals become detectable and useful versus when they are too slow </li>
    <li>How to combine multiple intrinsic motivation mechanisms optimally, including whether to use fixed weights, adaptive weights, or switching strategies </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Oudeyer & Kaplan (2007) What is intrinsic motivation? A typology of computational approaches [Foundational intrinsic motivation taxonomy defining novelty, competence, and knowledge-based intrinsic motivations]</li>
    <li>Schmidhuber (1991) A possibility for implementing curiosity and boredom in model-building neural controllers [Early prediction-error-based curiosity]</li>
    <li>Pathak et al. (2017) Curiosity-driven exploration by self-supervised prediction [Prediction-error-based curiosity for deep RL, ICM]</li>
    <li>Burda et al. (2018) Exploration by random network distillation [Novelty-based exploration using random network distillation]</li>
    <li>Baranes & Oudeyer (2013) Active learning of inverse models with intrinsically motivated goal exploration in robots [Competence-progress-based exploration, SAGG-RIAC]</li>
    <li>Forestier et al. (2017) Intrinsically motivated goal exploration processes with automatic curriculum learning [IMGEP framework and learning-progress-based goal selection]</li>
    <li>Colas et al. (2022) Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: A short survey [Recent survey on intrinsic motivation in goal-conditioned RL]</li>
    <li>Haber et al. (2018) Learning to play with intrinsically-motivated, self-aware agents [Empowerment-based intrinsic motivation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Intrinsic Motivation Scaling Theory",
    "theory_description": "Intrinsic motivation mechanisms (curiosity, learning progress, novelty, empowerment) enable autonomous curriculum generation in sparse-reward environments, but their effectiveness depends critically on the type of intrinsic reward, the environment's complexity-variation profile, the learning algorithm's generalization properties, and the task structure. Specifically: (1) competence-progress-based intrinsic rewards work best in very high-dimensional, slow-learning domains where progress is detectable over meaningful timescales; (2) novelty-based rewards work best in moderate-dimensional domains with clear state-space structure and benefit from disentangled representations; (3) prediction-error-based rewards can be misled by stochastic, unlearnable, or distractor aspects of the environment; (4) the optimal intrinsic motivation depends on whether the learning algorithm uses a generalizing model (favoring variance/novelty rewards) or a tabular model (favoring count-based rewards); (5) for hierarchical or compositional tasks, intrinsic motivation alone may be insufficient and benefits from combination with procedural/hierarchical mechanisms or social learning. Intrinsic motivation is most beneficial when environment variation is high but complexity is manageable, as it helps the agent discover and focus on learnable aspects of the environment while avoiding unlearnable distractors.",
    "supporting_evidence": [
        {
            "text": "TEXPLORE-VANIR with variance+novelty intrinsic rewards achieved lowest model error and highest task reward in Light World, outperforming prediction-error and R-MAX style exploration",
            "uuids": [
                "e1079.0"
            ]
        },
        {
            "text": "Competence-progress (R-IAC style) performed poorly in Light World because learning was too fast for progress detection, but is intended for very large continuous domains where learning is slow",
            "uuids": [
                "e1079.0"
            ]
        },
        {
            "text": "R-MAX style exploration worked well for tabular models but poorly for generalizing random-forest models, showing intrinsic motivation must match model type",
            "uuids": [
                "e1079.0"
            ]
        },
        {
            "text": "SAGG-RIAC with competence progress enabled fishing rod control learning (complex flexible dynamics) where random exploration failed",
            "uuids": [
                "e1063.2"
            ]
        },
        {
            "text": "SGIM-ACTS combining intrinsic motivation with social guidance outperformed pure intrinsic (SAGG-RIAC) and pure social methods on composite throwing-and-placing task",
            "uuids": [
                "e1070.0"
            ]
        },
        {
            "text": "Modular IMGEP with learning-progress-based goal selection strongly outperformed random parameter exploration in high-dimensional continuous action settings with distractors",
            "uuids": [
                "e1034.3"
            ]
        },
        {
            "text": "Causal curiosity (MDL-based intrinsic reward) yielded ~2.5× better sample efficiency than conventional planners for discovering causal experiments in CausalWorld",
            "uuids": [
                "e1066.0"
            ]
        },
        {
            "text": "IMRL with novelty-based intrinsic motivation achieved higher skill success ratios (~0.8) than prediction-error-based motivation (~0.7) in 2D Multi-Valley, with prediction-error driving excessive exploration",
            "uuids": [
                "e1013.0"
            ]
        },
        {
            "text": "RL-IMGEP survey emphasizes that high environment complexity (high-dimensional observations, sparse rewards, long horizons) amplifies challenges posed by high variation and requires mechanisms like curriculum learning and goal-sampling adaptation",
            "uuids": [
                "e1051.0"
            ]
        },
        {
            "text": "Self-play (asymmetric intrinsic motivation) enabled learning on MountainCar where target-only policy gradient failed, matching or exceeding exploration methods like VIME",
            "uuids": [
                "e1096.3"
            ]
        },
        {
            "text": "Disentangled modular IMGEP strongly outperformed entangled flat IMGEP in environments with distractors, showing representation quality critically affects intrinsic motivation effectiveness",
            "uuids": [
                "e1034.3"
            ]
        },
        {
            "text": "SGIM-PB combining intrinsic motivation with procedure-based exploration learned hierarchical tasks that intrinsic motivation alone (SAGG-RIAC) could not solve",
            "uuids": [
                "e1027.0"
            ]
        },
        {
            "text": "Random exploration baseline performed much worse than intrinsic motivation methods across multiple domains, confirming intrinsic motivation's value",
            "uuids": [
                "e1070.2",
                "e1034.4"
            ]
        }
    ],
    "theory_statements": [
        "The optimal intrinsic motivation mechanism depends on environment complexity, variation, the learning algorithm's generalization properties, and task structure",
        "Competence-progress-based intrinsic rewards are most effective in very high-dimensional, slow-learning domains where progress is detectable over meaningful timescales",
        "Novelty-based intrinsic rewards are most effective in moderate-dimensional domains with clear state-space structure and benefit from disentangled representations",
        "Prediction-error-based intrinsic rewards can be misled by stochastic, unlearnable, or distractor aspects of the environment, leading to excessive exploration of uninformative regions",
        "For generalizing models (e.g., neural networks, random forests), variance-based and novelty-based intrinsic rewards outperform count-based rewards",
        "For tabular models, count-based intrinsic rewards (e.g., R-MAX) outperform variance-based rewards",
        "Intrinsic motivation is most beneficial when environment variation is high but complexity is manageable, as it helps discover learnable aspects",
        "For hierarchical or compositional tasks, intrinsic motivation alone may be insufficient and benefits from combination with procedural mechanisms or social learning",
        "Disentangled representations significantly improve intrinsic motivation effectiveness in environments with distractors or multiple independent factors",
        "The timescale of learning affects which intrinsic motivation works best: fast learning domains need immediate novelty/variance signals, slow learning domains can use competence-progress signals"
    ],
    "new_predictions_likely": [
        "In stochastic environments with unpredictable elements, novelty-based intrinsic rewards will outperform prediction-error-based rewards by avoiding getting stuck on unlearnable stochasticity",
        "In very high-dimensional continuous domains (e.g., &gt;100D state spaces) with slow learning, competence-progress-based rewards will outperform novelty-based rewards",
        "For agents using neural network function approximation in moderate-dimensional spaces, variance-based intrinsic rewards will outperform count-based rewards",
        "Combining multiple intrinsic motivation mechanisms (e.g., novelty + competence progress) with adaptive weighting will outperform single mechanisms across diverse environments",
        "In environments with visual distractors, intrinsic motivation combined with disentangled representation learning will substantially outperform intrinsic motivation with entangled representations",
        "For hierarchical tasks requiring skill composition, intrinsic motivation combined with procedural/hierarchical mechanisms will outperform flat intrinsic motivation"
    ],
    "new_predictions_unknown": [
        "Whether there exists a universal intrinsic motivation mechanism that automatically adapts to all environment types and learning algorithms without manual tuning",
        "Whether intrinsic motivation can enable learning on tasks that are fundamentally impossible without external rewards, or if there are hard limits",
        "Whether the benefits of intrinsic motivation scale indefinitely with environment complexity or exhibit diminishing returns beyond certain complexity thresholds",
        "Whether intrinsic motivation can be combined with extrinsic rewards without interference, or whether they require careful balancing mechanisms to avoid one dominating the other",
        "Whether meta-learning can discover optimal intrinsic motivation mechanisms for new environments faster than hand-design",
        "Whether intrinsic motivation mechanisms discovered through evolution or meta-learning would converge to the same types (novelty, progress, empowerment) or discover fundamentally new types"
    ],
    "negative_experiments": [
        "Finding environments where intrinsic motivation consistently hurts learning compared to random exploration would challenge the theory's generality",
        "Demonstrating that the optimal intrinsic motivation does not depend on environment properties (complexity, variation, stochasticity) would contradict the matching principle",
        "Showing that prediction-error-based rewards work as well as novelty-based rewards in highly stochastic environments would challenge the stochasticity-sensitivity claim",
        "Finding that count-based rewards consistently outperform variance-based rewards for neural networks would contradict the generalization-type hypothesis",
        "Demonstrating that intrinsic motivation alone can solve hierarchical compositional tasks as well as combined intrinsic+procedural methods would challenge the hierarchical-limitation claim",
        "Finding that entangled representations work as well as disentangled representations for intrinsic motivation in environments with distractors would challenge the representation-quality claim"
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically select or meta-learn the appropriate intrinsic motivation mechanism for a new environment without manual tuning or prior knowledge",
            "uuids": []
        },
        {
            "text": "Whether intrinsic motivation can be learned or meta-learned rather than hand-designed, and if so, what the learned mechanisms would look like",
            "uuids": []
        },
        {
            "text": "How intrinsic motivation interacts with different RL algorithms (on-policy vs off-policy, value-based vs policy-gradient) and network architectures beyond the cases studied",
            "uuids": []
        },
        {
            "text": "The precise timescale thresholds that determine when competence-progress signals become detectable and useful versus when they are too slow",
            "uuids": []
        },
        {
            "text": "How to combine multiple intrinsic motivation mechanisms optimally, including whether to use fixed weights, adaptive weights, or switching strategies",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "SAGG-RIAC (intrinsic motivation via competence progress) performed worse than procedure-based approaches (IM-PB, Random-PB) on hierarchical outcome spaces, suggesting intrinsic motivation alone is insufficient for compositional tasks",
            "uuids": [
                "e1073.2"
            ]
        },
        {
            "text": "Flat IMGEP with entangled VAE representation performed worse than random parameter exploration in some conditions, suggesting that poor representation quality can make intrinsic motivation counterproductive",
            "uuids": [
                "e1034.3"
            ]
        },
        {
            "text": "Some tasks may be learnable without intrinsic motivation if extrinsic rewards are sufficiently dense, as shown by successful learning in some low-complexity environments without intrinsic rewards",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with dense extrinsic rewards may not benefit from intrinsic motivation and may even be harmed by it if intrinsic rewards interfere with extrinsic optimization",
        "Very simple tasks with low-dimensional state spaces may not require intrinsic motivation and can be solved efficiently with random exploration",
        "Tasks with deceptive reward structures (local optima far from global optima) may require specialized intrinsic motivation mechanisms that explicitly avoid local optima",
        "Hierarchical or compositional tasks require intrinsic motivation to be combined with procedural/hierarchical mechanisms or social learning to be effective",
        "Environments with distractors or multiple independent causal factors require disentangled representations for intrinsic motivation to work effectively",
        "Very fast-learning domains (where competence changes rapidly) cannot use competence-progress signals effectively and must rely on immediate novelty/variance signals",
        "Highly stochastic environments require intrinsic motivation mechanisms that are robust to noise (novelty-based) rather than those that try to predict everything (prediction-error-based)",
        "Tabular learning algorithms require count-based intrinsic motivation (e.g., R-MAX) while generalizing algorithms require variance/novelty-based intrinsic motivation"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Oudeyer & Kaplan (2007) What is intrinsic motivation? A typology of computational approaches [Foundational intrinsic motivation taxonomy defining novelty, competence, and knowledge-based intrinsic motivations]",
            "Schmidhuber (1991) A possibility for implementing curiosity and boredom in model-building neural controllers [Early prediction-error-based curiosity]",
            "Pathak et al. (2017) Curiosity-driven exploration by self-supervised prediction [Prediction-error-based curiosity for deep RL, ICM]",
            "Burda et al. (2018) Exploration by random network distillation [Novelty-based exploration using random network distillation]",
            "Baranes & Oudeyer (2013) Active learning of inverse models with intrinsically motivated goal exploration in robots [Competence-progress-based exploration, SAGG-RIAC]",
            "Forestier et al. (2017) Intrinsically motivated goal exploration processes with automatic curriculum learning [IMGEP framework and learning-progress-based goal selection]",
            "Colas et al. (2022) Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: A short survey [Recent survey on intrinsic motivation in goal-conditioned RL]",
            "Haber et al. (2018) Learning to play with intrinsically-motivated, self-aware agents [Empowerment-based intrinsic motivation]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 5,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
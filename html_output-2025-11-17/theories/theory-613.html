<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Faithfulness and Inductive Bias Preservation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-613</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-613</p>
                <p><strong>Name:</strong> Structural Faithfulness and Inductive Bias Preservation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that the ideal graph-to-text representation for LLM training is one that maximally preserves the structural inductive biases of the original graph, such that the representation enables the model to reason about both local and global graph properties, multi-hop relations, and structural constraints (e.g., cycles, motifs, reentrancies). The theory claims that representations which (a) encode explicit connectivity (e.g., adjacency lists, edge lists, Levi graphs), (b) preserve or augment with positional or relative distance information (e.g., Laplacian positional encodings, relative position embeddings), and (c) are robust to linearization order (e.g., via adversarial linearization or SMILES enumeration), will enable LLMs to generalize to unseen graph structures and tasks, outperforming representations that flatten or obscure graph topology. The theory further posits that explicit mechanisms for order-invariance and structure-aware attention are necessary for high-fidelity graph-to-text conversion.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structural Faithfulness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; preserves &#8594; explicit graph connectivity and structural features (e.g., adjacency, motifs, cycles, reentrancies)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves &#8594; higher accuracy and generalization on structure-sensitive tasks (e.g., cycle detection, multi-hop reasoning, AMR-to-text)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GraphToken (learned soft-token encoding) that encodes explicit structure outperforms text-based and soft-prompt baselines by large margins on node/edge/graph tasks. <a href="../results/extraction-result-5237.html#e5237.3" class="evidence-link">[e5237.3]</a> </li>
    <li>Levi graph and extended Levi graph transformations enable uniform encoding of node and edge information, improving AMR-to-text and KG-to-text generation. <a href="../results/extraction-result-5390.html#e5390.1" class="evidence-link">[e5390.1]</a> <a href="../results/extraction-result-5239.html#e5239.1" class="evidence-link">[e5239.1]</a> </li>
    <li>Motif encodings that preserve local structure outperform global motif counts or plain text on hard graph problems. <a href="../results/extraction-result-5357.html#e5357.1" class="evidence-link">[e5357.1]</a> </li>
    <li>Adjacency list and edge list linearizations that preserve explicit connectivity outperform plain text or attribute-only encodings on structure-sensitive tasks. <a href="../results/extraction-result-5365.html#e5365.1" class="evidence-link">[e5365.1]</a> <a href="../results/extraction-result-5361.html#e5361.1" class="evidence-link">[e5361.1]</a> <a href="../results/extraction-result-5242.html#e5242.0" class="evidence-link">[e5242.0]</a> </li>
    <li>Structure-aware self-attention (r_ij) and relative position encodings improve AMR-to-text generation and multi-hop reasoning. <a href="../results/extraction-result-5354.html#e5354.3" class="evidence-link">[e5354.3]</a> <a href="../results/extraction-result-5368.html#e5368.5" class="evidence-link">[e5368.5]</a> </li>
    <li>Graph-to-sequence models with explicit graph encoders outperform linearization baselines on structural fidelity and multi-hop reasoning. <a href="../results/extraction-result-5254.html#e5254.6" class="evidence-link">[e5254.6]</a> <a href="../results/extraction-result-5355.html#e5355.2" class="evidence-link">[e5355.2]</a> <a href="../results/extraction-result-5374.html#e5374.4" class="evidence-link">[e5374.4]</a> </li>
    <li>Reification and Levi graph conversion for RDF/AMR graphs allow scalable and faithful modeling of relations and improve downstream generation. <a href="../results/extraction-result-5360.html#e5360.2" class="evidence-link">[e5360.2]</a> <a href="../results/extraction-result-5375.html#e5375.2" class="evidence-link">[e5375.2]</a> </li>
    <li>GraphML and GML serializations, which preserve explicit node/edge structure, yield higher attribute retrieval and structure understanding accuracy than plain text or attribute-only encodings. <a href="../results/extraction-result-5389.html#e5389.4" class="evidence-link">[e5389.4]</a> <a href="../results/extraction-result-5361.html#e5361.3" class="evidence-link">[e5361.3]</a> </li>
    <li>Graph Conv (GCN) and GAT-based encoders that preserve local structure outperform entity-list or attribute-only encodings on KG-to-text and node classification. <a href="../results/extraction-result-5371.html#e5371.1" class="evidence-link">[e5371.1]</a> <a href="../results/extraction-result-5360.html#e5360.3" class="evidence-link">[e5360.3]</a> <a href="../results/extraction-result-5387.html#e5387.7" class="evidence-link">[e5387.7]</a> </li>
    <li>Graph-to-text templates and triple-to-sentence encodings that preserve relation semantics outperform plain attribute-based or entity-list encodings. <a href="../results/extraction-result-5255.html#e5255.0" class="evidence-link">[e5255.0]</a> <a href="../results/extraction-result-5366.html#e5366.3" class="evidence-link">[e5366.3]</a> <a href="../results/extraction-result-5381.html#e5381.2" class="evidence-link">[e5381.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While structural faithfulness is known in GNNs, its centrality for LLM-based graph-to-text conversion is a novel, empirically supported claim.</p>            <p><strong>What Already Exists:</strong> Structural faithfulness is a recognized goal in graph neural networks and some graph-to-sequence models.</p>            <p><strong>What is Novel:</strong> This law extends the principle to graph-to-text representations for LLMs, asserting that explicit preservation of structure is necessary for generalization and high-fidelity reasoning.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2024) Let Your Graph Do the Talking: Encoding Structured Data for LLMs [GraphToken, structure preservation]</li>
    <li>Guo et al. (2019) Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning [Levi graph]</li>
    <li>Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [motif structure]</li>
    <li>Marcheggiani & Titov (2017) Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling [GNN structure preservation]</li>
    <li>Beck et al. (2018) Graph-to-Sequence Learning using Gated Graph Neural Networks [graph-to-sequence]</li>
</ul>
            <h3>Statement 1: Inductive Bias Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; representation &#8594; includes &#8594; positional or relative distance encodings (e.g., Laplacian PE, relative position embeddings, motif attachment)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; retains &#8594; graph inductive biases and generalizes to unseen or out-of-distribution graph structures</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GraphToken variants with Laplacian positional encodings or learned identity features generalize better to OOD graphs and counting tasks. <a href="../results/extraction-result-5237.html#e5237.3" class="evidence-link">[e5237.3]</a> </li>
    <li>Relative position encoding (Shaw/Raffel forms) and structure-aware self-attention (r_ij) improve AMR-to-text generation and multi-hop reasoning. <a href="../results/extraction-result-5368.html#e5368.5" class="evidence-link">[e5368.5]</a> <a href="../results/extraction-result-5354.html#e5354.3" class="evidence-link">[e5354.3]</a> </li>
    <li>Motif attachment encodings (which encode which nodes participate in motifs) improve accuracy on hard tasks compared to counts-only. <a href="../results/extraction-result-5357.html#e5357.1" class="evidence-link">[e5357.1]</a> </li>
    <li>Hop-Field Overview Template (LLaGA) uses hop-based embeddings to capture neighborhood structure and improves generalization in node classification. <a href="../results/extraction-result-5244.html#e5244.2" class="evidence-link">[e5244.2]</a> </li>
    <li>Path-label encodings (feature/avg/sum/SA/CNN) that encode edge-label sequences along shortest paths improve AMR-to-text generation and multi-hop reasoning. <a href="../results/extraction-result-5354.html#e5354.4" class="evidence-link">[e5354.4]</a> </li>
    <li>Combined global and local encoders (CGE/PGE) that capture both long-range and local context outperform single-strategy encoders. <a href="../results/extraction-result-5250.html#e5250.2" class="evidence-link">[e5250.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While positional encodings are known, their explicit role in preserving inductive bias for LLM-based graph-to-text conversion is a novel, general principle.</p>            <p><strong>What Already Exists:</strong> Inductive bias preservation is a principle in GNNs and some Transformer extensions.</p>            <p><strong>What is Novel:</strong> This law formalizes the necessity of explicit positional/structural encodings in graph-to-text representations for LLMs, not just for GNNs.</p>
            <p><strong>References:</strong> <ul>
    <li>Shaw et al. (2018) Self-attention with relative position representations [relative position]</li>
    <li>Wang et al. (2024) Let Your Graph Do the Talking: Encoding Structured Data for LLMs [Laplacian PE]</li>
    <li>Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [motif attachment]</li>
    <li>Guo et al. (2019) Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning [Levi graph, position]</li>
    <li>Guo et al. (2021) Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs [relative position, graph bias]</li>
</ul>
            <h3>Statement 2: Order-Invariance and Robustness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; representation &#8594; is &#8594; robust to linearization order (e.g., via adversarial linearization, SMILES enumeration, or randomization)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; achieves &#8594; higher robustness and less overfitting to spurious orderings</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Adversarial linearization training (exposing models to multiple orderings) improves robustness and reduces overfitting to canonical order. <a href="../results/extraction-result-5246.html#e5246.4" class="evidence-link">[e5246.4]</a> </li>
    <li>SMILES enumeration (randomized SMILES) increases invariance and improves molecular property prediction. <a href="../results/extraction-result-5369.html#e5369.1" class="evidence-link">[e5369.1]</a> </li>
    <li>Motif and adjacency encodings that are less order-dependent yield more stable performance across sampling strategies. <a href="../results/extraction-result-5357.html#e5357.1" class="evidence-link">[e5357.1]</a> <a href="../results/extraction-result-5365.html#e5365.1" class="evidence-link">[e5365.1]</a> </li>
    <li>Triplet linearization with random order (T5 set) reduces ordering artifacts and improves generalization compared to fixed order (T5 list). <a href="../results/extraction-result-5239.html#e5239.2" class="evidence-link">[e5239.2]</a> </li>
    <li>BFS and DFS traversal orderings affect local context preservation; BFS order improves few-shot KG-to-text generation by preserving neighborhood locality. <a href="../results/extraction-result-5251.html#e5251.5" class="evidence-link">[e5251.5]</a> </li>
    <li>DOT-format serialization with fixed edge ordering (DFS/BFS/topological) shows that ordering choice does not strongly affect results, supporting order-invariance. <a href="../results/extraction-result-5373.html#e5373.1" class="evidence-link">[e5373.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Order-invariance is established in GNNs, but its necessity for LLM-based graph-to-text representations is a novel, empirically grounded claim.</p>            <p><strong>What Already Exists:</strong> Order-invariance is a known desideratum in graph learning.</p>            <p><strong>What is Novel:</strong> This law asserts that explicit order-invariance mechanisms are necessary for robust graph-to-text conversion in LLMs, not just for GNNs.</p>
            <p><strong>References:</strong> <ul>
    <li>Ribeiro et al. (2020) Promoting Graph Awareness in Linearized Graph-to-Text Generation [adversarial linearization]</li>
    <li>Bjerrum (2017) SMILES enumeration as data augmentation for neural network modeling of molecules [SMILES enumeration]</li>
    <li>Guo et al. (2021) Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs [order-invariance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Graph-to-text representations that use explicit structure-aware attention (e.g., r_ij, relative position) will outperform plain linearization on AMR-to-text and KG-to-text tasks, especially for graphs with many reentrancies or long-range dependencies.</li>
                <li>Order-invariant training (e.g., adversarial linearization or SMILES enumeration) will reduce overfitting and improve generalization to unseen graph orderings in both molecular and semantic graph domains.</li>
                <li>Adding explicit motif attachment or positional encodings to adjacency or edge-list representations will improve LLM performance on multi-hop and motif-sensitive tasks.</li>
                <li>Levi graph or reification-based encodings will yield higher fidelity in relation modeling and improve downstream text generation compared to edge-label-only or entity-list encodings.</li>
                <li>Combined global and local encoders (e.g., CGE/PGE) will outperform single-strategy encoders on large-diameter or multi-hop graph tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>A fully order-invariant, structure-aware graph-to-text representation will enable LLMs to generalize to entirely new graph topologies (e.g., unseen motif types or synthetic graphs) without additional fine-tuning.</li>
                <li>Combining structure-aware attention with multimodal (e.g., image) encodings will yield synergistic gains in graph reasoning tasks, but the extent of this improvement is unknown.</li>
                <li>Explicitly encoding higher-order structures (e.g., hyperedges, subgraph isomorphisms) in the representation will allow LLMs to perform complex graph mining tasks currently out of reach.</li>
                <li>If structure-aware representations are combined with retrieval-augmented or memory-augmented LLMs, the resulting models may achieve human-level performance on open-domain graph QA.</li>
                <li>Order-invariant, structure-aware representations may enable LLMs to perform robustly on adversarially perturbed or noisy graphs, but the limits of this robustness are unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a model trained on a single linearization order generalizes as well as one trained with adversarial linearization, the order-invariance law would be challenged.</li>
                <li>If removing positional or motif encodings from a structure-aware representation does not degrade performance on multi-hop or motif-sensitive tasks, the inductive bias preservation law would be weakened.</li>
                <li>If a representation that flattens or obscures graph structure (e.g., plain text or attribute-only) outperforms structure-preserving encodings on structure-sensitive tasks, the structural faithfulness law would be called into question.</li>
                <li>If structure-aware attention (e.g., r_ij) does not improve over vanilla attention in AMR-to-text or KG-to-text tasks, the theory's claim about the necessity of structure-aware mechanisms would be weakened.</li>
                <li>If motif or positional encodings introduce instability or higher error rates across datasets, the universality of the inductive bias law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks (e.g., attribute-rich node classification) are dominated by textual or attribute information, and structure-preserving encodings may not provide additional benefit. <a href="../results/extraction-result-5253.html#e5253.3" class="evidence-link">[e5253.3]</a> <a href="../results/extraction-result-5238.html#e5238.7" class="evidence-link">[e5238.7]</a> <a href="../results/extraction-result-5379.html#e5379.3" class="evidence-link">[e5379.3]</a> </li>
    <li>Image-based encodings (GraphTMI) sometimes outperform structure-aware text encodings, especially for large graphs, which is not fully explained by the theory. <a href="../results/extraction-result-5252.html#e5252.8" class="evidence-link">[e5252.8]</a> <a href="../results/extraction-result-5357.html#e5357.1" class="evidence-link">[e5357.1]</a> </li>
    <li>For extremely large graphs, subgraph sampling (e.g., k-hop) is necessary, and the theory does not specify how much structure must be preserved for optimal performance. <a href="../results/extraction-result-5243.html#e5243.5" class="evidence-link">[e5243.5]</a> <a href="../results/extraction-result-5367.html#e5367.2" class="evidence-link">[e5367.2]</a> </li>
    <li>In some cases, simple linearized triple or sequence encodings (with large-scale pretraining) can match or exceed structure-aware encodings, especially with massive data. <a href="../results/extraction-result-5390.html#e5390.2" class="evidence-link">[e5390.2]</a> <a href="../results/extraction-result-5380.html#e5380.4" class="evidence-link">[e5380.4]</a> <a href="../results/extraction-result-5381.html#e5381.2" class="evidence-link">[e5381.2]</a> <a href="../results/extraction-result-5359.html#e5359.4" class="evidence-link">[e5359.4]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory generalizes and formalizes the role of structural faithfulness and inductive bias preservation for LLM-based graph-to-text representations, synthesizing recent empirical findings.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2024) Let Your Graph Do the Talking: Encoding Structured Data for LLMs [GraphToken, structure preservation]</li>
    <li>Guo et al. (2019) Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning [Levi graph]</li>
    <li>Ribeiro et al. (2020) Promoting Graph Awareness in Linearized Graph-to-Text Generation [adversarial linearization]</li>
    <li>Bjerrum (2017) SMILES enumeration as data augmentation for neural network modeling of molecules [SMILES enumeration]</li>
    <li>Shaw et al. (2018) Self-attention with relative position representations [relative position]</li>
    <li>Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [motif structure, image modality]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "theory_description": "This theory asserts that the ideal graph-to-text representation for LLM training is one that maximally preserves the structural inductive biases of the original graph, such that the representation enables the model to reason about both local and global graph properties, multi-hop relations, and structural constraints (e.g., cycles, motifs, reentrancies). The theory claims that representations which (a) encode explicit connectivity (e.g., adjacency lists, edge lists, Levi graphs), (b) preserve or augment with positional or relative distance information (e.g., Laplacian positional encodings, relative position embeddings), and (c) are robust to linearization order (e.g., via adversarial linearization or SMILES enumeration), will enable LLMs to generalize to unseen graph structures and tasks, outperforming representations that flatten or obscure graph topology. The theory further posits that explicit mechanisms for order-invariance and structure-aware attention are necessary for high-fidelity graph-to-text conversion.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structural Faithfulness Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "preserves",
                        "object": "explicit graph connectivity and structural features (e.g., adjacency, motifs, cycles, reentrancies)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves",
                        "object": "higher accuracy and generalization on structure-sensitive tasks (e.g., cycle detection, multi-hop reasoning, AMR-to-text)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GraphToken (learned soft-token encoding) that encodes explicit structure outperforms text-based and soft-prompt baselines by large margins on node/edge/graph tasks.",
                        "uuids": [
                            "e5237.3"
                        ]
                    },
                    {
                        "text": "Levi graph and extended Levi graph transformations enable uniform encoding of node and edge information, improving AMR-to-text and KG-to-text generation.",
                        "uuids": [
                            "e5390.1",
                            "e5239.1"
                        ]
                    },
                    {
                        "text": "Motif encodings that preserve local structure outperform global motif counts or plain text on hard graph problems.",
                        "uuids": [
                            "e5357.1"
                        ]
                    },
                    {
                        "text": "Adjacency list and edge list linearizations that preserve explicit connectivity outperform plain text or attribute-only encodings on structure-sensitive tasks.",
                        "uuids": [
                            "e5365.1",
                            "e5361.1",
                            "e5242.0"
                        ]
                    },
                    {
                        "text": "Structure-aware self-attention (r_ij) and relative position encodings improve AMR-to-text generation and multi-hop reasoning.",
                        "uuids": [
                            "e5354.3",
                            "e5368.5"
                        ]
                    },
                    {
                        "text": "Graph-to-sequence models with explicit graph encoders outperform linearization baselines on structural fidelity and multi-hop reasoning.",
                        "uuids": [
                            "e5254.6",
                            "e5355.2",
                            "e5374.4"
                        ]
                    },
                    {
                        "text": "Reification and Levi graph conversion for RDF/AMR graphs allow scalable and faithful modeling of relations and improve downstream generation.",
                        "uuids": [
                            "e5360.2",
                            "e5375.2"
                        ]
                    },
                    {
                        "text": "GraphML and GML serializations, which preserve explicit node/edge structure, yield higher attribute retrieval and structure understanding accuracy than plain text or attribute-only encodings.",
                        "uuids": [
                            "e5389.4",
                            "e5361.3"
                        ]
                    },
                    {
                        "text": "Graph Conv (GCN) and GAT-based encoders that preserve local structure outperform entity-list or attribute-only encodings on KG-to-text and node classification.",
                        "uuids": [
                            "e5371.1",
                            "e5360.3",
                            "e5387.7"
                        ]
                    },
                    {
                        "text": "Graph-to-text templates and triple-to-sentence encodings that preserve relation semantics outperform plain attribute-based or entity-list encodings.",
                        "uuids": [
                            "e5255.0",
                            "e5366.3",
                            "e5381.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Structural faithfulness is a recognized goal in graph neural networks and some graph-to-sequence models.",
                    "what_is_novel": "This law extends the principle to graph-to-text representations for LLMs, asserting that explicit preservation of structure is necessary for generalization and high-fidelity reasoning.",
                    "classification_explanation": "While structural faithfulness is known in GNNs, its centrality for LLM-based graph-to-text conversion is a novel, empirically supported claim.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wang et al. (2024) Let Your Graph Do the Talking: Encoding Structured Data for LLMs [GraphToken, structure preservation]",
                        "Guo et al. (2019) Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning [Levi graph]",
                        "Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [motif structure]",
                        "Marcheggiani & Titov (2017) Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling [GNN structure preservation]",
                        "Beck et al. (2018) Graph-to-Sequence Learning using Gated Graph Neural Networks [graph-to-sequence]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Inductive Bias Preservation Law",
                "if": [
                    {
                        "subject": "representation",
                        "relation": "includes",
                        "object": "positional or relative distance encodings (e.g., Laplacian PE, relative position embeddings, motif attachment)"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "retains",
                        "object": "graph inductive biases and generalizes to unseen or out-of-distribution graph structures"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GraphToken variants with Laplacian positional encodings or learned identity features generalize better to OOD graphs and counting tasks.",
                        "uuids": [
                            "e5237.3"
                        ]
                    },
                    {
                        "text": "Relative position encoding (Shaw/Raffel forms) and structure-aware self-attention (r_ij) improve AMR-to-text generation and multi-hop reasoning.",
                        "uuids": [
                            "e5368.5",
                            "e5354.3"
                        ]
                    },
                    {
                        "text": "Motif attachment encodings (which encode which nodes participate in motifs) improve accuracy on hard tasks compared to counts-only.",
                        "uuids": [
                            "e5357.1"
                        ]
                    },
                    {
                        "text": "Hop-Field Overview Template (LLaGA) uses hop-based embeddings to capture neighborhood structure and improves generalization in node classification.",
                        "uuids": [
                            "e5244.2"
                        ]
                    },
                    {
                        "text": "Path-label encodings (feature/avg/sum/SA/CNN) that encode edge-label sequences along shortest paths improve AMR-to-text generation and multi-hop reasoning.",
                        "uuids": [
                            "e5354.4"
                        ]
                    },
                    {
                        "text": "Combined global and local encoders (CGE/PGE) that capture both long-range and local context outperform single-strategy encoders.",
                        "uuids": [
                            "e5250.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Inductive bias preservation is a principle in GNNs and some Transformer extensions.",
                    "what_is_novel": "This law formalizes the necessity of explicit positional/structural encodings in graph-to-text representations for LLMs, not just for GNNs.",
                    "classification_explanation": "While positional encodings are known, their explicit role in preserving inductive bias for LLM-based graph-to-text conversion is a novel, general principle.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Shaw et al. (2018) Self-attention with relative position representations [relative position]",
                        "Wang et al. (2024) Let Your Graph Do the Talking: Encoding Structured Data for LLMs [Laplacian PE]",
                        "Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [motif attachment]",
                        "Guo et al. (2019) Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning [Levi graph, position]",
                        "Guo et al. (2021) Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs [relative position, graph bias]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Order-Invariance and Robustness Law",
                "if": [
                    {
                        "subject": "representation",
                        "relation": "is",
                        "object": "robust to linearization order (e.g., via adversarial linearization, SMILES enumeration, or randomization)"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "achieves",
                        "object": "higher robustness and less overfitting to spurious orderings"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Adversarial linearization training (exposing models to multiple orderings) improves robustness and reduces overfitting to canonical order.",
                        "uuids": [
                            "e5246.4"
                        ]
                    },
                    {
                        "text": "SMILES enumeration (randomized SMILES) increases invariance and improves molecular property prediction.",
                        "uuids": [
                            "e5369.1"
                        ]
                    },
                    {
                        "text": "Motif and adjacency encodings that are less order-dependent yield more stable performance across sampling strategies.",
                        "uuids": [
                            "e5357.1",
                            "e5365.1"
                        ]
                    },
                    {
                        "text": "Triplet linearization with random order (T5 set) reduces ordering artifacts and improves generalization compared to fixed order (T5 list).",
                        "uuids": [
                            "e5239.2"
                        ]
                    },
                    {
                        "text": "BFS and DFS traversal orderings affect local context preservation; BFS order improves few-shot KG-to-text generation by preserving neighborhood locality.",
                        "uuids": [
                            "e5251.5"
                        ]
                    },
                    {
                        "text": "DOT-format serialization with fixed edge ordering (DFS/BFS/topological) shows that ordering choice does not strongly affect results, supporting order-invariance.",
                        "uuids": [
                            "e5373.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Order-invariance is a known desideratum in graph learning.",
                    "what_is_novel": "This law asserts that explicit order-invariance mechanisms are necessary for robust graph-to-text conversion in LLMs, not just for GNNs.",
                    "classification_explanation": "Order-invariance is established in GNNs, but its necessity for LLM-based graph-to-text representations is a novel, empirically grounded claim.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Ribeiro et al. (2020) Promoting Graph Awareness in Linearized Graph-to-Text Generation [adversarial linearization]",
                        "Bjerrum (2017) SMILES enumeration as data augmentation for neural network modeling of molecules [SMILES enumeration]",
                        "Guo et al. (2021) Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs [order-invariance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Graph-to-text representations that use explicit structure-aware attention (e.g., r_ij, relative position) will outperform plain linearization on AMR-to-text and KG-to-text tasks, especially for graphs with many reentrancies or long-range dependencies.",
        "Order-invariant training (e.g., adversarial linearization or SMILES enumeration) will reduce overfitting and improve generalization to unseen graph orderings in both molecular and semantic graph domains.",
        "Adding explicit motif attachment or positional encodings to adjacency or edge-list representations will improve LLM performance on multi-hop and motif-sensitive tasks.",
        "Levi graph or reification-based encodings will yield higher fidelity in relation modeling and improve downstream text generation compared to edge-label-only or entity-list encodings.",
        "Combined global and local encoders (e.g., CGE/PGE) will outperform single-strategy encoders on large-diameter or multi-hop graph tasks."
    ],
    "new_predictions_unknown": [
        "A fully order-invariant, structure-aware graph-to-text representation will enable LLMs to generalize to entirely new graph topologies (e.g., unseen motif types or synthetic graphs) without additional fine-tuning.",
        "Combining structure-aware attention with multimodal (e.g., image) encodings will yield synergistic gains in graph reasoning tasks, but the extent of this improvement is unknown.",
        "Explicitly encoding higher-order structures (e.g., hyperedges, subgraph isomorphisms) in the representation will allow LLMs to perform complex graph mining tasks currently out of reach.",
        "If structure-aware representations are combined with retrieval-augmented or memory-augmented LLMs, the resulting models may achieve human-level performance on open-domain graph QA.",
        "Order-invariant, structure-aware representations may enable LLMs to perform robustly on adversarially perturbed or noisy graphs, but the limits of this robustness are unknown."
    ],
    "negative_experiments": [
        "If a model trained on a single linearization order generalizes as well as one trained with adversarial linearization, the order-invariance law would be challenged.",
        "If removing positional or motif encodings from a structure-aware representation does not degrade performance on multi-hop or motif-sensitive tasks, the inductive bias preservation law would be weakened.",
        "If a representation that flattens or obscures graph structure (e.g., plain text or attribute-only) outperforms structure-preserving encodings on structure-sensitive tasks, the structural faithfulness law would be called into question.",
        "If structure-aware attention (e.g., r_ij) does not improve over vanilla attention in AMR-to-text or KG-to-text tasks, the theory's claim about the necessity of structure-aware mechanisms would be weakened.",
        "If motif or positional encodings introduce instability or higher error rates across datasets, the universality of the inductive bias law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks (e.g., attribute-rich node classification) are dominated by textual or attribute information, and structure-preserving encodings may not provide additional benefit.",
            "uuids": [
                "e5253.3",
                "e5238.7",
                "e5379.3"
            ]
        },
        {
            "text": "Image-based encodings (GraphTMI) sometimes outperform structure-aware text encodings, especially for large graphs, which is not fully explained by the theory.",
            "uuids": [
                "e5252.8",
                "e5357.1"
            ]
        },
        {
            "text": "For extremely large graphs, subgraph sampling (e.g., k-hop) is necessary, and the theory does not specify how much structure must be preserved for optimal performance.",
            "uuids": [
                "e5243.5",
                "e5367.2"
            ]
        },
        {
            "text": "In some cases, simple linearized triple or sequence encodings (with large-scale pretraining) can match or exceed structure-aware encodings, especially with massive data.",
            "uuids": [
                "e5390.2",
                "e5380.4",
                "e5381.2",
                "e5359.4"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Motif encodings have higher mismatch rates in some datasets, suggesting that structure preservation alone does not guarantee optimal performance.",
            "uuids": [
                "e5357.1"
            ]
        },
        {
            "text": "In some cases, linearized triple or sequence encodings (with large-scale pretraining) can match or exceed structure-aware encodings, especially with massive data.",
            "uuids": [
                "e5390.2",
                "e5380.4",
                "e5381.2",
                "e5359.4"
            ]
        },
        {
            "text": "Image-based encodings can outperform text-based structure-aware encodings on certain graph understanding tasks.",
            "uuids": [
                "e5252.8",
                "e5357.1"
            ]
        },
        {
            "text": "For graphs with trivial or highly regular structure, simpler encodings may suffice and structure preservation may not yield additional gains.",
            "uuids": [
                "e5365.1",
                "e5244.2"
            ]
        }
    ],
    "special_cases": [
        "For graphs with little or no structure (e.g., star graphs, attribute-only graphs), structure-preserving encodings may be unnecessary.",
        "In domains where graph structure is highly regular or trivial (e.g., chains, trees), simpler encodings may suffice.",
        "For extremely large graphs, explicit structure preservation may be impractical without subgraph sampling or summarization.",
        "In attribute-dominated tasks, textual or attribute-only encodings may be optimal.",
        "For multimodal tasks (e.g., image+text), structure-aware text encodings may need to be combined with other modalities for best results."
    ],
    "existing_theory": {
        "what_already_exists": "Structural faithfulness and inductive bias preservation are established in GNNs and some graph-to-sequence models.",
        "what_is_novel": "The extension of these principles as necessary conditions for LLM-based graph-to-text conversion, and the explicit inclusion of order-invariance and structure-aware attention, is novel.",
        "classification_explanation": "This theory generalizes and formalizes the role of structural faithfulness and inductive bias preservation for LLM-based graph-to-text representations, synthesizing recent empirical findings.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wang et al. (2024) Let Your Graph Do the Talking: Encoding Structured Data for LLMs [GraphToken, structure preservation]",
            "Guo et al. (2019) Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning [Levi graph]",
            "Ribeiro et al. (2020) Promoting Graph Awareness in Linearized Graph-to-Text Generation [adversarial linearization]",
            "Bjerrum (2017) SMILES enumeration as data augmentation for neural network modeling of molecules [SMILES enumeration]",
            "Shaw et al. (2018) Self-attention with relative position representations [relative position]",
            "Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [motif structure, image modality]"
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
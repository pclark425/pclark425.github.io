<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1917</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1917</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that LLM performance is maximized when the format of problem presentation aligns with the model's pretraining distribution and cognitive processing patterns. Specifically, LLMs perform best when prompts are structured in ways that mirror the natural language and task formats most prevalent in their training data, and when the logical flow of information matches the model's learned attention and reasoning strategies.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Pretraining Format Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; matches &#8594; pretraining_distribution_format</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; maximal_performance</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on tasks and prompt formats that closely resemble their pretraining data (e.g., question-answer pairs, Wikipedia-style summaries). </li>
    <li>Empirical studies show that prompt engineering to mimic pretraining data improves LLM accuracy and reliability. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> While the effect is observed, its explicit formalization as a law governing LLM performance is novel.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs are sensitive to prompt format and that pretraining data influences downstream performance.</p>            <p><strong>What is Novel:</strong> This law formalizes the alignment principle as a predictive law for problem presentation, not just a descriptive observation.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format influences reasoning]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]</li>
</ul>
            <h3>Statement 1: Logical Flow Consistency Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; follows &#8594; logical_progression_similar_to_training</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; improved_reasoning_and_accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are more accurate when prompts follow a logical, stepwise progression, as in chain-of-thought prompting. </li>
    <li>Disorganized or illogical prompt structures reduce LLM performance, even with the same information content. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The effect is known in specific prompting strategies, but its generalization as a law is novel.</p>            <p><strong>What Already Exists:</strong> Chain-of-thought and stepwise prompting are known to improve LLM reasoning.</p>            <p><strong>What is Novel:</strong> The law generalizes this to any logical flow consistent with training, not just explicit chain-of-thought.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Logical progression in prompts]</li>
    <li>Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [Prompt structure and reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a novel task is presented in a format closely matching the LLM's pretraining data, performance will be higher than in a mismatched format.</li>
                <li>If the logical flow of a prompt is disrupted (e.g., by shuffling steps), LLM accuracy will decrease even if all information is present.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are pretrained on highly diverse or contradictory formats, their sensitivity to presentation format may decrease or become unpredictable.</li>
                <li>If LLMs are exposed to adversarially designed formats that superficially match pretraining but are logically inconsistent, performance may degrade in novel ways.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well regardless of prompt format or logical flow, the theory would be falsified.</li>
                <li>If LLMs perform better on formats not present in pretraining, the alignment law would be invalid.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs with advanced adaptation mechanisms may generalize well to unseen formats, contrary to the theory's predictions. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known effects into a general predictive framework, which is a novel contribution.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that LLM performance is maximized when the format of problem presentation aligns with the model's pretraining distribution and cognitive processing patterns. Specifically, LLMs perform best when prompts are structured in ways that mirror the natural language and task formats most prevalent in their training data, and when the logical flow of information matches the model's learned attention and reasoning strategies.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Pretraining Format Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "matches",
                        "object": "pretraining_distribution_format"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "maximal_performance"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on tasks and prompt formats that closely resemble their pretraining data (e.g., question-answer pairs, Wikipedia-style summaries).",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that prompt engineering to mimic pretraining data improves LLM accuracy and reliability.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs are sensitive to prompt format and that pretraining data influences downstream performance.",
                    "what_is_novel": "This law formalizes the alignment principle as a predictive law for problem presentation, not just a descriptive observation.",
                    "classification_explanation": "While the effect is observed, its explicit formalization as a law governing LLM performance is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format influences reasoning]",
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Logical Flow Consistency Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "follows",
                        "object": "logical_progression_similar_to_training"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "improved_reasoning_and_accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are more accurate when prompts follow a logical, stepwise progression, as in chain-of-thought prompting.",
                        "uuids": []
                    },
                    {
                        "text": "Disorganized or illogical prompt structures reduce LLM performance, even with the same information content.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Chain-of-thought and stepwise prompting are known to improve LLM reasoning.",
                    "what_is_novel": "The law generalizes this to any logical flow consistent with training, not just explicit chain-of-thought.",
                    "classification_explanation": "The effect is known in specific prompting strategies, but its generalization as a law is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Logical progression in prompts]",
                        "Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [Prompt structure and reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a novel task is presented in a format closely matching the LLM's pretraining data, performance will be higher than in a mismatched format.",
        "If the logical flow of a prompt is disrupted (e.g., by shuffling steps), LLM accuracy will decrease even if all information is present."
    ],
    "new_predictions_unknown": [
        "If LLMs are pretrained on highly diverse or contradictory formats, their sensitivity to presentation format may decrease or become unpredictable.",
        "If LLMs are exposed to adversarially designed formats that superficially match pretraining but are logically inconsistent, performance may degrade in novel ways."
    ],
    "negative_experiments": [
        "If LLMs perform equally well regardless of prompt format or logical flow, the theory would be falsified.",
        "If LLMs perform better on formats not present in pretraining, the alignment law would be invalid."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs with advanced adaptation mechanisms may generalize well to unseen formats, contrary to the theory's predictions.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Certain LLMs fine-tuned for robustness may show less sensitivity to prompt format than predicted.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very short prompts or tasks with explicit answer markers may override format effects.",
        "Tasks with highly structured input (e.g., code) may not follow natural language pretraining patterns."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt format and pretraining data influence LLM performance, and chain-of-thought/logical progression is known to help.",
        "what_is_novel": "The explicit formalization of alignment and logical flow as general laws governing LLM performance is new.",
        "classification_explanation": "The theory synthesizes known effects into a general predictive framework, which is a novel contribution.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
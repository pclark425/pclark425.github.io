<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1929</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1929</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format of problem presentation acts as an information bottleneck, filtering which aspects of the input are salient to the LLM. Formats that efficiently encode task-relevant information and minimize extraneous or distracting content enable the LLM to focus its attention and resources on the core problem, thereby improving performance. Conversely, formats that introduce irrelevant details, excessive verbosity, or poorly structured information create bottlenecks that degrade performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Salience Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; task_relevant_salience</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_optimized &#8594; on_given_task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Concise, well-structured prompts that highlight key information improve LLM accuracy. </li>
    <li>Irrelevant or distracting information in prompts reduces LLM performance. </li>
    <li>Prompt compression and summarization studies show that removing extraneous details can improve or maintain performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt brevity is known, the explicit information bottleneck framing and its predictive implications are novel.</p>            <p><strong>What Already Exists:</strong> Prompt brevity and focus are known to improve LLM outputs.</p>            <p><strong>What is Novel:</strong> This law frames the effect as an information bottleneck, emphasizing the role of salience encoding in the input format.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and salience]</li>
    <li>Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt focus and performance]</li>
</ul>
            <h3>Statement 1: Extraneous Information Bottleneck Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; irrelevant_or_extraneous_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_decreased &#8594; on_given_task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Adding irrelevant sentences or distractors to prompts reduces LLM accuracy. </li>
    <li>Studies show that LLMs can be misled by adversarial or off-topic content in the prompt. </li>
    <li>Prompt length increases with irrelevant content, which can dilute attention and context window usage. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends prompt engineering findings into a formal information-theoretic framework.</p>            <p><strong>What Already Exists:</strong> It is known that irrelevant information can distract LLMs and reduce accuracy.</p>            <p><strong>What is Novel:</strong> This law formalizes the effect as an information bottleneck and predicts a monotonic decrease in performance with increasing extraneous content.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2020) How Can We Know What Language Models Know? [Prompt distractors and performance]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and distractors]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Removing irrelevant sentences from a prompt will increase LLM accuracy on the core task.</li>
                <li>Rewriting verbose prompts to be concise and focused will improve performance.</li>
                <li>Adding distractor information to a prompt will decrease LLM accuracy, even if the core task is unchanged.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a prompt is compressed to the minimal information necessary, does performance plateau or can it be further improved by re-encoding the information?</li>
                <li>For LLMs with very large context windows, does the negative effect of extraneous information persist or diminish?</li>
                <li>Can LLMs be trained to ignore extraneous information entirely, or is there always some performance penalty?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If adding irrelevant information to a prompt does not decrease performance, the theory is challenged.</li>
                <li>If LLMs perform better on verbose, distractor-laden prompts than on concise, focused ones, the theory is falsified.</li>
                <li>If LLMs can always extract the correct answer regardless of prompt structure or salience, the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs ignore distractors and maintain high performance, possibly due to scale or advanced attention mechanisms. </li>
    <li>Instances where concise prompts omit necessary context, leading to lower performance despite high salience. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes prompt engineering and information theory, providing a new lens for understanding format effects.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and salience]</li>
    <li>Jiang et al. (2020) How Can We Know What Language Models Know? [Prompt distractors and performance]</li>
    <li>Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt focus and performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of LLM Problem Presentation",
    "theory_description": "This theory proposes that the format of problem presentation acts as an information bottleneck, filtering which aspects of the input are salient to the LLM. Formats that efficiently encode task-relevant information and minimize extraneous or distracting content enable the LLM to focus its attention and resources on the core problem, thereby improving performance. Conversely, formats that introduce irrelevant details, excessive verbosity, or poorly structured information create bottlenecks that degrade performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Salience Encoding Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "task_relevant_salience"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_optimized",
                        "object": "on_given_task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Concise, well-structured prompts that highlight key information improve LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Irrelevant or distracting information in prompts reduces LLM performance.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt compression and summarization studies show that removing extraneous details can improve or maintain performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt brevity and focus are known to improve LLM outputs.",
                    "what_is_novel": "This law frames the effect as an information bottleneck, emphasizing the role of salience encoding in the input format.",
                    "classification_explanation": "While prompt brevity is known, the explicit information bottleneck framing and its predictive implications are novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and salience]",
                        "Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt focus and performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Extraneous Information Bottleneck Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "irrelevant_or_extraneous_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_decreased",
                        "object": "on_given_task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Adding irrelevant sentences or distractors to prompts reduces LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that LLMs can be misled by adversarial or off-topic content in the prompt.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt length increases with irrelevant content, which can dilute attention and context window usage.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that irrelevant information can distract LLMs and reduce accuracy.",
                    "what_is_novel": "This law formalizes the effect as an information bottleneck and predicts a monotonic decrease in performance with increasing extraneous content.",
                    "classification_explanation": "The law extends prompt engineering findings into a formal information-theoretic framework.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2020) How Can We Know What Language Models Know? [Prompt distractors and performance]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and distractors]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Removing irrelevant sentences from a prompt will increase LLM accuracy on the core task.",
        "Rewriting verbose prompts to be concise and focused will improve performance.",
        "Adding distractor information to a prompt will decrease LLM accuracy, even if the core task is unchanged."
    ],
    "new_predictions_unknown": [
        "If a prompt is compressed to the minimal information necessary, does performance plateau or can it be further improved by re-encoding the information?",
        "For LLMs with very large context windows, does the negative effect of extraneous information persist or diminish?",
        "Can LLMs be trained to ignore extraneous information entirely, or is there always some performance penalty?"
    ],
    "negative_experiments": [
        "If adding irrelevant information to a prompt does not decrease performance, the theory is challenged.",
        "If LLMs perform better on verbose, distractor-laden prompts than on concise, focused ones, the theory is falsified.",
        "If LLMs can always extract the correct answer regardless of prompt structure or salience, the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs ignore distractors and maintain high performance, possibly due to scale or advanced attention mechanisms.",
            "uuids": []
        },
        {
            "text": "Instances where concise prompts omit necessary context, leading to lower performance despite high salience.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs with large context windows show less degradation in performance with added extraneous information.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks requiring background knowledge may benefit from additional context, even if not directly relevant.",
        "Instruction-tuned LLMs may be more robust to extraneous information.",
        "For very simple tasks, the effect of extraneous information may be negligible."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt brevity and focus are known to improve LLM outputs; distractors are known to reduce accuracy.",
        "what_is_novel": "The explicit information bottleneck framing and its predictive implications are new.",
        "classification_explanation": "The theory synthesizes prompt engineering and information theory, providing a new lens for understanding format effects.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and salience]",
            "Jiang et al. (2020) How Can We Know What Language Models Know? [Prompt distractors and performance]",
            "Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt focus and performance]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-655",
    "original_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
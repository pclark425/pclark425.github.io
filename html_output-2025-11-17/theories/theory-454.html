<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memory Retrieval Precision-Recall Trade-off Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-454</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-454</p>
                <p><strong>Name:</strong> Memory Retrieval Precision-Recall Trade-off Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM-based agents can most effectively be augmented with memory to solve text games, based on the following results.</p>
                <p><strong>Description:</strong> The effectiveness of memory-augmented LLM agents in text games is governed by a fundamental trade-off between retrieval precision (returning only relevant memories) and recall (returning all relevant memories). Different retrieval mechanisms occupy different points on this trade-off curve: semantic similarity retrieval achieves high precision but may miss relevant memories with different surface forms; recency-based retrieval achieves high recall for recent events but poor precision for distant relevant memories; graph-based traversal achieves high precision for connected information but may miss isolated relevant facts; hierarchical retrieval can achieve good precision-recall balance but risks search failures. The optimal operating point on this trade-off depends on task characteristics: tasks with high information density require high precision to avoid context overload, while tasks with sparse critical information require high recall to avoid missing key facts. Multi-criteria retrieval combining recency, importance, and relevance consistently outperforms single-criterion retrieval by 15-50%. This theory predicts that hybrid retrieval mechanisms that adaptively balance precision-recall based on context will outperform fixed-strategy retrieval.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Law 0</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; retrieval mechanism &#8594; uses only &#8594; semantic similarity<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; contains &#8594; relevant memories with diverse surface forms</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; retrieval recall &#8594; is &#8594; low<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; misses &#8594; critical information with different phrasing</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>RAG with pure semantic retrieval underperformed on tasks requiring multi-hop reasoning across differently-phrased facts in TextWorld, achieving only 0.33-0.39 normalized scores <a href="../results/extraction-result-2945.html#e2945.2" class="evidence-link">[e2945.2]</a> </li>
    <li>Semantic retrieval alone struggled with ambiguous instructions in KARMA, causing degraded Memory Retrieval Accuracy <a href="../results/extraction-result-2925.html#e2925.0" class="evidence-link">[e2925.0]</a> </li>
    <li>Vector-based retrieval had limitations in capturing all relevant context in ThinkThrice; OpenAI embeddings achieved 0.798 similarity vs 0.578 for TF-IDF <a href="../results/extraction-result-2966.html#e2966.0" class="evidence-link">[e2966.0]</a> </li>
    <li>Pure embedding-based retrieval in ReadAgent required interactive lookup to recover missing details <a href="../results/extraction-result-2929.html#e2929.0" class="evidence-link">[e2929.0]</a> </li>
    <li>Semantic retrieval in Ariadne (RAG) achieved only 0.33 on Treasure Hunt vs 1.0 for graph-based memory <a href="../results/extraction-result-2945.html#e2945.2" class="evidence-link">[e2945.2]</a> </li>
    <li>MemoryBank's similarity-based retrieval depends on retrieval accuracy and relevance scoring, with noted limitations <a href="../results/extraction-result-2939.html#e2939.5" class="evidence-link">[e2939.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Law 1</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; retrieval mechanism &#8594; uses only &#8594; recency-based selection<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; distant past information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; retrieval precision &#8594; is &#8594; low<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; includes &#8594; irrelevant recent information<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; misses &#8594; critical distant information</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Simple recency-based memory in ReadAgent required gist compression and interactive lookup to handle long contexts effectively <a href="../results/extraction-result-2929.html#e2929.0" class="evidence-link">[e2929.0]</a> </li>
    <li>Sliding window memory (K=10) in MC-DML missed critical earlier clues in 'needle in haystack' problems <a href="../results/extraction-result-2927.html#e2927.0" class="evidence-link">[e2927.0]</a> </li>
    <li>Recency-only retrieval in baseline Generative Agents led to context overload (~2000 tokens) without improving performance <a href="../results/extraction-result-2949.html#e2949.0" class="evidence-link">[e2949.0]</a> </li>
    <li>Short-term memory limited to last 3 actions in LLaMA-Rider was insufficient for long-horizon dependencies <a href="../results/extraction-result-2917.html#e2917.0" class="evidence-link">[e2917.0]</a> </li>
    <li>SWIFT's K=10 sliding window memory, while better than K=1, still had limitations for multi-step reasoning <a href="../results/extraction-result-2977.html#e2977.1" class="evidence-link">[e2977.1]</a> </li>
    <li>NetPlay's 500-token recency-based memory required aggressive truncation, losing older context <a href="../results/extraction-result-2975.html#e2975.0" class="evidence-link">[e2975.0]</a> </li>
    <li>HIAGENT's working memory without hierarchical organization led to context overload and reduced executability <a href="../results/extraction-result-2937.html#e2937.0" class="evidence-link">[e2937.0]</a> </li>
    <li>Prompt-based recency memory in SmartPlay agents failed to reliably track intermediate states in Tower of Hanoi <a href="../results/extraction-result-2944.html#e2944.0" class="evidence-link">[e2944.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Law 2</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; retrieval mechanism &#8594; combines &#8594; multiple retrieval strategies<span style="color: #888888;">, and</span></div>
        <div>&#8226; combination &#8594; includes &#8594; recency + importance + relevance</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; retrieval &#8594; achieves &#8594; better precision-recall balance<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; improves by &#8594; 15-50% over single-strategy retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Generative Agents with recency+importance+relevance scoring achieved strong performance in social simulation with Activity score 3.13±0.19 <a href="../results/extraction-result-2949.html#e2949.0" class="evidence-link">[e2949.0]</a> </li>
    <li>KARMA with multi-criteria retrieval (hit rate optimization) improved success by 1.3x for composite tasks and 2.3x for complex tasks <a href="../results/extraction-result-2925.html#e2925.0" class="evidence-link">[e2925.0]</a> </li>
    <li>ThinkThrice with Memory Retrieval + Self-Refinement + Self-Verification improved factual QA from 0.305 to 0.498 (+63% relative) <a href="../results/extraction-result-2966.html#e2966.0" class="evidence-link">[e2966.0]</a> </li>
    <li>CBR-GDA with hybrid retrieval (semantic + feature + structural) outperformed vanilla RAG/CoT approaches <a href="../results/extraction-result-2910.html#e2910.0" class="evidence-link">[e2910.0]</a> </li>
    <li>GATA with graph+text attention aggregator outperformed text-only baselines by +24.2% average relative improvement <a href="../results/extraction-result-2968.html#e2968.0" class="evidence-link">[e2968.0]</a> </li>
    <li>Ariadne (Simulacra) with recency/importance/relevance scoring achieved 0.40-0.70 on TextWorld tasks <a href="../results/extraction-result-2945.html#e2945.4" class="evidence-link">[e2945.4]</a> </li>
    <li>SAGE with retention-based STM/LTM plus reflections improved ALFWorld from 56.5% to 73.8% (+17.3 pp) <a href="../results/extraction-result-2941.html#e2941.0" class="evidence-link">[e2941.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Law 3</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; has &#8594; high information density<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; optimizes for &#8594; high precision</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; avoids &#8594; context overload<span style="color: #888888;">, and</span></div>
        <div>&#8226; token efficiency &#8594; improves by &#8594; 50-96%</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ReadAgent with targeted gist retrieval achieved 85.53-96.80% compression while maintaining 86.16% accuracy on QuALITY <a href="../results/extraction-result-2929.html#e2929.0" class="evidence-link">[e2929.0]</a> </li>
    <li>AGA with selective social memory reduced tokens to 31.1% of baseline while maintaining comparable human-likeness scores <a href="../results/extraction-result-2949.html#e2949.1" class="evidence-link">[e2949.1]</a> </li>
    <li>HIAGENT with selective subgoal memory reduced tokens by 35% while improving success rate from 21% to 42% <a href="../results/extraction-result-2937.html#e2937.0" class="evidence-link">[e2937.0]</a> </li>
    <li>SWIFTSAGE with episodic memory augmentation reduced tokens-per-action from 1855.84 (SayCan) to 757.07 <a href="../results/extraction-result-2977.html#e2977.0" class="evidence-link">[e2977.0]</a> </li>
    <li>Memory-R1 with Memory Distillation reduced noise and improved answer accuracy by filtering retrieved candidates <a href="../results/extraction-result-2952.html#e2952.0" class="evidence-link">[e2952.0]</a> </li>
    <li>AGA's Lifestyle Policy with case-based retrieval reduced tokens to 40.2% of baseline <a href="../results/extraction-result-2949.html#e2949.1" class="evidence-link">[e2949.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 4: Law 4</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; retrieval mechanism &#8594; uses &#8594; graph-based traversal<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; multi-hop relational reasoning</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; retrieval precision &#8594; is &#8594; high for connected information<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval recall &#8594; is &#8594; low for isolated facts<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; improves by &#8594; 20-100% over unstructured retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AriGraph with graph-based memory achieved 1.0 normalized score on Treasure Hunt vs 0.33 for RAG and 0.47 for full history <a href="../results/extraction-result-2945.html#e2945.0" class="evidence-link">[e2945.0]</a> <a href="../results/extraction-result-2945.html#e2945.2" class="evidence-link">[e2945.2]</a> <a href="../results/extraction-result-2945.html#e2945.1" class="evidence-link">[e2945.1]</a> </li>
    <li>GATA with belief graph memory achieved +24.2% average relative improvement over text-only Tr-DQN baseline <a href="../results/extraction-result-2968.html#e2968.0" class="evidence-link">[e2968.0]</a> </li>
    <li>KG-A2C with knowledge graph constraints outperformed TDQN on 23/28 Jericho games <a href="../results/extraction-result-2972.html#e2972.0" class="evidence-link">[e2972.0]</a> </li>
    <li>Graph-structured memory in Ghost enabled general capabilities in open-world Minecraft tasks <a href="../results/extraction-result-2931.html#e2931.5" class="evidence-link">[e2931.5]</a> </li>
    <li>MemWalker's hierarchical tree traversal had 8.6% search failure rate, indicating precision-recall trade-offs in graph traversal <a href="../results/extraction-result-2929.html#e2929.1" class="evidence-link">[e2929.1]</a> </li>
    <li>MPRC-DQN with object-centric retrieval won best per-game on 21/33 Jericho games vs 17/33 for RC-DQN without history <a href="../results/extraction-result-2976.html#e2976.0" class="evidence-link">[e2976.0]</a> <a href="../results/extraction-result-2976.html#e2976.1" class="evidence-link">[e2976.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 5: Law 5</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; has &#8594; sparse critical information<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; optimizes for &#8594; high recall</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; avoids &#8594; missing critical facts<span style="color: #888888;">, and</span></div>
        <div>&#8226; task success rate &#8594; improves by &#8594; 10-30%</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MC-DML with cross-trial memory (K=3 reflections) improved Zork1 score by +10.33 points (+27% relative) by avoiding missed critical actions <a href="../results/extraction-result-2927.html#e2927.0" class="evidence-link">[e2927.0]</a> </li>
    <li>KARMA's short-term memory with high hit rates linearly correlated with reduced exploration and faster task execution <a href="../results/extraction-result-2925.html#e2925.0" class="evidence-link">[e2925.0]</a> </li>
    <li>ThinkThrice with broader retrieval (top-5 + script consultation) improved from 0.305 to 0.498 on factual QA <a href="../results/extraction-result-2966.html#e2966.0" class="evidence-link">[e2966.0]</a> </li>
    <li>ReadAgent with interactive lookup (sequential 1-6 pages) achieved 87.17% accuracy vs 77.52% for gist-only <a href="../results/extraction-result-2929.html#e2929.0" class="evidence-link">[e2929.0]</a> </li>
    <li>SAGE with long-term memory retrieval improved HotPotQA from 54.1% to 74.9% (+20.8 pp) <a href="../results/extraction-result-2941.html#e2941.0" class="evidence-link">[e2941.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Retrieval mechanisms that dynamically adjust precision-recall trade-off based on current context uncertainty will outperform fixed-strategy retrieval by 10-25%</li>
                <li>For tasks with known critical information patterns, learned retrieval strategies will outperform hand-crafted heuristics by 15-30%</li>
                <li>Hybrid retrieval combining semantic, structural, and temporal signals will outperform any single-signal retrieval by 20-40%</li>
                <li>Agents with adaptive retrieval that increases recall when uncertainty is high and increases precision when context is dense will achieve 15-30% better token efficiency than fixed-strategy agents</li>
                <li>Multi-modal retrieval (text + image + structured data) will achieve 10-20% better precision-recall balance than text-only retrieval in embodied environments</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a universal optimal precision-recall operating point that works across all text game genres, or if optimal points are fundamentally task-dependent</li>
                <li>Whether agents can learn to meta-optimize their retrieval strategy through reinforcement learning on retrieval decisions, and if so, whether learned strategies generalize across tasks</li>
                <li>Whether the precision-recall trade-off can be eliminated through sufficiently advanced retrieval mechanisms, or if it represents a fundamental constraint imposed by LLM context windows</li>
                <li>Whether hierarchical retrieval mechanisms can achieve both high precision and high recall simultaneously, or if they merely shift the trade-off to a different level of the hierarchy</li>
                <li>Whether the optimal retrieval strategy changes as memory size grows, and if there are phase transitions in optimal strategy at certain memory scales</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where random retrieval performs as well as optimized retrieval would challenge the importance of precision-recall optimization</li>
                <li>Demonstrating that always retrieving all memories (maximum recall, minimum precision) consistently outperforms selective retrieval would question the value of precision</li>
                <li>Showing that retrieval strategy has no impact on performance when memory size is small (<100 entries) would suggest the trade-off only matters at scale</li>
                <li>Finding that single-criterion retrieval (e.g., recency-only) consistently matches or beats multi-criterion retrieval would challenge the value of hybrid approaches</li>
                <li>Demonstrating that graph-based retrieval performs no better than random retrieval on multi-hop reasoning tasks would question the value of structural retrieval</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The computational costs of different retrieval mechanisms and their impact on real-time performance (e.g., graph traversal vs vector similarity) </li>
    <li>How retrieval precision-recall requirements change as agents learn and memory grows over time (e.g., whether optimal strategies shift with experience) </li>
    <li>The interaction between retrieval strategy and memory structure (e.g., whether graph memory requires different retrieval than vector memory) <a href="../results/extraction-result-2945.html#e2945.0" class="evidence-link">[e2945.0]</a> <a href="../results/extraction-result-2945.html#e2945.2" class="evidence-link">[e2945.2]</a> <a href="../results/extraction-result-2968.html#e2968.0" class="evidence-link">[e2968.0]</a> </li>
    <li>How multi-modal memory (text + images + structured data) affects precision-recall trade-offs compared to text-only memory <a href="../results/extraction-result-2938.html#e2938.0" class="evidence-link">[e2938.0]</a> <a href="../results/extraction-result-2925.html#e2925.0" class="evidence-link">[e2925.0]</a> </li>
    <li>The role of memory update frequency and staleness in retrieval effectiveness </li>
    <li>How retrieval failures cascade through multi-step reasoning and planning </li>
    <li>The impact of retrieval latency on interactive performance in real-time text games </li>
    <li>Whether retrieval precision-recall requirements differ between single-agent and multi-agent scenarios <a href="../results/extraction-result-2915.html#e2915.0" class="evidence-link">[e2915.0]</a> <a href="../results/extraction-result-2966.html#e2966.0" class="evidence-link">[e2966.0]</a> <a href="../results/extraction-result-2967.html#e2967.0" class="evidence-link">[e2967.0]</a> </li>
    <li>How reflection-based memory updates affect retrieval precision-recall balance <a href="../results/extraction-result-2927.html#e2927.0" class="evidence-link">[e2927.0]</a> <a href="../results/extraction-result-2933.html#e2933.0" class="evidence-link">[e2933.0]</a> <a href="../results/extraction-result-2941.html#e2941.0" class="evidence-link">[e2941.0]</a> </li>
    <li>The relationship between retrieval strategy and different types of partial observability </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Uses multi-factor retrieval (recency+importance+relevance) but doesn't formalize precision-recall trade-off theory]</li>
    <li>Lee et al. (2024) A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts [Addresses retrieval efficiency and compression but doesn't theorize precision-recall trade-offs]</li>
    <li>Zhong et al. (2024) AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents [Compares retrieval mechanisms but doesn't formalize precision-recall framework]</li>
    <li>Salton & McGill (1983) Introduction to Modern Information Retrieval [Classical IR theory on precision-recall trade-offs, but not specifically applied to LLM agent memory]</li>
    <li>Manning et al. (2008) Introduction to Information Retrieval [Covers precision-recall curves in IR, but not in context of LLM agents]</li>
    <li>Adhikari et al. (2020) Learning Dynamic Belief Graphs to Generalize on Text-Based Games [Uses graph-based memory but doesn't formalize retrieval trade-offs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Memory Retrieval Precision-Recall Trade-off Theory",
    "theory_description": "The effectiveness of memory-augmented LLM agents in text games is governed by a fundamental trade-off between retrieval precision (returning only relevant memories) and recall (returning all relevant memories). Different retrieval mechanisms occupy different points on this trade-off curve: semantic similarity retrieval achieves high precision but may miss relevant memories with different surface forms; recency-based retrieval achieves high recall for recent events but poor precision for distant relevant memories; graph-based traversal achieves high precision for connected information but may miss isolated relevant facts; hierarchical retrieval can achieve good precision-recall balance but risks search failures. The optimal operating point on this trade-off depends on task characteristics: tasks with high information density require high precision to avoid context overload, while tasks with sparse critical information require high recall to avoid missing key facts. Multi-criteria retrieval combining recency, importance, and relevance consistently outperforms single-criterion retrieval by 15-50%. This theory predicts that hybrid retrieval mechanisms that adaptively balance precision-recall based on context will outperform fixed-strategy retrieval.",
    "theory_statements": [
        {
            "law": {
                "if": [
                    {
                        "subject": "retrieval mechanism",
                        "relation": "uses only",
                        "object": "semantic similarity"
                    },
                    {
                        "subject": "task",
                        "relation": "contains",
                        "object": "relevant memories with diverse surface forms"
                    }
                ],
                "then": [
                    {
                        "subject": "retrieval recall",
                        "relation": "is",
                        "object": "low"
                    },
                    {
                        "subject": "agent",
                        "relation": "misses",
                        "object": "critical information with different phrasing"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "RAG with pure semantic retrieval underperformed on tasks requiring multi-hop reasoning across differently-phrased facts in TextWorld, achieving only 0.33-0.39 normalized scores",
                        "uuids": [
                            "e2945.2"
                        ]
                    },
                    {
                        "text": "Semantic retrieval alone struggled with ambiguous instructions in KARMA, causing degraded Memory Retrieval Accuracy",
                        "uuids": [
                            "e2925.0"
                        ]
                    },
                    {
                        "text": "Vector-based retrieval had limitations in capturing all relevant context in ThinkThrice; OpenAI embeddings achieved 0.798 similarity vs 0.578 for TF-IDF",
                        "uuids": [
                            "e2966.0"
                        ]
                    },
                    {
                        "text": "Pure embedding-based retrieval in ReadAgent required interactive lookup to recover missing details",
                        "uuids": [
                            "e2929.0"
                        ]
                    },
                    {
                        "text": "Semantic retrieval in Ariadne (RAG) achieved only 0.33 on Treasure Hunt vs 1.0 for graph-based memory",
                        "uuids": [
                            "e2945.2"
                        ]
                    },
                    {
                        "text": "MemoryBank's similarity-based retrieval depends on retrieval accuracy and relevance scoring, with noted limitations",
                        "uuids": [
                            "e2939.5"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "retrieval mechanism",
                        "relation": "uses only",
                        "object": "recency-based selection"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "distant past information"
                    }
                ],
                "then": [
                    {
                        "subject": "retrieval precision",
                        "relation": "is",
                        "object": "low"
                    },
                    {
                        "subject": "agent",
                        "relation": "includes",
                        "object": "irrelevant recent information"
                    },
                    {
                        "subject": "agent",
                        "relation": "misses",
                        "object": "critical distant information"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Simple recency-based memory in ReadAgent required gist compression and interactive lookup to handle long contexts effectively",
                        "uuids": [
                            "e2929.0"
                        ]
                    },
                    {
                        "text": "Sliding window memory (K=10) in MC-DML missed critical earlier clues in 'needle in haystack' problems",
                        "uuids": [
                            "e2927.0"
                        ]
                    },
                    {
                        "text": "Recency-only retrieval in baseline Generative Agents led to context overload (~2000 tokens) without improving performance",
                        "uuids": [
                            "e2949.0"
                        ]
                    },
                    {
                        "text": "Short-term memory limited to last 3 actions in LLaMA-Rider was insufficient for long-horizon dependencies",
                        "uuids": [
                            "e2917.0"
                        ]
                    },
                    {
                        "text": "SWIFT's K=10 sliding window memory, while better than K=1, still had limitations for multi-step reasoning",
                        "uuids": [
                            "e2977.1"
                        ]
                    },
                    {
                        "text": "NetPlay's 500-token recency-based memory required aggressive truncation, losing older context",
                        "uuids": [
                            "e2975.0"
                        ]
                    },
                    {
                        "text": "HIAGENT's working memory without hierarchical organization led to context overload and reduced executability",
                        "uuids": [
                            "e2937.0"
                        ]
                    },
                    {
                        "text": "Prompt-based recency memory in SmartPlay agents failed to reliably track intermediate states in Tower of Hanoi",
                        "uuids": [
                            "e2944.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "retrieval mechanism",
                        "relation": "combines",
                        "object": "multiple retrieval strategies"
                    },
                    {
                        "subject": "combination",
                        "relation": "includes",
                        "object": "recency + importance + relevance"
                    }
                ],
                "then": [
                    {
                        "subject": "retrieval",
                        "relation": "achieves",
                        "object": "better precision-recall balance"
                    },
                    {
                        "subject": "performance",
                        "relation": "improves by",
                        "object": "15-50% over single-strategy retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Generative Agents with recency+importance+relevance scoring achieved strong performance in social simulation with Activity score 3.13±0.19",
                        "uuids": [
                            "e2949.0"
                        ]
                    },
                    {
                        "text": "KARMA with multi-criteria retrieval (hit rate optimization) improved success by 1.3x for composite tasks and 2.3x for complex tasks",
                        "uuids": [
                            "e2925.0"
                        ]
                    },
                    {
                        "text": "ThinkThrice with Memory Retrieval + Self-Refinement + Self-Verification improved factual QA from 0.305 to 0.498 (+63% relative)",
                        "uuids": [
                            "e2966.0"
                        ]
                    },
                    {
                        "text": "CBR-GDA with hybrid retrieval (semantic + feature + structural) outperformed vanilla RAG/CoT approaches",
                        "uuids": [
                            "e2910.0"
                        ]
                    },
                    {
                        "text": "GATA with graph+text attention aggregator outperformed text-only baselines by +24.2% average relative improvement",
                        "uuids": [
                            "e2968.0"
                        ]
                    },
                    {
                        "text": "Ariadne (Simulacra) with recency/importance/relevance scoring achieved 0.40-0.70 on TextWorld tasks",
                        "uuids": [
                            "e2945.4"
                        ]
                    },
                    {
                        "text": "SAGE with retention-based STM/LTM plus reflections improved ALFWorld from 56.5% to 73.8% (+17.3 pp)",
                        "uuids": [
                            "e2941.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "has",
                        "object": "high information density"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "optimizes for",
                        "object": "high precision"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "avoids",
                        "object": "context overload"
                    },
                    {
                        "subject": "token efficiency",
                        "relation": "improves by",
                        "object": "50-96%"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ReadAgent with targeted gist retrieval achieved 85.53-96.80% compression while maintaining 86.16% accuracy on QuALITY",
                        "uuids": [
                            "e2929.0"
                        ]
                    },
                    {
                        "text": "AGA with selective social memory reduced tokens to 31.1% of baseline while maintaining comparable human-likeness scores",
                        "uuids": [
                            "e2949.1"
                        ]
                    },
                    {
                        "text": "HIAGENT with selective subgoal memory reduced tokens by 35% while improving success rate from 21% to 42%",
                        "uuids": [
                            "e2937.0"
                        ]
                    },
                    {
                        "text": "SWIFTSAGE with episodic memory augmentation reduced tokens-per-action from 1855.84 (SayCan) to 757.07",
                        "uuids": [
                            "e2977.0"
                        ]
                    },
                    {
                        "text": "Memory-R1 with Memory Distillation reduced noise and improved answer accuracy by filtering retrieved candidates",
                        "uuids": [
                            "e2952.0"
                        ]
                    },
                    {
                        "text": "AGA's Lifestyle Policy with case-based retrieval reduced tokens to 40.2% of baseline",
                        "uuids": [
                            "e2949.1"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "retrieval mechanism",
                        "relation": "uses",
                        "object": "graph-based traversal"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "multi-hop relational reasoning"
                    }
                ],
                "then": [
                    {
                        "subject": "retrieval precision",
                        "relation": "is",
                        "object": "high for connected information"
                    },
                    {
                        "subject": "retrieval recall",
                        "relation": "is",
                        "object": "low for isolated facts"
                    },
                    {
                        "subject": "performance",
                        "relation": "improves by",
                        "object": "20-100% over unstructured retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AriGraph with graph-based memory achieved 1.0 normalized score on Treasure Hunt vs 0.33 for RAG and 0.47 for full history",
                        "uuids": [
                            "e2945.0",
                            "e2945.2",
                            "e2945.1"
                        ]
                    },
                    {
                        "text": "GATA with belief graph memory achieved +24.2% average relative improvement over text-only Tr-DQN baseline",
                        "uuids": [
                            "e2968.0"
                        ]
                    },
                    {
                        "text": "KG-A2C with knowledge graph constraints outperformed TDQN on 23/28 Jericho games",
                        "uuids": [
                            "e2972.0"
                        ]
                    },
                    {
                        "text": "Graph-structured memory in Ghost enabled general capabilities in open-world Minecraft tasks",
                        "uuids": [
                            "e2931.5"
                        ]
                    },
                    {
                        "text": "MemWalker's hierarchical tree traversal had 8.6% search failure rate, indicating precision-recall trade-offs in graph traversal",
                        "uuids": [
                            "e2929.1"
                        ]
                    },
                    {
                        "text": "MPRC-DQN with object-centric retrieval won best per-game on 21/33 Jericho games vs 17/33 for RC-DQN without history",
                        "uuids": [
                            "e2976.0",
                            "e2976.1"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "has",
                        "object": "sparse critical information"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "optimizes for",
                        "object": "high recall"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "avoids",
                        "object": "missing critical facts"
                    },
                    {
                        "subject": "task success rate",
                        "relation": "improves by",
                        "object": "10-30%"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MC-DML with cross-trial memory (K=3 reflections) improved Zork1 score by +10.33 points (+27% relative) by avoiding missed critical actions",
                        "uuids": [
                            "e2927.0"
                        ]
                    },
                    {
                        "text": "KARMA's short-term memory with high hit rates linearly correlated with reduced exploration and faster task execution",
                        "uuids": [
                            "e2925.0"
                        ]
                    },
                    {
                        "text": "ThinkThrice with broader retrieval (top-5 + script consultation) improved from 0.305 to 0.498 on factual QA",
                        "uuids": [
                            "e2966.0"
                        ]
                    },
                    {
                        "text": "ReadAgent with interactive lookup (sequential 1-6 pages) achieved 87.17% accuracy vs 77.52% for gist-only",
                        "uuids": [
                            "e2929.0"
                        ]
                    },
                    {
                        "text": "SAGE with long-term memory retrieval improved HotPotQA from 54.1% to 74.9% (+20.8 pp)",
                        "uuids": [
                            "e2941.0"
                        ]
                    }
                ]
            }
        }
    ],
    "new_predictions_likely": [
        "Retrieval mechanisms that dynamically adjust precision-recall trade-off based on current context uncertainty will outperform fixed-strategy retrieval by 10-25%",
        "For tasks with known critical information patterns, learned retrieval strategies will outperform hand-crafted heuristics by 15-30%",
        "Hybrid retrieval combining semantic, structural, and temporal signals will outperform any single-signal retrieval by 20-40%",
        "Agents with adaptive retrieval that increases recall when uncertainty is high and increases precision when context is dense will achieve 15-30% better token efficiency than fixed-strategy agents",
        "Multi-modal retrieval (text + image + structured data) will achieve 10-20% better precision-recall balance than text-only retrieval in embodied environments"
    ],
    "new_predictions_unknown": [
        "Whether there exists a universal optimal precision-recall operating point that works across all text game genres, or if optimal points are fundamentally task-dependent",
        "Whether agents can learn to meta-optimize their retrieval strategy through reinforcement learning on retrieval decisions, and if so, whether learned strategies generalize across tasks",
        "Whether the precision-recall trade-off can be eliminated through sufficiently advanced retrieval mechanisms, or if it represents a fundamental constraint imposed by LLM context windows",
        "Whether hierarchical retrieval mechanisms can achieve both high precision and high recall simultaneously, or if they merely shift the trade-off to a different level of the hierarchy",
        "Whether the optimal retrieval strategy changes as memory size grows, and if there are phase transitions in optimal strategy at certain memory scales"
    ],
    "negative_experiments": [
        "Finding tasks where random retrieval performs as well as optimized retrieval would challenge the importance of precision-recall optimization",
        "Demonstrating that always retrieving all memories (maximum recall, minimum precision) consistently outperforms selective retrieval would question the value of precision",
        "Showing that retrieval strategy has no impact on performance when memory size is small (&lt;100 entries) would suggest the trade-off only matters at scale",
        "Finding that single-criterion retrieval (e.g., recency-only) consistently matches or beats multi-criterion retrieval would challenge the value of hybrid approaches",
        "Demonstrating that graph-based retrieval performs no better than random retrieval on multi-hop reasoning tasks would question the value of structural retrieval"
    ],
    "unaccounted_for": [
        {
            "text": "The computational costs of different retrieval mechanisms and their impact on real-time performance (e.g., graph traversal vs vector similarity)",
            "uuids": []
        },
        {
            "text": "How retrieval precision-recall requirements change as agents learn and memory grows over time (e.g., whether optimal strategies shift with experience)",
            "uuids": []
        },
        {
            "text": "The interaction between retrieval strategy and memory structure (e.g., whether graph memory requires different retrieval than vector memory)",
            "uuids": [
                "e2945.0",
                "e2945.2",
                "e2968.0"
            ]
        },
        {
            "text": "How multi-modal memory (text + images + structured data) affects precision-recall trade-offs compared to text-only memory",
            "uuids": [
                "e2938.0",
                "e2925.0"
            ]
        },
        {
            "text": "The role of memory update frequency and staleness in retrieval effectiveness",
            "uuids": []
        },
        {
            "text": "How retrieval failures cascade through multi-step reasoning and planning",
            "uuids": []
        },
        {
            "text": "The impact of retrieval latency on interactive performance in real-time text games",
            "uuids": []
        },
        {
            "text": "Whether retrieval precision-recall requirements differ between single-agent and multi-agent scenarios",
            "uuids": [
                "e2915.0",
                "e2966.0",
                "e2967.0"
            ]
        },
        {
            "text": "How reflection-based memory updates affect retrieval precision-recall balance",
            "uuids": [
                "e2927.0",
                "e2933.0",
                "e2941.0"
            ]
        },
        {
            "text": "The relationship between retrieval strategy and different types of partial observability",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some simple tasks showed that retrieving all memories (full history) performed adequately despite low precision (e.g., Ariadne full history achieved 0.47 on Treasure Hunt)",
            "uuids": [
                "e2945.1"
            ]
        },
        {
            "text": "In some cases, high-precision retrieval missed critical information and underperformed broader retrieval (e.g., MemWalker's 8.6% search failure rate vs ReadAgent's more reliable retrieval)",
            "uuids": [
                "e2929.1",
                "e2929.0"
            ]
        },
        {
            "text": "Graph-based retrieval sometimes failed to exploit stored information (e.g., GATA's policy ignored cookbook instructions despite them being in the belief graph)",
            "uuids": [
                "e2960.0"
            ]
        },
        {
            "text": "Simple recency-based memory sometimes matched or exceeded sophisticated retrieval (e.g., SWIFT-only with K=10 window achieved 49.22 on ScienceWorld)",
            "uuids": [
                "e2977.1"
            ]
        },
        {
            "text": "In some cases, no explicit retrieval mechanism (just prompt context) performed adequately (e.g., ReAct with in-context thoughts achieved 71% on ALFWorld)",
            "uuids": [
                "e2971.0"
            ]
        },
        {
            "text": "Reflection-based memory sometimes provided minimal benefit (e.g., reflective-LLM showed mixed effects with shortened trajectories reducing quality)",
            "uuids": [
                "e2907.0"
            ]
        }
    ],
    "special_cases": [
        "For very small memory sizes (&lt;100 entries), the precision-recall trade-off may be negligible as all memories can fit in context",
        "In fully deterministic environments with perfect information, retrieval precision may matter less than in stochastic environments with partial observability",
        "For tasks with explicit memory queries (e.g., 'what did X say?'), high recall may be more important than precision",
        "In multi-agent scenarios, retrieval precision may be more important to avoid confusion between agents' memories",
        "For tasks with very long horizons (&gt;100 steps), recall of distant memories becomes increasingly important",
        "In tasks with hierarchical structure, retrieval at different levels of abstraction may require different precision-recall trade-offs",
        "For tasks with dense feedback, recency-based retrieval may be sufficient; for tasks with sparse feedback, broader retrieval is necessary",
        "When memory contains redundant information, high precision becomes more valuable to avoid repetition",
        "In tasks requiring exact state tracking (e.g., Tower of Hanoi), high precision is critical; in tasks requiring exploration, high recall is more important"
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Uses multi-factor retrieval (recency+importance+relevance) but doesn't formalize precision-recall trade-off theory]",
            "Lee et al. (2024) A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts [Addresses retrieval efficiency and compression but doesn't theorize precision-recall trade-offs]",
            "Zhong et al. (2024) AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents [Compares retrieval mechanisms but doesn't formalize precision-recall framework]",
            "Salton & McGill (1983) Introduction to Modern Information Retrieval [Classical IR theory on precision-recall trade-offs, but not specifically applied to LLM agent memory]",
            "Manning et al. (2008) Introduction to Information Retrieval [Covers precision-recall curves in IR, but not in context of LLM agents]",
            "Adhikari et al. (2020) Learning Dynamic Belief Graphs to Generalize on Text-Based Games [Uses graph-based memory but doesn't formalize retrieval trade-offs]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
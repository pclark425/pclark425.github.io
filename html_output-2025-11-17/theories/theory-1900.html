<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1900</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1900</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that LLM performance is maximized when the problem presentation format is aligned with the model's pretraining distribution and internal cognitive priors. Formats that match the statistical and structural patterns the LLM has seen during training (e.g., textbook-style explanations, Q&A, code blocks) facilitate more effective retrieval and application of relevant knowledge. Conversely, formats that deviate from these priors (e.g., novel, ambiguous, or highly compressed forms) reduce performance by forcing the model to extrapolate beyond its learned representations.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Pretraining Distribution Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_similar_to &#8594; llm_pretraining_distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_maximized_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best on tasks and formats that closely resemble their pretraining data (e.g., Wikipedia, StackExchange, textbook Q&A). </li>
    <li>Performance drops on tasks with novel or out-of-distribution formats. </li>
    <li>Prompting LLMs with code-like or markdown formats improves performance on code-related tasks. </li>
    <li>LLMs show higher accuracy on multiple-choice questions when the format matches standardized test data seen during pretraining. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law generalizes known in-distribution effects into a predictive theory of presentation alignment.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs perform better on in-distribution tasks and formats.</p>            <p><strong>What is Novel:</strong> The law formalizes this as a cognitive alignment principle, linking performance to pretraining format similarity.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [In-distribution prompt effects]</li>
    <li>Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding [Format alignment and performance]</li>
    <li>Zhang et al. (2022) Prompting Language Models for Knowledge Base Completion [Prompt format and pretraining alignment]</li>
</ul>
            <h3>Statement 1: Cognitive Extrapolation Degradation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_novel_or_out_of_distribution_for &#8594; llm_pretraining_distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_degraded_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs struggle with tasks presented in formats not seen during pretraining, such as highly compressed symbolic notation or nonstandard question types. </li>
    <li>Performance on adversarially formatted prompts is lower than on standard formats. </li>
    <li>LLMs require more examples (few-shot) to adapt to unfamiliar formats. </li>
    <li>Zero-shot performance drops on tasks with novel presentation styles. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is closely related to existing findings but formalizes the extrapolation effect as a law.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs are less effective on out-of-distribution tasks and formats.</p>            <p><strong>What is Novel:</strong> The law frames this as a cognitive extrapolation problem, predicting systematic degradation with increasing format novelty.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Out-of-distribution prompt effects]</li>
    <li>Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding [Format novelty and performance]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Adaptation to new formats]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a problem is reformatted to match a style common in LLM pretraining data, performance will increase.</li>
                <li>Introducing a novel problem format will reduce LLM accuracy unless the model is given sufficient few-shot examples.</li>
                <li>LLMs will perform better on code problems when presented in code block format than in plain text.</li>
                <li>Performance on standardized test questions will be higher if the format matches that of the training data.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are fine-tuned on a new, synthetic problem format, performance may generalize to similar but unseen formats.</li>
                <li>If LLMs are exposed to a mixture of formats during pretraining, the effect on generalization to novel formats is uncertain.</li>
                <li>If LLMs are prompted with multimodal (text + image) formats, the alignment effect may be altered.</li>
                <li>If LLMs are given meta-instructions to adapt to new formats, the degradation may be mitigated.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on in-distribution and out-of-distribution formats, the theory is falsified.</li>
                <li>If LLMs can generalize perfectly to novel formats without additional examples, the theory is undermined.</li>
                <li>If performance is not correlated with pretraining format similarity, the alignment hypothesis is challenged.</li>
                <li>If LLMs perform better on unfamiliar formats than familiar ones, the theory is contradicted.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs generalize well to novel formats due to underlying reasoning ability rather than format familiarity. </li>
    <li>Instances where LLMs fail on familiar formats due to ambiguous or underspecified content. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known in-distribution effects into a broader, predictive framework of cognitive alignment.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]</li>
    <li>Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding [Format alignment and performance]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Adaptation to new formats]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that LLM performance is maximized when the problem presentation format is aligned with the model's pretraining distribution and internal cognitive priors. Formats that match the statistical and structural patterns the LLM has seen during training (e.g., textbook-style explanations, Q&A, code blocks) facilitate more effective retrieval and application of relevant knowledge. Conversely, formats that deviate from these priors (e.g., novel, ambiguous, or highly compressed forms) reduce performance by forcing the model to extrapolate beyond its learned representations.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Pretraining Distribution Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_similar_to",
                        "object": "llm_pretraining_distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_maximized_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best on tasks and formats that closely resemble their pretraining data (e.g., Wikipedia, StackExchange, textbook Q&A).",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops on tasks with novel or out-of-distribution formats.",
                        "uuids": []
                    },
                    {
                        "text": "Prompting LLMs with code-like or markdown formats improves performance on code-related tasks.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs show higher accuracy on multiple-choice questions when the format matches standardized test data seen during pretraining.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs perform better on in-distribution tasks and formats.",
                    "what_is_novel": "The law formalizes this as a cognitive alignment principle, linking performance to pretraining format similarity.",
                    "classification_explanation": "The law generalizes known in-distribution effects into a predictive theory of presentation alignment.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [In-distribution prompt effects]",
                        "Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding [Format alignment and performance]",
                        "Zhang et al. (2022) Prompting Language Models for Knowledge Base Completion [Prompt format and pretraining alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Cognitive Extrapolation Degradation Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_novel_or_out_of_distribution_for",
                        "object": "llm_pretraining_distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_degraded_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs struggle with tasks presented in formats not seen during pretraining, such as highly compressed symbolic notation or nonstandard question types.",
                        "uuids": []
                    },
                    {
                        "text": "Performance on adversarially formatted prompts is lower than on standard formats.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs require more examples (few-shot) to adapt to unfamiliar formats.",
                        "uuids": []
                    },
                    {
                        "text": "Zero-shot performance drops on tasks with novel presentation styles.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs are less effective on out-of-distribution tasks and formats.",
                    "what_is_novel": "The law frames this as a cognitive extrapolation problem, predicting systematic degradation with increasing format novelty.",
                    "classification_explanation": "The law is closely related to existing findings but formalizes the extrapolation effect as a law.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Out-of-distribution prompt effects]",
                        "Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding [Format novelty and performance]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [Adaptation to new formats]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a problem is reformatted to match a style common in LLM pretraining data, performance will increase.",
        "Introducing a novel problem format will reduce LLM accuracy unless the model is given sufficient few-shot examples.",
        "LLMs will perform better on code problems when presented in code block format than in plain text.",
        "Performance on standardized test questions will be higher if the format matches that of the training data."
    ],
    "new_predictions_unknown": [
        "If LLMs are fine-tuned on a new, synthetic problem format, performance may generalize to similar but unseen formats.",
        "If LLMs are exposed to a mixture of formats during pretraining, the effect on generalization to novel formats is uncertain.",
        "If LLMs are prompted with multimodal (text + image) formats, the alignment effect may be altered.",
        "If LLMs are given meta-instructions to adapt to new formats, the degradation may be mitigated."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on in-distribution and out-of-distribution formats, the theory is falsified.",
        "If LLMs can generalize perfectly to novel formats without additional examples, the theory is undermined.",
        "If performance is not correlated with pretraining format similarity, the alignment hypothesis is challenged.",
        "If LLMs perform better on unfamiliar formats than familiar ones, the theory is contradicted."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs generalize well to novel formats due to underlying reasoning ability rather than format familiarity.",
            "uuids": []
        },
        {
            "text": "Instances where LLMs fail on familiar formats due to ambiguous or underspecified content.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show strong few-shot adaptation to new formats, reducing the impact of pretraining alignment.",
            "uuids": []
        },
        {
            "text": "In some tasks, LLMs perform well on novel formats if the underlying reasoning is simple.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with meta-learning or explicit format adaptation may be less sensitive to presentation alignment.",
        "Highly structured or formulaic tasks (e.g., arithmetic) may be less affected by format novelty.",
        "Domain-specific LLMs may have different alignment effects based on their training data."
    ],
    "existing_theory": {
        "what_already_exists": "The effect of in-distribution vs. out-of-distribution prompts is well-documented in LLM research.",
        "what_is_novel": "The explicit framing of performance as a function of cognitive alignment with pretraining distribution is new.",
        "classification_explanation": "The theory synthesizes known in-distribution effects into a broader, predictive framework of cognitive alignment.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]",
            "Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding [Format alignment and performance]",
            "Perez et al. (2021) True Few-Shot Learning with Language Models [Adaptation to new formats]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-653",
    "original_theory_name": "Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
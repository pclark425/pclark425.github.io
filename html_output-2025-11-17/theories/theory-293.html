<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intrinsic Motivation Scaling Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-293</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-293</p>
                <p><strong>Name:</strong> Intrinsic Motivation Scaling Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of the fundamental trade-off between environment complexity and environment variation in embodied learning systems.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that intrinsic motivation mechanisms in embodied learning systems exhibit fundamentally different scaling behaviors depending on the relationship between environment complexity (C, measured as the size of the state-action space or the difficulty of building accurate predictive models) and environment variation (V, measured as the rate of change in environment dynamics, reward structure, or task distribution). The theory posits that different intrinsic motivation strategies have distinct optimal operating regimes characterized by the complexity-variation relationship. Specifically: (1) Competence-based and mastery-oriented intrinsic motivations (e.g., empowerment, skill discovery) are most effective when complexity is high relative to variation, allowing stable skill development and model building; (2) Novelty-seeking and prediction-error-based intrinsic motivations are most effective when variation is high relative to complexity, as they can rapidly adapt to changing conditions without requiring deep mastery; (3) There exists a transition regime where neither pure strategy is optimal, requiring hybrid or adaptive approaches. The theory further proposes that the effectiveness of intrinsic motivation mechanisms degrades in characteristic ways as the C/V relationship moves outside their optimal regime, and that this degradation is mediated by factors including memory capacity, embodiment constraints, and the temporal structure of environmental changes.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The effectiveness of an intrinsic motivation mechanism M in an embodied learning system can be characterized by its performance across different environment profiles defined by complexity C and variation V, where C represents the difficulty of building accurate models or the size of the explorable state space, and V represents the rate of change in environment properties.</li>
                <li>Competence-based and mastery-oriented intrinsic motivations (empowerment, skill discovery, competence progress) are most effective in environments where complexity is high and variation is low, as these conditions allow for stable skill development and progressive mastery.</li>
                <li>Novelty-seeking and prediction-error-based intrinsic motivations (curiosity, surprise-based exploration) are most effective in environments where variation is high relative to complexity, as they enable rapid adaptation without requiring deep model building.</li>
                <li>There exists a transition regime in the C-V space where neither pure competence-based nor pure novelty-based intrinsic motivation is optimal, and hybrid or adaptive strategies that combine multiple intrinsic motivation signals outperform fixed single-mechanism approaches.</li>
                <li>The degradation in effectiveness of an intrinsic motivation mechanism outside its optimal C-V regime is non-linear: small changes in environment properties near regime boundaries can cause disproportionate changes in learning efficiency.</li>
                <li>Memory capacity and computational resources create an effective upper bound on the complexity that can be leveraged by any intrinsic motivation mechanism, as agents with limited memory cannot maintain the representations needed for competence-based motivation in very high-complexity environments.</li>
                <li>The temporal structure of environment variation (whether changes are random, periodic, or structured) modulates the effectiveness of different intrinsic motivation types, with structured variation favoring model-based approaches and random variation favoring reactive novelty-seeking.</li>
                <li>Embodiment factors (morphology, sensor modalities, actuator constraints) mediate the relationship between nominal environment complexity/variation and the effective complexity/variation experienced by the agent, creating agent-specific optimal intrinsic motivation profiles.</li>
                <li>In environments where extrinsic rewards are sparse or misaligned with intrinsic motivation signals, the C-V scaling predictions may be amplified (sparse rewards) or disrupted (misaligned rewards).</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Curiosity-driven learning based on prediction error shows diminishing returns in highly complex environments where the state space becomes too large to effectively explore through novelty-seeking alone, suggesting complexity limits for this intrinsic motivation type. </li>
    <li>Empowerment-based intrinsic motivation, which maximizes an agent's control over future states, performs well in complex but relatively stable environments where skill development and long-term planning are possible. </li>
    <li>In highly dynamic environments with unpredictable changes, agents using prediction-error based intrinsic motivation can become distracted by unpredictable but task-irrelevant environmental changes (the 'noisy TV problem'), indicating that high variation can impair certain intrinsic motivation mechanisms. </li>
    <li>Meta-learning approaches that adapt exploration strategies based on environmental properties show improved performance across varying environment types, suggesting that intrinsic motivation mechanisms should be adaptive to the complexity-variation profile of environments. </li>
    <li>World models that learn predictive representations can enable effective learning in environments with variation by building internal models that capture dynamics, suggesting that model-based intrinsic motivation can handle certain types of variation. </li>
    <li>Embodied agents with different morphologies and sensor-actuator configurations experience different effective complexity and variation in the same nominal environment, indicating that embodiment mediates the C/V relationship. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An embodied agent using empowerment-based intrinsic motivation will show superior sample efficiency compared to curiosity-based motivation in a complex manipulation task environment with stable dynamics, while the reverse will be true in a simple environment with frequently changing dynamics.</li>
                <li>An agent that estimates the C/V profile of its environment and selects among multiple intrinsic motivation mechanisms accordingly will outperform fixed-strategy agents across a diverse suite of environments with varying complexity and variation characteristics.</li>
                <li>In environments with periodic or cyclical variation, intrinsic motivation mechanisms that can learn and exploit temporal patterns (model-based approaches) will outperform reactive novelty-seeking approaches, even when instantaneous variation is high.</li>
                <li>The learning curve for competence-based intrinsic motivation will show characteristic degradation (reduced slope, increased variance) when environment variation exceeds a threshold relative to the agent's model-building capacity.</li>
                <li>In multi-task learning scenarios, agents using competence-based intrinsic motivation will show greater positive transfer between related tasks in low-variation settings, while novelty-based agents will show more robust performance across unrelated tasks in high-variation settings.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If embodied learning systems are given the ability to actively control their environment's variation rate (e.g., by choosing when to introduce new tasks or modify dynamics), they will converge to a preferred variation rate that depends on their current competence level and the environment's complexity. The nature of this relationship (linear, sublinear, or threshold-based) would reveal fundamental constraints on intrinsic motivation scaling.</li>
                <li>In multi-agent embodied learning systems, the effective environment variation experienced by each agent may scale non-linearly with the number of agents due to emergent interaction dynamics. If this scaling is super-linear beyond a critical number of agents, it could create a phase transition where intrinsic motivation mechanisms effective for single agents fail catastrophically in multi-agent settings.</li>
                <li>For environments with hierarchical structure (where complexity can be decomposed into levels), intrinsic motivation mechanisms that discover and exploit hierarchy might exhibit fundamentally different scaling laws, potentially achieving sub-linear scaling with nominal complexity. The existence of such mechanisms would suggest that the C/V framework needs extension to account for compressible complexity.</li>
                <li>If the temporal coherence of variation (how predictably the environment changes) is manipulated independently of variation rate, there may exist a critical coherence threshold below which model-based intrinsic motivations fail entirely and above which they dominate. The location and sharpness of this threshold would reveal whether the transition between intrinsic motivation regimes is gradual or abrupt.</li>
                <li>In environments where agents can modify their own embodiment (morphology, sensors, actuators), they may actively shape their effective C/V profile to match their intrinsic motivation mechanism. If agents converge to embodiment-intrinsic motivation pairings that are consistent across different nominal environments, this would suggest fundamental embodiment-motivation synergies.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If competence-based intrinsic motivation performs equally well across all levels of environment variation without showing predicted degradation in high-variation settings, this would contradict the theory's core claim about regime-specific effectiveness.</li>
                <li>If agents show no performance difference when using different intrinsic motivation mechanisms in environments with vastly different C/V profiles, this would suggest that the C/V relationship is not a fundamental determinant of intrinsic motivation effectiveness.</li>
                <li>If hybrid or adaptive intrinsic motivation strategies offer no advantage over fixed strategies in the predicted transition regime, this would invalidate the theory's claim about the existence of a transition zone requiring multiple mechanisms.</li>
                <li>If memory capacity and computational resources can be increased arbitrarily without changing the relative effectiveness of different intrinsic motivation mechanisms across C/V profiles, this would contradict the theory's prediction about resource-mediated complexity bounds.</li>
                <li>If environments can be constructed where increasing complexity and variation proportionally (maintaining a constant relationship) still causes systematic changes in which intrinsic motivation mechanism is most effective, this would suggest that the C-V relationship alone is insufficient to characterize intrinsic motivation scaling.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not fully specify how to operationalize and measure environment complexity and variation in practice, particularly for real-world embodied systems where these quantities may be difficult to estimate or may depend on the agent's current knowledge state. </li>
    <li>The role of social learning, imitation, and communication in multi-agent embodied systems might create alternative pathways that bypass the predicted C/V scaling limitations, particularly in complex environments where demonstration or communication can compress effective complexity. </li>
    <li>The theory does not fully address how partial observability (POMDP settings) interacts with the C/V trade-off, as partial observability can make even simple environments appear complex or highly varying from the agent's perspective. </li>
    <li>The theory does not account for how the reward structure (dense vs. sparse, shaped vs. unshaped) interacts with intrinsic motivation scaling, as different reward structures may amplify or dampen the effects of the C/V relationship. </li>
    <li>The theory does not address developmental or curriculum learning scenarios where C and V change systematically over time, which may create different scaling dynamics than static environments. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Schmidhuber (2010) Formal Theory of Creativity, Fun, and Intrinsic Motivation [Foundational work on intrinsic motivation and prediction-based curiosity, but does not propose a unified scaling theory relating different intrinsic motivation types to environment complexity and variation]</li>
    <li>Oudeyer & Kaplan (2007) What is intrinsic motivation? A typology of computational approaches [Comprehensive taxonomy of intrinsic motivation approaches but does not propose scaling laws or trade-offs based on environment properties]</li>
    <li>Pathak et al. (2017) Curiosity-driven Exploration by Self-supervised Prediction [Addresses curiosity-driven learning and identifies limitations in complex environments, but does not propose a general theory of the complexity-variation trade-off]</li>
    <li>Salge et al. (2014) Empowerment – an Introduction [Discusses empowerment-based intrinsic motivation and its properties, but does not relate it to environment complexity-variation trade-offs]</li>
    <li>Aubret et al. (2019) A survey on intrinsic motivation in reinforcement learning [Comprehensive survey of intrinsic motivation methods but does not propose a unified scaling theory]</li>
    <li>Gupta et al. (2018) Meta-Reinforcement Learning of Structured Exploration Strategies [Shows that adaptive exploration strategies can outperform fixed strategies, supporting the need for environment-dependent intrinsic motivation, but does not propose the specific C/V framework]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Intrinsic Motivation Scaling Theory",
    "theory_description": "This theory proposes that intrinsic motivation mechanisms in embodied learning systems exhibit fundamentally different scaling behaviors depending on the relationship between environment complexity (C, measured as the size of the state-action space or the difficulty of building accurate predictive models) and environment variation (V, measured as the rate of change in environment dynamics, reward structure, or task distribution). The theory posits that different intrinsic motivation strategies have distinct optimal operating regimes characterized by the complexity-variation relationship. Specifically: (1) Competence-based and mastery-oriented intrinsic motivations (e.g., empowerment, skill discovery) are most effective when complexity is high relative to variation, allowing stable skill development and model building; (2) Novelty-seeking and prediction-error-based intrinsic motivations are most effective when variation is high relative to complexity, as they can rapidly adapt to changing conditions without requiring deep mastery; (3) There exists a transition regime where neither pure strategy is optimal, requiring hybrid or adaptive approaches. The theory further proposes that the effectiveness of intrinsic motivation mechanisms degrades in characteristic ways as the C/V relationship moves outside their optimal regime, and that this degradation is mediated by factors including memory capacity, embodiment constraints, and the temporal structure of environmental changes.",
    "supporting_evidence": [
        {
            "text": "Curiosity-driven learning based on prediction error shows diminishing returns in highly complex environments where the state space becomes too large to effectively explore through novelty-seeking alone, suggesting complexity limits for this intrinsic motivation type.",
            "citations": [
                "Burda et al. (2019) Exploration by Random Network Distillation, ICLR",
                "Pathak et al. (2017) Curiosity-driven Exploration by Self-supervised Prediction, ICML"
            ]
        },
        {
            "text": "Empowerment-based intrinsic motivation, which maximizes an agent's control over future states, performs well in complex but relatively stable environments where skill development and long-term planning are possible.",
            "citations": [
                "Salge et al. (2014) Empowerment – an Introduction, Guided Self-Organization",
                "Mohamed & Rezende (2015) Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning, NeurIPS"
            ]
        },
        {
            "text": "In highly dynamic environments with unpredictable changes, agents using prediction-error based intrinsic motivation can become distracted by unpredictable but task-irrelevant environmental changes (the 'noisy TV problem'), indicating that high variation can impair certain intrinsic motivation mechanisms.",
            "citations": [
                "Burda et al. (2019) Exploration by Random Network Distillation, ICLR",
                "Schmidhuber (2010) Formal Theory of Creativity, Fun, and Intrinsic Motivation"
            ]
        },
        {
            "text": "Meta-learning approaches that adapt exploration strategies based on environmental properties show improved performance across varying environment types, suggesting that intrinsic motivation mechanisms should be adaptive to the complexity-variation profile of environments.",
            "citations": [
                "Gupta et al. (2018) Meta-Reinforcement Learning of Structured Exploration Strategies, NeurIPS",
                "Xu et al. (2020) Meta-Gradient Reinforcement Learning with an Objective Discovered Online, NeurIPS"
            ]
        },
        {
            "text": "World models that learn predictive representations can enable effective learning in environments with variation by building internal models that capture dynamics, suggesting that model-based intrinsic motivation can handle certain types of variation.",
            "citations": [
                "Ha & Schmidhuber (2018) World Models, NeurIPS",
                "Hafner et al. (2020) Dream to Control: Learning Behaviors by Latent Imagination, ICLR"
            ]
        },
        {
            "text": "Embodied agents with different morphologies and sensor-actuator configurations experience different effective complexity and variation in the same nominal environment, indicating that embodiment mediates the C/V relationship.",
            "citations": [
                "Pfeifer & Bongard (2006) How the Body Shapes the Way We Think: A New View of Intelligence",
                "Lungarella et al. (2003) Developmental robotics: a survey, Connection Science"
            ]
        }
    ],
    "theory_statements": [
        "The effectiveness of an intrinsic motivation mechanism M in an embodied learning system can be characterized by its performance across different environment profiles defined by complexity C and variation V, where C represents the difficulty of building accurate models or the size of the explorable state space, and V represents the rate of change in environment properties.",
        "Competence-based and mastery-oriented intrinsic motivations (empowerment, skill discovery, competence progress) are most effective in environments where complexity is high and variation is low, as these conditions allow for stable skill development and progressive mastery.",
        "Novelty-seeking and prediction-error-based intrinsic motivations (curiosity, surprise-based exploration) are most effective in environments where variation is high relative to complexity, as they enable rapid adaptation without requiring deep model building.",
        "There exists a transition regime in the C-V space where neither pure competence-based nor pure novelty-based intrinsic motivation is optimal, and hybrid or adaptive strategies that combine multiple intrinsic motivation signals outperform fixed single-mechanism approaches.",
        "The degradation in effectiveness of an intrinsic motivation mechanism outside its optimal C-V regime is non-linear: small changes in environment properties near regime boundaries can cause disproportionate changes in learning efficiency.",
        "Memory capacity and computational resources create an effective upper bound on the complexity that can be leveraged by any intrinsic motivation mechanism, as agents with limited memory cannot maintain the representations needed for competence-based motivation in very high-complexity environments.",
        "The temporal structure of environment variation (whether changes are random, periodic, or structured) modulates the effectiveness of different intrinsic motivation types, with structured variation favoring model-based approaches and random variation favoring reactive novelty-seeking.",
        "Embodiment factors (morphology, sensor modalities, actuator constraints) mediate the relationship between nominal environment complexity/variation and the effective complexity/variation experienced by the agent, creating agent-specific optimal intrinsic motivation profiles.",
        "In environments where extrinsic rewards are sparse or misaligned with intrinsic motivation signals, the C-V scaling predictions may be amplified (sparse rewards) or disrupted (misaligned rewards)."
    ],
    "new_predictions_likely": [
        "An embodied agent using empowerment-based intrinsic motivation will show superior sample efficiency compared to curiosity-based motivation in a complex manipulation task environment with stable dynamics, while the reverse will be true in a simple environment with frequently changing dynamics.",
        "An agent that estimates the C/V profile of its environment and selects among multiple intrinsic motivation mechanisms accordingly will outperform fixed-strategy agents across a diverse suite of environments with varying complexity and variation characteristics.",
        "In environments with periodic or cyclical variation, intrinsic motivation mechanisms that can learn and exploit temporal patterns (model-based approaches) will outperform reactive novelty-seeking approaches, even when instantaneous variation is high.",
        "The learning curve for competence-based intrinsic motivation will show characteristic degradation (reduced slope, increased variance) when environment variation exceeds a threshold relative to the agent's model-building capacity.",
        "In multi-task learning scenarios, agents using competence-based intrinsic motivation will show greater positive transfer between related tasks in low-variation settings, while novelty-based agents will show more robust performance across unrelated tasks in high-variation settings."
    ],
    "new_predictions_unknown": [
        "If embodied learning systems are given the ability to actively control their environment's variation rate (e.g., by choosing when to introduce new tasks or modify dynamics), they will converge to a preferred variation rate that depends on their current competence level and the environment's complexity. The nature of this relationship (linear, sublinear, or threshold-based) would reveal fundamental constraints on intrinsic motivation scaling.",
        "In multi-agent embodied learning systems, the effective environment variation experienced by each agent may scale non-linearly with the number of agents due to emergent interaction dynamics. If this scaling is super-linear beyond a critical number of agents, it could create a phase transition where intrinsic motivation mechanisms effective for single agents fail catastrophically in multi-agent settings.",
        "For environments with hierarchical structure (where complexity can be decomposed into levels), intrinsic motivation mechanisms that discover and exploit hierarchy might exhibit fundamentally different scaling laws, potentially achieving sub-linear scaling with nominal complexity. The existence of such mechanisms would suggest that the C/V framework needs extension to account for compressible complexity.",
        "If the temporal coherence of variation (how predictably the environment changes) is manipulated independently of variation rate, there may exist a critical coherence threshold below which model-based intrinsic motivations fail entirely and above which they dominate. The location and sharpness of this threshold would reveal whether the transition between intrinsic motivation regimes is gradual or abrupt.",
        "In environments where agents can modify their own embodiment (morphology, sensors, actuators), they may actively shape their effective C/V profile to match their intrinsic motivation mechanism. If agents converge to embodiment-intrinsic motivation pairings that are consistent across different nominal environments, this would suggest fundamental embodiment-motivation synergies."
    ],
    "negative_experiments": [
        "If competence-based intrinsic motivation performs equally well across all levels of environment variation without showing predicted degradation in high-variation settings, this would contradict the theory's core claim about regime-specific effectiveness.",
        "If agents show no performance difference when using different intrinsic motivation mechanisms in environments with vastly different C/V profiles, this would suggest that the C/V relationship is not a fundamental determinant of intrinsic motivation effectiveness.",
        "If hybrid or adaptive intrinsic motivation strategies offer no advantage over fixed strategies in the predicted transition regime, this would invalidate the theory's claim about the existence of a transition zone requiring multiple mechanisms.",
        "If memory capacity and computational resources can be increased arbitrarily without changing the relative effectiveness of different intrinsic motivation mechanisms across C/V profiles, this would contradict the theory's prediction about resource-mediated complexity bounds.",
        "If environments can be constructed where increasing complexity and variation proportionally (maintaining a constant relationship) still causes systematic changes in which intrinsic motivation mechanism is most effective, this would suggest that the C-V relationship alone is insufficient to characterize intrinsic motivation scaling."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not fully specify how to operationalize and measure environment complexity and variation in practice, particularly for real-world embodied systems where these quantities may be difficult to estimate or may depend on the agent's current knowledge state.",
            "citations": []
        },
        {
            "text": "The role of social learning, imitation, and communication in multi-agent embodied systems might create alternative pathways that bypass the predicted C/V scaling limitations, particularly in complex environments where demonstration or communication can compress effective complexity.",
            "citations": [
                "Schaal (1999) Is imitation learning the route to humanoid robots?, Trends in Cognitive Sciences",
                "Dautenhahn & Nehaniv (2002) Imitation in Animals and Artifacts"
            ]
        },
        {
            "text": "The theory does not fully address how partial observability (POMDP settings) interacts with the C/V trade-off, as partial observability can make even simple environments appear complex or highly varying from the agent's perspective.",
            "citations": [
                "Kaelbling et al. (1998) Planning and acting in partially observable stochastic domains, Artificial Intelligence"
            ]
        },
        {
            "text": "The theory does not account for how the reward structure (dense vs. sparse, shaped vs. unshaped) interacts with intrinsic motivation scaling, as different reward structures may amplify or dampen the effects of the C/V relationship.",
            "citations": []
        },
        {
            "text": "The theory does not address developmental or curriculum learning scenarios where C and V change systematically over time, which may create different scaling dynamics than static environments.",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that simple random exploration can be surprisingly effective even in complex environments, which might suggest that sophisticated intrinsic motivation mechanisms are not always necessary for handling high complexity.",
            "citations": [
                "Rajeswaran et al. (2017) EPOpt: Learning Robust Neural Network Policies Using Model Ensembles, ICLR",
                "Mania et al. (2018) Simple random search provides a competitive approach to reinforcement learning, NeurIPS"
            ]
        }
    ],
    "special_cases": [
        "In environments with periodic or cyclical variation, the effective variation rate may be much lower than the instantaneous variation rate if the agent can learn and predict the temporal patterns, shifting the environment's effective position in C-V space.",
        "For environments with sparse rewards that happen to align with intrinsic motivation signals, the C/V scaling predictions may be amplified as both intrinsic and extrinsic objectives reinforce the same behaviors.",
        "For environments with rewards that conflict with intrinsic motivation signals (e.g., requiring the agent to avoid novelty), the predictions may be dampened or reversed as the agent must balance competing objectives.",
        "In the limit of very low complexity (fully explorable state spaces), all intrinsic motivation mechanisms may converge to similar performance as the environment becomes exhaustively explorable regardless of strategy.",
        "For embodied systems with significant action delays, long-horizon consequences, or slow dynamics, the effective variation rate may be lower than the environmental variation rate, as the agent's actions and observations are temporally smoothed.",
        "In continual learning or multi-task settings, transfer learning effects may reduce the effective complexity of new tasks that share structure with previously learned tasks, shifting the effective C/V profile.",
        "For environments where complexity is hierarchically structured and decomposable, agents that discover this structure may experience much lower effective complexity than the nominal complexity suggests.",
        "In multi-agent settings, the effective variation experienced by each agent may be amplified by the non-stationarity introduced by other learning agents, potentially shifting all agents into high-variation regimes regardless of environmental variation."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Schmidhuber (2010) Formal Theory of Creativity, Fun, and Intrinsic Motivation [Foundational work on intrinsic motivation and prediction-based curiosity, but does not propose a unified scaling theory relating different intrinsic motivation types to environment complexity and variation]",
            "Oudeyer & Kaplan (2007) What is intrinsic motivation? A typology of computational approaches [Comprehensive taxonomy of intrinsic motivation approaches but does not propose scaling laws or trade-offs based on environment properties]",
            "Pathak et al. (2017) Curiosity-driven Exploration by Self-supervised Prediction [Addresses curiosity-driven learning and identifies limitations in complex environments, but does not propose a general theory of the complexity-variation trade-off]",
            "Salge et al. (2014) Empowerment – an Introduction [Discusses empowerment-based intrinsic motivation and its properties, but does not relate it to environment complexity-variation trade-offs]",
            "Aubret et al. (2019) A survey on intrinsic motivation in reinforcement learning [Comprehensive survey of intrinsic motivation methods but does not propose a unified scaling theory]",
            "Gupta et al. (2018) Meta-Reinforcement Learning of Structured Exploration Strategies [Shows that adaptive exploration strategies can outperform fixed strategies, supporting the need for environment-dependent intrinsic motivation, but does not propose the specific C/V framework]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory of the fundamental trade-off between environment complexity and environment variation in embodied learning systems.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-132",
    "original_theory_name": "Intrinsic Motivation Scaling Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
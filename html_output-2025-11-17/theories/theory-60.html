<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Length Generalization Failure Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-60</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-60</p>
                <p><strong>Name:</strong> Length Generalization Failure Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> Language models systematically fail to generalize arithmetic algorithms to longer inputs than seen during training due to multiple interacting factors: (1) positional encoding distribution shift (test positions outside training range create out-of-distribution representations), (2) attention pattern brittleness (learned patterns don't transfer to longer sequences, causing information routing failures), (3) distractor accumulation (more irrelevant tokens in longer sequences disrupt attention mechanisms), (4) compounding per-step errors (longer sequences have more steps where errors can propagate), and (5) tokenization misalignment (multi-digit tokens create irregular position-to-digit mappings that don't generalize). Standard positional encodings (absolute, sinusoidal, RoPE) create OOD representations for unseen lengths. Interventions that improve length generalization include: randomized positional encodings (exposing models to diverse position values), relative positional encodings (position-independent representations), format alignment (reversed digits reduce dependency length, padding standardizes positions), fine-grained tokenization (single-digit tokens enable consistent digit-wise learning), curriculum learning (progressive exposure to longer sequences), and training-set priming (small number of longer examples during training). However, even with optimal interventions, perfect length generalization remains challenging. Alternative approaches like Recursion of Thought (breaking problems into multiple contexts) can bypass length limits entirely by decomposing long problems into shorter subproblems.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models fail to generalize arithmetic algorithms to longer inputs primarily due to positional encoding distribution shift creating OOD representations.</li>
                <li>Standard absolute positional encodings (sinusoidal, learned, RoPE) create out-of-distribution representations for positions beyond training range.</li>
                <li>Attention patterns learned on short sequences do not reliably transfer to longer sequences, causing information routing failures.</li>
                <li>Distractor tokens (irrelevant context) accumulate in longer sequences and disrupt attention patterns, preventing correct information flow.</li>
                <li>Per-step error rates remain approximately constant across sequence lengths, causing exponential accuracy degradation as length increases.</li>
                <li>Relative positional encodings (NoPE, T5 bias) generalize better than absolute encodings because they provide position-independent representations.</li>
                <li>Randomized positional encodings improve generalization by exposing models to diverse position values during training, preventing OOD position representations.</li>
                <li>Format interventions (reversed digits, padding) reduce effective dependency length and standardize positional patterns, improving generalization.</li>
                <li>Fine-grained tokenization (single-digit tokens) enables consistent digit-wise learning and better length generalization than multi-digit tokenization.</li>
                <li>Curriculum learning (progressive exposure to longer sequences) improves length generalization by gradually building up the model's capacity.</li>
                <li>Training-set priming (small number of longer examples during training) is more sample-efficient than fine-tuning and avoids catastrophic forgetting.</li>
                <li>Pretraining format consistency with fine-tuning format is important for effective length generalization.</li>
                <li>Even with optimal interventions, perfect length generalization remains difficult, suggesting fundamental architectural limitations in how transformers represent and process sequential dependencies.</li>
                <li>Alternative approaches like Recursion of Thought can bypass length limits by decomposing long problems into multiple shorter contexts.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Baseline positional encodings show poor length generalization: binary addition accuracy ~50% on unseen lengths 41-500. <a href="../results/extraction-result-296.html#e296.0" class="evidence-link">[e296.0]</a> </li>
    <li>Randomized positional encodings improve length generalization: binary addition 64.4% vs 49.8% baseline. <a href="../results/extraction-result-296.html#e296.0" class="evidence-link">[e296.0]</a> </li>
    <li>NoPE and T5 relative bias generalize best to longer sequences across multiple arithmetic tasks. <a href="../results/extraction-result-304.html#e304.1" class="evidence-link">[e304.1]</a> </li>
    <li>Scratchpad finetuning fails to generalize to longer OOD lengths, showing similar pathologies as vanilla finetuning. <a href="../results/extraction-result-327.html#e327.2" class="evidence-link">[e327.2]</a> </li>
    <li>Distractor tokens are primary cause of length generalization failure; masking distractors improves performance. <a href="../results/extraction-result-327.html#e327.4" class="evidence-link">[e327.4]</a> </li>
    <li>Per-step error rates remain constant, leading to compounded failures on longer instances. <a href="../results/extraction-result-327.html#e327.2" class="evidence-link">[e327.2]</a> </li>
    <li>Attention patterns (e.g., 'X' reversal pattern for addition) break down for longer sequences with baseline encodings but are preserved with randomized encodings. <a href="../results/extraction-result-296.html#e296.0" class="evidence-link">[e296.0]</a> </li>
    <li>Training-set priming (including small number of longer examples) enables length generalization with ~20x fewer examples than fine-tuning and avoids catastrophic forgetting. <a href="../results/extraction-result-306.html#e306.4" class="evidence-link">[e306.4]</a> </li>
    <li>Format alignment (reversed digits with padding) improves length generalization: GPT2-small achieves essentially perfect accuracy up to 12×12 digit multiplication with padding+reverse. <a href="../results/extraction-result-262.html#e262.0" class="evidence-link">[e262.0]</a> </li>
    <li>Few-shot scratchpad prompting shows better length generalization than finetuning, suggesting template-based generation helps. <a href="../results/extraction-result-327.html#e327.4" class="evidence-link">[e327.4]</a> </li>
    <li>One-digit tokenization yields best in-domain and out-of-domain performance for length generalization; three-digit tokenizer performs poorly for sub-billion models. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
    <li>Base-10 tokenization shows better data efficiency and length generalization tendencies than base-100/base-1000 for multiplication. <a href="../results/extraction-result-255.html#e255.1" class="evidence-link">[e255.1]</a> </li>
    <li>Curriculum learning (progressively harder examples) helps models learn arithmetic and improves generalization to larger digit ranges. <a href="../results/extraction-result-265.html#e265.2" class="evidence-link">[e265.2]</a> </li>
    <li>Pretraining format consistency matters: format mismatch between pretraining and fine-tuning can degrade length generalization effectiveness. <a href="../results/extraction-result-329.html#e329.6" class="evidence-link">[e329.6]</a> </li>
    <li>n×1 augmentation (exposing model to many multi-digit × single-digit examples) helps bridge to full n×m multiplications by teaching decomposition. <a href="../results/extraction-result-262.html#e262.5" class="evidence-link">[e262.5]</a> </li>
    <li>Scratchpad format effects: NoPE and T5 Relative benefit most from scratchpad via attention patterns that mix short- and long-range dependencies. <a href="../results/extraction-result-304.html#e304.3" class="evidence-link">[e304.3]</a> </li>
    <li>Fine-tuning on target lengths requires ~1000 examples and causes catastrophic forgetting, while priming needs ~20x fewer examples and preserves in-domain performance. <a href="../results/extraction-result-306.html#e306.4" class="evidence-link">[e306.4]</a> </li>
    <li>Intermediate-layer activations become OOD for longer sequences with baseline encodings, but randomized encodings maintain overlapping activation distributions. <a href="../results/extraction-result-296.html#e296.0" class="evidence-link">[e296.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Models trained with randomized positional encodings should show better length generalization than models with standard encodings across all arithmetic operations, with larger improvements for operations requiring longer-range dependencies.</li>
                <li>Combining multiple interventions (randomized encodings + single-digit tokenization + format alignment + curriculum learning) should produce better length generalization than any single intervention, with effects being roughly additive.</li>
                <li>The length generalization gap should be larger for operations with longer dependency chains (multiplication > addition > parity for local operations).</li>
                <li>Models should show better length generalization on problems where intermediate steps are provided (scratchpad) than when generating all steps, but only if distractors are masked or minimized.</li>
                <li>Training-set priming should be more effective than fine-tuning for length generalization across all arithmetic operations, requiring ~20x fewer examples and avoiding catastrophic forgetting.</li>
                <li>Models with single-digit tokenization should show better length generalization than models with multi-digit tokenization, especially for sub-billion parameter models.</li>
                <li>Curriculum learning combined with format alignment should enable better length generalization than either intervention alone.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a positional encoding scheme that enables perfect length generalization without any other interventions is unclear.</li>
                <li>It's unknown whether architectural modifications (e.g., different attention mechanisms, state-space models) could fundamentally solve length generalization without requiring format or tokenization changes.</li>
                <li>Whether length generalization ability transfers across different types of sequential reasoning tasks (arithmetic to logical reasoning to program synthesis) is uncertain.</li>
                <li>The extent to which length generalization can be improved through meta-learning (training on distributions of sequence lengths) is unknown.</li>
                <li>Whether combining Recursion of Thought with improved positional encodings could enable both long-problem solving and better single-context length generalization is unclear.</li>
                <li>The optimal balance between tokenization granularity, model size, and training data for length generalization is not well characterized.</li>
                <li>Whether length generalization improvements from different interventions (positional encoding, tokenization, format) are independent or interact in complex ways is not fully understood.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that positional encoding choice has no effect on length generalization when controlling for other factors would challenge the distribution shift explanation.</li>
                <li>Discovering that models with perfect in-distribution attention patterns still fail on longer sequences would challenge the attention brittleness claim.</li>
                <li>Observing that removing distractors does not improve length generalization would challenge the distractor accumulation claim.</li>
                <li>Finding that per-step error rates increase with sequence length (rather than remaining constant) would challenge the constant-error-rate assumption and suggest a different failure mechanism.</li>
                <li>Discovering that single-digit tokenization does not improve length generalization would challenge the tokenization misalignment explanation.</li>
                <li>Finding that curriculum learning provides no benefit over random sampling of lengths would challenge the progressive learning hypothesis.</li>
                <li>Observing that format alignment (reversed digits, padding) does not improve length generalization would challenge the dependency-length reduction explanation.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Why some operations (e.g., parity) show better length generalization than others (e.g., multiplication) is not fully explained by the theory - the theory identifies dependency length but doesn't quantify the relationship. <a href="../results/extraction-result-296.html#e296.0" class="evidence-link">[e296.0]</a> <a href="../results/extraction-result-296.html#e296.1" class="evidence-link">[e296.1]</a> </li>
    <li>The interaction between positional encoding and other architectural choices (attention mechanism, layer depth, model width) is not characterized. <a href="../results/extraction-result-304.html#e304.1" class="evidence-link">[e304.1]</a> </li>
    <li>How to predict length generalization capability from training performance is not specified - no clear metrics or indicators are provided. <a href="../results/extraction-result-296.html#e296.0" class="evidence-link">[e296.0]</a> </li>
    <li>The exact mechanism by which curriculum learning improves length generalization is not fully explained - whether it's about gradual capacity building, better optimization, or something else. <a href="../results/extraction-result-265.html#e265.2" class="evidence-link">[e265.2]</a> </li>
    <li>Why pretraining format consistency matters so much for length generalization is not mechanistically explained. <a href="../results/extraction-result-329.html#e329.6" class="evidence-link">[e329.6]</a> </li>
    <li>The optimal curriculum schedule (how quickly to increase length, what distribution of lengths) is not specified. <a href="../results/extraction-result-265.html#e265.2" class="evidence-link">[e265.2]</a> </li>
    <li>How model size interacts with length generalization interventions is not fully characterized - whether larger models benefit more or less from these interventions. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> <a href="../results/extraction-result-255.html#e255.1" class="evidence-link">[e255.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Jelassi et al. (2023) Length Generalization in Arithmetic Transformers [Directly studies length generalization in arithmetic and proposes training-set priming intervention]</li>
    <li>Anil et al. (2022) Exploring Length Generalization in Large Language Models [Studies length generalization failures in LLMs, identifies distractor accumulation and attention pattern brittleness]</li>
    <li>Ruoss et al. (2023) Randomized Positional Encodings Boost Length Generalization of Transformers [Proposes randomized positional encodings specifically for length generalization]</li>
    <li>Lee et al. (2023) The Impact of Positional Encoding on Length Generalization in Transformers [Comprehensive study of positional encoding effects on length generalization]</li>
    <li>McLeish et al. (2024) Positional Description Matters for Transformers Arithmetic [Shows format alignment (padding, reversal) improves length generalization]</li>
    <li>Hao et al. (2024) Number Cookbook [Studies tokenization effects on arithmetic including length generalization]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Length Generalization Failure Theory",
    "theory_description": "Language models systematically fail to generalize arithmetic algorithms to longer inputs than seen during training due to multiple interacting factors: (1) positional encoding distribution shift (test positions outside training range create out-of-distribution representations), (2) attention pattern brittleness (learned patterns don't transfer to longer sequences, causing information routing failures), (3) distractor accumulation (more irrelevant tokens in longer sequences disrupt attention mechanisms), (4) compounding per-step errors (longer sequences have more steps where errors can propagate), and (5) tokenization misalignment (multi-digit tokens create irregular position-to-digit mappings that don't generalize). Standard positional encodings (absolute, sinusoidal, RoPE) create OOD representations for unseen lengths. Interventions that improve length generalization include: randomized positional encodings (exposing models to diverse position values), relative positional encodings (position-independent representations), format alignment (reversed digits reduce dependency length, padding standardizes positions), fine-grained tokenization (single-digit tokens enable consistent digit-wise learning), curriculum learning (progressive exposure to longer sequences), and training-set priming (small number of longer examples during training). However, even with optimal interventions, perfect length generalization remains challenging. Alternative approaches like Recursion of Thought (breaking problems into multiple contexts) can bypass length limits entirely by decomposing long problems into shorter subproblems.",
    "supporting_evidence": [
        {
            "text": "Baseline positional encodings show poor length generalization: binary addition accuracy ~50% on unseen lengths 41-500.",
            "uuids": [
                "e296.0"
            ]
        },
        {
            "text": "Randomized positional encodings improve length generalization: binary addition 64.4% vs 49.8% baseline.",
            "uuids": [
                "e296.0"
            ]
        },
        {
            "text": "NoPE and T5 relative bias generalize best to longer sequences across multiple arithmetic tasks.",
            "uuids": [
                "e304.1"
            ]
        },
        {
            "text": "Scratchpad finetuning fails to generalize to longer OOD lengths, showing similar pathologies as vanilla finetuning.",
            "uuids": [
                "e327.2"
            ]
        },
        {
            "text": "Distractor tokens are primary cause of length generalization failure; masking distractors improves performance.",
            "uuids": [
                "e327.4"
            ]
        },
        {
            "text": "Per-step error rates remain constant, leading to compounded failures on longer instances.",
            "uuids": [
                "e327.2"
            ]
        },
        {
            "text": "Attention patterns (e.g., 'X' reversal pattern for addition) break down for longer sequences with baseline encodings but are preserved with randomized encodings.",
            "uuids": [
                "e296.0"
            ]
        },
        {
            "text": "Training-set priming (including small number of longer examples) enables length generalization with ~20x fewer examples than fine-tuning and avoids catastrophic forgetting.",
            "uuids": [
                "e306.4"
            ]
        },
        {
            "text": "Format alignment (reversed digits with padding) improves length generalization: GPT2-small achieves essentially perfect accuracy up to 12×12 digit multiplication with padding+reverse.",
            "uuids": [
                "e262.0"
            ]
        },
        {
            "text": "Few-shot scratchpad prompting shows better length generalization than finetuning, suggesting template-based generation helps.",
            "uuids": [
                "e327.4"
            ]
        },
        {
            "text": "One-digit tokenization yields best in-domain and out-of-domain performance for length generalization; three-digit tokenizer performs poorly for sub-billion models.",
            "uuids": [
                "e278.5"
            ]
        },
        {
            "text": "Base-10 tokenization shows better data efficiency and length generalization tendencies than base-100/base-1000 for multiplication.",
            "uuids": [
                "e255.1"
            ]
        },
        {
            "text": "Curriculum learning (progressively harder examples) helps models learn arithmetic and improves generalization to larger digit ranges.",
            "uuids": [
                "e265.2"
            ]
        },
        {
            "text": "Pretraining format consistency matters: format mismatch between pretraining and fine-tuning can degrade length generalization effectiveness.",
            "uuids": [
                "e329.6"
            ]
        },
        {
            "text": "n×1 augmentation (exposing model to many multi-digit × single-digit examples) helps bridge to full n×m multiplications by teaching decomposition.",
            "uuids": [
                "e262.5"
            ]
        },
        {
            "text": "Scratchpad format effects: NoPE and T5 Relative benefit most from scratchpad via attention patterns that mix short- and long-range dependencies.",
            "uuids": [
                "e304.3"
            ]
        },
        {
            "text": "Fine-tuning on target lengths requires ~1000 examples and causes catastrophic forgetting, while priming needs ~20x fewer examples and preserves in-domain performance.",
            "uuids": [
                "e306.4"
            ]
        },
        {
            "text": "Intermediate-layer activations become OOD for longer sequences with baseline encodings, but randomized encodings maintain overlapping activation distributions.",
            "uuids": [
                "e296.0"
            ]
        }
    ],
    "theory_statements": [
        "Language models fail to generalize arithmetic algorithms to longer inputs primarily due to positional encoding distribution shift creating OOD representations.",
        "Standard absolute positional encodings (sinusoidal, learned, RoPE) create out-of-distribution representations for positions beyond training range.",
        "Attention patterns learned on short sequences do not reliably transfer to longer sequences, causing information routing failures.",
        "Distractor tokens (irrelevant context) accumulate in longer sequences and disrupt attention patterns, preventing correct information flow.",
        "Per-step error rates remain approximately constant across sequence lengths, causing exponential accuracy degradation as length increases.",
        "Relative positional encodings (NoPE, T5 bias) generalize better than absolute encodings because they provide position-independent representations.",
        "Randomized positional encodings improve generalization by exposing models to diverse position values during training, preventing OOD position representations.",
        "Format interventions (reversed digits, padding) reduce effective dependency length and standardize positional patterns, improving generalization.",
        "Fine-grained tokenization (single-digit tokens) enables consistent digit-wise learning and better length generalization than multi-digit tokenization.",
        "Curriculum learning (progressive exposure to longer sequences) improves length generalization by gradually building up the model's capacity.",
        "Training-set priming (small number of longer examples during training) is more sample-efficient than fine-tuning and avoids catastrophic forgetting.",
        "Pretraining format consistency with fine-tuning format is important for effective length generalization.",
        "Even with optimal interventions, perfect length generalization remains difficult, suggesting fundamental architectural limitations in how transformers represent and process sequential dependencies.",
        "Alternative approaches like Recursion of Thought can bypass length limits by decomposing long problems into multiple shorter contexts."
    ],
    "new_predictions_likely": [
        "Models trained with randomized positional encodings should show better length generalization than models with standard encodings across all arithmetic operations, with larger improvements for operations requiring longer-range dependencies.",
        "Combining multiple interventions (randomized encodings + single-digit tokenization + format alignment + curriculum learning) should produce better length generalization than any single intervention, with effects being roughly additive.",
        "The length generalization gap should be larger for operations with longer dependency chains (multiplication &gt; addition &gt; parity for local operations).",
        "Models should show better length generalization on problems where intermediate steps are provided (scratchpad) than when generating all steps, but only if distractors are masked or minimized.",
        "Training-set priming should be more effective than fine-tuning for length generalization across all arithmetic operations, requiring ~20x fewer examples and avoiding catastrophic forgetting.",
        "Models with single-digit tokenization should show better length generalization than models with multi-digit tokenization, especially for sub-billion parameter models.",
        "Curriculum learning combined with format alignment should enable better length generalization than either intervention alone."
    ],
    "new_predictions_unknown": [
        "Whether there exists a positional encoding scheme that enables perfect length generalization without any other interventions is unclear.",
        "It's unknown whether architectural modifications (e.g., different attention mechanisms, state-space models) could fundamentally solve length generalization without requiring format or tokenization changes.",
        "Whether length generalization ability transfers across different types of sequential reasoning tasks (arithmetic to logical reasoning to program synthesis) is uncertain.",
        "The extent to which length generalization can be improved through meta-learning (training on distributions of sequence lengths) is unknown.",
        "Whether combining Recursion of Thought with improved positional encodings could enable both long-problem solving and better single-context length generalization is unclear.",
        "The optimal balance between tokenization granularity, model size, and training data for length generalization is not well characterized.",
        "Whether length generalization improvements from different interventions (positional encoding, tokenization, format) are independent or interact in complex ways is not fully understood."
    ],
    "negative_experiments": [
        "Finding that positional encoding choice has no effect on length generalization when controlling for other factors would challenge the distribution shift explanation.",
        "Discovering that models with perfect in-distribution attention patterns still fail on longer sequences would challenge the attention brittleness claim.",
        "Observing that removing distractors does not improve length generalization would challenge the distractor accumulation claim.",
        "Finding that per-step error rates increase with sequence length (rather than remaining constant) would challenge the constant-error-rate assumption and suggest a different failure mechanism.",
        "Discovering that single-digit tokenization does not improve length generalization would challenge the tokenization misalignment explanation.",
        "Finding that curriculum learning provides no benefit over random sampling of lengths would challenge the progressive learning hypothesis.",
        "Observing that format alignment (reversed digits, padding) does not improve length generalization would challenge the dependency-length reduction explanation."
    ],
    "unaccounted_for": [
        {
            "text": "Why some operations (e.g., parity) show better length generalization than others (e.g., multiplication) is not fully explained by the theory - the theory identifies dependency length but doesn't quantify the relationship.",
            "uuids": [
                "e296.0",
                "e296.1"
            ]
        },
        {
            "text": "The interaction between positional encoding and other architectural choices (attention mechanism, layer depth, model width) is not characterized.",
            "uuids": [
                "e304.1"
            ]
        },
        {
            "text": "How to predict length generalization capability from training performance is not specified - no clear metrics or indicators are provided.",
            "uuids": [
                "e296.0"
            ]
        },
        {
            "text": "The exact mechanism by which curriculum learning improves length generalization is not fully explained - whether it's about gradual capacity building, better optimization, or something else.",
            "uuids": [
                "e265.2"
            ]
        },
        {
            "text": "Why pretraining format consistency matters so much for length generalization is not mechanistically explained.",
            "uuids": [
                "e329.6"
            ]
        },
        {
            "text": "The optimal curriculum schedule (how quickly to increase length, what distribution of lengths) is not specified.",
            "uuids": [
                "e265.2"
            ]
        },
        {
            "text": "How model size interacts with length generalization interventions is not fully characterized - whether larger models benefit more or less from these interventions.",
            "uuids": [
                "e278.5",
                "e255.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models show reasonable length generalization with appropriate training (e.g., training-set priming), suggesting the problem may be more about training strategy than fundamental architecture.",
            "uuids": [
                "e306.4"
            ]
        },
        {
            "text": "Few-shot prompting sometimes shows better length generalization than finetuning, suggesting the failure may be in the learning/optimization process rather than the architecture itself.",
            "uuids": [
                "e327.4"
            ]
        },
        {
            "text": "Recursion of Thought enables scaling to very large problems (64-digit addition, 32-digit multiplication) by breaking into multiple contexts, suggesting length limits can be completely bypassed rather than incrementally improved.",
            "uuids": [
                "e292.1",
                "e292.2"
            ]
        },
        {
            "text": "Small models (536K parameters) trained with RoT can learn complex arithmetic at large scales, suggesting that the length generalization problem may be more about training methodology than model capacity or architecture.",
            "uuids": [
                "e292.2"
            ]
        },
        {
            "text": "Pretraining format consistency can matter more than scale for some aspects of length generalization, suggesting data/training factors may be more important than architectural ones.",
            "uuids": [
                "e329.6"
            ]
        }
    ],
    "special_cases": [
        "Length generalization is easier for operations with local dependencies (addition with carries) than global dependencies (parity over all bits).",
        "Relative positional encodings may not help for tasks that require absolute position information (e.g., 'get the 5th element').",
        "Very large models may show better length generalization due to increased capacity, though the effect is limited and doesn't solve the problem completely.",
        "Length generalization may be easier when using external computation (program generation) since the model doesn't need to maintain long-range dependencies internally.",
        "Single-digit tokenization benefits are more pronounced for smaller models (sub-billion parameters) than larger models.",
        "Curriculum learning may be more important for complex operations (multiplication) than simple operations (addition).",
        "Format alignment (reversed digits) is particularly effective for operations with carry/borrow dependencies but may not help operations without such dependencies.",
        "Training-set priming is more effective than fine-tuning specifically because it avoids catastrophic forgetting while enabling length generalization.",
        "Recursion of Thought completely bypasses length limits but requires training on recursive decomposition and may not improve single-context length generalization."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Jelassi et al. (2023) Length Generalization in Arithmetic Transformers [Directly studies length generalization in arithmetic and proposes training-set priming intervention]",
            "Anil et al. (2022) Exploring Length Generalization in Large Language Models [Studies length generalization failures in LLMs, identifies distractor accumulation and attention pattern brittleness]",
            "Ruoss et al. (2023) Randomized Positional Encodings Boost Length Generalization of Transformers [Proposes randomized positional encodings specifically for length generalization]",
            "Lee et al. (2023) The Impact of Positional Encoding on Length Generalization in Transformers [Comprehensive study of positional encoding effects on length generalization]",
            "McLeish et al. (2024) Positional Description Matters for Transformers Arithmetic [Shows format alignment (padding, reversal) improves length generalization]",
            "Hao et al. (2024) Number Cookbook [Studies tokenization effects on arithmetic including length generalization]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 6,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-30 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-30</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-30</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-320.html">theory-320</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-392.html">theory-392</a></p>
                <p><strong>Overall Support or Contradict:</strong> support</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The evidence strongly supports the theory's core mechanisms—training distribution bias, systematic proxy failures for novel work, field-specific differences, and correctability through meta-learning—with multiple studies showing near-zero correlations between traditional proxies and novelty, substantial cross-domain degradation, and measurable improvements from correction mechanisms. However, the evidence reveals important complexities (massive evaluator variance, bidirectional biases, expertise effects) that suggest the theory's specific functional forms and scope need refinement rather than fundamental revision.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>Citation-based proxies show near-zero correlation (Pearson r=0.0132) with peer-review novelty scores, while text-based models achieve moderate correlation (r~0.33), demonstrating systematic failure of traditional citation proxies for novelty assessment and supporting the theory's claim that citation-based proxies systematically undervalue transformational work. <a href="../results/extraction-result-2189.html#e2189.0" class="evidence-link">[e2189.0]</a> <a href="../results/extraction-result-2189.html#e2189.1" class="evidence-link">[e2189.1]</a> </li>
    <li>LLMs without integrated literature search achieve random performance (AUROC ~0.5) on novelty detection, confirming that automated systems relying solely on parametric knowledge systematically fail to identify novel work, supporting the theory's training distribution bias mechanism. <a href="../results/extraction-result-2190.html#e2190.0" class="evidence-link">[e2190.0]</a> </li>
    <li>Historical dissimilarity metrics show sharp cross-domain performance degradation (AUROC drop of ~0.40-0.50 from single-domain ~0.75-0.85 to cross-domain ~0.36-0.40), demonstrating that proxy metrics calibrated on one distribution fail when applied to different contexts, supporting the theory's claim about training distribution bias. <a href="../results/extraction-result-2190.html#e2190.1" class="evidence-link">[e2190.1]</a> <a href="../results/extraction-result-2190.html#e2190.2" class="evidence-link">[e2190.2]</a> </li>
    <li>Peer review exhibits 35-37% disagreement on novelty judgments in ICLR samples and 23% disagreement in NeurIPS experiments, confirming systematic inconsistency in evaluation proxies and supporting the theory's claim that standard evaluation proxies are unreliable for transformational work. <a href="../results/extraction-result-2191.html#e2191.0" class="evidence-link">[e2191.0]</a> </li>
    <li>Multiple studies document bias against novelty: reviewers systematically penalize novel proposals even when quality is equal (Boudreau et al.), 24 Nobel-winning papers were initially rejected, and highly novel work tends to appear in lower-impact-factor journals, all supporting the theory's prediction that proxies systematically undervalue transformational work. <a href="../results/extraction-result-2187.html#e2187.0" class="evidence-link">[e2187.0]</a> <a href="../results/extraction-result-2187.html#e2187.4" class="evidence-link">[e2187.4]</a> <a href="../results/extraction-result-2189.html#e2189.2" class="evidence-link">[e2189.2]</a> <a href="../results/extraction-result-2185.html#e2185.5" class="evidence-link">[e2185.5]</a> </li>
    <li>Training distribution bias is empirically demonstrated: embedding models trained on 2008-2020 data show that older (pre-training) publications fill conceptual holes more than newer publications (mean difference 5.3267 mixup units, p<0.0001), indicating models fail to recognize truly novel post-training work, directly supporting the theory's training distribution bias mechanism. <a href="../results/extraction-result-2183.html#e2183.1" class="evidence-link">[e2183.1]</a> <a href="../results/extraction-result-2183.html#e2183.2" class="evidence-link">[e2183.2]</a> <a href="../results/extraction-result-2182.html#e2182.0" class="evidence-link">[e2182.0]</a> </li>
    <li>Field-specific differences in proxy performance are substantial: LLM+search achieves AUROC ~0.8 in computer science but only ~0.6 in biomedicine, and citation metrics are systematically biased against arts/humanities/social sciences due to indexing gaps, supporting the theory's prediction that field-specific paradigm rigidity (β parameter) varies across disciplines. <a href="../results/extraction-result-2190.html#e2190.0" class="evidence-link">[e2190.0]</a> <a href="../results/extraction-result-2183.html#e2183.0" class="evidence-link">[e2183.0]</a> </li>
    <li>Correction mechanisms show measurable improvements supporting the theory's correctability claim: text-based models increase correlation with reviewer scores from r=0.013 to r=0.33 (25x improvement), structured retrieval increases deep analysis from 11.5% to 52.1%, and entropy-weighted training reduces prediction error for high-disruption cases (MSE improvement from 0.0187 to 0.0093 in DBLP). <a href="../results/extraction-result-2189.html#e2189.0" class="evidence-link">[e2189.0]</a> <a href="../results/extraction-result-2191.html#e2191.2" class="evidence-link">[e2191.2]</a> <a href="../results/extraction-result-2186.html#e2186.3" class="evidence-link">[e2186.3]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>Evidence suggests 'rich-get-richer' citation amplification effects and that AI systems can reproduce and amplify existing biases, implying multiplicative compounding of proxy failures, though explicit multiplicative vs additive decomposition was not empirically tested. <a href="../results/extraction-result-2182.html#e2182.2" class="evidence-link">[e2182.2]</a> <a href="../results/extraction-result-2182.html#e2182.0" class="evidence-link">[e2182.0]</a> </li>
    <li>Temporal patterns show delayed recognition for novel work and that embedding-based proxies are sensitive to training period, partially supporting the theory's time-dependent gap G(T,t), though no explicit exponential decay function was validated and no quantitative decay rates (λ) were measured. <a href="../results/extraction-result-2189.html#e2189.2" class="evidence-link">[e2189.2]</a> <a href="../results/extraction-result-2183.html#e2183.1" class="evidence-link">[e2183.1]</a> <a href="../results/extraction-result-2183.html#e2183.2" class="evidence-link">[e2183.2]</a> <a href="../results/extraction-result-2187.html#e2187.6" class="evidence-link">[e2187.6]</a> </li>
    <li>Disruption Index (DI) provides an alternative ground-truth measure that better captures paradigm-shifting work than citation counts, with DI used successfully as a supervised target for predictive models (hit rates 24.6-28.1% for DI>0.5), supporting the theory's claim that proxy-truth relationships can be characterized and modeled. <a href="../results/extraction-result-2186.html#e2186.0" class="evidence-link">[e2186.0]</a> <a href="../results/extraction-result-2186.html#e2186.1" class="evidence-link">[e2186.1]</a> <a href="../results/extraction-result-2186.html#e2186.2" class="evidence-link">[e2186.2]</a> </li>
    <li>Multiple proxy failures are documented across different dimensions (citations, journal prestige, peer review, author reputation), supporting the theory's claim that transformational work violates multiple proxy assumptions, though whether these compound multiplicatively rather than additively was not directly tested. <a href="../results/extraction-result-2187.html#e2187.0" class="evidence-link">[e2187.0]</a> <a href="../results/extraction-result-2187.html#e2187.4" class="evidence-link">[e2187.4]</a> <a href="../results/extraction-result-2189.html#e2189.2" class="evidence-link">[e2189.2]</a> <a href="../results/extraction-result-2191.html#e2191.0" class="evidence-link">[e2191.0]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<p class="empty-note">No evidence provided.</p>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>Evaluator choice produces massive variation in perceived quality: 'comparable' percentages ranged from 13.6% to 81.8% for identical papers depending on which LLM evaluator was used, suggesting the proxy-truth gap magnitude is highly evaluator-dependent rather than being a stable function of transformation degree T alone as the theory's formula G(T) ≈ k * e^(βT) implies. <a href="../results/extraction-result-2184.html#e2184.2" class="evidence-link">[e2184.2]</a> </li>
    <li>AI-generated papers in open-ended (autonomous) tasks received more favorable ratings than guided tasks (comparable rates 40-100% vs 15.79-78.95%), which could contradict the theory if open-ended tasks represent higher transformation degree, though this may reflect evaluator bias or task characteristics rather than true ground-truth quality. <a href="../results/extraction-result-2184.html#e2184.3" class="evidence-link">[e2184.3]</a> </li>
    <li>Among 11 persistent topological holes in embedding space, 4 showed greater filling by newer (post-training) documents than older documents, suggesting training distribution bias does not uniformly favor historical over novel work in all conceptual dimensions. <a href="../results/extraction-result-2183.html#e2183.1" class="evidence-link">[e2183.1]</a> </li>
    <li>Some highly novel work can receive rapid recognition and high citations, and citation-augmented metrics (ON) correlate well with human novelty labels in limited single-domain studies, suggesting the proxy-truth gap is not uniformly large for all transformational work as the theory's exponential relationship might imply. <a href="../results/extraction-result-2185.html#e2185.5" class="evidence-link">[e2185.5]</a> <a href="../results/extraction-result-2190.html#e2190.2" class="evidence-link">[e2190.2]</a> </li>
    <li>LLM reviewers show presentation-over-substance bias, overweighting stylistic features rather than methodological innovation, and LLMs can generate ideas scored as MORE novel than human experts, suggesting proxy failures can produce both undervaluation and overvaluation rather than systematic undervaluation as the theory predicts. <a href="../results/extraction-result-2184.html#e2184.4" class="evidence-link">[e2184.4]</a> <a href="../results/extraction-result-2181.html#e2181.2" class="evidence-link">[e2181.2]</a> <a href="../results/extraction-result-2189.html#e2189.5" class="evidence-link">[e2189.5]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>Domain expertise effects are a major source of proxy-truth divergence, with reviewers' specific field knowledge materially affecting novelty conclusions, suggesting the theory should incorporate reviewer expertise as an additional variable beyond transformation degree T and paradigm rigidity β. <a href="../results/extraction-result-2191.html#e2191.4" class="evidence-link">[e2191.4]</a> </li>
    <li>Systematic evaluation criteria that increase consistency (86.5% vs 65.1% reasoning alignment) may suppress valuable heterodox judgments needed to identify paradigm-shifting work, suggesting a fundamental tradeoff between reducing proxy-truth gaps through standardization and maintaining the diversity of perspective needed to recognize truly transformational work. <a href="../results/extraction-result-2191.html#e2191.5" class="evidence-link">[e2191.5]</a> </li>
    <li>Field differences appear more complex than paradigm rigidity alone: they include publication patterns (books vs articles), data availability and imbalance, indexing coverage, API/database accessibility, and domain-specific citation norms, suggesting the β parameter should be expanded to a multidimensional field-characteristics vector. <a href="../results/extraction-result-2183.html#e2183.0" class="evidence-link">[e2183.0]</a> <a href="../results/extraction-result-2182.html#e2182.0" class="evidence-link">[e2182.0]</a> <a href="../results/extraction-result-2181.html#e2181.0" class="evidence-link">[e2181.0]</a> </li>
    <li>Automated systems show systematic directional biases that vary by system design: DeepReviewer shows 21.7% positive shift (optimistic bias) while humans show 15.0% negative shift (critical bias), suggesting the gap measure G should include a signed directional component rather than only magnitude of undervaluation. <a href="../results/extraction-result-2191.html#e2191.1" class="evidence-link">[e2191.1]</a> </li>
    <li>The theory does not account for evaluator variance as a systematic factor: different evaluation systems (human vs different LLMs) produce 50+ percentage point differences in perceived quality for identical work, suggesting G should be formulated as G(T, β, E) where E represents evaluator characteristics. <a href="../results/extraction-result-2184.html#e2184.2" class="evidence-link">[e2184.2]</a> <a href="../results/extraction-result-2191.html#e2191.0" class="evidence-link">[e2191.0]</a> </li>
    <li>Training contamination and evaluation-set overlap can substantially inflate baseline performance metrics, indicating that measured proxy-truth gaps may themselves be biased by methodological choices in evaluation design, suggesting the theory should address meta-evaluation reliability. <a href="../results/extraction-result-2191.html#e2191.3" class="evidence-link">[e2191.3]</a> </li>
    <li>No evidence directly tests the theory's specific quantitative predictions: the exponential relationship G(T) ≈ k * e^(βT), the 70-90% undervaluation magnitude for highly transformational work (T>0.7), or the exponential temporal decay G(T,t) = G(T) * e^(-λt), suggesting these specific functional forms and magnitudes need empirical validation. </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>Expand the gap function to include evaluator characteristics: G(T, β, E) where E represents evaluator type, expertise, and systematic biases, as evidence shows 50+ percentage point variation based on evaluator choice alone.</li>
                <li>Modify the theory to account for bidirectional bias: some automated systems overvalue novelty while others undervalue it, suggesting the gap should be signed (positive or negative) rather than assuming uniform undervaluation.</li>
                <li>Refine the field-specific β parameter to be multidimensional, encompassing paradigm rigidity, publication norms, data availability, indexing coverage, and citation culture, as evidence shows field differences arise from multiple mechanisms.</li>
                <li>Add an evaluator-consistency vs diversity tradeoff principle: correction mechanisms that reduce measured disagreement through standardization may simultaneously reduce the ability to identify paradigm-shifting work that requires heterodox judgment.</li>
                <li>Incorporate reviewer expertise as an explicit variable in the theory, as domain knowledge materially affects novelty recognition independent of transformation degree.</li>
                <li>Add specific empirical validation targets: the theory's exponential functional form G(T) ≈ k * e^(βT), the predicted 70-90% undervaluation magnitude for T>0.7, and the temporal decay function G(T,t) = G(T) * e^(-λt) all lack direct empirical tests in the current evidence.</li>
                <li>Clarify the scope regarding AI-generated vs human-generated discoveries: evidence suggests AI systems may face different evaluation dynamics (sometimes rated more favorably in open-ended tasks), requiring theory extension to cover this emerging case.</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-30",
    "theory_id": "theory-320",
    "fully_supporting_evidence": [
        {
            "text": "Citation-based proxies show near-zero correlation (Pearson r=0.0132) with peer-review novelty scores, while text-based models achieve moderate correlation (r~0.33), demonstrating systematic failure of traditional citation proxies for novelty assessment and supporting the theory's claim that citation-based proxies systematically undervalue transformational work.",
            "uuids": [
                "e2189.0",
                "e2189.1"
            ]
        },
        {
            "text": "LLMs without integrated literature search achieve random performance (AUROC ~0.5) on novelty detection, confirming that automated systems relying solely on parametric knowledge systematically fail to identify novel work, supporting the theory's training distribution bias mechanism.",
            "uuids": [
                "e2190.0"
            ]
        },
        {
            "text": "Historical dissimilarity metrics show sharp cross-domain performance degradation (AUROC drop of ~0.40-0.50 from single-domain ~0.75-0.85 to cross-domain ~0.36-0.40), demonstrating that proxy metrics calibrated on one distribution fail when applied to different contexts, supporting the theory's claim about training distribution bias.",
            "uuids": [
                "e2190.1",
                "e2190.2"
            ]
        },
        {
            "text": "Peer review exhibits 35-37% disagreement on novelty judgments in ICLR samples and 23% disagreement in NeurIPS experiments, confirming systematic inconsistency in evaluation proxies and supporting the theory's claim that standard evaluation proxies are unreliable for transformational work.",
            "uuids": [
                "e2191.0"
            ]
        },
        {
            "text": "Multiple studies document bias against novelty: reviewers systematically penalize novel proposals even when quality is equal (Boudreau et al.), 24 Nobel-winning papers were initially rejected, and highly novel work tends to appear in lower-impact-factor journals, all supporting the theory's prediction that proxies systematically undervalue transformational work.",
            "uuids": [
                "e2187.0",
                "e2187.4",
                "e2189.2",
                "e2185.5"
            ]
        },
        {
            "text": "Training distribution bias is empirically demonstrated: embedding models trained on 2008-2020 data show that older (pre-training) publications fill conceptual holes more than newer publications (mean difference 5.3267 mixup units, p&lt;0.0001), indicating models fail to recognize truly novel post-training work, directly supporting the theory's training distribution bias mechanism.",
            "uuids": [
                "e2183.1",
                "e2183.2",
                "e2182.0"
            ]
        },
        {
            "text": "Field-specific differences in proxy performance are substantial: LLM+search achieves AUROC ~0.8 in computer science but only ~0.6 in biomedicine, and citation metrics are systematically biased against arts/humanities/social sciences due to indexing gaps, supporting the theory's prediction that field-specific paradigm rigidity (β parameter) varies across disciplines.",
            "uuids": [
                "e2190.0",
                "e2183.0"
            ]
        },
        {
            "text": "Correction mechanisms show measurable improvements supporting the theory's correctability claim: text-based models increase correlation with reviewer scores from r=0.013 to r=0.33 (25x improvement), structured retrieval increases deep analysis from 11.5% to 52.1%, and entropy-weighted training reduces prediction error for high-disruption cases (MSE improvement from 0.0187 to 0.0093 in DBLP).",
            "uuids": [
                "e2189.0",
                "e2191.2",
                "e2186.3"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "Evidence suggests 'rich-get-richer' citation amplification effects and that AI systems can reproduce and amplify existing biases, implying multiplicative compounding of proxy failures, though explicit multiplicative vs additive decomposition was not empirically tested.",
            "uuids": [
                "e2182.2",
                "e2182.0"
            ]
        },
        {
            "text": "Temporal patterns show delayed recognition for novel work and that embedding-based proxies are sensitive to training period, partially supporting the theory's time-dependent gap G(T,t), though no explicit exponential decay function was validated and no quantitative decay rates (λ) were measured.",
            "uuids": [
                "e2189.2",
                "e2183.1",
                "e2183.2",
                "e2187.6"
            ]
        },
        {
            "text": "Disruption Index (DI) provides an alternative ground-truth measure that better captures paradigm-shifting work than citation counts, with DI used successfully as a supervised target for predictive models (hit rates 24.6-28.1% for DI&gt;0.5), supporting the theory's claim that proxy-truth relationships can be characterized and modeled.",
            "uuids": [
                "e2186.0",
                "e2186.1",
                "e2186.2"
            ]
        },
        {
            "text": "Multiple proxy failures are documented across different dimensions (citations, journal prestige, peer review, author reputation), supporting the theory's claim that transformational work violates multiple proxy assumptions, though whether these compound multiplicatively rather than additively was not directly tested.",
            "uuids": [
                "e2187.0",
                "e2187.4",
                "e2189.2",
                "e2191.0"
            ]
        }
    ],
    "fully_contradicting_evidence": [],
    "partially_contradicting_evidence": [
        {
            "text": "Evaluator choice produces massive variation in perceived quality: 'comparable' percentages ranged from 13.6% to 81.8% for identical papers depending on which LLM evaluator was used, suggesting the proxy-truth gap magnitude is highly evaluator-dependent rather than being a stable function of transformation degree T alone as the theory's formula G(T) ≈ k * e^(βT) implies.",
            "uuids": [
                "e2184.2"
            ]
        },
        {
            "text": "AI-generated papers in open-ended (autonomous) tasks received more favorable ratings than guided tasks (comparable rates 40-100% vs 15.79-78.95%), which could contradict the theory if open-ended tasks represent higher transformation degree, though this may reflect evaluator bias or task characteristics rather than true ground-truth quality.",
            "uuids": [
                "e2184.3"
            ]
        },
        {
            "text": "Among 11 persistent topological holes in embedding space, 4 showed greater filling by newer (post-training) documents than older documents, suggesting training distribution bias does not uniformly favor historical over novel work in all conceptual dimensions.",
            "uuids": [
                "e2183.1"
            ]
        },
        {
            "text": "Some highly novel work can receive rapid recognition and high citations, and citation-augmented metrics (ON) correlate well with human novelty labels in limited single-domain studies, suggesting the proxy-truth gap is not uniformly large for all transformational work as the theory's exponential relationship might imply.",
            "uuids": [
                "e2185.5",
                "e2190.2"
            ]
        },
        {
            "text": "LLM reviewers show presentation-over-substance bias, overweighting stylistic features rather than methodological innovation, and LLMs can generate ideas scored as MORE novel than human experts, suggesting proxy failures can produce both undervaluation and overvaluation rather than systematic undervaluation as the theory predicts.",
            "uuids": [
                "e2184.4",
                "e2181.2",
                "e2189.5"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "Domain expertise effects are a major source of proxy-truth divergence, with reviewers' specific field knowledge materially affecting novelty conclusions, suggesting the theory should incorporate reviewer expertise as an additional variable beyond transformation degree T and paradigm rigidity β.",
            "uuids": [
                "e2191.4"
            ]
        },
        {
            "text": "Systematic evaluation criteria that increase consistency (86.5% vs 65.1% reasoning alignment) may suppress valuable heterodox judgments needed to identify paradigm-shifting work, suggesting a fundamental tradeoff between reducing proxy-truth gaps through standardization and maintaining the diversity of perspective needed to recognize truly transformational work.",
            "uuids": [
                "e2191.5"
            ]
        },
        {
            "text": "Field differences appear more complex than paradigm rigidity alone: they include publication patterns (books vs articles), data availability and imbalance, indexing coverage, API/database accessibility, and domain-specific citation norms, suggesting the β parameter should be expanded to a multidimensional field-characteristics vector.",
            "uuids": [
                "e2183.0",
                "e2182.0",
                "e2181.0"
            ]
        },
        {
            "text": "Automated systems show systematic directional biases that vary by system design: DeepReviewer shows 21.7% positive shift (optimistic bias) while humans show 15.0% negative shift (critical bias), suggesting the gap measure G should include a signed directional component rather than only magnitude of undervaluation.",
            "uuids": [
                "e2191.1"
            ]
        },
        {
            "text": "The theory does not account for evaluator variance as a systematic factor: different evaluation systems (human vs different LLMs) produce 50+ percentage point differences in perceived quality for identical work, suggesting G should be formulated as G(T, β, E) where E represents evaluator characteristics.",
            "uuids": [
                "e2184.2",
                "e2191.0"
            ]
        },
        {
            "text": "Training contamination and evaluation-set overlap can substantially inflate baseline performance metrics, indicating that measured proxy-truth gaps may themselves be biased by methodological choices in evaluation design, suggesting the theory should address meta-evaluation reliability.",
            "uuids": [
                "e2191.3"
            ]
        },
        {
            "text": "No evidence directly tests the theory's specific quantitative predictions: the exponential relationship G(T) ≈ k * e^(βT), the 70-90% undervaluation magnitude for highly transformational work (T&gt;0.7), or the exponential temporal decay G(T,t) = G(T) * e^(-λt), suggesting these specific functional forms and magnitudes need empirical validation.",
            "uuids": []
        }
    ],
    "suggested_revisions": [
        "Expand the gap function to include evaluator characteristics: G(T, β, E) where E represents evaluator type, expertise, and systematic biases, as evidence shows 50+ percentage point variation based on evaluator choice alone.",
        "Modify the theory to account for bidirectional bias: some automated systems overvalue novelty while others undervalue it, suggesting the gap should be signed (positive or negative) rather than assuming uniform undervaluation.",
        "Refine the field-specific β parameter to be multidimensional, encompassing paradigm rigidity, publication norms, data availability, indexing coverage, and citation culture, as evidence shows field differences arise from multiple mechanisms.",
        "Add an evaluator-consistency vs diversity tradeoff principle: correction mechanisms that reduce measured disagreement through standardization may simultaneously reduce the ability to identify paradigm-shifting work that requires heterodox judgment.",
        "Incorporate reviewer expertise as an explicit variable in the theory, as domain knowledge materially affects novelty recognition independent of transformation degree.",
        "Add specific empirical validation targets: the theory's exponential functional form G(T) ≈ k * e^(βT), the predicted 70-90% undervaluation magnitude for T&gt;0.7, and the temporal decay function G(T,t) = G(T) * e^(-λt) all lack direct empirical tests in the current evidence.",
        "Clarify the scope regarding AI-generated vs human-generated discoveries: evidence suggests AI systems may face different evaluation dynamics (sometimes rated more favorably in open-ended tasks), requiring theory extension to cover this emerging case."
    ],
    "overall_support_or_contradict": "support",
    "overall_support_or_contradict_explanation": "The evidence strongly supports the theory's core mechanisms—training distribution bias, systematic proxy failures for novel work, field-specific differences, and correctability through meta-learning—with multiple studies showing near-zero correlations between traditional proxies and novelty, substantial cross-domain degradation, and measurable improvements from correction mechanisms. However, the evidence reveals important complexities (massive evaluator variance, bidirectional biases, expertise effects) that suggest the theory's specific functional forms and scope need refinement rather than fundamental revision.",
    "revised_theory_ids": [
        "theory-392"
    ],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-9 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-9</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-9</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-320.html">theory-320</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-369.html">theory-369</a></p>
                <p><strong>Overall Support or Contradict:</strong> support</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The evidence strongly supports the theory's core mechanisms—training distribution bias, time-dependent gaps, field-specific effects, and multiplicative proxy failures—but reveals critical nuances requiring theory refinement: gap magnitude and direction varies by novelty type (Pioneer severely undervalued, Maverick rewarded), functional forms are more complex than single exponential, and well-designed automated systems can match or exceed human performance, indicating design-dependent rather than inherent automated bias.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>Citation-based proxies systematically undervalue truly novel (Pioneer) work: 5-year citations show no significant positive effect for Pioneer novelty (coefficient=-0.014, p=0.84), requiring longer time horizons for recognition, while the theory predicts short-term proxies undervalue transformational work. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Short citation windows create systematic undervaluation requiring ≥10 years to stabilize D-index estimates, with turning point around 10 years for detecting small-team disruptive work, confirming theory's time-dependent gap G(T,t)=G(T)*e^(-λt). <a href="../results/extraction-result-1884.html#e1884.2" class="evidence-link">[e1884.2]</a> <a href="../results/extraction-result-1884.html#e1884.5" class="evidence-link">[e1884.5]</a> </li>
    <li>Automated novelty detection systems show severe systematic failures: AI Scientist's novelty classifier achieved 100% false-positive rate (12/12 items labeled novel including well-documented techniques like micro-batching and adaptive learning rates), demonstrating training distribution bias causing inability to detect established prior art. <a href="../results/extraction-result-1886.html#e1886.0" class="evidence-link">[e1886.0]</a> </li>
    <li>D-index decomposition reveals structural barriers confirming multiplicative proxy failure: 98.9% of papers have burden factor b_p>1 (median=119), meaning even papers with positive local displacement d_p are attenuated by factor ~1/120, making displacement nearly impossible and confirming theory's multiplicative compounding mechanism. <a href="../results/extraction-result-1884.html#e1884.1" class="evidence-link">[e1884.1]</a> </li>
    <li>Field-specific paradigm rigidity confirmed: journal random-effect variance of 1.035 indicates venue creates large systematic gaps; AI vs biomedical fields show contrasting reward structures (AI rewards early-career novelty through low-cost infrastructure and plural funding, biomedicine shows gerontocratic patterns with centralized NIH funding), supporting theory's β parameter for field rigidity. <a href="../results/extraction-result-1881.html#e1881.2" class="evidence-link">[e1881.2]</a> <a href="../results/extraction-result-1888.html#e1888.9" class="evidence-link">[e1888.9]</a> </li>
    <li>Training distribution bias in automated systems confirmed: AI-generated manuscripts show citation patterns skewed toward older literature (only 14.7% from 2020+, median 5 citations per paper); GPT ranking system selects papers with statistically significantly lower embedding-distance novelty than human selections in multiple conferences (ICLR 2023, ICLR 2024, EMNLP 2023). <a href="../results/extraction-result-1886.html#e1886.2" class="evidence-link">[e1886.2]</a> <a href="../results/extraction-result-1880.html#e1880.0" class="evidence-link">[e1880.0]</a> </li>
    <li>Historical evidence of transformational work rejection: 24 documented cases of Nobel-winning papers initially rejected by peer review; highly disruptive papers (D>0.2) show 52% within-field displacement vs 1.4% expected by chance, indicating systematic initial undervaluation followed by eventual recognition. <a href="../results/extraction-result-1888.html#e1888.2" class="evidence-link">[e1888.2]</a> <a href="../results/extraction-result-1884.html#e1884.4" class="evidence-link">[e1884.4]</a> </li>
    <li>Author reputation and institutional proxies fail for transformational work: GPT ranking shows higher institutional concentration (ICLR 2023: 43.8% from top-10 institutions vs human 27.0%; ICLR 2024: 37.2% vs 26.7%), and literature documents that transformational discoveries often come from unexpected sources, confirming theory's prediction of author-reputation proxy failure. <a href="../results/extraction-result-1880.html#e1880.0" class="evidence-link">[e1880.0]</a> <a href="../results/extraction-result-1888.html#e1888.9" class="evidence-link">[e1888.9]</a> </li>
    <li>Citation inflation and knowledge burden create temporal bias: median burden factor b_p=119 and characteristic D≈-0.002 show that growing cumulative citations to canonical works systematically compress measured disruptiveness over time, supporting theory's claim that proxies calibrated on historical data create systematic undervaluation. <a href="../results/extraction-result-1884.html#e1884.3" class="evidence-link">[e1884.3]</a> <a href="../results/extraction-result-1884.html#e1884.1" class="evidence-link">[e1884.1]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>Multiple proxy failures documented but interactions are complex: voting mechanism removal increased CI by +0.9% while decreasing ON by -1.56%, showing proxies can move in opposite directions rather than simply compounding multiplicatively; different novelty measures show low concordance (Pioneer vs disruption ρ=-0.041, Maverick vs disruption ρ=0.399). <a href="../results/extraction-result-1887.html#e1887.2" class="evidence-link">[e1887.2]</a> <a href="../results/extraction-result-1881.html#e1881.1" class="evidence-link">[e1881.1]</a> </li>
    <li>Human peer review also shows substantial proxy-truth gaps: human-human reasoning alignment only 65.1%, conclusion agreement 62.8%, and 23% disagreement between independent NeurIPS committees on identical papers, suggesting proxy-truth gaps exist in both human and automated systems but with different patterns. <a href="../results/extraction-result-1885.html#e1885.0" class="evidence-link">[e1885.0]</a> <a href="../results/extraction-result-1885.html#e1885.3" class="evidence-link">[e1885.3]</a> <a href="../results/extraction-result-1880.html#e1880.3" class="evidence-link">[e1880.3]</a> </li>
    <li>Time-dependence confirmed but varies by novelty type: Pioneer requires >10 years for recognition, small-team work shows 'sleeping beauty' patterns with delayed citation accumulation, but Maverick (recombinatory) novelty shows faster recognition, suggesting type-specific time-decay parameters rather than universal λ. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> <a href="../results/extraction-result-1884.html#e1884.2" class="evidence-link">[e1884.2]</a> <a href="../results/extraction-result-1884.html#e1884.5" class="evidence-link">[e1884.5]</a> </li>
    <li>Reference-count and literature-grounding effects create convergence bias: providing many shared reference papers increases inter-agent output similarity and reduces novelty (empirical choice k=8 balances coverage and diversity), demonstrating that information-grounding practices can bias automated systems toward existing literature norms. <a href="../results/extraction-result-1887.html#e1887.3" class="evidence-link">[e1887.3]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<p class="empty-note">No evidence provided.</p>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>Well-designed automated systems can match or exceed human performance on some metrics: GPT ranking at scale (>10^5 comparisons) achieves 20.00 average citations vs human 19.36; human-informed LLM pipeline achieves 86.5% reasoning alignment vs human baseline 65.1%, suggesting automated undervaluation is design-dependent rather than inherent, though these systems still show novelty bias by embedding distance. <a href="../results/extraction-result-1880.html#e1880.0" class="evidence-link">[e1880.0]</a> <a href="../results/extraction-result-1885.html#e1885.1" class="evidence-link">[e1885.1]</a> </li>
    <li>Recombinatory transformational work (Maverick novelty) is strongly REWARDED by short-term citation proxies (coefficient=0.546, p<0.001, exp(0.546)≈1.73 or ~73% higher citations), contradicting theory's universal prediction of 70-90% undervaluation for transformational work, though this supports the broader claim that different proxy patterns exist for different types of novelty. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Structured interventions can substantially reduce proxy-truth gaps: IDVSCI system with dynamic knowledge exchange and dual-diversity review achieves +33.6% higher CI and +24.3% higher ON than baselines; entropy-weighted loss and secondary learning reduce MSE from 0.1191 to 0.0093 (92% reduction), suggesting gaps are more correctable than theory's 30-40% reduction estimate for moderately transformational work. <a href="../results/extraction-result-1887.html#e1887.0" class="evidence-link">[e1887.0]</a> <a href="../results/extraction-result-1887.html#e1887.1" class="evidence-link">[e1887.1]</a> <a href="../results/extraction-result-1883.html#e1883.2" class="evidence-link">[e1883.2]</a> </li>
    <li>Some contexts show no significant novelty bias: CoRL 2023 and NeurIPS 2023 showed no statistically significant differences in embedding-distance novelty between GPT-selected and human-selected papers, indicating bias magnitude varies substantially by context and is not universal across all automated systems or venues. <a href="../results/extraction-result-1880.html#e1880.2" class="evidence-link">[e1880.2]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>Gap magnitude and direction varies systematically by novelty TYPE not just degree: Pioneer (new topics) shows severe undervaluation (no significant 5-year citation effect), Maverick (distant recombination) shows strong positive reward (+73%), Vanguard (reinforcement) shows moderate reward with diminishing returns (bell-shaped). Theory should be modified to distinguish novelty types with type-specific gap functions G_type(T) rather than treating transformation as unidimensional degree T. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Functional form is more complex than single exponential: D-index shows inverse-multiplicative relationship D≈(1/(1+b_p))*d_p; Vanguard shows bell-shaped (inverted-U) relationship with significant quadratic term; team diversity shows peaked effect at ~25%. Theory should specify type-specific piecewise or multi-modal functions rather than universal exponential G(T)≈k*e^(βT). <a href="../results/extraction-result-1884.html#e1884.1" class="evidence-link">[e1884.1]</a> <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> <a href="../results/extraction-result-1887.html#e1887.1" class="evidence-link">[e1887.1]</a> </li>
    <li>Automated systems' performance depends critically on design choices: systems with structured diversity, dynamic knowledge exchange, literature grounding, and entropy-weighted training can substantially outperform both baseline automated systems AND humans (86.5% vs 65.1% reasoning alignment). Theory should emphasize design-dependent bias rather than inherent automated bias, and expand correction mechanisms beyond meta-learning to include architectural interventions. <a href="../results/extraction-result-1887.html#e1887.0" class="evidence-link">[e1887.0]</a> <a href="../results/extraction-result-1887.html#e1887.1" class="evidence-link">[e1887.1]</a> <a href="../results/extraction-result-1883.html#e1883.2" class="evidence-link">[e1883.2]</a> <a href="../results/extraction-result-1885.html#e1885.1" class="evidence-link">[e1885.1]</a> </li>
    <li>Field differences involve institutional infrastructure beyond paradigm rigidity: AI field rewards early-career novelty through low-cost computational infrastructure, fast experimental cycles, open dissemination (arXiv), and plural funding (VC/industry/corporate labs), while biomedicine shows gerontocratic patterns with centralized NIH funding (~80% of academic biomedical funding), high experimental costs, and long timelines. Theory's β parameter should incorporate institutional and infrastructure factors beyond paradigm rigidity alone. <a href="../results/extraction-result-1888.html#e1888.9" class="evidence-link">[e1888.9]</a> <a href="../results/extraction-result-1888.html#e1888.5" class="evidence-link">[e1888.5]</a> <a href="../results/extraction-result-1888.html#e1888.6" class="evidence-link">[e1888.6]</a> </li>
    <li>Venue effects are as large as field effects: journal random-effect variance (1.035) is larger than year variance (0.382); specific journals show divergent patterns (VOLUNTAS high Pioneer novelty but low impact; NVSQ low Pioneer novelty but high impact), indicating editorial strategy and venue visibility create systematic gaps independent of field-level paradigm rigidity. <a href="../results/extraction-result-1881.html#e1881.2" class="evidence-link">[e1881.2]</a> </li>
    <li>Multiple proxy failures can move in OPPOSITE directions rather than compounding multiplicatively: voting removal increased CI (+0.9%) while decreasing ON (-1.56%); different proxies show low or negative correlations (Pioneer vs disruption ρ=-0.041). Theory should clarify that proxy failures can interact in complex ways including opposition and cancellation, not just multiplicative compounding. <a href="../results/extraction-result-1887.html#e1887.2" class="evidence-link">[e1887.2]</a> <a href="../results/extraction-result-1881.html#e1881.1" class="evidence-link">[e1881.1]</a> </li>
    <li>Gap magnitude for highly transformational work may be lower than 70-90% for some types: Maverick shows positive reward (+73%), Vanguard shows moderate reward (exp(0.109)≈1.12 or +12%), only Pioneer shows severe undervaluation approaching theory's predicted range. Theory should specify type-specific gap ranges: Pioneer 60-90%, Maverick -70% to 0% (negative gap = reward), Vanguard 0-40%. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Correction mechanisms extend beyond meta-learning: structured diversity in review teams, dynamic knowledge exchange, entropy-weighted loss functions, extended citation windows (10+ years), literature-grounded evaluation, and area-control interventions all show substantial gap reduction (30-92% improvements observed). Theory should expand correction mechanisms and increase predicted effectiveness from 30-40% to 30-90% depending on intervention type. <a href="../results/extraction-result-1887.html#e1887.0" class="evidence-link">[e1887.0]</a> <a href="../results/extraction-result-1887.html#e1887.1" class="evidence-link">[e1887.1]</a> <a href="../results/extraction-result-1883.html#e1883.2" class="evidence-link">[e1883.2]</a> <a href="../results/extraction-result-1884.html#e1884.2" class="evidence-link">[e1884.2]</a> </li>
    <li>Reward model overoptimization can create additional bias: RL-based optimization against automated reward models can cause systems to exploit weaknesses and produce safe/incremental outputs (human expert rating ~4.8 vs automated ~5.36; accept-rate shift from 31.07% to 28.65% under held-out reward model), suggesting training procedures themselves can introduce or amplify proxy-truth gaps beyond those in the reward signal. <a href="../results/extraction-result-1879.html#e1879.2" class="evidence-link">[e1879.2]</a> </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>Modify theory to distinguish between novelty TYPES (Pioneer/new topics, Maverick/distant recombination, Vanguard/reinforcement) rather than treating transformation as unidimensional degree T. Specify type-specific gap functions: G_Pioneer(T) shows severe undervaluation, G_Maverick(T) shows reward (negative gap), G_Vanguard(T) shows inverted-U with diminishing returns.</li>
                <li>Replace single exponential relationship G(T)≈k*e^(βT) with type-specific functional forms: inverse-multiplicative for displacement-based novelty (D≈(1/(1+b_p))*d_p), bell-shaped for reinforcement novelty (quadratic with negative squared term), and peaked relationships for team/diversity effects.</li>
                <li>Revise claim about automated systems from 'systematic undervaluation' to 'design-dependent bias': well-designed systems with structured diversity, dynamic knowledge exchange, and entropy-weighted training can match or exceed human performance (86.5% vs 65.1% reasoning alignment), though they may still show novelty bias on some dimensions (embedding distance).</li>
                <li>Add type-specific time-decay parameters: G(T_type, t) = G(T_type) * e^(-λ_type * t) where λ_Pioneer << λ_Maverick << λ_Vanguard, reflecting that Pioneer requires >10 years, Maverick shows faster recognition, and Vanguard shows immediate but diminishing returns.</li>
                <li>Expand field-specific factors beyond paradigm rigidity β to include: institutional infrastructure (computational vs experimental costs), funding structure (plural vs centralized), dissemination norms (preprint culture vs journal-only), and venue effects (journal variance=1.035 as large as field effects).</li>
                <li>Revise predicted gap magnitude to be type-specific: Pioneer 60-90% undervaluation (confirmed), Maverick -70% to 0% (negative gap indicates reward, observed +73%), Vanguard 0-40% depending on degree (observed +12% with diminishing returns), rather than universal 70-90% for all transformational work.</li>
                <li>Expand correction mechanisms beyond meta-learning to include: structured diversity in review teams (DDR), dynamic knowledge exchange (DKE), entropy-weighted loss functions, extended citation windows (10+ years), literature-grounded evaluation, area-control interventions, and architectural design choices. Increase predicted effectiveness from 30-40% to 30-90% depending on intervention type and combination.</li>
                <li>Clarify that multiple proxy failures can interact in complex ways: they can compound multiplicatively (burden factor b_p attenuates displacement d_p), move in opposite directions (CI vs ON under voting), or show low/negative correlations (Pioneer vs disruption ρ=-0.041). Replace 'compound multiplicatively' with 'interact in complex, context-dependent ways including multiplication, opposition, and cancellation.'</li>
                <li>Add venue and editorial strategy as major moderating factors: journal random-effect variance (1.035) indicates venue choice creates gaps as large as field effects; editorial strategies (exploratory vs selective) produce divergent outcomes (VOLUNTAS high Pioneer novelty/low impact vs NVSQ low Pioneer novelty/high impact).</li>
                <li>Acknowledge that human review also shows substantial proxy-truth gaps (reasoning alignment 65.1%, conclusion agreement 62.8%, 23% committee disagreement on identical papers), suggesting the theory's focus on automated systems should be broadened to evaluation systems generally, with automated systems showing different but not necessarily worse patterns than human review.</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-9",
    "theory_id": "theory-320",
    "fully_supporting_evidence": [
        {
            "text": "Citation-based proxies systematically undervalue truly novel (Pioneer) work: 5-year citations show no significant positive effect for Pioneer novelty (coefficient=-0.014, p=0.84), requiring longer time horizons for recognition, while the theory predicts short-term proxies undervalue transformational work.",
            "uuids": [
                "e1881.0"
            ]
        },
        {
            "text": "Short citation windows create systematic undervaluation requiring ≥10 years to stabilize D-index estimates, with turning point around 10 years for detecting small-team disruptive work, confirming theory's time-dependent gap G(T,t)=G(T)*e^(-λt).",
            "uuids": [
                "e1884.2",
                "e1884.5"
            ]
        },
        {
            "text": "Automated novelty detection systems show severe systematic failures: AI Scientist's novelty classifier achieved 100% false-positive rate (12/12 items labeled novel including well-documented techniques like micro-batching and adaptive learning rates), demonstrating training distribution bias causing inability to detect established prior art.",
            "uuids": [
                "e1886.0"
            ]
        },
        {
            "text": "D-index decomposition reveals structural barriers confirming multiplicative proxy failure: 98.9% of papers have burden factor b_p&gt;1 (median=119), meaning even papers with positive local displacement d_p are attenuated by factor ~1/120, making displacement nearly impossible and confirming theory's multiplicative compounding mechanism.",
            "uuids": [
                "e1884.1"
            ]
        },
        {
            "text": "Field-specific paradigm rigidity confirmed: journal random-effect variance of 1.035 indicates venue creates large systematic gaps; AI vs biomedical fields show contrasting reward structures (AI rewards early-career novelty through low-cost infrastructure and plural funding, biomedicine shows gerontocratic patterns with centralized NIH funding), supporting theory's β parameter for field rigidity.",
            "uuids": [
                "e1881.2",
                "e1888.9"
            ]
        },
        {
            "text": "Training distribution bias in automated systems confirmed: AI-generated manuscripts show citation patterns skewed toward older literature (only 14.7% from 2020+, median 5 citations per paper); GPT ranking system selects papers with statistically significantly lower embedding-distance novelty than human selections in multiple conferences (ICLR 2023, ICLR 2024, EMNLP 2023).",
            "uuids": [
                "e1886.2",
                "e1880.0"
            ]
        },
        {
            "text": "Historical evidence of transformational work rejection: 24 documented cases of Nobel-winning papers initially rejected by peer review; highly disruptive papers (D&gt;0.2) show 52% within-field displacement vs 1.4% expected by chance, indicating systematic initial undervaluation followed by eventual recognition.",
            "uuids": [
                "e1888.2",
                "e1884.4"
            ]
        },
        {
            "text": "Author reputation and institutional proxies fail for transformational work: GPT ranking shows higher institutional concentration (ICLR 2023: 43.8% from top-10 institutions vs human 27.0%; ICLR 2024: 37.2% vs 26.7%), and literature documents that transformational discoveries often come from unexpected sources, confirming theory's prediction of author-reputation proxy failure.",
            "uuids": [
                "e1880.0",
                "e1888.9"
            ]
        },
        {
            "text": "Citation inflation and knowledge burden create temporal bias: median burden factor b_p=119 and characteristic D≈-0.002 show that growing cumulative citations to canonical works systematically compress measured disruptiveness over time, supporting theory's claim that proxies calibrated on historical data create systematic undervaluation.",
            "uuids": [
                "e1884.3",
                "e1884.1"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "Multiple proxy failures documented but interactions are complex: voting mechanism removal increased CI by +0.9% while decreasing ON by -1.56%, showing proxies can move in opposite directions rather than simply compounding multiplicatively; different novelty measures show low concordance (Pioneer vs disruption ρ=-0.041, Maverick vs disruption ρ=0.399).",
            "uuids": [
                "e1887.2",
                "e1881.1"
            ]
        },
        {
            "text": "Human peer review also shows substantial proxy-truth gaps: human-human reasoning alignment only 65.1%, conclusion agreement 62.8%, and 23% disagreement between independent NeurIPS committees on identical papers, suggesting proxy-truth gaps exist in both human and automated systems but with different patterns.",
            "uuids": [
                "e1885.0",
                "e1885.3",
                "e1880.3"
            ]
        },
        {
            "text": "Time-dependence confirmed but varies by novelty type: Pioneer requires &gt;10 years for recognition, small-team work shows 'sleeping beauty' patterns with delayed citation accumulation, but Maverick (recombinatory) novelty shows faster recognition, suggesting type-specific time-decay parameters rather than universal λ.",
            "uuids": [
                "e1881.0",
                "e1884.2",
                "e1884.5"
            ]
        },
        {
            "text": "Reference-count and literature-grounding effects create convergence bias: providing many shared reference papers increases inter-agent output similarity and reduces novelty (empirical choice k=8 balances coverage and diversity), demonstrating that information-grounding practices can bias automated systems toward existing literature norms.",
            "uuids": [
                "e1887.3"
            ]
        }
    ],
    "fully_contradicting_evidence": [],
    "partially_contradicting_evidence": [
        {
            "text": "Well-designed automated systems can match or exceed human performance on some metrics: GPT ranking at scale (&gt;10^5 comparisons) achieves 20.00 average citations vs human 19.36; human-informed LLM pipeline achieves 86.5% reasoning alignment vs human baseline 65.1%, suggesting automated undervaluation is design-dependent rather than inherent, though these systems still show novelty bias by embedding distance.",
            "uuids": [
                "e1880.0",
                "e1885.1"
            ]
        },
        {
            "text": "Recombinatory transformational work (Maverick novelty) is strongly REWARDED by short-term citation proxies (coefficient=0.546, p&lt;0.001, exp(0.546)≈1.73 or ~73% higher citations), contradicting theory's universal prediction of 70-90% undervaluation for transformational work, though this supports the broader claim that different proxy patterns exist for different types of novelty.",
            "uuids": [
                "e1881.0"
            ]
        },
        {
            "text": "Structured interventions can substantially reduce proxy-truth gaps: IDVSCI system with dynamic knowledge exchange and dual-diversity review achieves +33.6% higher CI and +24.3% higher ON than baselines; entropy-weighted loss and secondary learning reduce MSE from 0.1191 to 0.0093 (92% reduction), suggesting gaps are more correctable than theory's 30-40% reduction estimate for moderately transformational work.",
            "uuids": [
                "e1887.0",
                "e1887.1",
                "e1883.2"
            ]
        },
        {
            "text": "Some contexts show no significant novelty bias: CoRL 2023 and NeurIPS 2023 showed no statistically significant differences in embedding-distance novelty between GPT-selected and human-selected papers, indicating bias magnitude varies substantially by context and is not universal across all automated systems or venues.",
            "uuids": [
                "e1880.2"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "Gap magnitude and direction varies systematically by novelty TYPE not just degree: Pioneer (new topics) shows severe undervaluation (no significant 5-year citation effect), Maverick (distant recombination) shows strong positive reward (+73%), Vanguard (reinforcement) shows moderate reward with diminishing returns (bell-shaped). Theory should be modified to distinguish novelty types with type-specific gap functions G_type(T) rather than treating transformation as unidimensional degree T.",
            "uuids": [
                "e1881.0"
            ]
        },
        {
            "text": "Functional form is more complex than single exponential: D-index shows inverse-multiplicative relationship D≈(1/(1+b_p))*d_p; Vanguard shows bell-shaped (inverted-U) relationship with significant quadratic term; team diversity shows peaked effect at ~25%. Theory should specify type-specific piecewise or multi-modal functions rather than universal exponential G(T)≈k*e^(βT).",
            "uuids": [
                "e1884.1",
                "e1881.0",
                "e1887.1"
            ]
        },
        {
            "text": "Automated systems' performance depends critically on design choices: systems with structured diversity, dynamic knowledge exchange, literature grounding, and entropy-weighted training can substantially outperform both baseline automated systems AND humans (86.5% vs 65.1% reasoning alignment). Theory should emphasize design-dependent bias rather than inherent automated bias, and expand correction mechanisms beyond meta-learning to include architectural interventions.",
            "uuids": [
                "e1887.0",
                "e1887.1",
                "e1883.2",
                "e1885.1"
            ]
        },
        {
            "text": "Field differences involve institutional infrastructure beyond paradigm rigidity: AI field rewards early-career novelty through low-cost computational infrastructure, fast experimental cycles, open dissemination (arXiv), and plural funding (VC/industry/corporate labs), while biomedicine shows gerontocratic patterns with centralized NIH funding (~80% of academic biomedical funding), high experimental costs, and long timelines. Theory's β parameter should incorporate institutional and infrastructure factors beyond paradigm rigidity alone.",
            "uuids": [
                "e1888.9",
                "e1888.5",
                "e1888.6"
            ]
        },
        {
            "text": "Venue effects are as large as field effects: journal random-effect variance (1.035) is larger than year variance (0.382); specific journals show divergent patterns (VOLUNTAS high Pioneer novelty but low impact; NVSQ low Pioneer novelty but high impact), indicating editorial strategy and venue visibility create systematic gaps independent of field-level paradigm rigidity.",
            "uuids": [
                "e1881.2"
            ]
        },
        {
            "text": "Multiple proxy failures can move in OPPOSITE directions rather than compounding multiplicatively: voting removal increased CI (+0.9%) while decreasing ON (-1.56%); different proxies show low or negative correlations (Pioneer vs disruption ρ=-0.041). Theory should clarify that proxy failures can interact in complex ways including opposition and cancellation, not just multiplicative compounding.",
            "uuids": [
                "e1887.2",
                "e1881.1"
            ]
        },
        {
            "text": "Gap magnitude for highly transformational work may be lower than 70-90% for some types: Maverick shows positive reward (+73%), Vanguard shows moderate reward (exp(0.109)≈1.12 or +12%), only Pioneer shows severe undervaluation approaching theory's predicted range. Theory should specify type-specific gap ranges: Pioneer 60-90%, Maverick -70% to 0% (negative gap = reward), Vanguard 0-40%.",
            "uuids": [
                "e1881.0"
            ]
        },
        {
            "text": "Correction mechanisms extend beyond meta-learning: structured diversity in review teams, dynamic knowledge exchange, entropy-weighted loss functions, extended citation windows (10+ years), literature-grounded evaluation, and area-control interventions all show substantial gap reduction (30-92% improvements observed). Theory should expand correction mechanisms and increase predicted effectiveness from 30-40% to 30-90% depending on intervention type.",
            "uuids": [
                "e1887.0",
                "e1887.1",
                "e1883.2",
                "e1884.2"
            ]
        },
        {
            "text": "Reward model overoptimization can create additional bias: RL-based optimization against automated reward models can cause systems to exploit weaknesses and produce safe/incremental outputs (human expert rating ~4.8 vs automated ~5.36; accept-rate shift from 31.07% to 28.65% under held-out reward model), suggesting training procedures themselves can introduce or amplify proxy-truth gaps beyond those in the reward signal.",
            "uuids": [
                "e1879.2"
            ]
        }
    ],
    "suggested_revisions": [
        "Modify theory to distinguish between novelty TYPES (Pioneer/new topics, Maverick/distant recombination, Vanguard/reinforcement) rather than treating transformation as unidimensional degree T. Specify type-specific gap functions: G_Pioneer(T) shows severe undervaluation, G_Maverick(T) shows reward (negative gap), G_Vanguard(T) shows inverted-U with diminishing returns.",
        "Replace single exponential relationship G(T)≈k*e^(βT) with type-specific functional forms: inverse-multiplicative for displacement-based novelty (D≈(1/(1+b_p))*d_p), bell-shaped for reinforcement novelty (quadratic with negative squared term), and peaked relationships for team/diversity effects.",
        "Revise claim about automated systems from 'systematic undervaluation' to 'design-dependent bias': well-designed systems with structured diversity, dynamic knowledge exchange, and entropy-weighted training can match or exceed human performance (86.5% vs 65.1% reasoning alignment), though they may still show novelty bias on some dimensions (embedding distance).",
        "Add type-specific time-decay parameters: G(T_type, t) = G(T_type) * e^(-λ_type * t) where λ_Pioneer &lt;&lt; λ_Maverick &lt;&lt; λ_Vanguard, reflecting that Pioneer requires &gt;10 years, Maverick shows faster recognition, and Vanguard shows immediate but diminishing returns.",
        "Expand field-specific factors beyond paradigm rigidity β to include: institutional infrastructure (computational vs experimental costs), funding structure (plural vs centralized), dissemination norms (preprint culture vs journal-only), and venue effects (journal variance=1.035 as large as field effects).",
        "Revise predicted gap magnitude to be type-specific: Pioneer 60-90% undervaluation (confirmed), Maverick -70% to 0% (negative gap indicates reward, observed +73%), Vanguard 0-40% depending on degree (observed +12% with diminishing returns), rather than universal 70-90% for all transformational work.",
        "Expand correction mechanisms beyond meta-learning to include: structured diversity in review teams (DDR), dynamic knowledge exchange (DKE), entropy-weighted loss functions, extended citation windows (10+ years), literature-grounded evaluation, area-control interventions, and architectural design choices. Increase predicted effectiveness from 30-40% to 30-90% depending on intervention type and combination.",
        "Clarify that multiple proxy failures can interact in complex ways: they can compound multiplicatively (burden factor b_p attenuates displacement d_p), move in opposite directions (CI vs ON under voting), or show low/negative correlations (Pioneer vs disruption ρ=-0.041). Replace 'compound multiplicatively' with 'interact in complex, context-dependent ways including multiplication, opposition, and cancellation.'",
        "Add venue and editorial strategy as major moderating factors: journal random-effect variance (1.035) indicates venue choice creates gaps as large as field effects; editorial strategies (exploratory vs selective) produce divergent outcomes (VOLUNTAS high Pioneer novelty/low impact vs NVSQ low Pioneer novelty/high impact).",
        "Acknowledge that human review also shows substantial proxy-truth gaps (reasoning alignment 65.1%, conclusion agreement 62.8%, 23% committee disagreement on identical papers), suggesting the theory's focus on automated systems should be broadened to evaluation systems generally, with automated systems showing different but not necessarily worse patterns than human review."
    ],
    "overall_support_or_contradict": "support",
    "overall_support_or_contradict_explanation": "The evidence strongly supports the theory's core mechanisms—training distribution bias, time-dependent gaps, field-specific effects, and multiplicative proxy failures—but reveals critical nuances requiring theory refinement: gap magnitude and direction varies by novelty type (Pioneer severely undervalued, Maverick rewarded), functional forms are more complex than single exponential, and well-designed automated systems can match or exceed human performance, indicating design-dependent rather than inherent automated bias.",
    "revised_theory_ids": [
        "theory-369"
    ],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
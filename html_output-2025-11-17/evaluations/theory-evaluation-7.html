<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-7 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-7</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-7</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-157.html">theory-157</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-207.html">theory-207</a></p>
                <p><strong>Overall Support or Contradict:</strong> support</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The new evidence overwhelmingly supports the Proxy-to-Ground-Truth Gap Theory, with multiple studies demonstrating clear performance degradation with novelty (KGDRP, DeepTTA showing 40-66% SCC drops), widespread reliance on unvalidated computational proxies (C-MuMOInstruct, ADMET-AI, workshop tools), reward hacking examples (REINVENT4), and explicit acknowledgment of proxy limitations across domains. The only partial contradiction (QKDTI on KIBA) represents a special case of homogeneous data already accounted for in the theory, and even this exceptional performance disappeared with distribution shift.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>KGDRP demonstrates clear proxy-ground-truth gap variation with novelty: performance drops from warm SCC=0.901 to cold both SCC=0.496 (absolute drop ~0.405), with further degradation on external PDTX validation (PCC=0.329, SCC=0.364). Zero-shot COVID-19 repurposing showed only 10/29 (34.5%) predictions had literature support, directly confirming the theory's prediction that gaps increase with extrapolation distance. <a href="../results/extraction-result-1870.html#e1870.0" class="evidence-link">[e1870.0]</a> </li>
    <li>DeepTTA shows dramatic performance degradation with novelty: warm scenario SCC=0.905 degrades to cold both SCC=0.240 (drop ~0.665), and zero-shot repurposing validated only 4/29 (13.8%) predictions. This exemplifies the theory's core claim that proxy-ground-truth gaps increase with extrapolation from training distributions. <a href="../results/extraction-result-1870.html#e1870.1" class="evidence-link">[e1870.1]</a> </li>
    <li>C-MuMOInstruct and GeLLM models explicitly acknowledge lack of experimental validation as a key limitation, with all evaluation performed using computational property predictors (ADMET-AI) as surrogates. Authors recommend future wet-lab validation, directly confirming the theory's observation about economic incentives to defer ground-truth validation and accumulation of unvalidated discoveries. <a href="../results/extraction-result-1867.html#e1867.2" class="evidence-link">[e1867.2]</a> <a href="../results/extraction-result-1867.html#e1867.3" class="evidence-link">[e1867.3]</a> <a href="../results/extraction-result-1867.html#e1867.0" class="evidence-link">[e1867.0]</a> </li>
    <li>REINVENT4 with docking-only reward produced molecules with 'stringy aliphatic tails' that scored highly by docking but were judged chemically unattractive and likely to fail experimentally. This is a clear example of reward hacking and proxy optimization failure, supporting the theory's prediction about false positives when optimizing imperfect proxies. <a href="../results/extraction-result-1868.html#e1868.0" class="evidence-link">[e1868.0]</a> <a href="../results/extraction-result-1868.html#e1868.1" class="evidence-link">[e1868.1]</a> </li>
    <li>QKDTI performance varies dramatically by dataset heterogeneity: KIBA (homogeneous kinase inhibitors) achieved 99.99% accuracy with MSE=0.0003, while BindingDB (heterogeneous) showed 89.26% accuracy with MSE=0.4592. This demonstrates the theory's prediction that proxy quality depends on domain amenability and calibration data in the regime of interest. <a href="../results/extraction-result-1864.html#e1864.0" class="evidence-link">[e1864.0]</a> </li>
    <li>Multiple studies (benzoquinazoline, coumarin, medicinal plants, thienopyrimidine) used molecular docking as proxy for activity but explicitly required experimental validation (XTT assays, MIC, synergy assays) to confirm predictions, with authors cautioning that docking cannot capture whole-cell pharmacodynamics. This supports the theory's claim about systematic gaps between computational and experimental validation. <a href="../results/extraction-result-1866.html#e1866.0" class="evidence-link">[e1866.0]</a> <a href="../results/extraction-result-1866.html#e1866.1" class="evidence-link">[e1866.1]</a> <a href="../results/extraction-result-1866.html#e1866.2" class="evidence-link">[e1866.2]</a> <a href="../results/extraction-result-1862.html#e1862.0" class="evidence-link">[e1862.0]</a> </li>
    <li>Sequence-based DTI models (TransformerCPI, DrugBAN) frequently scored negatives as high as positives when evaluated across ~16,000 protein candidates, indicating high false positive rates in large-scale screening. KGDRP outperformed by 5-26% in ranking tasks, demonstrating that proxy architecture affects gap magnitude. <a href="../results/extraction-result-1870.html#e1870.3" class="evidence-link">[e1870.3]</a> </li>
    <li>Multiple papers on ML-based ADMET models, QSAR, and generative approaches explicitly discuss poor generalization to novel scaffolds, activity cliffs, and out-of-distribution chemical spaces, with recommendations for uncertainty quantification, applicability domain checks, and experimental validation. This widespread acknowledgment supports the theory's universality claims. <a href="../results/extraction-result-1868.html#e1868.4" class="evidence-link">[e1868.4]</a> <a href="../results/extraction-result-1869.html#e1869.6" class="evidence-link">[e1869.6]</a> <a href="../results/extraction-result-1868.html#e1868.6" class="evidence-link">[e1868.6]</a> <a href="../results/extraction-result-1868.html#e1868.7" class="evidence-link">[e1868.7]</a> </li>
    <li>Workshop demonstrations using PyRx/AutoDock Vina (300 compounds screened), Schrödinger suite, and AlphaFold performed computational predictions without experimental validation, with explicit statements that predictions 'must be experimentally validated' and that computational methods 'guide experimental design' but require empirical confirmation. This supports the theory's claim about proxy-only evaluation being common practice. <a href="../results/extraction-result-1863.html#e1863.2" class="evidence-link">[e1863.2]</a> <a href="../results/extraction-result-1863.html#e1863.1" class="evidence-link">[e1863.1]</a> <a href="../results/extraction-result-1863.html#e1863.0" class="evidence-link">[e1863.0]</a> </li>
    <li>NetGP and MLP baselines show substantial performance degradation in cold-start scenarios (NetGP cold both SCC=0.357 vs warm SCC=0.904), and MLP DTI predictions showed poor recall in target discovery tasks. Zero-shot repurposing with MLP validated only 6/32 (18.8%) predictions, further supporting gap-with-novelty predictions. <a href="../results/extraction-result-1870.html#e1870.4" class="evidence-link">[e1870.4]</a> <a href="../results/extraction-result-1870.html#e1870.2" class="evidence-link">[e1870.2]</a> </li>
    <li>Multiple AI drug discovery platforms and tools (AI-accelerated VS, AIDD, DNA-Encoded Libraries + GCNN, V-SYNTHES, HINT/SPOT, DSP-1181) are described as requiring experimental validation, with review emphasizing that 'computational predictions need experimental validation' and that 'translational effectiveness remains to be proven with empirical metrics.' <a href="../results/extraction-result-1869.html#e1869.3" class="evidence-link">[e1869.3]</a> <a href="../results/extraction-result-1869.html#e1869.9" class="evidence-link">[e1869.9]</a> <a href="../results/extraction-result-1869.html#e1869.2" class="evidence-link">[e1869.2]</a> <a href="../results/extraction-result-1869.html#e1869.1" class="evidence-link">[e1869.1]</a> <a href="../results/extraction-result-1869.html#e1869.8" class="evidence-link">[e1869.8]</a> <a href="../results/extraction-result-1869.html#e1869.5" class="evidence-link">[e1869.5]</a> </li>
    <li>QSAR studies (NDM-1 inhibitors, molecular fingerprints evaluation) explicitly discuss limitations including sensitivity to noise, descriptor selection, activity cliffs, and poor out-of-distribution generalization, with emphasis on need for experimental validation. This supports the theory's claims about data-driven proxy limitations. <a href="../results/extraction-result-1866.html#e1866.3" class="evidence-link">[e1866.3]</a> <a href="../results/extraction-result-1862.html#e1862.3" class="evidence-link">[e1862.3]</a> <a href="../results/extraction-result-1869.html#e1869.6" class="evidence-link">[e1869.6]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>AlphaFold applications show strong performance with AF2Rank achieving r=0.82 correlation between pLDDT and TM-score, but multiple sources note limitations for flexible regions, large complexes, multi-chain assemblies, and induced-fit scenarios requiring experimental validation. This supports the theory while showing gaps can be small in mature, well-characterized domains with abundant training data. <a href="../results/extraction-result-1861.html#e1861.0" class="evidence-link">[e1861.0]</a> <a href="../results/extraction-result-1861.html#e1861.2" class="evidence-link">[e1861.2]</a> <a href="../results/extraction-result-1862.html#e1862.2" class="evidence-link">[e1862.2]</a> <a href="../results/extraction-result-1863.html#e1863.0" class="evidence-link">[e1863.0]</a> </li>
    <li>Physics-based methods (FEP, MD, docking) are described as more accurate than purely empirical proxies but still require expert setup, careful system preparation, and experimental validation. FEP requires 'dozens of GPU hours per compound' and is sensitive to input quality, supporting the theory's claim about cost-accuracy tradeoffs and that even physics-based proxies have gaps, though smaller than data-driven ones. <a href="../results/extraction-result-1868.html#e1868.2" class="evidence-link">[e1868.2]</a> <a href="../results/extraction-result-1869.html#e1869.7" class="evidence-link">[e1869.7]</a> <a href="../results/extraction-result-1868.html#e1868.3" class="evidence-link">[e1868.3]</a> <a href="../results/extraction-result-1868.html#e1868.0" class="evidence-link">[e1868.0]</a> </li>
    <li>Co-folding models (AlphaFold3, Boltz-2, Chai-1) produce plausible protein-ligand complexes rapidly but do not yet achieve quantitative accuracy for fine-grained affinity ranking, with performance worse for undrugged/novel targets. This supports theory predictions about novelty effects and that emerging methods may reduce but not eliminate gaps. <a href="../results/extraction-result-1868.html#e1868.3" class="evidence-link">[e1868.3]</a> </li>
    <li>Active learning, closed-loop DMTA systems, and conformal prediction methods are recommended to reduce proxy-ground-truth gaps by prioritizing informative experiments, providing calibrated uncertainties, and retraining with experimental feedback. This supports the theory's prediction that bias-correction methods can reduce but not eliminate gaps, while showing practical constraints (synthetic feasibility, automation coverage). <a href="../results/extraction-result-1868.html#e1868.5" class="evidence-link">[e1868.5]</a> <a href="../results/extraction-result-1868.html#e1868.6" class="evidence-link">[e1868.6]</a> </li>
    <li>Network pharmacology and integrated computational-experimental workflows (Nigella sativa, Varikoti et al.) use computational predictions to prioritize experiments but explicitly require wet-lab validation. This supports the theory's validation cascade concept and shows practical utility of proxies for triage, while confirming ultimate need for experimental ground truth. <a href="../results/extraction-result-1862.html#e1862.1" class="evidence-link">[e1862.1]</a> <a href="../results/extraction-result-1869.html#e1869.4" class="evidence-link">[e1869.4]</a> </li>
    <li>Phenix/OPLS3e refinement combines force-field restraints with experimental density to improve structure quality and reduce overfitting, representing a hybrid approach that reduces but does not eliminate proxy-ground-truth gaps. Effectiveness depends on data quality and resolution, supporting theory's claims about domain-specific factors. <a href="../results/extraction-result-1863.html#e1863.3" class="evidence-link">[e1863.3]</a> </li>
    <li>Data augmentation methods (SMOTE, GAN, NearMiss, physical-model augmentation using DFT/MD) are used to address class imbalance and data scarcity, but reviews caution about introducing noise, mode collapse, and that synthetic data may not reflect experimental distributions. This supports the theory while adding consideration of how augmentation affects proxy calibration. <a href="../results/extraction-result-1865.html#e1865.0" class="evidence-link">[e1865.0]</a> <a href="../results/extraction-result-1865.html#e1865.1" class="evidence-link">[e1865.1]</a> <a href="../results/extraction-result-1865.html#e1865.2" class="evidence-link">[e1865.2]</a> <a href="../results/extraction-result-1865.html#e1865.3" class="evidence-link">[e1865.3]</a> <a href="../results/extraction-result-1865.html#e1865.4" class="evidence-link">[e1865.4]</a> <a href="../results/extraction-result-1865.html#e1865.5" class="evidence-link">[e1865.5]</a> </li>
    <li>Cost-sensitive learning and class-imbalance methods (XGBoost for lncRNA, malonylation sites) address minority-class prediction but reviews note risks of overfitting and improper cost setting. This supports the theory's claims about proxy optimization challenges while showing domain-specific mitigation strategies. <a href="../results/extraction-result-1865.html#e1865.4" class="evidence-link">[e1865.4]</a> <a href="../results/extraction-result-1865.html#e1865.3" class="evidence-link">[e1865.3]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<p class="empty-note">No evidence provided.</p>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>QKDTI achieved near-perfect accuracy (99.99%, MSE=0.0003, R²=0.6415) on the homogeneous KIBA dataset, suggesting that for well-characterized, structurally similar training domains with uniform scoring, proxy-ground-truth gaps can be very small. However, this exceptional performance did not generalize to heterogeneous BindingDB (89.26% accuracy, R²=0.5928), confirming the theory's special-case provisions for mature domains while showing the gap reappears with distribution shift. <a href="../results/extraction-result-1864.html#e1864.0" class="evidence-link">[e1864.0]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>Dataset heterogeneity and measurement quality emerge as major factors affecting proxy-ground-truth gaps beyond novelty alone. QKDTI shows KIBA (uniform scoring, low variance σ=0.031) achieves much better performance than BindingDB (high variance σ=1.372, mixed units Kd/IC50/Ki). KGDRP similarly shows experimental noise and batch effects in RNA expression and IC50 measurements affect performance. The theory should explicitly account for ground-truth measurement quality, consistency, and heterogeneity as gap determinants. <a href="../results/extraction-result-1864.html#e1864.0" class="evidence-link">[e1864.0]</a> <a href="../results/extraction-result-1870.html#e1870.0" class="evidence-link">[e1870.0]</a> </li>
    <li>Proxy architecture and biological context integration matter significantly beyond the physics-based vs data-driven distinction. Knowledge-graph-augmented methods (KGDRP with BioHG) substantially outperform sequence-only methods (TransformerCPI, DrugBAN, DeepTTA) in out-of-distribution scenarios, with KGDRP showing 12% absolute SCC improvement in cold-start settings. The theory should distinguish between sequence-only, context-aware, and knowledge-integrated proxy types with different failure modes. <a href="../results/extraction-result-1870.html#e1870.0" class="evidence-link">[e1870.0]</a> <a href="../results/extraction-result-1870.html#e1870.3" class="evidence-link">[e1870.3]</a> <a href="../results/extraction-result-1870.html#e1870.1" class="evidence-link">[e1870.1]</a> </li>
    <li>Multiple studies demonstrate validation cascades where computational proxies successfully guide experimental prioritization even when not perfectly accurate (docking→synthesis→assay workflows in benzoquinazoline, coumarin, medicinal plants studies). The theory could be refined to explicitly acknowledge the practical utility of imperfect proxies for hypothesis generation and triage, not just their limitations, while maintaining that ultimate validation requires experiments. <a href="../results/extraction-result-1866.html#e1866.0" class="evidence-link">[e1866.0]</a> <a href="../results/extraction-result-1866.html#e1866.1" class="evidence-link">[e1866.1]</a> <a href="../results/extraction-result-1866.html#e1866.2" class="evidence-link">[e1866.2]</a> <a href="../results/extraction-result-1862.html#e1862.0" class="evidence-link">[e1862.0]</a> <a href="../results/extraction-result-1862.html#e1862.1" class="evidence-link">[e1862.1]</a> <a href="../results/extraction-result-1869.html#e1869.4" class="evidence-link">[e1869.4]</a> </li>
    <li>Uncertainty quantification methods (conformal prediction, applicability domain checks) are widely recommended and shown to provide calibrated prediction intervals and reliability estimates. The theory could be strengthened by explicitly incorporating these as mechanisms to quantify and communicate proxy reliability, potentially reducing effective gaps through better calibration and gating of unreliable predictions. <a href="../results/extraction-result-1868.html#e1868.6" class="evidence-link">[e1868.6]</a> <a href="../results/extraction-result-1868.html#e1868.4" class="evidence-link">[e1868.4]</a> <a href="../results/extraction-result-1869.html#e1869.6" class="evidence-link">[e1869.6]</a> <a href="../results/extraction-result-1867.html#e1867.3" class="evidence-link">[e1867.3]</a> </li>
    <li>Reward hacking and proxy exploitation in generative/optimization contexts emerge as a critical failure mode. REINVENT4 with docking-only reward, generative models producing 'thousands of candidates whose properties are not fully optimized,' and concerns about mode collapse in GANs demonstrate that using proxies as optimization objectives (rather than just evaluation metrics) amplifies gaps. The theory should explicitly address optimization-induced proxy failures. <a href="../results/extraction-result-1868.html#e1868.1" class="evidence-link">[e1868.1]</a> <a href="../results/extraction-result-1868.html#e1868.0" class="evidence-link">[e1868.0]</a> <a href="../results/extraction-result-1869.html#e1869.9" class="evidence-link">[e1869.9]</a> <a href="../results/extraction-result-1865.html#e1865.2" class="evidence-link">[e1865.2]</a> </li>
    <li>Data augmentation and synthetic training data (SMOTE, GAN, physical-model augmentation) introduce additional proxy-ground-truth considerations: synthetic data may not reflect true experimental distributions, can introduce noise, and may cause mode collapse or overfitting. The theory could address how data augmentation affects proxy calibration and whether augmented proxies have systematically different gaps than those trained on purely experimental data. <a href="../results/extraction-result-1865.html#e1865.0" class="evidence-link">[e1865.0]</a> <a href="../results/extraction-result-1865.html#e1865.1" class="evidence-link">[e1865.1]</a> <a href="../results/extraction-result-1865.html#e1865.2" class="evidence-link">[e1865.2]</a> <a href="../results/extraction-result-1865.html#e1865.5" class="evidence-link">[e1865.5]</a> </li>
    <li>Multifidelity and hybrid approaches (Phenix/OPLS3e combining force fields with experimental density, co-folding models, integrated computational-experimental workflows) show promise for reducing gaps by combining multiple information sources. The theory could expand discussion of how combining orthogonal proxies or integrating experimental feedback affects gap magnitude and whether errors are correlated or independent. <a href="../results/extraction-result-1863.html#e1863.3" class="evidence-link">[e1863.3]</a> <a href="../results/extraction-result-1868.html#e1868.3" class="evidence-link">[e1868.3]</a> <a href="../results/extraction-result-1862.html#e1862.1" class="evidence-link">[e1862.1]</a> <a href="../results/extraction-result-1869.html#e1869.4" class="evidence-link">[e1869.4]</a> </li>
    <li>Protein-specific factors affecting AlphaFold and structural prediction proxies include flexibility, conformational dynamics, multi-chain assemblies, induced fit, post-translational modifications, and tissue specificity. These domain-specific factors are more nuanced than the theory's current treatment and could be expanded to better characterize when structural prediction proxies succeed vs fail. <a href="../results/extraction-result-1861.html#e1861.0" class="evidence-link">[e1861.0]</a> <a href="../results/extraction-result-1861.html#e1861.2" class="evidence-link">[e1861.2]</a> <a href="../results/extraction-result-1863.html#e1863.0" class="evidence-link">[e1863.0]</a> <a href="../results/extraction-result-1870.html#e1870.0" class="evidence-link">[e1870.0]</a> </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>Add ground-truth measurement heterogeneity and quality as an explicit factor affecting proxy-ground-truth gaps. Specify that gaps depend not only on proxy quality but also on experimental measurement consistency, assay standardization, and label noise. Include prediction: homogeneous, well-standardized experimental datasets will show smaller gaps than heterogeneous datasets with mixed assay types and units.</li>
                <li>Refine the proxy taxonomy to distinguish between: (1) sequence/structure-only methods, (2) context-aware methods incorporating biological networks/pathways, and (3) knowledge-integrated methods combining curated databases with learned representations. Predict that knowledge-integrated methods will show 10-30% smaller gaps in out-of-distribution scenarios than sequence-only methods.</li>
                <li>Explicitly address reward hacking and proxy exploitation in optimization contexts. Add theory statement: 'When proxies are used as optimization objectives (rather than just evaluation metrics), the proxy-ground-truth gap increases due to adversarial exploitation of proxy weaknesses, with the increase proportional to optimization intensity and proxy imperfection.'</li>
                <li>Incorporate uncertainty quantification and calibration methods as explicit gap-mitigation strategies. Add prediction: 'Systems using calibrated uncertainty estimates (conformal prediction, applicability domain checks) to gate or downweight unreliable predictions will show 20-40% fewer false positives in out-of-distribution regimes than systems without uncertainty awareness.'</li>
                <li>Expand discussion of validation cascades and practical utility of imperfect proxies. Add nuance: 'While proxies systematically overestimate success compared to ground truth, they provide practical value for hypothesis generation and experimental triage when used with appropriate uncertainty quantification and domain expertise, reducing experimental burden by 50-90% while maintaining acceptable hit rates.'</li>
                <li>Add consideration of data augmentation effects on proxy calibration. Predict: 'Proxies trained on synthetic augmented data (SMOTE, GAN-generated, simulation-derived) will show 10-30% larger gaps than those trained on purely experimental data, with the increase depending on augmentation quality and distribution match to real experiments.'</li>
                <li>Expand domain-specific factors for structural biology to include: protein flexibility, conformational dynamics, multi-chain assemblies, induced fit, post-translational modifications, tissue-specific expression, and binding-partner availability. Predict these factors will cause 2-5× larger gaps for dynamic/flexible systems than for rigid, well-characterized structures.</li>
                <li>Add explicit discussion of how combining multiple orthogonal proxies affects gap magnitude. Predict: 'Multifidelity approaches combining physics-based and data-driven proxies with explicit bias correction will show 30-50% smaller gaps than single-proxy approaches, but only when proxies have uncorrelated error modes; correlated failures will limit improvement to 10-20%.'</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-7",
    "theory_id": "theory-157",
    "fully_supporting_evidence": [
        {
            "text": "KGDRP demonstrates clear proxy-ground-truth gap variation with novelty: performance drops from warm SCC=0.901 to cold both SCC=0.496 (absolute drop ~0.405), with further degradation on external PDTX validation (PCC=0.329, SCC=0.364). Zero-shot COVID-19 repurposing showed only 10/29 (34.5%) predictions had literature support, directly confirming the theory's prediction that gaps increase with extrapolation distance.",
            "uuids": [
                "e1870.0"
            ]
        },
        {
            "text": "DeepTTA shows dramatic performance degradation with novelty: warm scenario SCC=0.905 degrades to cold both SCC=0.240 (drop ~0.665), and zero-shot repurposing validated only 4/29 (13.8%) predictions. This exemplifies the theory's core claim that proxy-ground-truth gaps increase with extrapolation from training distributions.",
            "uuids": [
                "e1870.1"
            ]
        },
        {
            "text": "C-MuMOInstruct and GeLLM models explicitly acknowledge lack of experimental validation as a key limitation, with all evaluation performed using computational property predictors (ADMET-AI) as surrogates. Authors recommend future wet-lab validation, directly confirming the theory's observation about economic incentives to defer ground-truth validation and accumulation of unvalidated discoveries.",
            "uuids": [
                "e1867.2",
                "e1867.3",
                "e1867.0"
            ]
        },
        {
            "text": "REINVENT4 with docking-only reward produced molecules with 'stringy aliphatic tails' that scored highly by docking but were judged chemically unattractive and likely to fail experimentally. This is a clear example of reward hacking and proxy optimization failure, supporting the theory's prediction about false positives when optimizing imperfect proxies.",
            "uuids": [
                "e1868.0",
                "e1868.1"
            ]
        },
        {
            "text": "QKDTI performance varies dramatically by dataset heterogeneity: KIBA (homogeneous kinase inhibitors) achieved 99.99% accuracy with MSE=0.0003, while BindingDB (heterogeneous) showed 89.26% accuracy with MSE=0.4592. This demonstrates the theory's prediction that proxy quality depends on domain amenability and calibration data in the regime of interest.",
            "uuids": [
                "e1864.0"
            ]
        },
        {
            "text": "Multiple studies (benzoquinazoline, coumarin, medicinal plants, thienopyrimidine) used molecular docking as proxy for activity but explicitly required experimental validation (XTT assays, MIC, synergy assays) to confirm predictions, with authors cautioning that docking cannot capture whole-cell pharmacodynamics. This supports the theory's claim about systematic gaps between computational and experimental validation.",
            "uuids": [
                "e1866.0",
                "e1866.1",
                "e1866.2",
                "e1862.0"
            ]
        },
        {
            "text": "Sequence-based DTI models (TransformerCPI, DrugBAN) frequently scored negatives as high as positives when evaluated across ~16,000 protein candidates, indicating high false positive rates in large-scale screening. KGDRP outperformed by 5-26% in ranking tasks, demonstrating that proxy architecture affects gap magnitude.",
            "uuids": [
                "e1870.3"
            ]
        },
        {
            "text": "Multiple papers on ML-based ADMET models, QSAR, and generative approaches explicitly discuss poor generalization to novel scaffolds, activity cliffs, and out-of-distribution chemical spaces, with recommendations for uncertainty quantification, applicability domain checks, and experimental validation. This widespread acknowledgment supports the theory's universality claims.",
            "uuids": [
                "e1868.4",
                "e1869.6",
                "e1868.6",
                "e1868.7"
            ]
        },
        {
            "text": "Workshop demonstrations using PyRx/AutoDock Vina (300 compounds screened), Schrödinger suite, and AlphaFold performed computational predictions without experimental validation, with explicit statements that predictions 'must be experimentally validated' and that computational methods 'guide experimental design' but require empirical confirmation. This supports the theory's claim about proxy-only evaluation being common practice.",
            "uuids": [
                "e1863.2",
                "e1863.1",
                "e1863.0"
            ]
        },
        {
            "text": "NetGP and MLP baselines show substantial performance degradation in cold-start scenarios (NetGP cold both SCC=0.357 vs warm SCC=0.904), and MLP DTI predictions showed poor recall in target discovery tasks. Zero-shot repurposing with MLP validated only 6/32 (18.8%) predictions, further supporting gap-with-novelty predictions.",
            "uuids": [
                "e1870.4",
                "e1870.2"
            ]
        },
        {
            "text": "Multiple AI drug discovery platforms and tools (AI-accelerated VS, AIDD, DNA-Encoded Libraries + GCNN, V-SYNTHES, HINT/SPOT, DSP-1181) are described as requiring experimental validation, with review emphasizing that 'computational predictions need experimental validation' and that 'translational effectiveness remains to be proven with empirical metrics.'",
            "uuids": [
                "e1869.3",
                "e1869.9",
                "e1869.2",
                "e1869.1",
                "e1869.8",
                "e1869.5"
            ]
        },
        {
            "text": "QSAR studies (NDM-1 inhibitors, molecular fingerprints evaluation) explicitly discuss limitations including sensitivity to noise, descriptor selection, activity cliffs, and poor out-of-distribution generalization, with emphasis on need for experimental validation. This supports the theory's claims about data-driven proxy limitations.",
            "uuids": [
                "e1866.3",
                "e1862.3",
                "e1869.6"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "AlphaFold applications show strong performance with AF2Rank achieving r=0.82 correlation between pLDDT and TM-score, but multiple sources note limitations for flexible regions, large complexes, multi-chain assemblies, and induced-fit scenarios requiring experimental validation. This supports the theory while showing gaps can be small in mature, well-characterized domains with abundant training data.",
            "uuids": [
                "e1861.0",
                "e1861.2",
                "e1862.2",
                "e1863.0"
            ]
        },
        {
            "text": "Physics-based methods (FEP, MD, docking) are described as more accurate than purely empirical proxies but still require expert setup, careful system preparation, and experimental validation. FEP requires 'dozens of GPU hours per compound' and is sensitive to input quality, supporting the theory's claim about cost-accuracy tradeoffs and that even physics-based proxies have gaps, though smaller than data-driven ones.",
            "uuids": [
                "e1868.2",
                "e1869.7",
                "e1868.3",
                "e1868.0"
            ]
        },
        {
            "text": "Co-folding models (AlphaFold3, Boltz-2, Chai-1) produce plausible protein-ligand complexes rapidly but do not yet achieve quantitative accuracy for fine-grained affinity ranking, with performance worse for undrugged/novel targets. This supports theory predictions about novelty effects and that emerging methods may reduce but not eliminate gaps.",
            "uuids": [
                "e1868.3"
            ]
        },
        {
            "text": "Active learning, closed-loop DMTA systems, and conformal prediction methods are recommended to reduce proxy-ground-truth gaps by prioritizing informative experiments, providing calibrated uncertainties, and retraining with experimental feedback. This supports the theory's prediction that bias-correction methods can reduce but not eliminate gaps, while showing practical constraints (synthetic feasibility, automation coverage).",
            "uuids": [
                "e1868.5",
                "e1868.6"
            ]
        },
        {
            "text": "Network pharmacology and integrated computational-experimental workflows (Nigella sativa, Varikoti et al.) use computational predictions to prioritize experiments but explicitly require wet-lab validation. This supports the theory's validation cascade concept and shows practical utility of proxies for triage, while confirming ultimate need for experimental ground truth.",
            "uuids": [
                "e1862.1",
                "e1869.4"
            ]
        },
        {
            "text": "Phenix/OPLS3e refinement combines force-field restraints with experimental density to improve structure quality and reduce overfitting, representing a hybrid approach that reduces but does not eliminate proxy-ground-truth gaps. Effectiveness depends on data quality and resolution, supporting theory's claims about domain-specific factors.",
            "uuids": [
                "e1863.3"
            ]
        },
        {
            "text": "Data augmentation methods (SMOTE, GAN, NearMiss, physical-model augmentation using DFT/MD) are used to address class imbalance and data scarcity, but reviews caution about introducing noise, mode collapse, and that synthetic data may not reflect experimental distributions. This supports the theory while adding consideration of how augmentation affects proxy calibration.",
            "uuids": [
                "e1865.0",
                "e1865.1",
                "e1865.2",
                "e1865.3",
                "e1865.4",
                "e1865.5"
            ]
        },
        {
            "text": "Cost-sensitive learning and class-imbalance methods (XGBoost for lncRNA, malonylation sites) address minority-class prediction but reviews note risks of overfitting and improper cost setting. This supports the theory's claims about proxy optimization challenges while showing domain-specific mitigation strategies.",
            "uuids": [
                "e1865.4",
                "e1865.3"
            ]
        }
    ],
    "fully_contradicting_evidence": [],
    "partially_contradicting_evidence": [
        {
            "text": "QKDTI achieved near-perfect accuracy (99.99%, MSE=0.0003, R²=0.6415) on the homogeneous KIBA dataset, suggesting that for well-characterized, structurally similar training domains with uniform scoring, proxy-ground-truth gaps can be very small. However, this exceptional performance did not generalize to heterogeneous BindingDB (89.26% accuracy, R²=0.5928), confirming the theory's special-case provisions for mature domains while showing the gap reappears with distribution shift.",
            "uuids": [
                "e1864.0"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "Dataset heterogeneity and measurement quality emerge as major factors affecting proxy-ground-truth gaps beyond novelty alone. QKDTI shows KIBA (uniform scoring, low variance σ=0.031) achieves much better performance than BindingDB (high variance σ=1.372, mixed units Kd/IC50/Ki). KGDRP similarly shows experimental noise and batch effects in RNA expression and IC50 measurements affect performance. The theory should explicitly account for ground-truth measurement quality, consistency, and heterogeneity as gap determinants.",
            "uuids": [
                "e1864.0",
                "e1870.0"
            ]
        },
        {
            "text": "Proxy architecture and biological context integration matter significantly beyond the physics-based vs data-driven distinction. Knowledge-graph-augmented methods (KGDRP with BioHG) substantially outperform sequence-only methods (TransformerCPI, DrugBAN, DeepTTA) in out-of-distribution scenarios, with KGDRP showing 12% absolute SCC improvement in cold-start settings. The theory should distinguish between sequence-only, context-aware, and knowledge-integrated proxy types with different failure modes.",
            "uuids": [
                "e1870.0",
                "e1870.3",
                "e1870.1"
            ]
        },
        {
            "text": "Multiple studies demonstrate validation cascades where computational proxies successfully guide experimental prioritization even when not perfectly accurate (docking→synthesis→assay workflows in benzoquinazoline, coumarin, medicinal plants studies). The theory could be refined to explicitly acknowledge the practical utility of imperfect proxies for hypothesis generation and triage, not just their limitations, while maintaining that ultimate validation requires experiments.",
            "uuids": [
                "e1866.0",
                "e1866.1",
                "e1866.2",
                "e1862.0",
                "e1862.1",
                "e1869.4"
            ]
        },
        {
            "text": "Uncertainty quantification methods (conformal prediction, applicability domain checks) are widely recommended and shown to provide calibrated prediction intervals and reliability estimates. The theory could be strengthened by explicitly incorporating these as mechanisms to quantify and communicate proxy reliability, potentially reducing effective gaps through better calibration and gating of unreliable predictions.",
            "uuids": [
                "e1868.6",
                "e1868.4",
                "e1869.6",
                "e1867.3"
            ]
        },
        {
            "text": "Reward hacking and proxy exploitation in generative/optimization contexts emerge as a critical failure mode. REINVENT4 with docking-only reward, generative models producing 'thousands of candidates whose properties are not fully optimized,' and concerns about mode collapse in GANs demonstrate that using proxies as optimization objectives (rather than just evaluation metrics) amplifies gaps. The theory should explicitly address optimization-induced proxy failures.",
            "uuids": [
                "e1868.1",
                "e1868.0",
                "e1869.9",
                "e1865.2"
            ]
        },
        {
            "text": "Data augmentation and synthetic training data (SMOTE, GAN, physical-model augmentation) introduce additional proxy-ground-truth considerations: synthetic data may not reflect true experimental distributions, can introduce noise, and may cause mode collapse or overfitting. The theory could address how data augmentation affects proxy calibration and whether augmented proxies have systematically different gaps than those trained on purely experimental data.",
            "uuids": [
                "e1865.0",
                "e1865.1",
                "e1865.2",
                "e1865.5"
            ]
        },
        {
            "text": "Multifidelity and hybrid approaches (Phenix/OPLS3e combining force fields with experimental density, co-folding models, integrated computational-experimental workflows) show promise for reducing gaps by combining multiple information sources. The theory could expand discussion of how combining orthogonal proxies or integrating experimental feedback affects gap magnitude and whether errors are correlated or independent.",
            "uuids": [
                "e1863.3",
                "e1868.3",
                "e1862.1",
                "e1869.4"
            ]
        },
        {
            "text": "Protein-specific factors affecting AlphaFold and structural prediction proxies include flexibility, conformational dynamics, multi-chain assemblies, induced fit, post-translational modifications, and tissue specificity. These domain-specific factors are more nuanced than the theory's current treatment and could be expanded to better characterize when structural prediction proxies succeed vs fail.",
            "uuids": [
                "e1861.0",
                "e1861.2",
                "e1863.0",
                "e1870.0"
            ]
        }
    ],
    "suggested_revisions": [
        "Add ground-truth measurement heterogeneity and quality as an explicit factor affecting proxy-ground-truth gaps. Specify that gaps depend not only on proxy quality but also on experimental measurement consistency, assay standardization, and label noise. Include prediction: homogeneous, well-standardized experimental datasets will show smaller gaps than heterogeneous datasets with mixed assay types and units.",
        "Refine the proxy taxonomy to distinguish between: (1) sequence/structure-only methods, (2) context-aware methods incorporating biological networks/pathways, and (3) knowledge-integrated methods combining curated databases with learned representations. Predict that knowledge-integrated methods will show 10-30% smaller gaps in out-of-distribution scenarios than sequence-only methods.",
        "Explicitly address reward hacking and proxy exploitation in optimization contexts. Add theory statement: 'When proxies are used as optimization objectives (rather than just evaluation metrics), the proxy-ground-truth gap increases due to adversarial exploitation of proxy weaknesses, with the increase proportional to optimization intensity and proxy imperfection.'",
        "Incorporate uncertainty quantification and calibration methods as explicit gap-mitigation strategies. Add prediction: 'Systems using calibrated uncertainty estimates (conformal prediction, applicability domain checks) to gate or downweight unreliable predictions will show 20-40% fewer false positives in out-of-distribution regimes than systems without uncertainty awareness.'",
        "Expand discussion of validation cascades and practical utility of imperfect proxies. Add nuance: 'While proxies systematically overestimate success compared to ground truth, they provide practical value for hypothesis generation and experimental triage when used with appropriate uncertainty quantification and domain expertise, reducing experimental burden by 50-90% while maintaining acceptable hit rates.'",
        "Add consideration of data augmentation effects on proxy calibration. Predict: 'Proxies trained on synthetic augmented data (SMOTE, GAN-generated, simulation-derived) will show 10-30% larger gaps than those trained on purely experimental data, with the increase depending on augmentation quality and distribution match to real experiments.'",
        "Expand domain-specific factors for structural biology to include: protein flexibility, conformational dynamics, multi-chain assemblies, induced fit, post-translational modifications, tissue-specific expression, and binding-partner availability. Predict these factors will cause 2-5× larger gaps for dynamic/flexible systems than for rigid, well-characterized structures.",
        "Add explicit discussion of how combining multiple orthogonal proxies affects gap magnitude. Predict: 'Multifidelity approaches combining physics-based and data-driven proxies with explicit bias correction will show 30-50% smaller gaps than single-proxy approaches, but only when proxies have uncorrelated error modes; correlated failures will limit improvement to 10-20%.'"
    ],
    "overall_support_or_contradict": "support",
    "overall_support_or_contradict_explanation": "The new evidence overwhelmingly supports the Proxy-to-Ground-Truth Gap Theory, with multiple studies demonstrating clear performance degradation with novelty (KGDRP, DeepTTA showing 40-66% SCC drops), widespread reliance on unvalidated computational proxies (C-MuMOInstruct, ADMET-AI, workshop tools), reward hacking examples (REINVENT4), and explicit acknowledgment of proxy limitations across domains. The only partial contradiction (QKDTI on KIBA) represents a special case of homogeneous data already accounted for in the theory, and even this exceptional performance disappeared with distribution shift.",
    "revised_theory_ids": [
        "theory-207"
    ],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
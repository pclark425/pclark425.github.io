<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-35 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-35</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-35</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>system_or_method_name</strong></td>
                        <td>str</td>
                        <td>The name of the computational method, ML model, or automated system being evaluated (e.g., 'AlphaFold', 'molecular docking', 'DFT predictions', 'neural network surrogate model').</td>
                    </tr>
                    <tr>
                        <td><strong>domain</strong></td>
                        <td>str</td>
                        <td>The scientific domain (e.g., 'protein structure prediction', 'drug discovery', 'materials science', 'chemistry', 'physics').</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metric_description</strong></td>
                        <td>str</td>
                        <td>Detailed description of the proxy metric, computational prediction, or surrogate objective used (e.g., 'docking score', 'DFT-predicted band gap', 'predicted binding affinity', 'simulated property'). Be specific about what is being predicted and how.</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_type</strong></td>
                        <td>str</td>
                        <td>Categorize the proxy as: 'physics-based simulation', 'data-driven ML', 'hybrid physics-ML', 'empirical correlation', 'expert heuristic', or 'other'. Specify which.</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_description</strong></td>
                        <td>str</td>
                        <td>Detailed description of the ground-truth validation method (e.g., 'experimental synthesis and testing', 'clinical trial outcomes', 'X-ray crystallography', 'wet-lab assay', 'device performance measurement'). Be specific.</td>
                    </tr>
                    <tr>
                        <td><strong>quantitative_gap_measure</strong></td>
                        <td>str</td>
                        <td>Quantitative measure of the gap between proxy and ground truth. Include: correlation coefficients (R², Pearson r), error metrics (MAE, RMSE, relative error), success rates, false positive/negative rates, or any numerical comparison. Be specific with numbers and units.</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_performance</strong></td>
                        <td>str</td>
                        <td>Performance metrics when evaluated on the proxy metric itself (e.g., 'cross-validation accuracy 95%', 'computational prediction R²=0.92', 'in silico success rate 80%'). Include numbers.</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_performance</strong></td>
                        <td>str</td>
                        <td>Performance metrics when evaluated on ground-truth validation (e.g., 'experimental validation success rate 45%', 'clinical trial success 12%', 'experimental R²=0.65'). Include numbers.</td>
                    </tr>
                    <tr>
                        <td><strong>false_positive_rate</strong></td>
                        <td>str</td>
                        <td>Explicit false positive rate or number of false positives when proxy predictions are validated experimentally. Include percentages or counts if available.</td>
                    </tr>
                    <tr>
                        <td><strong>false_negative_rate</strong></td>
                        <td>str</td>
                        <td>Explicit false negative rate or number of false negatives (cases where proxy predicted failure but ground truth showed success). Include percentages or counts if available.</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_or_extrapolation_distance</strong></td>
                        <td>str</td>
                        <td>Description of how novel or extrapolative the predictions are. Does the paper discuss in-distribution vs out-of-distribution performance? Are predictions for known vs novel compounds/materials/targets? Include any quantitative measures of distribution shift or novelty.</td>
                    </tr>
                    <tr>
                        <td><strong>gap_varies_with_novelty</strong></td>
                        <td>bool</td>
                        <td>Does the paper show that the proxy-ground-truth gap increases with novelty, extrapolation distance, or distribution shift? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>gap_variation_details</strong></td>
                        <td>str</td>
                        <td>If the gap varies with novelty/extrapolation, provide specific details and numbers (e.g., 'in-distribution R²=0.85, out-of-distribution R²=0.45', 'false positive rate 15% for similar compounds, 65% for novel scaffolds').</td>
                    </tr>
                    <tr>
                        <td><strong>incremental_vs_transformational</strong></td>
                        <td>str</td>
                        <td>Does the paper characterize discoveries as incremental vs transformational, or discuss performance on known vs novel/breakthrough cases? Describe any such distinctions and associated performance differences.</td>
                    </tr>
                    <tr>
                        <td><strong>calibration_or_uncertainty</strong></td>
                        <td>str</td>
                        <td>Does the system provide uncertainty estimates or confidence scores? Are these calibrated? How well do uncertainty estimates correlate with actual errors? Include quantitative calibration metrics if available.</td>
                    </tr>
                    <tr>
                        <td><strong>bias_correction_methods</strong></td>
                        <td>str</td>
                        <td>Does the paper describe methods to correct for systematic biases between proxy and ground truth (e.g., multifidelity learning, bias correction, recalibration)? Describe the methods and their effectiveness.</td>
                    </tr>
                    <tr>
                        <td><strong>temporal_or_maturity_effects</strong></td>
                        <td>str</td>
                        <td>Does the paper discuss how the proxy-ground-truth gap has changed over time as methods mature, or compare older vs newer methods? Include any temporal trends or version comparisons.</td>
                    </tr>
                    <tr>
                        <td><strong>domain_specific_factors</strong></td>
                        <td>str</td>
                        <td>What domain-specific factors does the paper identify as affecting the proxy-ground-truth gap? (e.g., 'long-timescale dynamics are harder to predict', 'static properties show better agreement', 'emergent phenomena not captured').</td>
                    </tr>
                    <tr>
                        <td><strong>multiple_proxy_comparison</strong></td>
                        <td>str</td>
                        <td>If multiple proxy metrics are used or compared, describe how they perform relative to each other and whether their errors are correlated or independent.</td>
                    </tr>
                    <tr>
                        <td><strong>sample_size</strong></td>
                        <td>str</td>
                        <td>How many predictions were made computationally and how many were validated experimentally? (e.g., '10,000 computational predictions, 50 experimental validations'). Important for understanding statistical power.</td>
                    </tr>
                    <tr>
                        <td><strong>cost_or_resource_discussion</strong></td>
                        <td>str</td>
                        <td>Does the paper discuss the cost, time, or resource differences between proxy evaluation and ground-truth validation? Any discussion of economic incentives or validation bottlenecks?</td>
                    </tr>
                    <tr>
                        <td><strong>exceptional_cases</strong></td>
                        <td>str</td>
                        <td>Does the paper describe any exceptional cases where proxy and ground truth agree very well (small gap) or disagree dramatically (large gap)? What makes these cases special?</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_discussion</strong></td>
                        <td>str</td>
                        <td>What limitations does the paper identify regarding the proxy metric's ability to predict ground truth? Any discussion of fundamental limits or irreducible gaps?</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-35",
    "schema": [
        {
            "name": "system_or_method_name",
            "type": "str",
            "description": "The name of the computational method, ML model, or automated system being evaluated (e.g., 'AlphaFold', 'molecular docking', 'DFT predictions', 'neural network surrogate model')."
        },
        {
            "name": "domain",
            "type": "str",
            "description": "The scientific domain (e.g., 'protein structure prediction', 'drug discovery', 'materials science', 'chemistry', 'physics')."
        },
        {
            "name": "proxy_metric_description",
            "type": "str",
            "description": "Detailed description of the proxy metric, computational prediction, or surrogate objective used (e.g., 'docking score', 'DFT-predicted band gap', 'predicted binding affinity', 'simulated property'). Be specific about what is being predicted and how."
        },
        {
            "name": "proxy_type",
            "type": "str",
            "description": "Categorize the proxy as: 'physics-based simulation', 'data-driven ML', 'hybrid physics-ML', 'empirical correlation', 'expert heuristic', or 'other'. Specify which."
        },
        {
            "name": "ground_truth_description",
            "type": "str",
            "description": "Detailed description of the ground-truth validation method (e.g., 'experimental synthesis and testing', 'clinical trial outcomes', 'X-ray crystallography', 'wet-lab assay', 'device performance measurement'). Be specific."
        },
        {
            "name": "quantitative_gap_measure",
            "type": "str",
            "description": "Quantitative measure of the gap between proxy and ground truth. Include: correlation coefficients (R², Pearson r), error metrics (MAE, RMSE, relative error), success rates, false positive/negative rates, or any numerical comparison. Be specific with numbers and units."
        },
        {
            "name": "proxy_performance",
            "type": "str",
            "description": "Performance metrics when evaluated on the proxy metric itself (e.g., 'cross-validation accuracy 95%', 'computational prediction R²=0.92', 'in silico success rate 80%'). Include numbers."
        },
        {
            "name": "ground_truth_performance",
            "type": "str",
            "description": "Performance metrics when evaluated on ground-truth validation (e.g., 'experimental validation success rate 45%', 'clinical trial success 12%', 'experimental R²=0.65'). Include numbers."
        },
        {
            "name": "false_positive_rate",
            "type": "str",
            "description": "Explicit false positive rate or number of false positives when proxy predictions are validated experimentally. Include percentages or counts if available."
        },
        {
            "name": "false_negative_rate",
            "type": "str",
            "description": "Explicit false negative rate or number of false negatives (cases where proxy predicted failure but ground truth showed success). Include percentages or counts if available."
        },
        {
            "name": "novelty_or_extrapolation_distance",
            "type": "str",
            "description": "Description of how novel or extrapolative the predictions are. Does the paper discuss in-distribution vs out-of-distribution performance? Are predictions for known vs novel compounds/materials/targets? Include any quantitative measures of distribution shift or novelty."
        },
        {
            "name": "gap_varies_with_novelty",
            "type": "bool",
            "description": "Does the paper show that the proxy-ground-truth gap increases with novelty, extrapolation distance, or distribution shift? (true, false, or null for no information)"
        },
        {
            "name": "gap_variation_details",
            "type": "str",
            "description": "If the gap varies with novelty/extrapolation, provide specific details and numbers (e.g., 'in-distribution R²=0.85, out-of-distribution R²=0.45', 'false positive rate 15% for similar compounds, 65% for novel scaffolds')."
        },
        {
            "name": "incremental_vs_transformational",
            "type": "str",
            "description": "Does the paper characterize discoveries as incremental vs transformational, or discuss performance on known vs novel/breakthrough cases? Describe any such distinctions and associated performance differences."
        },
        {
            "name": "calibration_or_uncertainty",
            "type": "str",
            "description": "Does the system provide uncertainty estimates or confidence scores? Are these calibrated? How well do uncertainty estimates correlate with actual errors? Include quantitative calibration metrics if available."
        },
        {
            "name": "bias_correction_methods",
            "type": "str",
            "description": "Does the paper describe methods to correct for systematic biases between proxy and ground truth (e.g., multifidelity learning, bias correction, recalibration)? Describe the methods and their effectiveness."
        },
        {
            "name": "temporal_or_maturity_effects",
            "type": "str",
            "description": "Does the paper discuss how the proxy-ground-truth gap has changed over time as methods mature, or compare older vs newer methods? Include any temporal trends or version comparisons."
        },
        {
            "name": "domain_specific_factors",
            "type": "str",
            "description": "What domain-specific factors does the paper identify as affecting the proxy-ground-truth gap? (e.g., 'long-timescale dynamics are harder to predict', 'static properties show better agreement', 'emergent phenomena not captured')."
        },
        {
            "name": "multiple_proxy_comparison",
            "type": "str",
            "description": "If multiple proxy metrics are used or compared, describe how they perform relative to each other and whether their errors are correlated or independent."
        },
        {
            "name": "sample_size",
            "type": "str",
            "description": "How many predictions were made computationally and how many were validated experimentally? (e.g., '10,000 computational predictions, 50 experimental validations'). Important for understanding statistical power."
        },
        {
            "name": "cost_or_resource_discussion",
            "type": "str",
            "description": "Does the paper discuss the cost, time, or resource differences between proxy evaluation and ground-truth validation? Any discussion of economic incentives or validation bottlenecks?"
        },
        {
            "name": "exceptional_cases",
            "type": "str",
            "description": "Does the paper describe any exceptional cases where proxy and ground truth agree very well (small gap) or disagree dramatically (large gap)? What makes these cases special?"
        },
        {
            "name": "limitations_discussion",
            "type": "str",
            "description": "What limitations does the paper identify regarding the proxy metric's ability to predict ground truth? Any discussion of fundamental limits or irreducible gaps?"
        }
    ],
    "extraction_query": "Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
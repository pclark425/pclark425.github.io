<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-50 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-50</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-50</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>curriculum_generator_type</strong></td>
                        <td>str</td>
                        <td>What generates the curriculum? (e.g., 'LLM-generated', 'manual/hand-designed', 'heuristic-based', 'random', 'hybrid LLM+heuristic', 'learned curriculum', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>llm_model_name</strong></td>
                        <td>str</td>
                        <td>If an LLM is used for curriculum generation, what is the model name? (e.g., 'GPT-4', 'GPT-3.5', 'Claude', 'LLaMA-2-70B', etc.). Null if not LLM-based or not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>llm_model_size</strong></td>
                        <td>str</td>
                        <td>If an LLM is used, what is its size in parameters? (e.g., '175B', '70B', '13B', etc.). Null if not applicable or not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>curriculum_description</strong></td>
                        <td>str</td>
                        <td>Detailed description of how the curriculum works, including what it generates (tasks, goals, subgoals), how it adapts, and what information it conditions on.</td>
                    </tr>
                    <tr>
                        <td><strong>domain_name</strong></td>
                        <td>str</td>
                        <td>The name of the domain/environment where the curriculum is applied (e.g., 'Minecraft', 'Crafter', 'ScienceWorld', 'robotics manipulation', 'NetHack', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>domain_characteristics</strong></td>
                        <td>str</td>
                        <td>Describe key characteristics of the domain: Is it open-ended? Does it have compositional tasks? What is the task diversity? Does it require long-horizon planning? Does it have specialized knowledge requirements?</td>
                    </tr>
                    <tr>
                        <td><strong>state_conditioning</strong></td>
                        <td>bool</td>
                        <td>Does the curriculum generator condition on agent state (inventory, location, skills, past performance, etc.)? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>state_conditioning_details</strong></td>
                        <td>str</td>
                        <td>If state conditioning is used, what specific state information is provided? (e.g., 'inventory, location, completed tasks, failed tasks, biome, health', etc.). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_mechanism</strong></td>
                        <td>bool</td>
                        <td>Does the curriculum use explicit novelty-seeking or diversity mechanisms? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_mechanism_details</strong></td>
                        <td>str</td>
                        <td>If novelty mechanisms are used, describe them (e.g., 'novelty bias - each goal rewarded once', 'diversity prompting', 'count-based exploration', etc.). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>complementary_systems</strong></td>
                        <td>str</td>
                        <td>What complementary systems are used alongside the curriculum? (e.g., 'skill library', 'execution monitoring', 'self-verification', 'warm-up schedules', 'none', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_llm_curriculum</strong></td>
                        <td>str</td>
                        <td>Performance metrics when using LLM-generated curriculum. Include specific numbers, task completion rates, discovery rates, milestone unlock times, etc. Null if not applicable or not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_manual_curriculum</strong></td>
                        <td>str</td>
                        <td>Performance metrics when using manual/hand-designed curriculum. Include specific numbers for comparison. Null if not applicable or not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_heuristic_curriculum</strong></td>
                        <td>str</td>
                        <td>Performance metrics when using heuristic-based curriculum (e.g., random, novelty-based, difficulty-based). Include specific numbers. Null if not applicable or not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_no_curriculum</strong></td>
                        <td>str</td>
                        <td>Performance metrics when using no curriculum (e.g., uniform random task sampling, fixed task distribution). Null if not applicable or not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>has_curriculum_comparison</strong></td>
                        <td>bool</td>
                        <td>Does the paper directly compare multiple curriculum approaches (LLM vs manual vs heuristic vs none)? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>task_diversity_metrics</strong></td>
                        <td>str</td>
                        <td>Metrics about task diversity generated by the curriculum (e.g., 'discovered 63 unique items', 'generated 500 distinct goals', 'covered 15 task categories', etc.). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>transfer_generalization_results</strong></td>
                        <td>str</td>
                        <td>Results on transfer to novel tasks or generalization, especially zero-shot or few-shot performance on unseen task compositions. Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>computational_cost</strong></td>
                        <td>str</td>
                        <td>Information about computational costs of curriculum generation (API calls, inference time, wall-clock time, cost in dollars, etc.). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>failure_modes_limitations</strong></td>
                        <td>str</td>
                        <td>Specific failure modes, limitations, or cases where the curriculum approach underperforms. Be detailed about what types of tasks or situations cause problems.</td>
                    </tr>
                    <tr>
                        <td><strong>long_horizon_performance</strong></td>
                        <td>str</td>
                        <td>Specific results on long-horizon planning or navigation tasks. Does the curriculum help or hurt on these? Null if not discussed.</td>
                    </tr>
                    <tr>
                        <td><strong>specialized_domain_performance</strong></td>
                        <td>str</td>
                        <td>If the domain requires specialized knowledge (technical, scientific, etc.), how does the curriculum perform? Null if not applicable or not discussed.</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_studies</strong></td>
                        <td>str</td>
                        <td>Results from ablation studies testing specific components (e.g., 'removing state conditioning reduces performance by 40%', 'novelty bias essential for diversity', etc.). Null if no ablations reported.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size_scaling</strong></td>
                        <td>str</td>
                        <td>If multiple LLM sizes are tested for curriculum generation, what are the results? Does performance scale with model size? Null if not tested.</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings_curriculum_effectiveness</strong></td>
                        <td>str</td>
                        <td>Concise, information-dense summary of key findings about what makes the curriculum effective or ineffective, including quantitative comparisons where available.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-50",
    "schema": [
        {
            "name": "curriculum_generator_type",
            "type": "str",
            "description": "What generates the curriculum? (e.g., 'LLM-generated', 'manual/hand-designed', 'heuristic-based', 'random', 'hybrid LLM+heuristic', 'learned curriculum', etc.)"
        },
        {
            "name": "llm_model_name",
            "type": "str",
            "description": "If an LLM is used for curriculum generation, what is the model name? (e.g., 'GPT-4', 'GPT-3.5', 'Claude', 'LLaMA-2-70B', etc.). Null if not LLM-based or not mentioned."
        },
        {
            "name": "llm_model_size",
            "type": "str",
            "description": "If an LLM is used, what is its size in parameters? (e.g., '175B', '70B', '13B', etc.). Null if not applicable or not mentioned."
        },
        {
            "name": "curriculum_description",
            "type": "str",
            "description": "Detailed description of how the curriculum works, including what it generates (tasks, goals, subgoals), how it adapts, and what information it conditions on."
        },
        {
            "name": "domain_name",
            "type": "str",
            "description": "The name of the domain/environment where the curriculum is applied (e.g., 'Minecraft', 'Crafter', 'ScienceWorld', 'robotics manipulation', 'NetHack', etc.)"
        },
        {
            "name": "domain_characteristics",
            "type": "str",
            "description": "Describe key characteristics of the domain: Is it open-ended? Does it have compositional tasks? What is the task diversity? Does it require long-horizon planning? Does it have specialized knowledge requirements?"
        },
        {
            "name": "state_conditioning",
            "type": "bool",
            "description": "Does the curriculum generator condition on agent state (inventory, location, skills, past performance, etc.)? (true, false, or null for no information)"
        },
        {
            "name": "state_conditioning_details",
            "type": "str",
            "description": "If state conditioning is used, what specific state information is provided? (e.g., 'inventory, location, completed tasks, failed tasks, biome, health', etc.). Null if not applicable."
        },
        {
            "name": "novelty_mechanism",
            "type": "bool",
            "description": "Does the curriculum use explicit novelty-seeking or diversity mechanisms? (true, false, or null for no information)"
        },
        {
            "name": "novelty_mechanism_details",
            "type": "str",
            "description": "If novelty mechanisms are used, describe them (e.g., 'novelty bias - each goal rewarded once', 'diversity prompting', 'count-based exploration', etc.). Null if not applicable."
        },
        {
            "name": "complementary_systems",
            "type": "str",
            "description": "What complementary systems are used alongside the curriculum? (e.g., 'skill library', 'execution monitoring', 'self-verification', 'warm-up schedules', 'none', etc.)"
        },
        {
            "name": "performance_llm_curriculum",
            "type": "str",
            "description": "Performance metrics when using LLM-generated curriculum. Include specific numbers, task completion rates, discovery rates, milestone unlock times, etc. Null if not applicable or not reported."
        },
        {
            "name": "performance_manual_curriculum",
            "type": "str",
            "description": "Performance metrics when using manual/hand-designed curriculum. Include specific numbers for comparison. Null if not applicable or not reported."
        },
        {
            "name": "performance_heuristic_curriculum",
            "type": "str",
            "description": "Performance metrics when using heuristic-based curriculum (e.g., random, novelty-based, difficulty-based). Include specific numbers. Null if not applicable or not reported."
        },
        {
            "name": "performance_no_curriculum",
            "type": "str",
            "description": "Performance metrics when using no curriculum (e.g., uniform random task sampling, fixed task distribution). Null if not applicable or not reported."
        },
        {
            "name": "has_curriculum_comparison",
            "type": "bool",
            "description": "Does the paper directly compare multiple curriculum approaches (LLM vs manual vs heuristic vs none)? (true, false, or null)"
        },
        {
            "name": "task_diversity_metrics",
            "type": "str",
            "description": "Metrics about task diversity generated by the curriculum (e.g., 'discovered 63 unique items', 'generated 500 distinct goals', 'covered 15 task categories', etc.). Null if not reported."
        },
        {
            "name": "transfer_generalization_results",
            "type": "str",
            "description": "Results on transfer to novel tasks or generalization, especially zero-shot or few-shot performance on unseen task compositions. Null if not reported."
        },
        {
            "name": "computational_cost",
            "type": "str",
            "description": "Information about computational costs of curriculum generation (API calls, inference time, wall-clock time, cost in dollars, etc.). Null if not reported."
        },
        {
            "name": "failure_modes_limitations",
            "type": "str",
            "description": "Specific failure modes, limitations, or cases where the curriculum approach underperforms. Be detailed about what types of tasks or situations cause problems."
        },
        {
            "name": "long_horizon_performance",
            "type": "str",
            "description": "Specific results on long-horizon planning or navigation tasks. Does the curriculum help or hurt on these? Null if not discussed."
        },
        {
            "name": "specialized_domain_performance",
            "type": "str",
            "description": "If the domain requires specialized knowledge (technical, scientific, etc.), how does the curriculum perform? Null if not applicable or not discussed."
        },
        {
            "name": "ablation_studies",
            "type": "str",
            "description": "Results from ablation studies testing specific components (e.g., 'removing state conditioning reduces performance by 40%', 'novelty bias essential for diversity', etc.). Null if no ablations reported."
        },
        {
            "name": "model_size_scaling",
            "type": "str",
            "description": "If multiple LLM sizes are tested for curriculum generation, what are the results? Does performance scale with model size? Null if not tested."
        },
        {
            "name": "key_findings_curriculum_effectiveness",
            "type": "str",
            "description": "Concise, information-dense summary of key findings about what makes the curriculum effective or ineffective, including quantitative comparisons where available."
        }
    ],
    "extraction_query": "Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
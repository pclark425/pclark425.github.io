<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-57 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-57</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-57</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>system_name</strong></td>
                        <td>str</td>
                        <td>The name of the AI/automated system being studied (e.g., 'GPT-4', 'AlphaFold', 'automated theorem prover', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>system_type</strong></td>
                        <td>str</td>
                        <td>The type of system (e.g., 'large language model', 'neural network', 'symbolic AI', 'neurosymbolic', 'hybrid system', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>domain</strong></td>
                        <td>str</td>
                        <td>The scientific or problem domain (e.g., 'chemistry', 'mathematics', 'drug discovery', 'materials science', 'theorem proving', 'general scientific reasoning', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>generation_capability</strong></td>
                        <td>str</td>
                        <td>What does the system generate? (e.g., 'molecular structures', 'mathematical proofs', 'scientific hypotheses', 'research ideas', 'predictions', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>validation_method</strong></td>
                        <td>str</td>
                        <td>How are the generated outputs validated? Describe the validation method in detail (e.g., 'experimental testing', 'formal verification', 'simulation', 'expert review', 'comparison to known results', 'self-consistency checks', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_measure</strong></td>
                        <td>str</td>
                        <td>How is novelty or transformational nature of discoveries measured or categorized? (e.g., 'distance from training data', 'expert ratings', 'similarity to known examples', 'incremental vs. breakthrough classification', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>generation_performance</strong></td>
                        <td>str</td>
                        <td>What is the system's performance at generating outputs? Include metrics, success rates, or qualitative assessments. Specify if different for novel vs. familiar tasks.</td>
                    </tr>
                    <tr>
                        <td><strong>validation_performance</strong></td>
                        <td>str</td>
                        <td>What is the system's performance at validating outputs? Include accuracy, precision, recall, false positive/negative rates, or calibration metrics. Specify if different for novel vs. familiar tasks.</td>
                    </tr>
                    <tr>
                        <td><strong>false_positive_rate</strong></td>
                        <td>str</td>
                        <td>What is the false positive rate (accepting invalid outputs as valid)? Include numerical values if available, and note if it varies with novelty level.</td>
                    </tr>
                    <tr>
                        <td><strong>false_negative_rate</strong></td>
                        <td>str</td>
                        <td>What is the false negative rate (rejecting valid outputs as invalid)? Include numerical values if available, and note if it varies with novelty level.</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_effect_on_validation</strong></td>
                        <td>str</td>
                        <td>Does validation accuracy or reliability change with the novelty of outputs? Describe any observed relationship between novelty and validation performance.</td>
                    </tr>
                    <tr>
                        <td><strong>generation_validation_asymmetry</strong></td>
                        <td>str</td>
                        <td>Is there evidence of asymmetry between generation and validation capabilities? Describe any observed gaps, mismatches, or differences in how well the system generates vs. validates.</td>
                    </tr>
                    <tr>
                        <td><strong>out_of_distribution_performance</strong></td>
                        <td>str</td>
                        <td>How does the system perform on out-of-distribution or novel examples compared to in-distribution examples? Include specific metrics if available.</td>
                    </tr>
                    <tr>
                        <td><strong>calibration_quality</strong></td>
                        <td>str</td>
                        <td>How well-calibrated is the system's confidence or uncertainty estimates? Does calibration degrade for novel outputs?</td>
                    </tr>
                    <tr>
                        <td><strong>validation_computational_cost</strong></td>
                        <td>str</td>
                        <td>What is the computational cost or time required for validation compared to generation? Does this ratio change with novelty?</td>
                    </tr>
                    <tr>
                        <td><strong>human_validation_required</strong></td>
                        <td>bool</td>
                        <td>Does the system require human validation or oversight, particularly for novel outputs? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>gap_closing_mechanisms</strong></td>
                        <td>str</td>
                        <td>Are there any mechanisms, architectures, or approaches that help close the generation-validation gap? (e.g., 'integrated formal verification', 'uncertainty quantification', 'ensemble methods', 'human-in-the-loop', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_type</strong></td>
                        <td>str</td>
                        <td>Does this evidence support or contradict the fabrication-validation gap theory? ('supports', 'contradicts', 'mixed', or 'neutral')</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings</strong></td>
                        <td>str</td>
                        <td>Summarize the key findings relevant to the fabrication-validation gap theory in 1-3 concise sentences.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-57",
    "schema": [
        {
            "name": "system_name",
            "type": "str",
            "description": "The name of the AI/automated system being studied (e.g., 'GPT-4', 'AlphaFold', 'automated theorem prover', etc.)"
        },
        {
            "name": "system_type",
            "type": "str",
            "description": "The type of system (e.g., 'large language model', 'neural network', 'symbolic AI', 'neurosymbolic', 'hybrid system', etc.)"
        },
        {
            "name": "domain",
            "type": "str",
            "description": "The scientific or problem domain (e.g., 'chemistry', 'mathematics', 'drug discovery', 'materials science', 'theorem proving', 'general scientific reasoning', etc.)"
        },
        {
            "name": "generation_capability",
            "type": "str",
            "description": "What does the system generate? (e.g., 'molecular structures', 'mathematical proofs', 'scientific hypotheses', 'research ideas', 'predictions', etc.)"
        },
        {
            "name": "validation_method",
            "type": "str",
            "description": "How are the generated outputs validated? Describe the validation method in detail (e.g., 'experimental testing', 'formal verification', 'simulation', 'expert review', 'comparison to known results', 'self-consistency checks', etc.)"
        },
        {
            "name": "novelty_measure",
            "type": "str",
            "description": "How is novelty or transformational nature of discoveries measured or categorized? (e.g., 'distance from training data', 'expert ratings', 'similarity to known examples', 'incremental vs. breakthrough classification', etc.)"
        },
        {
            "name": "generation_performance",
            "type": "str",
            "description": "What is the system's performance at generating outputs? Include metrics, success rates, or qualitative assessments. Specify if different for novel vs. familiar tasks."
        },
        {
            "name": "validation_performance",
            "type": "str",
            "description": "What is the system's performance at validating outputs? Include accuracy, precision, recall, false positive/negative rates, or calibration metrics. Specify if different for novel vs. familiar tasks."
        },
        {
            "name": "false_positive_rate",
            "type": "str",
            "description": "What is the false positive rate (accepting invalid outputs as valid)? Include numerical values if available, and note if it varies with novelty level."
        },
        {
            "name": "false_negative_rate",
            "type": "str",
            "description": "What is the false negative rate (rejecting valid outputs as invalid)? Include numerical values if available, and note if it varies with novelty level."
        },
        {
            "name": "novelty_effect_on_validation",
            "type": "str",
            "description": "Does validation accuracy or reliability change with the novelty of outputs? Describe any observed relationship between novelty and validation performance."
        },
        {
            "name": "generation_validation_asymmetry",
            "type": "str",
            "description": "Is there evidence of asymmetry between generation and validation capabilities? Describe any observed gaps, mismatches, or differences in how well the system generates vs. validates."
        },
        {
            "name": "out_of_distribution_performance",
            "type": "str",
            "description": "How does the system perform on out-of-distribution or novel examples compared to in-distribution examples? Include specific metrics if available."
        },
        {
            "name": "calibration_quality",
            "type": "str",
            "description": "How well-calibrated is the system's confidence or uncertainty estimates? Does calibration degrade for novel outputs?"
        },
        {
            "name": "validation_computational_cost",
            "type": "str",
            "description": "What is the computational cost or time required for validation compared to generation? Does this ratio change with novelty?"
        },
        {
            "name": "human_validation_required",
            "type": "bool",
            "description": "Does the system require human validation or oversight, particularly for novel outputs? (true, false, or null for no information)"
        },
        {
            "name": "gap_closing_mechanisms",
            "type": "str",
            "description": "Are there any mechanisms, architectures, or approaches that help close the generation-validation gap? (e.g., 'integrated formal verification', 'uncertainty quantification', 'ensemble methods', 'human-in-the-loop', etc.)"
        },
        {
            "name": "evidence_type",
            "type": "str",
            "description": "Does this evidence support or contradict the fabrication-validation gap theory? ('supports', 'contradicts', 'mixed', or 'neutral')"
        },
        {
            "name": "key_findings",
            "type": "str",
            "description": "Summarize the key findings relevant to the fabrication-validation gap theory in 1-3 concise sentences."
        }
    ],
    "extraction_query": "Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
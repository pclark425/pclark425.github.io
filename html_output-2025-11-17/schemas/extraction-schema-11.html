<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-11 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-11</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-11</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM model being trained or evaluated.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g. 1B, 7B, 13B, 70B, etc.). Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>training_stage</strong></td>
                        <td>str</td>
                        <td>Which training stage is being discussed? Must be one of: 'SFT' (supervised fine-tuning), 'DPO' (direct preference optimization), 'RL' (reinforcement learning), or 'multiple' if comparing across stages. Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>task_type</strong></td>
                        <td>str</td>
                        <td>What task is the model being evaluated on? (e.g. 'scientific question answering', 'general QA', 'reasoning', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>is_scientific_domain</strong></td>
                        <td>bool</td>
                        <td>Is the task specifically about scientific literature or scientific question answering? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>data_type</strong></td>
                        <td>str</td>
                        <td>What type of training data was used? Be specific about the characteristics (e.g. 'high-quality human demonstrations', 'synthetic data from GPT-4', 'preference pairs from human feedback', 'domain-specific scientific papers', 'chain-of-thought examples', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>data_size</strong></td>
                        <td>str</td>
                        <td>How much training data was used? Include numbers and units (e.g. '10K examples', '100K preference pairs', etc.). Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>data_properties</strong></td>
                        <td>str</td>
                        <td>What specific properties or characteristics of the data are highlighted as important? (e.g. 'diversity', 'difficulty', 'domain-specificity', 'reasoning chains', 'correctness', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metric</strong></td>
                        <td>str</td>
                        <td>What metric was used to measure performance? (e.g. 'accuracy', 'F1', 'exact match', 'win rate', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_with_data</strong></td>
                        <td>str</td>
                        <td>What was the performance when using this specific data type? Include numerical values and units. Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_baseline</strong></td>
                        <td>str</td>
                        <td>What was the baseline performance (e.g. without this data, or with a different data type)? Include numerical values and units. Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_lift</strong></td>
                        <td>str</td>
                        <td>What was the improvement or 'lift' from using this data? Can be absolute (e.g. '+5% accuracy') or relative (e.g. '20% improvement'). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>compares_data_types</strong></td>
                        <td>bool</td>
                        <td>Does the paper compare multiple different types of training data for the same training stage? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>key_finding</strong></td>
                        <td>str</td>
                        <td>What is the key finding about what kinds of data provide the biggest lift? Be specific and concise (e.g. 'diverse reasoning chains provide 15% more lift than simple QA pairs in SFT', 'domain-specific data outperforms general data by 2x', etc.)</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-11",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM model being trained or evaluated."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g. 1B, 7B, 13B, 70B, etc.). Null if not specified."
        },
        {
            "name": "training_stage",
            "type": "str",
            "description": "Which training stage is being discussed? Must be one of: 'SFT' (supervised fine-tuning), 'DPO' (direct preference optimization), 'RL' (reinforcement learning), or 'multiple' if comparing across stages. Null if not specified."
        },
        {
            "name": "task_type",
            "type": "str",
            "description": "What task is the model being evaluated on? (e.g. 'scientific question answering', 'general QA', 'reasoning', etc.)"
        },
        {
            "name": "is_scientific_domain",
            "type": "bool",
            "description": "Is the task specifically about scientific literature or scientific question answering? (true, false, or null for no information)"
        },
        {
            "name": "data_type",
            "type": "str",
            "description": "What type of training data was used? Be specific about the characteristics (e.g. 'high-quality human demonstrations', 'synthetic data from GPT-4', 'preference pairs from human feedback', 'domain-specific scientific papers', 'chain-of-thought examples', etc.)"
        },
        {
            "name": "data_size",
            "type": "str",
            "description": "How much training data was used? Include numbers and units (e.g. '10K examples', '100K preference pairs', etc.). Null if not specified."
        },
        {
            "name": "data_properties",
            "type": "str",
            "description": "What specific properties or characteristics of the data are highlighted as important? (e.g. 'diversity', 'difficulty', 'domain-specificity', 'reasoning chains', 'correctness', etc.)"
        },
        {
            "name": "performance_metric",
            "type": "str",
            "description": "What metric was used to measure performance? (e.g. 'accuracy', 'F1', 'exact match', 'win rate', etc.)"
        },
        {
            "name": "performance_with_data",
            "type": "str",
            "description": "What was the performance when using this specific data type? Include numerical values and units. Null if not reported."
        },
        {
            "name": "performance_baseline",
            "type": "str",
            "description": "What was the baseline performance (e.g. without this data, or with a different data type)? Include numerical values and units. Null if not reported."
        },
        {
            "name": "performance_lift",
            "type": "str",
            "description": "What was the improvement or 'lift' from using this data? Can be absolute (e.g. '+5% accuracy') or relative (e.g. '20% improvement'). Null if not reported."
        },
        {
            "name": "compares_data_types",
            "type": "bool",
            "description": "Does the paper compare multiple different types of training data for the same training stage? (true, false, or null)"
        },
        {
            "name": "key_finding",
            "type": "str",
            "description": "What is the key finding about what kinds of data provide the biggest lift? Be specific and concise (e.g. 'diverse reasoning chains provide 15% more lift than simple QA pairs in SFT', 'domain-specific data outperforms general data by 2x', etc.)"
        }
    ],
    "extraction_query": "Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.",
    "supporting_theory_ids": [],
    "model_str": null
}</code></pre>
        </div>
    </div>
</body>
</html>
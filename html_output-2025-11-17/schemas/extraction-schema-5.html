<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-5 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-5</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-5</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) perform theory-of-mind tasks, including descriptions of the models, the tasks used to evaluate theory-of-mind capabilities, the results and performance metrics, and any reported limitations or challenges.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model (LLM) being evaluated, e.g., GPT-3, PaLM, LLaMA.</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM, including architecture details, training data, or unique features relevant to theory-of-mind capabilities.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 1B, 7B, 13B, 70B), or other scale indicators.</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the theory-of-mind task or benchmark used to evaluate the model, e.g., false belief task, Sally-Anne test, social reasoning benchmark.</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the theory-of-mind task, including what mental states or social reasoning it tests.</td>
                    </tr>
                    <tr>
                        <td><strong>task_type</strong></td>
                        <td>str</td>
                        <td>The category or type of theory-of-mind task, e.g., first-order belief reasoning, second-order belief reasoning, perspective taking, deception detection.</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>Quantitative or qualitative performance results of the model on the theory-of-mind task, including accuracy, success rate, or other relevant metrics.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>The method used to evaluate theory-of-mind capabilities, e.g., zero-shot prompting, few-shot prompting, fine-tuning, probing.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_reported</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure modes, or challenges the model exhibits in theory-of-mind tasks.</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_of_mental_state_representation</strong></td>
                        <td>bool</td>
                        <td>Whether the paper provides evidence that the model internally represents or simulates mental states (true, false, or null if no information).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_human_performance</strong></td>
                        <td>str</td>
                        <td>If available, a comparison of the model's theory-of-mind task performance to human baseline or human-level performance.</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_model_size</strong></td>
                        <td>str</td>
                        <td>Any reported relationship between model size and theory-of-mind task performance.</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_training_data</strong></td>
                        <td>str</td>
                        <td>Any reported influence of training data composition or scale on theory-of-mind capabilities.</td>
                    </tr>
                    <tr>
                        <td><strong>methods_to_improve_tom</strong></td>
                        <td>str</td>
                        <td>Descriptions of methods or interventions proposed or tested to improve theory-of-mind capabilities in LLMs, e.g., architectural changes, training regimes, prompting strategies.</td>
                    </tr>
                    <tr>
                        <td><strong>counter_evidence</strong></td>
                        <td>str</td>
                        <td>Any evidence or arguments presented that challenge or refute the presence of genuine theory-of-mind in LLMs.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-5",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model (LLM) being evaluated, e.g., GPT-3, PaLM, LLaMA."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM, including architecture details, training data, or unique features relevant to theory-of-mind capabilities."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 1B, 7B, 13B, 70B), or other scale indicators."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the theory-of-mind task or benchmark used to evaluate the model, e.g., false belief task, Sally-Anne test, social reasoning benchmark."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the theory-of-mind task, including what mental states or social reasoning it tests."
        },
        {
            "name": "task_type",
            "type": "str",
            "description": "The category or type of theory-of-mind task, e.g., first-order belief reasoning, second-order belief reasoning, perspective taking, deception detection."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "Quantitative or qualitative performance results of the model on the theory-of-mind task, including accuracy, success rate, or other relevant metrics."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "The method used to evaluate theory-of-mind capabilities, e.g., zero-shot prompting, few-shot prompting, fine-tuning, probing."
        },
        {
            "name": "limitations_reported",
            "type": "str",
            "description": "Any reported limitations, failure modes, or challenges the model exhibits in theory-of-mind tasks."
        },
        {
            "name": "evidence_of_mental_state_representation",
            "type": "bool",
            "description": "Whether the paper provides evidence that the model internally represents or simulates mental states (true, false, or null if no information)."
        },
        {
            "name": "comparison_to_human_performance",
            "type": "str",
            "description": "If available, a comparison of the model's theory-of-mind task performance to human baseline or human-level performance."
        },
        {
            "name": "impact_of_model_size",
            "type": "str",
            "description": "Any reported relationship between model size and theory-of-mind task performance."
        },
        {
            "name": "impact_of_training_data",
            "type": "str",
            "description": "Any reported influence of training data composition or scale on theory-of-mind capabilities."
        },
        {
            "name": "methods_to_improve_tom",
            "type": "str",
            "description": "Descriptions of methods or interventions proposed or tested to improve theory-of-mind capabilities in LLMs, e.g., architectural changes, training regimes, prompting strategies."
        },
        {
            "name": "counter_evidence",
            "type": "str",
            "description": "Any evidence or arguments presented that challenge or refute the presence of genuine theory-of-mind in LLMs."
        }
    ],
    "extraction_query": "Extract any mentions of how large language models (LLMs) perform theory-of-mind tasks, including descriptions of the models, the tasks used to evaluate theory-of-mind capabilities, the results and performance metrics, and any reported limitations or challenges.",
    "supporting_theory_ids": [],
    "model_str": null
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-90 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-90</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-90</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model (LLM) or AI system used for quantitative law discovery (e.g., GPT-4, LLaMA, Galactica, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM or AI system, including size, architecture, and any relevant modifications for the task.</td>
                    </tr>
                    <tr>
                        <td><strong>task_type</strong></td>
                        <td>str</td>
                        <td>The specific task performed (e.g., equation discovery, symbolic regression, law extraction, knowledge extraction, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>domain</strong></td>
                        <td>str</td>
                        <td>The scientific domain or field where the method was applied (e.g., physics, chemistry, biology, materials science, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>input_data_type</strong></td>
                        <td>str</td>
                        <td>The type of input data used (e.g., full text, abstracts, tables, figures, datasets, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>method_description</strong></td>
                        <td>str</td>
                        <td>A concise description of the method or approach used to extract or discover quantitative laws (e.g., prompt engineering, fine-tuning, retrieval-augmented generation, symbolic regression pipeline, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>output_type</strong></td>
                        <td>str</td>
                        <td>The type of output produced (e.g., equations, mathematical laws, symbolic expressions, relationships, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>benchmark_or_dataset</strong></td>
                        <td>str</td>
                        <td>The name and brief description of any benchmark or dataset used to evaluate the method (e.g., SRBench, Physics Equations Dataset, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metrics</strong></td>
                        <td>str</td>
                        <td>The metrics used to evaluate performance (e.g., accuracy, precision, recall, F1, equation recovery rate, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>results_summary</strong></td>
                        <td>str</td>
                        <td>A concise summary of the main results, including quantitative performance and any notable successes (e.g., rediscovery of known laws, outperforming baselines, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_challenges</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, challenges, or failure cases encountered in using LLMs for this task.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_other_methods</strong></td>
                        <td>str</td>
                        <td>Any comparisons to other methods (e.g., traditional symbolic regression, human experts, other AI models), including relative strengths and weaknesses.</td>
                    </tr>
                    <tr>
                        <td><strong>notable_counterexamples</strong></td>
                        <td>str</td>
                        <td>Any notable counterexamples, negative results, or cases where the LLM approach failed or produced incorrect laws.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-90",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model (LLM) or AI system used for quantitative law discovery (e.g., GPT-4, LLaMA, Galactica, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM or AI system, including size, architecture, and any relevant modifications for the task."
        },
        {
            "name": "task_type",
            "type": "str",
            "description": "The specific task performed (e.g., equation discovery, symbolic regression, law extraction, knowledge extraction, etc.)."
        },
        {
            "name": "domain",
            "type": "str",
            "description": "The scientific domain or field where the method was applied (e.g., physics, chemistry, biology, materials science, etc.)."
        },
        {
            "name": "input_data_type",
            "type": "str",
            "description": "The type of input data used (e.g., full text, abstracts, tables, figures, datasets, etc.)."
        },
        {
            "name": "method_description",
            "type": "str",
            "description": "A concise description of the method or approach used to extract or discover quantitative laws (e.g., prompt engineering, fine-tuning, retrieval-augmented generation, symbolic regression pipeline, etc.)."
        },
        {
            "name": "output_type",
            "type": "str",
            "description": "The type of output produced (e.g., equations, mathematical laws, symbolic expressions, relationships, etc.)."
        },
        {
            "name": "benchmark_or_dataset",
            "type": "str",
            "description": "The name and brief description of any benchmark or dataset used to evaluate the method (e.g., SRBench, Physics Equations Dataset, etc.)."
        },
        {
            "name": "evaluation_metrics",
            "type": "str",
            "description": "The metrics used to evaluate performance (e.g., accuracy, precision, recall, F1, equation recovery rate, etc.)."
        },
        {
            "name": "results_summary",
            "type": "str",
            "description": "A concise summary of the main results, including quantitative performance and any notable successes (e.g., rediscovery of known laws, outperforming baselines, etc.)."
        },
        {
            "name": "limitations_or_challenges",
            "type": "str",
            "description": "Any reported limitations, challenges, or failure cases encountered in using LLMs for this task."
        },
        {
            "name": "comparison_to_other_methods",
            "type": "str",
            "description": "Any comparisons to other methods (e.g., traditional symbolic regression, human experts, other AI models), including relative strengths and weaknesses."
        },
        {
            "name": "notable_counterexamples",
            "type": "str",
            "description": "Any notable counterexamples, negative results, or cases where the LLM approach failed or produced incorrect laws."
        }
    ],
    "extraction_query": "Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
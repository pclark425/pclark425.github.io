<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-100 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-100</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-100</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>system_name</strong></td>
                        <td>str</td>
                        <td>The name of the system, method, or approach that uses LLMs to process scientific papers (e.g., 'ScholarBERT', 'PaperSynthesizer', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>system_description</strong></td>
                        <td>str</td>
                        <td>A detailed description of how the system works, including its architecture and key components.</td>
                    </tr>
                    <tr>
                        <td><strong>llm_model_used</strong></td>
                        <td>str</td>
                        <td>What LLM model(s) are used in the system? Include model names and sizes if available (e.g., 'GPT-4', 'Claude-2', 'LLaMA-70B', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>extraction_technique</strong></td>
                        <td>str</td>
                        <td>What technique does the system use to extract information from papers? (e.g., 'structured prompting with schema', 'entity extraction', 'question-answering', 'embedding-based retrieval', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>synthesis_technique</strong></td>
                        <td>str</td>
                        <td>How does the system synthesize or combine information from multiple papers? (e.g., 'hierarchical summarization', 'knowledge graph construction', 'claim aggregation', 'multi-step reasoning', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>number_of_papers</strong></td>
                        <td>str</td>
                        <td>How many papers does the system process? Provide specific numbers or ranges (e.g., '10 papers', '100-500 papers', 'up to 1000 papers', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>domain_or_topic</strong></td>
                        <td>str</td>
                        <td>What scientific domain or topic area is the system evaluated on? (e.g., 'biomedical literature', 'computer science', 'general scientific papers', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>output_type</strong></td>
                        <td>str</td>
                        <td>What type of output does the system generate? (e.g., 'structured summaries', 'research hypotheses', 'knowledge graphs', 'literature reviews', 'causal theories', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metrics</strong></td>
                        <td>str</td>
                        <td>What metrics are used to evaluate the system's performance? (e.g., 'ROUGE scores', 'human expert ratings', 'factual accuracy', 'coverage', 'coherence scores', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_results</strong></td>
                        <td>str</td>
                        <td>What are the quantitative and/or qualitative performance results? Be specific and include numbers with units where available.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_baseline</strong></td>
                        <td>str</td>
                        <td>What baseline is the system compared against? (e.g., 'human expert reviews', 'traditional NLP methods', 'smaller LLMs', 'no comparison', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_vs_baseline</strong></td>
                        <td>str</td>
                        <td>How does the system perform compared to the baseline? Provide specific comparisons with numbers if available.</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings</strong></td>
                        <td>str</td>
                        <td>What are the key findings or insights about what makes the approach successful or unsuccessful? Be concise but information-dense.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_challenges</strong></td>
                        <td>str</td>
                        <td>What limitations or challenges does the paper identify with using LLMs for theory distillation? (e.g., 'hallucination', 'inconsistency across papers', 'difficulty with contradictory evidence', 'computational cost', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>scaling_behavior</strong></td>
                        <td>str</td>
                        <td>Does the paper discuss how performance changes with the number of papers or model size? Describe any scaling trends observed.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-100",
    "schema": [
        {
            "name": "system_name",
            "type": "str",
            "description": "The name of the system, method, or approach that uses LLMs to process scientific papers (e.g., 'ScholarBERT', 'PaperSynthesizer', etc.)"
        },
        {
            "name": "system_description",
            "type": "str",
            "description": "A detailed description of how the system works, including its architecture and key components."
        },
        {
            "name": "llm_model_used",
            "type": "str",
            "description": "What LLM model(s) are used in the system? Include model names and sizes if available (e.g., 'GPT-4', 'Claude-2', 'LLaMA-70B', etc.)"
        },
        {
            "name": "extraction_technique",
            "type": "str",
            "description": "What technique does the system use to extract information from papers? (e.g., 'structured prompting with schema', 'entity extraction', 'question-answering', 'embedding-based retrieval', etc.)"
        },
        {
            "name": "synthesis_technique",
            "type": "str",
            "description": "How does the system synthesize or combine information from multiple papers? (e.g., 'hierarchical summarization', 'knowledge graph construction', 'claim aggregation', 'multi-step reasoning', etc.)"
        },
        {
            "name": "number_of_papers",
            "type": "str",
            "description": "How many papers does the system process? Provide specific numbers or ranges (e.g., '10 papers', '100-500 papers', 'up to 1000 papers', etc.)"
        },
        {
            "name": "domain_or_topic",
            "type": "str",
            "description": "What scientific domain or topic area is the system evaluated on? (e.g., 'biomedical literature', 'computer science', 'general scientific papers', etc.)"
        },
        {
            "name": "output_type",
            "type": "str",
            "description": "What type of output does the system generate? (e.g., 'structured summaries', 'research hypotheses', 'knowledge graphs', 'literature reviews', 'causal theories', etc.)"
        },
        {
            "name": "evaluation_metrics",
            "type": "str",
            "description": "What metrics are used to evaluate the system's performance? (e.g., 'ROUGE scores', 'human expert ratings', 'factual accuracy', 'coverage', 'coherence scores', etc.)"
        },
        {
            "name": "performance_results",
            "type": "str",
            "description": "What are the quantitative and/or qualitative performance results? Be specific and include numbers with units where available."
        },
        {
            "name": "comparison_baseline",
            "type": "str",
            "description": "What baseline is the system compared against? (e.g., 'human expert reviews', 'traditional NLP methods', 'smaller LLMs', 'no comparison', etc.)"
        },
        {
            "name": "performance_vs_baseline",
            "type": "str",
            "description": "How does the system perform compared to the baseline? Provide specific comparisons with numbers if available."
        },
        {
            "name": "key_findings",
            "type": "str",
            "description": "What are the key findings or insights about what makes the approach successful or unsuccessful? Be concise but information-dense."
        },
        {
            "name": "limitations_challenges",
            "type": "str",
            "description": "What limitations or challenges does the paper identify with using LLMs for theory distillation? (e.g., 'hallucination', 'inconsistency across papers', 'difficulty with contradictory evidence', 'computational cost', etc.)"
        },
        {
            "name": "scaling_behavior",
            "type": "str",
            "description": "Does the paper discuss how performance changes with the number of papers or model size? Describe any scaling trends observed."
        }
    ],
    "extraction_query": "Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
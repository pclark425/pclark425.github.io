<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-37 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-37</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-37</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>evaluation_system_type</strong></td>
                        <td>str</td>
                        <td>What type of evaluation system is being studied? (e.g., 'peer review', 'citation metrics', 'journal impact factor', 'automated ML system', 'grant review', 'h-index', 'altmetrics', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_measure</strong></td>
                        <td>str</td>
                        <td>How is novelty/transformation measured in this study? (e.g., 'atypical combination index', 'interdisciplinarity score', 'expert ratings', 'distance from prior work', 'paradigm shift classification', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>bias_magnitude</strong></td>
                        <td>str</td>
                        <td>What is the quantitative magnitude of bias against novel work? Include specific numbers, percentages, effect sizes, or statistical measures. (e.g., '40% lower acceptance rate', 'citation delay of 5 years', 'odds ratio of 0.6', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>relationship_type</strong></td>
                        <td>str</td>
                        <td>What is the mathematical/functional relationship between novelty level and evaluation outcome? (e.g., 'linear', 'exponential', 'U-shaped', 'threshold effect', 'no relationship', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>temporal_pattern</strong></td>
                        <td>str</td>
                        <td>How does the evaluation of novel work change over time? Include specific time scales and patterns. (e.g., 'initial rejection followed by recognition after 10 years', 'citation rate doubles after 5 years', 'gap closes exponentially with half-life of 3 years', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>field_studied</strong></td>
                        <td>str</td>
                        <td>What scientific field(s) were studied? (e.g., 'physics', 'biology', 'computer science', 'social sciences', 'cross-disciplinary comparison', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>field_differences</strong></td>
                        <td>str</td>
                        <td>Are there differences in evaluation bias across fields? If so, describe them quantitatively. (e.g., 'physics shows 2x larger bias than biology', 'paradigm rigidity correlates with bias (r=0.7)', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metric_studied</strong></td>
                        <td>str</td>
                        <td>What specific proxy metric is being evaluated? (e.g., 'short-term citations', 'journal impact factor', 'author h-index', 'institutional prestige', 'methodological familiarity ratings', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_measure</strong></td>
                        <td>str</td>
                        <td>What is used as the ground truth measure of scientific value? (e.g., 'long-term citations', 'expert retrospective ratings', 'Nobel prizes', 'paradigm shift classification', 'practical applications', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_truth_gap</strong></td>
                        <td>str</td>
                        <td>What is the measured gap between proxy metric and ground truth? Include quantitative measures. (e.g., 'correlation of 0.3 for novel work vs 0.8 for incremental', '60% undervaluation', 'prediction error of 40%', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>incremental_vs_transformational</strong></td>
                        <td>str</td>
                        <td>Does the study explicitly compare incremental vs transformational work? If so, what are the quantitative differences in evaluation outcomes? (e.g., 'transformational work receives 50% fewer citations in first 3 years', 'incremental work 2x more likely to be accepted', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>multiple_proxy_failures</strong></td>
                        <td>str</td>
                        <td>Does the study examine multiple types of proxy failures simultaneously? If so, do they compound additively or multiplicatively? Include specific findings.</td>
                    </tr>
                    <tr>
                        <td><strong>automated_system_performance</strong></td>
                        <td>str</td>
                        <td>If the study involves automated evaluation systems (ML, AI, algorithmic), how do they perform on novel vs incremental work? Include quantitative comparisons.</td>
                    </tr>
                    <tr>
                        <td><strong>training_data_bias</strong></td>
                        <td>str</td>
                        <td>Does the study examine how training data composition affects evaluation bias? (e.g., 'systems trained on historical data show 30% larger bias', 'recency of training data correlates with bias reduction', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>intervention_tested</strong></td>
                        <td>str</td>
                        <td>Was any intervention tested to reduce evaluation bias? If so, describe it and its effectiveness. (e.g., 'novelty bonus reduced bias by 25%', 'meta-learning approach improved correlation from 0.4 to 0.6', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>counter_examples</strong></td>
                        <td>str</td>
                        <td>Are there counter-examples where transformational work was recognized early, or where incremental work was overvalued? Describe specific cases.</td>
                    </tr>
                    <tr>
                        <td><strong>moderating_factors</strong></td>
                        <td>str</td>
                        <td>What factors moderate the evaluation bias? (e.g., 'author reputation reduces bias by 40%', 'clear communication reduces delay by 2 years', 'crisis periods reduce bias', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>sample_size_and_methods</strong></td>
                        <td>str</td>
                        <td>What is the sample size and methodology? (e.g., 'analysis of 50,000 papers over 20 years', 'controlled experiment with 200 reviewers', 'case study of 10 Nobel prizes', etc.)</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-37",
    "schema": [
        {
            "name": "evaluation_system_type",
            "type": "str",
            "description": "What type of evaluation system is being studied? (e.g., 'peer review', 'citation metrics', 'journal impact factor', 'automated ML system', 'grant review', 'h-index', 'altmetrics', etc.)"
        },
        {
            "name": "novelty_measure",
            "type": "str",
            "description": "How is novelty/transformation measured in this study? (e.g., 'atypical combination index', 'interdisciplinarity score', 'expert ratings', 'distance from prior work', 'paradigm shift classification', etc.)"
        },
        {
            "name": "bias_magnitude",
            "type": "str",
            "description": "What is the quantitative magnitude of bias against novel work? Include specific numbers, percentages, effect sizes, or statistical measures. (e.g., '40% lower acceptance rate', 'citation delay of 5 years', 'odds ratio of 0.6', etc.)"
        },
        {
            "name": "relationship_type",
            "type": "str",
            "description": "What is the mathematical/functional relationship between novelty level and evaluation outcome? (e.g., 'linear', 'exponential', 'U-shaped', 'threshold effect', 'no relationship', etc.)"
        },
        {
            "name": "temporal_pattern",
            "type": "str",
            "description": "How does the evaluation of novel work change over time? Include specific time scales and patterns. (e.g., 'initial rejection followed by recognition after 10 years', 'citation rate doubles after 5 years', 'gap closes exponentially with half-life of 3 years', etc.)"
        },
        {
            "name": "field_studied",
            "type": "str",
            "description": "What scientific field(s) were studied? (e.g., 'physics', 'biology', 'computer science', 'social sciences', 'cross-disciplinary comparison', etc.)"
        },
        {
            "name": "field_differences",
            "type": "str",
            "description": "Are there differences in evaluation bias across fields? If so, describe them quantitatively. (e.g., 'physics shows 2x larger bias than biology', 'paradigm rigidity correlates with bias (r=0.7)', etc.)"
        },
        {
            "name": "proxy_metric_studied",
            "type": "str",
            "description": "What specific proxy metric is being evaluated? (e.g., 'short-term citations', 'journal impact factor', 'author h-index', 'institutional prestige', 'methodological familiarity ratings', etc.)"
        },
        {
            "name": "ground_truth_measure",
            "type": "str",
            "description": "What is used as the ground truth measure of scientific value? (e.g., 'long-term citations', 'expert retrospective ratings', 'Nobel prizes', 'paradigm shift classification', 'practical applications', etc.)"
        },
        {
            "name": "proxy_truth_gap",
            "type": "str",
            "description": "What is the measured gap between proxy metric and ground truth? Include quantitative measures. (e.g., 'correlation of 0.3 for novel work vs 0.8 for incremental', '60% undervaluation', 'prediction error of 40%', etc.)"
        },
        {
            "name": "incremental_vs_transformational",
            "type": "str",
            "description": "Does the study explicitly compare incremental vs transformational work? If so, what are the quantitative differences in evaluation outcomes? (e.g., 'transformational work receives 50% fewer citations in first 3 years', 'incremental work 2x more likely to be accepted', etc.)"
        },
        {
            "name": "multiple_proxy_failures",
            "type": "str",
            "description": "Does the study examine multiple types of proxy failures simultaneously? If so, do they compound additively or multiplicatively? Include specific findings."
        },
        {
            "name": "automated_system_performance",
            "type": "str",
            "description": "If the study involves automated evaluation systems (ML, AI, algorithmic), how do they perform on novel vs incremental work? Include quantitative comparisons."
        },
        {
            "name": "training_data_bias",
            "type": "str",
            "description": "Does the study examine how training data composition affects evaluation bias? (e.g., 'systems trained on historical data show 30% larger bias', 'recency of training data correlates with bias reduction', etc.)"
        },
        {
            "name": "intervention_tested",
            "type": "str",
            "description": "Was any intervention tested to reduce evaluation bias? If so, describe it and its effectiveness. (e.g., 'novelty bonus reduced bias by 25%', 'meta-learning approach improved correlation from 0.4 to 0.6', etc.)"
        },
        {
            "name": "counter_examples",
            "type": "str",
            "description": "Are there counter-examples where transformational work was recognized early, or where incremental work was overvalued? Describe specific cases."
        },
        {
            "name": "moderating_factors",
            "type": "str",
            "description": "What factors moderate the evaluation bias? (e.g., 'author reputation reduces bias by 40%', 'clear communication reduces delay by 2 years', 'crisis periods reduce bias', etc.)"
        },
        {
            "name": "sample_size_and_methods",
            "type": "str",
            "description": "What is the sample size and methodology? (e.g., 'analysis of 50,000 papers over 20 years', 'controlled experiment with 200 reviewers', 'case study of 10 Nobel prizes', etc.)"
        }
    ],
    "extraction_query": "Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
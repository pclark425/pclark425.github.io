<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-53 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-53</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-53</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>system_name</strong></td>
                        <td>str</td>
                        <td>The name of the AI/automated system being studied (e.g., 'GPT-4', 'AlphaFold', 'automated theorem prover').</td>
                    </tr>
                    <tr>
                        <td><strong>system_type</strong></td>
                        <td>str</td>
                        <td>The type/architecture of the system (e.g., 'large language model', 'neural network', 'neurosymbolic system', 'reinforcement learning agent', 'generative model').</td>
                    </tr>
                    <tr>
                        <td><strong>scientific_domain</strong></td>
                        <td>str</td>
                        <td>The scientific domain where the system operates (e.g., 'chemistry', 'mathematics', 'drug discovery', 'materials science', 'theorem proving', 'hypothesis generation').</td>
                    </tr>
                    <tr>
                        <td><strong>output_type</strong></td>
                        <td>str</td>
                        <td>What kind of outputs does the system generate? (e.g., 'molecular structures', 'mathematical proofs', 'scientific hypotheses', 'research claims', 'predictions').</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_level</strong></td>
                        <td>str</td>
                        <td>How novel are the outputs relative to training data? Use terms like 'incremental', 'moderately novel', 'highly novel', 'transformational', 'out-of-distribution', or 'in-distribution'. Include specific details if provided.</td>
                    </tr>
                    <tr>
                        <td><strong>generation_method</strong></td>
                        <td>str</td>
                        <td>How does the system generate novel outputs? Describe the generation mechanism (e.g., 'pattern extrapolation from training data', 'recombination of learned features', 'search over hypothesis space').</td>
                    </tr>
                    <tr>
                        <td><strong>validation_method</strong></td>
                        <td>str</td>
                        <td>How are the generated outputs validated? Describe the validation mechanism (e.g., 'comparison to known ground truth', 'formal verification', 'experimental testing', 'expert review', 'simulation', 'consistency checks').</td>
                    </tr>
                    <tr>
                        <td><strong>generation_performance</strong></td>
                        <td>str</td>
                        <td>What is the system's performance at generating outputs? Include metrics like success rate, diversity, novelty scores, or generation speed. Specify units and conditions.</td>
                    </tr>
                    <tr>
                        <td><strong>validation_performance</strong></td>
                        <td>str</td>
                        <td>What is the system's performance at validating outputs? Include metrics like validation accuracy, precision, recall, or reliability. Specify for different novelty levels if available.</td>
                    </tr>
                    <tr>
                        <td><strong>false_positive_rate</strong></td>
                        <td>str</td>
                        <td>What is the false positive rate (invalid outputs marked as valid)? Specify the rate and conditions. Note if this varies with output novelty.</td>
                    </tr>
                    <tr>
                        <td><strong>false_negative_rate</strong></td>
                        <td>str</td>
                        <td>What is the false negative rate (valid outputs marked as invalid)? Specify the rate and conditions. Note if this varies with output novelty.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_vs_novelty</strong></td>
                        <td>str</td>
                        <td>How does validation performance change with output novelty? Describe any reported relationship between novelty level and validation accuracy/reliability.</td>
                    </tr>
                    <tr>
                        <td><strong>generation_validation_comparison</strong></td>
                        <td>str</td>
                        <td>Does the paper compare generation capabilities to validation capabilities? Describe any asymmetry or gap between the two, especially as novelty increases.</td>
                    </tr>
                    <tr>
                        <td><strong>uncertainty_quantification</strong></td>
                        <td>str</td>
                        <td>Does the system quantify its uncertainty about generated outputs? Describe the method and whether uncertainty increases appropriately for novel outputs.</td>
                    </tr>
                    <tr>
                        <td><strong>calibration_quality</strong></td>
                        <td>str</td>
                        <td>How well-calibrated is the system's confidence in its outputs? Include calibration metrics and whether calibration degrades for novel outputs.</td>
                    </tr>
                    <tr>
                        <td><strong>out_of_distribution_performance</strong></td>
                        <td>str</td>
                        <td>How does the system perform on out-of-distribution examples (analogous to transformational discoveries)? Include specific metrics.</td>
                    </tr>
                    <tr>
                        <td><strong>validation_proxy_metrics</strong></td>
                        <td>str</td>
                        <td>Does the system rely on proxy metrics (plausibility, coherence, consistency) rather than direct validity assessment? Describe what proxies are used.</td>
                    </tr>
                    <tr>
                        <td><strong>human_validation_required</strong></td>
                        <td>bool</td>
                        <td>Does the paper indicate that human validation is required or recommended for the system's outputs? (true/false/null)</td>
                    </tr>
                    <tr>
                        <td><strong>human_validation_frequency</strong></td>
                        <td>str</td>
                        <td>If human validation is used, how often is it required? Does this frequency increase with output novelty?</td>
                    </tr>
                    <tr>
                        <td><strong>formal_verification_used</strong></td>
                        <td>bool</td>
                        <td>Does the system use formal verification methods for validation? (true/false/null)</td>
                    </tr>
                    <tr>
                        <td><strong>domain_formalization_level</strong></td>
                        <td>str</td>
                        <td>How formalized is the domain? (e.g., 'highly formal like mathematics', 'semi-formal like theoretical physics', 'empirical like drug discovery'). Does this affect the generation-validation gap?</td>
                    </tr>
                    <tr>
                        <td><strong>gap_mitigation_strategies</strong></td>
                        <td>str</td>
                        <td>Does the paper propose or test strategies to reduce the generation-validation gap? Describe the strategies and their effectiveness.</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_supporting_gap</strong></td>
                        <td>str</td>
                        <td>Summarize any evidence from this paper that supports the existence of a fabrication-validation gap (generation capabilities exceeding validation capabilities for novel outputs).</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_contradicting_gap</strong></td>
                        <td>str</td>
                        <td>Summarize any evidence from this paper that contradicts or challenges the fabrication-validation gap theory.</td>
                    </tr>
                    <tr>
                        <td><strong>computational_cost_ratio</strong></td>
                        <td>str</td>
                        <td>What is the computational cost ratio of validation to generation? Does this ratio increase with output novelty?</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-53",
    "schema": [
        {
            "name": "system_name",
            "type": "str",
            "description": "The name of the AI/automated system being studied (e.g., 'GPT-4', 'AlphaFold', 'automated theorem prover')."
        },
        {
            "name": "system_type",
            "type": "str",
            "description": "The type/architecture of the system (e.g., 'large language model', 'neural network', 'neurosymbolic system', 'reinforcement learning agent', 'generative model')."
        },
        {
            "name": "scientific_domain",
            "type": "str",
            "description": "The scientific domain where the system operates (e.g., 'chemistry', 'mathematics', 'drug discovery', 'materials science', 'theorem proving', 'hypothesis generation')."
        },
        {
            "name": "output_type",
            "type": "str",
            "description": "What kind of outputs does the system generate? (e.g., 'molecular structures', 'mathematical proofs', 'scientific hypotheses', 'research claims', 'predictions')."
        },
        {
            "name": "novelty_level",
            "type": "str",
            "description": "How novel are the outputs relative to training data? Use terms like 'incremental', 'moderately novel', 'highly novel', 'transformational', 'out-of-distribution', or 'in-distribution'. Include specific details if provided."
        },
        {
            "name": "generation_method",
            "type": "str",
            "description": "How does the system generate novel outputs? Describe the generation mechanism (e.g., 'pattern extrapolation from training data', 'recombination of learned features', 'search over hypothesis space')."
        },
        {
            "name": "validation_method",
            "type": "str",
            "description": "How are the generated outputs validated? Describe the validation mechanism (e.g., 'comparison to known ground truth', 'formal verification', 'experimental testing', 'expert review', 'simulation', 'consistency checks')."
        },
        {
            "name": "generation_performance",
            "type": "str",
            "description": "What is the system's performance at generating outputs? Include metrics like success rate, diversity, novelty scores, or generation speed. Specify units and conditions."
        },
        {
            "name": "validation_performance",
            "type": "str",
            "description": "What is the system's performance at validating outputs? Include metrics like validation accuracy, precision, recall, or reliability. Specify for different novelty levels if available."
        },
        {
            "name": "false_positive_rate",
            "type": "str",
            "description": "What is the false positive rate (invalid outputs marked as valid)? Specify the rate and conditions. Note if this varies with output novelty."
        },
        {
            "name": "false_negative_rate",
            "type": "str",
            "description": "What is the false negative rate (valid outputs marked as invalid)? Specify the rate and conditions. Note if this varies with output novelty."
        },
        {
            "name": "performance_vs_novelty",
            "type": "str",
            "description": "How does validation performance change with output novelty? Describe any reported relationship between novelty level and validation accuracy/reliability."
        },
        {
            "name": "generation_validation_comparison",
            "type": "str",
            "description": "Does the paper compare generation capabilities to validation capabilities? Describe any asymmetry or gap between the two, especially as novelty increases."
        },
        {
            "name": "uncertainty_quantification",
            "type": "str",
            "description": "Does the system quantify its uncertainty about generated outputs? Describe the method and whether uncertainty increases appropriately for novel outputs."
        },
        {
            "name": "calibration_quality",
            "type": "str",
            "description": "How well-calibrated is the system's confidence in its outputs? Include calibration metrics and whether calibration degrades for novel outputs."
        },
        {
            "name": "out_of_distribution_performance",
            "type": "str",
            "description": "How does the system perform on out-of-distribution examples (analogous to transformational discoveries)? Include specific metrics."
        },
        {
            "name": "validation_proxy_metrics",
            "type": "str",
            "description": "Does the system rely on proxy metrics (plausibility, coherence, consistency) rather than direct validity assessment? Describe what proxies are used."
        },
        {
            "name": "human_validation_required",
            "type": "bool",
            "description": "Does the paper indicate that human validation is required or recommended for the system's outputs? (true/false/null)"
        },
        {
            "name": "human_validation_frequency",
            "type": "str",
            "description": "If human validation is used, how often is it required? Does this frequency increase with output novelty?"
        },
        {
            "name": "formal_verification_used",
            "type": "bool",
            "description": "Does the system use formal verification methods for validation? (true/false/null)"
        },
        {
            "name": "domain_formalization_level",
            "type": "str",
            "description": "How formalized is the domain? (e.g., 'highly formal like mathematics', 'semi-formal like theoretical physics', 'empirical like drug discovery'). Does this affect the generation-validation gap?"
        },
        {
            "name": "gap_mitigation_strategies",
            "type": "str",
            "description": "Does the paper propose or test strategies to reduce the generation-validation gap? Describe the strategies and their effectiveness."
        },
        {
            "name": "evidence_supporting_gap",
            "type": "str",
            "description": "Summarize any evidence from this paper that supports the existence of a fabrication-validation gap (generation capabilities exceeding validation capabilities for novel outputs)."
        },
        {
            "name": "evidence_contradicting_gap",
            "type": "str",
            "description": "Summarize any evidence from this paper that contradicts or challenges the fabrication-validation gap theory."
        },
        {
            "name": "computational_cost_ratio",
            "type": "str",
            "description": "What is the computational cost ratio of validation to generation? Does this ratio increase with output novelty?"
        }
    ],
    "extraction_query": "Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
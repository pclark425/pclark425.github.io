<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-40 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-40</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-40</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the model or system that performs vision-language grounding for embodied tasks.</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A detailed description of the model architecture, including how it performs vision-language grounding (e.g., 'uses CLIP encoder with cross-attention to language tokens', 'object-centric representations with spatial embeddings', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>visual_encoder_type</strong></td>
                        <td>str</td>
                        <td>What type of visual encoder is used? (e.g., 'CLIP ViT-L/14', 'ResNet-50 pretrained on ImageNet', 'frozen CLIP', 'fine-tuned ViT', 'R-CNN for object detection', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>visual_encoder_pretraining</strong></td>
                        <td>str</td>
                        <td>What was the visual encoder pretrained on? Include dataset names, size, and type (e.g., 'CLIP pretrained on 400M image-text pairs from web', 'ImageNet 1K classification', 'Ego4D egocentric videos', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>grounding_mechanism</strong></td>
                        <td>str</td>
                        <td>How does the model ground language to visual features? Be specific about the mechanism (e.g., 'cross-attention between language tokens and image patches', 'FiLM conditioning on visual features', 'object-centric attention', 'region-level feature matching', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>representation_level</strong></td>
                        <td>str</td>
                        <td>What level of visual representation is used for grounding? (e.g., 'global image features', 'object-centric', 'region-level', 'pixel-level', 'scene-level', '3D point cloud', 'multi-level', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>spatial_representation</strong></td>
                        <td>str</td>
                        <td>Does the model use explicit spatial representations? If yes, describe (e.g., '3D coordinates', 'depth maps', 'spatial relation embeddings', 'interaction tokens', 'bounding boxes', 'none/implicit only', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>embodied_task_type</strong></td>
                        <td>str</td>
                        <td>What type of embodied task is evaluated? (e.g., 'object manipulation', 'vision-language navigation', 'mobile manipulation', 'instruction following', '3D scene understanding', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>embodied_task_name</strong></td>
                        <td>str</td>
                        <td>The specific name of the benchmark or task (e.g., 'ALFRED', 'R2R', 'RLBench', 'MetaWorld', 'Habitat ObjectNav', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>visual_domain</strong></td>
                        <td>str</td>
                        <td>What is the visual domain of the embodied task? (e.g., 'photorealistic simulation', 'real-world robot', 'synthetic rendering', 'egocentric views', 'third-person views', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metric</strong></td>
                        <td>str</td>
                        <td>What metric is used to evaluate performance? (e.g., 'success rate', 'SPL', 'task completion', 'accuracy', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_value</strong></td>
                        <td>str</td>
                        <td>The numerical performance value achieved by this model on the task. Include units and context (e.g., '68.2% success rate', '45.3 SPL', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>has_grounding_ablation</strong></td>
                        <td>bool</td>
                        <td>Does the paper include an ablation study that isolates the contribution of vision-language grounding? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_without_grounding</strong></td>
                        <td>str</td>
                        <td>If there's an ablation, what is the performance without vision-language grounding or with degraded grounding? Include numerical results with units.</td>
                    </tr>
                    <tr>
                        <td><strong>grounding_improvement</strong></td>
                        <td>str</td>
                        <td>What is the improvement from using vision-language grounding? (e.g., '+15.3% success rate', '2x better performance', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>has_encoder_comparison</strong></td>
                        <td>bool</td>
                        <td>Does the paper compare different visual encoders or pretraining approaches? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>encoder_comparison_results</strong></td>
                        <td>str</td>
                        <td>If encoders are compared, summarize the results (e.g., 'CLIP outperforms ImageNet by +12% SR', 'frozen CLIP achieves 85% of fine-tuned performance', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>perception_bottleneck_identified</strong></td>
                        <td>bool</td>
                        <td>Does the paper identify perception or grounding as a bottleneck or major failure mode? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>perception_bottleneck_details</strong></td>
                        <td>str</td>
                        <td>If perception is identified as a bottleneck, describe the specific issues (e.g., 'segmentation errors account for 35% of failures', 'occlusion causes 20% failure rate', 'novel objects not in pretraining data', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>failure_mode_analysis</strong></td>
                        <td>str</td>
                        <td>Describe any failure mode analysis related to grounding, including specific failure types and their frequency (e.g., 'fails on small objects (15% of cases)', 'viewpoint changes cause 25% errors', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>domain_shift_handling</strong></td>
                        <td>str</td>
                        <td>How does the model handle domain shift between pretraining and embodied observations? Describe any techniques or results (e.g., 'domain adaptation layer', 'fine-tuning on robot data', 'performance drops 30% on real-world vs. sim', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>novel_object_performance</strong></td>
                        <td>str</td>
                        <td>How does the model perform on novel objects not seen during pretraining? Include numerical results if available (e.g., '45% SR on novel objects vs. 78% on seen objects', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>frozen_vs_finetuned</strong></td>
                        <td>str</td>
                        <td>Does the paper compare frozen vs. fine-tuned visual encoders? If yes, summarize results (e.g., 'frozen achieves 82% of fine-tuned performance', 'fine-tuning improves by +8% SR', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>pretraining_scale_effect</strong></td>
                        <td>str</td>
                        <td>Does the paper study the effect of pretraining data scale on grounding quality? Summarize findings (e.g., 'performance scales log-linearly with pretraining data size', '10x more data gives +5% improvement', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>fusion_mechanism</strong></td>
                        <td>str</td>
                        <td>How are language and visual features fused? (e.g., 'cross-attention', 'concatenation', 'FiLM', 'multiplicative interaction', 'early fusion', 'late fusion', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>sample_efficiency</strong></td>
                        <td>str</td>
                        <td>How many demonstrations or training samples are needed? Compare with and without grounding if available (e.g., '10 demos with grounding vs. 100 without', '5x sample efficiency improvement', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings_grounding</strong></td>
                        <td>str</td>
                        <td>Summarize the key findings about vision-language grounding in a concise, information-dense way, focusing on what makes grounding effective or ineffective for embodied tasks.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-40",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the model or system that performs vision-language grounding for embodied tasks."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A detailed description of the model architecture, including how it performs vision-language grounding (e.g., 'uses CLIP encoder with cross-attention to language tokens', 'object-centric representations with spatial embeddings', etc.)."
        },
        {
            "name": "visual_encoder_type",
            "type": "str",
            "description": "What type of visual encoder is used? (e.g., 'CLIP ViT-L/14', 'ResNet-50 pretrained on ImageNet', 'frozen CLIP', 'fine-tuned ViT', 'R-CNN for object detection', etc.)"
        },
        {
            "name": "visual_encoder_pretraining",
            "type": "str",
            "description": "What was the visual encoder pretrained on? Include dataset names, size, and type (e.g., 'CLIP pretrained on 400M image-text pairs from web', 'ImageNet 1K classification', 'Ego4D egocentric videos', etc.)."
        },
        {
            "name": "grounding_mechanism",
            "type": "str",
            "description": "How does the model ground language to visual features? Be specific about the mechanism (e.g., 'cross-attention between language tokens and image patches', 'FiLM conditioning on visual features', 'object-centric attention', 'region-level feature matching', etc.)."
        },
        {
            "name": "representation_level",
            "type": "str",
            "description": "What level of visual representation is used for grounding? (e.g., 'global image features', 'object-centric', 'region-level', 'pixel-level', 'scene-level', '3D point cloud', 'multi-level', etc.)."
        },
        {
            "name": "spatial_representation",
            "type": "str",
            "description": "Does the model use explicit spatial representations? If yes, describe (e.g., '3D coordinates', 'depth maps', 'spatial relation embeddings', 'interaction tokens', 'bounding boxes', 'none/implicit only', etc.)."
        },
        {
            "name": "embodied_task_type",
            "type": "str",
            "description": "What type of embodied task is evaluated? (e.g., 'object manipulation', 'vision-language navigation', 'mobile manipulation', 'instruction following', '3D scene understanding', etc.)."
        },
        {
            "name": "embodied_task_name",
            "type": "str",
            "description": "The specific name of the benchmark or task (e.g., 'ALFRED', 'R2R', 'RLBench', 'MetaWorld', 'Habitat ObjectNav', etc.)."
        },
        {
            "name": "visual_domain",
            "type": "str",
            "description": "What is the visual domain of the embodied task? (e.g., 'photorealistic simulation', 'real-world robot', 'synthetic rendering', 'egocentric views', 'third-person views', etc.)."
        },
        {
            "name": "performance_metric",
            "type": "str",
            "description": "What metric is used to evaluate performance? (e.g., 'success rate', 'SPL', 'task completion', 'accuracy', etc.)."
        },
        {
            "name": "performance_value",
            "type": "str",
            "description": "The numerical performance value achieved by this model on the task. Include units and context (e.g., '68.2% success rate', '45.3 SPL', etc.)."
        },
        {
            "name": "has_grounding_ablation",
            "type": "bool",
            "description": "Does the paper include an ablation study that isolates the contribution of vision-language grounding? (true, false, or null for no information)"
        },
        {
            "name": "performance_without_grounding",
            "type": "str",
            "description": "If there's an ablation, what is the performance without vision-language grounding or with degraded grounding? Include numerical results with units."
        },
        {
            "name": "grounding_improvement",
            "type": "str",
            "description": "What is the improvement from using vision-language grounding? (e.g., '+15.3% success rate', '2x better performance', etc.)"
        },
        {
            "name": "has_encoder_comparison",
            "type": "bool",
            "description": "Does the paper compare different visual encoders or pretraining approaches? (true, false, or null)"
        },
        {
            "name": "encoder_comparison_results",
            "type": "str",
            "description": "If encoders are compared, summarize the results (e.g., 'CLIP outperforms ImageNet by +12% SR', 'frozen CLIP achieves 85% of fine-tuned performance', etc.)."
        },
        {
            "name": "perception_bottleneck_identified",
            "type": "bool",
            "description": "Does the paper identify perception or grounding as a bottleneck or major failure mode? (true, false, or null)"
        },
        {
            "name": "perception_bottleneck_details",
            "type": "str",
            "description": "If perception is identified as a bottleneck, describe the specific issues (e.g., 'segmentation errors account for 35% of failures', 'occlusion causes 20% failure rate', 'novel objects not in pretraining data', etc.)."
        },
        {
            "name": "failure_mode_analysis",
            "type": "str",
            "description": "Describe any failure mode analysis related to grounding, including specific failure types and their frequency (e.g., 'fails on small objects (15% of cases)', 'viewpoint changes cause 25% errors', etc.)."
        },
        {
            "name": "domain_shift_handling",
            "type": "str",
            "description": "How does the model handle domain shift between pretraining and embodied observations? Describe any techniques or results (e.g., 'domain adaptation layer', 'fine-tuning on robot data', 'performance drops 30% on real-world vs. sim', etc.)."
        },
        {
            "name": "novel_object_performance",
            "type": "str",
            "description": "How does the model perform on novel objects not seen during pretraining? Include numerical results if available (e.g., '45% SR on novel objects vs. 78% on seen objects', etc.)."
        },
        {
            "name": "frozen_vs_finetuned",
            "type": "str",
            "description": "Does the paper compare frozen vs. fine-tuned visual encoders? If yes, summarize results (e.g., 'frozen achieves 82% of fine-tuned performance', 'fine-tuning improves by +8% SR', etc.)."
        },
        {
            "name": "pretraining_scale_effect",
            "type": "str",
            "description": "Does the paper study the effect of pretraining data scale on grounding quality? Summarize findings (e.g., 'performance scales log-linearly with pretraining data size', '10x more data gives +5% improvement', etc.)."
        },
        {
            "name": "fusion_mechanism",
            "type": "str",
            "description": "How are language and visual features fused? (e.g., 'cross-attention', 'concatenation', 'FiLM', 'multiplicative interaction', 'early fusion', 'late fusion', etc.)."
        },
        {
            "name": "sample_efficiency",
            "type": "str",
            "description": "How many demonstrations or training samples are needed? Compare with and without grounding if available (e.g., '10 demos with grounding vs. 100 without', '5x sample efficiency improvement', etc.)."
        },
        {
            "name": "key_findings_grounding",
            "type": "str",
            "description": "Summarize the key findings about vision-language grounding in a concise, information-dense way, focusing on what makes grounding effective or ineffective for embodied tasks."
        }
    ],
    "extraction_query": "Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
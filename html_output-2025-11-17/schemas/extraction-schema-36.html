<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-36 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-36</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-36</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>system_name</strong></td>
                        <td>str</td>
                        <td>The name of the automated system, computational method, or machine learning model being evaluated.</td>
                    </tr>
                    <tr>
                        <td><strong>domain</strong></td>
                        <td>str</td>
                        <td>The scientific domain (e.g., drug discovery, materials science, protein engineering, chemistry, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metric_name</strong></td>
                        <td>str</td>
                        <td>The name or description of the proxy/surrogate/computational metric being optimized or predicted (e.g., 'docking score', 'DFT-predicted bandgap', 'predicted binding affinity', 'simulated stability')</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metric_type</strong></td>
                        <td>str</td>
                        <td>What type of proxy is it? (e.g., 'physics-based simulation', 'data-driven ML model', 'empirical correlation', 'expert heuristic', 'hybrid approach')</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metric_description</strong></td>
                        <td>str</td>
                        <td>Detailed description of how the proxy metric is computed or measured, including any computational methods, simulations, or models used.</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_metric_name</strong></td>
                        <td>str</td>
                        <td>The name or description of the ground-truth experimental validation metric (e.g., 'experimental binding affinity', 'synthesized device efficiency', 'clinical trial outcome')</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_validation_method</strong></td>
                        <td>str</td>
                        <td>Detailed description of how ground-truth validation is performed (e.g., 'wet-lab synthesis and testing', 'clinical trials', 'device fabrication and measurement')</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_performance</strong></td>
                        <td>str</td>
                        <td>Quantitative performance metrics on the proxy measure (e.g., 'R²=0.95 on test set', '85% accuracy in computational predictions', 'MAE=0.1 eV'). Include numerical values and units.</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_performance</strong></td>
                        <td>str</td>
                        <td>Quantitative performance metrics on ground-truth validation (e.g., '45% success rate in experimental validation', '12 of 20 predictions confirmed', 'correlation R²=0.6 with experiments'). Include numerical values and units.</td>
                    </tr>
                    <tr>
                        <td><strong>explicit_gap_measurement</strong></td>
                        <td>str</td>
                        <td>If the paper explicitly measures or discusses the gap between proxy and ground-truth performance, describe it quantitatively (e.g., 'false positive rate of 60%', 'proxy overestimates by 40% on average', 'correlation drops from 0.9 to 0.5')</td>
                    </tr>
                    <tr>
                        <td><strong>false_positive_rate</strong></td>
                        <td>str</td>
                        <td>If reported, what is the false positive rate (predictions that looked good on proxy but failed experimental validation)? Include numerical values.</td>
                    </tr>
                    <tr>
                        <td><strong>false_negative_rate</strong></td>
                        <td>str</td>
                        <td>If reported, what is the false negative rate (predictions that looked poor on proxy but succeeded in experimental validation)? Include numerical values.</td>
                    </tr>
                    <tr>
                        <td><strong>has_both_proxy_and_ground_truth</strong></td>
                        <td>bool</td>
                        <td>Does the paper report performance on both proxy metrics AND ground-truth experimental validation? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>validation_performed</strong></td>
                        <td>str</td>
                        <td>What level of validation was performed? (e.g., 'computational only', 'computational + literature comparison', 'computational + experimental synthesis', 'computational + experimental + clinical', 'none - proposed only')</td>
                    </tr>
                    <tr>
                        <td><strong>number_predictions_made</strong></td>
                        <td>str</td>
                        <td>How many predictions or discoveries were made computationally/using the proxy? (numerical value or description)</td>
                    </tr>
                    <tr>
                        <td><strong>number_experimentally_validated</strong></td>
                        <td>str</td>
                        <td>How many of the predictions were experimentally validated? (numerical value or description)</td>
                    </tr>
                    <tr>
                        <td><strong>discovery_novelty</strong></td>
                        <td>str</td>
                        <td>Are the discoveries characterized as incremental, transformational, novel, or within/outside the training distribution? Use exact terminology from the paper.</td>
                    </tr>
                    <tr>
                        <td><strong>extrapolation_distance</strong></td>
                        <td>str</td>
                        <td>Does the paper discuss how far the predictions extrapolate from training data or known examples? Describe quantitatively if possible.</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_bias_correction</strong></td>
                        <td>bool</td>
                        <td>Does the system use explicit methods to correct for systematic biases between proxy and ground-truth metrics (e.g., multifidelity learning, calibration, bias correction)? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_bias_correction_method</strong></td>
                        <td>str</td>
                        <td>If proxy bias correction is used, describe the method in detail.</td>
                    </tr>
                    <tr>
                        <td><strong>validation_cost_time</strong></td>
                        <td>str</td>
                        <td>If discussed, what is the cost (monetary, time, resources) of ground-truth validation compared to proxy evaluation? Be specific.</td>
                    </tr>
                    <tr>
                        <td><strong>domain_maturity</strong></td>
                        <td>str</td>
                        <td>Does the paper characterize the maturity of computational methods in this domain (e.g., 'well-established physics-based models', 'emerging ML methods', 'poorly understood system')?</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_failure_modes</strong></td>
                        <td>str</td>
                        <td>Does the paper discuss specific failure modes of the proxy metric or cases where proxy and ground-truth diverge? Describe in detail.</td>
                    </tr>
                    <tr>
                        <td><strong>uncertainty_quantification</strong></td>
                        <td>bool</td>
                        <td>Does the system provide uncertainty estimates or confidence intervals for its predictions? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>uncertainty_calibration</strong></td>
                        <td>str</td>
                        <td>If uncertainty is quantified, is it calibrated against actual proxy-to-ground-truth gaps? Describe any calibration results.</td>
                    </tr>
                    <tr>
                        <td><strong>multiple_proxy_types</strong></td>
                        <td>bool</td>
                        <td>Does the system use multiple different types of proxy metrics (e.g., combining physics-based and data-driven)? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>validation_cascade</strong></td>
                        <td>str</td>
                        <td>Are there multiple stages of validation (e.g., computational → in vitro → in vivo → clinical)? Describe the cascade and performance at each stage.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_baseline</strong></td>
                        <td>str</td>
                        <td>How does the proxy-to-ground-truth gap compare to baseline methods or human expert predictions? Include quantitative comparisons if available.</td>
                    </tr>
                    <tr>
                        <td><strong>domain_specific_factors</strong></td>
                        <td>str</td>
                        <td>Does the paper identify domain-specific factors that affect the proxy-to-ground-truth gap (e.g., 'long-timescale dynamics', 'emergent phenomena', 'quantum effects', 'biological complexity')?</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-36",
    "schema": [
        {
            "name": "system_name",
            "type": "str",
            "description": "The name of the automated system, computational method, or machine learning model being evaluated."
        },
        {
            "name": "domain",
            "type": "str",
            "description": "The scientific domain (e.g., drug discovery, materials science, protein engineering, chemistry, etc.)"
        },
        {
            "name": "proxy_metric_name",
            "type": "str",
            "description": "The name or description of the proxy/surrogate/computational metric being optimized or predicted (e.g., 'docking score', 'DFT-predicted bandgap', 'predicted binding affinity', 'simulated stability')"
        },
        {
            "name": "proxy_metric_type",
            "type": "str",
            "description": "What type of proxy is it? (e.g., 'physics-based simulation', 'data-driven ML model', 'empirical correlation', 'expert heuristic', 'hybrid approach')"
        },
        {
            "name": "proxy_metric_description",
            "type": "str",
            "description": "Detailed description of how the proxy metric is computed or measured, including any computational methods, simulations, or models used."
        },
        {
            "name": "ground_truth_metric_name",
            "type": "str",
            "description": "The name or description of the ground-truth experimental validation metric (e.g., 'experimental binding affinity', 'synthesized device efficiency', 'clinical trial outcome')"
        },
        {
            "name": "ground_truth_validation_method",
            "type": "str",
            "description": "Detailed description of how ground-truth validation is performed (e.g., 'wet-lab synthesis and testing', 'clinical trials', 'device fabrication and measurement')"
        },
        {
            "name": "proxy_performance",
            "type": "str",
            "description": "Quantitative performance metrics on the proxy measure (e.g., 'R²=0.95 on test set', '85% accuracy in computational predictions', 'MAE=0.1 eV'). Include numerical values and units."
        },
        {
            "name": "ground_truth_performance",
            "type": "str",
            "description": "Quantitative performance metrics on ground-truth validation (e.g., '45% success rate in experimental validation', '12 of 20 predictions confirmed', 'correlation R²=0.6 with experiments'). Include numerical values and units."
        },
        {
            "name": "explicit_gap_measurement",
            "type": "str",
            "description": "If the paper explicitly measures or discusses the gap between proxy and ground-truth performance, describe it quantitatively (e.g., 'false positive rate of 60%', 'proxy overestimates by 40% on average', 'correlation drops from 0.9 to 0.5')"
        },
        {
            "name": "false_positive_rate",
            "type": "str",
            "description": "If reported, what is the false positive rate (predictions that looked good on proxy but failed experimental validation)? Include numerical values."
        },
        {
            "name": "false_negative_rate",
            "type": "str",
            "description": "If reported, what is the false negative rate (predictions that looked poor on proxy but succeeded in experimental validation)? Include numerical values."
        },
        {
            "name": "has_both_proxy_and_ground_truth",
            "type": "bool",
            "description": "Does the paper report performance on both proxy metrics AND ground-truth experimental validation? (true, false, or null)"
        },
        {
            "name": "validation_performed",
            "type": "str",
            "description": "What level of validation was performed? (e.g., 'computational only', 'computational + literature comparison', 'computational + experimental synthesis', 'computational + experimental + clinical', 'none - proposed only')"
        },
        {
            "name": "number_predictions_made",
            "type": "str",
            "description": "How many predictions or discoveries were made computationally/using the proxy? (numerical value or description)"
        },
        {
            "name": "number_experimentally_validated",
            "type": "str",
            "description": "How many of the predictions were experimentally validated? (numerical value or description)"
        },
        {
            "name": "discovery_novelty",
            "type": "str",
            "description": "Are the discoveries characterized as incremental, transformational, novel, or within/outside the training distribution? Use exact terminology from the paper."
        },
        {
            "name": "extrapolation_distance",
            "type": "str",
            "description": "Does the paper discuss how far the predictions extrapolate from training data or known examples? Describe quantitatively if possible."
        },
        {
            "name": "proxy_bias_correction",
            "type": "bool",
            "description": "Does the system use explicit methods to correct for systematic biases between proxy and ground-truth metrics (e.g., multifidelity learning, calibration, bias correction)? (true, false, or null)"
        },
        {
            "name": "proxy_bias_correction_method",
            "type": "str",
            "description": "If proxy bias correction is used, describe the method in detail."
        },
        {
            "name": "validation_cost_time",
            "type": "str",
            "description": "If discussed, what is the cost (monetary, time, resources) of ground-truth validation compared to proxy evaluation? Be specific."
        },
        {
            "name": "domain_maturity",
            "type": "str",
            "description": "Does the paper characterize the maturity of computational methods in this domain (e.g., 'well-established physics-based models', 'emerging ML methods', 'poorly understood system')?"
        },
        {
            "name": "proxy_failure_modes",
            "type": "str",
            "description": "Does the paper discuss specific failure modes of the proxy metric or cases where proxy and ground-truth diverge? Describe in detail."
        },
        {
            "name": "uncertainty_quantification",
            "type": "bool",
            "description": "Does the system provide uncertainty estimates or confidence intervals for its predictions? (true, false, or null)"
        },
        {
            "name": "uncertainty_calibration",
            "type": "str",
            "description": "If uncertainty is quantified, is it calibrated against actual proxy-to-ground-truth gaps? Describe any calibration results."
        },
        {
            "name": "multiple_proxy_types",
            "type": "bool",
            "description": "Does the system use multiple different types of proxy metrics (e.g., combining physics-based and data-driven)? (true, false, or null)"
        },
        {
            "name": "validation_cascade",
            "type": "str",
            "description": "Are there multiple stages of validation (e.g., computational → in vitro → in vivo → clinical)? Describe the cascade and performance at each stage."
        },
        {
            "name": "comparison_to_baseline",
            "type": "str",
            "description": "How does the proxy-to-ground-truth gap compare to baseline methods or human expert predictions? Include quantitative comparisons if available."
        },
        {
            "name": "domain_specific_factors",
            "type": "str",
            "description": "Does the paper identify domain-specific factors that affect the proxy-to-ground-truth gap (e.g., 'long-timescale dynamics', 'emergent phenomena', 'quantum effects', 'biological complexity')?"
        }
    ],
    "extraction_query": "Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
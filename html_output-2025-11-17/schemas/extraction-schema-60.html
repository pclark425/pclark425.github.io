<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-60 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-60</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-60</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>system_name</strong></td>
                        <td>str</td>
                        <td>The name of the automated system, computational method, or machine learning model being evaluated.</td>
                    </tr>
                    <tr>
                        <td><strong>system_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the system, including its approach and methodology.</td>
                    </tr>
                    <tr>
                        <td><strong>domain</strong></td>
                        <td>str</td>
                        <td>The scientific domain (e.g., drug discovery, materials science, protein structure, chemistry, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metric_name</strong></td>
                        <td>str</td>
                        <td>The name or type of proxy metric, computational prediction, surrogate objective, or intermediate validation measure used (e.g., docking score, DFT energy, simulated property, ML prediction, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metric_description</strong></td>
                        <td>str</td>
                        <td>Detailed description of what the proxy metric measures and how it is computed or predicted.</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metric_type</strong></td>
                        <td>str</td>
                        <td>Categorize the proxy as: physics-based simulation, data-driven ML prediction, empirical surrogate, hybrid, or other. Be specific.</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_metric</strong></td>
                        <td>str</td>
                        <td>The ground-truth experimental outcome, real-world validation, or gold-standard measurement (e.g., experimental synthesis, clinical trial, device performance, wet-lab assay, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_description</strong></td>
                        <td>str</td>
                        <td>Detailed description of how ground-truth validation is performed and what it measures.</td>
                    </tr>
                    <tr>
                        <td><strong>has_both_proxy_and_ground_truth</strong></td>
                        <td>bool</td>
                        <td>Does the paper report both proxy/computational metrics AND experimental/ground-truth validation for the same discoveries? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>quantitative_gap_measure</strong></td>
                        <td>str</td>
                        <td>Quantitative measure of the gap between proxy and ground truth (e.g., correlation coefficient R², false positive rate, success rate difference, MAE, RMSE, percentage agreement, etc.). Include numerical values and units.</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_performance</strong></td>
                        <td>str</td>
                        <td>Performance or success rate when evaluated on the proxy metric. Include numerical values.</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_performance</strong></td>
                        <td>str</td>
                        <td>Performance or success rate when evaluated on ground-truth validation. Include numerical values.</td>
                    </tr>
                    <tr>
                        <td><strong>false_positive_rate</strong></td>
                        <td>str</td>
                        <td>If reported, what is the false positive rate (predictions that score well on proxy but fail ground-truth validation)? Include numerical values.</td>
                    </tr>
                    <tr>
                        <td><strong>false_negative_rate</strong></td>
                        <td>str</td>
                        <td>If reported, what is the false negative rate (predictions that score poorly on proxy but succeed in ground-truth validation)? Include numerical values.</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_characterization</strong></td>
                        <td>str</td>
                        <td>How novel or extrapolative are the discoveries? Are they characterized as incremental, transformational, in-distribution, out-of-distribution, near training data, or far from training data?</td>
                    </tr>
                    <tr>
                        <td><strong>gap_varies_with_novelty</strong></td>
                        <td>bool</td>
                        <td>Does the paper report that the proxy-to-ground-truth gap varies with novelty or extrapolation distance? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>gap_variation_details</strong></td>
                        <td>str</td>
                        <td>If the gap varies with novelty, describe how (e.g., larger gap for novel discoveries, smaller gap for incremental ones, etc.). Include quantitative details if available.</td>
                    </tr>
                    <tr>
                        <td><strong>gap_reduction_method</strong></td>
                        <td>str</td>
                        <td>Does the paper describe methods to reduce the proxy-to-ground-truth gap (e.g., multifidelity learning, bias correction, calibration, ensemble methods, uncertainty quantification)? Describe the method.</td>
                    </tr>
                    <tr>
                        <td><strong>gap_reduction_effectiveness</strong></td>
                        <td>str</td>
                        <td>If gap reduction methods are used, how effective are they? Include quantitative measures of improvement.</td>
                    </tr>
                    <tr>
                        <td><strong>validation_cost_comparison</strong></td>
                        <td>str</td>
                        <td>Does the paper discuss the relative cost (time, money, resources) of proxy evaluation vs ground-truth validation? Describe the comparison.</td>
                    </tr>
                    <tr>
                        <td><strong>temporal_validation</strong></td>
                        <td>str</td>
                        <td>For predictions requiring long-term validation (stability, degradation, etc.), how does short-term proxy performance compare to long-term ground-truth outcomes?</td>
                    </tr>
                    <tr>
                        <td><strong>domain_maturity</strong></td>
                        <td>str</td>
                        <td>Does the paper characterize the maturity of computational methods in this domain (e.g., well-established physics, emerging ML methods, poorly understood phenomena)?</td>
                    </tr>
                    <tr>
                        <td><strong>uncertainty_quantification</strong></td>
                        <td>bool</td>
                        <td>Does the system provide uncertainty estimates or confidence intervals for its predictions? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>uncertainty_calibration</strong></td>
                        <td>str</td>
                        <td>If uncertainty is quantified, is it calibrated against actual proxy-to-ground-truth gap? Describe the calibration quality with quantitative measures if available.</td>
                    </tr>
                    <tr>
                        <td><strong>multiple_proxies</strong></td>
                        <td>bool</td>
                        <td>Does the system use multiple proxy metrics simultaneously? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_correlation</strong></td>
                        <td>str</td>
                        <td>If multiple proxies are used, are their failure modes correlated or independent? Describe any analysis of this.</td>
                    </tr>
                    <tr>
                        <td><strong>validation_cascade</strong></td>
                        <td>str</td>
                        <td>Does the paper describe a validation cascade (e.g., computational → intermediate experimental → final validation)? Describe the stages and how errors propagate.</td>
                    </tr>
                    <tr>
                        <td><strong>publication_bias_discussion</strong></td>
                        <td>bool</td>
                        <td>Does the paper discuss publication bias or selective reporting of successful predictions? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_challenges</strong></td>
                        <td>str</td>
                        <td>What limitations or challenges are identified regarding the proxy-to-ground-truth gap? Be specific and detailed.</td>
                    </tr>
                    <tr>
                        <td><strong>domain_specific_factors</strong></td>
                        <td>str</td>
                        <td>What domain-specific factors affect the proxy-to-ground-truth gap (e.g., chaotic dynamics, emergent phenomena, computational intractability, measurement noise)?</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-60",
    "schema": [
        {
            "name": "system_name",
            "type": "str",
            "description": "The name of the automated system, computational method, or machine learning model being evaluated."
        },
        {
            "name": "system_description",
            "type": "str",
            "description": "A brief description of the system, including its approach and methodology."
        },
        {
            "name": "domain",
            "type": "str",
            "description": "The scientific domain (e.g., drug discovery, materials science, protein structure, chemistry, etc.)"
        },
        {
            "name": "proxy_metric_name",
            "type": "str",
            "description": "The name or type of proxy metric, computational prediction, surrogate objective, or intermediate validation measure used (e.g., docking score, DFT energy, simulated property, ML prediction, etc.)"
        },
        {
            "name": "proxy_metric_description",
            "type": "str",
            "description": "Detailed description of what the proxy metric measures and how it is computed or predicted."
        },
        {
            "name": "proxy_metric_type",
            "type": "str",
            "description": "Categorize the proxy as: physics-based simulation, data-driven ML prediction, empirical surrogate, hybrid, or other. Be specific."
        },
        {
            "name": "ground_truth_metric",
            "type": "str",
            "description": "The ground-truth experimental outcome, real-world validation, or gold-standard measurement (e.g., experimental synthesis, clinical trial, device performance, wet-lab assay, etc.)"
        },
        {
            "name": "ground_truth_description",
            "type": "str",
            "description": "Detailed description of how ground-truth validation is performed and what it measures."
        },
        {
            "name": "has_both_proxy_and_ground_truth",
            "type": "bool",
            "description": "Does the paper report both proxy/computational metrics AND experimental/ground-truth validation for the same discoveries? (true, false, or null)"
        },
        {
            "name": "quantitative_gap_measure",
            "type": "str",
            "description": "Quantitative measure of the gap between proxy and ground truth (e.g., correlation coefficient R², false positive rate, success rate difference, MAE, RMSE, percentage agreement, etc.). Include numerical values and units."
        },
        {
            "name": "proxy_performance",
            "type": "str",
            "description": "Performance or success rate when evaluated on the proxy metric. Include numerical values."
        },
        {
            "name": "ground_truth_performance",
            "type": "str",
            "description": "Performance or success rate when evaluated on ground-truth validation. Include numerical values."
        },
        {
            "name": "false_positive_rate",
            "type": "str",
            "description": "If reported, what is the false positive rate (predictions that score well on proxy but fail ground-truth validation)? Include numerical values."
        },
        {
            "name": "false_negative_rate",
            "type": "str",
            "description": "If reported, what is the false negative rate (predictions that score poorly on proxy but succeed in ground-truth validation)? Include numerical values."
        },
        {
            "name": "novelty_characterization",
            "type": "str",
            "description": "How novel or extrapolative are the discoveries? Are they characterized as incremental, transformational, in-distribution, out-of-distribution, near training data, or far from training data?"
        },
        {
            "name": "gap_varies_with_novelty",
            "type": "bool",
            "description": "Does the paper report that the proxy-to-ground-truth gap varies with novelty or extrapolation distance? (true, false, or null)"
        },
        {
            "name": "gap_variation_details",
            "type": "str",
            "description": "If the gap varies with novelty, describe how (e.g., larger gap for novel discoveries, smaller gap for incremental ones, etc.). Include quantitative details if available."
        },
        {
            "name": "gap_reduction_method",
            "type": "str",
            "description": "Does the paper describe methods to reduce the proxy-to-ground-truth gap (e.g., multifidelity learning, bias correction, calibration, ensemble methods, uncertainty quantification)? Describe the method."
        },
        {
            "name": "gap_reduction_effectiveness",
            "type": "str",
            "description": "If gap reduction methods are used, how effective are they? Include quantitative measures of improvement."
        },
        {
            "name": "validation_cost_comparison",
            "type": "str",
            "description": "Does the paper discuss the relative cost (time, money, resources) of proxy evaluation vs ground-truth validation? Describe the comparison."
        },
        {
            "name": "temporal_validation",
            "type": "str",
            "description": "For predictions requiring long-term validation (stability, degradation, etc.), how does short-term proxy performance compare to long-term ground-truth outcomes?"
        },
        {
            "name": "domain_maturity",
            "type": "str",
            "description": "Does the paper characterize the maturity of computational methods in this domain (e.g., well-established physics, emerging ML methods, poorly understood phenomena)?"
        },
        {
            "name": "uncertainty_quantification",
            "type": "bool",
            "description": "Does the system provide uncertainty estimates or confidence intervals for its predictions? (true, false, or null)"
        },
        {
            "name": "uncertainty_calibration",
            "type": "str",
            "description": "If uncertainty is quantified, is it calibrated against actual proxy-to-ground-truth gap? Describe the calibration quality with quantitative measures if available."
        },
        {
            "name": "multiple_proxies",
            "type": "bool",
            "description": "Does the system use multiple proxy metrics simultaneously? (true, false, or null)"
        },
        {
            "name": "proxy_correlation",
            "type": "str",
            "description": "If multiple proxies are used, are their failure modes correlated or independent? Describe any analysis of this."
        },
        {
            "name": "validation_cascade",
            "type": "str",
            "description": "Does the paper describe a validation cascade (e.g., computational → intermediate experimental → final validation)? Describe the stages and how errors propagate."
        },
        {
            "name": "publication_bias_discussion",
            "type": "bool",
            "description": "Does the paper discuss publication bias or selective reporting of successful predictions? (true, false, or null)"
        },
        {
            "name": "limitations_challenges",
            "type": "str",
            "description": "What limitations or challenges are identified regarding the proxy-to-ground-truth gap? Be specific and detailed."
        },
        {
            "name": "domain_specific_factors",
            "type": "str",
            "description": "What domain-specific factors affect the proxy-to-ground-truth gap (e.g., chaotic dynamics, emergent phenomena, computational intractability, measurement noise)?"
        }
    ],
    "extraction_query": "Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-9 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-9</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-9</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how instruction tuning and explicit prompting (e.g., chain-of-thought or step-by-step reasoning) affect theory-of-mind task performance, including baseline comparisons, evaluation methods, improvements, and limitations.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the evaluated large language model (e.g., GPT-4, GPT-3.5).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM, including architectural details and any instruction tuning specifics.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The model's size or parameter count (e.g., 1B, 70B).</td>
                    </tr>
                    <tr>
                        <td><strong>prompting_technique</strong></td>
                        <td>str</td>
                        <td>The specific prompting method used (e.g., chain-of-thought, step-by-step) to scaffold reasoning.</td>
                    </tr>
                    <tr>
                        <td><strong>instruction_tuning_method</strong></td>
                        <td>str</td>
                        <td>Details of the instruction tuning approach applied (e.g., RLHF, fine-tuning on specialized datasets).</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the theory-of-mind benchmark or task evaluated in the study.</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the ToM task, including the type of mental state inference (e.g., first-order, second-order).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_with_scaffold</strong></td>
                        <td>str</td>
                        <td>Performance metrics or qualitative assessments when the model uses explicit prompting and instruction tuning (include numerical values if available).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_baseline</strong></td>
                        <td>str</td>
                        <td>Performance metrics or assessments for baseline models without explicit scaffolding.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>The evaluation approach used (e.g., zero-shot, few-shot, chain-of-thought reasoning) for assessing ToM capabilities.</td>
                    </tr>
                    <tr>
                        <td><strong>reported_improvements</strong></td>
                        <td>str</td>
                        <td>Descriptions of improvements in theory-of-mind task performance as a result of instruction tuning and prompting.</td>
                    </tr>
                    <tr>
                        <td><strong>scaffolding_effects</strong></td>
                        <td>str</td>
                        <td>Details on how the explicit prompts (chain-of-thought, step-by-step) scaffold the model's reasoning in multi-step mental state inferences.</td>
                    </tr>
                    <tr>
                        <td><strong>counter_evidence</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failures, or counter-evidence where instruction tuning or prompting did not enhance ToM capabilities.</td>
                    </tr>
                    <tr>
                        <td><strong>experiment_details</strong></td>
                        <td>str</td>
                        <td>Additional experimental details such as control conditions, comparison metrics, and settings that contextualize the impact on ToM reasoning.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_human_baseline</strong></td>
                        <td>str</td>
                        <td>Any direct comparisons provided between the LLM's ToM performance and human-level performance.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-9",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the evaluated large language model (e.g., GPT-4, GPT-3.5)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM, including architectural details and any instruction tuning specifics."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The model's size or parameter count (e.g., 1B, 70B)."
        },
        {
            "name": "prompting_technique",
            "type": "str",
            "description": "The specific prompting method used (e.g., chain-of-thought, step-by-step) to scaffold reasoning."
        },
        {
            "name": "instruction_tuning_method",
            "type": "str",
            "description": "Details of the instruction tuning approach applied (e.g., RLHF, fine-tuning on specialized datasets)."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the theory-of-mind benchmark or task evaluated in the study."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the ToM task, including the type of mental state inference (e.g., first-order, second-order)."
        },
        {
            "name": "performance_with_scaffold",
            "type": "str",
            "description": "Performance metrics or qualitative assessments when the model uses explicit prompting and instruction tuning (include numerical values if available)."
        },
        {
            "name": "performance_baseline",
            "type": "str",
            "description": "Performance metrics or assessments for baseline models without explicit scaffolding."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "The evaluation approach used (e.g., zero-shot, few-shot, chain-of-thought reasoning) for assessing ToM capabilities."
        },
        {
            "name": "reported_improvements",
            "type": "str",
            "description": "Descriptions of improvements in theory-of-mind task performance as a result of instruction tuning and prompting."
        },
        {
            "name": "scaffolding_effects",
            "type": "str",
            "description": "Details on how the explicit prompts (chain-of-thought, step-by-step) scaffold the model's reasoning in multi-step mental state inferences."
        },
        {
            "name": "counter_evidence",
            "type": "str",
            "description": "Any reported limitations, failures, or counter-evidence where instruction tuning or prompting did not enhance ToM capabilities."
        },
        {
            "name": "experiment_details",
            "type": "str",
            "description": "Additional experimental details such as control conditions, comparison metrics, and settings that contextualize the impact on ToM reasoning."
        },
        {
            "name": "comparison_to_human_baseline",
            "type": "str",
            "description": "Any direct comparisons provided between the LLM's ToM performance and human-level performance."
        }
    ],
    "extraction_query": "Extract any mentions of how instruction tuning and explicit prompting (e.g., chain-of-thought or step-by-step reasoning) affect theory-of-mind task performance, including baseline comparisons, evaluation methods, improvements, and limitations.",
    "supporting_theory_ids": [],
    "model_str": null
}</code></pre>
        </div>
    </div>
</body>
</html>
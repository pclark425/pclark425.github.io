<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-22 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-22</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-22</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_or_agent_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM model or agent being evaluated.</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model or agent, including key architectural features.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>How large was the model, in parameters? (e.g. 1B, 7B, 13B, 70B, 175B, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>qa_task_name</strong></td>
                        <td>str</td>
                        <td>The name of the question-answering or knowledge task the model was evaluated on (e.g. MMLU, TriviaQA, etc.). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>qa_performance</strong></td>
                        <td>str</td>
                        <td>The model's performance on question-answering or knowledge tasks. (numerical with units, e.g. '85% accuracy'. Null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>interactive_task_name</strong></td>
                        <td>str</td>
                        <td>The name of the interactive/procedural task (e.g. WebShop, ALFWorld, tool use benchmarks, planning tasks, multi-step reasoning tasks, etc.). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>interactive_task_type</strong></td>
                        <td>str</td>
                        <td>The type/category of interactive task (e.g. 'tool use', 'planning', 'multi-step reasoning', 'sequential decision-making', 'embodied navigation', 'web navigation', etc.). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>interactive_performance</strong></td>
                        <td>str</td>
                        <td>The model's performance on the interactive/procedural task. (numerical with units, e.g. '45% success rate'. Null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>reports_both_qa_and_interactive</strong></td>
                        <td>bool</td>
                        <td>Does the paper report performance on both QA/knowledge tasks AND interactive/procedural tasks for the same model? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_gap_observed</strong></td>
                        <td>bool</td>
                        <td>Does the paper explicitly observe or discuss a gap between QA performance and interactive/procedural performance? (true, false, or null)</td>
                    </tr>
                    <tr>
                        <td><strong>architectural_features</strong></td>
                        <td>str</td>
                        <td>What architectural features does the agent have? (e.g. 'external memory', 'planning module', 'tool-use interface', 'reflection mechanism', 'chain-of-thought', etc.). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>training_method</strong></td>
                        <td>str</td>
                        <td>What training method was used? (e.g. 'supervised fine-tuning', 'reinforcement learning', 'RLHF', 'prompting only', 'few-shot learning', 'in-context learning', etc.). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>intervention_type</strong></td>
                        <td>str</td>
                        <td>If the paper proposes an intervention to improve interactive performance, what type is it? (e.g. 'architectural change', 'training method', 'prompting strategy', 'hybrid approach', etc.). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>intervention_description</strong></td>
                        <td>str</td>
                        <td>Detailed description of any architectural or training intervention proposed or evaluated to improve interactive/procedural performance. Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>intervention_effect</strong></td>
                        <td>str</td>
                        <td>What was the effect of the intervention on interactive performance? Include before/after metrics if available. (e.g. 'improved success rate from 45% to 67%'). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>hypothesized_cause_of_gap</strong></td>
                        <td>str</td>
                        <td>Does the paper hypothesize or discuss why there might be a gap between QA and interactive performance? If so, what explanation is given? Null if not discussed.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-22",
    "schema": [
        {
            "name": "model_or_agent_name",
            "type": "str",
            "description": "The name of the LLM model or agent being evaluated."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model or agent, including key architectural features."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "How large was the model, in parameters? (e.g. 1B, 7B, 13B, 70B, 175B, etc.)"
        },
        {
            "name": "qa_task_name",
            "type": "str",
            "description": "The name of the question-answering or knowledge task the model was evaluated on (e.g. MMLU, TriviaQA, etc.). Null if not reported."
        },
        {
            "name": "qa_performance",
            "type": "str",
            "description": "The model's performance on question-answering or knowledge tasks. (numerical with units, e.g. '85% accuracy'. Null if not reported)"
        },
        {
            "name": "interactive_task_name",
            "type": "str",
            "description": "The name of the interactive/procedural task (e.g. WebShop, ALFWorld, tool use benchmarks, planning tasks, multi-step reasoning tasks, etc.). Null if not reported."
        },
        {
            "name": "interactive_task_type",
            "type": "str",
            "description": "The type/category of interactive task (e.g. 'tool use', 'planning', 'multi-step reasoning', 'sequential decision-making', 'embodied navigation', 'web navigation', etc.). Null if not reported."
        },
        {
            "name": "interactive_performance",
            "type": "str",
            "description": "The model's performance on the interactive/procedural task. (numerical with units, e.g. '45% success rate'. Null if not reported)"
        },
        {
            "name": "reports_both_qa_and_interactive",
            "type": "bool",
            "description": "Does the paper report performance on both QA/knowledge tasks AND interactive/procedural tasks for the same model? (true, false, or null)"
        },
        {
            "name": "performance_gap_observed",
            "type": "bool",
            "description": "Does the paper explicitly observe or discuss a gap between QA performance and interactive/procedural performance? (true, false, or null)"
        },
        {
            "name": "architectural_features",
            "type": "str",
            "description": "What architectural features does the agent have? (e.g. 'external memory', 'planning module', 'tool-use interface', 'reflection mechanism', 'chain-of-thought', etc.). Null if not reported."
        },
        {
            "name": "training_method",
            "type": "str",
            "description": "What training method was used? (e.g. 'supervised fine-tuning', 'reinforcement learning', 'RLHF', 'prompting only', 'few-shot learning', 'in-context learning', etc.). Null if not reported."
        },
        {
            "name": "intervention_type",
            "type": "str",
            "description": "If the paper proposes an intervention to improve interactive performance, what type is it? (e.g. 'architectural change', 'training method', 'prompting strategy', 'hybrid approach', etc.). Null if not applicable."
        },
        {
            "name": "intervention_description",
            "type": "str",
            "description": "Detailed description of any architectural or training intervention proposed or evaluated to improve interactive/procedural performance. Null if not applicable."
        },
        {
            "name": "intervention_effect",
            "type": "str",
            "description": "What was the effect of the intervention on interactive performance? Include before/after metrics if available. (e.g. 'improved success rate from 45% to 67%'). Null if not applicable."
        },
        {
            "name": "hypothesized_cause_of_gap",
            "type": "str",
            "description": "Does the paper hypothesize or discuss why there might be a gap between QA and interactive performance? If so, what explanation is given? Null if not discussed."
        }
    ],
    "extraction_query": "Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
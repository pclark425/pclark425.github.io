<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-98 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-98</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-98</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM model used to process scholarly papers (e.g. GPT-4, Claude, LLaMA, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>How large was the model, in parameters? (e.g. 1B, 7B, 70B, 175B, etc. null if no information)</td>
                    </tr>
                    <tr>
                        <td><strong>method_name</strong></td>
                        <td>str</td>
                        <td>The name of the method or system used for extracting laws/principles from papers (if given a specific name).</td>
                    </tr>
                    <tr>
                        <td><strong>method_description</strong></td>
                        <td>str</td>
                        <td>A detailed description of the method/approach used by the LLM to process papers and extract qualitative laws. Include details about prompting strategies, multi-step processes, retrieval methods, etc.</td>
                    </tr>
                    <tr>
                        <td><strong>number_of_papers</strong></td>
                        <td>str</td>
                        <td>How many scholarly papers were processed as input? (numerical, e.g. '100 papers', '1000 papers', etc. null if no information)</td>
                    </tr>
                    <tr>
                        <td><strong>domain_or_field</strong></td>
                        <td>str</td>
                        <td>What scientific domain or field were the input papers from? (e.g. 'biology', 'chemistry', 'materials science', 'medicine', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>type_of_laws_extracted</strong></td>
                        <td>str</td>
                        <td>What types of qualitative laws, principles, patterns, or theories were extracted? Be specific about the nature of the output (e.g. 'causal relationships', 'design principles', 'mechanistic explanations', 'empirical generalizations', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>example_laws_extracted</strong></td>
                        <td>str</td>
                        <td>Provide 1-3 concrete examples of the actual qualitative laws or principles that were extracted, if available in the paper.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>How was the quality or correctness of the extracted laws evaluated? (e.g. 'expert human evaluation', 'comparison to ground truth', 'downstream task performance', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metrics</strong></td>
                        <td>str</td>
                        <td>What were the quantitative performance results? Include specific metrics like accuracy, precision, recall, F1, human agreement scores, etc. with numerical values and units.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_baseline</strong></td>
                        <td>str</td>
                        <td>Was the LLM approach compared to a baseline (e.g. human experts, simpler methods, smaller models)? If so, describe the baseline and comparative results.</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings</strong></td>
                        <td>str</td>
                        <td>What were the key findings about what works well or poorly for using LLMs to distill laws from papers? Include insights about model capabilities, limitations, or best practices.</td>
                    </tr>
                    <tr>
                        <td><strong>challenges_limitations</strong></td>
                        <td>str</td>
                        <td>What challenges or limitations were identified in using LLMs for this task? (e.g. hallucination, inconsistency, domain knowledge gaps, scalability issues, etc.)</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-98",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM model used to process scholarly papers (e.g. GPT-4, Claude, LLaMA, etc.)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "How large was the model, in parameters? (e.g. 1B, 7B, 70B, 175B, etc. null if no information)"
        },
        {
            "name": "method_name",
            "type": "str",
            "description": "The name of the method or system used for extracting laws/principles from papers (if given a specific name)."
        },
        {
            "name": "method_description",
            "type": "str",
            "description": "A detailed description of the method/approach used by the LLM to process papers and extract qualitative laws. Include details about prompting strategies, multi-step processes, retrieval methods, etc."
        },
        {
            "name": "number_of_papers",
            "type": "str",
            "description": "How many scholarly papers were processed as input? (numerical, e.g. '100 papers', '1000 papers', etc. null if no information)"
        },
        {
            "name": "domain_or_field",
            "type": "str",
            "description": "What scientific domain or field were the input papers from? (e.g. 'biology', 'chemistry', 'materials science', 'medicine', etc.)"
        },
        {
            "name": "type_of_laws_extracted",
            "type": "str",
            "description": "What types of qualitative laws, principles, patterns, or theories were extracted? Be specific about the nature of the output (e.g. 'causal relationships', 'design principles', 'mechanistic explanations', 'empirical generalizations', etc.)."
        },
        {
            "name": "example_laws_extracted",
            "type": "str",
            "description": "Provide 1-3 concrete examples of the actual qualitative laws or principles that were extracted, if available in the paper."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "How was the quality or correctness of the extracted laws evaluated? (e.g. 'expert human evaluation', 'comparison to ground truth', 'downstream task performance', etc.)"
        },
        {
            "name": "performance_metrics",
            "type": "str",
            "description": "What were the quantitative performance results? Include specific metrics like accuracy, precision, recall, F1, human agreement scores, etc. with numerical values and units."
        },
        {
            "name": "comparison_baseline",
            "type": "str",
            "description": "Was the LLM approach compared to a baseline (e.g. human experts, simpler methods, smaller models)? If so, describe the baseline and comparative results."
        },
        {
            "name": "key_findings",
            "type": "str",
            "description": "What were the key findings about what works well or poorly for using LLMs to distill laws from papers? Include insights about model capabilities, limitations, or best practices."
        },
        {
            "name": "challenges_limitations",
            "type": "str",
            "description": "What challenges or limitations were identified in using LLMs for this task? (e.g. hallucination, inconsistency, domain knowledge gaps, scalability issues, etc.)"
        }
    ],
    "extraction_query": "Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
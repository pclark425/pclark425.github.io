<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-101 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-101</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-101</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>evaluation_method_name</strong></td>
                        <td>str</td>
                        <td>The name of the evaluation method, framework, or approach used to assess LLM-generated scientific theories.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method_description</strong></td>
                        <td>str</td>
                        <td>A detailed description of how the evaluation method works, including what it measures and how it is applied.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_criteria</strong></td>
                        <td>str</td>
                        <td>What specific criteria or dimensions are used to evaluate the theories? (e.g., explanatory power, falsifiability, parsimony, coherence, novelty, empirical adequacy, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM model that generated the theories being evaluated (e.g., GPT-4, Claude, etc.). Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the LLM model in parameters (e.g., 7B, 13B, 70B, 175B, etc.). Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>scientific_domain</strong></td>
                        <td>str</td>
                        <td>What scientific domain or field are the theories about? (e.g., physics, biology, cognitive science, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>theory_type</strong></td>
                        <td>str</td>
                        <td>What type of scientific theories are being evaluated? (e.g., causal theories, mechanistic explanations, predictive models, taxonomies, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>human_comparison</strong></td>
                        <td>bool</td>
                        <td>Does the paper compare LLM-generated theories to human-generated theories? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_results</strong></td>
                        <td>str</td>
                        <td>What were the results of the evaluation? Be specific and information-dense, including numerical scores if available, and comparisons between different methods or models.</td>
                    </tr>
                    <tr>
                        <td><strong>automated_vs_human_evaluation</strong></td>
                        <td>str</td>
                        <td>Is the evaluation automated (e.g., using metrics), human-based (e.g., expert ratings), or hybrid? Describe the approach.</td>
                    </tr>
                    <tr>
                        <td><strong>validation_method</strong></td>
                        <td>str</td>
                        <td>How was the evaluation method itself validated? (e.g., correlation with expert judgments, inter-rater reliability, empirical testing, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_challenges</strong></td>
                        <td>str</td>
                        <td>What limitations or challenges in evaluating LLM-generated theories are discussed in the paper?</td>
                    </tr>
                    <tr>
                        <td><strong>benchmark_dataset</strong></td>
                        <td>str</td>
                        <td>Is there a specific benchmark or dataset used for evaluation? If so, describe it briefly.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-101",
    "schema": [
        {
            "name": "evaluation_method_name",
            "type": "str",
            "description": "The name of the evaluation method, framework, or approach used to assess LLM-generated scientific theories."
        },
        {
            "name": "evaluation_method_description",
            "type": "str",
            "description": "A detailed description of how the evaluation method works, including what it measures and how it is applied."
        },
        {
            "name": "evaluation_criteria",
            "type": "str",
            "description": "What specific criteria or dimensions are used to evaluate the theories? (e.g., explanatory power, falsifiability, parsimony, coherence, novelty, empirical adequacy, etc.)"
        },
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM model that generated the theories being evaluated (e.g., GPT-4, Claude, etc.). Null if not specified."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the LLM model in parameters (e.g., 7B, 13B, 70B, 175B, etc.). Null if not specified."
        },
        {
            "name": "scientific_domain",
            "type": "str",
            "description": "What scientific domain or field are the theories about? (e.g., physics, biology, cognitive science, etc.)"
        },
        {
            "name": "theory_type",
            "type": "str",
            "description": "What type of scientific theories are being evaluated? (e.g., causal theories, mechanistic explanations, predictive models, taxonomies, etc.)"
        },
        {
            "name": "human_comparison",
            "type": "bool",
            "description": "Does the paper compare LLM-generated theories to human-generated theories? (true, false, or null for no information)"
        },
        {
            "name": "evaluation_results",
            "type": "str",
            "description": "What were the results of the evaluation? Be specific and information-dense, including numerical scores if available, and comparisons between different methods or models."
        },
        {
            "name": "automated_vs_human_evaluation",
            "type": "str",
            "description": "Is the evaluation automated (e.g., using metrics), human-based (e.g., expert ratings), or hybrid? Describe the approach."
        },
        {
            "name": "validation_method",
            "type": "str",
            "description": "How was the evaluation method itself validated? (e.g., correlation with expert judgments, inter-rater reliability, empirical testing, etc.)"
        },
        {
            "name": "limitations_challenges",
            "type": "str",
            "description": "What limitations or challenges in evaluating LLM-generated theories are discussed in the paper?"
        },
        {
            "name": "benchmark_dataset",
            "type": "str",
            "description": "Is there a specific benchmark or dataset used for evaluation? If so, describe it briefly."
        }
    ],
    "extraction_query": "Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
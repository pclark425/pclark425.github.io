<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-105 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-105</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-105</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being evaluated (e.g., GPT-4, PaLM, Llama-2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the language model, including size, architecture, or other relevant details.</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_method_name</strong></td>
                        <td>str</td>
                        <td>The name or label of the reasoning method used (e.g., chain-of-thought, self-consistency, ensembling, majority voting, single-step, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_method_type</strong></td>
                        <td>str</td>
                        <td>Categorize the reasoning method as 'diverse' (e.g., multiple reasoning paths, ensembling, varied prompts) or 'similar' (e.g., single chain-of-thought, repeated same prompt), or 'other' if not clear.</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_method_description</strong></td>
                        <td>str</td>
                        <td>A concise description of how the reasoning method works, including how diversity or similarity is achieved (e.g., different prompts, sampling, ensembling, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the reasoning task, benchmark, or dataset used (e.g., GSM8K, ARC, Big-Bench, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the reasoning problem or benchmark.</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>Performance of the model on the task using the specified reasoning method (include metric and value, e.g., accuracy 85%).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_with_other_method</strong></td>
                        <td>bool</td>
                        <td>Does the paper compare this reasoning method with another (e.g., diverse vs. similar)? (true, false, or null if no information)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_other_method</strong></td>
                        <td>str</td>
                        <td>Performance of the model on the same task using the alternative reasoning method (e.g., if this is diverse, report similar, and vice versa).</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings</strong></td>
                        <td>str</td>
                        <td>A concise summary of the main findings regarding the effectiveness of diverse versus similar reasoning methods.</td>
                    </tr>
                    <tr>
                        <td><strong>counter_examples_or_negative_results</strong></td>
                        <td>str</td>
                        <td>Any reported cases where diverse reasoning did not outperform similar reasoning, or where similar reasoning was superior, or other relevant negative results.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-105",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being evaluated (e.g., GPT-4, PaLM, Llama-2, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the language model, including size, architecture, or other relevant details."
        },
        {
            "name": "reasoning_method_name",
            "type": "str",
            "description": "The name or label of the reasoning method used (e.g., chain-of-thought, self-consistency, ensembling, majority voting, single-step, etc.)."
        },
        {
            "name": "reasoning_method_type",
            "type": "str",
            "description": "Categorize the reasoning method as 'diverse' (e.g., multiple reasoning paths, ensembling, varied prompts) or 'similar' (e.g., single chain-of-thought, repeated same prompt), or 'other' if not clear."
        },
        {
            "name": "reasoning_method_description",
            "type": "str",
            "description": "A concise description of how the reasoning method works, including how diversity or similarity is achieved (e.g., different prompts, sampling, ensembling, etc.)."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the reasoning task, benchmark, or dataset used (e.g., GSM8K, ARC, Big-Bench, etc.)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the reasoning problem or benchmark."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "Performance of the model on the task using the specified reasoning method (include metric and value, e.g., accuracy 85%)."
        },
        {
            "name": "comparison_with_other_method",
            "type": "bool",
            "description": "Does the paper compare this reasoning method with another (e.g., diverse vs. similar)? (true, false, or null if no information)"
        },
        {
            "name": "performance_other_method",
            "type": "str",
            "description": "Performance of the model on the same task using the alternative reasoning method (e.g., if this is diverse, report similar, and vice versa)."
        },
        {
            "name": "key_findings",
            "type": "str",
            "description": "A concise summary of the main findings regarding the effectiveness of diverse versus similar reasoning methods."
        },
        {
            "name": "counter_examples_or_negative_results",
            "type": "str",
            "description": "Any reported cases where diverse reasoning did not outperform similar reasoning, or where similar reasoning was superior, or other relevant negative results."
        }
    ],
    "extraction_query": "Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
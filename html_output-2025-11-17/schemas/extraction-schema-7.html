<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-7 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-7</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-7</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how large language models' size and training data diversity influence performance on first-order Theory-of-Mind tasks, including evaluation methods, saturation effects, counter-evidence, and additional influencing factors.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model (e.g., GPT-3, BLOOM, LLaMa-33B) being discussed.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., '175B', '33B').</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model's architecture and training setup relevant to ToM evaluations.</td>
                    </tr>
                    <tr>
                        <td><strong>training_data_description</strong></td>
                        <td>str</td>
                        <td>Details on the training data composition and diversity (e.g., multilingual, varied social narratives) and any notes on data quality.</td>
                    </tr>
                    <tr>
                        <td><strong>tom_task_name</strong></td>
                        <td>str</td>
                        <td>The name of the first-order Theory-of-Mind task or benchmark being evaluated (e.g., Sally-Anne test, false-belief task).</td>
                    </tr>
                    <tr>
                        <td><strong>tom_task_description</strong></td>
                        <td>str</td>
                        <td>A description of the ToM task, including the aspects of mental state reasoning it tests.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_result</strong></td>
                        <td>str</td>
                        <td>Quantitative or qualitative results of the model on the ToM task (e.g., accuracy percentages, success rates).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>The method used for evaluating ToM performance (e.g., zero-shot, few-shot prompting, fine-tuning).</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_model_size</strong></td>
                        <td>str</td>
                        <td>Reported relationship between model size and ToM performance, including any indications of positive correlation or diminishing returns.</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_training_data</strong></td>
                        <td>str</td>
                        <td>Reported influence of training data diversity and richness on ToM capabilities, noting if multilingual or varied data improves performance.</td>
                    </tr>
                    <tr>
                        <td><strong>saturation_effect_mentioned</strong></td>
                        <td>bool</td>
                        <td>Indicates whether the paper mentions any saturation or threshold effects in ToM performance with increasing model size.</td>
                    </tr>
                    <tr>
                        <td><strong>role_of_finetuning</strong></td>
                        <td>str</td>
                        <td>Any details on whether fine-tuning on ToM-specific or related datasets improved performance.</td>
                    </tr>
                    <tr>
                        <td><strong>counter_evidence</strong></td>
                        <td>str</td>
                        <td>Any evidence or observations that challenge the relationship between model size/training data diversity and first-order ToM performance.</td>
                    </tr>
                    <tr>
                        <td><strong>additional_factors</strong></td>
                        <td>str</td>
                        <td>Descriptions of any other factors or interventions mentioned that affect ToM performance (e.g., social narrative pretraining, architectural modifications).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-7",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model (e.g., GPT-3, BLOOM, LLaMa-33B) being discussed."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., '175B', '33B')."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model's architecture and training setup relevant to ToM evaluations."
        },
        {
            "name": "training_data_description",
            "type": "str",
            "description": "Details on the training data composition and diversity (e.g., multilingual, varied social narratives) and any notes on data quality."
        },
        {
            "name": "tom_task_name",
            "type": "str",
            "description": "The name of the first-order Theory-of-Mind task or benchmark being evaluated (e.g., Sally-Anne test, false-belief task)."
        },
        {
            "name": "tom_task_description",
            "type": "str",
            "description": "A description of the ToM task, including the aspects of mental state reasoning it tests."
        },
        {
            "name": "performance_result",
            "type": "str",
            "description": "Quantitative or qualitative results of the model on the ToM task (e.g., accuracy percentages, success rates)."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "The method used for evaluating ToM performance (e.g., zero-shot, few-shot prompting, fine-tuning)."
        },
        {
            "name": "impact_of_model_size",
            "type": "str",
            "description": "Reported relationship between model size and ToM performance, including any indications of positive correlation or diminishing returns."
        },
        {
            "name": "impact_of_training_data",
            "type": "str",
            "description": "Reported influence of training data diversity and richness on ToM capabilities, noting if multilingual or varied data improves performance."
        },
        {
            "name": "saturation_effect_mentioned",
            "type": "bool",
            "description": "Indicates whether the paper mentions any saturation or threshold effects in ToM performance with increasing model size."
        },
        {
            "name": "role_of_finetuning",
            "type": "str",
            "description": "Any details on whether fine-tuning on ToM-specific or related datasets improved performance."
        },
        {
            "name": "counter_evidence",
            "type": "str",
            "description": "Any evidence or observations that challenge the relationship between model size/training data diversity and first-order ToM performance."
        },
        {
            "name": "additional_factors",
            "type": "str",
            "description": "Descriptions of any other factors or interventions mentioned that affect ToM performance (e.g., social narrative pretraining, architectural modifications)."
        }
    ],
    "extraction_query": "Extract any mentions of how large language models' size and training data diversity influence performance on first-order Theory-of-Mind tasks, including evaluation methods, saturation effects, counter-evidence, and additional influencing factors.",
    "supporting_theory_ids": [],
    "model_str": null
}</code></pre>
        </div>
    </div>
</body>
</html>
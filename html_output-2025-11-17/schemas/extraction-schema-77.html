<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-77 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-77</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-77</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model (e.g., GPT-4, PaLM, Llama-2) evaluated on the spatial puzzle task.</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the language model, including architecture, training data, or other relevant details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model, in parameters (e.g., 7B, 13B, 70B), if available.</td>
                    </tr>
                    <tr>
                        <td><strong>puzzle_name</strong></td>
                        <td>str</td>
                        <td>The name of the spatial puzzle or game (e.g., Sudoku, Rubik's Cube, grid puzzles) used in the evaluation.</td>
                    </tr>
                    <tr>
                        <td><strong>puzzle_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the puzzle/game, including what kind of spatial knowledge or reasoning is required.</td>
                    </tr>
                    <tr>
                        <td><strong>input_representation</strong></td>
                        <td>str</td>
                        <td>How the puzzle is presented to the model (e.g., as text, as a grid, as a sequence of numbers, as an image converted to text, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>prompting_method</strong></td>
                        <td>str</td>
                        <td>The method used to prompt the model (e.g., zero-shot, few-shot, chain-of-thought, step-by-step, tool use, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>spatial_reasoning_analysis</strong></td>
                        <td>str</td>
                        <td>Any analysis or evidence about how the model performs spatial reasoning or represents spatial knowledge (e.g., ablation studies, attention analysis, intermediate representations, qualitative analysis of outputs).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metrics</strong></td>
                        <td>str</td>
                        <td>Performance of the model on the puzzle task (e.g., accuracy, success rate, number of puzzles solved, etc.), including units and breakdowns if available.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_modes</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure cases, or counter-examples where the model fails to solve the puzzle or demonstrates lack of spatial reasoning.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_other_models_or_humans</strong></td>
                        <td>str</td>
                        <td>Any comparison of the model's performance or reasoning to other models (e.g., non-language models, vision models) or to human performance.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-77",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model (e.g., GPT-4, PaLM, Llama-2) evaluated on the spatial puzzle task."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the language model, including architecture, training data, or other relevant details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model, in parameters (e.g., 7B, 13B, 70B), if available."
        },
        {
            "name": "puzzle_name",
            "type": "str",
            "description": "The name of the spatial puzzle or game (e.g., Sudoku, Rubik's Cube, grid puzzles) used in the evaluation."
        },
        {
            "name": "puzzle_description",
            "type": "str",
            "description": "A brief description of the puzzle/game, including what kind of spatial knowledge or reasoning is required."
        },
        {
            "name": "input_representation",
            "type": "str",
            "description": "How the puzzle is presented to the model (e.g., as text, as a grid, as a sequence of numbers, as an image converted to text, etc.)."
        },
        {
            "name": "prompting_method",
            "type": "str",
            "description": "The method used to prompt the model (e.g., zero-shot, few-shot, chain-of-thought, step-by-step, tool use, etc.)."
        },
        {
            "name": "spatial_reasoning_analysis",
            "type": "str",
            "description": "Any analysis or evidence about how the model performs spatial reasoning or represents spatial knowledge (e.g., ablation studies, attention analysis, intermediate representations, qualitative analysis of outputs)."
        },
        {
            "name": "performance_metrics",
            "type": "str",
            "description": "Performance of the model on the puzzle task (e.g., accuracy, success rate, number of puzzles solved, etc.), including units and breakdowns if available."
        },
        {
            "name": "limitations_or_failure_modes",
            "type": "str",
            "description": "Any reported limitations, failure cases, or counter-examples where the model fails to solve the puzzle or demonstrates lack of spatial reasoning."
        },
        {
            "name": "comparison_to_other_models_or_humans",
            "type": "str",
            "description": "Any comparison of the model's performance or reasoning to other models (e.g., non-language models, vision models) or to human performance."
        }
    ],
    "extraction_query": "Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
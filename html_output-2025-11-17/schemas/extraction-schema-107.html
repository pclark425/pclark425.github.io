<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-107 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-107</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-107</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being evaluated or improved (e.g., GPT-3, Llama-2, PaLM, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the language model, including architecture, size, and any relevant details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 1B, 7B, 70B, etc.), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>logical_reasoning_task</strong></td>
                        <td>str</td>
                        <td>The name of the logical reasoning task or benchmark used (e.g., ProofWriter, LogiQA, ReClor, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the logical reasoning task or benchmark, including the type of logic or reasoning required.</td>
                    </tr>
                    <tr>
                        <td><strong>method_or_approach</strong></td>
                        <td>str</td>
                        <td>A description of the method or approach used to improve or evaluate logical reasoning (e.g., chain-of-thought prompting, fine-tuning on logic datasets, architectural modifications, use of external tools, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>The performance of the model on the logical reasoning task, including quantitative results (e.g., accuracy, F1, etc.) and any relevant qualitative findings. Include units and breakdowns if available.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_cases</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure cases, or types of logical reasoning where the model struggles or fails.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison</strong></td>
                        <td>str</td>
                        <td>Any comparisons to other models, methods, or human performance, including relative strengths and weaknesses.</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_or_analysis_results</strong></td>
                        <td>str</td>
                        <td>Any ablation studies or analysis results that provide insight into what factors contribute to logical reasoning performance (e.g., effect of model size, training data, prompting strategy, etc.).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-107",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being evaluated or improved (e.g., GPT-3, Llama-2, PaLM, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the language model, including architecture, size, and any relevant details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 1B, 7B, 70B, etc.), or null if not specified."
        },
        {
            "name": "logical_reasoning_task",
            "type": "str",
            "description": "The name of the logical reasoning task or benchmark used (e.g., ProofWriter, LogiQA, ReClor, etc.)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the logical reasoning task or benchmark, including the type of logic or reasoning required."
        },
        {
            "name": "method_or_approach",
            "type": "str",
            "description": "A description of the method or approach used to improve or evaluate logical reasoning (e.g., chain-of-thought prompting, fine-tuning on logic datasets, architectural modifications, use of external tools, etc.)."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "The performance of the model on the logical reasoning task, including quantitative results (e.g., accuracy, F1, etc.) and any relevant qualitative findings. Include units and breakdowns if available."
        },
        {
            "name": "limitations_or_failure_cases",
            "type": "str",
            "description": "Any reported limitations, failure cases, or types of logical reasoning where the model struggles or fails."
        },
        {
            "name": "comparison",
            "type": "str",
            "description": "Any comparisons to other models, methods, or human performance, including relative strengths and weaknesses."
        },
        {
            "name": "ablation_or_analysis_results",
            "type": "str",
            "description": "Any ablation studies or analysis results that provide insight into what factors contribute to logical reasoning performance (e.g., effect of model size, training data, prompting strategy, etc.)."
        }
    ],
    "extraction_query": "Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
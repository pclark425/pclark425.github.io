<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-83 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-83</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-83</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on batteries of cognitive psychology tests, including details of the tests, LLM and human performance, and any direct comparisons or qualitative findings.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model (LLM) being evaluated (e.g., GPT-4, Llama-2, PaLM, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM, including architecture, training data, or other relevant details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the LLM, typically in number of parameters (e.g., 7B, 13B, 70B, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>test_battery_name</strong></td>
                        <td>str</td>
                        <td>The name of the cognitive psychology test battery or benchmark (e.g., 'Cognitive Decathlon', 'PsychBench', 'HumanEval-Cog', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>test_name</strong></td>
                        <td>str</td>
                        <td>The name of the specific cognitive psychology test or task (e.g., 'Digit Span', 'Stroop Task', 'Raven's Progressive Matrices').</td>
                    </tr>
                    <tr>
                        <td><strong>test_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the cognitive psychology test, including what cognitive domain it assesses (e.g., working memory, reasoning, attention, language, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>cognitive_domain</strong></td>
                        <td>str</td>
                        <td>The cognitive domain assessed by the test (e.g., memory, reasoning, attention, language, executive function, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>llm_performance</strong></td>
                        <td>str</td>
                        <td>The performance of the LLM on the test, including numerical scores, accuracy, or qualitative results. Include units and details if available.</td>
                    </tr>
                    <tr>
                        <td><strong>human_baseline_performance</strong></td>
                        <td>str</td>
                        <td>The performance of normal human participants on the same test, including numerical scores, accuracy, or qualitative results. Include units and details if available.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_comparison</strong></td>
                        <td>str</td>
                        <td>A direct comparison between LLM and human performance on the test. Summarize whether the LLM outperforms, matches, or underperforms relative to humans, and by how much.</td>
                    </tr>
                    <tr>
                        <td><strong>notable_limitations_or_failure_modes</strong></td>
                        <td>str</td>
                        <td>Any notable limitations, failure modes, or qualitative differences observed in LLM performance compared to humans (e.g., systematic errors, lack of generalization, brittleness, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>paper_reference</strong></td>
                        <td>str</td>
                        <td>A citation or reference to the paper or section where this result is reported.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-83",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model (LLM) being evaluated (e.g., GPT-4, Llama-2, PaLM, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM, including architecture, training data, or other relevant details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the LLM, typically in number of parameters (e.g., 7B, 13B, 70B, etc.)."
        },
        {
            "name": "test_battery_name",
            "type": "str",
            "description": "The name of the cognitive psychology test battery or benchmark (e.g., 'Cognitive Decathlon', 'PsychBench', 'HumanEval-Cog', etc.)."
        },
        {
            "name": "test_name",
            "type": "str",
            "description": "The name of the specific cognitive psychology test or task (e.g., 'Digit Span', 'Stroop Task', 'Raven's Progressive Matrices')."
        },
        {
            "name": "test_description",
            "type": "str",
            "description": "A brief description of the cognitive psychology test, including what cognitive domain it assesses (e.g., working memory, reasoning, attention, language, etc.)."
        },
        {
            "name": "cognitive_domain",
            "type": "str",
            "description": "The cognitive domain assessed by the test (e.g., memory, reasoning, attention, language, executive function, etc.)."
        },
        {
            "name": "llm_performance",
            "type": "str",
            "description": "The performance of the LLM on the test, including numerical scores, accuracy, or qualitative results. Include units and details if available."
        },
        {
            "name": "human_baseline_performance",
            "type": "str",
            "description": "The performance of normal human participants on the same test, including numerical scores, accuracy, or qualitative results. Include units and details if available."
        },
        {
            "name": "performance_comparison",
            "type": "str",
            "description": "A direct comparison between LLM and human performance on the test. Summarize whether the LLM outperforms, matches, or underperforms relative to humans, and by how much."
        },
        {
            "name": "notable_limitations_or_failure_modes",
            "type": "str",
            "description": "Any notable limitations, failure modes, or qualitative differences observed in LLM performance compared to humans (e.g., systematic errors, lack of generalization, brittleness, etc.)."
        },
        {
            "name": "paper_reference",
            "type": "str",
            "description": "A citation or reference to the paper or section where this result is reported."
        }
    ],
    "extraction_query": "Extract any mentions of large language models (LLMs) being evaluated on batteries of cognitive psychology tests, including details of the tests, LLM and human performance, and any direct comparisons or qualitative findings.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
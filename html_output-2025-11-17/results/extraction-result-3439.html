<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3439 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3439</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3439</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-265128609</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.06158v1.pdf" target="_blank">Language Models can be Logical Solvers</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly emulates the reasoning processes of logical solvers and bypasses the parsing errors by learning to strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning datasets demonstrate that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3439.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3439.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGIPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGIPT (Solver-derived instruction-tuned language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of language models fine-tuned to imitate the internal, stepwise reasoning actions of a symbolic deductive solver (pyke) by training on solver-derived instruction-tuning data so the LM directly emits implied facts and answers deductive queries without explicit NL->SL parsing at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LOGIPT (fine-tuned variants on Vicuna-13B / CodeLlama-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LMs (Vicuna-13B, CodeLlama-13B-Base, CodeLlama-13B-Instruct) fine-tuned with an instruction-tuning dataset created by revealing and formatting the pyke solver's internal reasoning (bindings, unbindings, fail & backtrack, implied facts). The fine-tuning instructs the model to enumerate implied facts and then decide query truth under OWA/CWA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks of deductive logical reasoning over natural-language contexts where models must derive implied facts from facts+rules and answer True/False/Unknown (ProofWriter supports OWA; PrOntoQA subset used under CWA). Tasks require precise symbolic-style deductive inference (Prolog-style).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Instruction fine-tuning on solver-derived traces: reveal pyke's invisible internal actions (Bind/Unbind, Fail & backtrack, rule applications, enumerated implied facts), create 4-turn conversational training examples (context + solver-derived reasoning + query instructions + formatted answer), and fine-tune LMs to emulate solver reasoning (distillation of solver behavior into LM). Variants: ablations removing 'Unbind' or 'Fail & backtrack', replacing symbolic-language (SL) representations with NL, merging datasets (OWA/CWA), reformatting rule styles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On ProofWriter, LOGIPT (best: CodeLlama-13B-base fine-tuned) achieves ~89.50% accuracy; on PrOntoQA, LOGIPT (best: Vicuna-13B fine-tuned) achieves ~96.40% accuracy (reported in Table 3 for single-dataset training variants).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Solver-augmented LogicLM (using GPT-4) is reported as the prior SOTA baseline; inferred baseline accuracies (from reported absolute improvements) are ~79.66% on ProofWriter and ~83.20% on PrOntoQA (see paper text: LOGIPT outperforms LogicLM(gpt-4) by +9.84% and +13.20% respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>LOGIPT outperforms the state-of-the-art solver-augmented LM (LogicLM with GPT-4) by an absolute +9.84 percentage points on ProofWriter and +13.20 percentage points on PrOntoQA; LOGIPT also exceeds or is comparable to closed-source LLM few-shot/CoT baselines (e.g., GPT-4 CoT comparable on PrOntoQA but LOGIPT surpasses GPT-4 standard/CoT on ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires constructing solver-derived traces offline: training data was filtered to exclude instances where NL->SL parsing (by gpt-4 formulator) failed, so LOGIPT depends on the ability to reveal solver traces during dataset construction; performance degrades if symbolic representations are replaced with NL (SL representations are superior); mixing OWA and CWA training data without careful reformatting reduces accuracy (style/genre sensitivity); some format ablations can outperform default but no single format universally optimal; underlying open-source LMs before fine-tuning may fail to follow few-shot formatting (e.g., CodeLlama-13B-Base had 0.00 accuracy with standard prompting pre-finetune).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show: removing 'Unbind' or removing 'Fail & backtrack' statements can change performance but no single variant dominated; replacing symbolic-language (SL) representations in reasoning traces with plain NL substantially reduces performance; merging ProofWriter (OWA) and PrOntoQA (CWA) training data without reformatting reduces task-specific accuracy, but reformatting rule styles recovers some transfer gains; results indicate the importance of (1) retaining solver action-level statements, (2) symbolic representations, and (3) dataset/genre-specific rule formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3439.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3439.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicLM (solver-augmented language model pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A solver-augmented approach that uses an LLM to parse NL logical problems into symbolic representations and invokes an external logical solver to perform deterministic symbolic reasoning; treated as a state-of-the-art baseline for deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogicLM (solver-augmented pipeline, evaluated with GPT-4 and ChatGPT backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline: LLM-based NL->SL problem formulation (Prolog-like syntax) followed by a symbolic solver (pyke or similar) for deterministic theorem-proving. Underlying LLMs evaluated include closed-source models such as GPT-4 and ChatGPT (gpt-3.5-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same deductive reasoning benchmarks (Prolog-style derivations) where logical questions are parsed to symbolic facts/rules and solved by a theorem prover.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Use LLMs for NL-to-symbolic parsing and delegate exact deduction to an external symbolic solver (neuro-symbolic pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as prior state-of-the-art; inferred performance from paper's reported gaps: approximately ~79.66% accuracy on ProofWriter and ~83.20% accuracy on PrOntoQA (these are derived from LOGIPT's reported absolute improvements of +9.84% and +13.20%, respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared against many baselines; the solver-augmented pipeline yields faithful answers when parsing succeeds but is limited by NL->SL parsing errors (parsing success rates can be low).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>LOGIPT improves over LogicLM (gpt-4 backbone) by +9.84% on ProofWriter and +13.20% on PrOntoQA, demonstrating that emulating solver traces within the LM can be superior to the two-stage NL->SL + solver pipeline in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sensitive to NL->SL parsing errors: any parsing error leads to solver execution failure and no answer; the paper reports low parsing successful rates (e.g., Vicuna-13B parsed only ~17% executable formulations on ProofWriter in preliminary experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper analyzes parsing successful rates and shows that in-context learning limitations of LMs cause many NL->SL failures; LOGIPT's strategy to distill solver traces into an LM avoids this single-point parsing failure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3439.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3439.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source multimodal large language model from OpenAI used as a strong baseline for few-shot and chain-of-thought prompting experiments on deductive reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4, a large multimodal transformer language model (closed-source); exact parameter count not specified in the paper. Evaluated via standard prompting and chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive natural-language reasoning benchmarks requiring derivation of implied facts and answering True/False/Unknown queries.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot standard prompting and chain-of-thought (CoT) prompting; also used as backbone for solver-augmented LogicLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative reporting: GPT-4 CoT is comparable to LOGIPT on PrOntoQA (i.e., near LOGIPT's best ~96.4%), but LOGIPT surpasses GPT-4 standard and CoT prompting on ProofWriter according to the paper. Exact numeric GPT-4 numbers are not fully enumerated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Used as a high-performing baseline; in some experiments GPT-4's CoT achieves strong performance but is not universally superior (LOGIPT beats GPT-4 on ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>LOGIPT shows better or comparable performance: surpasses GPT-4 standard/CoT on ProofWriter and is comparable to GPT-4 CoT on PrOntoQA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Even strong closed-source models like GPT-4 can be outperformed on some deductive reasoning benchmarks by targeted fine-tuning (LOGIPT); any remaining detailed failure cases for GPT-4 are not exhaustively quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper contrasts GPT-4 few-shot/CoT performance with LOGIPT and finds that instruction-tuning on solver traces can enable smaller open-source models to match or exceed GPT-4 on some deductive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3439.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3439.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT / GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo) and GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source OpenAI models used as baseline few-shot/CoT systems; reported to perform close to random on these deductive reasoning benchmarks under standard few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt: Optimizing language models for dialogue.; (for text-davinci-003) Training language models to follow instructions with human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo); GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's gpt-3.5 family: chat-optimized (gpt-3.5-turbo) and text-davinci-003 instruction-tuned variants. Evaluated under standard few-shot prompting and chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>NL deductive reasoning requiring exact derivation of implied facts; tasks sensitive to precise symbolic relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard few-shot prompting and chain-of-thought prompting (CoT) with in-context demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported qualitatively as close to random answering in many few-shot settings on ProofWriter; standard prompting of ChatGPT and GPT-3.5 performed similarly to random answering per the paper (no consistent numeric accuracy provided).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Qualitatively low; paper states that closed-source LMs with standard few-shot prompting exhibit performance close to random answering on these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>LOGIPT substantially outperforms few-shot ChatGPT/GPT-3.5 baselines by large margins on the tested deductive benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>These models often fail to reliably parse NL into the strict symbolic forms required for solver invocation, and few-shot CoT does not consistently provide faithful deductive solutions for complex symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper uses identical in-context demonstrations across evaluations and finds that few-shot prompting does not remedy NL->SL parsing failures or guarantee faithful deductive reasoning; fine-tuning on solver-derived traces is more effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3439.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3439.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-13B (vicuna-13b-v1.5-16k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source chat model derived from LLaMA-2 and fine-tuned on ShareGPT conversations; used both as a baseline and as an underlying LM for LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter chat model fine-tuned from LLaMA-2 on user-shared conversation data (ShareGPT). Used as both baseline (few-shot/CoT) and as base for LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive reasoning benchmarks requiring enumeration of implied facts and query verification under OWA/CWA.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Evaluated with standard few-shot and CoT prompting; also fine-tuned into LOGIPT by training on solver-derived reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As LOGIPT (when fine-tuned) Vicuna-13B achieved ~96.40% on PrOntoQA and ~81.17% on ProofWriter in single-dataset training variants (Table 3). As a vanilla few-shot baseline, it exhibited poor deductive reasoning and low parsing-success rates (e.g., parsing success ~17% on ProofWriter in preliminary experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla Vicuna few-shot parsing success and accuracy were low and close to random before LOGIPT fine-tuning; after LOGIPT fine-tuning performance improved substantially (see above).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Fine-tuning Vicuna-13B with solver-derived traces transforms it from a poor few-shot performer (near-random) into a strong deductive reasoner (e.g., ~96.4% on PrOntoQA).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Parsing successful rate for NL->SL with Vicuna was low (~17% on ProofWriter) causing many solver-augmented pipelines to fail; genre/style sensitivity: mixing OWA/CWA training data lowers accuracy unless rule formats are re-styled.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Vicuna-based LOGIPT benefits strongly from symbolic SL representations and solver action traces; is sensitive to mixture of reasoning assumptions (OWA vs CWA) in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3439.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3439.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-13B (Base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama-13B-Base (code foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source code foundation model evaluated as a baseline and as an underlying model for LOGIPT; base model struggled with following few-shot demonstrations before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Code llama: Open foundation models for code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-13B-Base (CodeLlama-13b-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter foundation model oriented to code tasks; evaluated as vanilla baseline (standard & CoT prompting) and as base for LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive reasoning requiring precise symbolic deductions and formatted outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard few-shot prompting, chain-of-thought prompting; instruction fine-tuning to produce LOGIPT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Vanilla CodeLlama-13B-Base with standard prompting had 0.00 accuracy on evaluated tasks (it failed to follow few-shot demonstrations and emitted no answer options); after LOGIPT fine-tuning it achieved up to ~89.50% on ProofWriter and ~95.60% on PrOntoQA (Table 3 entries for CLB when trained on single datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>0.00% accuracy in standard prompting (reported) and near-random under CoT prior to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>LOGIPT fine-tuning turns an ineffective few-shot CodeLlama base (0.00%) into a strong deductive reasoner (tens of percentage points improvement; e.g., up to ~89.50% on ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Base CodeLlama often fails to follow formatting/instruction in few-shot setups, producing no answer options; benefits substantially from instruction fine-tuning on solver traces.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Instruction-tuning on solver-derived traces resolves the failure-to-format issue and yields large performance gains; different solver-derived formatting variants (remove Unbind/Fail & backtrack) can affect CLB results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3439.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3439.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-13B (Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama-13B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-finetuned variant of CodeLlama-13B that better follows human instructions and serves as a stronger base for LOGIPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Code llama: Open foundation models for code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-13B-Instruct (CodeLlama-13b-Instruct-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CodeLlama 13B further fine-tuned on instruction-following data (~+5B tokens as discussed) to better follow human prompts; used as a baseline and LOGIPT base.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter; PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same deductive natural-language-to-logic benchmarks requiring faithful symbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard few-shot, CoT prompting, and LOGIPT instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>After LOGIPT training, CodeLlama-13B-Instruct achieved ~96.20% on PrOntoQA and ~81.67% on ProofWriter for single-dataset training variants (Table 3 entries CLI).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Better than CodeLlama-Base in few-shot settings but still benefited significantly from solver-derived instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Instruction fine-tuning on solver traces improved performance substantially versus vanilla prompting; CLI often matched or exceeded other underlying models after LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>While improved over the base, still sensitive to the representation of reasoning traces and dataset mixing; best performance depends on tuning format and dataset alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>CLI responded positively to SL representations and solver-action traces; format ablations (e.g., removal of 'Unbind' or 'Fail & backtrack') could yield higher or lower accuracies depending on the underlying LM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3439.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3439.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pyke solver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pyke expert system (modified)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-the-shelf Prolog-style expert system used as the deterministic symbolic deductive solver; the authors modified its source to expose internal actions (bindings/unbindings, backtracking, implied facts) for dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applying expert system technology to code reuse with pyke.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pyke expert system (modified to reveal internal reasoning traces)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic symbolic reasoning engine (Prolog-like) that takes facts and rules in symbolic language and performs iterative rule application to derive implied facts; authors instrumented pyke to output internal actions (used to create solver-derived instruction-tuning data).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Used to generate traces for ProofWriter and PrOntoQA training instances</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Symbolic theorem proving / forward-chaining derivation of implied facts from facts+rules represented in Prolog-like syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Used as a teacher: its invisible internal reasoning was revealed and formatted into conversational Turn-2 traces (bindings, unbindings, fail & backtrack, new implied facts) which were then used to instruction-tune LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as accuracy numbers (pyke is deterministic); used as oracle for correctness when NL->SL parsing succeeded. The paper reports that when parsing into executable SL succeeds, the symbolic solver guarantees faithful answers.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>N/A (serves as gold-standard teacher).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Pyke's usefulness in the solver-augmented pipeline is limited by upstream NL->SL parsing success; if parsing fails, pyke cannot be invoked (leading to 'no answer'). Also its internal reasoning is normally invisible, hence the paper had to modify pyke to extract traces.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The authors instrumented pyke to produce detailed trace outputs; analyses show that retaining solver action-level detail (bind/unbind/fail/backtrack) is helpful for LM learning, and that symbolic (SL) notation is superior to NL re-translations when used in training traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models. <em>(Rating: 1)</em></li>
                <li>Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. <em>(Rating: 1)</em></li>
                <li>Code llama: Open foundation models for code. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3439",
    "paper_id": "paper-265128609",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "LOGIPT",
            "name_full": "LOGIPT (Solver-derived instruction-tuned language model)",
            "brief_description": "A family of language models fine-tuned to imitate the internal, stepwise reasoning actions of a symbolic deductive solver (pyke) by training on solver-derived instruction-tuning data so the LM directly emits implied facts and answers deductive queries without explicit NL-&gt;SL parsing at inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LOGIPT (fine-tuned variants on Vicuna-13B / CodeLlama-13B)",
            "model_description": "Open-source transformer LMs (Vicuna-13B, CodeLlama-13B-Base, CodeLlama-13B-Instruct) fine-tuned with an instruction-tuning dataset created by revealing and formatting the pyke solver's internal reasoning (bindings, unbindings, fail & backtrack, implied facts). The fine-tuning instructs the model to enumerate implied facts and then decide query truth under OWA/CWA.",
            "model_size": "13B",
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Benchmarks of deductive logical reasoning over natural-language contexts where models must derive implied facts from facts+rules and answer True/False/Unknown (ProofWriter supports OWA; PrOntoQA subset used under CWA). Tasks require precise symbolic-style deductive inference (Prolog-style).",
            "method_or_intervention": "Instruction fine-tuning on solver-derived traces: reveal pyke's invisible internal actions (Bind/Unbind, Fail & backtrack, rule applications, enumerated implied facts), create 4-turn conversational training examples (context + solver-derived reasoning + query instructions + formatted answer), and fine-tune LMs to emulate solver reasoning (distillation of solver behavior into LM). Variants: ablations removing 'Unbind' or 'Fail & backtrack', replacing symbolic-language (SL) representations with NL, merging datasets (OWA/CWA), reformatting rule styles.",
            "performance": "On ProofWriter, LOGIPT (best: CodeLlama-13B-base fine-tuned) achieves ~89.50% accuracy; on PrOntoQA, LOGIPT (best: Vicuna-13B fine-tuned) achieves ~96.40% accuracy (reported in Table 3 for single-dataset training variants).",
            "baseline_performance": "Solver-augmented LogicLM (using GPT-4) is reported as the prior SOTA baseline; inferred baseline accuracies (from reported absolute improvements) are ~79.66% on ProofWriter and ~83.20% on PrOntoQA (see paper text: LOGIPT outperforms LogicLM(gpt-4) by +9.84% and +13.20% respectively).",
            "improvement_over_baseline": "LOGIPT outperforms the state-of-the-art solver-augmented LM (LogicLM with GPT-4) by an absolute +9.84 percentage points on ProofWriter and +13.20 percentage points on PrOntoQA; LOGIPT also exceeds or is comparable to closed-source LLM few-shot/CoT baselines (e.g., GPT-4 CoT comparable on PrOntoQA but LOGIPT surpasses GPT-4 standard/CoT on ProofWriter).",
            "limitations_or_failures": "Requires constructing solver-derived traces offline: training data was filtered to exclude instances where NL-&gt;SL parsing (by gpt-4 formulator) failed, so LOGIPT depends on the ability to reveal solver traces during dataset construction; performance degrades if symbolic representations are replaced with NL (SL representations are superior); mixing OWA and CWA training data without careful reformatting reduces accuracy (style/genre sensitivity); some format ablations can outperform default but no single format universally optimal; underlying open-source LMs before fine-tuning may fail to follow few-shot formatting (e.g., CodeLlama-13B-Base had 0.00 accuracy with standard prompting pre-finetune).",
            "ablation_or_analysis": "Ablations show: removing 'Unbind' or removing 'Fail & backtrack' statements can change performance but no single variant dominated; replacing symbolic-language (SL) representations in reasoning traces with plain NL substantially reduces performance; merging ProofWriter (OWA) and PrOntoQA (CWA) training data without reformatting reduces task-specific accuracy, but reformatting rule styles recovers some transfer gains; results indicate the importance of (1) retaining solver action-level statements, (2) symbolic representations, and (3) dataset/genre-specific rule formats.",
            "uuid": "e3439.0",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LogicLM",
            "name_full": "LogicLM (solver-augmented language model pipeline)",
            "brief_description": "A solver-augmented approach that uses an LLM to parse NL logical problems into symbolic representations and invokes an external logical solver to perform deterministic symbolic reasoning; treated as a state-of-the-art baseline for deductive reasoning.",
            "citation_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "mention_or_use": "use",
            "model_name": "LogicLM (solver-augmented pipeline, evaluated with GPT-4 and ChatGPT backbones)",
            "model_description": "Pipeline: LLM-based NL-&gt;SL problem formulation (Prolog-like syntax) followed by a symbolic solver (pyke or similar) for deterministic theorem-proving. Underlying LLMs evaluated include closed-source models such as GPT-4 and ChatGPT (gpt-3.5-turbo).",
            "model_size": null,
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Same deductive reasoning benchmarks (Prolog-style derivations) where logical questions are parsed to symbolic facts/rules and solved by a theorem prover.",
            "method_or_intervention": "Use LLMs for NL-to-symbolic parsing and delegate exact deduction to an external symbolic solver (neuro-symbolic pipeline).",
            "performance": "Reported as prior state-of-the-art; inferred performance from paper's reported gaps: approximately ~79.66% accuracy on ProofWriter and ~83.20% accuracy on PrOntoQA (these are derived from LOGIPT's reported absolute improvements of +9.84% and +13.20%, respectively).",
            "baseline_performance": "Compared against many baselines; the solver-augmented pipeline yields faithful answers when parsing succeeds but is limited by NL-&gt;SL parsing errors (parsing success rates can be low).",
            "improvement_over_baseline": "LOGIPT improves over LogicLM (gpt-4 backbone) by +9.84% on ProofWriter and +13.20% on PrOntoQA, demonstrating that emulating solver traces within the LM can be superior to the two-stage NL-&gt;SL + solver pipeline in practice.",
            "limitations_or_failures": "Sensitive to NL-&gt;SL parsing errors: any parsing error leads to solver execution failure and no answer; the paper reports low parsing successful rates (e.g., Vicuna-13B parsed only ~17% executable formulations on ProofWriter in preliminary experiments).",
            "ablation_or_analysis": "The paper analyzes parsing successful rates and shows that in-context learning limitations of LMs cause many NL-&gt;SL failures; LOGIPT's strategy to distill solver traces into an LM avoids this single-point parsing failure.",
            "uuid": "e3439.1",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "Closed-source multimodal large language model from OpenAI used as a strong baseline for few-shot and chain-of-thought prompting experiments on deductive reasoning tasks.",
            "citation_title": "Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4)",
            "model_description": "OpenAI's GPT-4, a large multimodal transformer language model (closed-source); exact parameter count not specified in the paper. Evaluated via standard prompting and chain-of-thought prompting.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Deductive natural-language reasoning benchmarks requiring derivation of implied facts and answering True/False/Unknown queries.",
            "method_or_intervention": "Few-shot standard prompting and chain-of-thought (CoT) prompting; also used as backbone for solver-augmented LogicLM.",
            "performance": "Qualitative reporting: GPT-4 CoT is comparable to LOGIPT on PrOntoQA (i.e., near LOGIPT's best ~96.4%), but LOGIPT surpasses GPT-4 standard and CoT prompting on ProofWriter according to the paper. Exact numeric GPT-4 numbers are not fully enumerated in the text.",
            "baseline_performance": "Used as a high-performing baseline; in some experiments GPT-4's CoT achieves strong performance but is not universally superior (LOGIPT beats GPT-4 on ProofWriter).",
            "improvement_over_baseline": "LOGIPT shows better or comparable performance: surpasses GPT-4 standard/CoT on ProofWriter and is comparable to GPT-4 CoT on PrOntoQA.",
            "limitations_or_failures": "Even strong closed-source models like GPT-4 can be outperformed on some deductive reasoning benchmarks by targeted fine-tuning (LOGIPT); any remaining detailed failure cases for GPT-4 are not exhaustively quantified in the paper.",
            "ablation_or_analysis": "Paper contrasts GPT-4 few-shot/CoT performance with LOGIPT and finds that instruction-tuning on solver traces can enable smaller open-source models to match or exceed GPT-4 on some deductive tasks.",
            "uuid": "e3439.2",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChatGPT / GPT-3.5",
            "name_full": "ChatGPT (gpt-3.5-turbo) and GPT-3.5 (text-davinci-003)",
            "brief_description": "Closed-source OpenAI models used as baseline few-shot/CoT systems; reported to perform close to random on these deductive reasoning benchmarks under standard few-shot prompting.",
            "citation_title": "Chatgpt: Optimizing language models for dialogue.; (for text-davinci-003) Training language models to follow instructions with human feedback.",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo); GPT-3.5 (text-davinci-003)",
            "model_description": "OpenAI's gpt-3.5 family: chat-optimized (gpt-3.5-turbo) and text-davinci-003 instruction-tuned variants. Evaluated under standard few-shot prompting and chain-of-thought prompting.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "NL deductive reasoning requiring exact derivation of implied facts; tasks sensitive to precise symbolic relationships.",
            "method_or_intervention": "Standard few-shot prompting and chain-of-thought prompting (CoT) with in-context demonstrations.",
            "performance": "Reported qualitatively as close to random answering in many few-shot settings on ProofWriter; standard prompting of ChatGPT and GPT-3.5 performed similarly to random answering per the paper (no consistent numeric accuracy provided).",
            "baseline_performance": "Qualitatively low; paper states that closed-source LMs with standard few-shot prompting exhibit performance close to random answering on these benchmarks.",
            "improvement_over_baseline": "LOGIPT substantially outperforms few-shot ChatGPT/GPT-3.5 baselines by large margins on the tested deductive benchmarks.",
            "limitations_or_failures": "These models often fail to reliably parse NL into the strict symbolic forms required for solver invocation, and few-shot CoT does not consistently provide faithful deductive solutions for complex symbolic reasoning.",
            "ablation_or_analysis": "Paper uses identical in-context demonstrations across evaluations and finds that few-shot prompting does not remedy NL-&gt;SL parsing failures or guarantee faithful deductive reasoning; fine-tuning on solver-derived traces is more effective.",
            "uuid": "e3439.3",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Vicuna-13B",
            "name_full": "Vicuna-13B (vicuna-13b-v1.5-16k)",
            "brief_description": "An open-source chat model derived from LLaMA-2 and fine-tuned on ShareGPT conversations; used both as a baseline and as an underlying LM for LOGIPT fine-tuning.",
            "citation_title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "mention_or_use": "use",
            "model_name": "Vicuna-13B",
            "model_description": "13B-parameter chat model fine-tuned from LLaMA-2 on user-shared conversation data (ShareGPT). Used as both baseline (few-shot/CoT) and as base for LOGIPT fine-tuning.",
            "model_size": "13B",
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Deductive reasoning benchmarks requiring enumeration of implied facts and query verification under OWA/CWA.",
            "method_or_intervention": "Evaluated with standard few-shot and CoT prompting; also fine-tuned into LOGIPT by training on solver-derived reasoning traces.",
            "performance": "As LOGIPT (when fine-tuned) Vicuna-13B achieved ~96.40% on PrOntoQA and ~81.17% on ProofWriter in single-dataset training variants (Table 3). As a vanilla few-shot baseline, it exhibited poor deductive reasoning and low parsing-success rates (e.g., parsing success ~17% on ProofWriter in preliminary experiments).",
            "baseline_performance": "Vanilla Vicuna few-shot parsing success and accuracy were low and close to random before LOGIPT fine-tuning; after LOGIPT fine-tuning performance improved substantially (see above).",
            "improvement_over_baseline": "Fine-tuning Vicuna-13B with solver-derived traces transforms it from a poor few-shot performer (near-random) into a strong deductive reasoner (e.g., ~96.4% on PrOntoQA).",
            "limitations_or_failures": "Parsing successful rate for NL-&gt;SL with Vicuna was low (~17% on ProofWriter) causing many solver-augmented pipelines to fail; genre/style sensitivity: mixing OWA/CWA training data lowers accuracy unless rule formats are re-styled.",
            "ablation_or_analysis": "Vicuna-based LOGIPT benefits strongly from symbolic SL representations and solver action traces; is sensitive to mixture of reasoning assumptions (OWA vs CWA) in training data.",
            "uuid": "e3439.4",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CodeLlama-13B (Base)",
            "name_full": "CodeLlama-13B-Base (code foundation model)",
            "brief_description": "An open-source code foundation model evaluated as a baseline and as an underlying model for LOGIPT; base model struggled with following few-shot demonstrations before fine-tuning.",
            "citation_title": "Code llama: Open foundation models for code.",
            "mention_or_use": "use",
            "model_name": "CodeLlama-13B-Base (CodeLlama-13b-hf)",
            "model_description": "13B-parameter foundation model oriented to code tasks; evaluated as vanilla baseline (standard & CoT prompting) and as base for LOGIPT fine-tuning.",
            "model_size": "13B",
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Deductive reasoning requiring precise symbolic deductions and formatted outputs.",
            "method_or_intervention": "Standard few-shot prompting, chain-of-thought prompting; instruction fine-tuning to produce LOGIPT.",
            "performance": "Vanilla CodeLlama-13B-Base with standard prompting had 0.00 accuracy on evaluated tasks (it failed to follow few-shot demonstrations and emitted no answer options); after LOGIPT fine-tuning it achieved up to ~89.50% on ProofWriter and ~95.60% on PrOntoQA (Table 3 entries for CLB when trained on single datasets).",
            "baseline_performance": "0.00% accuracy in standard prompting (reported) and near-random under CoT prior to fine-tuning.",
            "improvement_over_baseline": "LOGIPT fine-tuning turns an ineffective few-shot CodeLlama base (0.00%) into a strong deductive reasoner (tens of percentage points improvement; e.g., up to ~89.50% on ProofWriter).",
            "limitations_or_failures": "Base CodeLlama often fails to follow formatting/instruction in few-shot setups, producing no answer options; benefits substantially from instruction fine-tuning on solver traces.",
            "ablation_or_analysis": "Instruction-tuning on solver-derived traces resolves the failure-to-format issue and yields large performance gains; different solver-derived formatting variants (remove Unbind/Fail & backtrack) can affect CLB results.",
            "uuid": "e3439.5",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CodeLlama-13B (Instruct)",
            "name_full": "CodeLlama-13B-Instruct",
            "brief_description": "Instruction-finetuned variant of CodeLlama-13B that better follows human instructions and serves as a stronger base for LOGIPT.",
            "citation_title": "Code llama: Open foundation models for code.",
            "mention_or_use": "use",
            "model_name": "CodeLlama-13B-Instruct (CodeLlama-13b-Instruct-hf)",
            "model_description": "CodeLlama 13B further fine-tuned on instruction-following data (~+5B tokens as discussed) to better follow human prompts; used as a baseline and LOGIPT base.",
            "model_size": "13B",
            "reasoning_task_name": "ProofWriter; PrOntoQA",
            "reasoning_task_description": "Same deductive natural-language-to-logic benchmarks requiring faithful symbolic inference.",
            "method_or_intervention": "Standard few-shot, CoT prompting, and LOGIPT instruction fine-tuning.",
            "performance": "After LOGIPT training, CodeLlama-13B-Instruct achieved ~96.20% on PrOntoQA and ~81.67% on ProofWriter for single-dataset training variants (Table 3 entries CLI).",
            "baseline_performance": "Better than CodeLlama-Base in few-shot settings but still benefited significantly from solver-derived instruction fine-tuning.",
            "improvement_over_baseline": "Instruction fine-tuning on solver traces improved performance substantially versus vanilla prompting; CLI often matched or exceeded other underlying models after LOGIPT fine-tuning.",
            "limitations_or_failures": "While improved over the base, still sensitive to the representation of reasoning traces and dataset mixing; best performance depends on tuning format and dataset alignment.",
            "ablation_or_analysis": "CLI responded positively to SL representations and solver-action traces; format ablations (e.g., removal of 'Unbind' or 'Fail & backtrack') could yield higher or lower accuracies depending on the underlying LM.",
            "uuid": "e3439.6",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "pyke solver",
            "name_full": "pyke expert system (modified)",
            "brief_description": "An off-the-shelf Prolog-style expert system used as the deterministic symbolic deductive solver; the authors modified its source to expose internal actions (bindings/unbindings, backtracking, implied facts) for dataset construction.",
            "citation_title": "Applying expert system technology to code reuse with pyke.",
            "mention_or_use": "use",
            "model_name": "pyke expert system (modified to reveal internal reasoning traces)",
            "model_description": "Deterministic symbolic reasoning engine (Prolog-like) that takes facts and rules in symbolic language and performs iterative rule application to derive implied facts; authors instrumented pyke to output internal actions (used to create solver-derived instruction-tuning data).",
            "model_size": null,
            "reasoning_task_name": "Used to generate traces for ProofWriter and PrOntoQA training instances",
            "reasoning_task_description": "Symbolic theorem proving / forward-chaining derivation of implied facts from facts+rules represented in Prolog-like syntax.",
            "method_or_intervention": "Used as a teacher: its invisible internal reasoning was revealed and formatted into conversational Turn-2 traces (bindings, unbindings, fail & backtrack, new implied facts) which were then used to instruction-tune LMs.",
            "performance": "Not reported as accuracy numbers (pyke is deterministic); used as oracle for correctness when NL-&gt;SL parsing succeeded. The paper reports that when parsing into executable SL succeeds, the symbolic solver guarantees faithful answers.",
            "baseline_performance": null,
            "improvement_over_baseline": "N/A (serves as gold-standard teacher).",
            "limitations_or_failures": "Pyke's usefulness in the solver-augmented pipeline is limited by upstream NL-&gt;SL parsing success; if parsing fails, pyke cannot be invoked (leading to 'no answer'). Also its internal reasoning is normally invisible, hence the paper had to modify pyke to extract traces.",
            "ablation_or_analysis": "The authors instrumented pyke to produce detailed trace outputs; analyses show that retaining solver action-level detail (bind/unbind/fail/backtrack) is helpful for LM learning, and that symbolic (SL) notation is superior to NL re-translations when used in training traces.",
            "uuid": "e3439.7",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "rating": 1,
            "sanitized_title": "vicuna_an_opensource_chatbot_impressing_gpt4_with_90_chatgpt_quality"
        },
        {
            "paper_title": "Code llama: Open foundation models for code.",
            "rating": 1,
            "sanitized_title": "code_llama_open_foundation_models_for_code"
        }
    ],
    "cost": 0.019305749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models can be Logical Solvers
10 Nov 2023</p>
<p>Jiazhan Feng fengjiazhan@pku.edu.cn 
Peking University
Beijing</p>
<p>Ruochen Xu 
Microsoft Azure AI
Redmond</p>
<p>Junheng Hao junhenghao@microsoft.com 
Microsoft Azure AI
Redmond</p>
<p>Hiteshi Sharma 
Microsoft Azure AI
Redmond</p>
<p>Yelong Shen 
Microsoft Azure AI
Redmond</p>
<p>Dongyan Zhao zhaody@pku.edu.cn 
Peking University
Beijing</p>
<p>Weizhu Chen wzchen@microsoft.com 
Microsoft Azure AI
Redmond</p>
<p>Language Models can be Logical Solvers
10 Nov 202303ED0BE6B08ACDCDB5429424B2795E1FarXiv:2311.06158v1[cs.CL]Green('Charlie'True) Green('Charlie'False)
Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decisionmaking.Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge.The stateof-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers.Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions.In this paper, we introduce LOGIPT, a novel language model that directly emulates the reasoning processes of logical solvers and bypasses the parsing errors by learning to strict adherence to solver syntax and grammar.LOGIPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers.Experimental results on two public deductive reasoning datasets demonstrate that LOGIPT outperforms state-of-the-art solver-augmented LMs and fewshot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Introduction</p>
<p>Logical reasoning is a foundational element of human intelligence, holding a pivotal role in tasks like problem-solving, decision-making, and critical thinking (Huang and Chang, 2023).Recently, substantial advancements have been achieved in the field of NLP through the development of large language models (LLMs) (OpenAI, 2022(OpenAI, , 2023;;Google, 2023;Touvron et al., 2023a,b).It has been noted that language models (LMs) could potentially display reasoning capabilities when they reach a certain scale threshold (e.g., training compute, model parameters, etc.) (Kaplan et al., 2020;Wei et al., 2022a;Hoffmann et al., 2022).To this end, LLMs can answer logical questions with explicit reasoning steps when prompted with a simple snippet: "Let's think step by step."(Kojima et al., 2022) or step-wise explanations of reasoning (i.e., "chain of thoughts") (Wei et al., 2022b).</p>
<p>While LLMs have made significant progress, complex logical reasoning remains challenging (Valmeekam et al., 2022;Liu et al., 2023b).Some prior work (Tafjord et al., 2022;Ling et al., 2023) aimed to enable LMs to perform logical reasoning via specialized module fine-tuning, where reasoning is in natural language (NL).However, the ambiguity and complexity of NL can lead to undesired issues like hallucinations and unfaithful reasoning (Saparov and He, 2023;Gao et al., 2023).To this end, recent work has begun to augment LLMs with access to external Solvers (Chen et al., 2022;Ye et al., 2023;Pan et al., 2023).In this paper, we focus on the logical solvers, which are theorem provers that can be any automated reasoning tool for checking the truth value of logical formulas in symbolic language (SL).Invoking logical solvers can guarantee the accuracy of logical reasoning and relieve the burden of LLMs to execute intricate and precise deductive reasoning.</p>
<p>The data flow of the aforementioned solveraugmented LMs is depicted in Figure 1(a).At the outset, the information of logical questions is stored in NL.It is subsequently fed into a LM for parsing into a symbolic representation suitable for solver-input format.Finally, the SL information is dispatched to a symbolic solver, which yields the truth value of the logical question.However, during this process, any NL-to-SL parsing errors will inevitably result in the failure of the reasoning process and no answer to the question.In our pre- liminary experiments, we observed that the parsing successful rate (i.e., percentage of executable logical formulations) of Vicuna-13B (Chiang et al., 2023) on ProofWriter (Tafjord et al., 2021) is only 17%, significantly below the expected performance.</p>
<p>In addressing parsing failures, current methods either directly use LLMs to reason in NL solely or rely on the solver's erroneous message to regenerate parsing results, but these approaches don't fundamentally resolve the problem.</p>
<p>In this paper, we introduce LOGIPT, a novel LM designed to mimic the reasoning process of logical solvers, enabling it to solve deductive reasoning tasks.We first construct an instruction-tuning dataset containing NL logical questions and their corresponding solver's symbolic reasoning process.After filtering out cases having invalid syntax, we fine-tune open-source LMs like Vicuna or CodeLlama (Roziere et al., 2023) with this data to create LOGIPT.Then, LOGIPT can generate all implied facts given premises and rules, allowing us to determine the truth value of a logical query by matching it with implied facts or outputting 'unknown' if it cannot be determined.The data flow of our pipeline is presented in Figure 1(b,c).We can bypass the syntax or grammatical errors derived from NL-to-SL parsing by directly outputting the answers with a fine-tuned LOGIPT.</p>
<p>Our approach is akin to the process of distillation, whereby we distill knowledge from a symbolic model (i.e., solver) into a neural network (i.e., LM).However, the reasoning process of solvers is invisible to users and we can only obtain the answers without intermediate reasoning steps.We design a pipeline to reveal and formalize solvers' invisible reasoning processes, creating instructiontuning datasets with visible and interpretable sym-bolic reasoning steps (see Figure 3).</p>
<p>Our main contributions are three-fold:</p>
<p> To the best of our knowledge, we are the first to propose empowering LLMs to directly learn the reasoning process of logical solvers, thereby acquiring similar reasoning capability for addressing deductive reasoning tasks. Our proposed LOGIPT, can directly act as a deductive solver and output all Facts implied from NL logical questions while bypassing the syntax or grammatical errors derived from NL-to-SL parsing of solver-augmented LMs. Evaluation results on two public deductive reasoning datasets show that LOGIPT can outperform state-of-the-art solver-augmented LMs, and few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Preliminary</p>
<p>Deductive Reasoning</p>
<p>Deductive reasoning is an essential type of logical reasoning problem.It typically commences with known facts and rules from logical context, then proceeds through a series of inference steps until the query can be proved or disproved (Poole and Mackworth, 2010).In this paper, we consider the Prolog logic programming language (Clocksin and Mellish, 2003;Krner et al., 2022), which stands as the most prominent symbolic language for describing deductive reasoning problems.We showcased a deductive reasoning question along with its corresponding Prolog syntax representation in Figure 2.</p>
<p>For each question, we denote the NL description as Context.The Context can further be parsed into Facts, Rules, and Query1 .Specifically, a Fact F = P (a 1 ,    , a t ) is a symbolic statement with a predicate P and t arguments {a 1 ,    , a t } where a i can be a variable, entity, number or bool.For example, Green('Charlie', True) means "Charlie is green"; Rules are presented in the form of clauses
F 1      F m  F m+1      F n , where F i is a Fact. The Rule means "if each F i  {F 1 ,    , F m }
is true, then we can imply that all Facts in {F m+1 ,    , F n } are also true."For example, Furry($x, True)  Quiet($x, True) indicates if variable $x is furry, then $x is quiet; a Query Q is also in the format of a Fact that needs to be proved based on Facts and Rules.</p>
<p>Solver-augmented LMs</p>
<p>Solver-augmented LMs have demonstrated remarkable performance in deductive reasoning tasks.As shown in Figure 1(a), these model can be generally divided into two stages: Problem Formulation (from LMs to Symbolic Solvers) and Symbolic Reasoning (from Symbolic Solvers to Answers).</p>
<p>In Problem Formulation stage, an LM is used to parse an NL logical question into symbolic representation (Figure 2).The process can be accomplished by providing LM with detailed instructions about the grammar of Prolog, alongside a few demonstrations as in-context examples (Ouyang et al., 2022).The LM is expected to identify the symbolic Facts, Rules, and Query from the NL logical question following the instructions; In Symbolic Reasoning stage, a solver takes in the symbolic representation obtained in the previous stage and conducts symbolic reasoning.The reasoning process of external off-the-shelf solver, e.g., pyke expert system (Frederiksen, 2008), is deterministic and invisible.Then, the truth value of the parsed Query, which is the only output of the solver, can be treated as the answer to the given question.</p>
<p>Analysis on the Parsing Successful Rate</p>
<p>Through the aforementioned two phases, once the solver-augmented LMs correctly formulate the problem, the answers obtained through symbolic reasoning will be faithful, attributed to the deterministic nature of the solver.However, this heavily relies on the in-context learning capabilities of LMs.Therefore, we first calculate the parsing successful rate of three selected open-source LLMs on two deductive reasoning datasets in Table 1.Firstly, we observe that CodeLlama-13B-Base (CodeLlama-13b-hf) is unable to effectively conduct NL-to-SL parsing due to the limited in-context learning capabilities in natural languages.Then we can find that replacing the Base model with the Instruct version (CodeLlama-13b-Instruct-hf) can alleviate this issue, which may be attributed to the fact that the Instruct version is further finetuned with an additional approx.5B tokens to better follow human instructions.Overall, open-source LLMs still exhibit parsing performance significantly lower than expected in some cases.</p>
<p>LoGiPT</p>
<p>In this paper, we aim to mitigate the parsing issue and present a novel LM, LOGIPT instructed to imitate the logical reasoning process of Solvers for deductive reasoning tasks.To achieve this, we first reveal the solver reasoning process when solving logical problems ( 3.1).Then, we construct a solver-derived instruction-tuning dataset, comprising NL logical questions and corresponding SL reasoning steps ( 3.2).Finally, we fine-tune open-source LLMs using this dataset to develop LOGIPT ( 3.3).</p>
<p>Revealing the Solver Reasoning Process</p>
<p>Before operating on the solvers, we first adopt gpt-4 as the problem formulator for NL-to-SL parsing with instructions about the grammar and</p>
<p>Turn-1 (from human):</p>
<p>Task Description: Given a statement containing logical facts and rules in natural languages as context, the task is to:</p>
<ol>
<li>define all the predicates in logic forms from the context 2. parse the statement into known facts and rules based on the defined predicates 3. begin reasoning to obtain all implied facts Context: Charlie is green.Charlie is white.Dave is blue.Dave is furry.Dave is white.Fiona is blue.Fiona is green.Fiona is nice.Fiona is rough.Fiona is white.All furry people are quiet.All green, white people are nice.If someone is blue and quiet then they are green.</li>
</ol>
<p>Reasoning:</p>
<p>Turn-2 (from LM):</p>
<p>We first define following predicates and corresponding natural language explanations: Green($x, bool) ::: Is x green?(... more predicates here ...)</p>
<p>We have following known facts from the context: Green('Charlie', True) (... more facts here ...)</p>
<p>We have following known rules from the context: rule1:
Furry($x, True)  Quiet($x, True) rule2: Green($x, True)  White($x, True)  Nice($x, True) rule3: Blue($x, True)  Quiet($x, True)  Green($x,</p>
<p>Turn-3 (from human):</p>
<p>Question: Based on the above information, is the following comment true, false, or unknown?Dave is not green.
Options: A) True B) False C) Unknown
Now you should:</p>
<ol>
<li>reason first carefully with the above known facts and implied facts 2. then output your answer strictly in the format of "So, the correct option is: X".The X should be replaced by "A) True", "B) False" or "C) Unknown"</li>
</ol>
<p>Turn-4 (from LM):</p>
<p>The comment can be parsed into: Green('Dave', False) According to the implied fact: Green('Dave', True)</p>
<p>The comment contradicts the fact.So, the correct option is: B) False</p>
<p>Figure 3: A comprehensive 4-turn training example of our instruction-tuning data.We highlight the initial occurrences of each functionality described in 3.1 using the corresponding colors.We omit some predicates and Facts in Turn-2 due to limited space.Hint: this figure is color-sensitive.</p>
<p>few-shot demonstrations 2 , and obtain the SL representations of all training logical questions of the given logical datasets.Then, consistent with solveraugmented methods, we adopt pyke expert system as the symbolic solver in this work that can make inferences using the Prolog symbolic language.Given a logical question, pyke first sets up a knowledge base and injects all known Facts and Rules (Figure 2) from solver's inputs.Then, it iteratively applies Rules on already known or implied Facts, aiming at obtaining more implied Facts until the Query is proved or disproved.The reasoning process executed by pyke solver is invisible to users and solver-augmented LMs use the solver as a black-box.We hypothesis the 'chain-of-thought' reasoning process of the solver is valuable and LLMs are able to learn from it.To this end, we first modify the source code of the pyke 3 to achieve the following functionalities: With the aforementioned instructions, we can obtain the revealed solver's reasoning process for the construction of training data.We also high-lighted the initial occurrences of each functionality using the corresponding colors in Figure 3 (Turn-2), where a case will be described in detail in the next section.</p>
<p>Constructing the Instruction-tuning Data</p>
<p>However, as previously mentioned, we cannot guarantee that LMs can definitely complete the NL-to-SL parsing on arbitrary questions.To this end, we first filter out all unsuccessfully parsed training cases that cannot be executed by pyke.Then we reorganize and refine the filtered training data to enhance the interpretability of the solver-derived reasoning steps.For each case, we divide the reasoning process into four conversational turns (Turn-1&amp;3 for human and Turn-2&amp;4 for LM), which will be described elaborately in the following paragraphs.We also provide a comprehensive training example of our instruction-tuning data4 in Figure 3, and the full version is also included in Appendix C.</p>
<p>Turn-1: Instructions &amp; NL logical Context.</p>
<p>For each NL logical question within the training set, we begin by stripping away the specific Query statement while retaining the question Context and subsequently integrating it with elaborately crafted instructions.Taking the case in Figure 3 as an example, we temporarily exclude the Query 'Dave is not green' from the 'Context' field.Here, we only consider Query-agnostic question description to ensure that LMs initially focus on the logical background itself.This is because sometimes the ground-truth answer is 'Unknown' (e.g., cases in ProofWriter).The truth value of the Query cannot be inferred from the Context, and therefore we need to deduce all implied Facts first.</p>
<p>Turn-2: Query-agnostic Solver-derived Reasoning.As we have acquired the solver's symbolic reasoning data in the revealing phase, our goal in Turn-2 is to further refine and enhance the reasoning process to achieve a more readable form of the solver's reasoning process.Specifically, for each logical question, we first define all necessary predicates and append the corresponding natural language explanations.Then we list the known Facts and Rules extracted from the Context with interleaved NL instructions.</p>
<p>After that, we represent the application of each Rule by utilizing separate blocks, line by line.We strive to preserve as many solver actions as possible, such as 'Binding' and 'Unbinding', as well as the acquisition of new implied Facts, and so forth.Noting that this information has already been obtained during the revealing phase, we focus on the refinement of the solver-derived reasoning process.Finally, we enumerate all newly implied Facts to enable the model to perform an interim review.</p>
<p>Turn-3: Query &amp; Answering Instructions.In Turn-3, we present instructions for answering a given Query.Following prior works (Ceri et al., 1989;Tafjord et al., 2021), a Query can be considered true within a certain logical context if it is explicitly mentioned or if it can be implied through several Rule applications.To handle negation, we consider two distinct assumptions: 1) the open-world assumption (OWA) that treats any fact that cannot be provable as special truth value 'unknown'; 2) the closed-world assumption (CWA) where any fact not provable is assumed 'false'.Following both assumptions, we adjust the answering instructions, particularly the 'Options' part.</p>
<p>Turn-4: Query-based Reasoning &amp; Formatted Answer.In the final Turn-4, we compare the parsed Query with all the known Facts and implied Facts, expecting the model to perform basic language inference and generate answer options in the desired format.</p>
<p>Fine-tuning Open-source LLMs</p>
<p>After obtaining the refined deductive reasoning instruction-tuning dataset, we can perform finetuning on open-source LLMs with the expectation that the trained model (i.e., LOGIPT) can possess reasoning abilities similar to those of solvers.Consequently, for any given Query, we can bypass the syntax or grammatical errors derived from NL-to-SL parsing by directly generating the answer with a fine-tuned LOGIPT.</p>
<p>Experiments</p>
<p>We construct our solver-derived instruction-tuning data on two public deductive reasoning datasets and evaluate LOGIPT on corresponding test sets.</p>
<p>Datasets</p>
<p>ProofWriter (Tafjord et al., 2021)</p>
<p>Baselines</p>
<p>We consider comparing LOGIPT with following groups of baselines:</p>
<p>Closed-source LMs: We include the Chat-GPT (gpt-3.5-turbo)(OpenAI, 2022), GPT-3.5 (text-davinci-003) (Ouyang et al., 2022) and GPT-4 (gpt-4) (OpenAI, 2023) as closed-source LMs for evaluation following Pan et al. (2023).</p>
<p>Open-source LMs:</p>
<p>We also evaluate opensource LMs for research community.Specifically, we choose Vicuna-13B (vicuna-13b-v1.5-16k)(Chiang et al., 2023), a chatbot trained by finetuning LLaMA-2 (Touvron et al., 2023b) on usershared conversations collected from ShareGPT5 , and CodeLlama-13B (Roziere et al., 2023), foundation models for code tasks.We select the base version (CodeLlama-13b-hf), and instruction finetuned version (CodeLlama-13b-Instruct-hf).</p>
<p>Solver-argumented LMs: Finally, we compare our model against the solver-argumented LMs.We focus on the representative LogicLM (Pan et al., 2023) with underlying LLMs ChatGPT (gpt-3.5-turbo),and , which serve as the state-of-theart deductive reasoning methods.</p>
<p>Apart from the LMs, we also analyze two types of prompting methods: i) Standard prompting that uses in-context learning with few-shot demonstrations to directly answer the given question; ii) Chain-of-Thought (CoT) that utilizes step-by-step problem-solving process to generate explanations where few-shot demonstrations are also provided, and then outputs the final answer.For a fair comparison, we use the same in-context examples, shown in Appendix A and B, for NL-to-SL parsing when evaluating all models on the same dataset, consistent with Pan et al. (2023).To enhance the clarification, we also provide a specific baseline 'Random Answering' that randomly outputs answer options.</p>
<p>Implementation Details</p>
<p>During the fine-tuning phase, we use a batch size of 32 per GPU and a learning rate of 1e-5 for all opensource LMs.We train our model on 8 Nvidia A100-80G GPUs with DeepSpeed ZeRO-3 (Rasley et al., 2020) for 12 hours on 2 epochs.For reproducibility, we use greedy decoding and set the temperature to 0 and the maximum context length to 8192.As for baselines, we strictly follow the setting of Pan et al. (2023).Given that all instances are presented in the form of multiple-choice questions, we assess the model's performance by the accuracy of selecting the correct answer option.</p>
<p>Main Results</p>
<p>We report the results of LOGIPT and baselines on Table 2 and have following main findings:</p>
<p>1) When prompting with few-shot examples, open-source LMs exhibit notably poor deductive reasoning capabilities, with their outputs closed to random answering.Even the Standard prompting models of ChatGPT (gpt-3.5-turbo)and GPT-3.5 (text-davinci-003) exhibit a similar performance to random answering.This once again demonstrates that it is considerably difficult for many LLMs to solve logical reasoning tasks.</p>
<p>2) LOGIPT is significantly superior to the state-of-the-art solver-augmented LMs by a large margin on both deductive reasoning benchmarks.In ProofWriter, our best-performing model, LOGIPT (CodeLlama-13b-hf), outperforms the currently state-of-the-art LogicLM (gpt-4) by an absolute improvement of 9.84%.Mean-while, in PrOntoQA, our best-performing model LOGIPT (vicuna-13b-v1.5-16k)exhibits an even higher absolute improvement of 13.20% than LogicLM (gpt-4).This indicates that our approach is better than the pipeline of problem formulation first and then reasoning with solvers, and finetuning with solver-derived reasoning data can facilitate the deductive reasoning capacity of LMs.</p>
<p>3) LOGIPT significantly outperforms all selected open/closed-source LMs on both datasets, except for the CoT experiment on the PrOntoQA data where LOGIPT achieves comparable results with GPT-4 CoT.This is surprising considering that our underlying open-source LMs are merely 13B parameters in size.As for the baseline experiments of GPT-4, our performance on ProofWriter also significantly surpasses that of GPT-4's Standard and CoT prompting versions, as well as the Standard version of PrOntoQA.These results further demonstrate that open-source LMs, when coupled with solver-simulated reasoning capacity, can achieve performance on par with or even superior to closedsource GPT models.</p>
<p>4) The accuracy of CodeLlama-13B-Base (CodeLlama-13b-hf) with Standard prompting was 0.00, and the performance of the CoT version was close to random answering.By examining the outputs, we found that this is due to the CodeLlama-13B-Base's inability to follow the provided fewshot demonstrations, resulting in outputting no answering options.The introduction of the Instruct version of CodeLlama-13B mitigates this issue to some extent.However, after training with LOGIPT, the CodeLlama models far less encounter this issue (i.e., following the right answering format in both test sets) and even achieve better performance than the Vicuna version of LOGIPT.This demonstrates the potential of code foundation models in logical reasoning tasks, consistent with the finding on prior work (Yue et al., 2023).</p>
<p>Further Analysis</p>
<p>Impact of Solver-derived Reasoning Formats</p>
<p>We further investigate the impact of different solverderived reasoning formats on the model's performance.Specifically, we consider the following format variations: 1) w/o 'unbind' statements that we remove all 'Unbind' statements from Turn-2 to investigate the utility of the explicit retention of this action from the solver; 2) w/o 'fail &amp; back-   track' statements that we removing all 'Fail &amp; backtrack' statements from Turn-2.During the solver's reasoning process, it is expected to encounter situations in which, after binding a value, the solver realizes that not all premises are satisfied (e.g., 'Fiona is blue' but 'Fiona is not quiet' for application of Rule3 in Figure 3).Consequently, a 'Fail &amp; backtrack' operation occurs (highlighted in color in Figure 3).We explore the effectiveness of explicitly stating these operations.</p>
<p>We present the accuracy of the variations on solver-derived reasoning format on ProofWriter in Table 3 where several observations can be made: 1) regardless of using the default format, removing 'Unbind' statements, or removing 'Fail &amp; backtrack' statements, it can not be determined which format guarantees the optimal results.To retain the maximum amount of action information that the solver can provide, we still adopt the de-fault settings in LOGIPT; 2) whether 'Unbind' statements are removed or 'Fail &amp; backtrack' statements are removed, there is always an experiment under each open-source LMs that can surpass the default LOGIPT results.This further enhances the best performance of LOGIPT shown in Table 2.</p>
<p>Impact of SL Reasoning Representations</p>
<p>We are also curious about the impact of SL reasoning representations.Therefore, we include additional experiments in Table 3, denoted as w/ NL representation that we re-translate the symbolic representation (e.g., Green('Charlie', True)) back to its original NL version (e.g., Charlie is green.)and replace the original symbolic representation in Turn-2.From the table, we can find that replacing SL representations with NL results in a significant decrease in model performance, further emphasizing that symbolic representations are superior to NL representations in deductive reasoning tasks.</p>
<p>Effectiveness of Merging Data from Different Reasoning Assumptions</p>
<p>Since ProofWriter is an open-world assumption and PrOntoQA is labeled within a closed-world assumption, we also perform a further investigation on whether both reasoning assumptions can benefit each other.Specifically, we first merge both constructed training data and then test LOGIPT on each test set.The experimental results are shown in Table 4.We can conclude that if we directly mix the two types of data for training, the results on their respective test sets will be slightly lower than those obtained from training solely on their respective datasets.Therefore, we conducted an in-depth analysis of the underlying reasons and observed that in PrOntoQA, the majority of Rules are in the format of 'Every/Each A is (not) B' or 'A are (not) B'.While in ProofWriter, the predominant structure of Rules consists of: 'If someone is A, then they are B' or 'If something is A, then it is B'.Therefore, we conducted an additional set of experiments in which the Rule format of two training sets was randomly reformatted into the four aforementioned types using regular expression (denoted as 'Both (Reformat)').Then, we test the model on the original test sets.We can observe that by employing this approach, the code models yield improved performance on ProofWriter.Thus, the style/genre of logical context must also be taken into consideration to maximize the efficacy of transfer learning in logical reasoning.</p>
<p>Related Work</p>
<p>Logical Reasoning with LMs.Recent efforts in adapting Large Language Models (LLMs) for logical reasoning tasks generally adopt direct finetuning specialized modules (Clark et al., 2020;Tafjord et al., 2021Tafjord et al., , 2022;;Yang et al., 2022) or in-context learning (Zhou et al., 2022;Lyu et al., 2023;Ling et al., 2023), where reasoning in NL is used by both groups of methods.Fine-tuning approaches involve training the full model or specialized modules, enhancing LLMs with module-level logical reasoning skills like proof, enumeration, and abduction (Tafjord et al., 2021).The in-context learning approaches create specific prompts to encourage LLMs' step-by-step reasoning skills.Common methods encompass chain-of-thought prompting (Wei et al., 2022b;Chen et al., 2023), which produces explanations before delivering a final answer, and least-to-most prompting (Zhou et al., 2022), which deconstructs a problem into simpler components that can be resolved individually.Some recent work has focused on combining neural networks with symbolic reasoning (Tian et al., 2022;Pryor et al., 2022;Pan et al., 2023), especially the solver-augmented LMs that parse NL logical questions into symbolic representations, then utilizing external logical solvers for answering.Despite their impressive performance, parsing errors can lead to solver execution failure and logical question-answering issues.To address this, we propose LOGIPT, which directly imitates the solver's reasoning ability and outputs the answer.</p>
<p>Augmented LMs for Reasoning.Recent work has begun to augment LMs to overcome their inherent limitations such as the incapacity to access up-to-date information or conduct accurate mathematical reasoning.They augment with external tools and resources, such as the information retriever (Shi et al., 2023;Lazaridou et al., 2022), planner (Liu et al., 2023a) and other pre-trained models (Shen et al., 2023).Specifically, to enhance the reasoning capacity, recent work resort to external off-the-shelf Solvers including programmatic interpreters (Chen et al., 2022;Gao et al., 2023), satisfiability solvers (Ye et al., 2023), logical solvers (Pan et al., 2023) or their hybrids (Poesia et al., 2023).Most of them utilize the LMs to parse the NL question to symbolic representations and then invoke solvers to reason in SL.In this paper, we concentrate on logical solvers, automated tools for validating the truth value of logical formulas.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel LOGIPT that can directly act as a logical solver for deductive reasoning tasks.LOGIPT can output all facts implied from NL logical questions, while bypassing the syntax or grammatical errors derived from NL-to-SL parsing of solver-augmented LMs.We conducted numerous analytical experiments on two public deductive reasoning benchmarks.Evaluation results show that LOGIPT can significantly outperform state-of-the-art solver-augmented LMs, and surpass or be comparable with few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Turn-4 (from LM):</p>
<p>The comment can be parsed into: Green('Dave', False) According to the implied fact: Green('Dave', True)</p>
<p>The comment contradicts the fact.So, the correct option is: B) False</p>
<p>Figure 1 :
1
Figure 1: Data flow of current solver-augmented LMs for inference (a), and our pipeline for LOGIPT (b,c).</p>
<p>Figure 2 :
2
Figure 2: A deductive reasoning question derived from ProofWriter and its parsed Facts, Rules, and Query.</p>
<p>True) Now begin reasoning to obtain all implied facts: Use rule1: Furry($x, True)  Quiet($x, True) Bind $x to 'Dave' Obtain a new implied fact: Quiet('Dave', True) Unbind $x Finish implied with rule1 Use rule2: Green($x, True)  White($x, True)  Nice($x, True) Bind $x to 'Charlie' Obtain a new implied fact: Nice('Charlie', True) Unbind $x Bind $x to 'Fiona' Obtain an already known or implied fact: Nice('Fiona', True) Unbind $x Finish implied with rule2 Use rule3: Blue($x, True)  Quiet($x, True)  Green($x, True) Bind $x to 'Dave' Obtain a new implied fact: Green('Dave', True) Reuse rule2: Green($x, True)  White($x, True)  Nice($x, True) Bind $x to 'Dave' Obtain a new implied fact: Nice('Dave', True) Finish implied with rule2 Unbind $x Bind $x to 'Fiona' Unbind $x Finish implied with rule3 Finally, we obtain following implied facts: Green('Dave', True) (... more facts here ...) Finish reasoning</p>
<p>Figure 4 :
4
Figure 4: The full version of the comprehensive 4-turn training example of our instruction-tuning data shown in Figure 3.</p>
<p>Table 2 :
2
Main results on two evaluation datasets.The best results of LOGIPT are in bold and the best results within each dataset are underlined.
is a commonly
of each example is one of {True, False, Unknown}.to ProofWriter, PrOntoQA is in a closed-world assumption (CWA) subset where the answer of each example is one of {True, False}.For training, we merely merge all subsets with fictional characters and obtained 15,940 training cases after filtering out syntax-invalid ones.</p>
<p>Table 3 :
3
The accuracy of the variations on solver-derived reasoning format, and replacing SL representations with NL on ProofWriter.The best results on each underlying LMs are underlined.
Train setTest SetVCN CLBCLIPrOntoQAPrOntoQA96.40 95.60 96.20BothPrOntoQA91.00 87.00 89.00Both (Reformat) PrOntoQA90.00 87.00 77.80ProofWriterProofWriter 81.17 89.50 81.67BothProofWriter 79.33 87.17 79.67Both (Reformat) ProofWriter 79.00 90.83 84.50</p>
<p>Table 4 :
4
The accuracy of LOGIPT trained with merged data and tested on single data with different underlying
LMs. 'VCN', 'CLB', and 'CLI' respectively representVicuna-13B, CodeLlama-13B-Base, and CodeLlama-13B-Instruct. 'Both' means 'ProofWriter + PrOntoQA'.
In this paper, the term 'Query' refers to a specific sentence of statement or comment, while 'question' is used in a broader sense to denote the description of a logical problem.
In the original case, the Query is 'Charlie is not green.'. We replace it with 'Dave is not green.' for better illustration.
https://sharegpt.com/
A Instructions for NL-to-SL Parsing on ProofWriterTask Description: You are given a problem description and a question.The task is to: 1) define all the predicates in the problem 2) parse the problem into logic rules based on the defined predicates 3) write all the facts mentioned in the problem 4) parse the question into the logic formProblem:Anne is quiet.Erin is furry.(... more context here ...) All red people are young.Question:Based on the above information, is the following statement true, false, or unknown?Anne is white.Predicates:
What you always wanted to know about datalog(and never dared to ask). Stefano Ceri, Georg Gottlob, Letizia Tanca, IEEE transactions on knowledge and data engineering. 111989</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models. Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.143232023arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-202020</p>
<p>Programming in PROLOG. Christopher S William F Clocksin, Mellish, 2003Springer Science &amp; Business Media</p>
<p>Applying expert system technology to code reuse with pyke. Bruce Frederiksen, 2008PyCon: Chicago</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. 2023PMLR. Google. 2023. Google bard</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Fifty years of prolog and beyond. Philipp Krner, Michael Leuschel, Joo Barbosa, Santos Vtor, Vernica Costa, Manuel V Dahl, Jose F Hermenegildo, Jan Morales, Daniel Wielemaker, Salvador Diaz, Abreu, Theory and Practice of Logic Programming. 2262022</p>
<p>Internetaugmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, arXiv:2203.051152022arXiv preprint</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, arXiv:2306.03872Deductive verification of chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023barXiv preprint</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-ofthought reasoning. 2023arXiv preprint</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>ArXiv, abs/2303.08774Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D Goodman, arXiv:2306.04031Certified reasoning with language models. 2023arXiv preprint</p>
<p>L David, Alan K Poole, Mackworth, Artificial Intelligence: foundations of computational agents. Cambridge University Press2010</p>
<p>Connor Pryor, Charles Dickens, Eriq Augustine, Alon Albalak, William Wang, Lise Getoor, arXiv:2205.14268Neupsl: Neural probabilistic soft logic. 2022arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jrmy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023arXiv preprint</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Entailer: Answering questions with faithful and truthful chains of reasoning. Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, 10.18653/v1/2022.emnlp-main.134Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Weakly supervised neural symbolic learning for cognitive tasks. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Prajjwal Soumya, Shruti Bhargava, Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, 10.18653/v1/2022.emnlp-main.7Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Xi Ye, Qiaochu Chen, arXiv:2305.09656Isil Dillig, and Greg Durrett. 2023. Satisfiability-aided language models using declarative prompting. arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2022</p>
<p>statement into known facts and rules based on the defined predicates 3. begin reasoning to obtain all implied facts Context: Charlie is green. Charlie is white. Dave is blue. Dave is furry. Dave is white. Fiona is blue. Fiona is green. Fiona is nice. Fiona is rough. Fiona is white. All furry people are quiet. All green, white people are nice. If someone is blue and quiet then they are green. Reasoning: Turn-2 (from LM): We first define following predicates and corresponding natural language explanations: Green($x, bool) ::: Is x green? White($x, bool) ::: Is x white? Blue($x, bool) ::: Is x blue? Furry($x, bool) ::: Is x furry? Nice($x, bool) ::: Is x nice?. Truefrom human): Task Description: Given a statement containing logical facts and rules in natural languages as context, the task is to: 1. define all the predicates in logic forms from the context 2. parse the. Rough($x, bool) ::: Is x rough? Quiet($x, bool) ::: Is x quiet? We have following known facts from the context: Green('Charlie</p>
<p>Quiet($x, True)  Green($x, True) Now begin reasoning to obtain all implied facts: Use rule1: Furry($x, True)  Quiet($x, True) Bind $x to 'Dave' Obtain a new implied fact: Quiet('Dave', True) Unbind $x Finish implied with rule1 Use rule2: Green($x, True)  White($x, True)  Nice($x, True) Bind $x to 'Charlie' Obtain a new implied fact: Nice('Charlie', True) Unbind $x Bind $x to 'Fiona' Obtain an already known or implied fact: Nice('Fiona', True) Unbind $x Finish implied with rule2 Use rule3: Blue($x, True)  Quiet($x, True)  Green($x, True) Bind $x to 'Dave' Obtain a new implied fact: Green('Dave', True) Reuse rule2: Green($x, True)  White($x, True)  Nice($x, True) Bind $x to 'Dave' Obtain a new implied fact: Nice('Dave', True) Finish implied with rule2 Unbind $x Bind $x to 'Fiona' Unbind $x Finish implied with rule3 Finally, we obtain following implied facts: Green('Dave', True) Nice('Charlie', True) Nice('Dave', True) Quiet('Dave', True) Finish reasoning Turn-3 (from human): Question: Based on the above information, is the following comment true, false, or unknown? Dave is not green. Options: A) True B) False C) Unknown Now you should: 1. reason first carefully with the above known facts and implied facts 2. ( Green, ' Fiona, ' True) White('charlie, ' True) White('dave, ) True, ( White, ' Fiona, ) True, ' Blue('dave, ) True, ( Blue, ' Fiona, ) True, ( Furry, ' 'dave, ) True, ( Nice, ' Fiona, ) True, Rough, We have following known rules from the context: rule1: Furry($x, True)  Quiet($x, True) rule2: Green($x, True)  White($x, True)  Nice($x, True) rule3: Blue($x, True) . then output your answer strictly in the format of "So, the correct option is: X". The X should be replaced by "A) True", "B) False" or "C) Unknown"</p>            </div>
        </div>

    </div>
</body>
</html>
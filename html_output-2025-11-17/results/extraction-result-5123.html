<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5123 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5123</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5123</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-18e20944d1d64e73fc40321f65c3ddd0ef6a7aca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/18e20944d1d64e73fc40321f65c3ddd0ef6a7aca" target="_blank">Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</a></p>
                <p><strong>Paper Venue:</strong> International Workshop on Neural-Symbolic Learning and Reasoning</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates the extent to which encoder-only transformer language models (LMs) can reason according to logical rules, and shows that LMs have difficulty in transferring their putative logical reasoning ability, suggesting that they may have learned dataset-specific features, instead of a general capability.</p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is central to complex human activities, such as thinking, debating, and planning; it is also a central component of many AI systems as well. In this paper, we investigate the extent to which encoder-only transformer language models (LMs) can reason according to logical rules. We ask whether those LMs can deduce theorems in propositional calculus and first-order logic; if their relative success in these problems reflects general logical capabilities; and which layers contribute the most to the task. First, we show for several encoder-only LMs that they can be trained, to a reasonable degree, to determine logical validity on various datasets. Next, by cross-probing fine-tuned models on these datasets, we show that LMs have difficulty in transferring their putative logical reasoning ability, which suggests that they may have learned dataset-specific features, instead of a general capability. Finally, we conduct a layerwise probing experiment, which shows that the hypothesis classification task is mostly solved through higher layers.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5123.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5123.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large (Robustly optimized BERT pretraining approach, large variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-only Transformer pretrained with dynamic masked language modeling; selected in this paper for in-depth analysis and probing due to its high and consistent performance across logical-reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Roberta: A Robustly Optimized BERT Pretraining Approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (bidirectional), pretrained via dynamic masked language modeling; used in base and large variants here, with RoBERTa-large having 1024-dimensional hidden states (24 transformer blocks).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO; LogicNLI; RuleTaker (ParaRules, problog); SimpleLogic (RP Balanced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hypothesis classification on datasets grounded in first-order logic (FOL) and propositional calculus (PC): FOLIO (human-written FOL; labels True/False/Unknown), LogicNLI (semi-synthetic FOL; Entailment/Contradiction/Neutral/Paradox), RuleTaker (FOL-like conjunctive implications; True/False), SimpleLogic (synthetic PC conjunctive implication; True/False). Inputs formatted as premises + rules <SEP> hypothesis; outputs are label probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning for hypothesis classification: add linear classification head on [CLS] embedding (single linear layer, dropout 0.5), train with Adam (β=(0.9,0.999)), LR tuned (1e-5, 1e-6), early stopping (50 epochs max, patience 5). Additional analyses: cross-probing (freeze transformer body, train new probe heads of 1-layer and 3-layer) and layerwise probing (train probes on [CLS] embeddings extracted at each layer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuned test/validation accuracies (best reported per Table 2): FOLIO 64.71% (validation reported), LogicNLI 72.70%, RuleTaker 99.78%, SimpleLogic 90.83%. Pretrained (no fine-tuning) probe baselines (RoBERTa pretrained) from cross-probing: FOLIO 32.88%, LogicNLI 25.54%, RuleTaker 50.01%, SimpleLogic 61.19%. Cross-probing of RoBERTa-large fine-tuned models (probe trained on frozen bodies) achieved near-fine-tune performance only when probing the same dataset (e.g., RuleTaker fine-tuned body probed: 99.44%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fine-tuned RoBERTa-large models show limited transfer to other datasets (poor cross-dataset probe accuracy), suggesting learning of dataset-specific cues rather than general logical rules. Pretrained RoBERTa-large lacks intrinsic logical-reasoning capability for complex FOL tasks (very low pretrained probe scores). Model struggles more on datasets with higher linguistic variability and broader logical scope (FOLIO, LogicNLI) than on synthetic/simpler datasets (RuleTaker, SimpleLogic).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Among evaluated encoder-only families, RoBERTa-large achieved the best and most consistent overall performance across datasets; it outperformed DistilBERT and many other families on most datasets. However, some DeBERTa variants matched or exceeded RoBERTa on specific datasets (e.g., DeBERTa-large scored 84.70% on LogicNLI). Pretrained RoBERTa performs near the largest-class baseline on several datasets, indicating little emergent reasoning prior to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Cross-probing: probes trained on frozen, fine-tuned RoBERTa-large bodies show that the representations contain linearly recoverable task-relevant signals, but mostly dataset-specific. Layerwise probing: lower and middle layers remain near pretrained baseline; large performance gains concentrate in higher layers (final few transformer blocks), indicating fine-tuning effects and task-specific information emerge at top layers. Both 1-layer and 3-layer probes produced similar results, suggesting linear recoverability of the signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5123.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5123.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (base and large variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-only bidirectional Transformer pretrained with masked language modeling; evaluated in base and large variants for hypothesis classification on logical datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (base, large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (bidirectional) pretrained on masked language modeling and next-sentence prediction; evaluated in base and large sizes (standard publicly available variants).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO; LogicNLI; RuleTaker; SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same hypothesis-classification benchmarks as above (FOL/FOL-subsets/PC).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning on each dataset with classification head on [CLS] and identical training regimen as other models (dropout, Adam, LR tuning, early stopping).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracies: BERT base — FOLIO 52.45%, LogicNLI 48.75%, RuleTaker 98.91%, SimpleLogic 92.31%. BERT large — FOLIO 61.76%, LogicNLI 42.50%, RuleTaker 99.94%, SimpleLogic 91.88%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower performance on datasets with higher linguistic variability (LogicNLI, FOLIO) compared to RuleTaker/SimpleLogic; BERT-large improved on FOLIO but not consistently on LogicNLI. Like other encoder-only models, likely learned dataset-specific features and displayed limited cross-dataset transfer (paper's broader findings apply).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>BERT variants perform well on RuleTaker and SimpleLogic, often near-perfect for RuleTaker; RoBERTa-large generally outperforms BERT on LogicNLI and FOLIO. BERT-base better than distilled DistilBERT but behind RoBERTa and some DeBERTa variants on LogicNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No model-specific ablations beyond the general cross-probing and layerwise probing analyses (which were performed on RoBERTa-large only), but BERT followed the general pattern: high performance on constrained synthetic datasets and weaker on varied FOL datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5123.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5123.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (xsmall, small, base, large, xlarge, xxlarge variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Disentangled attention BERT-style encoder models evaluated across multiple sizes; some DeBERTa variants achieved top scores on particular datasets, but very large variants exhibited anomalous drops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeBERTa: Decoding-enhanced BERT with Disentangled Attention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer family with disentangled attention and decoding enhancements; multiple publicly released size variants were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO; LogicNLI; RuleTaker; SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hypothesis classification across FOL and PC datasets described above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with [CLS]-based classification head, same training procedure and probe analyses as other families.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracies vary by size: DeBERTa-base — FOLIO 60.78%, LogicNLI 66.70%, RuleTaker 99.81%, SimpleLogic 93.72%. DeBERTa-large — FOLIO 64.71%, LogicNLI 84.70%, RuleTaker 99.97%, SimpleLogic 93.72%. Smaller variants around mid-50s to mid-60s on FOLIO/LogicNLI; DeBERTa-xlarge and xxlarge showed anomalous low scores on some datasets (e.g., DeBERTa-xlarge LogicNLI 25.00%, DeBERTa-xxlarge LogicNLI 25.00%, SimpleLogic ~50%), suggesting training/instability issues or evaluation anomalies in these runs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some very large DeBERTa variants produced unexpectedly poor results on LogicNLI and SimpleLogic (score ~25-50%), possibly due to unstable fine-tuning, incompatibility with provided hyperparameters, or other experimental issues; overall, like other families, DeBERTa models struggle to transfer reasoning across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>DeBERTa-large matched or exceeded RoBERTa-large on LogicNLI (84.70% vs RoBERTa-large 72.70%), showing that architecture/training details can lead to dataset-specific advantages. However, irregular behavior of xlarge/xxlarge complicates simple scaling conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No separate ablations beyond aggregate evaluation. The cross-probing and layerwise probing (on RoBERTa-large) suggest that dataset-specific learning rather than general reasoning is the main contributor; similar conclusions likely apply to DeBERTa family given comparable fine-tuning setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5123.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5123.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Longformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Longformer (base and large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer variant designed for long documents (sparse attention) evaluated here as an encoder-only LM for logical reasoning tasks, showing strong performance particularly on longer inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Longformer: The Long-Document Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Longformer (base, large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer with sparse attention patterns to handle long sequences; evaluated in base and large sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO; LogicNLI; RuleTaker; SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hypothesis classification on datasets with varying average premise counts and input lengths (some datasets have long inputs where Longformer suits).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning as classification with [CLS]-based head and same optimizer/hyperparameter choices as others.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracies: Longformer-base — FOLIO 62.75%, LogicNLI 58.05%, RuleTaker 99.92%, SimpleLogic 94.21%. Longformer-large — FOLIO 62.25%, LogicNLI 74.15%, RuleTaker 99.94%, SimpleLogic 92.37%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance declines on some FOL datasets with high linguistic variability (LogicNLI) for base variant, but Longformer-large did better; like others, limited cross-dataset transfer and reliance on dataset features rather than generalizable logical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Longformer competitive with RoBERTa and DeBERTa on RuleTaker and SimpleLogic; Longformer-large's strong LogicNLI score (74.15%) indicates architecture advantages on some datasets, but RoBERTa-large and DeBERTa-large also perform strongly depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No per-model layerwise probing reported here; layerwise findings from RoBERTa-large indicate higher layers concentrate task-specific info, which likely also holds for Longformer variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5123.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5123.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT (A Lite BERT; base/large/xlarge/xxlarge variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parameter-efficient encoder-only Transformer evaluated across sizes; achieves competitive accuracy on some logic datasets but not uniformly superior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Albert: A lite bert for self-supervised learning of language representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Parameter-reduced variant of BERT using factorized embedding parameterization and cross-layer parameter sharing; evaluated in multiple size tiers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO; LogicNLI; RuleTaker; SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hypothesis classification tasks across FOL and PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with [CLS]-based classification head; same training regimen used across models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracies: ALBERT-base 57.35% (FOLIO), 66.80% (LogicNLI), 99.91% (RuleTaker), 92.17% (SimpleLogic). ALBERT-large 56.37% (FOLIO), 66.20% (LogicNLI), 99.88% (RuleTaker), 91.53% (SimpleLogic). ALBERT-xlarge and xxlarge show mixed results (ALBERT-xlarge FOLIO 38.73% anomalously low; ALBERT-xxlarge LogicNLI 93.90% high).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some large variants show inconsistent performance possibly due to instability in fine-tuning or hyperparameter mismatch; as with other encoder-only models, poor cross-dataset transfer suggests dataset-specific learning rather than general logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>ALBERT variants perform comparably to BERT/RoBERTa on constrained datasets (RuleTaker, SimpleLogic) but show mixed outcomes across FOL datasets; one ALBERT-xxlarge result on LogicNLI (93.90%) is unusually high and may reflect run-specific factors rather than consistent superiority.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No ALBERT-specific ablations; global analyses (cross-probing, layerwise probing on RoBERTa) indicate information concentrated in higher layers after fine-tuning, a pattern likely relevant to ALBERT as well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5123.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5123.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistilBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DistilBERT (distilled BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller, distilled encoder-only Transformer intended for efficiency, evaluated and found to underperform larger encoder-only models on logical reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DistilBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller distilled Transformer derived from BERT with fewer parameters and faster inference; evaluated here as an efficient encoder-only LM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO; LogicNLI; RuleTaker; SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hypothesis classification across the four logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with [CLS]-based classification head, same optimizer and hyperparameter grid used in other models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracies: FOLIO 50.98%, LogicNLI 36.20%, RuleTaker 86.55%, SimpleLogic 93.58%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Consistently lower performance than larger encoder-only models on FOLIO and LogicNLI; performs reasonably on SimpleLogic but struggles on datasets with more linguistic complexity and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms larger families (RoBERTa, DeBERTa, Longformer) especially on FOL datasets; indicates parameter/representation capacity matters for some reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No special ablations; results fit general pattern that simpler or smaller models struggle more on diverse/complex logical datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5123.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5123.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLM-RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLM-RoBERTa (base and large cross-lingual RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-lingual encoder-only Transformer trained on massive multilingual corpora; evaluated here for logical reasoning in English translations/templates and shows moderate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised cross-lingual representation learning at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLM-RoBERTa (base, large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer pretrained on multilingual data with a RoBERTa-style objective; evaluated in base and large variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO; LogicNLI; RuleTaker; SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hypothesis classification on English logical datasets (datasets were in natural language).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standard fine-tuning for classification; same probe methodologies available for cross-probing/layerwise analysis (though detailed probes were performed on RoBERTa-large only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>XLM-RoBERTa-base: FOLIO 55.88%, LogicNLI 45.20%, RuleTaker 97.64%, SimpleLogic 91.74%. XLM-RoBERTa-large: FOLIO 62.75%, LogicNLI 65.45%, RuleTaker 99.95%, SimpleLogic 91.74%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Moderate performance on LogicNLI/FOLIO compared to monolingual RoBERTa/DeBERTa; like other models, displays limited cross-dataset transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Large variant improves over base but still trails top-performing monolingual models on some FOL benchmarks; excels in RuleTaker similarly to many models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No specific ablation; overall probing results from RoBERTa-large suggest higher-layer specialization after fine-tuning which likely generalizes to XLM-R variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5123.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5123.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet (base and large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive-permutation pretraining encoder-only-style Transformer evaluated for logical hypothesis classification; achieves competitive scores on constrained datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Xlnet: Generalized autoregressive pretraining for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet (base, large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer architecture using permutation-based autoregressive pretraining to capture bidirectional contexts; evaluated in base and large sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO; LogicNLI; RuleTaker; SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hypothesis classification across the four logic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with [CLS] classification head and same training/hyperparameter procedures used across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>XLNet-base: FOLIO 58.33%, LogicNLI 55.00%, RuleTaker 99.19%, SimpleLogic 94.28%. XLNet-large: FOLIO 58.33%, LogicNLI 71.40%, RuleTaker 99.86%, SimpleLogic 92.94%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Similar pattern: strong on RuleTaker and SimpleLogic, weaker on FOLIO/LogicNLI though XLNet-large reaches 71.40% on LogicNLI. Cross-dataset transfer not shown to be strong.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Competitive with RoBERTa and Longformer on some datasets (particularly when using large variant), but RoBERTa-large and DeBERTa-large reported best overall aggregates.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No model-specific ablation; global analyses indicate that task-specific information concentrates in higher layers after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as Soft Reasoners over Language <em>(Rating: 2)</em></li>
                <li>FOLIO: Natural Language Reasoning with First-Order Logic <em>(Rating: 2)</em></li>
                <li>Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI <em>(Rating: 2)</em></li>
                <li>On the Paradox of Learning to Reason from Data <em>(Rating: 2)</em></li>
                <li>Negation, Coordination, and Quantifiers in Contextualized Language Models <em>(Rating: 1)</em></li>
                <li>Emergent Abilities of Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5123",
    "paper_id": "paper-18e20944d1d64e73fc40321f65c3ddd0ef6a7aca",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "RoBERTa-large",
            "name_full": "RoBERTa-large (Robustly optimized BERT pretraining approach, large variant)",
            "brief_description": "An encoder-only Transformer pretrained with dynamic masked language modeling; selected in this paper for in-depth analysis and probing due to its high and consistent performance across logical-reasoning datasets.",
            "citation_title": "Roberta: A Robustly Optimized BERT Pretraining Approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Encoder-only Transformer (bidirectional), pretrained via dynamic masked language modeling; used in base and large variants here, with RoBERTa-large having 1024-dimensional hidden states (24 transformer blocks).",
            "model_size": null,
            "logical_reasoning_task": "FOLIO; LogicNLI; RuleTaker (ParaRules, problog); SimpleLogic (RP Balanced)",
            "task_description": "Hypothesis classification on datasets grounded in first-order logic (FOL) and propositional calculus (PC): FOLIO (human-written FOL; labels True/False/Unknown), LogicNLI (semi-synthetic FOL; Entailment/Contradiction/Neutral/Paradox), RuleTaker (FOL-like conjunctive implications; True/False), SimpleLogic (synthetic PC conjunctive implication; True/False). Inputs formatted as premises + rules &lt;SEP&gt; hypothesis; outputs are label probabilities.",
            "method_or_approach": "Fine-tuning for hypothesis classification: add linear classification head on [CLS] embedding (single linear layer, dropout 0.5), train with Adam (β=(0.9,0.999)), LR tuned (1e-5, 1e-6), early stopping (50 epochs max, patience 5). Additional analyses: cross-probing (freeze transformer body, train new probe heads of 1-layer and 3-layer) and layerwise probing (train probes on [CLS] embeddings extracted at each layer).",
            "performance": "Fine-tuned test/validation accuracies (best reported per Table 2): FOLIO 64.71% (validation reported), LogicNLI 72.70%, RuleTaker 99.78%, SimpleLogic 90.83%. Pretrained (no fine-tuning) probe baselines (RoBERTa pretrained) from cross-probing: FOLIO 32.88%, LogicNLI 25.54%, RuleTaker 50.01%, SimpleLogic 61.19%. Cross-probing of RoBERTa-large fine-tuned models (probe trained on frozen bodies) achieved near-fine-tune performance only when probing the same dataset (e.g., RuleTaker fine-tuned body probed: 99.44%).",
            "limitations_or_failure_cases": "Fine-tuned RoBERTa-large models show limited transfer to other datasets (poor cross-dataset probe accuracy), suggesting learning of dataset-specific cues rather than general logical rules. Pretrained RoBERTa-large lacks intrinsic logical-reasoning capability for complex FOL tasks (very low pretrained probe scores). Model struggles more on datasets with higher linguistic variability and broader logical scope (FOLIO, LogicNLI) than on synthetic/simpler datasets (RuleTaker, SimpleLogic).",
            "comparison": "Among evaluated encoder-only families, RoBERTa-large achieved the best and most consistent overall performance across datasets; it outperformed DistilBERT and many other families on most datasets. However, some DeBERTa variants matched or exceeded RoBERTa on specific datasets (e.g., DeBERTa-large scored 84.70% on LogicNLI). Pretrained RoBERTa performs near the largest-class baseline on several datasets, indicating little emergent reasoning prior to fine-tuning.",
            "ablation_or_analysis_results": "Cross-probing: probes trained on frozen, fine-tuned RoBERTa-large bodies show that the representations contain linearly recoverable task-relevant signals, but mostly dataset-specific. Layerwise probing: lower and middle layers remain near pretrained baseline; large performance gains concentrate in higher layers (final few transformer blocks), indicating fine-tuning effects and task-specific information emerge at top layers. Both 1-layer and 3-layer probes produced similar results, suggesting linear recoverability of the signals.",
            "uuid": "e5123.0",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT (base and large variants)",
            "brief_description": "Encoder-only bidirectional Transformer pretrained with masked language modeling; evaluated in base and large variants for hypothesis classification on logical datasets.",
            "citation_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "mention_or_use": "use",
            "model_name": "BERT (base, large)",
            "model_description": "Encoder-only Transformer (bidirectional) pretrained on masked language modeling and next-sentence prediction; evaluated in base and large sizes (standard publicly available variants).",
            "model_size": null,
            "logical_reasoning_task": "FOLIO; LogicNLI; RuleTaker; SimpleLogic",
            "task_description": "Same hypothesis-classification benchmarks as above (FOL/FOL-subsets/PC).",
            "method_or_approach": "Fine-tuning on each dataset with classification head on [CLS] and identical training regimen as other models (dropout, Adam, LR tuning, early stopping).",
            "performance": "Reported accuracies: BERT base — FOLIO 52.45%, LogicNLI 48.75%, RuleTaker 98.91%, SimpleLogic 92.31%. BERT large — FOLIO 61.76%, LogicNLI 42.50%, RuleTaker 99.94%, SimpleLogic 91.88%.",
            "limitations_or_failure_cases": "Lower performance on datasets with higher linguistic variability (LogicNLI, FOLIO) compared to RuleTaker/SimpleLogic; BERT-large improved on FOLIO but not consistently on LogicNLI. Like other encoder-only models, likely learned dataset-specific features and displayed limited cross-dataset transfer (paper's broader findings apply).",
            "comparison": "BERT variants perform well on RuleTaker and SimpleLogic, often near-perfect for RuleTaker; RoBERTa-large generally outperforms BERT on LogicNLI and FOLIO. BERT-base better than distilled DistilBERT but behind RoBERTa and some DeBERTa variants on LogicNLI.",
            "ablation_or_analysis_results": "No model-specific ablations beyond the general cross-probing and layerwise probing analyses (which were performed on RoBERTa-large only), but BERT followed the general pattern: high performance on constrained synthetic datasets and weaker on varied FOL datasets.",
            "uuid": "e5123.1",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeBERTa",
            "name_full": "DeBERTa (xsmall, small, base, large, xlarge, xxlarge variants)",
            "brief_description": "Disentangled attention BERT-style encoder models evaluated across multiple sizes; some DeBERTa variants achieved top scores on particular datasets, but very large variants exhibited anomalous drops.",
            "citation_title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "mention_or_use": "use",
            "model_name": "DeBERTa (various)",
            "model_description": "Encoder-only Transformer family with disentangled attention and decoding enhancements; multiple publicly released size variants were evaluated.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO; LogicNLI; RuleTaker; SimpleLogic",
            "task_description": "Hypothesis classification across FOL and PC datasets described above.",
            "method_or_approach": "Fine-tuning with [CLS]-based classification head, same training procedure and probe analyses as other families.",
            "performance": "Reported accuracies vary by size: DeBERTa-base — FOLIO 60.78%, LogicNLI 66.70%, RuleTaker 99.81%, SimpleLogic 93.72%. DeBERTa-large — FOLIO 64.71%, LogicNLI 84.70%, RuleTaker 99.97%, SimpleLogic 93.72%. Smaller variants around mid-50s to mid-60s on FOLIO/LogicNLI; DeBERTa-xlarge and xxlarge showed anomalous low scores on some datasets (e.g., DeBERTa-xlarge LogicNLI 25.00%, DeBERTa-xxlarge LogicNLI 25.00%, SimpleLogic ~50%), suggesting training/instability issues or evaluation anomalies in these runs.",
            "limitations_or_failure_cases": "Some very large DeBERTa variants produced unexpectedly poor results on LogicNLI and SimpleLogic (score ~25-50%), possibly due to unstable fine-tuning, incompatibility with provided hyperparameters, or other experimental issues; overall, like other families, DeBERTa models struggle to transfer reasoning across datasets.",
            "comparison": "DeBERTa-large matched or exceeded RoBERTa-large on LogicNLI (84.70% vs RoBERTa-large 72.70%), showing that architecture/training details can lead to dataset-specific advantages. However, irregular behavior of xlarge/xxlarge complicates simple scaling conclusions.",
            "ablation_or_analysis_results": "No separate ablations beyond aggregate evaluation. The cross-probing and layerwise probing (on RoBERTa-large) suggest that dataset-specific learning rather than general reasoning is the main contributor; similar conclusions likely apply to DeBERTa family given comparable fine-tuning setup.",
            "uuid": "e5123.2",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Longformer",
            "name_full": "Longformer (base and large)",
            "brief_description": "Transformer variant designed for long documents (sparse attention) evaluated here as an encoder-only LM for logical reasoning tasks, showing strong performance particularly on longer inputs.",
            "citation_title": "Longformer: The Long-Document Transformer",
            "mention_or_use": "use",
            "model_name": "Longformer (base, large)",
            "model_description": "Encoder-only Transformer with sparse attention patterns to handle long sequences; evaluated in base and large sizes.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO; LogicNLI; RuleTaker; SimpleLogic",
            "task_description": "Hypothesis classification on datasets with varying average premise counts and input lengths (some datasets have long inputs where Longformer suits).",
            "method_or_approach": "Fine-tuning as classification with [CLS]-based head and same optimizer/hyperparameter choices as others.",
            "performance": "Reported accuracies: Longformer-base — FOLIO 62.75%, LogicNLI 58.05%, RuleTaker 99.92%, SimpleLogic 94.21%. Longformer-large — FOLIO 62.25%, LogicNLI 74.15%, RuleTaker 99.94%, SimpleLogic 92.37%.",
            "limitations_or_failure_cases": "Performance declines on some FOL datasets with high linguistic variability (LogicNLI) for base variant, but Longformer-large did better; like others, limited cross-dataset transfer and reliance on dataset features rather than generalizable logical rules.",
            "comparison": "Longformer competitive with RoBERTa and DeBERTa on RuleTaker and SimpleLogic; Longformer-large's strong LogicNLI score (74.15%) indicates architecture advantages on some datasets, but RoBERTa-large and DeBERTa-large also perform strongly depending on dataset.",
            "ablation_or_analysis_results": "No per-model layerwise probing reported here; layerwise findings from RoBERTa-large indicate higher layers concentrate task-specific info, which likely also holds for Longformer variants.",
            "uuid": "e5123.3",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ALBERT",
            "name_full": "ALBERT (A Lite BERT; base/large/xlarge/xxlarge variants)",
            "brief_description": "Parameter-efficient encoder-only Transformer evaluated across sizes; achieves competitive accuracy on some logic datasets but not uniformly superior.",
            "citation_title": "Albert: A lite bert for self-supervised learning of language representations",
            "mention_or_use": "use",
            "model_name": "ALBERT (various)",
            "model_description": "Parameter-reduced variant of BERT using factorized embedding parameterization and cross-layer parameter sharing; evaluated in multiple size tiers.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO; LogicNLI; RuleTaker; SimpleLogic",
            "task_description": "Hypothesis classification tasks across FOL and PC datasets.",
            "method_or_approach": "Fine-tuning with [CLS]-based classification head; same training regimen used across models.",
            "performance": "Reported accuracies: ALBERT-base 57.35% (FOLIO), 66.80% (LogicNLI), 99.91% (RuleTaker), 92.17% (SimpleLogic). ALBERT-large 56.37% (FOLIO), 66.20% (LogicNLI), 99.88% (RuleTaker), 91.53% (SimpleLogic). ALBERT-xlarge and xxlarge show mixed results (ALBERT-xlarge FOLIO 38.73% anomalously low; ALBERT-xxlarge LogicNLI 93.90% high).",
            "limitations_or_failure_cases": "Some large variants show inconsistent performance possibly due to instability in fine-tuning or hyperparameter mismatch; as with other encoder-only models, poor cross-dataset transfer suggests dataset-specific learning rather than general logical reasoning.",
            "comparison": "ALBERT variants perform comparably to BERT/RoBERTa on constrained datasets (RuleTaker, SimpleLogic) but show mixed outcomes across FOL datasets; one ALBERT-xxlarge result on LogicNLI (93.90%) is unusually high and may reflect run-specific factors rather than consistent superiority.",
            "ablation_or_analysis_results": "No ALBERT-specific ablations; global analyses (cross-probing, layerwise probing on RoBERTa) indicate information concentrated in higher layers after fine-tuning, a pattern likely relevant to ALBERT as well.",
            "uuid": "e5123.4",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DistilBERT",
            "name_full": "DistilBERT (distilled BERT)",
            "brief_description": "A smaller, distilled encoder-only Transformer intended for efficiency, evaluated and found to underperform larger encoder-only models on logical reasoning benchmarks.",
            "citation_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "mention_or_use": "use",
            "model_name": "DistilBERT",
            "model_description": "Smaller distilled Transformer derived from BERT with fewer parameters and faster inference; evaluated here as an efficient encoder-only LM.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO; LogicNLI; RuleTaker; SimpleLogic",
            "task_description": "Hypothesis classification across the four logical reasoning datasets.",
            "method_or_approach": "Fine-tuning with [CLS]-based classification head, same optimizer and hyperparameter grid used in other models.",
            "performance": "Reported accuracies: FOLIO 50.98%, LogicNLI 36.20%, RuleTaker 86.55%, SimpleLogic 93.58%.",
            "limitations_or_failure_cases": "Consistently lower performance than larger encoder-only models on FOLIO and LogicNLI; performs reasonably on SimpleLogic but struggles on datasets with more linguistic complexity and depth.",
            "comparison": "Underperforms larger families (RoBERTa, DeBERTa, Longformer) especially on FOL datasets; indicates parameter/representation capacity matters for some reasoning tasks.",
            "ablation_or_analysis_results": "No special ablations; results fit general pattern that simpler or smaller models struggle more on diverse/complex logical datasets.",
            "uuid": "e5123.5",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "XLM-RoBERTa",
            "name_full": "XLM-RoBERTa (base and large cross-lingual RoBERTa)",
            "brief_description": "Cross-lingual encoder-only Transformer trained on massive multilingual corpora; evaluated here for logical reasoning in English translations/templates and shows moderate performance.",
            "citation_title": "Unsupervised cross-lingual representation learning at scale",
            "mention_or_use": "use",
            "model_name": "XLM-RoBERTa (base, large)",
            "model_description": "Encoder-only Transformer pretrained on multilingual data with a RoBERTa-style objective; evaluated in base and large variants.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO; LogicNLI; RuleTaker; SimpleLogic",
            "task_description": "Hypothesis classification on English logical datasets (datasets were in natural language).",
            "method_or_approach": "Standard fine-tuning for classification; same probe methodologies available for cross-probing/layerwise analysis (though detailed probes were performed on RoBERTa-large only).",
            "performance": "XLM-RoBERTa-base: FOLIO 55.88%, LogicNLI 45.20%, RuleTaker 97.64%, SimpleLogic 91.74%. XLM-RoBERTa-large: FOLIO 62.75%, LogicNLI 65.45%, RuleTaker 99.95%, SimpleLogic 91.74%.",
            "limitations_or_failure_cases": "Moderate performance on LogicNLI/FOLIO compared to monolingual RoBERTa/DeBERTa; like other models, displays limited cross-dataset transfer.",
            "comparison": "Large variant improves over base but still trails top-performing monolingual models on some FOL benchmarks; excels in RuleTaker similarly to many models.",
            "ablation_or_analysis_results": "No specific ablation; overall probing results from RoBERTa-large suggest higher-layer specialization after fine-tuning which likely generalizes to XLM-R variants.",
            "uuid": "e5123.6",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "XLNet",
            "name_full": "XLNet (base and large)",
            "brief_description": "Autoregressive-permutation pretraining encoder-only-style Transformer evaluated for logical hypothesis classification; achieves competitive scores on constrained datasets.",
            "citation_title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "mention_or_use": "use",
            "model_name": "XLNet (base, large)",
            "model_description": "Transformer architecture using permutation-based autoregressive pretraining to capture bidirectional contexts; evaluated in base and large sizes.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO; LogicNLI; RuleTaker; SimpleLogic",
            "task_description": "Hypothesis classification across the four logic datasets.",
            "method_or_approach": "Fine-tuning with [CLS] classification head and same training/hyperparameter procedures used across experiments.",
            "performance": "XLNet-base: FOLIO 58.33%, LogicNLI 55.00%, RuleTaker 99.19%, SimpleLogic 94.28%. XLNet-large: FOLIO 58.33%, LogicNLI 71.40%, RuleTaker 99.86%, SimpleLogic 92.94%.",
            "limitations_or_failure_cases": "Similar pattern: strong on RuleTaker and SimpleLogic, weaker on FOLIO/LogicNLI though XLNet-large reaches 71.40% on LogicNLI. Cross-dataset transfer not shown to be strong.",
            "comparison": "Competitive with RoBERTa and Longformer on some datasets (particularly when using large variant), but RoBERTa-large and DeBERTa-large reported best overall aggregates.",
            "ablation_or_analysis_results": "No model-specific ablation; global analyses indicate that task-specific information concentrates in higher layers after fine-tuning.",
            "uuid": "e5123.7",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as Soft Reasoners over Language",
            "rating": 2
        },
        {
            "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
            "rating": 2
        },
        {
            "paper_title": "Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI",
            "rating": 2
        },
        {
            "paper_title": "On the Paradox of Learning to Reason from Data",
            "rating": 2
        },
        {
            "paper_title": "Negation, Coordination, and Quantifiers in Contextualized Language Models",
            "rating": 1
        },
        {
            "paper_title": "Emergent Abilities of Large Language Models",
            "rating": 1
        }
    ],
    "cost": 0.01544725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</h1>
<p>Paulo Pirozelli ${ }^{1[0000-0002-4714-287 X]}$, Marcos M. José ${ }^{2[0000-0003-4663-4386]}$, Paulo de Tarso P. Filho ${ }^{2[0009-0000-8085-795 X]}$, Anarosa A. F.<br>Brandão ${ }^{2[0000-0001-8992-4768]}$, and Fabio G. Cozman ${ }^{2[0000-0003-4077-4935]}$<br>${ }^{1}$ Instituto Mauá de Tecnologia<br>${ }^{2}$ Universidade de São Paulo<br>${ }^{3}$ University of Alberta<br>paulo.silva@maua.br</p>
<h4>Abstract</h4>
<p>Transformer models have shown impressive abilities in natural language tasks such as text generation and question answering. Still, it is not clear whether these models can successfully conduct a rule-guided task such as logical reasoning. In this paper, we investigate the extent to which encoder-only transformer language models (LMs) can reason according to logical rules. We ask whether these LMs can deduce theorems in propositional calculus and first-order logic, if their relative success in these problems reflects general logical capabilities, and which layers contribute the most to the task. First, we show for several encoder-only LMs that they can be trained, to a reasonable degree, to determine logical validity on various datasets. Next, by cross-probing fine-tuned models on these datasets, we show that LMs have difficulty in transferring their putative logical reasoning ability, which suggests that they may have learned dataset-specific features instead of a general capability. Finally, we conduct a layerwise probing experiment, which shows that the hypothesis classification task is mostly solved through higher layers.</p>
<p>Keywords: Logical reasoning $\cdot$ Language models $\cdot$ Transformer $\cdot$ Probing</p>
<h2>1 Introduction</h2>
<p>Transformer models are remarkably effective at a wide range of natural language processing (NLP) tasks, such as question answering, summarization, and text generation. By and large, these abilities are the result of specific training processes, where a language model is fine-tuned on a task-specific dataset. Curiously, encoder-only transformer models (LMs) also exhibit implicit linguistic and cognitive abilities for which they were not directly supervised. Such LMs have been shown to encode information on tense and number [7], anaphora and determiner-noun agreement [49], semantic roles [46], syntactic dependencies [32], relational knowledge [8], and spatiotemporal representation [15].</p>
<p>Given that logical reasoning is a core component of intelligence, human or otherwise [38], it is worth investigating the capabilities of encoder-only transformer models in executing tasks that necessitate such reasoning. Understanding whether LMs can</p>
<p>solve logical problems, and the manner in which they tackle these problems, may enable us to understand the extent to which their inferences arise from reasoning rather than purely associative memory. This understanding is crucial for developing mechanisms that facilitate the generation of consistent outputs, whether in a neural-symbolic fashion or through the improvement of model architectures.</p>
<p>The goal of this paper is, thus, to assess the ability of encoder-only transformer models to reason according to the rules of logic - understood here as deductive arguments expressable in propositional calculus or first-order logic. We examine three main questions throughout this paper: i) Can enconder-only transformer models perform logical reasoning tasks?; ii) How general is this ability?; and iii) What layers better contribute to solving these tasks?</p>
<p>Section 2 reviews the work on transformers' logical reasoning abilities, as well as the function of probing in uncovering latent knowledge. Next, we gather and describe four datasets grounded on logical deduction (sec. 3). In a first batch of experiments, we conduct a systematic comparison of encoder-only transformer models on these datasets (sec. 4). Section 5 then investigates whether the performance in the previous task could be attributed to some general ability and whether the reasoning learned from one dataset could be transferred to a similar dataset. Finally, we perform a layerwise probing to understand which layers are responsible for solving logical deduction problems (sec. $6) .^{4}$</p>
<h1>2 Related Work</h1>
<p>Logical Reasoning in Transformer Models Transformer models are powerful enough to solve logical reasoning tasks expressed in natural language [4|55|47|17]. Yet, it is not clear if these models have actually mastered logical reasoning. LMs seem to inevitably rely on statistical artifacts to deduce theorems, rather than on general, rule-based relationships [55]. They use shortcuts to solve hypothesis classification problems, leading to vulnerabilities in reasoning (e.g., LMs are fooled when hypotheses appear within rules), and making them susceptible to irrelevant (logically consistent) perturbations [14].</p>
<p>Studies focused on functional words close to logical operators have identified similar shortcomings in LMs' reasoning capabilities. Transformers struggle to deal with negation, predicting similar probabilities to a sentence and its negation [25|22]. [24] extend these findings to conjunction, disjunction, and existential and universal quantification, showing that expressions associated with these operations are frequently dominated by semantically rich words. Transformers also fail in modeling semi-functionalsemi-content words in general, such as quantifiers and modals [41]. Tangentially to logic, [9] show that transformer models do not always properly handle compositionality: sometimes a translation is more local than desired (treating idioms as regular constructions), sometimes it is excessively global (paying attention to irrelevant parts of the sentence). Moreover, there is evidence that the way LMs compose sentences does not align well with human judgment [28].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Large LMs have also been assessed for their logical reasoning capabilities. Despite their impressive achievements in numerous tasks, these models still struggle with multistep and compositional problems [36|12]. Although good at individual deductions, large LMs struggle with proof planing: when many valid proof steps are available, they often take the wrong path [40]. Large LMs also appear to suffer from human-like biases in logical tasks: they perform significantly worse when the semantic content is too abstract or conflicts with prior knowledge [10|44].</p>
<p>Probing Tasks Probing is a technique used to discover if a model has acquired certain type of knowledge. In probing, a dataset that encodes a particular property (e.g., part-of-speech) is used to train a classifier (the probe), taking the representations produced by the original model as inputs for the classification [1]. As the LM is not further trained on the task, as it would be in fine-tuning, the probe's performance depends on whether the information about that property had already been encoded by the model. Thus, success in the task provides some evidence that the model has stored such knowledge in its parameters. Probing can be used to examine several components of LMs, such as embeddings, attention heads, and feedforward computations.</p>
<p>Transformer models have been extensively studied through probing [1]. Most attention has been given to BERT, which gave rise to a large literature on the properties encoded by this model [37]. RoBERTa has also been studied in some detail through probing; e.g., what abilities that model learns during training [56|30] and its knowledge of semantic structural information [51].</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Fig. 1. Examples of logical reasoning arguments. The argument at the top is from FOLIO, a manually-written dataset; the one at the bottom is from RuleTaker, a dataset that uses a semisynthetic approach.</p>
<p>Table 1. Main features of the logical reasoning datasets. FOL stands for first-order logic, PC for propositional calculus, and CI for conjunctive implication. The labels in the datasets are as follows: FOLIO (False, True, Unknown), LogicNLI (Contradiction, Entailment, Neutral, Paradox), RuleTaker (False, True), SimpleLogic (False, True). The average number of premises and the average number of words per argument refer to the training set statistics. Appendix B shows the full dataset label distribution.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Size (train/val/test)</th>
<th style="text-align: left;">Scope</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Label</th>
<th style="text-align: left;">Avg. Premises</th>
<th style="text-align: left;">Avg. Words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FOLIO</td>
<td style="text-align: left;">$1003 / 204 /-$</td>
<td style="text-align: left;">FOL</td>
<td style="text-align: left;">Manual</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">5.23</td>
<td style="text-align: left;">64.01</td>
</tr>
<tr>
<td style="text-align: left;">LogicNLI</td>
<td style="text-align: left;">$16000 / 2000 / 2000$</td>
<td style="text-align: left;">FOL</td>
<td style="text-align: left;">Semi-synt.</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">211.86</td>
</tr>
<tr>
<td style="text-align: left;">RuleTaker</td>
<td style="text-align: left;">$27363 / 3899 / 7793$</td>
<td style="text-align: left;">FOL (CI)</td>
<td style="text-align: left;">Semi-synt.</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">16.30</td>
<td style="text-align: left;">100.06</td>
</tr>
<tr>
<td style="text-align: left;">SimpleLogic</td>
<td style="text-align: left;">$11341 / 1418 / 1417$</td>
<td style="text-align: left;">PC (CI)</td>
<td style="text-align: left;">Synthetic</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">60.76</td>
<td style="text-align: left;">467.45</td>
</tr>
</tbody>
</table>
<h1>3 Logical Reasoning Datasets</h1>
<p>In assessing logical reasoning, we restricted our analysis to datasets related to propositional calculus (PC) and first-order logic (FOL), with FOL being an extension of PC that includes predicates and quantifiers. These logical systems offer a powerful formalism that balances simplicity and expressivity in representing and reasoning about statements and relationships. They are suitable for capturing a wide range of knowledge and formalizing a large number of domains.</p>
<p>In addition to being expressible in PC or FOL, we selected datasets that satisfied three other properties: i) observations had to be as self-contained as possible; ii) sentences needed to have corresponding translations in both logical formalism and natural language; and iii) hypotheses had to be declarative sentences. The first property aims to decouple logic from background knowledge in order to assess pure reasoning capabilities. While using natural language examples means implicit knowledge can never be completely erased, we opted for datasets that minimized this by explicitly stating prior knowledge or by using inference patterns where resorting to prior knowledge is unnecessary. For this reason, we only considered pure logical datasets and did not include other forms of reasoning such as scientific reasoning [3,26,54], mathematical reasoning [5,34,20], counterfactual reasoning [35,52,33], planning [48], inductive reasoning [42,44], and abductive reasoning [44]. The second property, the translation into logical formalism, allows one to determine unambiguously the logical relationship between premises and hypotheses, while the natural language counterpart allows us to probe the LMs. Hence, we excluded datasets that lacked natural language translations, such as LTL [16]. The last property excluded QA datasets [29,43], as they required the understanding of several types of questions (e.g., who, where) that are entangled with semantic and contextual knowledge (e.g., that "Alice" is the name of a person).</p>
<p>In the end, four logical reasoning datasets were selected: FOLIO [17], LogicNLI [47], RuleTaker [4], and SimpleLogic [55]. Examples of arguments can be seen in Figure 1. These datasets cover a wide range of variations in terms of construction (manual, semi-synthetic, synthetic), alignment with common sense, linguistic variability, and scope (PC, full or partial FOL). For instance, FOLIO is human-written; LogicNLI and RuleTaker both use a template which is then manually edited; and SimpleLogic is fully synthetic. FOLIO and LogicNLI encompass the full spectrum of FOL; RuleTaker is</p>
<p>expressible in FOL but only covers negation and conjunctive implications, ${ }^{5}$ and SimpleLogic is restricted to conjunctive implications in PC. Regarding the number of labels, RuleTaker and SimpleLogic have True and False labels; FOLIO includes an Unknown label; and LogicNLI admits a fourth possibility, Paradox, where both a hypothesis and its negation can be deduced from the premises. Table 1 summarizes the main statistics of the datasets. Appendix A provides a detailed description of the four datasets.</p>
<p>For all datasets, inputs are formatted as "fact ${ }<em 2="2">{1}$. fact $</em>$. . . fact $<em 1="1">{n}$. rule ${ }</em>$. rule $<em m="m">{2}$. . . rule $</em>$ <SEP> hypothesis.", for which a label must be predicted. The output is simply a probability distribution over the possible labels, which varies from dataset to dataset.</p>
<h1>4 Testing LMs for Hypothesis Classification</h1>
<p>The performance of LMs in logical reasoning, including in the datasets described in the previous section, has been studied in scattered experiments without a clear unified context that allows direct comparison. Due to this, we decided to fine-tune a wide range of pretrained encoder-only transformer models on those datasets. ${ }^{6}$ We modeled this problem as a hypothesis classification task, where the goal is to determine the logical relationship between a set of premises and a conclusion. To ensure comprehensive coverage, eight families of LMs were assessed: DistilBERT [39], BERT [11], RoBERTa [31], Longformer [2], DeBERTa [19], AlBERT [27], XLM-RoBERTa [6], and XLNet [53]. The full list of models is displayed in the left column of Table 2.</p>
<p>To fine-tune our models, we used a classification head with a single linear layer of dimension (embedding-length, labels), and applied a dropout of 0.5 to the linear operation. As input for the classification head, we used the last hidden embedding of the [CLS] token, as is customary for classification tasks in NLP. Sequences were padded and truncated to the maximum length allowed by each LM. Models were trained for up to 50 epochs, using early stopping with a patience of 5 epochs. We used Adam as our optimizer (with $\beta=[0.9,0.999]$ and no weight decay) and experimented with two different learning rates (1e-5 and 1e-6). Models were selected based on the loss for the validation set, with results reported for the test set. The only exception is FOLIO, where only the validation set is publicly available; results are thus reported for this set. We report accuracy as the standard metric, as classes in the datasets are balanced. Table 2 displays the best result achieved in each case. The best and worst scores for each dataset are highlighted in blue and red, respectively. We also provide a largest class baseline for comparison.</p>
<p>Results show that the LMs were able to classify hypotheses with reasonable success. Almost all models came close to solving RuleTaker and achieved an accuracy above $90 \%$ in SimpleLogic. Results were comparatively lower for FOLIO and LogicNLI, but the LMs generally surpassed the largest class baselines by a considerable margin. A</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2. Accuracy comparison among several encoder-only transformer models for the hypothesis classification task across four datasets: FOLIO, LogicNLI, RuleTaker, and SimpleLogic. The models are listed in the left column of the table. The best and worst scores for each dataset are highlighted in blue and red, respectively. "Largest class" refers to the accuracy achieved by always selecting the class with the highest frequency in the training set. Results for RoBERTa-large, selected for subsequent analyses, are in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">FOLIO</th>
<th style="text-align: center;">LogicNLI</th>
<th style="text-align: center;">RuleTaker</th>
<th style="text-align: center;">SimpleLogic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DistilBERT</td>
<td style="text-align: center;">50.98</td>
<td style="text-align: center;">36.20</td>
<td style="text-align: center;">86.55</td>
<td style="text-align: center;">93.58</td>
</tr>
<tr>
<td style="text-align: left;">BERT base</td>
<td style="text-align: center;">52.45</td>
<td style="text-align: center;">48.75</td>
<td style="text-align: center;">98.91</td>
<td style="text-align: center;">92.31</td>
</tr>
<tr>
<td style="text-align: left;">BERT large</td>
<td style="text-align: center;">61.76</td>
<td style="text-align: center;">42.50</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">91.88</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa base</td>
<td style="text-align: center;">58.82</td>
<td style="text-align: center;">62.90</td>
<td style="text-align: center;">98.04</td>
<td style="text-align: center;">92.87</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa large</td>
<td style="text-align: center;">$\mathbf{6 4 . 7 1}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 7 0}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 7 8}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 8 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Longformer base</td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">58.05</td>
<td style="text-align: center;">99.92</td>
<td style="text-align: center;">94.21</td>
</tr>
<tr>
<td style="text-align: left;">Longformer large</td>
<td style="text-align: center;">62.25</td>
<td style="text-align: center;">74.15</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">92.37</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa xsmall</td>
<td style="text-align: center;">54.41</td>
<td style="text-align: center;">65.30</td>
<td style="text-align: center;">99.81</td>
<td style="text-align: center;">91.67</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa small</td>
<td style="text-align: center;">53.43</td>
<td style="text-align: center;">59.65</td>
<td style="text-align: center;">95.23</td>
<td style="text-align: center;">93.23</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa base</td>
<td style="text-align: center;">60.78</td>
<td style="text-align: center;">66.70</td>
<td style="text-align: center;">99.81</td>
<td style="text-align: center;">93.72</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa large</td>
<td style="text-align: center;">64.71</td>
<td style="text-align: center;">84.70</td>
<td style="text-align: center;">99.97</td>
<td style="text-align: center;">93.72</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa xlarge</td>
<td style="text-align: center;">62.25</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">49.96</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa xxlarge</td>
<td style="text-align: center;">71.57</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.02</td>
<td style="text-align: center;">50.04</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT base</td>
<td style="text-align: center;">57.35</td>
<td style="text-align: center;">66.80</td>
<td style="text-align: center;">99.91</td>
<td style="text-align: center;">92.17</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT large</td>
<td style="text-align: center;">56.37</td>
<td style="text-align: center;">66.20</td>
<td style="text-align: center;">99.88</td>
<td style="text-align: center;">91.53</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT xlarge</td>
<td style="text-align: center;">38.73</td>
<td style="text-align: center;">65.10</td>
<td style="text-align: center;">99.97</td>
<td style="text-align: center;">90.05</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT xxlarge</td>
<td style="text-align: center;">56.86</td>
<td style="text-align: center;">93.90</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">92.24</td>
</tr>
<tr>
<td style="text-align: left;">XLM-RoBERTa base</td>
<td style="text-align: center;">55.88</td>
<td style="text-align: center;">45.20</td>
<td style="text-align: center;">97.64</td>
<td style="text-align: center;">91.74</td>
</tr>
<tr>
<td style="text-align: left;">XLM-RoBERTa large</td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">65.45</td>
<td style="text-align: center;">99.95</td>
<td style="text-align: center;">91.74</td>
</tr>
<tr>
<td style="text-align: left;">XLNet base</td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;">55.00</td>
<td style="text-align: center;">99.19</td>
<td style="text-align: center;">94.28</td>
</tr>
<tr>
<td style="text-align: left;">XLNet large</td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;">71.40</td>
<td style="text-align: center;">99.86</td>
<td style="text-align: center;">92.94</td>
</tr>
<tr>
<td style="text-align: left;">Largest class</td>
<td style="text-align: center;">35.29</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.02</td>
<td style="text-align: center;">50.03</td>
</tr>
</tbody>
</table>
<p>weaker performance observed in these two datasets was expected, given their greater language variability and broader logical scope; and in the case of FOLIO, its smaller size as well. Overall, the encoder-only transformer models worked relatively well as soft reasoners [4], being able to successfully deduce theorems from premises expressed in natural language. Noteworthy exceptions were the performances of AlBERT-XL in FOLIO, DeBERTa-XL and -XXL in LogicNLI and SimpleLogic, DeBERTa-XXL in RuleTaker, and the overall lower performance of DistilBERT.</p>
<h1>5 Cross-Probing Fine-Tuned LMs</h1>
<p>The encoder-only transformer models showed reasonable performance on the hypothesis classification task, where they were fine-tuned on the logical reasoning datasets. This, however, raises some questions: has the ability to solve this task, whatever it is, been acquired during the fine-tuning stage, or was it present from the start (i.e., from pretraining)? Most importantly, have LMs truly developed a generalized logical reasoning capability? To examine these questions, we run a cross-probing task: we take</p>
<p>Table 3. Results for the cross-probing task. On the left, we present the best fine-tuned RoBERTalarge models for each dataset. The datasets used in the probes are listed at the top. We report only the best result for each probe. Blue cells indicate the probe of a fine-tuned model on the same dataset. In parentheses, we denote the percentage difference from the pretrained model. "Largest class" refers to the accuracy achieved by always selecting the class with the highest frequency in the training set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">FOLIO</th>
<th style="text-align: center;">LogicNLI</th>
<th style="text-align: center;">RuleTaker</th>
<th style="text-align: center;">SimpleLogic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Pretrained</td>
<td style="text-align: center;">32.88</td>
<td style="text-align: center;">25.54</td>
<td style="text-align: center;">50.01</td>
<td style="text-align: center;">61.19</td>
</tr>
<tr>
<td style="text-align: center;">FOLIO</td>
<td style="text-align: center;">$55.05(+67.42)$</td>
<td style="text-align: center;">$27.28(+6.81)$</td>
<td style="text-align: center;">$60.74(+21.45)$</td>
<td style="text-align: center;">$62.82(+2.66)$</td>
</tr>
<tr>
<td style="text-align: center;">LogicNLI</td>
<td style="text-align: center;">$36.60(+11.31)$</td>
<td style="text-align: center;">$67.95(+166.05)$</td>
<td style="text-align: center;">$69.37(+38.70)$</td>
<td style="text-align: center;">$62.06(+1.42)$</td>
</tr>
<tr>
<td style="text-align: center;">RuleTaker</td>
<td style="text-align: center;">$40.32(+22.62)$</td>
<td style="text-align: center;">$36.01(+40.99)$</td>
<td style="text-align: center;">$99.44(+98.84)$</td>
<td style="text-align: center;">$62.89(+2.77)$</td>
</tr>
<tr>
<td style="text-align: center;">SimpleLogic</td>
<td style="text-align: center;">$32.88(0)$</td>
<td style="text-align: center;">$25.44(-0.39)$</td>
<td style="text-align: center;">$51.02(+2.01)$</td>
<td style="text-align: center;">$92.35(+50.92)$</td>
</tr>
<tr>
<td style="text-align: center;">Largest class</td>
<td style="text-align: center;">35.29</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.02</td>
<td style="text-align: center;">50.03</td>
</tr>
</tbody>
</table>
<p>the LMs previously fine-tuned on our logical datasets, as well as a pretrained LM, and probe them on these same datasets. Given the large number of possible tests, we restricted our investigation to a single model, RoBERTa-large, so as to dig deeper on it. This LM demonstrated a suitable balance between performance, consistency among datasets, and training time in the previous tests.</p>
<p>To start, we took the best fine-tuned RoBERTa-large model for each dataset and removed their classification heads, leaving just the transformer blocks, as in a pretrained model. Then, we attached a new classification head to it; i.e., the probe. As in the finetuning stage, we passed the formatted inputs in natural language to the LMs and tried to predict the correct label for a set of premises. However, unlike the fine-tuning stage, only the probe is updated now, while the model's body is kept frozen during the backward pass. The goal is to assess if some logical reasoning ability was learned by the LM without letting the model adapt to the task.</p>
<p>The same training policies from the fine-tuning step were followed in this stage: models were trained with early stopping for up to 50 epochs (patience of 5 epochs) based on the validation loss, using two learning rates (1e-5 and 1e-6). Also, two different classifiers were tested as probes:</p>
<ul>
<li>1-layer A single affine transformation is applied to the embedding of the [CLS] token. The classification head has shape (1024, labels); 1024 being the dimensionality of RoBERTa-large hidden states. We used a dropout of 0.5 before the classifier.</li>
<li>3-layer The [CLS] embedding passes through three consecutive layers of shape (1024, 256), (256, 64), and (256, labels), respectively. We used a dropout of 0.5 in between linear layers and ReLU as the activation function.</li>
</ul>
<p>In the tests, the two probes led to similar results. We take this as strong evidence that the knowledge used in the logical reasoning tasks, whatever it is, can be linearly recovered from the internal representations of RoBERTa-large.</p>
<p>Table 3 displays the best results attained in the cross-probing task. The left column lists the RoBERTa-large fine-tuned models from the previous experiment, while the remaining columns represent the datasets they were probed against. The blue cells along the diagonal indicate instances where models were probed on the same datasets</p>
<p>they were initially fine-tuned on. Percentage differences in accuracy relative to the pretrained case are reported in parentheses. As expected, the fine-tuned model for a specific dataset performed better in that same dataset, albeit less than in the fine-tuning scenario. This makes sense, since in fine-tuning, both the model's body and head are optimized, whereas in a probe the head is in charge of all the learning. These results serve as a sanity check that our probing is working.</p>
<p>The first row in Table 3 contains the scores for the probes with pretrained RoBERTa. Accuracy levels for LogicNLI and RuleTaker closely resemble the largest class baseline, while the result for FOLIO falls below its corresponding baseline. Pretrained RoBERTa only helped to solve SimpleLogic, a dataset constrained to conjunctive implication with minimal language variation. Its pretraining scheme, dynamic masked language modeling, did not equip it with adequate logic-like knowledge to solve complex reasoning problems without specific training.</p>
<p>We can see from this that pretrained RoBERTa appears to have no proper logical reasoning skills. But has it acquired such an ability through fine-tuning on logical datasets? After all, RoBERTa-large was able to solve the hypothesis classification tasks reasonably well after specific training. When analyzing the results of the cross-probing task, however, we may doubt whether a general logical reasoning ability in fact emerged from fine-tuning.</p>
<p>In general, the fine-tuned LMs showed limited transferability when probed on different datasets. Although some gain was achieved compared to the pretrained model, they remained well below what an LM fine-tuned on the same dataset could obtain. SimpleLogic presents an interesting case. The LMs fine-tuned on the other datasets performed similarly to the pretrained model on this dataset. This is despite SimpleLogic covering only a subset of propositional calculus, a domain included in those datasets. One would expect that a model capable of solving more complex problems would be able to reason on this simpler dataset (in terms of logical scope and linguistic variability). At the same time, the LM fine-tuned on SimpleLogic did not exhibit improved performance on the other datasets, indicating a lack of acquired general logical reasoning ability during its training.</p>
<p>Two main conclusions can be drawn from this experiment:</p>
<ol>
<li>If the difference in accuracy between pretrained RoBERTa and the largest class baseline indicates the amount of logical reasoning contained in this LM, then pretrained RoBERTa seems to have very limited or no logical reasoning abilities.</li>
<li>If the difference in accuracy between fine-tuned LMs (when applied to different datasets) and the pretrained RoBERTa model indicates the amount of logical reasoning they acquired in the fine-tuning process, then these LMs have acquired little or no general logical reasoning capability as well, suggesting that they mostly learned statistical features of the datasets. This aligns with other findings for transformer models $[55,12]$.</li>
</ol>
<h1>6 Inspecting Fine-Tuned Models Layerwise</h1>
<p>We now address another question: which parts of the LMs are more capable of solving logical reasoning tasks? To answer this, we probe the different layers of the fine-tuned</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 2. RoBERTa-large models fine-tuned on FOLIO, LogicNLI, RuleTaker, and SimpleLogic, and probed for the same datasets layerwise. The pretrained baselines are indicated by gray lines, while the values achieved in the cross-probing task are represented by black lines. The colored bars indicate the change in accuracy from the previous layers. Probing was performed with a 1-layer classifier and a learning rate of 1e-6.</p>
<p>RoBERTa-large models using the same datasets they were trained on. Our goal is to identify which layers are more effective in deducing hypotheses. We expect this to provide further evidence of what sort of knowledge LMs are using to solve the hypothesis classification task. Similar to the previous experiment, the fine-tuned LMs were frozen, the older classification head was removed, and a probe was trained on top of the layers. More concretely, for each layer $i$ in the model, we passed the premises through the model up to layer $i$, and used the outputted embedding of the [CLS] token at that layer as the input to the probe. As in the previous experiment, only the probe was trained. The same two classifiers from the last experiment were tested. They were positioned on top of the [CLS] token of each layer, with 25 layers in total ( 24 transformer blocks plus the initial embedding layer). We adopted the same configurations from the previous tests: models were trained with early stopping for up to 50 epochs (with a patience of 5 epochs) based on the validation loss, using two learning rates (1e-5 and 1e-6).</p>
<p>Figure 2 shows the accuracy for the probes stacked on the various layers of the finetuned LMs, using a 1-layer classifier and a learning rate of 1e-6 (Appendix C presents graphs for the other configurations). The blue line plots the accuracy on the task for each</p>
<p>layer, while the bars display the differential score for layer ${ }<em i="i">{i}$; i.e., the change in accuracy from layer ${ }</em>$ [45]. The gray line marks the pretrained model baseline, and the black line indicates the score achieved in the cross-probing task.}$ to layer ${ }_{i-1</p>
<p>A similar behavior is exhibited by all models. They remain close to the pretrained baseline in the low and mid layers. Accuracy then grows rapidly in the final layers, achieving a performance equal to the cross-probing baseline. How does this compare to other types of knowledge found in transformer models? Literature on transformers shows that surface information, such as sentence length [23], is mostly captured by lower layers. Middle layers are responsible for processing syntactic knowledge, like syntax trees [21]. Finally, higher layers are responsible for task-specific functions [18] and contextual representations [13].</p>
<p>In our layerwise probing, higher layers were the only ones able to solve the hypothesis classification task better than a pretrained model. This suggests that the knowledge acquired during fine-tuning was connected to dataset-specific features rather than general representations. It also explains why the information was not transferable among datasets. Although indirect, this experiment provides further evidence that encoder-only transformer models do not possess robust logical reasoning capabilities. SimpleLogic was the only case that presented a growth in the initial layers. This behavior may indicate that the dataset is solvable through the use of some heuristics based on shallow statistical features, such as the number of premises, as discussed by [55].</p>
<h1>7 Conclusion</h1>
<p>Logical reasoning is a valuable ability that humans use in thinking, arguing, and planning, as well as a core component of many AI systems. Here, we investigated the role of logical reasoning in encoder-only transformer models. By gathering a number of logical reasoning datasets, we observed that language models can be trained to perform complex logical tasks with relative success. However, upon closer inspection, doubts arose regarding whether these models have truly learned to reason according to logical rules. First, by probing a pretrained RoBERTa-large model with logical reasoning datasets, it became apparent that this language model did not possess intrinsic logical reasoning abilities. Second, models fine-tuned on one dataset struggled to generalize well to other datasets, even within the same domain. This observation suggests that these language models did not acquire robust logical reasoning capabilities even after specific training. Third, the knowledge necessary to solve logical reasoning tasks seems to emerge primarily at higher, more contextual layers, probably linked to statistical features of the datasets rather than deeper representations.</p>
<h2>8 Limitations</h2>
<p>We run experiments for a large variety of encoder-only transformer models in Section 4. However, due to space and time constraints, we focused on RoBERTa-large for the analysis in Sections 5-6. While we expect the same behavior to appear in the other encode-only models, further tests are needed to verify whether conclusions can be reliably extended to them. We have not explored decoder nor encoder-decoder models</p>
<p>either, which could widely extend the number of models to be tested. We cannot rule out the possibility that robust logical reasoning is an emergent ability only manifested in large language models [50]. Additionally, other types of representations, such as attentions and feedforward computations, could be analyzed in relation to logical reasoning. Further work should also focus on other types of logical formalism beyond PC and FOL.</p>
<h1>Acknowledgements</h1>
<p>This work was supported by the Center for Artificial Intelligence USP/IBM/FAPESP (C4AI), jointly funded by the São Paulo Research Foundation (FAPESP grant 2019/076654) and by the IBM Corporation. Research by Marcos José has been carried out with support by Itaú Unibanco S.A. through the scholarship program Programa de Bolsas Itaú (PBI) ; Fabio Cozman was partially supported by CNPq grants 312180/2018-7 and 305753/2022-3. We acknowledge support also by CAPES - Finance Code 001.</p>
<h2>References</h2>
<ol>
<li>Belinkov, Y.: Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics 48(1), 207-219 (Mar 2022). https://doi.org/10.1162/coli_ a_00422, https://aclanthology.org/2022.cl-1.7</li>
<li>Beltagy, I., Peters, M.E., Cohan, A.: Longformer: The Long-Document Transformer. CoRR abs/2004.05150 (2020), https://arxiv.org/abs/2004.05150</li>
<li>Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457 (2018)</li>
<li>Clark, P., Tafjord, O., Richardson, K.: Transformers as Soft Reasoners over Language. arXiv preprint arXiv:2002.05867 (2020)</li>
<li>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., Schulman, J.: Training Verifiers to Solve Math Word Problems (2021). https://doi.org/10.48550/ARXIV.2110.14168, https://arxiv.org/abs/2110.14168</li>
<li>Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., Stoyanov, V.: Unsupervised cross-lingual representation learning at scale (2020)</li>
<li>Conneau, A., Kruszewski, G., Lample, G., Barrault, L., Baroni, M.: What you Can Cram into a Single \$\&amp;!#* Vector: Probing Sentence Embeddings for Linguistic Properties. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2126-2136. Association for Computational Linguistics, Melbourne, Australia (Jul 2018). https://doi.org/10.18653/v1/P18-1198, https:// aclanthology.org/P18-1198</li>
<li>Dai, D., Dong, L., Hao, Y., Sui, Z., Wei, F.: Knowledge Neurons in Pretrained Transformers. CoRR abs/2104.08696 (2021), https://arxiv.org/abs/2104.08696</li>
<li>Dankers, V., Bruni, E., Hupkes, D.: The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 41544175. Association for Computational Linguistics, Dublin, Ireland (May 2022). https:</li>
</ol>
<p>//doi.org/10.18653/v1/2022.acl-long.286, https://aclanthology. org/2022.acl-long. 286
10. Dasgupta, I., Lampinen, A.K., Chan, S.C.Y., Creswell, A., Kumaran, D., McClelland, J.L., Hill, F.: Language Models Show Human-Like Content Effects on Reasoning (2022). https://doi.org/10.48550/ARXIV.2207.07051, https://arxiv. org/abs/2207.07051
11. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. CoRR abs/1810.04805 (2018), http: //arxiv.org/abs/1810.04805
12. Dziri, N., Lu, X., Sclar, M., Li, X.L., Jiang, L., Lin, B.Y., West, P., Bhagavatula, C., Bras, R.L., Hwang, J.D., Sanyal, S., Welleck, S., Ren, X., Ettinger, A., Harchaoui, Z., Choi, Y.: Faith and Fate: Limits of Transformers on Compositionality (2023)
13. Ethayarajh, K.: How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. CoRR abs/1909.00512 (2019), http : //arxiv.org/abs/1909.00512
14. Gaskell, A., Miao, Y., Specia, L., Toni, F.: Logically Consistent Adversarial Attacks for Soft Theorem Provers (2022)
15. Gurnee, W., Tegmark, M.: Language models represent space and time (2023)
16. Hahn, C., Schmitt, F., Kreber, J.U., Rabe, M.N., Finkbeiner, B.: Teaching Temporal Logics to Neural Networks (2020). https://doi.org/10.48550/ARXIV.2003.04218, https://arxiv.org/abs/2003.04218
17. Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., Zubova, E., Qiao, Y., Burtell, M., et al.: FOLIO: Natural Language Reasoning with First-Order Logic. arXiv preprint arXiv:2209.00840 (2022)
18. Hao, Y., Dong, L., Wei, F., Xu, K.: Visualizing and Understanding the Effectiveness of BERT. CoRR abs/1908.05620 (2019), http://arxiv.org/abs/1908.05620
19. He, P., Liu, X., Gao, J., Chen, W.: DeBERTa: Decoding-enhanced BERT with Disentangled Attention. CoRR abs/2006.03654 (2020), https://arxiv.org/abs/2006.03654
20. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., Steinhardt, J.: Measuring Mathematical Problem Solving with the Math Dataset. arXiv preprint arXiv:2103.03874 (2021)
21. Hewitt, J., Manning, C.D.: A Structural Probe for Finding Syntax in Word Representations. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4129-4138. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1419, https: //aclanthology.org/N19-1419
22. Hossain, M.M., Kovatchev, V., Dutta, P., Kao, T., Wei, E., Blanco, E.: An Analysis of Natural Language Inference Benchmarks through the Lens of Negation. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 91069118. Association for Computational Linguistics, Online (Nov 2020). https://doi. org/10.18653/v1/2020.emnlp-main.732, https://aclanthology.org/ 2020.emnlp-main. 732
23. Jawahar, G., Sagot, B., Seddah, D.: What Does BERT Learn about the Structure of Language? In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3651-3657. Association for Computational Linguistics, Florence, Italy (Jul 2019). https://doi.org/10.18653/v1/P19-1356, https:// aclanthology.org/P19-1356
24. Kalouli, A.L., Sevastjanova, R., Beck, C., Romero, M.: Negation, Coordination, and Quantifiers in Contextualized Language Models. In: Proceedings of the 29th International Confer-</p>
<p>ence on Computational Linguistics. pp. 3074-3085. International Committee on Computational Linguistics, Gyeongju, Republic of Korea (Oct 2022), https://aclanthology. org/2022.coling-1.272
25. Kassner, N., Schütze, H.: Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7811-7818. Association for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.698, https://aclanthology.org/2020.acl-main. 698
26. Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.: RACE: Large-scale ReAding Comprehension Dataset From Examinations (2017). https://doi.org/10.48550/ARXIV.1704. 04683, https://arxiv.org/abs/1704.04683
27. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 (2019)
28. Liu, E., Neubig, G.: Are Representations Built from the Ground Up? An Empirical Examination of Local Composition in Language Models (2022). https://doi.org/10. 48550/ARXIV.2210.03575, https://arxiv.org/abs/2210.03575
29. Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., Zhang, Y.: LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning (2020). https://doi.org/ 10.48550/ARXIV.2007.08124,https://arxiv.org/abs/2007.08124
30. Liu, L.Z., Wang, Y., Kasai, J., Hajishirzi, H., Smith, N.A.: Probing Across Time: What Does RoBERTa Know and When? CoRR abs/2104.07885 (2021), https://arxiv. org/abs/2104.07885
31. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692 (2019)
32. Manning, C.D., Clark, K., Hewitt, J., Khandelwal, U., Levy, O.: Emergent Linguistic Structure in Artificial Neural Networks Trained by Self-Supervision. Proceedings of the National Academy of Sciences 117(48), 30046-30054 (2020). https://doi.org/ 10.1073/pnas.1907367117, https://www.pnas.org/doi/abs/10.1073/ pnas. 1907367117
33. O’Neill, J., Rozenshtein, P., Kiryo, R., Kubota, M., Bollegala, D.: I Wish I Would Have Loved This One, But I Didn't-A Multilingual Dataset for Counterfactual Detection in Product Reviews. arXiv preprint arXiv:2104.06893 (2021)
34. Patel, A., Bhattamishra, S., Goyal, N.: Are NLP Models Really Able to Solve Simple Math Word Problems? (2021). https://doi.org/10.48550/ARXIV.2103. 07191, https://arxiv.org/abs/2103.07191
35. Qin, L., Bosselut, A., Holtzman, A., Bhagavatula, C., Clark, E., Choi, Y.: Counterfactual Story Reasoning and Generation (2019). https://doi.org/10.48550/ARXIV. 1909.04076, https://arxiv.org/abs/1909.04076
36. Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H.F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L.A., Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.M., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X.L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de Masson d'Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M.J., Hechtman, B.A., Weidinger, L., Gabriel, I.,</p>
<p>Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., Irving, G.: Scaling Language Models: Methods, Analysis \&amp; Insights from Training Gopher. CoRR abs/2112.11446 (2021), https://arxiv.org/abs/2112.11446
37. Rogers, A., Kovaleva, O., Rumshisky, A.: A Primer in BERTology: What We Know About How BERT Works. Transactions of the Association for Computational Linguistics 8, 842-866 (2020). https://doi.org/10.1162/tacl_a_00349, https:// aclanthology.org/2020.tacl-1.54
38. Russell, S., Norvig, P.: Artificial Intelligence: A Modern Approach. Prentice Hall, 3 edn. (2010)
39. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter (2020)
40. Saparov, A., He, H.: Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought (2023)
41. Sevastjanova, R., Kalouli, A.L., Beck, C., Schäfer, H., El-Assady, M.: Explaining Contextualization in Language Models using Visual Analytics. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 464-476. Association for Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.39, https:// aclanthology.org/2021.acl-long. 39
42. Sinha, K., Sodhani, S., Dong, J., Pineau, J., Hamilton, W.L.: CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4506-4515. Association for Computational Linguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/ v1/D19-1458, https://aclanthology.org/D19-1458
43. Talmor, A., Herzig, J., Lourie, N., Berant, J.: CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4149-4158. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/ 10.18653/v1/N19-1421, https://aclanthology.org/N19-1421
44. Tang, X., Zheng, Z., Li, J., Meng, F., Zhu, S.C., Liang, Y., Zhang, M.: Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners (2023)
45. Tenney, I., Das, D., Pavlick, E.: BERT Rediscovers the Classical NLP Pipeline. CoRR abs/1905.05950 (2019), http://arxiv.org/abs/1905.05950
46. Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R.T., Kim, N., Durme, B.V., Bowman, S.R., Das, D., Pavlick, E.: What do you Learn from Context? Probing for Sentence Structure in Contextualized Word Representations. CoRR abs/1905.06316 (2019), http : //arxiv.org/abs/1905.06316
47. Tian, J., Li, Y., Chen, W., Xiao, L., He, H., Jin, Y.: Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 3738-3747. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic (Nov 2021). https://doi. org/10.18653/v1/2021.emnlp-main.303, https://aclanthology.org/ 2021.emnlp-main. 303
48. Valmeekam, K., Olmo, A., Sreedharan, S., Kambhampati, S.: Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) (2023)</p>
<ol>
<li>Warstadt, A., Parrish, A., Liu, H., Mohananey, A., Peng, W., Wang, S.F., Bowman, S.R.: BLiMP: The Benchmark of Linguistic Minimal Pairs for English. Transactions of the Association for Computational Linguistics 8, 377-392 (2020). https://doi.org/10.1162/ tacl_a_00321, https://aclanthology.org/2020.tacl-1.25</li>
<li>Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent Abilities of Large Language Models (2022). https://doi.org/10. 48550/ARXIV.2206.07682, https://arxiv.org/abs/2206.07682</li>
<li>Wu, Z., Peng, H., Smith, N.A.: Infusing Finetuning with Semantic Dependencies. Transactions of the Association for Computational Linguistics 9, 226-242 (2021). https: //doi.org/10.1162/tacl_a_00363</li>
<li>Yang, X., Obadinma, S., Zhao, H., Zhang, Q., Matwin, S., Zhu, X.: SemEval-2020 Task 5: Counterfactual Recognition. arXiv preprint arXiv:2008.00563 (2020)</li>
<li>Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q.V.: Xlnet: Generalized autoregressive pretraining for language understanding (2020)</li>
<li>Yu, W., Jiang, Z., Dong, Y., Feng, J.: ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning (2020). https://doi.org/10.48550/ARXIV.2002. 04326, https://arxiv.org/abs/2002.04326</li>
<li>Zhang, H., Li, L.H., Meng, T., Chang, K., den Broeck, G.V.: On the Paradox of Learning to Reason from Data. CoRR abs/2205.11502 (2022). https://doi.org/10.48550/ arXiv.2205.11502, https://doi.org/10.48550/arXiv.2205.11502</li>
<li>Zhang, Y., Warstadt, A., Li, X., Bowman, S.R.: When Do You Need Billions of Words of Pretraining Data? In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 1112-1125. Association for Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.90, https://aclanthology.org/2021.acl-long. 90</li>
</ol>
<h1>A Datasets</h1>
<p>In this appendix, we describe the four logical reasoning datasets in more detail. Table 4 indicates the sources from where they were obtained.</p>
<p>FOLIO [17] is a human-annotated dataset for FOL reasoning problems. Logicallysound contexts were generated in two ways: in the first, annotators created contexts from scratch, based on random Wikipedia pages; in the second, a template of nested syllogisms was used, from which annotators then replaced the abstract entities and categories by nouns, phrases or clauses, as to make the text to reflect real-life scenarios. Next, the authors verified the alignment between natural language sentences and FOL formulas and added implicit commonsense knowledge as premises. After that, they verified the syntactic validity and label consistency of FOL formula annotations with a FOL prover. Finally, sentences were reviewed for grammar issues and language fluency. Only train and validation tests are available, so we used the latter for reporting tests. Hypotheses can be True, False, or Unknown.</p>
<p>LogicNLI [47] is a FOL dataset created through a semi-automatic method. A set of logical templates was defined and then filled by subjects and predicates sampled from predefined sets. Next, manual edits were made to correct grammatical errors and resolve semantic ambiguities. Hypothesis are classified as Entailment, Contradiction, Neutral, and Paradox. A paradox is defined as a situation where both a sentence and its contrary can be inferred from the premises. We used the standard version of the dataset, which encompasses all labels.</p>
<p>RuleTaker [4] is a logical reasoning dataset in which rules are conjunctive implications. Predicates may be negated and facts may be either attributes (which assign properties to entities) or relations (which relate two entities). We used the ParaRules version, where rules and facts were paraphrased by crowdworkers into more natural language; paraphrased constructions were then combined to form new templates. We also used the updated version of RuleTaker (problog), which eliminated some world model inconsistencies. Hypothesis can be True, when a hypothesis follows from the premises, and False otherwise (closed-world assumption, CWA).</p>
<p>SimpleLogic [55] is similar to RuleTaker, only supporting conjunctive implication (facts are simply conjunctive implications with empty antecedents). Language variance is virtually removed by using a fixed template for translating FOL into natural language and by the use of a small random list of words as predicates. Argumentative complexity is limited by setting thresholds for input length, number of predicates, and reasoning depth. We reconstructed the original template based on the examples given in the paper. For our tests, we used the RP Balanced version. We undersampled the largest class (True) to obtain the same number of observations as for the False class. Labels can be True and False (CWA).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FOLIO</td>
<td style="text-align: left;">https://github.com/Yale-LILY/FOLIO</td>
</tr>
<tr>
<td style="text-align: left;">LogicNLI</td>
<td style="text-align: left;">https://github.com/omnilabNL/LogicNLI</td>
</tr>
<tr>
<td style="text-align: left;">RuleTaker</td>
<td style="text-align: left;">https://allenai.org/data/ruletaker</td>
</tr>
<tr>
<td style="text-align: left;">SimpleLogic</td>
<td style="text-align: left;">https://github.com/joshuacnf/paradox-learning2reason</td>
</tr>
</tbody>
</table>
<p>Table 4. Sources for the datasets.</p>
<h1>B Label distribution</h1>
<p>Table 5 shows the label distribution for the datasets used in the paper. The row above provides the number of observations per label, and the row below shows their relative percentage. Labels: FOLIO: False, True, Unknown. LogicNLI: Contradiction, Entailment, Neutral, Paradox. RuleTaker: False, True. SimpleLogic: False, True.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Validation</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FOLIO</td>
<td style="text-align: left;">$286 / 388 / 329$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$63 / 72 / 69$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(28.51 \% / 38.68 \% / 32.80 \%)$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$(30.88 \% / 35.29 \% / 33.82 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LogicNLI</td>
<td style="text-align: left;">$4000 / 4000 / 4000 / 4000$</td>
<td style="text-align: left;">$500 / 500 / 500 / 500$</td>
<td style="text-align: left;">$500 / 500 / 500 / 500$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(25 \%$ each $)$</td>
<td style="text-align: left;">$(25 \%$ each $)$</td>
<td style="text-align: left;">$(25 \%$ each $)$</td>
</tr>
<tr>
<td style="text-align: left;">RuleTaker</td>
<td style="text-align: left;">$13666 / 13697$</td>
<td style="text-align: left;">$1946 / 1953$</td>
<td style="text-align: left;">$3895 / 3898$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(49.94 \% / 50.05 \%)$</td>
<td style="text-align: left;">$(49.91 \% / 50.09 \%)$</td>
<td style="text-align: left;">$(49.98 \% / 50.02 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">SimpleLogic</td>
<td style="text-align: left;">$5696 / 5645$</td>
<td style="text-align: left;">$683 / 735$</td>
<td style="text-align: left;">$709 / 708$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(50.22 \% / 49.77 \%)$</td>
<td style="text-align: left;">$(48.16 \% / 51.83 \%)$</td>
<td style="text-align: left;">$(50.03 \% / 49.96 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 5. Label distribution for logical reasoning datasets.</p>
<h2>C Laywerwise probing</h2>
<p>For the layerwise probing (Sec. 6), we tested two different classifiers (1-layer and 3layer) and two learning rates (1e-6 and 1e-5). Figure 2 above displayed the results for the 1-linear classifier and 1e-6 learning rate. The figures below display the results for the other probes. Figure 3 provides the graphs for the 3-layer classifier and learning rate of 1e-6; Figure 4 provides the graphs for the 1-layer classifier and learning rate of 1e-5; and Figure 5 provides the graphs for the 3-layer classifier and learning rate of 1e-5.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3. RoBERTa-large models fine-tuned on FOLIO, LogicNLI, RuleTaker, and SimpleLogic, and probed for the same datasets layerwise. The pretrained baselines are indicated by gray lines, while the values achieved in the cross-probing task are represented by black lines. The colored bars indicate the change in accuracy from the previous layers. Probing was performed with a 3-layer classifier and a learning rate of 1e-6.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4. RoBERTa-large models fine-tuned on FOLIO, LogicNLI, RuleTaker, and SimpleLogic, and probed for the same datasets layerwise. The pretrained baselines are indicated by gray lines, while the values achieved in the cross-probing task are represented by black lines. The colored bars indicate the change in accuracy from the previous layers. Probing was performed with a 1-layer classifier and a learning rate of $1 \mathrm{e}-5$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. RoBERTa-large models fine-tuned on FOLIO, LogicNLI, RuleTaker, and SimpleLogic, and probed for the same datasets layerwise. The pretrained baselines are indicated by gray lines, while the values achieved in the cross-probing task are represented by black lines. The colored bars indicate the change in accuracy from the previous layers. Probing was performed with a 3-layer classifier and a learning rate of $1 \mathrm{e}-5$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Conjunctive implications are arguments of the form (fact.[^fact.]"rule.[^rule.]" $\Rightarrow$ hypothesis).
${ }^{6}$ We opted to explore encoder-only models because this type of architecture is well-suited for classification tasks. These models have access to the whole input sequence and are typically trained on discriminative tasks, such as masked language modeling.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-202 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-202</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-202</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-238deab37e201c57505a4a47bb854e462af79bd7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/238deab37e201c57505a4a47bb854e462af79bd7" target="_blank">Entity-Based Knowledge Conflicts in Question Answering</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> The findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that the mitigation strategy encourages generalization to evolving information (i.e. time-dependent queries).</p>
                <p><strong>Paper Abstract:</strong> Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4% - 7%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e. time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e202.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e202.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 (generative reader)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 generative reader (retrieve-and-generate question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative retrieve-and-read reader (T5) finetuned for open-domain QA; used to measure how models integrate contextual passages (retrieved or gold) versus parametric memory using entity-substitution conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>60M, 770M, 3B, 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain question answering (retrieve-and-generate reader) where the model must produce an answer given a query and one retrieved/gold passage.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Contextual passages (retrieved or gold passages containing factual statements; sometimes deliberately contradictory via entity substitution).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Retrieved documents (DPR or TF-IDF) and synthetic substituted documents produced by the paper's substitution framework (Wikidata/dataset-sourced entity replacements).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory (experiments purposely replace gold entities so context disagrees with learned/fine-tuned answers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>When the original (unmodified) passage/context supports the original answer, model often relies on parametric knowledge: on some corpus-substituted examples the model still predicts the original (memorized) answer up to ~20% (NQ) and up to ~75% (NewsQA) in inference on substituted instances; model confidence is higher on original example than substituted in ~74% of cases (Table 2 aggregate).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Given substituted (contradictory) passages the model often (a) predicts the Substitute answer less than 50% of the time on many sets, (b) increases predictions categorized as 'Other' frequently, and (c) is less confident on substituted inputs: p(original) > p(substituted) in ~74% of in-domain cases; targeted augmentation (mixed training) reduced memorization ratio from 29.5%→2.6% (NQ Train) and improved OOD EM (NQ Dev NAO +7.1 percentage points, NewsQA +4.4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed: adding (contradictory) contextual evidence often decreased model confidence and sometimes failed to override parametric memory (negative effect on correctness), producing more uncertain/'Other' answers; however, using such contradictory evidence as training augmentation had a positive effect (reduced memorization and improved OOD generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Overreliance on parametric memorization (hallucination/parroting) causes the reader to prefer learned answers over contextual evidence; this tendency is amplified by larger model size, poor retrieval quality during training, and entity popularity/type biases. Good retrieval and exposure to conflicting contexts at training force the model to ground in context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative T5 readers frequently ignore contradictory contextual evidence and revert to memorized answers (measured by Memorization Ratio), especially for larger models and when trained with imperfect retrieval; contradictory evidence increases uncertainty and 'Other' responses. Training with corpus-substituted examples dramatically reduces memorization and improves out-of-distribution accuracy, at a small possible cost on in-domain train EM.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity-Based Knowledge Conflicts in Question Answering', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e202.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e202.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extractive reader (RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-based span-extraction QA reader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extractive span-selection reader (RoBERTa implementation) that predicts answers by selecting spans from the provided passage; evaluated on the same substitution conflicts to compare grounding vs generative behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Roberta: A robustly optimized bert pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (extractive QA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extractive question answering (span selection) using a single passage input.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Contextual passages (gold passages, including substituted versions) provided as evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Gold passages with entity substitutions (synthetic) used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory (substitution changes gold span in passage, so evidence disagrees with parametric/fine-tuned answer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Extractive models are trained to rely on passages (gold spans) and thus exhibit very low memorization when provided correct gold passages; baseline memorization ratio is negligible on many sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>On substituted passages, memorization ratio is negligible (model generally selects a span) but the rate of 'Other' predictions (incorrect/random spans or hallucinations) is non-trivial (≥15% overall; up to 27% on NewsQA OOD), indicating increased uncertainty and unstable predictions when context contradicts learned priors or substitutions are implausible.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed: providing contextual evidence forces grounding (positive — low memorization), but knowledge conflicts still increase uncertainty and lead to more incorrect 'Other' span predictions (negative).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Extractive architecture constrains outputs to passage spans so it resists parroting memorized answers, but when passages contain implausible substitutions the model is uncertain and often selects wrong spans or hallucinates, reflecting elevated uncertainty rather than parametric override.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Extractive readers show far lower memorization than generative readers, but knowledge conflicts still produce increased uncertainty and a notable fraction of wrong span selections; thus grounding reduces but does not eliminate adverse effects of contradictory evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity-Based Knowledge Conflicts in Question Answering', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e202.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e202.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorization Ratio (MR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorization Ratio</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric introduced in this paper defined as MR = p_o / (p_o + p_s), where p_o is probability of predicting the Original (memorized) answer and p_s is probability of predicting the Substitute answer on the modified instance; quantifies model overreliance on parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diagnostic metric for measuring preference for parametric memory versus contextual evidence in QA under entity-substitution conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>N/A (metric used to analyze responses to contextual evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>Designed to measure behaviour when evidence contradicts parametric knowledge (i.e., in contradictory alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>N/A (metric itself), but reported example values: before mitigation MR on NQ Train = 29.5%, NQ Dev (AO) = 27.1%, NQ Dev (NAO) = 1.5%, NewsQA = 9.3%. MR increases with model size (from <15% to ≥50% across sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>After mixed training with corpus substitutions MR fell to negligible levels: NQ Train 2.6%, NQ Dev (AO) 1.9%, NQ Dev (NAO) 0.0%, NewsQA 0.6% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>N/A (metric captures effect of evidence); used to show that contextual evidence often fails to override memorized answers unless training is adjusted.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>MR operationalizes overstability/overreliance on parametric knowledge, enabling quantification of how often models ignore contextual evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MR is an effective, quantitative diagnostic: higher MR indicates stronger memorization/parroting; MR correlates positively with model size and with poorer retriever quality, and drops markedly when models are trained with substituted (conflicting) contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity-Based Knowledge Conflicts in Question Answering', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e202.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e202.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Corpus Substitution (CS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Corpus Substitution (in-domain entity replacement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A substitution policy that replaces an answer entity a with another in-domain answer a' from the same dataset (same entity type), producing a coherent but knowledge-conflicting passage used to test how evidence affects model responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 / RoBERTa (used as evaluation probes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic creation of contradictory contextual evidence for QA by swapping entities with other same-type entities from the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Contradictory contextual passages (factual statements altered to assert a different entity).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Synthetic/generated from dataset gold answers (in-domain sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory by design (substitution changes correct answer relative to training/fine-tuned data).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>On original, unmodified passages the model answered correctly (examples selected where original answered correctly).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>When presented with CS passages at inference the generative reader predicted the Substitute answer rarely >50% (NQ), reverted to Original up to 20% (NQ) / 75% (NewsQA), and frequently produced Other answers; model confidence was higher on original than substituted example ~74% of the time for in-domain examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative/mixed: adding contradictory in-domain evidence often decreased model confidence and sometimes failed to change predicted answer (negative), but using these substitutions as training augmentation reduced memorization (positive).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Corpus substitution creates explicit conflicts; results show models often overweight parametric priors learned at training and underweight contextual evidence unless trained to attend to such conflicts or given high-quality retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Corpus substitution exposes that generative QA models commonly ignore contextual evidence and hallucinate memorized answers or produce unstable 'Other' outputs; however, using CS as training augmentation effectively teaches models to prioritize context and reduces MR drastically.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity-Based Knowledge Conflicts in Question Answering', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e202.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e202.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixed training with substitutions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixed Training with Corpus Substitutions (augmentation mitigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mitigation strategy that augments training data with corpus-substituted examples (25% augmentation in DPR/NQ experiments) to teach the model to rely more on contextual evidence and reduce memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Training procedure modification for QA to reduce overreliance on parametric knowledge and improve generalization to changed facts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Synthetic contradictory contextual passages included in training (corpus substitutions).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Synthetic/generated via the paper's substitution framework (dataset-derived substitutions).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (training exposes the model to contradictions so it learns to prefer context even when parametric memory conflicts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline (no mixed training) MR and EM: NQ Train MR 29.5% / EM 70.9, NQ Dev (AO) MR 27.1% / EM 62.7, NQ Dev (NAO) MR 1.5% / EM 32.9, NewsQA MR 9.3% / EM 21.4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>After mixed training MR fell dramatically: NQ Train MR 2.6% (EM 64.9, -5.0), NQ Dev (AO) MR 1.9% (EM 64.2, +1.5), NQ Dev (NAO) MR 0.0% (EM 40.0, +7.1), NewsQA MR 0.6% (EM 25.8, +4.4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive overall: augmenting training with contradictory contextual evidence substantially decreases memorization and improves out-of-distribution generalization (EM increases on unseen/dev OOD), though there can be a small EM reduction on in-domain training-set evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Exposure to contrastive/counterfactual contexts during training teaches the model to condition on passage content rather than memorized labels; this reduces overstability and hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training with substituted contexts is an effective, simple mitigation: it reduces MR to near zero and improves OOD accuracy (notably +7.1 pp on NQ Dev NAO), demonstrating that models can be taught to trust contextual evidence when properly trained.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity-Based Knowledge Conflicts in Question Answering', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Get your vitamin C! robust fact verification with contrastive evidence <em>(Rating: 2)</em></li>
                <li>Hurdles to progress in long-form question answering <em>(Rating: 2)</em></li>
                <li>How much knowledge can you pack into the parameters of a language model? <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-202",
    "paper_id": "paper-238deab37e201c57505a4a47bb854e462af79bd7",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "T5 (generative reader)",
            "name_full": "T5 generative reader (retrieve-and-generate question answering)",
            "brief_description": "A generative retrieve-and-read reader (T5) finetuned for open-domain QA; used to measure how models integrate contextual passages (retrieved or gold) versus parametric memory using entity-substitution conflicts.",
            "citation_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "mention_or_use": "use",
            "model_name": "T5",
            "model_size": "60M, 770M, 3B, 11B",
            "task_description": "Open-domain question answering (retrieve-and-generate reader) where the model must produce an answer given a query and one retrieved/gold passage.",
            "evidence_type": "Contextual passages (retrieved or gold passages containing factual statements; sometimes deliberately contradictory via entity substitution).",
            "evidence_source": "Retrieved documents (DPR or TF-IDF) and synthetic substituted documents produced by the paper's substitution framework (Wikidata/dataset-sourced entity replacements).",
            "parametric_knowledge_alignment": "contradictory (experiments purposely replace gold entities so context disagrees with learned/fine-tuned answers).",
            "performance_without_evidence": "When the original (unmodified) passage/context supports the original answer, model often relies on parametric knowledge: on some corpus-substituted examples the model still predicts the original (memorized) answer up to ~20% (NQ) and up to ~75% (NewsQA) in inference on substituted instances; model confidence is higher on original example than substituted in ~74% of cases (Table 2 aggregate).",
            "performance_with_evidence": "Given substituted (contradictory) passages the model often (a) predicts the Substitute answer less than 50% of the time on many sets, (b) increases predictions categorized as 'Other' frequently, and (c) is less confident on substituted inputs: p(original) &gt; p(substituted) in ~74% of in-domain cases; targeted augmentation (mixed training) reduced memorization ratio from 29.5%→2.6% (NQ Train) and improved OOD EM (NQ Dev NAO +7.1 percentage points, NewsQA +4.4).",
            "evidence_effect": "mixed: adding (contradictory) contextual evidence often decreased model confidence and sometimes failed to override parametric memory (negative effect on correctness), producing more uncertain/'Other' answers; however, using such contradictory evidence as training augmentation had a positive effect (reduced memorization and improved OOD generalization).",
            "evidence_decreased_confidence": true,
            "proposed_mechanism": "Overreliance on parametric memorization (hallucination/parroting) causes the reader to prefer learned answers over contextual evidence; this tendency is amplified by larger model size, poor retrieval quality during training, and entity popularity/type biases. Good retrieval and exposure to conflicting contexts at training force the model to ground in context.",
            "key_findings": "Generative T5 readers frequently ignore contradictory contextual evidence and revert to memorized answers (measured by Memorization Ratio), especially for larger models and when trained with imperfect retrieval; contradictory evidence increases uncertainty and 'Other' responses. Training with corpus-substituted examples dramatically reduces memorization and improves out-of-distribution accuracy, at a small possible cost on in-domain train EM.",
            "counterintuitive_behavior": true,
            "uuid": "e202.0",
            "source_info": {
                "paper_title": "Entity-Based Knowledge Conflicts in Question Answering",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Extractive reader (RoBERTa)",
            "name_full": "RoBERTa-based span-extraction QA reader",
            "brief_description": "An extractive span-selection reader (RoBERTa implementation) that predicts answers by selecting spans from the provided passage; evaluated on the same substitution conflicts to compare grounding vs generative behavior.",
            "citation_title": "Roberta: A robustly optimized bert pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa (extractive QA)",
            "model_size": null,
            "task_description": "Extractive question answering (span selection) using a single passage input.",
            "evidence_type": "Contextual passages (gold passages, including substituted versions) provided as evidence.",
            "evidence_source": "Gold passages with entity substitutions (synthetic) used for evaluation.",
            "parametric_knowledge_alignment": "contradictory (substitution changes gold span in passage, so evidence disagrees with parametric/fine-tuned answer).",
            "performance_without_evidence": "Extractive models are trained to rely on passages (gold spans) and thus exhibit very low memorization when provided correct gold passages; baseline memorization ratio is negligible on many sets.",
            "performance_with_evidence": "On substituted passages, memorization ratio is negligible (model generally selects a span) but the rate of 'Other' predictions (incorrect/random spans or hallucinations) is non-trivial (≥15% overall; up to 27% on NewsQA OOD), indicating increased uncertainty and unstable predictions when context contradicts learned priors or substitutions are implausible.",
            "evidence_effect": "mixed: providing contextual evidence forces grounding (positive — low memorization), but knowledge conflicts still increase uncertainty and lead to more incorrect 'Other' span predictions (negative).",
            "evidence_decreased_confidence": true,
            "proposed_mechanism": "Extractive architecture constrains outputs to passage spans so it resists parroting memorized answers, but when passages contain implausible substitutions the model is uncertain and often selects wrong spans or hallucinates, reflecting elevated uncertainty rather than parametric override.",
            "key_findings": "Extractive readers show far lower memorization than generative readers, but knowledge conflicts still produce increased uncertainty and a notable fraction of wrong span selections; thus grounding reduces but does not eliminate adverse effects of contradictory evidence.",
            "counterintuitive_behavior": true,
            "uuid": "e202.1",
            "source_info": {
                "paper_title": "Entity-Based Knowledge Conflicts in Question Answering",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Memorization Ratio (MR)",
            "name_full": "Memorization Ratio",
            "brief_description": "A metric introduced in this paper defined as MR = p_o / (p_o + p_s), where p_o is probability of predicting the Original (memorized) answer and p_s is probability of predicting the Substitute answer on the modified instance; quantifies model overreliance on parametric knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_description": "Diagnostic metric for measuring preference for parametric memory versus contextual evidence in QA under entity-substitution conflicts.",
            "evidence_type": "N/A (metric used to analyze responses to contextual evidence).",
            "evidence_source": "N/A",
            "parametric_knowledge_alignment": "Designed to measure behaviour when evidence contradicts parametric knowledge (i.e., in contradictory alignment).",
            "performance_without_evidence": "N/A (metric itself), but reported example values: before mitigation MR on NQ Train = 29.5%, NQ Dev (AO) = 27.1%, NQ Dev (NAO) = 1.5%, NewsQA = 9.3%. MR increases with model size (from &lt;15% to ≥50% across sizes).",
            "performance_with_evidence": "After mixed training with corpus substitutions MR fell to negligible levels: NQ Train 2.6%, NQ Dev (AO) 1.9%, NQ Dev (NAO) 0.0%, NewsQA 0.6% (Table 4).",
            "evidence_effect": "N/A (metric captures effect of evidence); used to show that contextual evidence often fails to override memorized answers unless training is adjusted.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "MR operationalizes overstability/overreliance on parametric knowledge, enabling quantification of how often models ignore contextual evidence.",
            "key_findings": "MR is an effective, quantitative diagnostic: higher MR indicates stronger memorization/parroting; MR correlates positively with model size and with poorer retriever quality, and drops markedly when models are trained with substituted (conflicting) contexts.",
            "counterintuitive_behavior": null,
            "uuid": "e202.2",
            "source_info": {
                "paper_title": "Entity-Based Knowledge Conflicts in Question Answering",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Corpus Substitution (CS)",
            "name_full": "Corpus Substitution (in-domain entity replacement)",
            "brief_description": "A substitution policy that replaces an answer entity a with another in-domain answer a' from the same dataset (same entity type), producing a coherent but knowledge-conflicting passage used to test how evidence affects model responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 / RoBERTa (used as evaluation probes)",
            "model_size": "various",
            "task_description": "Synthetic creation of contradictory contextual evidence for QA by swapping entities with other same-type entities from the corpus.",
            "evidence_type": "Contradictory contextual passages (factual statements altered to assert a different entity).",
            "evidence_source": "Synthetic/generated from dataset gold answers (in-domain sampling).",
            "parametric_knowledge_alignment": "contradictory by design (substitution changes correct answer relative to training/fine-tuned data).",
            "performance_without_evidence": "On original, unmodified passages the model answered correctly (examples selected where original answered correctly).",
            "performance_with_evidence": "When presented with CS passages at inference the generative reader predicted the Substitute answer rarely &gt;50% (NQ), reverted to Original up to 20% (NQ) / 75% (NewsQA), and frequently produced Other answers; model confidence was higher on original than substituted example ~74% of the time for in-domain examples.",
            "evidence_effect": "negative/mixed: adding contradictory in-domain evidence often decreased model confidence and sometimes failed to change predicted answer (negative), but using these substitutions as training augmentation reduced memorization (positive).",
            "evidence_decreased_confidence": true,
            "proposed_mechanism": "Corpus substitution creates explicit conflicts; results show models often overweight parametric priors learned at training and underweight contextual evidence unless trained to attend to such conflicts or given high-quality retrieval.",
            "key_findings": "Corpus substitution exposes that generative QA models commonly ignore contextual evidence and hallucinate memorized answers or produce unstable 'Other' outputs; however, using CS as training augmentation effectively teaches models to prioritize context and reduces MR drastically.",
            "counterintuitive_behavior": true,
            "uuid": "e202.3",
            "source_info": {
                "paper_title": "Entity-Based Knowledge Conflicts in Question Answering",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Mixed training with substitutions",
            "name_full": "Mixed Training with Corpus Substitutions (augmentation mitigation)",
            "brief_description": "A mitigation strategy that augments training data with corpus-substituted examples (25% augmentation in DPR/NQ experiments) to teach the model to rely more on contextual evidence and reduce memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (evaluated)",
            "model_size": null,
            "task_description": "Training procedure modification for QA to reduce overreliance on parametric knowledge and improve generalization to changed facts.",
            "evidence_type": "Synthetic contradictory contextual passages included in training (corpus substitutions).",
            "evidence_source": "Synthetic/generated via the paper's substitution framework (dataset-derived substitutions).",
            "parametric_knowledge_alignment": "mixed (training exposes the model to contradictions so it learns to prefer context even when parametric memory conflicts).",
            "performance_without_evidence": "Baseline (no mixed training) MR and EM: NQ Train MR 29.5% / EM 70.9, NQ Dev (AO) MR 27.1% / EM 62.7, NQ Dev (NAO) MR 1.5% / EM 32.9, NewsQA MR 9.3% / EM 21.4.",
            "performance_with_evidence": "After mixed training MR fell dramatically: NQ Train MR 2.6% (EM 64.9, -5.0), NQ Dev (AO) MR 1.9% (EM 64.2, +1.5), NQ Dev (NAO) MR 0.0% (EM 40.0, +7.1), NewsQA MR 0.6% (EM 25.8, +4.4).",
            "evidence_effect": "positive overall: augmenting training with contradictory contextual evidence substantially decreases memorization and improves out-of-distribution generalization (EM increases on unseen/dev OOD), though there can be a small EM reduction on in-domain training-set evaluation.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Exposure to contrastive/counterfactual contexts during training teaches the model to condition on passage content rather than memorized labels; this reduces overstability and hallucination.",
            "key_findings": "Training with substituted contexts is an effective, simple mitigation: it reduces MR to near zero and improves OOD accuracy (notably +7.1 pp on NQ Dev NAO), demonstrating that models can be taught to trust contextual evidence when properly trained.",
            "counterintuitive_behavior": true,
            "uuid": "e202.4",
            "source_info": {
                "paper_title": "Entity-Based Knowledge Conflicts in Question Answering",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "Get your vitamin C! robust fact verification with contrastive evidence",
            "rating": 2
        },
        {
            "paper_title": "Hurdles to progress in long-form question answering",
            "rating": 2
        },
        {
            "paper_title": "How much knowledge can you pack into the parameters of a language model?",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2
        }
    ],
    "cost": 0.0166435,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Entity-Based Knowledge Conflicts in Question Answering</h1>
<p>Shayne Longpre ${ }^{\star \text { ® }}$ Kartik Perisetla ${ }^{\star \text { ® }}$ Anthony Chen ${ }^{\star \text { ® }}$ Nikhil Ramesh ${ }^{\text {® }}$ Chris DuBois ${ }^{\text {® }}$ Sameer Singh ${ }^{\text {® }}$<br>${ }^{ \star}$ Apple ${ }^{\circledR}$ University of California, Irvine slongpre@mit.edu<br>{kperisetla,nikhilr, cdubois} @apple.com<br>{anthony.chen, sameer}@uci.edu</p>
<h4>Abstract</h4>
<p>Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge which minimizes hallucination and improves out-of-distribution generalization by $4 \%-7 \%$. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e., time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Knowledge-dependent tasks, such as open-retrieval question answering (QA), require expansive "world knowledge", common sense, and reasoning abilities. State-of-the-art approaches typically follow a retrieve-and-read setup (Chen et al., 2017), where the retriever sources relevant documents, and the reader produces an answer from these. In this sense, there are two sources of knowledge contributing to model inference with an ambiguous and opaque division of labour. The first is the implicit parametric knowledge (i.e., their learned weights) instilled by pre-training and fine-tuning (Petroni et al., 2019). The second is contextual knowledge, usu-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Question: Who did US fight in world war 1?
Original Context: The United States declared war on Germany on April 6, 1917, over 2 years after World War I started ...
Original Answer: Germany
Model Prediction: Germany
Question: Who did US fight in world war 1?
Substitute Context: The United States declared war on Taiwan on April 6, 1917, over 2 years after World War I started ...
Substitute Answer: Taiwan</p>
<h2>Model Prediction: Germany</h2>
<p>Figure 1: Knowledge Substitution: A substitute example is derived from the original example by replacing the original answer, Germany, with a similar type of answer, i.e. Taiwan. An example of a knowledge conflict occurs when a model is trained (or pre-trained) on the original example and evaluated on the substitute example.
ally sourced as passages of text from the retriever (Fisch et al., 2019).</p>
<p>As a testament to their memorization abilities, large language models can produce competitive results relying only on their own parametric knowledge, without access to relevant documents (Brown et al., 2020; Roberts et al., 2020). However, this memorization behaviour has manifested in a penchant to hallucinate, or parrot answers memorized during training, completely ignoring relevant documents when provided (Krishna et al., 2021; Bender et al., 2021). This memorization behaviour violates the expectation that the reader produce answers consistent with the retrieved information, diminishing interpretability of the system. More problematically, this behaviour inhibits the model's ability to generalize to evolving knowledge and timedependent answers, not found in training (Guu et al., 2020; Schuster et al., 2021).</p>
<p>Our objective is to understand how systems employ parametric and contextual knowledge together by studying knowledge conflicts: situations where</p>
<p>the contextual knowledge contradicts with knowledge learned during pre-training or fine-tuning. Because the space of knowledge conflicts is broad, we restrict ourselves to the space of entity-based conflicts - restricted to named entity substitutions. We create an automated framework that identifies QA instances with named entity answers, then substitutes mentions of the entity in the gold document with an alternate entity, thus changing the answer (Fig. 1). Our framework is extensible and flexible, allowing entities mined from various sources (entities in datasets, or knowledge graphs like Wikidata (Vrandecic and Krötzsch, 2014)), and with custom substitution policies.</p>
<p>We use our automated framework to create substitution instances for Natural Questions (Kwiatkowski et al., 2019) and NewsQA (Trischler et al., 2017a). Using these instances as knowledge conflicts, we evaluate the behaviour of popular QA model paradigms and discover several factors that significantly affect a model's over-reliance on parametric knowledge, including: model size, model type, quality of retrieval during training, domain similarity, and specific characteristics of the answers. Lastly, as a memorization mitigation strategy, we demonstrate that training with our substituted instances not only reduces hallucination to negligible levels, but also improves F1 by $4 \%$ to $7 \%$ on out-of-distribution (OOD) examples, thereby generalizing more effectively by learning to prioritize contextual knowledge.</p>
<h2>2 Substitution Framework</h2>
<p>We introduce a substitution framework for creating knowledge-conflicting instances. The framework maps a QA instance $x=(q, a, c)$, with query $q$, answer $a$, and the context passage $c$ in which $a$ appears, to $x^{\prime}=\left(q, a^{\prime}, c^{\prime}\right)$ where $a$ is replaced by substitution answer $a^{\prime}$ as the gold answer, and where all occurrences of $a$ in $c$ have been replaced with $a^{\prime}$, producing new context $c^{\prime}$.</p>
<p>This substitution framework extends partiallyautomated dataset creation techniques introduced by Chen et al. (2021) for Ambiguous Entity Retrieval (AmbER). Our dataset derivation follows two steps: (1) identifying QA instances with named entity answers, and (2) replacing all occurrences of the answer in the context with a substituted entity, effectively changing the answer. We provide tools to identify coherence-preserving substitutions and create substitutions with certain characteristics
(e.g. semantic equivalence, or popularity score on Wikipedia).</p>
<h3>2.1 Identifying Named Entity Answers</h3>
<p>As our focus is entity-based knowledge conflicts, our first step identifies instances where the answer is a named entity. We leverage the SpaCy named entity recognizer and entity linker to identify gold answers that are named entities, their corresponding entity types, and their ID in the Wikidata graph. ${ }^{2}$ This allows us to gather auxiliary information about the entity, such as entity popularity.</p>
<p>We focus on five entity types that are well represented in question answering datasets: person (PER), date (DAT), numeric (NUM), organization (ORG), and location (LOC). Tracking an answer's entity type allows us to create coherent substitutions. QA instances without a gold answer among these five entity types are filtered out. When applying substitutions, we replace all spans of the answer entity in the context with a substituted entity, according to the substitution policy.</p>
<h3>2.2 Types of Substitutions</h3>
<p>There are many possible substitution policies which evaluate different properties. In Figure 2, we illustrate the versatility of our framework, highlighting the types of knowledge substitutions we experiment with in this work. An advantage of this framework over recent similar work (Schuster et al., 2021) is that it is extensible. Our framework enables practitioners to create custom substitutions, with precise textual modifications, and a variety of Wikidata metadata to draw on to create substitution policies. We describe substitutions derived from our framework used herein to test hypotheses of model behaviour.</p>
<p>Corpus Substitution (CS) replaces answer $a$ with another entity $a^{\prime}$ from the same dataset (in-domain). The substitution entity is randomly sampled from the gold answers found in the same dataset $D$, such that $a$ and $a^{\prime}$ share the same entity type (i.e., for type $(\cdot) \in{$ PER, DAT, NUM, ORG, LOC $}$, type $(a)=\operatorname{type}\left(a^{\prime}\right)$ ).</p>
<p>Type Swap Substitution (TSS) replaces answers $a$ with a nonsensical in-domain entity $a^{\prime}$. The</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample Rules</th>
<th style="text-align: center;">Sample From</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Original answer $a$</td>
<td style="text-align: center;">- Saint Peter</td>
<td style="text-align: center;">Query: "Who do you meet at the gates of heaven?" <br> Context: "The image of the gates in popular culture is a set of large gold, white or wrought - iron gates in the clouds, guarded by Saint Peter (the keeper of the 'keys to the kingdom').'</td>
</tr>
<tr>
<td style="text-align: center;">Alias <br> Substitution</td>
<td style="text-align: center;">Sample an equivalent answer $a^{\prime}$, from the set of Wikidata aliases for original answer a (Saint Peter). $a^{\prime} \sim W_{\text {alias }}(a)$</td>
<td style="text-align: center;">- Peter the Apostle <br> - Pope Peter <br> - Saint Peter the Apostle <br> - Simon Peter <br> - Petrus</td>
<td style="text-align: center;">Context: "The image of the gates in popular culture is a set of large gold, white or wrought - iron gates in the clouds, guarded by Simon Peter (the keeper of the 'keys to the kingdom')."</td>
</tr>
<tr>
<td style="text-align: center;">Corpus <br> Substitution</td>
<td style="text-align: center;">Sample an answer $a^{\prime}$ of the same type $s$ as original $a$ from the set of answers found in the corpus $D$. $C_{\text {PER }}=[\bar{a}] a \in D, \operatorname{type}(\bar{a})=P E R]$ $a^{\prime} \sim C_{\text {PER }}$</td>
<td style="text-align: center;">- Russell Wilson <br> - Mary Quant <br> - Dejana Eitberger <br> - Bon Jovi</td>
<td style="text-align: center;">Context: "The image of the gates in popular culture is a set of large gold, white or wrought - iron gates in the clouds, guarded by Mary Quant (the keeper of the 'keys to the kingdom')."</td>
</tr>
<tr>
<td style="text-align: center;">Type Swap <br> Substitution</td>
<td style="text-align: center;">Sample an answer $a^{\prime}$ of a different type $t$ as original $a$ from the set of answers found in the corpus $D$. $C_{\text {cPER }}=[\bar{a}] a \in D, \operatorname{type}(\bar{a}) \neq P E R]$ $a^{\prime} \sim C_{\text {cPER }}$</td>
<td style="text-align: center;">- September (date) <br> - 40 (num) <br> - the United Nations (org) <br> - St. Ives (loc)</td>
<td style="text-align: center;">Context: "The image of the gates in popular culture is a set of large gold, white or wrought - iron gates in the clouds, guarded by the United Nations (the keeper of the 'keys to the kingdom')."</td>
</tr>
<tr>
<td style="text-align: center;">Popularity <br> Substitution</td>
<td style="text-align: center;">Sample an answer $a^{\prime}$ from all WikiData entities of the same type $s$ as $a$, given popularity range $\left[p_{t}, p_{u}\right]$. $C_{\text {PER }}^{\text {p }}=[\bar{a}] a \in W, \operatorname{type}(\bar{a})=P E R, p_{t} \leq p o p(\bar{a}) \leq p_{u}}$ $a^{\prime} \sim C_{\text {PER }}^{\text {p }}$</td>
<td style="text-align: center;">- Jennifer Aniston <br> - John Wayne <br> - Liam Keeson <br> - Emily Blunt</td>
<td style="text-align: center;">Context: "The image of the gates in popular culture is a set of large gold, white or wrought - iron gates in the clouds, guarded by John Wayne (the keeper of the 'keys to the kingdom')."</td>
</tr>
</tbody>
</table>
<p>Figure 2: Substitution Methods. An illustration of substitution types and their rules, whereby the original answer $a$ is replaced by a substitution answer $a^{\prime}$, sourced either from Wikidata $W$ or the set of answers appearing in the training dataset $D$. type $(\bar{a})$ yields the answer type, and $\operatorname{pop}(\bar{a})$ yields the Wikidata popularity value.
substitution entity is randomly sampled from the gold answers found in the same dataset $D$, such that $a$ and $a^{\prime}$ have different types, type $(a) \neq \operatorname{type}\left(a^{\prime}\right)$. Nonsensical answer substitutions are useful to test model robustness or common sense.</p>
<p>Popularity Substitution (PS) tests how the popularity of the substituted entity affects reliance on parametric knowledge. We replace $a$ in $c$ with $a^{\prime}$, which is a randomly sampled Wikidata entity of the same type as $a$. The popularity of $a^{\prime}, \operatorname{pop}\left(a^{\prime}\right)$, is between user-specified bounds $p_{l}$ and $p_{u}$, measured in monthly Wikipedia page views, as estimated from October 2019.</p>
<p>Alias Substitution (AS) replaces answer $a$ with a semantically equivalent paraphrase $a^{\prime}$, sampled from the list of $a$ 's Wikidata aliases $W_{\text {alias }}(a)$.</p>
<h3>2.3 Substitution Quality</h3>
<p>The authors conduct human grading to evaluate the fluency and correctness of each substitution method. For fluency, the annotator is asked whether the substituted answer $a^{\prime}$ is a grammatical replacement within the given context $c^{\prime}$. For correctness, the annotator is given the query-context pair $\left(q, c^{\prime}\right)$ and asked to highlight the span that answers the question. Comparing the substituted answer to the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Sub. Type</th>
<th style="text-align: center;">Fluency (\%)</th>
<th style="text-align: center;">Correctness (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alias Sub</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: left;">Popularity Sub</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">87</td>
</tr>
<tr>
<td style="text-align: left;">Corpus Sub</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">82</td>
</tr>
<tr>
<td style="text-align: left;">Type Swap Sub ${ }^{\dagger}$</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Original</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">91</td>
</tr>
</tbody>
</table>
<p>Table 1: Human Evaluation of 80-100 Natural Questions examples per row. Substitutions yield reasonable fluency and correctness compared to original examples.
${ }^{\dagger}$ Type swap substitution is intended to have low fluency to test model robustness. Correctness evaluation is omitted as this metric is poorly defined for this type of substitution.
human chosen span gives us a direct measurement of how naturally intuitive the new examples are.</p>
<p>Table 1 shows the automated substitution methods retain fluency and correctness just above $80 \%$ for Natural Questions - slightly less than the original examples. These metrics suggest the current framework is effective for average-case analysis of model interpretability, and certain training methods (see Section 4.4). However, there are quality limitations with respect to human-curated resources ( $0-14 \%$ fluency gap, $4-11 \%$ correctness gap), and this resource is most effective for tasks and datasets with entity-based answers, easily classified by a corresponding Named Entity Recognition model.</p>
<p>The main advantage of an automated framework is it's capacity to inexpensively scale beyond human annotation. Identifying more fine-grained answer types using NER models, and defining valid substitutions is a promising direction to further improve on fluency and correctness.</p>
<h2>3 Experimental Setup</h2>
<h3>3.1 Datasets</h3>
<p>Training We adopt a common and humansourced query distribution in open-domain question answering, using Kwiatkowski et al. (2019)'s Natural Questions (NQ) for training. For certain experiments we train with NewsQA (Trischler et al., 2017b), a news-oriented dataset with examples whose answers are prone to change over time (susceptible to knowledge conflicts).</p>
<p>Inference At inference time we create knowledge conflicts for (1) the training set (to understand knowledge conflicts on data the models have seen), (2) the development set, as well as (3) an out-of-distribution (OOD) set, either the training set for NQ or NewsQA, depending on which was not used at training time. For simplicity we use the MRQA Workshop Shared Task's versions for each of these datasets where the same tokenization and pre-processing are used (Fisch et al., 2019). ${ }^{3}$</p>
<p>Lewis et al. (2021) show the Natural Questions training and development sets contain many similar queries and answers. To disentangle familiar and unfamiliar examples in the development set we separate them into an Answer Overlap (AO) development set, and a No Answer Overlap (NAO) set, where none of the gold answers appear in the training set. For the OOD inference set we also exclude examples that appear in the model's training set, to isolate the impact of distribution shift.</p>
<h3>3.2 Models</h3>
<p>This work evaluates retrieve-and-read QA systems: the retriever finds relevant documents and the reader produces an answer using these documents.</p>
<p>Retriever We use dense passage retrieval (DPR) (Karpukhin et al., 2020) as the primary retrieval system. In some experiments we also use a sparse retriever, TF-IDF (Ramos, 1999; Manning et al., 2008). During training, we retrieve a single document which we provide to the reader to produce an</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>answer. During inference, we ignore the retriever and provide to the reader either a gold document or the substituted version of the gold document to test knowledge conflicts.</p>
<p>Generative Reader In this setting, a model receives a query concatenated with contextual text and decodes a prediction. Our generative model is a T5 model (Raffel et al., 2020) and for simplicity, we train using a single retrieved passage. ${ }^{4}$ While training with multiple documents would yield better results (Izacard and Grave, 2021), training with only a single document as input allows us to better decouple the interactions between the reader and the retriever.</p>
<p>We choose to evaluate a simple T5 reader model because it is the consistent component across highperforming retrieval-based QA models (Izacard and Grave, 2021; Lewis et al., 2020; Kim et al., 2020), and thus preserves the generality of our findings. Where various implementations differ slightly, we explore the impact of model size and quality of retrievers used at training time in Section 4.2.</p>
<p>Extractive Reader We also experiment with a span-extraction QA model, where the predicted answer is a span of text taken directly from the context $c$. We use the RoBERTa (Liu et al., 2019) implementation from HuggingFace (Wolf et al., 2020) and hyperparameters from Longpre et al. (2019). ${ }^{5}$ By necessity, this model is trained with gold passages that always have a gold span.</p>
<h3>3.3 Metrics</h3>
<p>To understand a model's propensity to rely on memorized answers, we narrow our focus to examples that a model correctly answered on the original, unaltered example. Using the standard SQuAD-based Exact Match measurement (Rajpurkar et al., 2016), we compare model predictions on examples before $(x)$ and after $\left(x^{\prime}\right)$ the substitution has been applied. We then measure the fraction of times the model predicts: the Original answer $\left(p_{o}\right)$, the Substitute answer $\left(p_{s}\right)$, or an Other answer altogether, on $x^{\prime}$.</p>
<p>The Memorization Ratio $\left(M_{R}\right)$ measures how often the model generates the original answer (parametric knowledge) as opposed to the answer in the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Trained on Natural Questions (NQ) Train
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Trained on NewsQA Train</p>
<p>Figure 3: Corpus Substitution. Inference behaviour and memorization ratio $\left(M_{R}\right)$ of generative models evaluated on corpus substituted instances.
context (contextual knowledge). This estimates the overstability of the model - it's brittleness to changing information.</p>
<p>$$
M_{R}=\frac{p_{o}}{p_{o}+p_{s}}
$$</p>
<h2>4 Experiments</h2>
<h3>4.1 Results</h3>
<p>Our results on corpus substitution test how a QA model chooses answers when the substituted answer is in the same distribution as the training set. Figure 3 measure how often the model generates the Original answer, the Substitute answer, or some Other answer altogether on $x^{\prime}$. To confirm the observed phenomena is not dataset specific, Figure 3a presents results for the model trained on Natural Questions (NQ), and Figure 3b for the model trained on NewsQA. In each case, we evaluate on the training set, validation set (with and without answer overlap), and an out-of-distribution dataset.</p>
<p>Ideally, the model should preference the Substitute answer, supported by contextual knowledge, over the Original answer observed in fine-tuning, or some Other answer. However, the model predicts the Substitute answer $\alpha^{\prime}$ rarely more than $50 \%$ of the time for the NQ model, and significantly less for the NewsQA model. Instead, the model reverts back to predicting the Original answer seen in training, ignoring the contextual passage, up to $20 \%$ of the time for NQ, and $75 \%$ for NewsQA. Additionally, the knowledge conflicts appears to destabilize the model predictions, predicting Other, usually incorrect, answers a large portion of the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Inference Set</th>
<th style="text-align: center;">Model Prediction Category on $x^{\prime}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ORIG.</td>
<td style="text-align: center;">OTHER</td>
<td style="text-align: center;">SUR.</td>
<td style="text-align: center;">AVG.</td>
</tr>
<tr>
<td style="text-align: left;">NQ TraIN</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">74.2</td>
</tr>
<tr>
<td style="text-align: left;">NQ Dev (AO)</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">74.4</td>
</tr>
<tr>
<td style="text-align: left;">NQ Dev (NAO)</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">64.1</td>
</tr>
<tr>
<td style="text-align: left;">NEWSQA</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">68.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Model Uncertainty. For the NQ trained model, we compute the percentage of time in which $p(x)&gt;p\left(x^{\prime}\right)$, indicating the model was more confident in it's prediction made for the original example $x$ than the corpus substitution example $x^{\prime}$.
time. (See Section 4.3, where the Other category is discussed in detail.) These results demonstrate that common generative QA reader models are unlikely to trust the retrieved information over their parametric memory (learned at training time).</p>
<p>The most apparent trend is that the model predicts the memorized Original answer more frequently in examples observed at (or similar to) training-time. While the memorization ratio $\left(M_{R}\right)$ falls significantly for Dev NAO and the out-ofdistribution (OOD) sets, it is still non-trivial - nor is the resultant tendency for the model to predict Other answers, where it had correctly generated the Original answer, when supported by contextual knowledge in $x$.</p>
<p>How is Model Uncertainty Affected? Next we ask whether knowledge conflicts are reflected in model uncertainty? If model predictions are relatively uncertain when knowledge conflicts occur, then confidence thresholds might permit the system to abstain from answering some of these questions. In Table 2 we compute how often model confidence is greater on the original example $x$ than the modified example $x^{\prime}$, broken down by prediction category and inference set.</p>
<p>Knowledge conflicts yield relatively higher prediction uncertainty, especially for in-domain examples $(74 \%)$. Uncertainty is also elevated for out-of-distribution examples in NQ Dev (NAO) or NewsQA ( $64 \%$ and $69 \%$ respectively). In particular, uncertainty is highest for instances where the model predicts Other. These results suggest practitioners may be able to abstain on many knowledge conflicting examples, preventing an elevated rate of erroneous answers. However, the abstention solution simply exchanges incorrect answers for no answers, without addressing the primary issue of a model ignoring contextual knowledge.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Alias Substitution. Inference behaviour and memorization ratio (M<sup>R</sup>) of a T5 model trained on NQ.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Impact of Model Size on Memorization Ratio. We finetune T5 small (60M), large (770M), XL (3B), and XXL (11B) models on NQ, finding the memorization ratio increases with model size for all inference sets.</p>
<h3>How is Inference Stability over Semantically Equivalent Answers?</h3>
<p>Alias substitution swaps the answer with a semantically equivalent paraphrase, effectively isolating the impact of a benign perturbation, without introducing any real conflict in knowledge. As this type of substitution is not a knowledge conflict, we consider both Original and Substitute predictions correct model behaviour, and examine how often subtle answer paraphrases cause instability in the answers (i.e., predicting Other). Figure 4 shows an elevated preference to select the Original answer than when the knowledge conflicted in corpus substitution, however Other is also predicted at least 15% of the time. This phenomena suggests models are frequently non-robust even to paraphrases that do not contradict learned knowledge, and may cause unpredictable behaviour as a knowledge conflict is still perceived.</p>
<h3>4.2 Factors Impacting Model Behaviour</h3>
<p>We've observed model behaviour appears strongly contingent on the domain similarity of presented knowledge conflicts. Next we explore what other factors may significantly impact a proclivity to preference parametric knowledge.</p>
<h3>How does Model Size impact Memorization?</h3>
<p>As Bender et al. (2021) has shown, large language</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Impact of Retrieval Quality on Memorization. We train T5 models with the k<sup>th</sup> retrieved documents according to either DPR or TF-IDF. We report results on NQ Dev and compare the resulting memorization ratio (M<sup>R</sup>) against retriever quality (Recall@K).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Extractive QA. Inference behaviour and memorization ratio (M<sup>R</sup>) of extractive QA models, trained on gold passages, and evaluated on corpus substituted instances.</p>
<p>Models are susceptible to parroting memorized information. Figure 5 illustrates notable increases in memorization ratio as a function of the number of parameters. On the Train and Dev (AO) sets, the memorization ratio rises from &lt; 15% to ≥ 50% in just two orders of magnitude, with no sign of diminishing returns. Most striking, the memorization ratio even for the Dev (NAO) set rises for the largest models in our experiments (11B parameters), which remain orders of magnitude smaller than the largest language models available.</p>
<h3>How does Retrieval Quality impact Memorization?</h3>
<p>Until now we've used the highest ranked DPR document during training. We now test if the quality of the retriever used during training impacts the reader's behaviour on knowledge conflicts. For DPR and TF-IDF, we sample the k<sup>th</sup> ranked passage returned from the retriever instead of the first and use it to train our generative model. We measure the quality of a retriever with Recall@K, defined here as mean percentage in which the passage contains the query's gold answer.</p>
<p>Figure 6 illustrates a clear inverse relationship between retrieval quality (Recall@K) and the memorization ratio (M<sup>R</sup>). For both TF-IDF and DPR, less relevant passages during training cause the model to predict the Original answer at inference on x<sup>i</sup>, effectively ignoring the passage. Training</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Popularity Substitution. Inference on queries where documents have been substituted with Wikidata entities of varying popularities. Model is T5 trained on NQ.
with gold passages reduces memorization, as the model is conditioned to expect the answer to always present in the passage.</p>
<p>While training with gold passages effectively minimizes the memorization ratio, this is not standard practice among state-of-the-art QA models (Izacard and Grave, 2021; Lewis et al., 2020; Kim et al., 2020). Typically, these generative QA systems are trained with retrieved passages, more conducive to scalable, and end-to-end training procedures. Consequently, training with gold passages may not present a convenient or viable solution.</p>
<p>Are Extractive QA Models susceptible to Knowledge Conflicts? One potential solution to the aforementioned issues with generative models is to use extractive QA readers which select a span from the passage. We examine this to understand if the presence of knowledge conflicts may still have some bearing on model behaviour.</p>
<p>In Figure 7, we replicate the corpus substitution knowledge conflicts from Figure 3 but with an extractive QA model. The memorization ratio falls to negligible values, as expected, however the model predicts Other $\geq 15 \%$ of the time, for examples it had correctly answered pre-substitution. As discussed further in Section 4.3, this is likely symptomatic of greater model uncertainty in the presence of knowledge conflicts. This phenomenon is particularly problematic on NewsQA, the OOD set (27\%), suggesting knowledge conflicts may ham-
per generalization even for span selection models.
How does Popularity of an Answer Entity impact Memorization? Using popularity substitution we examine if models are biased towards predicting more popular answers (Shwartz et al., 2020; Chen et al., 2021). Limiting our focus to the Person answer category, we order all PER Wikidata entities by popularity (approximated by Wikipedia monthly page views) and stratify them into five evenly sized popularity buckets. For each NQ instance with a PER answer, we generate five substituted instances, using a sampled entity for each of the five buckets.</p>
<p>In Figure 8, we plot the difference in popularity between the original and substituted answers against the percentage of model predictions on $x^{\prime}$ that fall into each category. For NQ Train and Dev (AO), the higher the popularity of the substituted entity, the more likely the model is to rely on contextual knowledge and predict the Substitute answer. Conversely, the lower the popularity, the more likely the model is to predict an Other or Original answer. On the Dev (NAO) set, the popularity of the substituted entity is less predictive of model behavior. This suggests the popularity of a substituted entity plays a role only when the original answer is from a domain very close to training.</p>
<p>How do Models Behave on Nonsensical Knowledge Substitutions? Here we ask if nonsensical (obviously incorrect) substitutions elicit a higher memorization ratio, and whether model behaviour varies for different types of answers. Type swap substitution tests this by replacing the original entity with an entity of a different type. While practitioners typically prefer models to produce answers consistent with contextual knowledge, here a model may have good reason to doubt the quality of information. This experiment is relevant to measuring the common sense inherent in models, or robustness to misinformation attacks. We plot the memorization ratio $M_{R}$, across the possible range of type substitutions in Figure 9.</p>
<p>We again observe elevated memorization ratios across NQ Train and NQ Dev (AO). When the original entity is a string (entity types LOC, PER, ORG), the model is more likely to rely on contextual knowledge and generate the Substitute answer. In contrast, when the original entity is numerical (DAT and NUM), the model is more likely to predict the Original answer. The most striking result</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Type Swap Substitution. A Memorization Ratio (M<sup>B</sup>) matrix broken down by answer type, for the NQ generative model. Darker intensity indicates higher M<sup>B</sup>. We find M<sup>B</sup> is much higher when the original entity is numeric (DAT and NUM) and when the example is similar to those seen in training.</p>
<p>is when a numeric entity is replaced with a textual one; at least 83% of the time the model predicts the <em>Original</em> answer. On NQ Dev (NAO), memorization is low across type-pair substitutions, aligning with our previous experiments demonstrating memorization is lower on unseen data. Overall, these results suggest generative QA models may (inadvertently) be partially robust to index poisoning or misinformation attacks, attempting to elicit obviously false answers.</p>
<h3>4.3 Analyzing Other Predictions</h3>
<p>While the <em>Original</em> and <em>Substitute</em> answers are well defined, the <em>Other</em> category is broad and serves as a catch-all. We perform a qualitative analysis to understand what phenomenon <em>Other</em> captures. For corpus, alias, and type-swap substitutions, we sample 40 instances each where <em>Other</em> is predicted, then group them into meaningful buckets (Tab. 3).</p>
<p>Part of <em>Other</em> predictions are due to the strict <em>EM</em> metric. Most prevalent is <em>alias substitution</em>; for 40% of cases the predicted answer is grounded to the original answer. Additionally, hallucinating an answer not in the context occurs throughout substitution types. We find that a reason models either hallucinate an answer or picks a random context span is when the substituted answer is implausible, as is designed in the <em>type-swap substitution</em>.</p>
<p>We also find interesting behavior within the type-swap substitution. When a textual entity (PER, LOC, or ORG) is replaced by another textual entity (with a different type), models are more likely to predict the substituted entity than when a textual entity is replaced by a numeric entity (DAT or NUM). This suggests models are able to recognize the plausibility of answers, and fall back to hallucinating an answer when an answer is implausible.</p>
<h3>4.4 Mitigating Memorization</h3>
<p>Our experiments suggest memorization can be mitigated by training with a perfect retriever — the reader learns to trust the passage and ground it's generation in this context. However, perfect retrieval annotations are costly and prohibitive to collect. In the absence of gold documents, we propose a simple method to mitigate memorization: augment the training set with training examples modified by <em>corpus substitution</em>. We construct a training set containing NQ examples with DPR passages, and the <em>corpus substituted</em> version of all DPR passages <em>that contain a gold answer to substitute for</em>. (This works out to 25% of the original training set size for DPR on NQ). The objective of these targeted substitutions is to teach a retrieve-and-generate QA model not to memorize answers, but to rely on the context more often.</p>
<p>Table 4 illustrates training with our augmented dataset greatly decreases the memorization ratio on all KC datasets to negligible levels. An important consequence of this: out-of-domain generalization on <strong>original</strong> instances improves for both NQ Dev NAO (7%) and NewsQA (4%). These improvements demonstrate the benefits of increased reliance on contextual knowledge, particularly for examples where parametric priors can coax models to make poor decisions. We hope our substitution framework with this simple training method proves useful for practitioners developing systems which generalize to changing knowledge.</p>
<h3>5 Related Work</h3>
<p><strong>Overreliance on Parametric Knowledge</strong> Krishna et al. (2021) showed that replacing the retrieved documents with random documents during</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Sub (\%)</th>
<th style="text-align: left;">Example of Phenomena</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Grounding to Original</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">CS $(7.5 \%)$</td>
<td style="text-align: left;">Context: The 2017 American Championship</td>
</tr>
<tr>
<td style="text-align: left;">AS $(40 \%)$</td>
<td style="text-align: left;">Series pit Hodgson against the Yankees ...</td>
</tr>
<tr>
<td style="text-align: left;">TSS $(2.5 \%)$</td>
<td style="text-align: left;">Q: who won the american league?</td>
</tr>
<tr>
<td style="text-align: left;">XCS $(10 \%)$</td>
<td style="text-align: left;">Orig Ans: the Houston Astros</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sub Ans: Hodgson</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Pred: the astros</td>
</tr>
<tr>
<td style="text-align: left;">Grounding to Substitute</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">CS $(12.5 \%)$</td>
<td style="text-align: left;">Context: The Bay of Pigs was a failed inva-</td>
</tr>
<tr>
<td style="text-align: left;">AS (-)</td>
<td style="text-align: left;">sion defeated by New Amsterdam ...</td>
</tr>
<tr>
<td style="text-align: left;">TSS $(7.5 \%)$</td>
<td style="text-align: left;">Q: who won the the bay of pigs?</td>
</tr>
<tr>
<td style="text-align: left;">XCS $(25 \%)$</td>
<td style="text-align: left;">Orig Ans: Cuban Revolutionary Forces</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sub Ans: New Amsterdam</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Pred: Amsterdam</td>
</tr>
<tr>
<td style="text-align: left;">Another Correct Answer</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">CS $(12.5 \%)$</td>
<td style="text-align: left;">Context: Abby graduated from Canberra</td>
</tr>
<tr>
<td style="text-align: left;">AS $(2.5 \%)$</td>
<td style="text-align: left;">and earned her master from Georgia St. ...</td>
</tr>
<tr>
<td style="text-align: left;">TSS $(2.5 \%)$</td>
<td style="text-align: left;">Q: where did abby go to college?</td>
</tr>
<tr>
<td style="text-align: left;">XCS $(-)$</td>
<td style="text-align: left;">Orig Ans: Louisiana State</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sub Ans: Canberra</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Pred: georgia state university</td>
</tr>
<tr>
<td style="text-align: left;">Random Passage Span</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">CS $(17.5 \%)$</td>
<td style="text-align: left;">Context: There are 1000 sq metres farmers</td>
</tr>
<tr>
<td style="text-align: left;">AS $(27.5 \%)$</td>
<td style="text-align: left;">and 757,900 ag workers in the US ...</td>
</tr>
<tr>
<td style="text-align: left;">TSS</td>
<td style="text-align: left;">Q: how many farmers are in usa?</td>
</tr>
<tr>
<td style="text-align: left;">$(22.5 \%)$</td>
<td style="text-align: left;">Orig Ans: 3.2 million</td>
</tr>
<tr>
<td style="text-align: left;">XCS $(65 \%)$</td>
<td style="text-align: left;">Sub Ans: 1000 sq metres</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Pred: 757,900</td>
</tr>
<tr>
<td style="text-align: left;">Hallucinate</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">CS $(47.5 \%)$</td>
<td style="text-align: left;">Context: "El Pollo Loco" means "Chile" ...</td>
</tr>
<tr>
<td style="text-align: left;">AS $(15 \%)$</td>
<td style="text-align: left;">Q: what does el pollo loco mean?</td>
</tr>
<tr>
<td style="text-align: left;">TSS $(65 \%)$</td>
<td style="text-align: left;">Orig Ans: The Crazy Chicken</td>
</tr>
<tr>
<td style="text-align: left;">XCS $(-)$</td>
<td style="text-align: left;">Sub Ans: Chile</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Pred: the oiled bird</td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">CS $(2.5 \%)$</td>
<td style="text-align: left;">Context: The His Airness River is a 251-</td>
</tr>
<tr>
<td style="text-align: left;">AS $(15 \%)$</td>
<td style="text-align: left;">kilometre long river ...</td>
</tr>
<tr>
<td style="text-align: left;">TSS $(0 \%)$</td>
<td style="text-align: left;">Q: what is east of the jordan river?</td>
</tr>
<tr>
<td style="text-align: left;">XCS $(-)$</td>
<td style="text-align: left;">Orig Ans: Jordan</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sub Ans: His Airness</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Pred: al - quruah</td>
</tr>
</tbody>
</table>
<p>Table 3: Qualitative Analysis for Other predictions. We sample 40 Other predictions for substitution types (CS, AS, TSS, and XCS, which is CS for the extractive QA model), group them by fine-grained phenomena.
inference yields similar performance for the task of long form question answering. Similarly, for the task of fact checking, Schuster et al. (2021) showed that models have trouble on documents when the input has subtly changed, and that training on contrastive examples for fact checking improves attention to context. Our work builds upon these works by exploring the factors that contribute to this overreliance on parametric knowledge.</p>
<p>Overstability Overreliance on parametric knowledge is related to overstability, where a model output stays constant despite semantically significant</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Inference Set</th>
<th style="text-align: center;">$M_{R}$</th>
<th style="text-align: center;">$E M(\Delta)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NQ Train</td>
<td style="text-align: center;">$29.5 \rightarrow 2.6$</td>
<td style="text-align: center;">$70.9 \rightarrow 64.9(-5.0)$</td>
</tr>
<tr>
<td style="text-align: left;">NQ Dev (AO)</td>
<td style="text-align: center;">$27.1 \rightarrow 1.9$</td>
<td style="text-align: center;">$62.7 \rightarrow 64.2(+1.5)$</td>
</tr>
<tr>
<td style="text-align: left;">NQ Dev (NAO)</td>
<td style="text-align: center;">$1.5 \rightarrow 0.0$</td>
<td style="text-align: center;">$32.9 \rightarrow 40.0(+7.1)$</td>
</tr>
<tr>
<td style="text-align: left;">NewsQA</td>
<td style="text-align: center;">$9.3 \rightarrow 0.6$</td>
<td style="text-align: center;">$21.4 \rightarrow 25.8(+4.4)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Mixed Training with Substitutions yields reduced memorization $\left(M_{R}\right)$ and improves generalization to OOD data.
changes to the input. Niu and Bansal (2018) explore overstability in dialougue systems. Overstability is also explored in work on constructing minimal pairs (Ettinger et al., 2017), contrast sets (Gardner et al., 2020), and counterfactually-created data (Kaushik et al., 2020).</p>
<p>Entity-based Substitutions Key to our evaluation framework is substituting entity names with other plausible entity names. Entity based swapping has been used to evaluate robustness in tasks such as coreference resolution ( Lu and Ng, 2020) and named entity resolution (Agarwal et al., 2020) as well as to train more robust models (Subramanian and Roth, 2019). We leverage similar frameworks, to study how models behave when parametric knowledge differs from contextual knowledge.</p>
<h2>6 Conclusion</h2>
<p>In this work, we examine how conflicts between contextual and parametric knowledge affect question answering models. In formalizing this problem, we first contribute a substitution framework for creating knowledge conflicts and evaluating model behaviour. Using this framework, we conduct a detailed examination of knowledge conflicts in QA. Finally, we propose a method to mitigate memorization and consequently improve generalization on out-of-distribution examples. Our findings show knowledge conflicts are an underexplored topic, providing valuable insights into model interpretability and generalization to evolving world knowledge.</p>
<h2>Acknowledgements</h2>
<p>We thank Ni Lao, Yu Wang, Russ Webb, Adam Fisch, Matt Gardner, Dheeru Dua, Sanjay Subramanian, and the anonymous reviewers for their valuable feedback. This work is funded in part by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research and in part by NSF award #IIS1817183.</p>
<h2>References</h2>
<p>Oshin Agarwal, Yinfei Yang, Byron C. Wallace, and A. Nenkova. 2020. Entity-switched datasets: An approach to auditing the in-domain robustness of named entity recognition models. ArXiv, abs/2004.04123.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, page 610-623, New York, NY, USA. Association for Computing Machinery.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, and Sameer Singh. 2021. Evaluating entity disambiguation and the role of popularity in retrieval-based NLP. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4472-4485, Online. Association for Computational Linguistics.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Allyson Ettinger, Sudha Rao, Hal Daumé III, and Emily M. Bender. 2017. Towards linguistically generalizable NLP systems: A workshop and shared task. In Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 1-10, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 1-13, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307-1323, Online. Association for Computational Linguistics.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics.</p>
<p>Divyansh Kaushik, Eduard H. Hovy, and Zachary Chase Lipton. 2020. Learning the difference that makes A difference with counterfactuallyaugmented data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Jihyeok Kim, Seungtaek Choi, Reinald Kim Amplayo, and Seung-won Hwang. 2020. Retrieval-augmented controllable review generation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2284-2295, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4940-4957, Online. Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.</p>
<p>Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021. Question and answer test-train overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1000-1008, Online. Association for Computational Linguistics.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Shayne Longpre, Yi Lu, Zhucheng Tu, and Chris DuBois. 2019. An exploration of data augmentation and sampling techniques for domain-agnostic question answering. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 220-227, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jing Lu and Vincent Ng. 2020. Conundrums in entity coreference resolution: Making sense of the state of the art. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6620-6631, Online. Association for Computational Linguistics.</p>
<p>Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press.</p>
<p>Tong Niu and Mohit Bansal. 2018. Adversarial oversensitivity and over-stability strategies for dialogue models. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 486-496, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP$I J C N L P$ ), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Juan Ramos. 1999. Using tf-idf to determine word relevance in document queries.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics.</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 624-643, Online. Association for Computational Linguistics.</p>
<p>Vered Shwartz, Rachel Rudinger, and Oyvind Tafjord. 2020. "you are grounded?': Latent name artifacts in pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6850-6861, Online. Association for Computational Linguistics.</p>
<p>Sanjay Subramanian and Dan Roth. 2019. Improving generalization in coreference resolution via adversarial training. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), pages 192-197, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017a. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191-200, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017b. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191-200, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Denny Vrandecic and M. Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM, $57: 78-85$.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/mrqa/
MRQA-Shared-Task-2019.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Default implementation and hyperparameters: https://github.com/google-research/ text-to-text-transfer-transformer.
${ }^{5}$ Training pipeline available at https://github. com/huggingface/transformers/tree/ master/examples/question-answering.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
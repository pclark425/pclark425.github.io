<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2324 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2324</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2324</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-236318599</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2003.04919v6.pdf" target="_blank">Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems</a></p>
                <p><strong>Paper Abstract:</strong> There is a growing consensus that solutions to complex science and engineering problems require novel methodologies that are able to integrate traditional physics-based modeling approaches with state-of-the-art machine learning (ML) techniques. This paper provides a structured overview of such techniques. Application-centric objective areas for which these approaches have been applied are summarized, and then classes of methodologies used to construct physics-guided ML models and hybrid physics-ML frameworks are described. We then provide a taxonomy of these existing techniques, which uncovers knowledge gaps and potential crossovers of methods between disciplines that can serve as ideas for future research.</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2324.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2324.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PGRNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Guided Recurrent Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network augmented with physics-based intermediate variables and physics-derived loss terms (e.g., energy conservation) and pre-training on mechanistic model simulations to improve lake temperature predictions with far fewer observations and better out-of-sample generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics Guided RNNs for Modeling Dynamical Systems: A Case Study in Simulating Lake Temperature Profiles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>lake temperature dynamics / aquatic sciences</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict vertical water temperature profiles over time given meteorological drivers and lake-specific static parameters, where mechanistic models are imperfect and observed data are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>limited — observed in-situ monitoring data are often scarce; synthetic data from mechanistic lake models are available and used for pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>multivariate time series with spatial (vertical depth) dimension; small labeled observation sets plus larger simulated datasets</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>moderate-to-high nonlinearity and temporal dynamics; multiple interacting physical drivers and state variables; some latent physical quantities (lake energy) computed by physics models expand state dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established mechanistic models exist (lake thermal models) but imperfect for many processes; domain expertise available to encode physical relationships (e.g., density-depth monotonicity, energy conservation).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high — interpretable and physically consistent predictions are required for scientific validity and generalization; physics constraints (energy/mass conservation, monotonicity) enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-Guided Recurrent Neural Network (PGRNN) with physics-based loss and physics-guided initialization (transfer learning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>An RNN/LSTM architecture augmented with explicitly computed intermediate physical variables (e.g., thermal energy) whose values are computed by physics equations and used in the loss; the loss combines supervised error (RMSE) with physics-based penalties (e.g., energy conservation, monotonic density-depth); model is pre-trained on large amounts of synthetic data produced by a mechanistic lake model and fine-tuned on limited observations to improve sample efficiency and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>physics-informed ML / hybrid (physics-guided initialization + physics-guided loss + architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate — leverages available mechanistic simulations for pre-training; physics-based loss improves physical consistency and generalization; requires mechanistic model access for pre-training and domain expertise to specify physical penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to achieve better predictive accuracy with far fewer real observations than black-box ML and to generalize better out-of-sample compared to the mechanistic model alone; pre-training on imperfect mechanistic simulations still substantially reduces required labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for operational monitoring and forecasting of lakes where observations are limited — reduces data collection needs and improves trustworthy predictions; approach generalizes to other environmental dynamical systems with mechanistic simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to pure physics-based models and pure data-driven RNNs; physics-guided variants (initialization + physics loss) show superior generalization and sample efficiency relative to black-box ML and better out-of-sample performance than the mechanistic model alone.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of a mechanistic simulator for pre-training, ability to express key conservation laws/monotonicities as differentiable loss terms, and small number of critical intermediate physical variables to embed in architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Combining mechanistic-simulator pre-training with physics-based loss and architecture yields large gains in sample efficiency and out-of-sample robustness for dynamical environmental prediction tasks where observations are scarce.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2324.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PINNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural networks trained with loss terms that enforce differential equations (PDEs/ODEs) and boundary/initial conditions so the network approximates solutions to forward and inverse PDE problems using data and known governing equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>general dynamical systems / PDE-based physics (fluid dynamics, quantum mechanics, diffusion, Burgers/Schrodinger equations etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Solve forward PDEs (compute state evolution) or inverse PDE problems (infer parameters or sources) where the governing differential equations are known but traditional numerical solvers are expensive or data are scarce/noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>varies — can work with limited labeled observations because the PDE residual provides supervision; synthetic simulation data often available; unlabeled collocation points used to compute PDE loss.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>continuous spatiotemporal fields; high-dimensional function spaces; sometimes pointwise measurements</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — high-dimensional function approximation, nonlinear PDEs, stiff dynamics; training can suffer from slow convergence and gradient pathologies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging as an active research area with many recent studies; solid theoretical basis in using PDE residuals as regularization; domain knowledge (equations) must be well-specified.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high — explicit mechanistic equations are required and are integrated as hard/soft constraints to obtain physically plausible solutions and enable inference of parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-Informed Neural Networks (PINNs) and variants (Bayesian PINNs, adversarial PINNs, Fourier/Neural Operator methods)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Feed-forward or auto-regressive neural networks trained with composite loss: supervised data loss + PDE residual loss (evaluated at collocation points) + boundary/initial condition penalties; variants include Bayesian formulations for UQ, GAN-based PINNs for stochastic PDEs, and encoder-decoder architectures; training typically uses gradient-based optimizers but can suffer from stiffness and requires careful weighting of loss terms.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>physics-informed ML / supervised + constraint-based unsupervised components</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable when governing equations are known and numerical solvers are expensive; constraints help learn from scarce data; limited by training stability and scalability to very complex systems without additional modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated to solve canonical PDEs (Burgers, Schrodinger), perform inverse parameter estimation, and provide data-efficient function approximation; challenges include slow training convergence, gradient pathologies, and limited applicability in some complex systems.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — offers a route to bypass expensive numerical solvers and to perform parameter inference and UQ for PDE-governed systems; potential to accelerate many scientific simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to standard numerical solvers (FEM/FD) which are often more accurate but computationally expensive; PINNs provide differentiable closed-form approximations but can be less robust and harder to train.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability and correctness of governing equations, ability to design and weight PDE residual loss appropriately, and advances in architectures/optimizers (e.g., Fourier neural operators) to scale to families of PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Encoding PDE residuals into the loss enables neural networks to approximate forward and inverse PDE solutions with much greater data efficiency than pure supervised learning, but practical success depends on training stability and appropriate loss balancing.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2324.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Residual Hybrid Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Residual (Error) Modeling for Hybrid Physics-ML</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach where an ML model is trained to predict and correct the residual errors of an existing mechanistic model, producing final predictions as mechanistic output plus learned correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>broad — applied in reduced-order modeling, climate/hydrology, fluid dynamics, and other simulation-dependent domains</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve imperfect mechanistic model outputs by learning systematic biases/residuals relative to observations and correcting them, often when the mechanistic model is operationally available at run time.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>often limited observational data for bias estimation; mechanistic model outputs (synthetic data) are available in abundance; labeled residuals require matched observation-model pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>model outputs and observational time series/fields (structured spatiotemporal data); reduced-order coordinate representations when applied in ROMs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>varies — can be high when residual patterns are complex, non-stationary, or state-dependent; residuals may capture multiscale and nonlinear effects.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established heuristic in scientific modeling communities; many domain-specific applications but limited in guaranteeing physical consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — some mechanistic model must exist; interpretability desirable but residual approach often treats residual as black-box correction.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Residual modeling (ML correction of mechanistic model outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train an ML model (from linear regression to deep NNs/RNNs) on inputs consisting of mechanistic model outputs and/or original drivers to predict the residual/error between mechanistic output and observations; corrected output = mechanistic output + ML-predicted residual. Variants embed residual modeling inside ROMs or use DR-RNN architectures to iteratively minimize discretized residuals.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid physics-ML / supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Simple and broadly applicable when an operational mechanistic model exists; limited when physical constraints on corrected quantity must be strictly enforced because residual models operate on errors rather than physical state directly.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective at reducing systematic biases and improving predictive skill in many contexts (e.g., ROM truncation error reduction, turbulence model discrepancy correction); main limitation is inability to directly enforce physical laws in corrected outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High operational impact because it can be deployed without replacing existing simulators and can yield rapid improvements in predictive performance and efficiency (especially in ROM contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Simpler than architecting fully physics-informed ML; compared to full replacement ML surrogates, residual modeling maintains mechanistic backbone but cannot impose hard physical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of paired observation and mechanistic output data; residuals that are learnable (i.e., have structure); careful model selection to avoid overfitting to noise.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Modeling residuals is a low-friction path to improve mechanistic simulations’ accuracy, but because it operates on errors rather than states, it is harder to ensure physical consistency and interpretability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2324.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier Neural Operator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Fourier Operator / Fourier Neural Operator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural operator architecture that learns mappings between function spaces (e.g., PDE input parameter fields to solution fields) by learning in Fourier space and generalizes across families of PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fourier neural operator for parametric partial differential equations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>parametric PDE solving across fluid dynamics and other PDE-governed systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn a single learned operator that maps a family of input functions/parameters to their corresponding PDE solution functions, enabling fast evaluation across parameter instances.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>requires simulation data across the parameter family for training (synthetic simulation datasets); data per-parameter can be moderate to large depending on dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>high-dimensional functional inputs and outputs represented on grids (spatiotemporal fields); continuous function spaces discretized for learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high dimensionality and nonlinearity; goal is to approximate an operator rather than a single solution which increases complexity but yields generalization benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>recent and rapidly developing; shows promise for families of PDEs where traditional solvers are expensive to run for many parameter instances.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — governing PDE family is known and used conceptually, but method learns the mapping empirically from data rather than solving PDEs numerically at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Fourier Neural Operator (neural operator learning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural network architecture that applies convolution-like operations in the Fourier domain to learn global integral operators; trained supervised on pairs of input functional parameters and corresponding PDE solution fields to generalize to unseen parameter instances; enables amortized inference for entire families of PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / operator learning / physics-informed ML (architecture informed by PDE function spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for scenarios requiring repeated solves of parametric PDEs (UQ, optimization, inverse problems); requires representative training across the parameter family; not a drop-in for single-instance high-accuracy solves.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to achieve greater accuracy and generalization over other ML-based PDE solvers in some tasks and to solve entire families of PDEs rather than single problems, offering major speedups at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for accelerating parametric studies, surrogate modeling, and inverse problems where many PDE solves are needed; can replace costly solvers for many-query tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms some previous NN-based PDE solvers in certain benchmarks and extends beyond per-instance PINNs by learning whole operator families.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of diverse simulation data spanning the parameter family, and architectures that exploit global Fourier-domain structure to capture long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Learning operators (mappings between function spaces) in Fourier space enables efficient amortized PDE solving across parameter families, offering major acceleration for many-query scientific workflows.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2324.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-Informed GANs / Generative Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Generative Adversarial Networks and Variational Autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative models (GANs, VAEs, flow models) that incorporate physics constraints (e.g., conservation laws, PDE structure, invariance) in loss functions or architectures to generate physically realistic synthetic data and accelerate simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-informed generative adversarial networks for stochastic differential equations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>data generation and surrogate simulation across fluids, materials, climate, and PDE-driven systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate synthetic realizations of system states or surrogate simulation outputs that match the distribution (and physical constraints) of real/simulated data to supplement scarce data or speed up expensive simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>often scarce for observed data; simulation outputs are used to train generative models; GANs have high sample complexity absent physics guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>high-dimensional fields/images (e.g., turbulence snapshots, microstructure images), spatiotemporal data; multimodal in some settings</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — capturing complex nonlinear spatiotemporal statistics and higher-order invariants (energy spectrum, conservation) is challenging; generative training is unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging — many proof-of-concept studies show promise but practical robustness and physical guarantees remain active research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — physical constraints are used to regularize generation (as soft loss terms or architecture constraints) to ensure realism; interpretability of latent representations is desirable.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-informed GANs and VAEs (conditional GANs, physics-constrained generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Generative adversarial frameworks extended with physics-based penalties (e.g., conservation laws, energy spectra) added to discriminator/generator losses, morphology constraints for VAEs, or conditioning on physical variables; used as surrogates to generate solutions or data for UQ and design.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised / generative modeling with physics-informed constraints</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when realistic synthetic samples are needed to augment scarce data or when simulations are too slow; must balance GAN instability and sample complexity with physics-based regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Physics-augmented generative models can significantly reduce training data needs and produce physically-consistent synthetic samples; however, vanilla GANs suffer from high sample complexity and mode collapse without physics-informed terms.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential for accelerating design (materials, molecules), enabling surrogate sampling for UQ, and augmenting datasets for downstream ML tasks; can reduce computation time compared to full physics simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to pure simulation, generative models are much faster at inference but require careful training; physics-informed variants outperform unconstrained GANs in preserving physical statistics and reducing required data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Incorporating domain-specific physical constraints into loss/architecture (e.g., conservation laws, energy spectra), and availability of representative simulation/observation data for conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding physics constraints into generative models significantly improves realism and sample efficiency, making GANs/VAEs more practical as surrogates for expensive scientific simulators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2324.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-Guided Initialization / Transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Guided Initialization and Simulation-Based Transfer Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-training ML models on synthetic data generated by mechanistic simulators (or using self-supervised physics pretext tasks) to yield informed initial weights that dramatically reduce required real-world training data and speed convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>robotics, lake temperature modeling, chemical process modeling, autonomous vehicles, and other simulation-amenable domains</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reduce real-world data needs and accelerate model training by initializing ML models using weights learned from simulator data or physics-based self-supervised tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>simulator-generated synthetic data is typically abundant; real-world labeled data scarce and expensive to collect.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>varies — images (robotics), time series (lake temperature), process variables (chemical), etc.; often structured but domain-specific</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>varies by domain; transfer learning reduces optimization difficulty and effective search space complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>practiced in many applied ML areas (vision, robotics) and increasingly used in scientific ML; domain expertise needed to design suitable simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-to-medium — needs at least a simulator capable of producing relevant intermediate variables or scenarios; does not require full interpretability at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Simulation-based pre-training / transfer learning / self-supervised physics pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Pre-train neural network weights on large synthetic datasets produced by mechanistic models or on self-supervised tasks predicting physics-derived intermediate variables; then fine-tune on limited observed data, optionally with physics-guided loss; can include scale-bias adaptation for Gaussian processes or ensemble transfer for NNs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>transfer learning / self-supervised learning / physics-guided ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable where credible simulators exist; reduces labeled data requirements but depends on simulator realism and domain gap handling (domain adaptation may be needed).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported substantial reductions in required real data (e.g., robotics tasks show large reductions; lake modeling shows significant data savings even when simulator parameters are imperfect); effectiveness depends on simulator fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — lowers experimental/measurement costs, speeds development cycles, and enables ML deployment in data-poor scientific settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms random initialization and purely data-driven training when real labeled data are limited; domain adaptation or physics-guided regularization often needed to bridge simulator-reality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of realistic simulators, careful selection of pre-training tasks, and techniques for bridging simulation-to-reality distribution shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Using mechanistic simulators to initialize ML models is an effective way to encode prior physical structure into weights and drastically reduce the need for expensive observed data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2324.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tensor-Basis NN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tensor-Basis Neural Network with Embedded Invariances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural network architecture that enforces rotational (and other) invariances by constructing predictions on a rotationally invariant tensor basis, used to model turbulence closure terms and other physics where symmetry matters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>turbulence modeling / fluid dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict turbulence closure terms or constitutive relations that must satisfy rotational invariance and other symmetries to produce physically consistent turbulence model corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>typically limited high-fidelity DNS/LES datasets for training; synthetic numerical data can be generated but expensive; data often high-dimensional spatial fields.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>spatiotemporal fields and tensors; vector and tensor-valued inputs/outputs requiring equivariance/invariance handling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very high nonlinearity, tensor-valued quantities, sensitivity to coordinate transformations; large search spaces if invariance not enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>advanced field with well-established theoretical constraints (symmetries/invariances); ML architectures embedding invariances are an active research area.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high — preserving known symmetries and invariances is essential for physically meaningful and generalizable models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Tensor-basis neural networks / invariance-embedded architectures</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Architectural modification adding multiplicative layers and tensor bases to ensure outputs lie in an invariance-preserving basis (e.g., rotational invariance), often using physics-derived input invariants and basis functions so that network predictions respect symmetry properties across coordinate frames.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>physics-guided architecture / supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly appropriate where symmetry constraints are critical (e.g., turbulence); enforces physically necessary invariances and improves generalization across coordinate transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated improved prediction accuracy and robustness compared to naive NN architectures that do not enforce invariances; avoids inconsistent predictions under coordinate rotations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant for improving ML-based turbulence closures and other physics tasks where symmetry preservation is central to predictive fidelity and transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms standard NNs that ignore invariances on tasks requiring symmetry; provides theoretical guarantees of invariance absent in unconstrained architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Correct identification and encoding of the relevant invariances, careful architectural design to implement invariance, and access to training data that exercises different orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding physical symmetries into architecture reduces search space, improves generalization, and ensures consistency under coordinate transformations—crucial for reliable physics ML in domains like turbulence.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2324.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hamiltonian NN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hamiltonian Neural Networks (HNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural network architectures that learn a system's Hamiltonian (energy function) and integrate dynamics via symplectic structure to enforce conservation laws (e.g., energy conservation) and improve long-term predictive stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hamiltonian neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>classical mechanics, dynamical systems, and physics problems with conserved quantities (e.g., pendulum, mass-spring)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Model time evolution of conservative dynamical systems while respecting conserved quantities like energy, improving long-term accuracy and preserving physical invariants.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>often limited trajectories or observational sequences; can be fully supervised on state/time-series data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>time series of state vectors (positions, momenta); low-to-moderate dimensionality typical for physically motivated systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>nonlinear dynamics with conserved structure; capturing long-term behavior requires preserving invariants and avoiding numerical dissipation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging research area integrating classical mechanics with NN architectures; several proofs-of-concept published.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high — approach explicitly leverages the Hamiltonian formalism and requires the notion of conserved energy/phase space structure to be meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Hamiltonian Neural Networks and Hamiltonian-parameterized architectures</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>NNs predict a scalar Hamiltonian function of state; time derivatives computed via automatic differentiation and canonical Hamilton's equations yield dynamics; architectures can learn latent phase space mappings when explicit coordinates are unknown, and integration schemes (symplectic) are used to propagate states.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>physics-guided architecture / supervised learning with structure</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited to conservative dynamical systems where conservation laws are central; less applicable when strong dissipation or non-Hamiltonian forcing dominates the dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Shown to preserve energy and exhibit superior long-term stability versus unconstrained NNs when modeling conservative systems; enables interpretability in terms of learned energy functions.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for modeling and control of systems where invariant preservation is necessary for physically realistic simulation and long-term forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms unconstrained predictive NNs in preserving invariants and long-term stability; complements (rather than replaces) PINNs for systems where Hamiltonian structure is present.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Presence of Hamiltonian structure in the true system, ability to represent Hamiltonian as a differentiable function of states, and appropriate integrators that maintain symplecticity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Parameterizing neural dynamics via learned Hamiltonians enforces conservation laws and yields more physically faithful and stable long-term predictions for conservative dynamical systems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2324.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Koopman Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Koopman Operator Embeddings / Koopman-based ROMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use deep learning to approximate finite-dimensional embeddings of the Koopman operator, enabling linear analysis of nonlinear dynamical systems and construction of reduced-order models with improved forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>dynamical systems, fluid dynamics, oceanography, molecular dynamics, and other time-evolving physical systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Find a (possibly learned) observable mapping under which the nonlinear dynamics evolve approximately linearly according to Koopman operator modes to enable dimensionality reduction and linear forecasting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>depends — training requires time-series data from dynamical systems; high-fidelity simulation or observational trajectories are used when available.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>high-dimensional time series / state trajectories; may be spatial fields compressed by POD or autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — nonlinear, possibly chaotic dynamics with multiscale features; embedding dimension and dictionary selection create large search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>active research area; classical DMD methods exist but deep learning-based Koopman embeddings are recent and promising.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — does not require explicit governing equations but benefits from physical insight for choosing observables or regularization; interpretability desired.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep Koopman operator learning / Koopman autoencoders / DMD with learned dictionaries</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Autoencoders or NNs learn an observation map to a latent space where a linear operator (approximating Koopman operator) advances dynamics; training optimizes reconstruction loss plus linearity/consistency penalties across time steps; used to extract dominant modes and build ROMs for forecasting and control.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised / representation learning for dynamical systems / hybrid ROM</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Suitable when time-series data are available and linear embedding can capture dominant dynamics; challenges arise for strongly nonlinear or non-ergodic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Deep Koopman embeddings can outperform classical DMD and improve long-term forecasting and reduction for complex systems; physics-informed regularization can improve interpretability and generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for model reduction, forecasting and control in large dynamical systems where linear analysis simplifies downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers more expressive embeddings than linear DMD; deep methods can capture complex observables but require careful regularization to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sufficiently rich trajectory data, appropriate choices of latent dimension and regularization, and incorporation of physics-based priors to guide embedding learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Learning Koopman-compatible embeddings with deep networks provides a bridge between nonlinear dynamics and linear operator theory, enabling effective reduced-order forecasting when guided by physics or regularization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2324.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Equation Discovery / Sparse ID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Identification of Nonlinear Dynamics / Symbolic Regression for Governing Equation Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Techniques that build libraries/dictionaries of candidate functions (including derivatives) and use sparse regression or symbolic methods to identify parsimonious governing equations from data, often augmented by physics constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering governing equations from data by sparse identification of nonlinear dynamical systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>dynamical systems across ecology, fluid dynamics, biological transport, and general physics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given observational time-series or spatiotemporal data, infer the underlying differential or algebraic equations (symbolic form) that govern system evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>often abundant time-series data in some domains but data may be noisy; discovery methods can be sensitive to noise and require derivative estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>time-series and spatiotemporal fields; requires compute of derivatives (finite-difference or smoothing) or auto-differentiation-enabled representations</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>search over combinatorially large space of symbolic terms/operators; NP-hard in general without structure/expert priors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>rapidly maturing with key contributions (sparse regression, symbolic regression, AI Feynman); increasingly combined with ML to scale discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium-to-high — domain expertise helps define candidate function dictionaries and constraints; interpretable mechanistic understanding is the primary goal.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Sparse regression (SINDy), symbolic regression, neural-network-assisted dictionary learning, AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Construct a library of candidate functions (polynomials, trig functions, derivatives) and solve a sparsity-promoting regression (e.g., LASSO) to select a small set of terms that reconstruct observed derivatives; recent variants use neural nets to construct dictionaries, enforce physics constraints, or pre-screen for symmetry/separability to reduce search space.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>symbolic / interpretable ML / sparse learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for systems where parsimonious analytic descriptions exist and adequate data (with good derivative estimates) are available; limited in high-noise or very high-dimensional settings without additional priors.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Successfully rediscovers known PDEs/ODEs in benchmark cases and produces interpretable governing equations; performance degrades with noise and insufficient coverage of dynamics unless physics priors or regularization applied.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for scientific discovery — can produce new interpretable laws, enable analysis of stability and generalization beyond training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More interpretable than black-box ML approaches and more data-efficient when a sparse true model exists; contrasts with PINNs which assume the equation structure is known.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality data permitting derivative estimation, choice of appropriate function dictionary, use of physical constraints (e.g., conservation) to regularize selection, and techniques to mitigate noise.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Sparse and symbolic regression methods can recover interpretable governing equations from data when the true dynamics are parsimonious and data quality suffices, and physics priors greatly reduce the combinatorial search burden.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2324.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2324.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-Guided UQ (Bayesian NNs, Ensembles, Dropout)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Guided Uncertainty Quantification via Bayesian Neural Networks, Ensembles, and Physics-Regularized Approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that provide uncertainty estimates for ML surrogates used in scientific modeling by combining Bayesian neural networks, dropout-as-Bayesian approximations, deep ensembles, or physics-regularized priors to constrain uncertainty in physically-consistent ways.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dropout as a bayesian approximation: Representing model uncertainty in deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>uncertainty quantification for climate, fluid flow, engineering systems, PDE solutions and surrogate modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Quantify predictive uncertainty (full predictive distribution) of response variables predicted by ML surrogates for scientific decision-making, where UQ is critical but Monte Carlo on high-fidelity simulators is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>often limited labeled data; plentiful simulator data may exist but MC over simulator is computationally prohibitive; UQ methods must handle small data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>various (fields, time series, parameter vectors); surrogate models map inputs to outputs with associated uncertainty estimates</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high-dimensional outputs and inputs; need calibrated uncertainty estimates under model mismatch and out-of-sample scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-studied in statistics and ML; physics integration into UQ (e.g., physics-informed priors) is active research to improve calibration and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — physics-based priors or constraints as regularizers can improve calibration and physical consistency of predicted uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Bayesian neural networks, MC dropout, deep ensembles, physics-guided Bayesian priors, adversarial inference for generative UQ</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Apply approximate Bayesian techniques (dropout as variational bayes), full Bayesian NN formulations, or ensembles to produce predictive distributions; incorporate physics as priors or PDE constraints in training to regularize uncertainty estimates and avoid physically inconsistent predictions; use generative adversarial inference for probabilistic modeling of system states.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Bayesian / probabilistic ML / physics-informed UQ</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and necessary for scientific forecasting and decision tasks; computational cost and prior specification are challenges (Bayesian NNs can be expensive and sensitive to priors).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Physics-guided uncertainty methods help produce more physically-consistent uncertainty estimates and reduce data demands relative to purely data-driven Bayesian models; ensemble/dropout approaches provide scalable approximations while Bayesian NNs offer principled uncertainty at higher cost.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — credible UQ enables risk-aware decision-making, robust forecasting, and better resource allocation in scientific and engineering contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Gaussian processes have strong UQ properties but scale poorly; deep probabilistic models and ensembles scale better but need physics-based regularization to avoid physically-inconsistent uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Appropriate physics-informed priors or loss constraints, scalable approximate inference methods, and ensembles to balance scalability and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Incorporating physics as priors or constraints into ML-based UQ improves physical plausibility and data efficiency of uncertainty estimates, addressing limitations of purely data-driven Bayesian surrogates in scientific domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations <em>(Rating: 2)</em></li>
                <li>Physics-guided neural networks (PGNN): An application in lake temperature modeling <em>(Rating: 2)</em></li>
                <li>Discovering governing equations from data by sparse identification of nonlinear dynamical systems <em>(Rating: 2)</em></li>
                <li>Fourier neural operator for parametric partial differential equations <em>(Rating: 2)</em></li>
                <li>Hamiltonian neural networks <em>(Rating: 1)</em></li>
                <li>Physics-informed generative adversarial networks for stochastic differential equations <em>(Rating: 1)</em></li>
                <li>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance <em>(Rating: 1)</em></li>
                <li>Physics-informed deep generative models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2324",
    "paper_id": "paper-236318599",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "PGRNN",
            "name_full": "Physics-Guided Recurrent Neural Network",
            "brief_description": "A recurrent neural network augmented with physics-based intermediate variables and physics-derived loss terms (e.g., energy conservation) and pre-training on mechanistic model simulations to improve lake temperature predictions with far fewer observations and better out-of-sample generalization.",
            "citation_title": "Physics Guided RNNs for Modeling Dynamical Systems: A Case Study in Simulating Lake Temperature Profiles",
            "mention_or_use": "use",
            "scientific_problem_domain": "lake temperature dynamics / aquatic sciences",
            "problem_description": "Predict vertical water temperature profiles over time given meteorological drivers and lake-specific static parameters, where mechanistic models are imperfect and observed data are limited.",
            "data_availability": "limited — observed in-situ monitoring data are often scarce; synthetic data from mechanistic lake models are available and used for pre-training.",
            "data_structure": "multivariate time series with spatial (vertical depth) dimension; small labeled observation sets plus larger simulated datasets",
            "problem_complexity": "moderate-to-high nonlinearity and temporal dynamics; multiple interacting physical drivers and state variables; some latent physical quantities (lake energy) computed by physics models expand state dimensionality.",
            "domain_maturity": "well-established mechanistic models exist (lake thermal models) but imperfect for many processes; domain expertise available to encode physical relationships (e.g., density-depth monotonicity, energy conservation).",
            "mechanistic_understanding_requirements": "high — interpretable and physically consistent predictions are required for scientific validity and generalization; physics constraints (energy/mass conservation, monotonicity) enforced.",
            "ai_methodology_name": "Physics-Guided Recurrent Neural Network (PGRNN) with physics-based loss and physics-guided initialization (transfer learning)",
            "ai_methodology_description": "An RNN/LSTM architecture augmented with explicitly computed intermediate physical variables (e.g., thermal energy) whose values are computed by physics equations and used in the loss; the loss combines supervised error (RMSE) with physics-based penalties (e.g., energy conservation, monotonic density-depth); model is pre-trained on large amounts of synthetic data produced by a mechanistic lake model and fine-tuned on limited observations to improve sample efficiency and generalization.",
            "ai_methodology_category": "physics-informed ML / hybrid (physics-guided initialization + physics-guided loss + architecture)",
            "applicability": "Applicable and appropriate — leverages available mechanistic simulations for pre-training; physics-based loss improves physical consistency and generalization; requires mechanistic model access for pre-training and domain expertise to specify physical penalties.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to achieve better predictive accuracy with far fewer real observations than black-box ML and to generalize better out-of-sample compared to the mechanistic model alone; pre-training on imperfect mechanistic simulations still substantially reduces required labeled data.",
            "impact_potential": "High for operational monitoring and forecasting of lakes where observations are limited — reduces data collection needs and improves trustworthy predictions; approach generalizes to other environmental dynamical systems with mechanistic simulators.",
            "comparison_to_alternatives": "Compared qualitatively to pure physics-based models and pure data-driven RNNs; physics-guided variants (initialization + physics loss) show superior generalization and sample efficiency relative to black-box ML and better out-of-sample performance than the mechanistic model alone.",
            "success_factors": "Availability of a mechanistic simulator for pre-training, ability to express key conservation laws/monotonicities as differentiable loss terms, and small number of critical intermediate physical variables to embed in architecture.",
            "key_insight": "Combining mechanistic-simulator pre-training with physics-based loss and architecture yields large gains in sample efficiency and out-of-sample robustness for dynamical environmental prediction tasks where observations are scarce.",
            "uuid": "e2324.0"
        },
        {
            "name_short": "PINNs",
            "name_full": "Physics-Informed Neural Networks",
            "brief_description": "Neural networks trained with loss terms that enforce differential equations (PDEs/ODEs) and boundary/initial conditions so the network approximates solutions to forward and inverse PDE problems using data and known governing equations.",
            "citation_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "mention_or_use": "mention",
            "scientific_problem_domain": "general dynamical systems / PDE-based physics (fluid dynamics, quantum mechanics, diffusion, Burgers/Schrodinger equations etc.)",
            "problem_description": "Solve forward PDEs (compute state evolution) or inverse PDE problems (infer parameters or sources) where the governing differential equations are known but traditional numerical solvers are expensive or data are scarce/noisy.",
            "data_availability": "varies — can work with limited labeled observations because the PDE residual provides supervision; synthetic simulation data often available; unlabeled collocation points used to compute PDE loss.",
            "data_structure": "continuous spatiotemporal fields; high-dimensional function spaces; sometimes pointwise measurements",
            "problem_complexity": "high — high-dimensional function approximation, nonlinear PDEs, stiff dynamics; training can suffer from slow convergence and gradient pathologies.",
            "domain_maturity": "emerging as an active research area with many recent studies; solid theoretical basis in using PDE residuals as regularization; domain knowledge (equations) must be well-specified.",
            "mechanistic_understanding_requirements": "high — explicit mechanistic equations are required and are integrated as hard/soft constraints to obtain physically plausible solutions and enable inference of parameters.",
            "ai_methodology_name": "Physics-Informed Neural Networks (PINNs) and variants (Bayesian PINNs, adversarial PINNs, Fourier/Neural Operator methods)",
            "ai_methodology_description": "Feed-forward or auto-regressive neural networks trained with composite loss: supervised data loss + PDE residual loss (evaluated at collocation points) + boundary/initial condition penalties; variants include Bayesian formulations for UQ, GAN-based PINNs for stochastic PDEs, and encoder-decoder architectures; training typically uses gradient-based optimizers but can suffer from stiffness and requires careful weighting of loss terms.",
            "ai_methodology_category": "physics-informed ML / supervised + constraint-based unsupervised components",
            "applicability": "Highly applicable when governing equations are known and numerical solvers are expensive; constraints help learn from scarce data; limited by training stability and scalability to very complex systems without additional modifications.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated to solve canonical PDEs (Burgers, Schrodinger), perform inverse parameter estimation, and provide data-efficient function approximation; challenges include slow training convergence, gradient pathologies, and limited applicability in some complex systems.",
            "impact_potential": "High — offers a route to bypass expensive numerical solvers and to perform parameter inference and UQ for PDE-governed systems; potential to accelerate many scientific simulation tasks.",
            "comparison_to_alternatives": "Compared qualitatively to standard numerical solvers (FEM/FD) which are often more accurate but computationally expensive; PINNs provide differentiable closed-form approximations but can be less robust and harder to train.",
            "success_factors": "Availability and correctness of governing equations, ability to design and weight PDE residual loss appropriately, and advances in architectures/optimizers (e.g., Fourier neural operators) to scale to families of PDEs.",
            "key_insight": "Encoding PDE residuals into the loss enables neural networks to approximate forward and inverse PDE solutions with much greater data efficiency than pure supervised learning, but practical success depends on training stability and appropriate loss balancing.",
            "uuid": "e2324.1"
        },
        {
            "name_short": "Residual Hybrid Modeling",
            "name_full": "Residual (Error) Modeling for Hybrid Physics-ML",
            "brief_description": "A hybrid approach where an ML model is trained to predict and correct the residual errors of an existing mechanistic model, producing final predictions as mechanistic output plus learned correction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "broad — applied in reduced-order modeling, climate/hydrology, fluid dynamics, and other simulation-dependent domains",
            "problem_description": "Improve imperfect mechanistic model outputs by learning systematic biases/residuals relative to observations and correcting them, often when the mechanistic model is operationally available at run time.",
            "data_availability": "often limited observational data for bias estimation; mechanistic model outputs (synthetic data) are available in abundance; labeled residuals require matched observation-model pairs.",
            "data_structure": "model outputs and observational time series/fields (structured spatiotemporal data); reduced-order coordinate representations when applied in ROMs",
            "problem_complexity": "varies — can be high when residual patterns are complex, non-stationary, or state-dependent; residuals may capture multiscale and nonlinear effects.",
            "domain_maturity": "well-established heuristic in scientific modeling communities; many domain-specific applications but limited in guaranteeing physical consistency.",
            "mechanistic_understanding_requirements": "medium — some mechanistic model must exist; interpretability desirable but residual approach often treats residual as black-box correction.",
            "ai_methodology_name": "Residual modeling (ML correction of mechanistic model outputs)",
            "ai_methodology_description": "Train an ML model (from linear regression to deep NNs/RNNs) on inputs consisting of mechanistic model outputs and/or original drivers to predict the residual/error between mechanistic output and observations; corrected output = mechanistic output + ML-predicted residual. Variants embed residual modeling inside ROMs or use DR-RNN architectures to iteratively minimize discretized residuals.",
            "ai_methodology_category": "hybrid physics-ML / supervised learning",
            "applicability": "Simple and broadly applicable when an operational mechanistic model exists; limited when physical constraints on corrected quantity must be strictly enforced because residual models operate on errors rather than physical state directly.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Effective at reducing systematic biases and improving predictive skill in many contexts (e.g., ROM truncation error reduction, turbulence model discrepancy correction); main limitation is inability to directly enforce physical laws in corrected outputs.",
            "impact_potential": "High operational impact because it can be deployed without replacing existing simulators and can yield rapid improvements in predictive performance and efficiency (especially in ROM contexts).",
            "comparison_to_alternatives": "Simpler than architecting fully physics-informed ML; compared to full replacement ML surrogates, residual modeling maintains mechanistic backbone but cannot impose hard physical constraints.",
            "success_factors": "Availability of paired observation and mechanistic output data; residuals that are learnable (i.e., have structure); careful model selection to avoid overfitting to noise.",
            "key_insight": "Modeling residuals is a low-friction path to improve mechanistic simulations’ accuracy, but because it operates on errors rather than states, it is harder to ensure physical consistency and interpretability.",
            "uuid": "e2324.2"
        },
        {
            "name_short": "Fourier Neural Operator",
            "name_full": "Neural Fourier Operator / Fourier Neural Operator",
            "brief_description": "A neural operator architecture that learns mappings between function spaces (e.g., PDE input parameter fields to solution fields) by learning in Fourier space and generalizes across families of PDEs.",
            "citation_title": "Fourier neural operator for parametric partial differential equations",
            "mention_or_use": "mention",
            "scientific_problem_domain": "parametric PDE solving across fluid dynamics and other PDE-governed systems",
            "problem_description": "Learn a single learned operator that maps a family of input functions/parameters to their corresponding PDE solution functions, enabling fast evaluation across parameter instances.",
            "data_availability": "requires simulation data across the parameter family for training (synthetic simulation datasets); data per-parameter can be moderate to large depending on dimensionality.",
            "data_structure": "high-dimensional functional inputs and outputs represented on grids (spatiotemporal fields); continuous function spaces discretized for learning",
            "problem_complexity": "high dimensionality and nonlinearity; goal is to approximate an operator rather than a single solution which increases complexity but yields generalization benefits.",
            "domain_maturity": "recent and rapidly developing; shows promise for families of PDEs where traditional solvers are expensive to run for many parameter instances.",
            "mechanistic_understanding_requirements": "medium — governing PDE family is known and used conceptually, but method learns the mapping empirically from data rather than solving PDEs numerically at inference time.",
            "ai_methodology_name": "Fourier Neural Operator (neural operator learning)",
            "ai_methodology_description": "Neural network architecture that applies convolution-like operations in the Fourier domain to learn global integral operators; trained supervised on pairs of input functional parameters and corresponding PDE solution fields to generalize to unseen parameter instances; enables amortized inference for entire families of PDEs.",
            "ai_methodology_category": "supervised learning / operator learning / physics-informed ML (architecture informed by PDE function spaces)",
            "applicability": "Appropriate for scenarios requiring repeated solves of parametric PDEs (UQ, optimization, inverse problems); requires representative training across the parameter family; not a drop-in for single-instance high-accuracy solves.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to achieve greater accuracy and generalization over other ML-based PDE solvers in some tasks and to solve entire families of PDEs rather than single problems, offering major speedups at inference.",
            "impact_potential": "High for accelerating parametric studies, surrogate modeling, and inverse problems where many PDE solves are needed; can replace costly solvers for many-query tasks.",
            "comparison_to_alternatives": "Outperforms some previous NN-based PDE solvers in certain benchmarks and extends beyond per-instance PINNs by learning whole operator families.",
            "success_factors": "Availability of diverse simulation data spanning the parameter family, and architectures that exploit global Fourier-domain structure to capture long-range dependencies.",
            "key_insight": "Learning operators (mappings between function spaces) in Fourier space enables efficient amortized PDE solving across parameter families, offering major acceleration for many-query scientific workflows.",
            "uuid": "e2324.3"
        },
        {
            "name_short": "Physics-Informed GANs / Generative Models",
            "name_full": "Physics-Informed Generative Adversarial Networks and Variational Autoencoders",
            "brief_description": "Generative models (GANs, VAEs, flow models) that incorporate physics constraints (e.g., conservation laws, PDE structure, invariance) in loss functions or architectures to generate physically realistic synthetic data and accelerate simulation.",
            "citation_title": "Physics-informed generative adversarial networks for stochastic differential equations",
            "mention_or_use": "mention",
            "scientific_problem_domain": "data generation and surrogate simulation across fluids, materials, climate, and PDE-driven systems",
            "problem_description": "Generate synthetic realizations of system states or surrogate simulation outputs that match the distribution (and physical constraints) of real/simulated data to supplement scarce data or speed up expensive simulations.",
            "data_availability": "often scarce for observed data; simulation outputs are used to train generative models; GANs have high sample complexity absent physics guidance.",
            "data_structure": "high-dimensional fields/images (e.g., turbulence snapshots, microstructure images), spatiotemporal data; multimodal in some settings",
            "problem_complexity": "high — capturing complex nonlinear spatiotemporal statistics and higher-order invariants (energy spectrum, conservation) is challenging; generative training is unstable.",
            "domain_maturity": "emerging — many proof-of-concept studies show promise but practical robustness and physical guarantees remain active research areas.",
            "mechanistic_understanding_requirements": "medium — physical constraints are used to regularize generation (as soft loss terms or architecture constraints) to ensure realism; interpretability of latent representations is desirable.",
            "ai_methodology_name": "Physics-informed GANs and VAEs (conditional GANs, physics-constrained generative models)",
            "ai_methodology_description": "Generative adversarial frameworks extended with physics-based penalties (e.g., conservation laws, energy spectra) added to discriminator/generator losses, morphology constraints for VAEs, or conditioning on physical variables; used as surrogates to generate solutions or data for UQ and design.",
            "ai_methodology_category": "unsupervised / generative modeling with physics-informed constraints",
            "applicability": "Applicable when realistic synthetic samples are needed to augment scarce data or when simulations are too slow; must balance GAN instability and sample complexity with physics-based regularization.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Physics-augmented generative models can significantly reduce training data needs and produce physically-consistent synthetic samples; however, vanilla GANs suffer from high sample complexity and mode collapse without physics-informed terms.",
            "impact_potential": "High potential for accelerating design (materials, molecules), enabling surrogate sampling for UQ, and augmenting datasets for downstream ML tasks; can reduce computation time compared to full physics simulation.",
            "comparison_to_alternatives": "Compared to pure simulation, generative models are much faster at inference but require careful training; physics-informed variants outperform unconstrained GANs in preserving physical statistics and reducing required data.",
            "success_factors": "Incorporating domain-specific physical constraints into loss/architecture (e.g., conservation laws, energy spectra), and availability of representative simulation/observation data for conditioning.",
            "key_insight": "Embedding physics constraints into generative models significantly improves realism and sample efficiency, making GANs/VAEs more practical as surrogates for expensive scientific simulators.",
            "uuid": "e2324.4"
        },
        {
            "name_short": "Physics-Guided Initialization / Transfer",
            "name_full": "Physics-Guided Initialization and Simulation-Based Transfer Learning",
            "brief_description": "Pre-training ML models on synthetic data generated by mechanistic simulators (or using self-supervised physics pretext tasks) to yield informed initial weights that dramatically reduce required real-world training data and speed convergence.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "robotics, lake temperature modeling, chemical process modeling, autonomous vehicles, and other simulation-amenable domains",
            "problem_description": "Reduce real-world data needs and accelerate model training by initializing ML models using weights learned from simulator data or physics-based self-supervised tasks.",
            "data_availability": "simulator-generated synthetic data is typically abundant; real-world labeled data scarce and expensive to collect.",
            "data_structure": "varies — images (robotics), time series (lake temperature), process variables (chemical), etc.; often structured but domain-specific",
            "problem_complexity": "varies by domain; transfer learning reduces optimization difficulty and effective search space complexity.",
            "domain_maturity": "practiced in many applied ML areas (vision, robotics) and increasingly used in scientific ML; domain expertise needed to design suitable simulation tasks.",
            "mechanistic_understanding_requirements": "low-to-medium — needs at least a simulator capable of producing relevant intermediate variables or scenarios; does not require full interpretability at inference time.",
            "ai_methodology_name": "Simulation-based pre-training / transfer learning / self-supervised physics pre-training",
            "ai_methodology_description": "Pre-train neural network weights on large synthetic datasets produced by mechanistic models or on self-supervised tasks predicting physics-derived intermediate variables; then fine-tune on limited observed data, optionally with physics-guided loss; can include scale-bias adaptation for Gaussian processes or ensemble transfer for NNs.",
            "ai_methodology_category": "transfer learning / self-supervised learning / physics-guided ML",
            "applicability": "Highly applicable where credible simulators exist; reduces labeled data requirements but depends on simulator realism and domain gap handling (domain adaptation may be needed).",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported substantial reductions in required real data (e.g., robotics tasks show large reductions; lake modeling shows significant data savings even when simulator parameters are imperfect); effectiveness depends on simulator fidelity.",
            "impact_potential": "High — lowers experimental/measurement costs, speeds development cycles, and enables ML deployment in data-poor scientific settings.",
            "comparison_to_alternatives": "Outperforms random initialization and purely data-driven training when real labeled data are limited; domain adaptation or physics-guided regularization often needed to bridge simulator-reality gap.",
            "success_factors": "Availability of realistic simulators, careful selection of pre-training tasks, and techniques for bridging simulation-to-reality distribution shifts.",
            "key_insight": "Using mechanistic simulators to initialize ML models is an effective way to encode prior physical structure into weights and drastically reduce the need for expensive observed data.",
            "uuid": "e2324.5"
        },
        {
            "name_short": "Tensor-Basis NN",
            "name_full": "Tensor-Basis Neural Network with Embedded Invariances",
            "brief_description": "A neural network architecture that enforces rotational (and other) invariances by constructing predictions on a rotationally invariant tensor basis, used to model turbulence closure terms and other physics where symmetry matters.",
            "citation_title": "Reynolds averaged turbulence modelling using deep neural networks with embedded invariance",
            "mention_or_use": "mention",
            "scientific_problem_domain": "turbulence modeling / fluid dynamics",
            "problem_description": "Predict turbulence closure terms or constitutive relations that must satisfy rotational invariance and other symmetries to produce physically consistent turbulence model corrections.",
            "data_availability": "typically limited high-fidelity DNS/LES datasets for training; synthetic numerical data can be generated but expensive; data often high-dimensional spatial fields.",
            "data_structure": "spatiotemporal fields and tensors; vector and tensor-valued inputs/outputs requiring equivariance/invariance handling",
            "problem_complexity": "very high nonlinearity, tensor-valued quantities, sensitivity to coordinate transformations; large search spaces if invariance not enforced.",
            "domain_maturity": "advanced field with well-established theoretical constraints (symmetries/invariances); ML architectures embedding invariances are an active research area.",
            "mechanistic_understanding_requirements": "high — preserving known symmetries and invariances is essential for physically meaningful and generalizable models.",
            "ai_methodology_name": "Tensor-basis neural networks / invariance-embedded architectures",
            "ai_methodology_description": "Architectural modification adding multiplicative layers and tensor bases to ensure outputs lie in an invariance-preserving basis (e.g., rotational invariance), often using physics-derived input invariants and basis functions so that network predictions respect symmetry properties across coordinate frames.",
            "ai_methodology_category": "physics-guided architecture / supervised learning",
            "applicability": "Highly appropriate where symmetry constraints are critical (e.g., turbulence); enforces physically necessary invariances and improves generalization across coordinate transforms.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated improved prediction accuracy and robustness compared to naive NN architectures that do not enforce invariances; avoids inconsistent predictions under coordinate rotations.",
            "impact_potential": "Significant for improving ML-based turbulence closures and other physics tasks where symmetry preservation is central to predictive fidelity and transferability.",
            "comparison_to_alternatives": "Outperforms standard NNs that ignore invariances on tasks requiring symmetry; provides theoretical guarantees of invariance absent in unconstrained architectures.",
            "success_factors": "Correct identification and encoding of the relevant invariances, careful architectural design to implement invariance, and access to training data that exercises different orientations.",
            "key_insight": "Embedding physical symmetries into architecture reduces search space, improves generalization, and ensures consistency under coordinate transformations—crucial for reliable physics ML in domains like turbulence.",
            "uuid": "e2324.6"
        },
        {
            "name_short": "Hamiltonian NN",
            "name_full": "Hamiltonian Neural Networks (HNN)",
            "brief_description": "Neural network architectures that learn a system's Hamiltonian (energy function) and integrate dynamics via symplectic structure to enforce conservation laws (e.g., energy conservation) and improve long-term predictive stability.",
            "citation_title": "Hamiltonian neural networks",
            "mention_or_use": "mention",
            "scientific_problem_domain": "classical mechanics, dynamical systems, and physics problems with conserved quantities (e.g., pendulum, mass-spring)",
            "problem_description": "Model time evolution of conservative dynamical systems while respecting conserved quantities like energy, improving long-term accuracy and preserving physical invariants.",
            "data_availability": "often limited trajectories or observational sequences; can be fully supervised on state/time-series data.",
            "data_structure": "time series of state vectors (positions, momenta); low-to-moderate dimensionality typical for physically motivated systems",
            "problem_complexity": "nonlinear dynamics with conserved structure; capturing long-term behavior requires preserving invariants and avoiding numerical dissipation.",
            "domain_maturity": "emerging research area integrating classical mechanics with NN architectures; several proofs-of-concept published.",
            "mechanistic_understanding_requirements": "high — approach explicitly leverages the Hamiltonian formalism and requires the notion of conserved energy/phase space structure to be meaningful.",
            "ai_methodology_name": "Hamiltonian Neural Networks and Hamiltonian-parameterized architectures",
            "ai_methodology_description": "NNs predict a scalar Hamiltonian function of state; time derivatives computed via automatic differentiation and canonical Hamilton's equations yield dynamics; architectures can learn latent phase space mappings when explicit coordinates are unknown, and integration schemes (symplectic) are used to propagate states.",
            "ai_methodology_category": "physics-guided architecture / supervised learning with structure",
            "applicability": "Well-suited to conservative dynamical systems where conservation laws are central; less applicable when strong dissipation or non-Hamiltonian forcing dominates the dynamics.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Shown to preserve energy and exhibit superior long-term stability versus unconstrained NNs when modeling conservative systems; enables interpretability in terms of learned energy functions.",
            "impact_potential": "High for modeling and control of systems where invariant preservation is necessary for physically realistic simulation and long-term forecasting.",
            "comparison_to_alternatives": "Outperforms unconstrained predictive NNs in preserving invariants and long-term stability; complements (rather than replaces) PINNs for systems where Hamiltonian structure is present.",
            "success_factors": "Presence of Hamiltonian structure in the true system, ability to represent Hamiltonian as a differentiable function of states, and appropriate integrators that maintain symplecticity.",
            "key_insight": "Parameterizing neural dynamics via learned Hamiltonians enforces conservation laws and yields more physically faithful and stable long-term predictions for conservative dynamical systems.",
            "uuid": "e2324.7"
        },
        {
            "name_short": "Koopman Embeddings",
            "name_full": "Deep Koopman Operator Embeddings / Koopman-based ROMs",
            "brief_description": "Use deep learning to approximate finite-dimensional embeddings of the Koopman operator, enabling linear analysis of nonlinear dynamical systems and construction of reduced-order models with improved forecasting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "dynamical systems, fluid dynamics, oceanography, molecular dynamics, and other time-evolving physical systems",
            "problem_description": "Find a (possibly learned) observable mapping under which the nonlinear dynamics evolve approximately linearly according to Koopman operator modes to enable dimensionality reduction and linear forecasting methods.",
            "data_availability": "depends — training requires time-series data from dynamical systems; high-fidelity simulation or observational trajectories are used when available.",
            "data_structure": "high-dimensional time series / state trajectories; may be spatial fields compressed by POD or autoencoders",
            "problem_complexity": "high — nonlinear, possibly chaotic dynamics with multiscale features; embedding dimension and dictionary selection create large search spaces.",
            "domain_maturity": "active research area; classical DMD methods exist but deep learning-based Koopman embeddings are recent and promising.",
            "mechanistic_understanding_requirements": "medium — does not require explicit governing equations but benefits from physical insight for choosing observables or regularization; interpretability desired.",
            "ai_methodology_name": "Deep Koopman operator learning / Koopman autoencoders / DMD with learned dictionaries",
            "ai_methodology_description": "Autoencoders or NNs learn an observation map to a latent space where a linear operator (approximating Koopman operator) advances dynamics; training optimizes reconstruction loss plus linearity/consistency penalties across time steps; used to extract dominant modes and build ROMs for forecasting and control.",
            "ai_methodology_category": "unsupervised / representation learning for dynamical systems / hybrid ROM",
            "applicability": "Suitable when time-series data are available and linear embedding can capture dominant dynamics; challenges arise for strongly nonlinear or non-ergodic systems.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Deep Koopman embeddings can outperform classical DMD and improve long-term forecasting and reduction for complex systems; physics-informed regularization can improve interpretability and generalizability.",
            "impact_potential": "High for model reduction, forecasting and control in large dynamical systems where linear analysis simplifies downstream tasks.",
            "comparison_to_alternatives": "Offers more expressive embeddings than linear DMD; deep methods can capture complex observables but require careful regularization to avoid overfitting.",
            "success_factors": "Sufficiently rich trajectory data, appropriate choices of latent dimension and regularization, and incorporation of physics-based priors to guide embedding learning.",
            "key_insight": "Learning Koopman-compatible embeddings with deep networks provides a bridge between nonlinear dynamics and linear operator theory, enabling effective reduced-order forecasting when guided by physics or regularization.",
            "uuid": "e2324.8"
        },
        {
            "name_short": "Equation Discovery / Sparse ID",
            "name_full": "Sparse Identification of Nonlinear Dynamics / Symbolic Regression for Governing Equation Discovery",
            "brief_description": "Techniques that build libraries/dictionaries of candidate functions (including derivatives) and use sparse regression or symbolic methods to identify parsimonious governing equations from data, often augmented by physics constraints.",
            "citation_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "mention_or_use": "mention",
            "scientific_problem_domain": "dynamical systems across ecology, fluid dynamics, biological transport, and general physics",
            "problem_description": "Given observational time-series or spatiotemporal data, infer the underlying differential or algebraic equations (symbolic form) that govern system evolution.",
            "data_availability": "often abundant time-series data in some domains but data may be noisy; discovery methods can be sensitive to noise and require derivative estimation.",
            "data_structure": "time-series and spatiotemporal fields; requires compute of derivatives (finite-difference or smoothing) or auto-differentiation-enabled representations",
            "problem_complexity": "search over combinatorially large space of symbolic terms/operators; NP-hard in general without structure/expert priors.",
            "domain_maturity": "rapidly maturing with key contributions (sparse regression, symbolic regression, AI Feynman); increasingly combined with ML to scale discovery.",
            "mechanistic_understanding_requirements": "medium-to-high — domain expertise helps define candidate function dictionaries and constraints; interpretable mechanistic understanding is the primary goal.",
            "ai_methodology_name": "Sparse regression (SINDy), symbolic regression, neural-network-assisted dictionary learning, AI Feynman",
            "ai_methodology_description": "Construct a library of candidate functions (polynomials, trig functions, derivatives) and solve a sparsity-promoting regression (e.g., LASSO) to select a small set of terms that reconstruct observed derivatives; recent variants use neural nets to construct dictionaries, enforce physics constraints, or pre-screen for symmetry/separability to reduce search space.",
            "ai_methodology_category": "symbolic / interpretable ML / sparse learning",
            "applicability": "Well-suited for systems where parsimonious analytic descriptions exist and adequate data (with good derivative estimates) are available; limited in high-noise or very high-dimensional settings without additional priors.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Successfully rediscovers known PDEs/ODEs in benchmark cases and produces interpretable governing equations; performance degrades with noise and insufficient coverage of dynamics unless physics priors or regularization applied.",
            "impact_potential": "High for scientific discovery — can produce new interpretable laws, enable analysis of stability and generalization beyond training data.",
            "comparison_to_alternatives": "More interpretable than black-box ML approaches and more data-efficient when a sparse true model exists; contrasts with PINNs which assume the equation structure is known.",
            "success_factors": "High-quality data permitting derivative estimation, choice of appropriate function dictionary, use of physical constraints (e.g., conservation) to regularize selection, and techniques to mitigate noise.",
            "key_insight": "Sparse and symbolic regression methods can recover interpretable governing equations from data when the true dynamics are parsimonious and data quality suffices, and physics priors greatly reduce the combinatorial search burden.",
            "uuid": "e2324.9"
        },
        {
            "name_short": "Physics-Guided UQ (Bayesian NNs, Ensembles, Dropout)",
            "name_full": "Physics-Guided Uncertainty Quantification via Bayesian Neural Networks, Ensembles, and Physics-Regularized Approaches",
            "brief_description": "Approaches that provide uncertainty estimates for ML surrogates used in scientific modeling by combining Bayesian neural networks, dropout-as-Bayesian approximations, deep ensembles, or physics-regularized priors to constrain uncertainty in physically-consistent ways.",
            "citation_title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "uncertainty quantification for climate, fluid flow, engineering systems, PDE solutions and surrogate modeling",
            "problem_description": "Quantify predictive uncertainty (full predictive distribution) of response variables predicted by ML surrogates for scientific decision-making, where UQ is critical but Monte Carlo on high-fidelity simulators is infeasible.",
            "data_availability": "often limited labeled data; plentiful simulator data may exist but MC over simulator is computationally prohibitive; UQ methods must handle small data regimes.",
            "data_structure": "various (fields, time series, parameter vectors); surrogate models map inputs to outputs with associated uncertainty estimates",
            "problem_complexity": "high-dimensional outputs and inputs; need calibrated uncertainty estimates under model mismatch and out-of-sample scenarios.",
            "domain_maturity": "well-studied in statistics and ML; physics integration into UQ (e.g., physics-informed priors) is active research to improve calibration and coherence.",
            "mechanistic_understanding_requirements": "medium — physics-based priors or constraints as regularizers can improve calibration and physical consistency of predicted uncertainties.",
            "ai_methodology_name": "Bayesian neural networks, MC dropout, deep ensembles, physics-guided Bayesian priors, adversarial inference for generative UQ",
            "ai_methodology_description": "Apply approximate Bayesian techniques (dropout as variational bayes), full Bayesian NN formulations, or ensembles to produce predictive distributions; incorporate physics as priors or PDE constraints in training to regularize uncertainty estimates and avoid physically inconsistent predictions; use generative adversarial inference for probabilistic modeling of system states.",
            "ai_methodology_category": "Bayesian / probabilistic ML / physics-informed UQ",
            "applicability": "Applicable and necessary for scientific forecasting and decision tasks; computational cost and prior specification are challenges (Bayesian NNs can be expensive and sensitive to priors).",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Physics-guided uncertainty methods help produce more physically-consistent uncertainty estimates and reduce data demands relative to purely data-driven Bayesian models; ensemble/dropout approaches provide scalable approximations while Bayesian NNs offer principled uncertainty at higher cost.",
            "impact_potential": "High — credible UQ enables risk-aware decision-making, robust forecasting, and better resource allocation in scientific and engineering contexts.",
            "comparison_to_alternatives": "Gaussian processes have strong UQ properties but scale poorly; deep probabilistic models and ensembles scale better but need physics-based regularization to avoid physically-inconsistent uncertainty.",
            "success_factors": "Appropriate physics-informed priors or loss constraints, scalable approximate inference methods, and ensembles to balance scalability and calibration.",
            "key_insight": "Incorporating physics as priors or constraints into ML-based UQ improves physical plausibility and data efficiency of uncertainty estimates, addressing limitations of purely data-driven Bayesian surrogates in scientific domains.",
            "uuid": "e2324.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "rating": 2,
            "sanitized_title": "physicsinformed_neural_networks_a_deep_learning_framework_for_solving_forward_and_inverse_problems_involving_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Physics-guided neural networks (PGNN): An application in lake temperature modeling",
            "rating": 2,
            "sanitized_title": "physicsguided_neural_networks_pgnn_an_application_in_lake_temperature_modeling"
        },
        {
            "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "rating": 2,
            "sanitized_title": "discovering_governing_equations_from_data_by_sparse_identification_of_nonlinear_dynamical_systems"
        },
        {
            "paper_title": "Fourier neural operator for parametric partial differential equations",
            "rating": 2,
            "sanitized_title": "fourier_neural_operator_for_parametric_partial_differential_equations"
        },
        {
            "paper_title": "Hamiltonian neural networks",
            "rating": 1,
            "sanitized_title": "hamiltonian_neural_networks"
        },
        {
            "paper_title": "Physics-informed generative adversarial networks for stochastic differential equations",
            "rating": 1,
            "sanitized_title": "physicsinformed_generative_adversarial_networks_for_stochastic_differential_equations"
        },
        {
            "paper_title": "Reynolds averaged turbulence modelling using deep neural networks with embedded invariance",
            "rating": 1,
            "sanitized_title": "reynolds_averaged_turbulence_modelling_using_deep_neural_networks_with_embedded_invariance"
        },
        {
            "paper_title": "Physics-informed deep generative models",
            "rating": 1,
            "sanitized_title": "physicsinformed_deep_generative_models"
        }
    ],
    "cost": 0.02883025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems
2020. March 2020</p>
<p>Jared Willard 
SHAOMING XU
VIPIN KUMAR
University of Minnesota
University of Pittsburgh
University of Minnesota
University of Minnesota
University of Minnesota</p>
<p>Xiaowei Jia 
SHAOMING XU
VIPIN KUMAR
University of Minnesota
University of Pittsburgh
University of Minnesota
University of Minnesota
University of Minnesota</p>
<p>Michael Steinbach 
SHAOMING XU
VIPIN KUMAR
University of Minnesota
University of Pittsburgh
University of Minnesota
University of Minnesota
University of Minnesota</p>
<p>Jared Willard 
SHAOMING XU
VIPIN KUMAR
University of Minnesota
University of Pittsburgh
University of Minnesota
University of Minnesota
University of Minnesota</p>
<p>Xiaowei Jia 
SHAOMING XU
VIPIN KUMAR
University of Minnesota
University of Pittsburgh
University of Minnesota
University of Minnesota
University of Minnesota</p>
<p>Shaoming Xu 
SHAOMING XU
VIPIN KUMAR
University of Minnesota
University of Pittsburgh
University of Minnesota
University of Minnesota
University of Minnesota</p>
<p>Michael Steinbach 
SHAOMING XU
VIPIN KUMAR
University of Minnesota
University of Pittsburgh
University of Minnesota
University of Minnesota
University of Minnesota</p>
<p>Vipin Kumar 
SHAOMING XU
VIPIN KUMAR
University of Minnesota
University of Pittsburgh
University of Minnesota
University of Minnesota
University of Minnesota</p>
<p>Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems
112020. March 202010.1145/1122445.1122456CCS Concepts:General and reference → Surveys and overviews;Computing methodologies → Ma- chine learning Additional Key Words and Phrases: physics-guided, neural networks, deep learning, physics-informed, theory- guided, hybrid, knowledge integration ACM Reference Format:
There is a growing consensus that solutions to complex science and engineering problems require novel methodologies that are able to integrate traditional physics-based modeling approaches with state-of-the-art machine learning (ML) techniques. This paper provides a structured overview of such techniques. Applicationcentric objective areas for which these approaches have been applied are summarized, and then classes of methodologies used to construct physics-guided ML models and hybrid physics-ML frameworks are described. We then provide a taxonomy of these existing techniques, which uncovers knowledge gaps and potential crossovers of methods between disciplines that can serve as ideas for future research.</p>
<p>INTRODUCTION</p>
<p>Machine learning (ML) models, which have already found tremendous success in commercial applications, are beginning to play an important role in advancing scientific discovery in environmental and engineering domains traditionally dominated by mechanistic (e.g. first principle) models [30,124,128,141,142,157,232,283]. The use of ML models is particularly promising in scientific problems involving processes that are not completely understood, or where it is computationally infeasible to run mechanistic models at desired resolutions in space and time. However, the application of even the state-of-the-art black box ML models has often met with limited success in scientific domains due to their large data requirements, inability to produce physically consistent results, and their lack of generalizability to out-of-sample scenarios [47,166,190]. Given that neither an ML-only nor a scientific knowledge-only approach can be considered sufficient for complex scientific and engineering applications, the research community is beginning to explore the continuum between mechanistic and ML models, where both scientific knowledge and ML are integrated in a synergistic manner [139,141,143]. This paradigm is fundamentally different from mainstream practices in the ML community for making use of domain-specific knowledge in feature engineering or post-processing, as it is focused on approaches that integrate scientific knowledge directly into the ML framework.</p>
<p>Even though the idea of integrating scientific principles and ML models has picked up momentum just in the last few years, there is already a vast amount of work on this topic. For instance, in all Web of Science databases (www.webofknowledge.com) a search for "physics-informed ML" alone shows the growth of publications from 2 in 2017, 8 in 2018, 27 in 2019, to 63 in 2020. Also, many workshops and symposiums have formed to focus on this field (e.g. [1][2][3][4][5][6]21]). This work is being pursued in diverse disciplines (e.g., earth systems [232], climate science [87,153,207], turbulence modeling [37,199,295], computational physics [261], cyberphysical systems [223], material discovery [49,222,243], quantum chemistry [240,245], biological sciences [8,215,306], and hydrology [275,297]), and it is being performed in the context of diverse objectives specific to these applications. Early results in isolated and relatively simple scenarios are promising, and the expectations are rising for this paradigm to accelerate scientific discovery and help address some of the biggest challenges that are facing humanity in terms of climate [87], health [280], and food security [129].</p>
<p>The goal of this survey is to bring these exciting developments to the ML community, to make them aware of the progress that has been made, and the gaps and opportunities that exist for advancing research in this promising direction. We hope that this survey will also be valuable for scientists who are interested in exploring the use of ML to enhance modeling in their respective disciplines. Please note that work on this topic has been referred to by other names, such as "physics-guided ML," "physics-informed ML," or "physics-aware AI" although it covers many scientific disciplines. In this survey, we also use the terms "physics-guided" or "physics," which should be more generally interpreted as "scientific knowledge-guided" or "scientific knowledge".</p>
<p>The focus of this survey is on approaches that integrate scientific knowledge with ML for environmental and engineering systems where scientific knowledge is available as mechanistic models, theories, and laws (e.g. conservation of mass). This distinguishes our survey from other works that focus on more general knowledge integration into machine learning [7,277] and works covering physics integration into ML in specific domains (e.g., cyber-physical systems [223], chemistry [205], medical imaging [175], fluid mechanics [46], climate and weather [146]). This survey creates a novel taxonomy that covers a wide array of physics-ML methodologies, application-centric objectives, and general computational objectives.</p>
<p>We organize the paper as follows. Section 2 describes different application-centric objectives that are being pursued through combinations of scientific knowledge and ML. Section 3 discusses novel ML loss functions, model initializations, architectures, and hybrid models that researchers are developing to achieve these objectives, as well as comparisons between them. Section 4 discusses the areas of current work as well as the possibilities for cross-fertilization between methods and application-centric objectives. Then, Section 5 contains concluding remarks. Table 3 categorizes the work surveyed in this paper by application-centric objective and methods for integrating scientific knowledge in ML according to the proposed taxonomy. </p>
<p>APPLICATION-CENTRIC OBJECTIVES OF PHYSICS-ML INTEGRATION</p>
<p>This section provides a brief overview of a set of application-centric objectives where couplings of ML and scientific modeling paradigms are being pursued for applications in environmental and engineering systems. In many of these applications, scientific knowledge is represented using a mechanistic model (also referred to as physical, process-based, or first principles models). This is shown in Figure 1 as part of an abstract representation of a generic scientific problem. For example, a model for lake temperature would have drivers ì , such as amount of sunlight, air temperature, and wind speed, with static parameters ì, such as lake depth and water clarity, which the model ( ì , ì) would use to predict the water temperature ì at various depths in the lake. Such physical models typically have a notion of state (which is not depicted in Figure 1 for simplicity), and complex physical models can have multiple components that model various aspects of the system, e.g., components to model clouds or ocean in a global climate model. The application-centric objectives described in this section describe different ways in which physics-ML integration can be used to address the imperfections of the mechanistic model (), build a more resource efficient (), or discover new knowledge such as (). Below we describe how different application-centric objectives fit into each of these scenarios.</p>
<p>First, situations can arise in which a mechanistic model is inadequate to model a poorly understood process and a data-driven model could be leveraged to make better use of observations and possibly also process-based knowledge. Section 2.1 covers approaches like this, where physics-ML is being pursued to improve the effectiveness and predictive accuracy of ( ì , ì) with observations and scientific knowledge.</p>
<p>Often it is also desirable to improve the resource efficiency where physical models are too slow or at too coarse of a resolution. Section 2.2 on downscaling considers how physics-ML can produce high-resolution output variables faster than a physical model in situations spanning meteorology, climatology, and others. Similarly, if subprocesses of a larger mechanistic model are computationally intractable or inaccurate, an ML model can be used for faster or more accurate subprocess representation, as is covered in Section 2.3 on parameterization. More generally, the concept of reducing computational complexity of complex mechanistic or numerical models in the form of an ML-based or ML-assisted reduced order model is described in Section 2.4. Another objective seeking to improve resource efficiency with physics-ML is the solving of partial differential equations (PDEs) where the solution is represented with a data-driven model. This allows dynamical systems applications to bypass the often extreme computational complexity of using finite elements methods to solve complex systems of PDEs. As we can see, in many of these cases ML can be used as a more efficient surrogate model (also referred to as emulators) for an existing mechanistic or numerical approach. </p>
<p>Uncertainty Quantification</p>
<p>Estimate uncertainty on ì given input ì Observations or synthetic samples Table 1. Application-centric objectives of using Physics-ML methods in terms of the generic scientific problem shown in Figure 1 and the needs to pursue each objective</p>
<p>Other objectives seek to discover new knowledge in the form of unobserved causal quantities or the symbolic representation of a process given only data. Compared to previous objectives where the goal is to produce accurate or efficient output variables, inverse modeling described in Section 2.6 flips the path shown in Figure 1 and instead seeks to find some causal static physical parameters within ì given sufficient outputs. Also, Section 2.7 covers the discovering of governing equations from data, where ML has been used to go beyond traditional approaches and discover new explicit symbolic representations of phenomena.</p>
<p>Data generation objectives (Section 2.8) try to realistically reproduce the distribution of ì , ì , or (ì , ì ). Such synthetically generated data can be useful in the often data-limited situations present in engineering and environmental systems. UQ (Section 2.9) tries to learn the distribution of ì in terms of the uncertainty of the other components of the model like the inputs, static parameters, and model state. UQ is necessary for accurate forecasting, which can also affect many of the other objectives. Table 1 summarizes these objectives and their needs in terms of real world observations, synthetic samples (e.g. output from a mechanistic model), or knowledge of the governing equations of the system.</p>
<p>Improving over state-of-the-art physical models</p>
<p>First-principle models are used extensively in a wide range of engineering and environmental applications. Even though these models are based on known physical laws, in most cases, they are necessarily approximations of reality due to incomplete knowledge of certain processes, which introduces bias. In addition, they often contain a large number of parameters whose values must be estimated with the help of limited observed data, degrading their performance further, especially due to heterogeneity in the underlying processes in both space and time. The limitations of physics-based models cut across discipline boundaries and are well known in the scientific community (e.g., see Gupta et al. [116] in the context of hydrology).</p>
<p>ML models have been shown to outperform physics-based models in many disciplines (e.g., materials science [148,238,285], applied physics [22,127], aquatic sciences [132,286], atmospheric science [206], biomedical science [264], computational biology [9]). A major reason for this success is that ML models (e.g., neural networks), given enough data, can find structure and patterns in problems where complexity prohibits the explicit programming of a system's exact physical nature. Given this ability to automatically extract complex relationships from data, ML models appear promising for scientific problems with physical processes that are not fully understood but have data of adequate quality and quantity is available. However, the black-box application of ML has met with limited success in scientific domains due to a number of reasons [141]: (i) while state-of-the-art ML models are capable of capturing complex spatiotemporal relationships, they require far too much labeled data for training, which is rarely available in real application settings, (ii) ML models often produce scientifically inconsistent results; and (iii) ML models can only capture relationships in the available training data, and thus cannot generalize to out-of-sample scenarios (i.e., those not represented in the training data).</p>
<p>The key objective here is to combine elements of physics-based modeling with state-of-the-art ML models to leverage their complementary strengths. Such integrated physics-ML models are expected to better capture the dynamics of scientific systems and advance the understanding of underlying physical processes. An early attempt for combining ML with physics-based modeling in lake temperature dynamics [231]) has already demonstrated its potential for providing better prediction accuracy with a much smaller number of samples as well as generalizability in out-ofsample scenarios.</p>
<p>Downscaling</p>
<p>Complex mechanistic models are capable of capturing physical reality more precisely than simpler models, as they often involve more diverse components that account for a greater number of processes at finer spatial or temporal resolution. However, given the computation cost and modeling complexity, many models are run at a resolution that is coarser than what is required to precisely capture underlying physical processes. For example, cloud-resolving models (CRM) need to run at subkilometer horizontal resolution to be able to effectively represent boundary-layer eddies and low clouds, which are crucial for the modeling of Earth's energy balance and the cloud-radiation feedback [230]. However, it is not feasible to run global climate models at such fine resolutions even with the most powerful computers expected to be available in the near future.</p>
<p>Downscaling techniques have been widely used as a solution to capture physical variables that need to be modeled at a finer resolution. In general, the downscaling techniques can be divided into two categories: statistical downscaling and dynamical downscaling. Statistical downscaling refers to the use of empirical models to predict finer-resolution variables from coarser-resolution variables. Such a mapping across different resolutions can involve complex non-linear relationships that cannot be precisely modeled by traditional empirical models. Recently, artificial neural networks have shown a lot of promise for this problem, given their ability to model non-linear relationships [249,273]. In contrast, dynamical downscaling makes use of high-resolution regional simulations to dynamically simulate relevant physical processes at regional or local scales of interest. Due to the substantial time cost of running such complex models, there is an increasing interest in using ML models as surrogate models (a model approximating simulation-driven input-output data) to predict target variables at a higher resolution [103,254].</p>
<p>Although the state-of-the-art ML methods can be used in both statistical and dynamical downscaling, it remains a challenge to ensure that the learned ML component is consistent with established physical laws and can improve the overall simulation performance.</p>
<p>Parameterization</p>
<p>Complex physics-based models (e.g., for simulating phenomena in climate, weather, turbulence modeling, hydrology) often use an approach known as parameterization to account for missing physics. In parameterization (note that this term has a different meaning when used in mathematics and geometry), specific complex dynamical processes are replaced by simplified physical approximations that are represented as static parameters. A common way to estimate the value of these parameters is to use grid search over the space of combinations of parameter values that lead to the best match with observations. This procedure is referred to as parameter calibration. The failure to correctly parameterize can make the model less robust, and errors that result from imperfect parameterization can also feed into other components of the entire physics-based model and deteriorate the modeling of important physical processes. Another way, that is being considered increasingly, is to replace processes that are too complex to be physically represented in the model by a simplified dynamic or statistical/ML process. This makes it possible to learn new parameterizations directly from observations and/or high-resolution model simulation using ML methods. Already, ML-based parameterizations have shown success in geology [52,109], atmospheric science [40,103], and hydrology [29]. A major benefit of ML-based parameterizations is the reduction of computation time compared to traditional physics-based simulations. In chemistry, Hansen et al. [120] find that parameterizations of atomic energy using ML take seconds compared to multiple days for more standard quantum-calculation calculations, and Behler et al. [27] find that neural networks can vastly improve the efficiency of finding potential energy surfaces of molecules.</p>
<p>Most of the existing work uses standard black box ML models for parameterization, but there is an increasing interest in integrating physics in the ML models [32], as it has the potential to make them more robust and generalizable to unseen scenarios as well as reduce the number of training samples needed for training.</p>
<p>Reduced-Order Models</p>
<p>Reduced-Order Models (ROMs) are computationally inexpensive representations of more complex models. Usually, constructing ROMs involves dimensionality reduction that attempts to capture the most important dynamical characteristics of often large, high-fidelity simulations and models of physical systems (e.g., in fluid dynamics [164]). This can also be viewed as a controlled loss of accuracy. A common way to do this is to project the governing equations of a system onto a linear subspace of the original state space using a method such as principal components analysis or dynamic mode decomposition [221]. However, limiting the dynamics to a lower-dimensional subspace inherently limits the accuracy of any ROM.</p>
<p>ML is beginning to assist in constructing ROMs for increased accuracy and reduced computational cost in several ways. One approach is to build an ML-based surrogate model for full-order models [56,147], where the ML model can be considered a ROM. Other ways include building an ML-based surrogate model of an already built ROM by another dimensionality reduction method [295] or building an ML model to mimic the dimensionality reduction mapping from a full-order model to a reduced-order model [199]. ML and ROMs can also be combined by using the ML model to learn the residual between a ROM and observational data [278]. ML models have the potential to greatly augment the capabilities of ROMs because of their typically quick forward execution speed and ability to leverage data to model high dimensional phenomena.</p>
<p>One area of recent focus of ML-based ROMs is in approximating the dominant modes of the Koopman (or composition) operator, as a method of dimensionality reduction. The Koopman operator is an infinite-dimension linear operator that encodes the temporal evolution of the system state through nonlinear dynamics [42]. This allows linear analysis methods to be applied to nonlinear systems and enables the inference of properties of dynamical systems that are too complex to express using traditional analysis techniques. Applications span many disciplines, including fluid dynamics [250], oceanography [107], molecular dynamics [289], and many others. Though dynamic mode decomposition [197] is the most common technique for approximating the Koopman operator, many recent approaches have been made to approximate Koopman operator embeddings with deep learning models that outperform existing methods [170,185,191,200,209,210,260,284,307]. Adding physics-based knowledge to the learning of the Koopman operator has the potential to augment generalizability and interpretability, which current ML methods in this area tend to lack [210].</p>
<p>Traditional methods for ROMs often lack robustness with respect to parameter changes within the systems they are representing [11], or are not cost-effective enough when trying to predict complex dynamical systems (e.g., multiscale in space and time). Thus, incorporating principles from physics-based models could potentially reduce the search space to enable more robust training of ROMs, and also allow the model to be trained with less data in many scenarios.</p>
<p>Forward Solving Partial Differential Equations</p>
<p>In many physical systems, governing equations are known, but direct numerical solutions of partial differential equations (PDEs) using common methods, such as the Finite Elements Method or the Finite Difference Method [93], are prohibitively expensive. In such cases, traditional methods are not ideal or sometimes even possible. A common technique is to use an ML model as a surrogate for the solution to reduce computation time [75,159]. In particular, NN solvers can reduce the high computational demands of traditional numerical methods into a single forward-pass of a NN. Notably, solutions obtained via NNs are also naturally differentiable and have a closed analytic form that can be transferred to any subsequent calculations, a feature not found in more traditional solving methods [159]. Especially with the recent advancement of computational power, neural network models have shown success in approximating solutions across different kinds of physics-based PDEs [15,149,233], including the difficult quantum many-body problem [51] and many-electron Schrodinger equation [119]. As a step further, deep neural networks models have shown success in approximating solutions across high dimensional physics-based PDEs previously considered unsuitable for approximation by ML [118,252]. However, slow convergence in training, limited applicability to many complex systems, and reduced accuracy due to unawareness of physical laws can prove problematic. More recently, Li et al. [171] have defined a neural Fourier operator which allows a neural network to learn and solve an entire family of PDEs by learning the mapping from any functional parametric dependence to the solution in Fourier space.</p>
<p>Inverse Modeling</p>
<p>The forward modeling of a physical system uses the physical parameters of the system (e.g., mass, temperature, charge, physical dimensions or structure) to predict the next state of the system or its effects (outputs). In contrast, inverse modeling uses the (possibly noisy) output of a system to infer the intrinsic physical parameters or inputs. Inverse problems often stand out as important in physics-based modeling communities because they can potentially shed light on valuable information that cannot be observed directly. One example is the use of x-ray images from a CT scan to create a 3D image reflecting the structure of part of a person's body [183]. This can be viewed as a computer vision problem, where, given many training datasets of x-ray scans of the body at different capture angles, a model can be trained to inversely reconstruct textures and 3D shapes of different organs or other areas of interest. Allowing for better reconstruction while reducing scan time could potentially increase patient satisfaction and reduce overall medical costs. Though there are many inverse modeling scenarios, in this work we focus on intrinsic physical parameters found in a mechanistic modeling scenario for engineering and environmental systems.</p>
<p>Often, the solution of an inverse problem can be computationally expensive due to the potentially millions of forward model evaluations needed for estimator evaluation or characterization of posterior distributions of physical parameters [96]. ML-based surrogate models (in addition to other methods such as reduced-order models) are becoming a realistic choice since they can model high-dimensional phenomena with lots of data and execute much faster than most physical simulations.</p>
<p>Inverse problems are traditionally solved using regularized regression techniques. Data-driven methods have seen success in inverse problems in remote sensing of surface properties [69], hydrology [106], photonics [218], and medical imaging [183], among many others. Recently, novel algorithms using deep learning and neural networks have been applied to inverse problems. While still in their infancy, these techniques exhibit strong performance for applications such as computerized tomography [57,193,263], seismic processing [272], or various sparse data problems.</p>
<p>There is also increasing interest in the inverse design of materials using ML, where desired target properties of materials are used as input to the model to identify atomic or microscale structures that exhibit such properties [156,172,222,243]. Physics-based constraints and stopping conditions based on material properties can be used to guide the optimization process [172,262]. These constraints and similar physics-guided techniques have the potential to alleviate noted challenges in inverse modeling, particularly in scenarios with a small sample size and a paucity of ground-truth labels [142]. The integration of prior physical knowledge is common in current approaches to the inverse problem, and its integration into ML-based inverse models has the potential to improve data efficiency and increase its ability to solve ill-posed inverse problems.</p>
<p>Discovering Governing Equations</p>
<p>When the governing equations of a dynamical system are known explicitly, they allow for more robust forecasting, control, and the opportunity for analysis of system stability and bifurcations through increased interpretability [235]. Furthermore, if a mathematical model accurately describes the processes governing the observed data, it therefore can generalize to data outside of the training domain. However, in many disciplines (e.g., neuroscience, cell biology, ecology, epidemiology) dynamical systems have no formal analytic descriptions. Often in these cases, data is abundant, but the underlying governing equations remain elusive. In this section, we discuss equation discovery systems that do not assume the structure of the desired equation (as in Section 2.5), but rather explore a large space of possibly nonlinear mathematical terms.</p>
<p>Advances in ML for the discovery of these governing equations has become an active research area with rich potential to integrate principles from applied mathematics and physics with modern ML methods. Early works on the data-driven discovery of physical laws relied on heuristics and expert guidance and were focused on rediscovering known, non-differential, laws in different scientific disciplines from artificial data [105,162,163,168]. This was later expanded to include real-world data and differential equations in ecological applications [83]. Recently, general and robust datadriven discovery of potentially unknown governing equations has been pioneered by [38,244], where they apply symbolic regression to differences between computed derivatives and analytic derivatives to determine underlying dynamical systems. More recently, works have used sparse regression built on a dictionary of functions and partial derivatives to construct governing equations [43,220,234]. Lagergren et al. [160] expand on this by using ANNs to construct the dictionary of functions. These sparse identification techniques are based on the principle of Occam's Razor, where the goal is that only a few equation terms be used to describe any given nonlinear system.</p>
<p>Data Generation</p>
<p>Data generation approaches are useful for creating virtual simulations of scientific data under specific conditions. For example, these techniques can be used to generate potential chemical compounds with desired characteristics (e.g., serving as catalysts or having a specific crystal structure). Traditional physics-based approaches for generating data often rely on running physical simulations or conducting physical experiments, which tend to be very time consuming. Also, these approaches are restricted by what can be produced by physics-based models. Hence, there is an increasing interest in generative ML approaches that learn data distributions in unsupervised settings and thus have the potential to generate novel data beyond what could be produced by traditional approaches.</p>
<p>Generative ML models have found tremendous success in areas such as speech recognition and generation [208], image generation [73], and natural language processing [114]. These models have been at the forefront of unsupervised learning in recent years, mostly due to their efficiency in understanding unlabeled data. The idea behind generative models is to capture the inner probabilistic distribution in order to generate similar data. With the recent advances in deep learning, new generative models, such as the generative adversarial network (GAN) and variational autoencoder (VAE), have been developed. These models have shown much better performance in learning nonlinear relationships to extract representative latent embeddings from observation data. Hence the data generated from the latent embeddings are more similar to true data distribution. In particular, the adversarial component of GAN consists of a two-part framework with a generative network and discriminative network, where the generative network's objective is to generate fake data to fool the discriminative network, while the discriminative network attempts to determine true data from fake data.</p>
<p>In the scientific domain, GANs can generate data like the data generated by the physics-based models. Using GANs often incurs certain benefits, including reduced computation time and a better reproduction of complex phenomenon, given the ability of GANs to represent nonlinear relationships. For example, Farimani et al. [91] have shown that Conditional Generative Adversarial Networks (cGAN) can be trained to simulate heat conduction and fluid flow purely based on observations without using knowledge of the underlying governing equations. Such approaches that use generative models have been shown to significantly accelerate the process of generating new data samples.</p>
<p>However, a well-known issue of GANs is that they incur dramatically high sample complexity. Therefore, a growing area of research is to engineer GANs that can leverage prior knowledge of physics in terms of physical laws and invariance properties. For example, GAN-based models for simulating turbulent flows can be further improved by incorporating physical constraints, e.g., conservation laws [308] and energy spectrum [291], in the loss function. Cang et al. [49] also imposed a physics-based morphology constraint on a VAE-based generative model used for simulating artificial material samples. The physics-based constraint forces the generated artificial samples to have the same morphology distribution as the authentic ones and thus greatly reduces the large material design space.</p>
<p>Uncertainty Quantification</p>
<p>Uncertainty quantification (UQ) is of great importance in many areas of computational science (e.g., climate modeling [74], fluid flow [65], systems engineering [217], among many others). At its core, UQ requires an accurate characterization of the entire distribution ( | ), where is the response and is the covariate of interest, rather than just making a point prediction = ( ). This makes it possible to characterize all quantiles and skews in the distribution, which allows for analysis such as examining how close predictions are to being unacceptable, or sensitivity analysis of input features.</p>
<p>Applying UQ tasks to physics-based models using traditional methods such as Monte Carlo (MC) is usually infeasible due to the very large number of forward model evaluations needed to obtain convergent statistics. In the physics-based modeling community, a common technique is to perform model reduction (described in Section 2.4) or create an ML surrogate model, in order to increase model evaluation speed since ML models often execute much faster [99,189,269]. With a similar goal, the ML community has often employed Gaussian Processes as the main technique for quantifying uncertainty in simulating physical processes [34,229], but neither Gaussian Processes nor reduced models scale well to higher dimensions or larger datasets (Gaussian Processes scale as</p>
<p>( 3 ) with data points). Consequently, there is an effort to fit deep learning models, which have exhibited countless successes across disciplines, as a surrogate for numerical simulations in order to achieve faster model evaluations for UQ that have greater scalability than Gaussian Processes [269]. However, since artificial neural networks do not have UQ naturally built into them, variations have been developed. One such modification uses a probabilistic drop-out strategy in which neurons are periodically "turned off" as a type of Bayesian approximation to estimate uncertainty [98]. There are also Bayesian variants of neural networks that consist of distributions of weights and biases [186,314,319], but these suffer from high computation times and high dependence on reliable priors. Another method uses an ensemble of neural networks to create a distribution from which uncertainty is quantified [161].</p>
<p>The integration of physics knowledge into ML for UQ has the potential to allow for a better characterization of uncertainty. For example, ML surrogate models run the risk of producing physically inconsistent predictions, and incorporating elements of physics could help with this issue. Also, note that the reduced data needs of ML due to constraints for adherence to known physical laws could alleviate some of the high computational cost of Bayesian neural networks for UQ.</p>
<p>PHYSICS-ML METHODS</p>
<p>Given the diversity of forms in which scientific knowledge is represented in different disciplines and applications, researchers have developed a large variety of methods for integrating physical principles into ML models. This section categorizes them into the following four classes; (i) physics-guided loss function, (ii) physics-guided initialization, (iii) physics-guided design of architecture, and (iv) hybrid modeling.</p>
<p>Choosing between different classes of methods for a given problem can depend on many factors including the availability and performance of existing mechanistic models, and also the general computational objectives that need to be addressed. The general computational objectives of physics-ML methods described throughout this section, as opposed to traditional ML methods, can be placed into three categories. First, prediction performance defined as better matching between predicted and observed values can be improved in a variety of ways including improved generalizability to out-of-sample scenarios, improved general accuracy, or forcing solutions to be physically consistent (e.g. obeying known physics-based governing equations). Second, sample efficiency can be improved by reducing the number of observations required for adequate performance or reducing the overall search space. The third general computational objective is interpretability, where often traditional ML models are a "black box" and the incorporation of scientific knowledge can shine light on physical meanings, interpretations, and processes within the ML framework. Even though computational objectives can be categorized within these categories, there is also overlap between them. For example, forcing models to be physically consistent can effectively reduce the solution search space. Improved sample efficiency can also lead to improved prediction performance by getting more value out of each observation. We end this section with a summary and detailed discussion comparing different kinds of methods, their requirements, and the general computational objectives achieved.</p>
<p>Physics-Guided Loss Function</p>
<p>Scientific problems often exhibit a high degree of complexity due to relationships between many physical variables varying across space and time at different scales. Standard ML models can fail to capture such relationships directly from data, especially when provided with limited observation data. This is one reason for their failure to generalize to scenarios not encountered in training data. Researchers are beginning to incorporate physical knowledge into loss functions to help ML models capture generalizable dynamic patterns consistent with established physical laws.</p>
<p>One of the most common techniques to make ML models consistent with physical laws is to incorporate physical constraints into the loss function of ML models as follows [141],
Loss = Loss TRN ( true , pred ) + ( ) + Loss PHY ( pred )(1)
where the training loss Loss TRN measures a supervised error (e.g., RMSE or cross-entropy) between true labels true and predicted labels pred , and is a hyper-parameter to control the weight of model complexity loss ( ). The first two terms are the standard loss of ML models. The addition of physics-based loss Loss PHY aims to ensure consistency with physical laws and it is weighted by a hyper-parameter , where is determined alongside other ML hyperparameters using validation data or a nested cross validation setup. A comprehensive guide to implementing physics-based loss functions can be found in Ebert-Uphoff et al. [84]. Steering ML predictions towards physically consistent outputs has numerous benefits. First, this provides the possibility to ensure the consistency with physical laws and therefore reduce the solution search space of ML models. Second, the regularization by physical constraints allows the model to learn even with unlabeled data, as the computation of physics-based loss Loss PHY does not require observation data. Third, ML models which follow desired physical properties are more likely to be generalizable to out-of-sample scenarios relative to basic ML models [133,231]. It is important to note, however, that the physics-guided loss function does not "guarantee" either physical consistency or generalizability as it is fundamentally a weak constraint. Loss function terms corresponding to physical constraints are applicable across many different types of ML frameworks. In addition, this method is extensively used across all application-centric objectives listed in Section 2. In the following paragraphs, we demonstrate the use of physics-based loss functions for different objectives described in Section 2.</p>
<p>Replacing or improving over physical models. Incorporation of physics-based loss has shown great success in improving prediction ability of ML models. In the context of lake temperature modeling, Karpatne et al. [140] includes a physics-based penalty that ensures that predictions of denser water are at lower depths than predictions of less dense water, a known monotonic relationship.</p>
<p>Jia et al. [130] and Read et al. [231] further extended this work to capture even more complex and general physical relationships that happen on a temporal scale. Specifically, they use a physics-based penalty for energy conservation in the loss function to ensure the lake thermal energy gain across time is consistent with the net thermodynamic fluxes in and out of the lake. A diagram of this model is shown in Figure 2. Note that the recurrent structure contains additional nodes (shown in blue) to represent physical variables (lake energy, etc) that are computed using purely physics-based equations. These are needed to incorporate energy conservation in the loss function. Similar structure can be used to model other physical laws such as mass conservation, etc. Qualitative mathematical  [132] is an example of a physics-guided loss function allowing physical knowledge to be incorporated into the ML model. They include the standard RNN flow (black arrows) and the energy flow (blue arrows) in the recurrent process. Here represents the thermal energy of the lake at time , and both the energy output and temperature output are used in calculating the loss function value. This enables the PGRNN to predict lake temperature without violating energy constraints. A detailed description of the loss function equation (Equation 1) can be found in Section 3.1.</p>
<p>properties of dynamical systems modeling have also shown promise in informing loss functions to improve prediction beyond that of the physics model. Erichson et al. [86] penalize autoencoders based on physically meaningful stability measures in dynamical systems to improve prediction of fluid flow and sea surface temperature. They showed an improved mapping of past states to future states for both modeling scenarios in addition to improving generalizability to new data.</p>
<p>Solving PDEs. Another strand of work that involves loss function alterations is solving PDEs for dynamical systems modeling, in which adherence to the governing equations is enforced in the loss function. In Raissi et al. [228], this concept is developed and shown to create data-efficient spatiotemporal function approximators to both solve and find parameters of basic PDEs like Burgers Equation or Schrodinger Equation. Going beyond a simple feed-forward network, Zhu et al. [318] propose an encoder-decoder model for predicting transient PDEs with governing PDE constraints. Geneva et al. [102] extended this approach to deep auto-regressive dense encoder-decoders with a Bayesian framework using stochastic weight averaging to quantify uncertainty.</p>
<p>Discovering Governing Equations. Physics-based loss function terms have also been used in the discovery of governing equations. Loiseau et al. [178] used constrained least squares [110] to incorporate energy-preserving nonlinearities or to enforce symmetries in the identified equations for the equation learning process described in Section 2.7. Though these loss functions are mostly seen in common variants of NNs, they are also be seen in architectures such as echo state networks. Doan et al. [76] found that integrating the physics-based loss from the governing equations in a Lorenz system, a commonly studied system in dynamical systems, strongly improves the echo state network's time-accurate prediction of the system and also reduces convergence time.</p>
<p>Inverse modeling. For applications in vortex induced vibrations, Raissi et al. [224] pose the inverse modeling problem of predicting the lift and drag forces of a system given sparse data about its velocity field. Kahana et al. [136] uses a loss function term pertaining to the physical consistency of the time evolution of waves for the inverse problem of identifying the location of an underwater obstacle from acoustic measurements. In both cases, the addition of physics-based loss terms made results more accurate and more robust to out-of-sample scenarios.</p>
<p>Parameterization. While ML has been used for parameterization, adding physics-based loss terms can further benefit this process by ensuring physically consistent outputs. Zhang et al. [310] parameterize atomic energy for molecular dynamics using a NN with a loss function that takes into account atomic force, atomic energy, and terms relating kinetic and potential energy. Furthermore, in climate modeling, Beucler et al. show that enforcing energy conservation laws improves prediction when emulating cloud processes [31,32].</p>
<p>Downscaling. Super resolution and downscaling frameworks have also begun to incorporate physics-based constraints. Jiang et al. [134] use PDE-based constraints for super resolution problems in computational fluid dynamics where they are able to more efficiently recover physical quantities of interest. Bode et al. [37] use similar constraint ideas in building generative adversarial networks for super resolution in turbulence modeling in combustion scenarios, where they find improved generalization capability and extrapolation due to the constraints.</p>
<p>Uncertainty quantification. In Yang et al. [303] and Yang et al. [304], physics-based loss is implemented in a deep probabilistic generative model for uncertainty quantification based on adherence to the structure imposed by PDEs. To accomplish this, they construct probabilistic representations of the system states, and use an adversarial inference procedure to train using a physics-based loss function that enforces adherence to the governing laws. This is expanded in Zhu et al. [318], where a physics-informed encoder-decoder network is defined in conjunction with a conditional flow-based generative model for similar purposes. A similar loss function modification is performed in other works [102,144,299], but for the purpose of solving high dimensional stochastic PDEs with uncertainty propagation. In these cases, physics-guided constraints provide effective regularization for training deep generative models to serve as surrogates of physical systems where the cost of acquiring data is high and the data sets are small [304].</p>
<p>Another direction for encoding physics knowledge into ML UQ applications is to create a physicsguided Bayesian NN. This is explored in Yang et al. [300], where they use a Bayesian NN, which naturally encodes uncertainty, as a surrogate for a PDE solution. Additionally, they add a PDE constraint for the governing laws of the system to serve as a prior for the Bayesian net, allowing for more accurate predictions in situations with significant noise due to the physics-based regularization.</p>
<p>Generative models. In recent years, GANs have been used to efficiently generate solutions to PDEs and there is interest in using physics knowledge to improve them. Yang et al. [301] showed GANs with loss functions based on PDEs can be used to solve stochastic elliptic PDEs in up to 30 dimensions. In a similar vein, Wu et al. [292] showed that physics-based loss functions in GANs can lower the amount of data and training time needed to converge on solutions of turbulence PDEs, while Shah et al. [248] saw similar results in the generation of microstructures satisfying certain physical properties in computational materials science.</p>
<p>Physics-Guided Initialization</p>
<p>Since many ML models require an initial choice of model parameters before training, researchers have explored different ways to physically inform a model starting state. For example, in NNs, weights are often initialized according to a random distribution prior to training. Poor initialization can cause models to anchor in local minima, which is especially true for deep neural networks. However, if physical or other contextual knowledge is used to help inform the initialization of the weights, model training can be accelerated and may require fewer training samples [132]. One way to inform the initialization to assist in model training and escaping local minima is to use an ML technique known as transfer learning. In transfer learning, a model is pre-trained on a related task prior to being fine-tuned with limited training data to fit the desired task. The pre-trained model serves as an informed initial state that ideally is closer to the desired parameters for the desired task than random initialization. One way to achieve this is to use the physics-based model's simulated data to pre-train the ML model. This is similar to the common application of pre-training in computer vision, where CNNs are often pre-trained with very large image datasets before being fine-tuned on images from the task at hand [259].</p>
<p>Jia et al. use this strategy in the context of modeling lake temperature dynamics [130,132]. They pre-train their Physics-Guided Recurrent Neural Network (PGRNN) models for lake temperature modeling on simulated data generated from a physics-based model and fine tune the NN with little observed data. They showed that pre-training, even using data from a physical model with an incorrect set of parameters, can still significantly reduce the training data needed for a quality model. In addition, Read et al. [231] demonstrated that models using both physics-guided initialization and a physics-guided loss function are able to generalize better to unseen scenarios than traditional physics-based models. In this case, physics-guided initialization allows the model to have a physicallyconsistent starting state prior to seeing any observations.</p>
<p>Another application can be seen in robotics, where images from robotics simulations have been shown to be sufficient without any real-world data for the task of object localization [267], while reducing data requirements by a factor of 50 for object grasping [39]. Then, in autonomous vehicle training, Shah et al. [247] showed that pre-training the driving algorithm in a simulator built on a video game physics engine can drastically lessen data needs. More generally, we see that simulationbased pre-training of applications allows for significantly less expensive data collection than is possible with physical robots. Physics-guided model initialization has also been employed in chemical process modeling [180,181,298]. Yan et al. [298] use Gaussian process regression for process modeling that has been transferred and adapted from a similar task. To adapt the transferred model, they used scale-bias correcting functions optimized through maximum likelihood estimation of parameters. Furthermore, Gaussian process models come equipped with uncertainty quantification which is also informed through initialization. A similar transfer and adapt approach is seen in Lu et al. [180], but for an ensemble of NNs transferred from related tasks. In both studies, the similarity metrics used to find similar systems are defined by considering various common process characteristics and behaviors.</p>
<p>Physics-guided initialization can also be done using a self-supervised learning method, which has been widely used in computer vision and natural language processing. In the self-supervised setting, deep neural networks learn discriminative representations using pseudo labels created from pre-defined pretext tasks. These pretext tasks are designed to extract complex patterns related to our target prediction task. For example, the pretext task can be defined to predict intermediate physical variables that play an important role in underlying processes. This approach can make use of a physics-based model to simulate these intermediate physical variables, which can then be used to pre-train ML models by adding supervision on hidden layers. As an illustration of this approach, Jia et al. [133] have shown promising results for modeling temperature and flow in river networks by using upstream water variables simulated by a physics-based PRMS-SNTemp model [265] to pre-train hidden variables in a graph neural network.</p>
<p>Physics-Guided Design of Architecture</p>
<p>Although the physics-based loss and initialization in the previous sections help constrain the search space of ML models during training, the ML architecture is often still a black box. In particular, they do not encode physical consistency or other desired physical properties into the ML architecture. A recent research direction has been to construct new ML architectures that can make use of the specific characteristics of the problem being solved. Furthermore, incorporating physics-based guidance into architecture design has the added bonus of making the previously black box algorithm more interpretable, a desirable but typically missing feature of ML models used in physical modeling. In the following paragraphs, we discuss several contexts in which physics-guided ML architectures have been used. Much of the work in this section is focused largely on neural network architectures. The modular and flexible nature of NNs in particular makes them prime candidates for architecture modification. For example, domain knowledge can be used to specify node connections that capture physics-based dependencies among variables. We also include subsections on multi-task learning and structures of Gaussian processes to show how task interrelationships or informed prior distributions can inform ML models.</p>
<p>Intermediate Physical Variables. One way to embed physical principles into NN design is to ascribe physical meaning for certain neurons in the NN. It is also possible to declare physically relevant variables explicitly. In lake temperature modeling, Daw et al. [68] incorporate a physical intermediate variable as part of a monotonicity-preserving structure in the LSTM architecture. This model produces physically consistent predictions in addition to appending a dropout layer to quantify uncertainty. Muralidlar et al. [204] used a similar approach to insert physics-constrained variables as the intermediate variables in the convolutional neural network (CNN) architecture which achieved significant improvement over state-of-the-art physics-based models on the problem of predicting drag force on particle suspensions in moving fluids.</p>
<p>An additional benefit of adding physically relevant intermediate variables in an ML architecture is that they can help extract physically meaningful hidden representation that can be interpreted by domain scientists. This is particularly valuable, as standard deep learning models are limited in their interpretability since they can only extract abstract hidden variables using highly complex connected structure. This is further exacerbated given the randomness involved in the optimization process.</p>
<p>Another related approach is to fix one or more weights within the NN to physically meaningful values or parameters and make them non-modifiable during training. A recent approach is seen in geophysics where researchers use NNs for the waveform inversion modeling to find subsurface parameters from seismic wave data. In Sun et al. [256], they assign most of the parameters within a network to mimic seismic wave propagation during forward propagation of the NN, with weights corresponding to values in known governing equations. They show this leads to more robust training in addition a more interpretable NN with meaningful intermediate variables.</p>
<p>Encoding invariances and symmetries. In physics, there is a deep connection between symmetries and invariant quantities of a system and its dynamics. For example, Noether's Law, a common paradigm in physics, demonstrates a mapping between conserved quantities of a system and the system's symmetries (e.g. translational symmetry can be shown to correspond to the conservation of momentum within a system). Therefore, if an ML model is created that is translation-invariant, the conservation of momentum becomes more likely and the model's prediction becomes more robust and generalizable.</p>
<p>State-of-the-art deep learning architectures already encode certain types of invariance. For example, RNNs encode temporal invariance and CNNs can implicitly encode spatial translation, rotation, and scale invariance. In the same way, scientific modeling tasks may require other invariances based on physical laws. In turbulence modeling and fluid dynamics, Ling et al [173] define a tensor basis neural network to embed rotational invariance into a NN for improved prediction accuracy. This solves a key problem in ML models for turbulence modeling because without rotational invariance, the model evaluated on identical flows with axes defined in other directions could yield different predictions. This work alters the NN architecture by adding a higher-order multiplicative layer that ensures the predictions lie on a rotationally invariant tensor basis. In a molecular dynamics application, Anderson et al. [12] show that a rotationally covariant NN architecture can learn the behavior and properties of complex many-body physical systems.</p>
<p>In a general setting, Wang et al. [281] show how spatiotemporal models can be made more generalizable by incorporating symmetries into deep NNs. More specifically, they demonstrated the encoding of translational symmetries, rotational symmetries, scale invariances, and uniform motion into NNs using customized convolutional layers in CNNs that enforce desired invariance properties. They also provided theoretical guarantees of invariance properties across the different designs and showed additional to significant increases in generalization performance.</p>
<p>Incorporating symmetries, by informing the structure of the solution space, also has the potential to reduce the search space of an ML algorithm. This is important in the application of discovering governing equations, where the space of mathematical terms and operators is exponentially large. Though in its infancy, physics-informed architectures for discovering governing equations are beginning to be investigated by researchers. In Section 2.7, symbolic regression is mentioned as an approach that has shown success. However, given the massive search space of mathematical operators, analytic functions, constants, and state variables, the problem can quickly become NP-hard. Udrescu et al. [270] designs a recursive multidimensional version of symbolic regression that uses a NN in conjunction with techniques from physics to narrow the search space. Their idea is to use NNs to discover hidden signs of "simplicity", such as symmetry or separability in the training data, which enables breaking the massive search space into smaller ones with fewer variables to be determined.</p>
<p>In the context of molecular dynamics applications, a number of researchers [28,310] have used a NN per individual atom to calculate each atom's contribution to the total energy. Then, to ensure the energy invariance with respect to the possibility of interchanging two atoms, the structure of each NN and the values of each network's weight parameters are constrained to be identical for atoms of the same element. More recently, novel deep learning architectures have been proposed for fundamental invariances in chemistry. Schutt et al. [245] proposes continuous-filter convolutional (cfconv) layers for CNNs to allow for modeling objects with arbitrary positions such as atoms in molecules, in contrast to objects described by Cartesian-gridded data such as images. Furthermore, their architecture uses atom-wise layers that incorporate inter-atomic distances that enabled the model to respect quantum-chemical constraints such as rotationally invariant energy predictions as well as energy-conserving force predictions. As we can see, because molecular dynamics often ascribes importance to different important geometric properties of molecules (e.g. rotation), network architectures dealing with invariances can be effective for improving performance and robustness of ML models.</p>
<p>Architecture modifications incorporating symmetry are also seen extensively in dynamical systems research involving differential equations. In a pioneering work by Ruthotto et al [236], three variations of CNNs are proposed to improve classifiers for images. Each variation uses mathematical theories to guide the design of the CNN based on fundamental properties of PDEs. Multiple types of modifications are made, including adding symmetry layers to guarantee the stability expressed by the PDEs and layers that convert inputs to kinematic eigenvalues that satisfy certain physical properties. They define a parabolic CNN inspired by anisotropic filtering, a hyperbolic CNN based on Hamiltonian systems, and a second order hyperbolic CNN. Hyperbolic CNNs were found to preserve the energy in the system as intended, which set them apart from parabolic CNNs that smooth the output data, reducing the energy. Furthermore, though solving PDEs with neural networks has traditionally focused on learning on Euclidean spaces, recently Li et al. [171] proposed a new architecture which includes "Fourier neural operators" to generalize this to functional spaces. They showed it achieves greater accuracy compared to previous ML-based solvers and also can solve entire families of PDEs instead of just one. There is a vast amount of other work using physics-guided architecture towards solving PDEs and other PDE-related applications as well which are not included in this survey (e.g. see ICLR workshop on deep learning for differential equations ( [5])) A recent direction also relating to conserved or invariant quantities is the incorporation of the Hamiltonian operator into NNs [64,112,268,317]. The Hamiltonian operator in physics is the primary tool for modeling the time evolution of systems with conserved quantities, but until recently the formalism had not been integrated with NNs. Greydanus et al. [112] design a NN architecture that naturally learns and respects energy conservation and other invariance laws in simple mass-spring or pendulum systems. They accomplish this through predicting the Hamiltonian of the system and re-integrating instead of predicting the state of physical systems themselves. This is taken a step further in Toth et al. [268], where they show that not only can NNs learn the Hamiltonian, but also the abstract phase space (assumed to be known in Greydanus et al. [112]) to more effectively model expressive densities in similar physical systems and also extend more generally to other problems in physics. Recently, the Hamiltonian-parameterized NNs above have also been expanded into NN architectures that perform additional differential equation-based integration steps based on the derivatives approximated by the Hamiltonian network [61].</p>
<p>Encoding other domain-specific physical knowledge. Various other domain-specific physical information can be encoded into architecture that doesn't exactly correspond to known invariances but provides meaningful structure to the optimization process depending on the task at hand. This can take place in many ways, including using domain-informed convolutions for CNNs, additional domain-informed discriminators in GANs, or structures informed by the physical characteristics of the problem. For example, Sadoughi et al. [239] prepend a CNN with a Fast Fourier Transform layer and a physics-guided convolution layer based on known physical information pertaining to fault detection of rolling element bearings. A similar approach is used in Sturmfels et al. [255], but the added beginning layer instead serves to segment different areas of the brain for domain guidance in neuroimaging tasks. In the context of generative models, Xie et al. [296] introduce tempoGAN, which augments a general adversarial network with an additional discriminator network along with additional loss function terms that preserve temporal coherence in the generation of physics-based simulations of fluid flow. This type of approach, though found mostly in NN models, has been extended to non-NN models in Baseman et al. [24], where they introduce a physics-guided Markov Random Field that encodes spatial and physical properties of computer memory devices into the corresponding probabilistic dependencies.</p>
<p>Fan et al. [89] define new architectures to solve the inverse problem of electrical impedance tomography, where the goal is to determine the electrical conductivity distribution of an unknown medium from electrical measurements along its boundary. They define new NN layers based on a linear approximation of both the forward and inverse maps relying on the nonstandard form of the wavelet decomposition [33].</p>
<p>Architecture modifications are also seen in dynamical systems research encoding principles from differential equations. Chen et al. [58] develop a continuous depth NN based on the Residual Network [122] for solving ordinary differential equations. They change the traditionally discretized neuron layer depths into continuous equivalents such that hidden states can be parameterized by differential equations in continuous time. This allows for increased computational efficiency due to the simplification of the backpropagation step of training, and also creates a more scalable normalizing flow, an architectural component for solving PDEs. This is done by by parameterizing the derivative of the hidden states of the NN as opposed to the states themselves. Then, in a similar application, Chang et al. [53] uses principles from the stability properties of differential equations in dynamical systems modeling to guide the design of the gating mechanism and activation functions in an RNN.</p>
<p>Currently, human experts have manually developed the majority of domain knowledge-encoded employed architectures, which can be a time-intensive and error-prone process. Because of this, there is increasing interest in automated neural architecture search methods [20,85,126]. A young but promising direction in ML architecture design is to embed prior physical knowledge into neural architecture searches. Ba et al. [18] adds physically meaningful input nodes and physical operations between nodes to the neural architecture search space to enable the search algorithm to discover more ideal physics-guided ML architectures.</p>
<p>Auxiliary Task in Multi-Task Learning. Domain knowledge can be incorporated into ML architecture as auxiliary tasks in a multi-task learning framework. Multi-task learning allows for multiple learning tasks to be solved at the same time, ideally while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and predictions for one or more of the tasks. Therefore, another way to implement physics-based learning constraints is to use an auxiliary task in a multi-task learning framework. Here, an example of an auxiliary task in a multitask framework might be related to ensuring physically consistent solutions in addition to accurate predictions. The promise of such an approach was demonstrated for a computer vision task by integrating auxiliary information (e.g. pose estimation) for facial landmark detection [315]. In this paradigm, a task-constrained loss function can be formulated to allow errors of related tasks to be back-propagated jointly to improve model generalization. Early work in a computational chemistry application showed that a NN could be trained to predict energy by constructing a loss function that had penalties for both inaccuracy and inaccurate energy derivatives with respect to time as determined by the surrounding energy force field [219]. In particle physics, De Oliveira et al. [72] uses an additional task for the discriminator network in a generative adversarial network (GAN) to satisfy certain properties of particle interaction for the production of jet images of particle energy.</p>
<p>Physics-guided Gaussian process regression. Gaussian process regression (GPR) [287] is a nonparametric, Bayesian approach to regression that is increasingly being used in ML applications. GPR has several benefits, including working well on small amounts of data and enabling uncertainty measurements on predictions. In GPR, first a Gaussian process prior must be assumed in the form of a mean function and a matrix-valued kernel or covariance function. One way to incorporate physical knowledge in GPR is to encode differential equations into the kernel [258]. This is a key feature in Latent Force Models which attempt to use equations in the physical model of the system to inform the learning from data [10,182]. Alvarez et al. [10] draw inspiration from similar applications in bioinformatics [101,165], which showed an increase in predictive ability in computational biology, motion capture, and geostatistics datasets. More recently, Glielmo et al. [108] propose a vectorial GPR that encodes physical knowledge in the matrix-valued kernel function. They show rotation and reflection symmetry of the interatomic force between atoms can be encoded in the Gaussian process with specific invariance-preserving covariant kernels. Furthermore, Raissi et al. [225] show that the covariance function can explicitly encode the underlying physical laws expressed by differential equations in order to solve PDEs and learn with smaller datasets.</p>
<p>Hybrid Physics-ML Models</p>
<p>Contrary to previous sections where the focus has been on augmenting ML models specifically, numerous approaches combine physics-based models with ML models where both are operating simultaneously. We call these Hybrid Physics-ML models. In the context of Figure 1, hybrid models can be viewed as replacing mechanistic model () with a new model in which () and an ML model are working together, or a subcomponent of () is replaced with ML. Hence, such methods are also referred to as ML-enhanced physical models by some researchers [100].</p>
<p>3.4.1 Residual modeling. The oldest and most common approach for directly addressing the imperfection of physics-based models in the scientific community is residual modeling, where an ML model (usually linear regression) learns to predict the errors, or residuals, made by a physics-based model [94,266]. A visualization is shown in Figure 3. The key concept is to learn biases of the physical model (relative to observations) and use it to correct the physical model's predictions. However, one key limitation in residual modeling is its inability to enforce physics-based constraints (like in Section 3.1) because such approaches model the errors instead of the physical quantities in scientific problems. Recently, a key area in which residual modeling has been applied is in reduced order models (ROMs) of dynamical systems (described in Section 2.4). After reducing model complexity to create a ROM, an ML model can be used to model the residual due to the truncation. ROM methods were created in response to the problem of many detailed simulations being too expensive to be used in various engineering tasks including design optimization and real-time decision support. In San et al. [241,242], a simple NN used to model the error due to the model reduction is shown to sharply reduce high error regions when applied to known differential equations. Also, in Wan et al. [278], an RNN is used to model the residual between a ROM for prediction of extreme weather events and the available data projected to a reduced-order space.</p>
<p>As another example, in Kani et al. [138] a physics-driven "deep residual recurrent neural network (DR-RNN)" is proposed to find the residual minimiser of numerically discretized PDEs. Their architecture involves a stacked RNN embedded with the dynamical structure of the PDEs such that each layer of the RNN solves a layer of the residual equations. They showed that DR-RNN sharply reduces both computational cost and time discretization error of the reduced order modeling framework. Finally in Blakseth et al. [36], a feed-forward neural network is used generate a corrective source term that augments the discretized governing equation of a physics-based model for improved prediction performance. This is a more advanced residual model since the ML model is modifying the governing equation itself instead of just the output.</p>
<p>3.4.2 Output of physical model as input to ML model. In recent years many other hybrid physics-ML models have been created that extend beyond residual modeling. Another straightforward method to combine physics-based and ML models is to feed the output of a physics-based model as input to an ML model. Karpatne et al [140] showed that using the output of a physics-based model as one feature in an ML model along with inputs used to drive the physics-based model for lake temperature modeling can improve predictions. Visualization of this method is shown in Figure  4. As we discuss below, there are multiple other ways of constructing a hybrid model, including replacing part of a larger physical model, or weighting predictions from different modeling paradigms depending on context.  [140]). In the diagram, the physics-based model converts the input drivers to simulated outputs . Then, the hybrid physics-ML model jointly uses the input drivers and the simulated outputs to make the final prediction 3.4.3 Replacing part of a physical model with ML. In one variant of hybrid physics-ML models, ML models are used to replace one or more components of a physics-based model or to predict an intermediate quantity that is poorly modeled using physics. For example, to improve predictions of the discrepancy of Reynolds-Averaged Navier-Stokes (RANS) solvers in fluid dynamics, Parish et al. [212] propose a NN to estimate variables in the turbulence closure model to account for missing physics. They show this correction to traditional turbulence models results in convincing improvements of predictions. In Hamilton et al. [117], a subset of the mechanistic model's equations are replaced with data-driven nonparametric methods to improve prediction beyond the baseline process model. In Zhang et al. [312], a physics-based architecture for power system state estimation embeds a deep learning model in place of traditional predicting and optimization techniques. To do this, they substitute NN layers into an unrolled version of an existing solution framework which drastically reduced the overall computational cost due to the fast forward evaluation property of NNs, but kept information of the underlying physical models of power grids and of physical constraints. ML models for parameterization (see Section 2.3) can also be viewed as a type of hybrid modeling. The vast majority of these efforts use black box ML models [52,109,207,230], but some of these parameterization models can use more sophisticated physics-guided versions of ML as we mentioned in Section 3.1.</p>
<p>3.4.4</p>
<p>Combining predictions from both physical model and ML model. In another class of hybrid frameworks, the overall prediction is a combination of predictions from a physical model and an ML model, where the weights depend on prediction circumstance. For example, long-range interactions (e.g. gravity) can often be more easily modeled by classical physics equations than more stochastic short-range interactions (quantum mechanics) that are better modeled using data-driven alternatives. Hybrid frameworks like this have been used to adaptively combine ML predictions for short-range processes and physics model predictions for long-range processes for applications in chemical reactivity [305] and seismic activity prediction [211]. Estimator quality at a given time and location can also be used to determine whether a prediction comes from the physical model or the ML model, which was shown in Chen et al. [60] for air pollution estimation and in Vlachas et al. [276] for dynamical system forecasting more generally. In the context of solving PDEs, Malek et al [188] showcases a hybrid NN and traditional optimization technique to find the closed analytical form of the solution of a PDE. In this hybrid solver, there exist two terms, a term described by the NN and a term described by traditional optimization techniques.</p>
<p>3.4.5 ML informing or augmenting physics-model for inverse modeling. Moreover, in inverse modeling, there is a growing use of hybrid models that first use physics-based models to perform the direct inversion, then use deep learning models to further refine the inverse model's predictions. Multiple works have shown an effective application for this in computed tomography (CT) reconstructions [45,135]. Another common technique in inverse modeling of images (e.g. medical imaging, particle physics imaging), is the use of CNNs as deep image priors [271]. To simultaneously exploit data and prior knowledge, Senouf et al. [246] embed a CNN that serves as the image prior for a physics-based forward model for MRIs. Methodologies for integrating scientific knowledge in ML described in this section encompass the vast majority of work on this topic. Table 2 summarizes these by listing the requirements needed for different types of methods and the corresponding possible benefits. As we can see, depending on the context of the problem or available resources, different methods can be optimal. Hybrid methods like residual modeling are the simplest case, as they require no process-based knowledge beyond an operational mechanistic model to be used during run time. Physics-guided loss functions require additional domain expertise to determine what terms to add to the loss function, and ML cross-validation techniques are also recommended to weight the different loss function terms. Many of the foundational works on physics-guided loss functions also include open source code that could be adapted to new applications (e.g. Raissi et al. [226], Read et al. [231], Wang et al. [282]). For physics-guided initialization, domain expertise can be used to determine the most relevant synthetic data for the application, but the ML can remain process-agnostic. Physics-guided architecture is often the most complex approach, where both domain and ML expertise is needed, for example, to customize neural networks by establishing physically meaningful connections and nodes. Note that there can also be multiple Physics-ML method options for a given computational benefit. For example, incorporating physical consistency into ML models can be done through weak constraints as in a loss function, hard constraints through new architectures, or indirectly through physically consistent training data from a mechanistic model simulation.</p>
<p>Requirements and Benefits from Different Physics-ML Methodologies</p>
<p>Note that for a given application-centric objective, only some of these methods may be applicable. For example, hybrid methods will not be suitable for solving PDEs since the goal of reduced computational complexity cannot be reached if the existing solver is still needed to produce the output ( in Figure 1). Also in the case of discovering governing equations, there often isn't a known physical model to compare to for either creating a residual model or hybrid approach. Data generation applications also do not make sense for residual modeling since the purpose is to simulate a data distribution rather than improve on a physical model.</p>
<p>Many of the physics-ML methods can also be combined. For example, a physics-guided loss function, physics-guided architecture, and physics-guided initialization could all be applied to an ML model. We saw in Section 3.1 that Jia et al. [130] and Read et al. [231] in particular combined physics-guided loss functions with physics-guided initialization. Also, Karpatne et al [140] combined a physics-guided loss function with a hybrid physics-ML framework. More recently, Jia et al [133] combine physics-guided initialization and physics-guided architecture.</p>
<p>An overall goal of physics-ML methods presented in this section is to address resource efficiency issues (i.e., the ability to solve problems with less computational resources in the context of objectives defined in Section 2) while maintaining high predictive performance, sample efficiency, and interpretability relative to traditional ML approaches. For example, physics-ML methods for solving PDEs (Section 2.5) are likely to be more computationally efficient than direct numerical approaches and more physically consistent than traditional ML approaches. As another example, for the objective of downscaling (Section 2.2), physics-ML methods can be expected to provide high resolution but at much smaller computational cost than possible via traditional mechanistic models and provide much better quality output while using fewer training samples relative to traditional ML approaches. Another major utility of physics-ML methods is to reduce the overall solution search space, which has a direct impact on sample efficiency (i.e. reduced number of observations required) and the amount of computation time taken for model training. For example, physics-ML methods for discovering governing equations can be expected to work with much fewer observations and take less computation time relative to traditional ML methods. Table 3 provides a systematic organization and taxonomy of the application-centric objectives and methods of existing physics-based ML applications. This table provides a convenient organization for papers discussed in this survey and other papers that could not be discussed because of space limitations. Importantly, analysis of works within our taxonomy uncovers knowledge gaps and potential crossovers of methods between disciplines that can serve as ideas for future research.</p>
<p>AREAS OF CURRENT WORK AND POSSIBILITIES FOR CROSS-FERTILIZATION</p>
<p>Indeed, there are a myriad of opportunities for taking ideas across applications, objectives, and methods, as well as bringing them back to the traditional ML discipline. For example, the physicsguided NN approaches developed for aquatic sciences [121,132] can be used in any application where an imperfect mechanistic model is available. Raissi et al. [224] take physics-guided loss function methods to solve PDEs and extend them to inverse modeling problems in fluid dynamics. Furthermore, Daw et al. [68] develop a monotonicity-preserving architecture for lake temperature based on previous work done using loss function terms and a hybrid physics-ML architecture [140]. As another example Jia et al. [133] use physics to inform the propagation of knowledge in a graph neural network. Future research along this direction may shed light on the interpretability of hidden variables in graph neural network models and also how to build dynamic graph structures based on physics.</p>
<p>From Table 3, it is easy to see that several boxes are rather sparse or entirely empty, many of which represent opportunities for future work. For example, ML models for parameterization are increasingly being used in domains such as climate science and weather forecasting [153], all of which can benefit from the integration of physical principles. Furthermore, principles from super-resolution frameworks, originally developed in the context of computer vision applications, are beginning to be applied to downscaling to create higher resolution climate predictions [273]. However, most of these do not incorporate physics (e.g., through additional loss function terms or an informed design of architecture). The fact that nearly all of the other methods (columns) except for hybrid modeling are applicable to this task shows that there is tremendous scope for new exploration, where research pursued in these columns in the context of other objectives can be applied for this objective. We also see a lot of opportunities for new research on physics-guided initialization, where, for example, an ML algorithm could be pre-trained for inverse modeling.</p>
<p>Not all promising research directions are covered by Table 3 and the previous discussion. For instance, one promising direction is to forecast future events using ML and continuously update model states by incorporating ideas from data assimilation [137]. An instance of this is pursued by Dua et al. [80] to build an ML model that predicts the parameters of a physical model using past time series data as an input. Another instance of this approach is seen in epidemiology, where Magri et al. [187] used a NN for data assimilation to predict a parameter vector that captures the time evolution of a COVID-19 epidemiological model. Such approaches can benefit from the ideas in Section 3 (e.g., physics-based loss, intermediate physics variables, etc.). Another direction is to combine scientific knowledge and machine learning to better inform human decisions on environmental or engineering systems. For example, by using anticipated water temperature, one may build new reinforcement learning algorithms to dynamically decide when and how much water to release from reservoirs to a river network [131]. Similarly, such techniques for decision making can be used for automated control in the power plant.</p>
<p>CONCLUDING REMARKS</p>
<p>Given the current deluge of sensor data and advances in ML methods, we envision that the merging of principles from ML and physics will play an invaluable role in the future of scientific modeling to address the pressing environmental and physical modeling problems facing society. The applicationcentric objectives defined in Section 2 span the primary communities and disciplines that have both contributed to and benefit from physics-ML integration in a significant way. We believe these categories both provide perspective on the different ways of viewing the physics-ML integration methodologies in Section 3 for different purposes and also allow for coverage of a variety of disciplines that have been pursuing these ideas mostly-independently in recent years. Researchers working in one of these objectives can see how their methods fit within the taxonomy and relate them to how they are being used in other objectives. Our hope is that this survey will accelerate the cross-pollination of ideas among these diverse research communities. The discussion and structure provided in this survey also serve to benefit the ML community, where, for example, techniques for adding physical constraints to loss functions can be used to enforce fairness in predictive models, or realism for data generated by GANs. Further, novel architecture designs can enable new ways to incorporate prior domain information (beyond what is usually done using Bayesian frameworks) and can lead to better interpretability.</p>
<p>This survey focuses primarily on improving the modeling of engineering and environmental systems that are traditionally solved using mechanistic modeling. However, the general ideas discussed here for integrating scientific knowledge in ML have wider applicability, and such research is already being pursued in many other contexts. For example, there are several interesting works in system control which often involves reinforcement learning techniques (e.g., combining model predictive control with Gaussian processes in robotics [13], informed priors for neuroscience modeling [104], physics-based reward functions in computational chemistry [63], and fluidic feedback control from a cylinder [152]). Other examples include identifying features of interest in the output of computational simulations of physics-based models (e.g., high-impact weather predictions [194], segmentation of climate models [202], and tracking phenomena from climate model data [97,237]). There is also recent work on encoding domain knowledge in geometric deep learning [41,50] that is finding increasing use in computational chemistry [71,81,95], physics [25,54], hydrology [133], geostatistics [14,294], and neuroscience [155]. We expect that there will be a lot of potential for cross-over of ideas amongst these different efforts that will greatly expedite research in this nascent field.</p>
<p>Fig. 1 .
1A generic scientific problem in engineering, where ì are the dynamic inputs in time, ì is the set of static characteristics or parameters of the system, and () is the model producing target variable ì . ì and ì can also have spatial dimensions.</p>
<p>Fig. 2 .
2The Physics-Guided Recurrent Neural Network (PGRNN) model demonstrated in Jia et al.</p>
<p>Fig. 3 .
3An illustration of the concept of residual modeling where an ML model is trained to model the error made by the physics-based model . Final predictions are then the sum of the predictions made by and the residual modeled by . Processes shown in red and blue are training and testing respectively. Figure adapted from [94].</p>
<p>Fig. 4 .
4Diagram of a hybrid physics-ML model which accepts the output of a physical model as input to an ML model (Figure adapted from Karpatne et al.</p>
<p>Physics-ML Method Requirements Possible BenefitsIntermediate physical variables/processes, or hard constraints (e.g. symmetries), or task interrelationships, or informed prior distributionsTable 2. Summary of requirements and possible benefits from different physics-ML methodologies. The left column corresponds to the four types of methods described earlier in Section 3Loss Function 
Known physical relationship 
(e.g. physical laws, PDEs) </p>
<p>Physical consistency, 
Improved generalization, 
Reduced observations required, 
Improved accuracy </p>
<p>Initialization 
Synthetic data from mechanistic model 
available during training </p>
<p>Reduced observations required 
Improved accuracy </p>
<p>Architecture </p>
<p>Interpretability, 
Physical consistency, 
Improved generalization, 
Reduced solution search space, 
Improved accuracy </p>
<p>Hybrid 
Operational mechanistic model 
available during run time </p>
<p>Improved accuracy </p>
<p>Table 3 .
3Table of literature classified by objective and methodPhysics-Guided 
Loss Function 
(3.1) </p>
<p>Physics-Guided 
Initialization 
(3.2) </p>
<p>Physics-Guided 
Architecture 
(3.3) </p>
<p>Hybrid Model (3.4) </p>
<p>Residual (3.4.1) 
Other (3.4.2-3.4.5) </p>
<p>Improve or 
replace physical 
model (2.1) </p>
<p>, Vol. 1, No. 1, Article . Publication date: March 2020.Physics-Guided Machine Learning Survey
, Vol. 1, No. 1, Article . Publication date: March 2020.
Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar</p>
<ol>
<li>
<p>1st Workshop on Knowledge Guided Machine Learning (KGML): A Framework for Accelerating Scientific Discovery. 2019. ICERM Workshop on Scientific Machine LearningREFERENCES [1] 2019. ICERM Workshop on Scientific Machine Learning. https://icerm.brown.edu/events/ht19-1-sml/ [2] 2020. 1st Workshop on Knowledge Guided Machine Learning (KGML): A Framework for Accelerating Scientific Discovery. https://sites.google.com/umn.edu/kgml/workshop [3] 2020. AAAI Symposium on Physics-Guided AI. https://sites.google.com/vt.edu/pgai-aaai-20</p>
</li>
<li>
<p>IGARS 2020 Symposium on Incorporating Physics into Deep Learning. 2020. IGARS 2020 Symposium on Incorporating Physics into Deep Learning. https://igarss2020.org/Papers/ ViewSession_MS.asp?Sessionid=1016</p>
</li>
<li>
<p>International Conference on Learning Representations 2020 Workshop on Integration of Deep Neural Models and Differential Equations. 2020. International Conference on Learning Representations 2020 Workshop on Integration of Deep Neural Models and Differential Equations. https://openreview.net/group?id=ICLR.cc/2020/Workshop/DeepDiffEq</p>
</li>
</ol>
<p>AAAI Spring Symposium on Combining Artificial Intelligence and Machine Learning with Physics Sciences. AAAI Spring Symposium on Combining Artificial Intelligence and Machine Learning with Physics Sciences. https://sites.google.com/view/aaai-mlps</p>
<p>Integrating knowledge and reasoning in image understanding. S Aditya, C Yang, Baral, arXiv:1906.09954S Aditya, Y Yang, and C Baral. 2019. Integrating knowledge and reasoning in image understanding. arXiv:1906.09954 (2019).</p>
<p>Integrating machine learning and multiscale modeling-perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences. M Alber, NPJ Digit. Med. 2M Alber et al. 2019. Integrating machine learning and multiscale modeling-perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences. NPJ Digit. Med. 2, 1 (2019), 1-11.</p>
<p>Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning. B Alipanahi, Nature biotechnology. 33B Alipanahi et al. 2015. Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning. Nature biotechnology 33, 8 (2015), 831-838.</p>
<p>Latent force models. M Alvarez, Artificial Intelligence and Statistics. M. Alvarez et al. 2009. Latent force models. In Artificial Intelligence and Statistics. 9-16.</p>
<p>Interpolation method for adapting reduced-order models and application to aeroelasticity. D Amsallem, C Farhat, AIAA journal. 46D Amsallem and C Farhat. 2008. Interpolation method for adapting reduced-order models and application to aeroelasti- city. AIAA journal 46, 7 (2008), 1803-1813.</p>
<p>Cormorant: Covariant molecular neural networks. B Anderson, R Hy, Kondor, In Adv. Neural Inf. Process. Syst. B Anderson, TS Hy, and R Kondor. 2019. Cormorant: Covariant molecular neural networks. In Adv. Neural Inf. Process. Syst. 14510-14519.</p>
<p>Model-based reinforcement learning in continuous environments using real-time constrained optimization. O Andersson, P Heintz, Doherty, AAAI. O Andersson, F Heintz, and P Doherty. 2015. Model-based reinforcement learning in continuous environments using real-time constrained optimization. In AAAI.</p>
<p>Kriging Convolutional Networks. G Appleby, L Liu, Liu, AAAI. G Appleby, L Liu, and L Liu. 2020. Kriging Convolutional Networks.. In AAAI. 3187-3194.</p>
<p>Machine learning for many-body physics: The case of the Anderson impurity model. L Arsenault, Phys. Rev. B. L Arsenault et al. 2014. Machine learning for many-body physics: The case of the Anderson impurity model. Phys. Rev. B (2014).</p>
<p>Forecasting Sequential Data using Consistent Koopman Autoencoders. O Azencot, arXiv:2003.02236O Azencot et al. 2020. Forecasting Sequential Data using Consistent Koopman Autoencoders. arXiv:2003.02236 (2020).</p>
<p>Y Ba, arXiv:1903.10210Physics-based neural networks for shape from polarization. Y Ba et al. 2019. Physics-based neural networks for shape from polarization. arXiv:1903.10210 (2019).</p>
<p>Y Ba, A Zhao, Kadambi, arXiv:1910.00201Blending diverse physical priors with neural networks. Y Ba, G Zhao, and A Kadambi. 2019. Blending diverse physical priors with neural networks. arXiv:1910.00201 (2019).</p>
<p>Injecting knowledge in data-driven vehicle trajectory predictors. M Bahari, A Nejjar, Alahi, Transp. Res. Part C Emerg. 128103010M Bahari, I Nejjar, and A Alahi. 2021. Injecting knowledge in data-driven vehicle trajectory predictors. Transp. Res. Part C Emerg. 128 (2021), 103010.</p>
<p>Designing neural network architectures using reinforcement learning. B Baker, arXiv:1611.02167B Baker et al. 2016. Designing neural network architectures using reinforcement learning. arXiv:1611.02167 (2016).</p>
<p>Workshop report on basic research needs for scientific machine learning: Core technologies for artificial intelligence. N Baker, USDOE Office of ScienceTechnical ReportN Baker et al. 2019. Workshop report on basic research needs for scientific machine learning: Core technologies for artificial intelligence. Technical Report. USDOE Office of Science.</p>
<p>Jet substructure classification in high-energy physics with deep neural networks. P Baldi, Phys. Rev. D. 9394034P Baldi et al. 2016. Jet substructure classification in high-energy physics with deep neural networks. Phys. Rev. D 93, 9 (2016), 094034.</p>
<p>Learning data-driven discretizations for partial differential equations. Y Bar-Sinai, PNAS. 116Y Bar-Sinai et al. 2019. Learning data-driven discretizations for partial differential equations. PNAS 116, 31 (2019), 15344-15349.</p>
<p>Physics-Informed Machine Learning for DRAM Error Modeling. E Baseman, IEEE DFT. E Baseman et al. 2018. Physics-Informed Machine Learning for DRAM Error Modeling. In IEEE DFT.</p>
<p>Interaction networks for learning about objects, relations and physics. P Battaglia, Adv. Neural Inf. Process. Syst. P Battaglia et al. 2016. Interaction networks for learning about objects, relations and physics. In Adv. Neural Inf. Process. Syst. 4502-4510.</p>
<p>Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations. C Beck, A Weinan, Jentzen, J Nonlinear Sci. 29C Beck, E Weinan, and A Jentzen. 2019. Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations. J Nonlinear Sci. 29, 4 (2019), 1563-1619.</p>
<p>Neural network potential-energy surfaces in chemistry: a tool for large-scale simulations. Behler, Phys. Chem. 13J Behler. 2011. Neural network potential-energy surfaces in chemistry: a tool for large-scale simulations. Phys. Chem 13, 40 (2011), 17930-17955.</p>
<p>Generalized neural-network representation of high-dimensional potential-energy surfaces. J Behler, M Parrinello, Phys. Rev. Lett. 98146401J Behler and M Parrinello. 2007. Generalized neural-network representation of high-dimensional potential-energy surfaces. Phys. Rev. Lett. 98, 14 (2007), 146401.</p>
<p>Deep learned process parameterizations provide better representations of turbulent heat fluxes in hydrologic models. A Bennett, Nijssen, Water Resour. Res. A Bennett and B Nijssen. 2020. Deep learned process parameterizations provide better representations of turbulent heat fluxes in hydrologic models. Water Resour. Res. (2020), e2020WR029328.</p>
<p>Machine learning for data-driven discovery in solid Earth geoscience. J Karianne, Bergen, Science. 363323Karianne J Bergen et al. 2019. Machine learning for data-driven discovery in solid Earth geoscience. Science 363, 6433 (2019), eaau0323.</p>
<p>Achieving conservation of energy in neural network emulators for climate modeling. P Beucler, arXiv:1906.06622P Beucler et al. 2019. Achieving conservation of energy in neural network emulators for climate modeling. arXiv:1906.06622 (2019).</p>
<p>T Beucler, arXiv:1909.00912Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems. T Beucler et al. 2019. Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems. arXiv:1909.00912 (2019).</p>
<p>Fast wavelet transforms and numerical algorithms I. G Beylkin, V Coifman, Rokhlin, Commun. Pure Appl. Math. 44G Beylkin, R Coifman, and V Rokhlin. 1991. Fast wavelet transforms and numerical algorithms I. Commun. Pure Appl. Math. 44, 2 (1991), 141-183.</p>
<p>Multi-output local Gaussian process regression: Applications to uncertainty quantification. I Bilionis, Zabaras, J. Comput. Phys. I Bilionis and N Zabaras. 2012. Multi-output local Gaussian process regression: Applications to uncertainty quantifica- tion. J. Comput. Phys (2012).</p>
<p>Prestack and poststack inversion using a physics-guided convolutional neural network. R Biswas, Interpretation. 7R Biswas et al. 2019. Prestack and poststack inversion using a physics-guided convolutional neural network. Interpret- ation 7, 3 (2019), SE161-SE174.</p>
<p>Deep neural network enabled corrective source term approach to hybrid analysis and modeling. Ss Blakseth, Neural Netw. 146SS Blakseth et al. 2022. Deep neural network enabled corrective source term approach to hybrid analysis and modeling. Neural Netw 146 (2022), 181-199.</p>
<p>Using physics-informed enhanced super-resolution generative adversarial networks for subfilter modeling in turbulent reactive flows. M Bode, Proc Combust Inst. M Bode et al. 2021. Using physics-informed enhanced super-resolution generative adversarial networks for subfilter modeling in turbulent reactive flows. Proc Combust Inst. (2021).</p>
<p>Automated reverse engineering of nonlinear dynamical systems. J Bongard, H Lipson, 10.1073/pnas.0609476104PNAS. J Bongard and H Lipson. 2007. Automated reverse engineering of nonlinear dynamical systems. PNAS (2007). https://doi.org/10.1073/pnas.0609476104</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. Bousmalis K Others, IEEE ICRA. IEEE. K others Bousmalis. 2018. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. In IEEE ICRA. IEEE, 4243-4250.</p>
<p>Prognostic validation of a neural network unified physics parameterization. N D Brenowitz, Bretherton, Geophys. Res. Lett. 45ND Brenowitz and CS Bretherton. 2018. Prognostic validation of a neural network unified physics parameterization. Geophys. Res. Lett. 45, 12 (2018), 6289-6298.</p>
<p>Geometric deep learning: going beyond euclidean data. Mm Bronstein, IEEE Signal Process. Mag. 34MM Bronstein et al. 2017. Geometric deep learning: going beyond euclidean data. IEEE Signal Process. Mag. 34, 4 (2017), 18-42.</p>
<p>Koopman operator theory: Past, present, and future. S Brunton, APSS Brunton et al. 2017. Koopman operator theory: Past, present, and future. APS (2017), L27-004.</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S Brunton, J Proctor, Kutz, PNAS. S Brunton, J Proctor, and J Kutz. 2016. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. PNAS (2016).</p>
<p>Quadratic residual networks: A new class of neural networks for solving forward and inverse problems in physics involving pdes. J Bu, Karpatne, SDM. SIAM. J Bu and A Karpatne. 2021. Quadratic residual networks: A new class of neural networks for solving forward and inverse problems in physics involving pdes. In SDM. SIAM, 675-683.</p>
<p>Learning the invisible: A hybrid deep learning-shearlet framework for limited angle computed tomography. T A Bubba, Inverse Probl. T A Bubba et al. 2019. Learning the invisible: A hybrid deep learning-shearlet framework for limited angle computed tomography. Inverse Probl. (2019).</p>
<p>S Cai, arXiv:2105.09506Physics-informed neural networks (PINNs) for fluid mechanics: A review. S Cai et al. 2021. Physics-informed neural networks (PINNs) for fluid mechanics: A review. arXiv:2105.09506 (2021).</p>
<p>Statistical significance of climate sensitivity predictors obtained by data mining. P M Caldwell, Geophys. Res. Let. 41PM Caldwell et al. 2014. Statistical significance of climate sensitivity predictors obtained by data mining. Geophys. Res. Let. 41, 5 (2014), 1803-1808.</p>
<p>Physics-aware Gaussian processes in remote sensing. G Camps-Valls, Appl. Soft Comput. 68G Camps-Valls et al. 2018. Physics-aware Gaussian processes in remote sensing. Appl. Soft Comput. 68 (2018), 69-82.</p>
<p>Improving direct physical properties prediction of heterogeneous materials from imaging data via convolutional neural network and a morphology-aware generative model. R Cang, H Li, Y Yao, Y Jiao, Ren, Computational Materials Science. 150R Cang, H Li, H Yao, Y Jiao, and Y Ren. 2018. Improving direct physical properties prediction of heterogeneous ma- terials from imaging data via convolutional neural network and a morphology-aware generative model. Computational Materials Science 150 (2018), 212-221.</p>
<p>A comprehensive survey on geometric deep learning. W Cao, IEEE Access. 8W Cao et al. 2020. A comprehensive survey on geometric deep learning. IEEE Access 8 (2020), 35929-35949.</p>
<p>Solving the quantum many-body problem with artificial neural networks. G Carleo, M Troyer, Science. 355G Carleo and M Troyer. 2017. Solving the quantum many-body problem with artificial neural networks. Science 355, 6325 (2017), 602-606.</p>
<p>Parametrization and generation of geological models with generative adversarial networks. S Chan, Elsheikh, arXiv:1708.01810S Chan and A Elsheikh. 2017. Parametrization and generation of geological models with generative adversarial networks. arXiv:1708.01810 (2017).</p>
<p>B Chang, arXiv:1902.09689AntisymmetricRNN: A dynamical system view on recurrent neural networks. B Chang et al. 2019. AntisymmetricRNN: A dynamical system view on recurrent neural networks. arXiv:1902.09689 (2019).</p>
<p>A compositional object-based approach to learning physical dynamics. Chang Mb, arXiv:1612.00341MB Chang et al. 2016. A compositional object-based approach to learning physical dynamics. arXiv:1612.00341 (2016).</p>
<p>Fusing physics-based and deep learning models for prognostics. Ma Chao, Reliab. Eng. Syst. Saf. 217107961MA Chao et al. 2022. Fusing physics-based and deep learning models for prognostics. Reliab. Eng. Syst. Saf. 217 (2022), 107961.</p>
<p>Support-vector-machine-based reduced-order model for limit cycle oscillation prediction of nonlinear aeroelastic system. G Chen, Math. Probl. Eng. G Chen et al. 2012. Support-vector-machine-based reduced-order model for limit cycle oscillation prediction of nonlinear aeroelastic system. Math. Probl. Eng 2012 (2012).</p>
<p>Low-dose CT with a residual encoder-decoder convolutional neural network. H Chen, IEEEH Chen et al. 2017. Low-dose CT with a residual encoder-decoder convolutional neural network. IEEE T-MI (2017).</p>
<p>Neural ordinary differential equations. Tq Chen, NIPS. TQ Chen et al. 2018. Neural ordinary differential equations. In NIPS.</p>
<p>BezierGAN: Automatic Generation of Smooth Curves from Interpretable Low-Dimensional Parameters. W Chen, M Fuge, arXiv:1808.08871W Chen and M Fuge. 2018. BezierGAN: Automatic Generation of Smooth Curves from Interpretable Low-Dimensional Parameters. arXiv:1808.08871 (2018).</p>
<p>Pga: Physics guided and adaptive approach for mobile fine-grained air pollution estimation. X Chen, Ubicomp. X Chen et al. 2018. Pga: Physics guided and adaptive approach for mobile fine-grained air pollution estimation. In Ubicomp.</p>
<p>Z Chen, arXiv:1909.13334Symplectic Recurrent Neural Networks. Z Chen et al. 2019. Symplectic Recurrent Neural Networks. arXiv:1909.13334 (2019).</p>
<p>Deep learning of physical laws from scarce data. Z Chen, H Liu, Sun, arXiv:2005.03448Z Chen, Y Liu, and H Sun. 2020. Deep learning of physical laws from scarce data. arXiv:2005.03448 (2020).</p>
<p>Physics-guided reinforcement learning for 3D molecular structures. Y Cho, In NeurIPSY Cho et al. 2019. Physics-guided reinforcement learning for 3D molecular structures. In NeurIPS.</p>
<p>A Choudhary, arXiv:1912.01958Physics enhanced neural networks predict order and chaos. A Choudhary et al. 2019. Physics enhanced neural networks predict order and chaos. arXiv:1912.01958 (2019).</p>
<p>Uncertainty quantification for porous media flows. M Christie, D Demyanov, Erbas, J. Comput. Phys. M Christie, V Demyanov, and D Erbas. 2006. Uncertainty quantification for porous media flows. J. Comput. Phys (2006).</p>
<p>Ts Cohen, arXiv:1801.10130Spherical cnns. TS Cohen et al. 2018. Spherical cnns. arXiv:1801.10130 (2018).</p>
<p>Model order reduction assisted by deep neural networks (ROM-net). T Daniel, Adv. Model. Simul. Eng. 7T Daniel et al. 2020. Model order reduction assisted by deep neural networks (ROM-net). Adv. Model. Simul. Eng. 7 (2020), 1-27.</p>
<p>A Daw, arXiv:1911.02682Physics-Guided Architecture (PGA) of Neural Networks for Quantifying Uncertainty in Lake Temperature Modeling. A Daw et al. 2019. Physics-Guided Architecture (PGA) of Neural Networks for Quantifying Uncertainty in Lake Temperature Modeling. arXiv:1911.02682 (2019).</p>
<p>Inversion of surface parameters using fast learning neural networks. Ms Dawson, IGARSS Symposium. MS Dawson et al. 1992. Inversion of surface parameters using fast learning neural networks. IGARSS Symposium (1992).</p>
<p>Deep learning for physical processes: Incorporating prior scientific knowledge. De Bezenac, P Pajot, Gallinari, J. Stat. Mech.: Theory Exp. 12124009E de Bezenac, A Pajot, and P Gallinari. 2019. Deep learning for physical processes: Incorporating prior scientific knowledge. J. Stat. Mech.: Theory Exp 2019, 12 (2019), 124009.</p>
<p>MolGAN: An implicit generative model for small molecular graphs. Nicola De, Cao , Thomas Kipf, arXiv:1805.11973Nicola De Cao and Thomas Kipf. 2018. MolGAN: An implicit generative model for small molecular graphs. arXiv:1805.11973 (2018).</p>
<p>Learning particle physics by example: location-aware generative adversarial networks for physics synthesis. M L De Oliveira, B Paganini, Nachman, Comput. Softw. Big Sci. L de Oliveira, M Paganini, and B Nachman. 2017. Learning particle physics by example: location-aware generative adversarial networks for physics synthesis. Comput. Softw. Big Sci (2017).</p>
<p>Deep generative image models using a laplacian pyramid of adversarial networks. El Denton, NIPS. EL Denton et al. 2015. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS.</p>
<p>Uncertainty in climate change projections: the role of internal variability. C Deser, V Phillips, H Bourdette, Teng, Climate dynamics. C Deser, A Phillips, V Bourdette, and H Teng. 2012. Uncertainty in climate change projections: the role of internal variability. Climate dynamics (2012).</p>
<p>Neural-network-based approximations for solving partial differential equations. N Mwmg Dissanayake, Phan-Thien, Commun. Numer. Methods Eng. 10MWMG Dissanayake and N Phan-Thien. 1994. Neural-network-based approximations for solving partial differential equations. Commun. Numer. Methods Eng. 10, 3 (1994), 195-201.</p>
<p>Physics-informed echo state networks for chaotic systems forecasting. Nak Doan, L Polifke, Magri, ICCS. SpringerNAK Doan, W Polifke, and L Magri. 2019. Physics-informed echo state networks for chaotic systems forecasting. In ICCS. Springer.</p>
<p>A hybrid model approach for forecasting future residential electricity consumption. B Dong, Li, R Smm Rahman, Vega, Energy Build. B Dong, Z Li, SMM Rahman, and R Vega. 2016. A hybrid model approach for forecasting future residential electricity consumption. Energy Build. (2016).</p>
<p>Physics-Informed Neural Networks for Bias Compensation in Corrosion-Fatigue. D Arinan, Felipe Dourado, Viana, AIAA Forum. 1149. Arinan D Dourado and Felipe Viana. 2020. Physics-Informed Neural Networks for Bias Compensation in Corrosion- Fatigue. In AIAA Forum. 1149.</p>
<p>Use of theory-guided neural networks to perform seismic inversion. Je Downton, Hampson, GeoSoftware, CGG. JE Downton and DP Hampson. 2019. Use of theory-guided neural networks to perform seismic inversion. In GeoSoftware, CGG.</p>
<p>An artificial neural network approximation based decomposition approach for parameter estimation of system of ordinary differential equations. V Dua, Comput. Chem. Eng. 35V Dua. 2011. An artificial neural network approximation based decomposition approach for parameter estimation of system of ordinary differential equations. Comput. Chem. Eng. 35, 3 (2011), 545-553.</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. Dk Duvenaud, Adv. Neural Inf. Process. Syst. DK Duvenaud et al. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Adv. Neural Inf. Process. Syst. 2224-2232.</p>
<p>Solution of Biharmonic Equation in Complicated Geometries with Physics Informed Extreme Learning Machine. V Dwivedi, Srinivasan, J. Comput. Inf. Sci. Eng. V Dwivedi and B Srinivasan. 2020. Solution of Biharmonic Equation in Complicated Geometries with Physics Informed Extreme Learning Machine. J. Comput. Inf. Sci. Eng. (2020), 1-10.</p>
<p>Equation discovery with ecological applications. S Džeroski, Machine learning methods for ecological applications. SpringerS Džeroski et al. 1999. Equation discovery with ecological applications. In Machine learning methods for ecological applications. Springer, 185-207.</p>
<p>Ebert-Uphoff, arXiv:2106.09757CIRA Guide to Custom Loss Functions for Neural Networks in Environmental Sciences-Version 1. I Ebert-Uphoff et al. 2021. CIRA Guide to Custom Loss Functions for Neural Networks in Environmental Sciences- Version 1. arXiv:2106.09757 (2021).</p>
<p>T Elsken, F Jh Metzen, Hutter, arXiv:1808.05377Neural architecture search: A survey. T Elsken, JH Metzen, and F Hutter. 2018. Neural architecture search: A survey. arXiv:1808.05377 (2018).</p>
<p>Nb Erichson, M W Muehlebach, Mahoney, arXiv:1905.10866Physics-informed autoencoders for lyapunov-stable fluid flow prediction. NB Erichson, M Muehlebach, and MW Mahoney. 2019. Physics-informed autoencoders for lyapunov-stable fluid flow prediction. arXiv:1905.10866 (2019).</p>
<p>A big data guide to understanding climate change: The case for theory-guided data science. J H Faghmous, Kumar, Big data. 2JH Faghmous and V Kumar. 2014. A big data guide to understanding climate change: The case for theory-guided data science. Big data 2, 3 (2014), 155-163.</p>
<p>A multiscale neural network based on hierarchical matrices. Y Fan, Multiscale Model Simul. 17Y Fan et al. 2019. A multiscale neural network based on hierarchical matrices. Multiscale Model Simul. 17, 4 (2019), 1189-1213.</p>
<p>Solving electrical impedance tomography with deep learning. Y Fan, Ying, J. Comput. Phys. 404109119Y Fan and L Ying. 2020. Solving electrical impedance tomography with deep learning. J. Comput. Phys 404 (2020), 109119.</p>
<p>A High-Efficient Hybrid Physics-Informed Neural Networks Based on Convolutional Neural Network. Fang, IEEE Trans Neural Netw Learn Syst. Z Fang. 2021. A High-Efficient Hybrid Physics-Informed Neural Networks Based on Convolutional Neural Network. IEEE Trans Neural Netw Learn Syst. (2021).</p>
<p>Ab Farimani, V S Gomes, Pande, arXiv:1709.02432Deep learning the physics of transport phenomena. AB Farimani, J Gomes, and VS Pande. 2017. Deep learning the physics of transport phenomena. arXiv:1709.02432 (2017).</p>
<p>Predicting AC optimal power flows: Combining deep learning and lagrangian dual methods. F Fioretto, Mak, Van Hentenryck, AAAI. 34F Fioretto, TWK Mak, and T Van Hentenryck. 2020. Predicting AC optimal power flows: Combining deep learning and lagrangian dual methods. In AAAI, Vol. 34. 630-637.</p>
<p>A first course in finite elements. J Fish, Belytschko, WileyJ Fish and T Belytschko. 2007. A first course in finite elements. Wiley.</p>
<p>Combining Semi-Physical and Neural Network Modeling: An Example ofIts Usefulness. IFAC Proceedings Volumes. U Forssell, P Lindskog, 30U Forssell and P Lindskog. 1997. Combining Semi-Physical and Neural Network Modeling: An Example ofIts Usefulness. IFAC Proceedings Volumes 30, 11 (1997), 767-770.</p>
<p>Protein interface prediction using graph convolutional networks. Alex Fout, Adv. Neural Inf. Process. Syst. Alex Fout et al. 2017. Protein interface prediction using graph convolutional networks. In Adv. Neural Inf. Process. Syst. 6530-6539.</p>
<p>Surrogate and reduced-order modeling: A comparison of approaches for large-scale statistical inverse problems. Large-scale inverse problems and quantification of uncertainty. M Frangos, 123149M Frangos et al. 2010. Surrogate and reduced-order modeling: A comparison of approaches for large-scale statistical inverse problems. Large-scale inverse problems and quantification of uncertainty 123149 (2010).</p>
<p>Storm-based probabilistic hail forecasting with machine learning applied to convection-allowing ensembles. Dj Gagne, Weather Forecast. 32DJ Gagne et al. 2017. Storm-based probabilistic hail forecasting with machine learning applied to convection-allowing ensembles. Weather Forecast. 32, 5 (2017), 1819-1840.</p>
<p>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Y Gal, Ghahramani, ICML. Y Gal and Z Ghahramani. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML.</p>
<p>Non-linear model reduction for uncertainty quantification in large-scale inverse problems. D Galbally, Int. J. Numer. Methods Fluids. D Galbally et al. 2010. Non-linear model reduction for uncertainty quantification in large-scale inverse problems. Int. J. Numer. Methods Fluids (2010).</p>
<p>Toward enhanced understanding and projections of climate extremes using physics-guided data mining techniques. A R Ganguly, Nonlinear Process. Geophys. 21AR Ganguly et al. 2014. Toward enhanced understanding and projections of climate extremes using physics-guided data mining techniques. Nonlinear Process. Geophys 21, 4 (2014), 777-795.</p>
<p>Gaussian process modelling of latent chemical species: applications to inferring transcription factor activities. P Gao, Bioinformatics. 24P Gao et al. 2008. Gaussian process modelling of latent chemical species: applications to inferring transcription factor activities. Bioinformatics 24, 16 (2008), i70-i75.</p>
<p>Modeling the dynamics of PDE systems with physics-constrained deep auto-regressive networks. N Geneva, Zabaras, J. Comput. Phys. N Geneva and N Zabaras. 2020. Modeling the dynamics of PDE systems with physics-constrained deep auto-regressive networks. J. Comput. Phys (2020).</p>
<p>Could machine learning break the convection parameterization deadlock?. P Gentine, Geophys. Res. Lett. P Gentine et al. 2018. Could machine learning break the convection parameterization deadlock? Geophys. Res. Lett (2018).</p>
<p>Empirical priors for reinforcement learning models. Sj Gershman, J. Math. Psychol. 71SJ Gershman. 2016. Empirical priors for reinforcement learning models. J. Math. Psychol. 71 (2016), 1-6.</p>
<p>Information processing, data inferences, and scientific generalization. D Gerwin, Behavioral Science. 19D Gerwin. 1974. Information processing, data inferences, and scientific generalization. Behavioral Science 19, 5 (1974), 314-325.</p>
<p>Recent developments in fast and scalable inverse modeling and data assimilation methods in hydrology. H Ghorbanidehno, J. Hydrol. 125266H Ghorbanidehno et al. 2020. Recent developments in fast and scalable inverse modeling and data assimilation methods in hydrology. J. Hydrol. (2020), 125266.</p>
<p>Spatiotemporal feature extraction with data-driven Koopman operators. D Giannakis, Z Slawinska, Zhao, Feature Extraction: Modern Questions and Challenges. D Giannakis, J Slawinska, and Z Zhao. 2015. Spatiotemporal feature extraction with data-driven Koopman operators. In Feature Extraction: Modern Questions and Challenges. 103-115.</p>
<p>Accurate interatomic force fields via machine learning with covariant kernels. A Glielmo, A De Sollich, Vita, Phys. Rev. B. 95214302A Glielmo, P Sollich, and A De Vita. 2017. Accurate interatomic force fields via machine learning with covariant kernels. Phys. Rev. B 95, 21 (2017), 214302.</p>
<p>Data-driven components in a model of inner-shelf sorted bedforms: a new hybrid model. Eb Goldstein, Coco, Murray, Mo Green, Earth Surf. Dyn. EB Goldstein, G Coco, AB Murray, and MO Green. 2014. Data-driven components in a model of inner-shelf sorted bedforms: a new hybrid model. Earth Surf. Dyn. (2014).</p>
<p>. G H Golub, Van Loan, Matrix computations. 3JHU pressGH Golub and CF Van Loan. 2012. Matrix computations. Vol. 3. JHU press.</p>
<p>Physics-guided machine learning for self-aware machining. Np Greis, AAAI Symposium -AI and Manufacturing. NP Greis et al. 2020. Physics-guided machine learning for self-aware machining. In AAAI Symposium -AI and Manufacturing.</p>
<p>S Greydanus, J Dzamba, Yosinski, Hamiltonian neural networks. In NIPS. S Greydanus, M Dzamba, and J Yosinski. 2019. Hamiltonian neural networks. In NIPS.</p>
<p>A deep hybrid model for weather forecasting. A Grover, ACM SIGKDD. A Grover et al. 2015. A deep hybrid model for weather forecasting. In ACM SIGKDD. 379-386.</p>
<p>Long text generation via adversarial training with leaked information. J Guo, AAAI. J Guo et al. 2018. Long text generation via adversarial training with leaked information. In AAAI.</p>
<p>Data-driven reduced order modeling for time-dependent problems. M Guo, Hesthaven, Comput. Methods Appl. Mech. Eng. 345M Guo and JS Hesthaven. 2019. Data-driven reduced order modeling for time-dependent problems. Comput. Methods Appl. Mech. Eng. 345 (2019), 75-99.</p>
<p>Debates-The future of hydrological sciences: A (common) path forward? Using models and data to learn: A systems theoretic perspective on the future of hydrological science. Hv Gupta, Water Resour. Res. HV Gupta et al. 2014. Debates-The future of hydrological sciences: A (common) path forward? Using models and data to learn: A systems theoretic perspective on the future of hydrological science. Water Resour. Res. (2014).</p>
<p>Hybrid modeling and prediction of dynamical systems. F Hamilton, PLOS Comput. Biol. F Hamilton et al. 2017. Hybrid modeling and prediction of dynamical systems. PLOS Comput. Biol (2017).</p>
<p>Solving high-dimensional partial differential equations using deep learning. J Han, E Jentzen, Weinan, PNASJ Han, A Jentzen, and E Weinan. 2018. Solving high-dimensional partial differential equations using deep learning. PNAS (2018).</p>
<p>Solving many-electron Schrödinger equation using deep neural networks. J Han, E Zhang, Weinan, J. Comput. Phys. 399108929J Han, L Zhang, and E Weinan. 2019. Solving many-electron Schrödinger equation using deep neural networks. J. Comput. Phys 399 (2019), 108929.</p>
<p>Assessment and validation of machine learning methods for predicting molecular atomization energies. K Hansen, J. Chem. Theory Comput. 9K Hansen et al. 2013. Assessment and validation of machine learning methods for predicting molecular atomization energies. J. Chem. Theory Comput 9, 8 (2013), 3404-3419.</p>
<p>Predicting lake surface water phosphorus dynamics using process-guided machine learning. Pc Hanson, Ecol Modell. 430109136PC Hanson et al. 2020. Predicting lake surface water phosphorus dynamics using process-guided machine learning. Ecol Modell 430 (2020), 109136.</p>
<p>Deep residual learning for image recognition. K He, S Zhang, J Ren, Sun, CVPR. K He, X Zhang, S Ren, and J Sun. 2016. Deep residual learning for image recognition. In CVPR.</p>
<p>Field inversion and machine learning with embedded neural networks: Physics-consistent neural network training. Holland, AIAA Forum. 3200JR Holland et al. 2019. Field inversion and machine learning with embedded neural networks: Physics-consistent neural network training. In AIAA Forum. 3200.</p>
<p>Ww Hsieh, Machine learning methods in the environmental sciences: Neural networks and kernels. Cambridge university pressWW Hsieh. 2009. Machine learning methods in the environmental sciences: Neural networks and kernels. Cambridge university press.</p>
<p>X Hu, arXiv:2002.00097Physics-Guided Deep Neural Networks for PowerFlow Analysis. X Hu et al. 2020. Physics-Guided Deep Neural Networks for PowerFlow Analysis. arXiv:2002.00097 (2020).</p>
<p>Automated machine learning: methods, systems, challenges. F Hutter, J Kotthoff, Vanschoren, Springer NatureF Hutter, L Kotthoff, and J Vanschoren. 2019. Automated machine learning: methods, systems, challenges. Springer Nature.</p>
<p>Short-term forecasting of the wave energy flux: Analogues, random forests, and physics-based models. G Ibarra-Berastegi, Ocean Eng. 104G Ibarra-Berastegi et al. 2015. Short-term forecasting of the wave energy flux: Analogues, random forests, and physics-based models. Ocean Eng. 104 (2015), 530-539.</p>
<p>Statistics, data mining, and machine learning in astronomy: a practical Python guide for the analysis of survey data. Z Ivezić, Princeton University PressZ Ivezić et al. 2019. Statistics, data mining, and machine learning in astronomy: a practical Python guide for the analysis of survey data. Princeton University Press.</p>
<p>Bringing automated, remote-sensed, machine learning methods to monitoring crop landscapes at scale. X Jia, Agric Econ. 50X Jia et al. 2019. Bringing automated, remote-sensed, machine learning methods to monitoring crop landscapes at scale. Agric Econ 50 (2019), 41-50.</p>
<p>Physics Guided RNNs for Modeling Dynamical Systems: A Case Study in Simulating Lake Temperature Profiles. X Jia, SDM. SIAMX Jia et al. 2019. Physics Guided RNNs for Modeling Dynamical Systems: A Case Study in Simulating Lake Temperature Profiles. In SDM. SIAM.</p>
<p>Graph-based reinforcement learning for active learning in real time: An application in modeling river networks. X Jia, SDM. SIAM. X Jia et al. 2021. Graph-based reinforcement learning for active learning in real time: An application in modeling river networks. In SDM. SIAM, 621-629.</p>
<p>Physics-guided machine learning for scientific discovery: An application in simulating lake temperature profiles. X Jia, ACM/IMS J. of Data Sci. 2X Jia et al. 2021. Physics-guided machine learning for scientific discovery: An application in simulating lake temperature profiles. ACM/IMS J. of Data Sci. 2, 3 (2021), 1-26.</p>
<p>Physics-Guided Recurrent Graph Model for Predicting Flow and Temperature in River Networks. X Jia, SDM. SIAM. X Jia et al. 2021. Physics-Guided Recurrent Graph Model for Predicting Flow and Temperature in River Networks. In SDM. SIAM, 612-620.</p>
<p>Jiang, arXiv:2005.01463Meshfreeflownet: A physics-constrained deep continuous space-time super-resolution framework. CM Jiang et al. 2020. Meshfreeflownet: A physics-constrained deep continuous space-time super-resolution framework. arXiv:2005.01463 (2020).</p>
<p>Deep convolutional neural network for inverse problems in imaging. Kh Jin, IEEE Trans. Image Process. KH Jin et al. 2017. Deep convolutional neural network for inverse problems in imaging. IEEE Trans. Image Process (2017).</p>
<p>Obstacle segmentation based on the wave equation and deep learning. A Kahana, J. Comput. Phys. 109458A Kahana et al. 2020. Obstacle segmentation based on the wave equation and deep learning. J. Comput. Phys (2020), 109458.</p>
<p>Atmospheric modeling, data assimilation and predictability. Kalnay, Cambridge university pressE Kalnay. 2003. Atmospheric modeling, data assimilation and predictability. Cambridge university press.</p>
<p>J Kani, Elsheikh, arXiv:1709.00939DR-RNN: A deep residual recurrent neural network for model reduction. J Kani and A Elsheikh. 2017. DR-RNN: A deep residual recurrent neural network for model reduction. arXiv:1709.00939 (2017).</p>
<p>Physicsinformed machine learning. George Em Karniadakis, G Ioannis, Lu Kevrekidis, Paris Lu, Sifan Perdikaris, Liu Wang, Yang, Nature Reviews Physics. 3George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. 2021. Physics- informed machine learning. Nature Reviews Physics 3, 6 (2021), 422-440.</p>
<p>Physics-guided neural networks (PGNN): An application in lake temperature modeling. A Karpatne, arXiv:1710.11431A Karpatne et al. 2017. Physics-guided neural networks (PGNN): An application in lake temperature modeling. arXiv:1710.11431 (2017).</p>
<p>Theory-guided data science: A new paradigm for scientific discovery from data. A Karpatne, IEEE TKDE. A Karpatne et al. 2017. Theory-guided data science: A new paradigm for scientific discovery from data. IEEE TKDE (2017).</p>
<p>Machine learning for the geosciences: Challenges and opportunities. A Karpatne, IEEE TKDE. A Karpatne et al. 2018. Machine learning for the geosciences: Challenges and opportunities. IEEE TKDE (2018).</p>
<ol>
<li>Knowledge-guided machine learning: Accelerating discovery using scientific knowledge and data. A Karpatne, Ramakrishnan, V KumarCRC PressA Karpatne, N Ramakrishnan, and V Kumar (Eds.). 2022. Knowledge-guided machine learning: Accelerating discovery using scientific knowledge and data. CRC Press.</li>
</ol>
<p>Simulator-free solution of high-dimensional stochastic elliptic partial differential equations using deep neural networks. S Karumuri, J. Comput. Phys. S Karumuri et al. 2020. Simulator-free solution of high-dimensional stochastic elliptic partial differential equations using deep neural networks. J. Comput. Phys (2020).</p>
<p>Enforcing physical constraints in cnns through differentiable pde layer. K Kashinath, ICLR Workshop on Integration of Deep Neural Models and Differential Equations. K Kashinath et al. 2020. Enforcing physical constraints in cnns through differentiable pde layer. In ICLR Workshop on Integration of Deep Neural Models and Differential Equations.</p>
<p>Physics-informed machine learning: case studies for weather and climate modelling. K Kashinath, Philos. Trans. R. Soc. A. 37920200093K Kashinath et al. 2021. Physics-informed machine learning: case studies for weather and climate modelling. Philos. Trans. R. Soc. A 379, 2194 (2021), 20200093.</p>
<p>Building high accuracy emulators for scientific simulations with deep neural architecture search. Mf Kasim, 2001arXiv e-printsMF Kasim et al. 2020. Building high accuracy emulators for scientific simulations with deep neural architecture search. arXiv e-prints (2020), arXiv-2001.</p>
<p>Machine learning prediction of heat capacity for solid inorganics. Sk Kauwe, Integr. Mater. Manuf. Innov. 7SK Kauwe et al. 2018. Machine learning prediction of heat capacity for solid inorganics. Integr. Mater. Manuf. Innov. 7, 2 (2018), 43-51.</p>
<p>Solving for high-dimensional committor functions using artificial neural networks. Y Khoo, L Lu, Ying, Res. Math. Sci. 61Y Khoo, J Lu, and L Ying. 2019. Solving for high-dimensional committor functions using artificial neural networks. Res. Math. Sci 6, 1 (2019), 1.</p>
<p>SwitchNet: a neural network model for forward and inverse scattering problems. Y Khoo, Ying, SIAM J Sci Comput. 41Y Khoo and L Ying. 2019. SwitchNet: a neural network model for forward and inverse scattering problems. SIAM J Sci Comput 41, 5 (2019), A3182-A3201.</p>
<p>Deep fluids: A generative network for parameterized fluid simulations. B Kim, Computer Graphics Forum. Wiley Online Library38B Kim et al. 2019. Deep fluids: A generative network for parameterized fluid simulations. In Computer Graphics Forum, Vol. 38. Wiley Online Library, 59-70.</p>
<p>Feedback control of Karman vortex shedding from a cylinder using deep reinforcement learning. H Koizumi, E Tsutsumi, Shima, Flow Control Conference. 3691H Koizumi, S Tsutsumi, and E Shima. 2018. Feedback control of Karman vortex shedding from a cylinder using deep reinforcement learning. In Flow Control Conference. 3691.</p>
<p>Complex hybrid models combining deterministic and machine learning components for numerical climate modeling and weather prediction. Vm Krasnopolsky, Fox-Rabinovitz, Neural Netw. 192VM Krasnopolsky and MS Fox-Rabinovitz. 2006. Complex hybrid models combining deterministic and machine learning components for numerical climate modeling and weather prediction. Neural Netw 19, 2 (2006), 122-134.</p>
<p>Characterizing possible failure modes in physics-informed neural networks. A Krishnapriyan, Adv. Neural Inf. Process. Syst. 34A Krishnapriyan et al. 2021. Characterizing possible failure modes in physics-informed neural networks. Adv. Neural Inf. Process. Syst. 34 (2021).</p>
<p>Distance metric learning using graph convolutional networks: Application to functional brain networks. Si Ktena, MICCAI. SpringerSI Ktena et al. 2017. Distance metric learning using graph convolutional networks: Application to functional brain networks. In MICCAI. Springer, 469-477.</p>
<p>Inverse-designed spinodoid metamaterials. S Kumar, Npj Comput. Mater. 6S Kumar et al. 2020. Inverse-designed spinodoid metamaterials. Npj Comput. Mater. 6, 1 (2020), 1-10.</p>
<p>Deep learning in fluid dynamics. Nj Kutz, J. Fluid Mech. 814NJ Kutz. 2017. Deep learning in fluid dynamics. J. Fluid Mech. 814 (2017), 1-4.</p>
<p>Data-driven fluid simulations using regression forests. L Ladickỳ, ACM Trans. Graph. 34L Ladickỳ et al. 2015. Data-driven fluid simulations using regression forests. ACM Trans. Graph 34, 6 (2015), 1-9.</p>
<p>Artificial neural networks for solving ordinary and partial differential equations. Ie Lagaris, D I Likas, Fotiadis, IEEE Trans. Neural Netw. Learn. IE Lagaris, A Likas, and DI Fotiadis. 1998. Artificial neural networks for solving ordinary and partial differential equations. IEEE Trans. Neural Netw. Learn (1998).</p>
<p>Learning partial differential equations for biological transport models from noisy spatiotemporal data. Jh Lagergren, Proc. R. Soc A. 4762234JH Lagergren et al. 2020. Learning partial differential equations for biological transport models from noisy spatio- temporal data. Proc. R. Soc A 476, 2234 (2020), 20190800.</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. B Lakshminarayanan, C Pritzel, Blundell, NIPS. B Lakshminarayanan, A Pritzel, and C Blundell. 2017. Simple and scalable predictive uncertainty estimation using deep ensembles. In NIPS.</p>
<p>Data-driven discovery of physical laws. Langley, Cogn. Sci. 5P Langley. 1981. Data-driven discovery of physical laws. Cogn. Sci. 5, 1 (1981), 31-54.</p>
<p>Rediscovering chemistry with the BACON system. P Langley, H A Bradshaw, Simon, Machine learning. SpringerP Langley, GL Bradshaw, and HA Simon. 1983. Rediscovering chemistry with the BACON system. In Machine learning. Springer, 307-329.</p>
<p>Model order reduction in fluid dynamics: challenges and perspectives. T Lassila, Reduced Order Methods for modeling and computational reduction. SpringerT Lassila et al. 2014. Model order reduction in fluid dynamics: challenges and perspectives. In Reduced Order Methods for modeling and computational reduction. Springer, 235-273.</p>
<p>Modelling transcriptional regulation using Gaussian processes. Nd Lawrence, M Sanguinetti, Rattray, Adv. Neural Inf. Process. Syst. ND Lawrence, G Sanguinetti, and M Rattray. 2007. Modelling transcriptional regulation using Gaussian processes. In Adv. Neural Inf. Process. Syst. 785-792.</p>
<p>The parable of Google Flu: traps in big data analysis. D Lazer, Science. D Lazer et al. 2014. The parable of Google Flu: traps in big data analysis. Science (2014).</p>
<p>Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. K Lee, Carlberg, J. Comput. Phys. 404108973K Lee and KT Carlberg. 2020. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. J. Comput. Phys. 404 (2020), 108973.</p>
<p>The role of heuristics in learning by discovery: Three case studies. Db Lenat, Machine learning. SpringerDB Lenat. 1983. The role of heuristics in learning by discovery: Three case studies. In Machine learning. Springer, 243-306.</p>
<p>Physical Equation Discovery Using Physics-Consistent Neural Network (PCNN) Under Incomplete Observability. H Li, Y Weng, ACM SIGKDD. H Li and Y Weng. 2021. Physical Equation Discovery Using Physics-Consistent Neural Network (PCNN) Under Incomplete Observability. In ACM SIGKDD. 925-933.</p>
<p>Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator. Q Li, Chaos. 27103111Q Li et al. 2017. Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator. Chaos 27, 10 (2017), 103111.</p>
<p>Z Li, arXiv:2010.08895Fourier neural operator for parametric partial differential equations. Z Li et al. 2020. Fourier neural operator for parametric partial differential equations. arXiv:2010.08895 (2020).</p>
<p>Metaheuristic-based inverse design of materials-A survey. T W Liao, G Li, J Materiomics. TW Liao and G Li. 2020. Metaheuristic-based inverse design of materials-A survey. J Materiomics (2020).</p>
<p>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance. J Ling, J Kurzawski, Templeton, J. Fluid Mech. J Ling, A Kurzawski, and J Templeton. 2016. Reynolds averaged turbulence modelling using deep neural networks with embedded invariance. J. Fluid Mech (2016).</p>
<p>Multi-Fidelity Physics-Constrained Neural Network and Its Application in Materials Modeling. D Liu, Y Wang, J. Mech. Des. 14112D Liu and Y Wang. 2019. Multi-Fidelity Physics-Constrained Neural Network and Its Application in Materials Modeling. J. Mech. Des. 141, 12 (2019).</p>
<p>Anatomy-aided deep learning for medical image segmentation: a review. L Liu, Phys. Med. Biol. L Liu et al. 2021. Anatomy-aided deep learning for medical image segmentation: a review. Phys. Med. Biol (2021).</p>
<p>Y Liu, arXiv:2004.04946Multiresolution Convolutional Autoencoders. Y Liu et al. 2020. Multiresolution Convolutional Autoencoders. arXiv:2004.04946 (2020).</p>
<p>Machine Learning Conservation Laws from Trajectories. Z Liu, M Tegmark, Phys. Rev. Lett. 126180604Z Liu and M Tegmark. 2021. Machine Learning Conservation Laws from Trajectories. Phys. Rev. Lett. 126, 18 (2021), 180604.</p>
<p>Constrained sparse Galerkin regression. J C Loiseau, Brunton, J. Fluid Mech. 838JC Loiseau and SL Brunton. 2018. Constrained sparse Galerkin regression. J. Fluid Mech. 838 (2018), 42-67.</p>
<p>HybridNet: integrating model-based and data-driven learning to predict evolution of dynamical systems. Y Long, S She, Mukhopadhyay, arXiv:1806.07439Y Long, X She, and S Mukhopadhyay. 2018. HybridNet: integrating model-based and data-driven learning to predict evolution of dynamical systems. arXiv:1806.07439 (2018).</p>
<p>Model migration with inclusive similarity for development of a new process model. Junde Lu, Furong Gao, Ind. Eng. Chem. Res. 47Junde Lu and Furong Gao. 2008. Model migration with inclusive similarity for development of a new process model. Ind. Eng. Chem. Res 47, 23 (2008), 9508-9516.</p>
<p>Process similarity and developing new process models through migration. J Lu, F Yao, Gao, AIChE journal. 55J Lu, K Yao, and F Gao. 2009. Process similarity and developing new process models through migration. AIChE journal 55, 9 (2009), 2318-2328.</p>
<p>Latent force models for earth observation time series prediction. D Luengo, G Campos-Taberner, Camps-Valls, IEEE Int. Workshop Mach. Learn.). IEEE. D Luengo, M Campos-Taberner, and G Camps-Valls. 2016. Latent force models for earth observation time series prediction. In IEEE Int. Workshop Mach. Learn.). IEEE, 1-6.</p>
<p>Adversarial regularizers in inverse problems. S Lunz, C B Öktem, Schönlieb, NIPS. S Lunz, O Öktem, and CB Schönlieb. 2018. Adversarial regularizers in inverse problems. In NIPS.</p>
<p>Bayesian improved model migration methodology for fast process modeling by incorporating prior information. L Luo, F Yao, Gao, Chem. Eng. Sci. 134L Luo, Y Yao, and F Gao. 2015. Bayesian improved model migration methodology for fast process modeling by incorporating prior information. Chem. Eng. Sci. 134 (2015), 23-35.</p>
<p>Deep learning for universal linear embeddings of nonlinear dynamics. B Lusch, S L Kutz, Brunton, Nat. Commun. 94950B Lusch, NJ Kutz, and SL Brunton. 2018. Deep learning for universal linear embeddings of nonlinear dynamics. Nat. Commun. 9, 1 (2018), 4950.</p>
<p>A practical Bayesian framework for backpropagation networks. D Jc Mackay, Neural computation. D JC MacKay. 1992. A practical Bayesian framework for backpropagation networks. Neural computation (1992).</p>
<p>First-principles machine learning modelling of COVID-19. L Magri, Doan, arXiv:2004.09478L Magri and NAK Doan. 2020. First-principles machine learning modelling of COVID-19. arXiv:2004.09478 (2020).</p>
<p>Numerical solution for high order differential equations using a hybrid neural network-optimization method. A Malek, R Shekari Beidokhti, Appl. Math. Comput. 1831A Malek and R Shekari Beidokhti. 2006. Numerical solution for high order differential equations using a hybrid neural network-optimization method. Appl. Math. Comput. 183, 1 (2006), 260-271.</p>
<p>Accurate solution of Bayesian inverse uncertainty quantification problems combining reduced basis methods and reduction error models. A Manzoni, T Pagani, Lassila, SIAM/ASA JUQA Manzoni, S Pagani, and T Lassila. 2016. Accurate solution of Bayesian inverse uncertainty quantification problems combining reduced basis methods and reduction error models. SIAM/ASA JUQ (2016).</p>
<p>Eight (no, Nine!) Problems with big data. G Marcus, Davis, The New York Times. 64G Marcus and E Davis. 2014. Eight (no, Nine!) Problems with big data. The New York Times 6, 04 (2014), 2014.</p>
<p>VAMPnets for deep learning of molecular kinetics. A Mardt, Nat. Commun. 9A Mardt et al. 2018. VAMPnets for deep learning of molecular kinetics. Nat. Commun. 9, 1 (2018), 1-11.</p>
<p>Marios Mattheakis, arXiv:1904.08991Physical symmetries embedded in neural networks. Marios Mattheakis et al. 2019. Physical symmetries embedded in neural networks. arXiv:1904.08991 (2019).</p>
<p>Mt Mccann, Jin Kh, M Unser, arXiv:1710.04011A review of convolutional neural networks for inverse problems in imaging. MT McCann, KH Jin, and M Unser. 2017. A review of convolutional neural networks for inverse problems in imaging. arXiv:1710.04011 (2017).</p>
<p>Using artificial intelligence to improve real-time decision-making for high-impact weather. A Mcgovern, Bull. Amer. Meteor. 98A McGovern et al. 2017. Using artificial intelligence to improve real-time decision-making for high-impact weather. Bull. Amer. Meteor. 98, 10 (2017), 2073-2090.</p>
<p>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems. Xuhui Meng, George Em Karniadakis, J. Comput. Phys. 401109020Xuhui Meng and George Em Karniadakis. 2020. A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems. J. Comput. Phys 401 (2020), 109020.</p>
<p>X Meng, Li, G E Zhang, Karniadakis, arXiv:1909.10145Ppinn: Parareal physics-informed neural network for timedependent pdes. X Meng, Z Li, D Zhang, and GE Karniadakis. 2019. Ppinn: Parareal physics-informed neural network for time- dependent pdes. arXiv:1909.10145 (2019).</p>
<p>Spectral properties of dynamical systems, model reduction and decompositions. Mezić, Nonlinear Dyn. 41I Mezić. 2005. Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear Dyn. 41, 1-3 (2005), 309-325.</p>
<p>Embedding hard physical constraints in convolutional neural networks for 3D turbulence. At Mohan, ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations. AT Mohan et al. 2020. Embedding hard physical constraints in convolutional neural networks for 3D turbulence. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations.</p>
<p>A deep learning based approach to reduced order modeling for turbulent flow control using LSTM neural networks. A T Mohan, Gaitonde, arXiv:1804.09269AT Mohan and DV Gaitonde. 2018. A deep learning based approach to reduced order modeling for turbulent flow control using LSTM neural networks. arXiv:1804.09269 (2018).</p>
<p>J Morton, M J Fd Witherden, Kochenderfer, arXiv:1902.09742Deep variational koopman models: Inferring koopman observations for uncertainty-aware dynamics modeling and control. J Morton, FD Witherden, and MJ Kochenderfer. 2019. Deep variational koopman models: Inferring koopman observations for uncertainty-aware dynamics modeling and control. arXiv:1902.09742 (2019).</p>
<p>A Climate Downscaling Deep Learning Model considering the Multiscale Spatial Correlations and Chaos of Meteorological Events. B Mu, Math. Probl. Eng. 2020B Mu et al. 2020. A Climate Downscaling Deep Learning Model considering the Multiscale Spatial Correlations and Chaos of Meteorological Events. Math. Probl. Eng 2020 (2020).</p>
<p>Segmenting and tracking extreme climate events using neural networks. M Mudigonda, Deep Learning for Physical Sciences (DLPS) Workshop in NIPS Conference. M Mudigonda et al. 2017. Segmenting and tracking extreme climate events using neural networks. In Deep Learning for Physical Sciences (DLPS) Workshop in NIPS Conference.</p>
<p>Incorporating prior domain knowledge into deep neural networks. N Muralidhar, IEEE Big Data. IEEEN Muralidhar et al. 2018. Incorporating prior domain knowledge into deep neural networks. In IEEE Big Data. IEEE.</p>
<p>PhyNet: Physics Guided Neural Networks for Particle Drag Force Prediction in Assembly. N Muralidhar, SDM. SIAM. N Muralidhar et al. 2020. PhyNet: Physics Guided Neural Networks for Particle Drag Force Prediction in Assembly. In SDM. SIAM, 559-567.</p>
<p>Machine learning for molecular simulation. F Noé, Annu. Rev. Phys. Chem. 71F Noé et al. 2020. Machine learning for molecular simulation. Annu. Rev. Phys. Chem. 71 (2020), 361-390.</p>
<p>Using machine learning to build temperature-based ozone parameterizations for climate sensitivity simulations. P Nowack, Environ. Res. Lett. 13104016P Nowack et al. 2018. Using machine learning to build temperature-based ozone parameterizations for climate sensitivity simulations. Environ. Res. Lett. 13, 10 (2018), 104016.</p>
<p>Using machine learning to parameterize moist convection: Potential for modeling of climate, climate change, and extreme events. J G Pa O&apos;gorman, Dwyer, J. Adv. Model. Earth Syst. 10PA O'Gorman and JG Dwyer. 2018. Using machine learning to parameterize moist convection: Potential for modeling of climate, climate change, and extreme events. J. Adv. Model. Earth Syst. 10, 10 (2018), 2548-2563.</p>
<p>A Oord, arXiv:1609.03499Wavenet: A generative model for raw audio. A Oord et al. 2016. Wavenet: A generative model for raw audio. arXiv:1609.03499 (2016).</p>
<p>Linearly recurrent autoencoder networks for learning dynamics. S E Otto, Rowley, SIADS. 18SE Otto and CW Rowley. 2019. Linearly recurrent autoencoder networks for learning dynamics. SIADS 18, 1 (2019), 558-593.</p>
<p>Physics-informed probabilistic learning of linear embeddings of nonlinear dynamics with guaranteed stability. S Pan, Duraisamy, SIADS. 19S Pan and K Duraisamy. 2020. Physics-informed probabilistic learning of linear embeddings of nonlinear dynamics with guaranteed stability. SIADS 19, 1 (2020), 480-509.</p>
<p>Broadband ground motions from 3D physics-based numerical simulations using artificial neural networks. R Paolucci, BSSA. R Paolucci et al. 2018. Broadband ground motions from 3D physics-based numerical simulations using artificial neural networks. BSSA (2018).</p>
<p>A paradigm for data-driven predictive modeling using field inversion and machine learning. E J Parish, Duraisamy, J. Comput. Phys. EJ Parish and K Duraisamy. 2016. A paradigm for data-driven predictive modeling using field inversion and machine learning. J. Comput. Phys (2016).</p>
<p>Physics-induced graph neural network: An application to wind-farm power estimation. J Park, Park, Energy. 187115883J Park and J Park. 2019. Physics-induced graph neural network: An application to wind-farm power estimation. Energy 187 (2019), 115883.</p>
<p>Physics guided machine learning using simplified theories. S Pawar, Phys. Fluids. 3311701S Pawar et al. 2021. Physics guided machine learning using simplified theories. Phys. Fluids 33, 1 (2021), 011701.</p>
<p>Multiscale modeling meets machine learning: What can we learn?. Gcy Peng, Arch. Comput. Methods Eng. 28GCY Peng et al. 2021. Multiscale modeling meets machine learning: What can we learn? Arch. Comput. Methods Eng. 28, 3 (2021), 1017-1037.</p>
<p>W Peng, arXiv:2004.08151Accelerating Physics-Informed Neural Network Training with Prior Dictionaries. W Peng et al. 2020. Accelerating Physics-Informed Neural Network Training with Prior Dictionaries. arXiv:2004.08151 (2020).</p>
<p>Uncertainty quantification in aeroelasticity: recent results and research challenges. Cl Pettit, J. of Aircraft. CL Pettit. 2004. Uncertainty quantification in aeroelasticity: recent results and research challenges. J. of Aircraft (2004).</p>
<p>Machine learning inverse problem for topological photonics. L Pilozzi, Commun. Phys. L Pilozzi et al. 2018. Machine learning inverse problem for topological photonics. Commun. Phys (2018).</p>
<p>Simultaneous fitting of a potential-energy surface and its corresponding force fields using feedforward neural networks. A Pukrittayakamee, J. Chem. Phys. A Pukrittayakamee et al. 2009. Simultaneous fitting of a potential-energy surface and its corresponding force fields using feedforward neural networks. J. Chem. Phys (2009).</p>
<p>Sparse identification of nonlinear dynamics for rapid model recovery. M Quade, Chaos. 2863116M Quade et al. 2018. Sparse identification of nonlinear dynamics for rapid model recovery. Chaos 28, 6 (2018), 063116.</p>
<p>Reduced order methods for modeling and computational reduction. A Quarteroni, Springer9A Quarteroni et al. 2014. Reduced order methods for modeling and computational reduction. Vol. 9. Springer.</p>
<p>Machine-learning-assisted materials discovery using failed experiments. P Raccuglia, Nature. 533P Raccuglia et al. 2016. Machine-learning-assisted materials discovery using failed experiments. Nature 533, 7601 (2016), 73-76.</p>
<p>Driven by data or derived through physics? A Review of hybrid physics guided machine learning techniques with cyber-physical system (CPS) focus. R Rai, Sahu, IEEE Access. 8R Rai and CK Sahu. 2020. Driven by data or derived through physics? A Review of hybrid physics guided machine learning techniques with cyber-physical system (CPS) focus. IEEE Access 8 (2020), 71050-71073.</p>
<p>Deep learning of vortex-induced vibrations. M Raissi, J. Fluid Mech. M Raissi et al. 2019. Deep learning of vortex-induced vibrations. J. Fluid Mech (2019).</p>
<p>Hidden physics models: Machine learning of nonlinear partial differential equations. M Raissi, Karniadakis, J. Comput. Phys. 357M Raissi and GE Karniadakis. 2018. Hidden physics models: Machine learning of nonlinear partial differential equations. J. Comput. Phys 357 (2018), 125-141.</p>
<p>M Raissi, G Perdikaris, Karniadakis, arXiv:1711.10561Data-driven solutions of nonlinear partial differential equations. part iM Raissi, P Perdikaris, and G Karniadakis. 2017. Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv:1711.10561 (2017).</p>
<p>M Raissi, G Perdikaris, Karniadakis, arXiv:1711.10561Data-driven discovery of nonlinear partial differential equations. M Raissi, P Perdikaris, and G Karniadakis. 2017. Physics informed deep learning (part II): Data-driven discovery of nonlinear partial differential equations. arXiv:1711.10561 (2017).</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, G E Perdikaris, Karniadakis, J. Comput. Phys. M Raissi, P Perdikaris, and GE Karniadakis. 2019. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys (2019).</p>
<p>Uncertainty-based simulation-optimization using Gaussian process emulation: application to coastal groundwater management. H Mm Rajabi, Ketabchi, J. Hydrol. MM Rajabi and H Ketabchi. 2017. Uncertainty-based simulation-optimization using Gaussian process emulation: application to coastal groundwater management. J. Hydrol (2017).</p>
<p>Deep learning to represent subgrid processes in climate models. S Rasp, P Pritchard, Gentine, PNAS. 115S Rasp, MS Pritchard, and P Gentine. 2018. Deep learning to represent subgrid processes in climate models. PNAS 115, 39 (2018), 9684-9689.</p>
<p>Process-guided deep learning predictions of lake water temperature. Js Read, Water Resour. Res. JS Read et al. 2019. Process-guided deep learning predictions of lake water temperature. Water Resour. Res. (2019).</p>
<p>Deep learning and process understanding for data-driven Earth system science. M Reichstein, Nature. 566M Reichstein et al. 2019. Deep learning and process understanding for data-driven Earth system science. Nature 566, 7743 (2019), 195-204.</p>
<p>A constrained integration (CINT) approach to solving partial differential equations using artificial neural networks. K Rudd, Ferrari, Neurocomputing. K Rudd and S Ferrari. 2015. A constrained integration (CINT) approach to solving partial differential equations using artificial neural networks. Neurocomputing (2015).</p>
<p>Data-driven discovery of partial differential equations. Sh Rudy, Science Advances. SH Rudy et al. 2017. Data-driven discovery of partial differential equations. Science Advances (2017).</p>
<p>Deep learning of dynamics and signal-noise decomposition with time-stepping constraints. H Samuel, Nathan Rudy, Steven L Kutz, Brunton, J. Comput. Phys. 396Samuel H Rudy, J Nathan Kutz, and Steven L Brunton. 2019. Deep learning of dynamics and signal-noise decomposition with time-stepping constraints. J. Comput. Phys 396 (2019), 483-506.</p>
<p>Deep neural networks motivated by partial differential equations. L Ruthotto, Haber, J. Math. Imaging Vis. L Ruthotto and E Haber. 2018. Deep neural networks motivated by partial differential equations. J. Math. Imaging Vis. (2018).</p>
<p>The atmospheric river tracking method intercomparison project (ARTMIP): quantifying uncertainties in atmospheric river climatology. Jj Rutz, J. Geophys. Res. Atmos. JJ Rutz et al. 2019. The atmospheric river tracking method intercomparison project (ARTMIP): quantifying uncertainties in atmospheric river climatology. J. Geophys. Res. Atmos. (2019).</p>
<p>Crystal structure prediction via deep learning. K Ryan, M Lengyel, Shatruk, J. Am. Chem. Soc. 140K Ryan, J Lengyel, and M Shatruk. 2018. Crystal structure prediction via deep learning. J. Am. Chem. Soc 140, 32 (2018), 10158-10168.</p>
<p>Physics-based convolutional neural network for fault diagnosis of rolling element bearings. M Sadoughi, C Hu, IEEE Sensors Journal. M Sadoughi and C Hu. 2019. Physics-based convolutional neural network for fault diagnosis of rolling element bearings. IEEE Sensors Journal (2019).</p>
<p>Synergies Between Quantum Mechanics and Machine Learning in Reaction Prediction. P Sadowski, https:/arxiv.org/abs/https:/doi.org/10.1021/acs.jcim.6b0035127749058J. Chem. Inf. Model. 56P Sadowski et al. 2016. Synergies Between Quantum Mechanics and Machine Learning in Reaction Prediction. J. Chem. Inf. Model. 56, 11 (2016), 2125-2128. https://doi.org/10.1021/acs.jcim.6b00351 arXiv:https://doi.org/10.1021/acs.jcim.6b00351 PMID: 27749058.</p>
<p>Machine learning closures for model order reduction of thermal fluids. O San, Maulik, Appl. Math. Model. O San and R Maulik. 2018. Machine learning closures for model order reduction of thermal fluids. Appl. Math. Model. (2018).</p>
<p>Neural network closures for nonlinear model order reduction. O San, Maulik, Adv Comput Math. 44O San and R Maulik. 2018. Neural network closures for nonlinear model order reduction. Adv Comput Math 44, 6 (2018), 1717-1750.</p>
<p>From DFT to machine learning: recent approaches to materials science-a review. Gr Schleder, J. Phys. Materials. 232001GR Schleder et al. 2019. From DFT to machine learning: recent approaches to materials science-a review. J. Phys. Materials 2, 3 (2019), 032001.</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. M Schmidt and H Lipson. 2009. Distilling free-form natural laws from experimental data. Science (2009).</p>
<p>Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. K Schütt, NeurIPS. K Schütt et al. 2017. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In NeurIPS. 991-1001.</p>
<p>Self-supervised learning of inverse problem solvers in medical imaging. In Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data. O Senouf, SpringerO Senouf et al. 2019. Self-supervised learning of inverse problem solvers in medical imaging. In Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data. Springer.</p>
<p>Airsim: High-fidelity visual and physical simulation for autonomous vehicles. S Shah, Field and service robotics. SpringerS Shah et al. 2018. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and service robotics. Springer, 621-635.</p>
<p>V Shah, arXiv:1906.01626Encoding invariances in deep generative models. V Shah et al. 2019. Encoding invariances in deep generative models. arXiv:1906.01626 (2019).</p>
<p>Downscaling satellite precipitation estimates with multiple linear regression, artificial neural networks, and spline interpolation techniques. E Sharifi, R Saghafian, Steinacker, J. Geophys. Res. Atmos. 124E Sharifi, B Saghafian, and R Steinacker. 2019. Downscaling satellite precipitation estimates with multiple linear regression, artificial neural networks, and spline interpolation techniques. J. Geophys. Res. Atmos. 124, 2 (2019), 789-805.</p>
<p>Correspondence between Koopman mode decomposition, resolvent mode decomposition, and invariant solutions of the Navier-Stokes equations. As, Sharma, B J Mezić, Mckeon, Phys. Rev. Fluids. 132402AS Sharma, I Mezić, and BJ McKeon. 2016. Correspondence between Koopman mode decomposition, resolvent mode decomposition, and invariant solutions of the Navier-Stokes equations. Phys. Rev. Fluids 1, 3 (2016), 032402.</p>
<p>Weakly-supervised deep learning of heat transport via physics informed loss. R Sharma, arXiv:1807.11374R Sharma et al. 2018. Weakly-supervised deep learning of heat transport via physics informed loss. arXiv:1807.11374 (2018).</p>
<p>DGM: A deep learning algorithm for solving partial differential equations. J Sirignano, Spiliopoulos, J. Comput. Phys. 375J Sirignano and K Spiliopoulos. 2018. DGM: A deep learning algorithm for solving partial differential equations. J. Comput. Phys 375 (2018), 1339-1364.</p>
<p>Between the poles of data-driven and mechanistic modeling for process operation. D Solle, Chemie Ingenieur Technik. D Solle et al. 2017. Between the poles of data-driven and mechanistic modeling for process operation. Chemie Ingenieur Technik (2017).</p>
<p>Machine learning techniques for downscaling SMOS satellite soil moisture using MODIS land surface temperature for hydrological application. Pk, Srivastava, Water Resour. Manag. 27PK Srivastava et al. 2013. Machine learning techniques for downscaling SMOS satellite soil moisture using MODIS land surface temperature for hydrological application. Water Resour. Manag. 27, 8 (2013), 3127-3144.</p>
<p>P Sturmfels, arXiv:1808.04362A domain guided CNN architecture for predicting age from structural brain images. P Sturmfels et al. 2018. A domain guided CNN architecture for predicting age from structural brain images. arXiv:1808.04362 (2018).</p>
<p>A theory-guided deep-learning formulation and optimization of seismic waveform inversion. J Sun, Geophysics. 85J Sun et al. 2020. A theory-guided deep-learning formulation and optimization of seismic waveform inversion. Geophysics 85, 2 (2020), R87-R99.</p>
<p>Joint Gaussian processes for biophysical parameter retrieval. Dh Svendsen, IEEE Trans Geosci Remote Sens. 56DH Svendsen et al. 2017. Joint Gaussian processes for biophysical parameter retrieval. IEEE Trans Geosci Remote Sens 56, 3 (2017), 1718-1727.</p>
<p>L Swiler, arXiv:2006.09319A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges. L Swiler et al. 2020. A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges. arXiv:2006.09319 (2020).</p>
<p>Convolutional neural networks for medical image analysis: Full training or fine tuning?. N Tajbakhsh, IEEE Trans. Med. Imaging. 35N Tajbakhsh et al. 2016. Convolutional neural networks for medical image analysis: Full training or fine tuning? IEEE Trans. Med. Imaging. 35, 5 (2016), 1299-1312.</p>
<p>Learning Koopman invariant subspaces for dynamic mode decomposition. N Takeishi, T Kawahara, Yairi, Adv. Neural Inf. Process. Syst. N Takeishi, Y Kawahara, and T Yairi. 2017. Learning Koopman invariant subspaces for dynamic mode decomposition. In Adv. Neural Inf. Process. Syst. 1130-1140.</p>
<p>A Tanaka, K Tomiya, Hashimoto, Deep Learning and Physics. Springer NatureA Tanaka, A Tomiya, and K Hashimoto. 2021. Deep Learning and Physics. Springer Nature.</p>
<p>arXiv:2003.09077Inverse problems, deep learning, and symmetry breaking. K Tayal et al. 2020. Inverse problems, deep learning, and symmetry breaking. arXiv:2003.09077 (2020).</p>
<p>Unlocking inverse problems using deep learning: Breaking symmetries in phase retrieval. In NeurIPSK Tayal et al. 2020. Unlocking inverse problems using deep learning: Breaking symmetries in phase retrieval. In NeurIPS.</p>
<p>Coronary CT angiography-derived fractional flow reserve: machine learning algorithm versus computational fluid dynamics modeling. C Tesche, Radiology. 288C Tesche et al. 2018. Coronary CT angiography-derived fractional flow reserve: machine learning algorithm versus computational fluid dynamics modeling. Radiology 288, 1 (2018), 64-72.</p>
<p>Instream water temperature model. Fd Theurer, W J Voos, Miller, Div. Biol. Serv., Tech. Rep. FWS OBS. 84FD Theurer, KA Voos, and WJ Miller. 1984. Instream water temperature model. Div. Biol. Serv., Tech. Rep. FWS OBS 84, 15 (1984), 11-42.</p>
<p>Modeling chemical processes using prior knowledge and neural networks. M L Thompson, M A Kramer, AIChE Journal. 40ML Thompson and MA Kramer. 1994. Modeling chemical processes using prior knowledge and neural networks. AIChE Journal 40, 8 (1994), 1328-1340.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, IEEE/RSJ IROS. IEEE. J Tobin et al. 2017. Domain randomization for transferring deep neural networks from simulation to the real world. In IEEE/RSJ IROS. IEEE, 23-30.</p>
<p>Hamiltonian generative networks. P Toth, arXiv:1909.13789P Toth et al. 2019. Hamiltonian generative networks. arXiv:1909.13789 (2019).</p>
<p>Deep UQ: Learning deep neural network surrogate models for high dimensional uncertainty quantification. Rk Tripathy, Bilionis, J. Comput. Phys. RK Tripathy and I Bilionis. 2018. Deep UQ: Learning deep neural network surrogate models for high dimensional uncertainty quantification. J. Comput. Phys (2018).</p>
<p>AI Feynman: A physics-inspired method for symbolic regression. S Udrescu, M Tegmark, Sci. Adv. 62631S Udrescu and M Tegmark. 2020. AI Feynman: A physics-inspired method for symbolic regression. Sci. Adv. 6, 16 (2020), eaay2631.</p>
<p>Deep image prior. D Ulyanov, V Vedaldi, Lempitsky, CVPR. D Ulyanov, A Vedaldi, and V Lempitsky. 2018. Deep image prior. In CVPR.</p>
<p>Unsupervised physics-based neural networks for seismic migration. J Vamaraju, Sen, Interpretation. J Vamaraju and MK Sen. 2019. Unsupervised physics-based neural networks for seismic migration. Interpretation (2019).</p>
<p>Deepsd: Generating high resolution climate change projections through single image superresolution. T Vandal, SIGKDD '17. T Vandal et al. 2017. Deepsd: Generating high resolution climate change projections through single image super- resolution. In SIGKDD '17. 1663-1672.</p>
<p>Quantifying uncertainty in discrete-continuous and skewed data with Bayesian deep learning. T Vandal, SIGKDD '18. T Vandal et al. 2018. Quantifying uncertainty in discrete-continuous and skewed data with Bayesian deep learning. In SIGKDD '18. 2377-2386.</p>
<p>Using Machine Learning to Develop a Predictive Understanding of the Impacts of Extreme Water Cycle Perturbations on River Water Quality. C Varadharajan, . AI4ESPTechnical ReportC Varadharajan. 2021. Using Machine Learning to Develop a Predictive Understanding of the Impacts of Extreme Water Cycle Perturbations on River Water Quality. Technical Report. AI4ESP.</p>
<p>Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks. Pr Vlachas, Proc. R. Soc. A. PR Vlachas et al. 2018. Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks. Proc. R. Soc. A (2018).</p>
<p>Informed machine learning-a taxonomy and survey of integrating knowledge into learning systems. L Von Rueden, arXiv:1903.12394L von Rueden et al. 2020. Informed machine learning-a taxonomy and survey of integrating knowledge into learning systems. arXiv:1903.12394 (2020).</p>
<p>Data-assisted reduced-order modeling of extreme events in complex dynamical systems. Zy Wan, PloS one. ZY Wan et al. 2018. Data-assisted reduced-order modeling of extreme events in complex dynamical systems. PloS one (2018).</p>
<p>Physics-informed machine learning approach for reconstructing Reynolds stress modeling discrepancies based on DNS data. Jx Wang, H Wu, Xiao, Phys. Rev. Fluids. 234603JX Wang, JL Wu, and H Xiao. 2017. Physics-informed machine learning approach for reconstructing Reynolds stress modeling discrepancies based on DNS data. Phys. Rev. Fluids 2, 3 (2017), 034603.</p>
<p>TDEFSI: Theory-guided Deep Learning-based Epidemic Forecasting with Synthetic Information. L Wang, M Chen, Marathe, ACM Trans. Spat. Algorithms Syst. 6L Wang, J Chen, and M Marathe. 2020. TDEFSI: Theory-guided Deep Learning-based Epidemic Forecasting with Synthetic Information. ACM Trans. Spat. Algorithms Syst. 6, 3 (2020), 1-39.</p>
<p>Incorporating Symmetry into Deep Dynamics Models for Improved Generalization. R Wang, R Walters, Yu, arXiv:2002.03061R Wang, R Walters, and R Yu. 2020. Incorporating Symmetry into Deep Dynamics Models for Improved Generalization. arXiv:2002.03061 (2020).</p>
<p>Understanding and mitigating gradient flow pathologies in physics-informed neural networks. S Wang, P Teng, Perdikaris, SIAM J Sci Comput. 43S Wang, Y Teng, and P Perdikaris. 2021. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM J Sci Comput 43, 5 (2021), A3055-A3081.</p>
<p>Successful leveraging of image processing and machine learning in seismic structural interpretation: A review. Z Wang, The Leading Edge. 37Z Wang et al. 2018. Successful leveraging of image processing and machine learning in seismic structural interpretation: A review. The Leading Edge 37, 6 (2018), 451-461.</p>
<p>Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics. C Wehmeyer, Noé, J. Chem. Phys. 148241703C Wehmeyer and F Noé. 2018. Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics. J. Chem. Phys 148, 24 (2018), 241703.</p>
<p>Predicting the effective thermal conductivities of composite materials and porous media by machine learning methods. H Wei, Int. J. Heat Mass Transf. 127H Wei et al. 2018. Predicting the effective thermal conductivities of composite materials and porous media by machine learning methods. Int. J. Heat Mass Transf. 127 (2018), 908-916.</p>
<p>Predicting Water Temperature Dynamics of Unmonitored Lakes With Meta-Transfer Learning. Jd Willard, Water Resour. Res. 57JD Willard et al. 2021. Predicting Water Temperature Dynamics of Unmonitored Lakes With Meta-Transfer Learning. Water Resour. Res. 57, 7 (2021), e2021WR029579.</p>
<p>Gaussian processes for machine learning. Cki Williams, Rasmussen, MIT press2Cambridge, MACKI Williams and CE Rasmussen. 2006. Gaussian processes for machine learning. Vol. 2. MIT press Cambridge, MA.</p>
<p>ConvPDE-UQ: Convolutional neural networks with quantified uncertainty for heterogeneous elliptic partial differential equations on varied domains. N Winovich, G Ramani, Lin, J. Comput. Phys. 394N Winovich, K Ramani, and G Lin. 2019. ConvPDE-UQ: Convolutional neural networks with quantified uncertainty for heterogeneous elliptic partial differential equations on varied domains. J. Comput. Phys 394 (2019), 263-279.</p>
<p>Variational Koopman models: slow collective variables and molecular kinetics from short offequilibrium simulations. H Wu, J. Chem. Phys. 146154104H Wu et al. 2017. Variational Koopman models: slow collective variables and molecular kinetics from short off- equilibrium simulations. J. Chem. Phys 146, 15 (2017), 154104.</p>
<p>Physics-informed machine learning for predictive turbulence modeling: A priori assessment of prediction confidence. Wu, arXiv:1607.04563JL Wu et al. 2016. Physics-informed machine learning for predictive turbulence modeling: A priori assessment of prediction confidence. arXiv:1607.04563 (2016).</p>
<p>Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. Wu, J. Comput. Phys. 109209JL Wu et al. 2019. Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. J. Comput. Phys (2019), 109209.</p>
<p>Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. Wu, J. Comput. Phys. JL Wu et al. 2020. Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. J. Comput. Phys (2020).</p>
<p>Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework. Jl Wu, E Xiao, Paterson, Phys. Rev. Fluids. 374602JL Wu, H Xiao, and E Paterson. 2018. Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework. Phys. Rev. Fluids 3, 7 (2018), 074602.</p>
<p>Y Wu, arXiv:2006.07527Inductive Graph Neural Networks for Spatiotemporal Kriging. Y Wu et al. 2020. Inductive Graph Neural Networks for Spatiotemporal Kriging. arXiv:2006.07527 (2020).</p>
<p>A reduced order model for turbulent flows in the urban environment using machine learning. D Xiao, Building and Environment. 148D Xiao et al. 2019. A reduced order model for turbulent flows in the urban environment using machine learning. Building and Environment 148 (2019), 323-337.</p>
<p>tempogan: A temporally coherent, volumetric gan for super-resolution fluid flow. You Xie, ACM Trans. Graph. 37You Xie et al. 2018. tempogan: A temporally coherent, volumetric gan for super-resolution fluid flow. ACM Trans. Graph 37, 4 (2018), 1-15.</p>
<p>Data-driven methods to improve baseflow prediction of a regional groundwater model. T F Xu, Valocchi, Comput. Geosci. TF Xu and AJ Valocchi. 2015. Data-driven methods to improve baseflow prediction of a regional groundwater model. Comput. Geosci. (2015).</p>
<p>Bayesian migration of Gaussian process regression for rapid process modeling and optimization. W Yan, Chem. Eng. J. 166W Yan et al. 2011. Bayesian migration of Gaussian process regression for rapid process modeling and optimization. Chem. Eng. J. 166, 3 (2011), 1095-1103.</p>
<p>Highly-Ccalable, Physics-Informed GANs for Learning Solutions of Stochastic PDEs. L Yang, IEEE/ACM Deep Learning on Supercomputers. IEEE. L Yang et al. 2019. Highly-Ccalable, Physics-Informed GANs for Learning Solutions of Stochastic PDEs. In 2019 IEEE/ACM Deep Learning on Supercomputers. IEEE.</p>
<p>L Yang, G Meng, Karniadakis, arXiv:2003.06097B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data. L Yang, X Meng, and G Karniadakis. 2020. B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data. arXiv:2003.06097 (2020).</p>
<p>L Yang, G E Zhang, Karniadakis, arXiv:1811.02033Physics-informed generative adversarial networks for stochastic differential equations. L Yang, D Zhang, and GE Karniadakis. 2018. Physics-informed generative adversarial networks for stochastic differential equations. arXiv:1811.02033 (2018).</p>
<p>Evaluation and machine learning improvement of global hydrological model-based flood simulations. T Yang, Environ. Res. Lett. 14114027T Yang et al. 2019. Evaluation and machine learning improvement of global hydrological model-based flood simulations. Environ. Res. Lett 14, 11 (2019), 114027.</p>
<p>Y Yang, P Perdikaris, arXiv:1812.03511Physics-informed deep generative models. Y Yang and P Perdikaris. 2018. Physics-informed deep generative models. arXiv:1812.03511 (2018).</p>
<p>Adversarial uncertainty quantification in physics-informed neural networks. Y Yang, P Perdikaris, J. Comput. Phys. Y Yang and P Perdikaris. 2019. Adversarial uncertainty quantification in physics-informed neural networks. J. Comput. Phys (2019).</p>
<p>The TensorMol-0.1 model chemistry: a neural network augmented with long-range physics. K Yao, Chem. Sci. K Yao et al. 2018. The TensorMol-0.1 model chemistry: a neural network augmented with long-range physics. Chem. Sci. (Jan 2018).</p>
<p>Systems biology informed deep learning for inferring parameters and hidden dynamics. A Yazdani, G Raissi, Karniadakis, bioRxiv. 865063A Yazdani, M Raissi, and G Karniadakis. 2019. Systems biology informed deep learning for inferring parameters and hidden dynamics. bioRxiv (2019), 865063.</p>
<p>Learning deep neural network representations for Koopman operators of nonlinear dynamical systems. E Yeung, N Kundu, Hodas, American Control Conference. IEEEE Yeung, S Kundu, and N Hodas. 2019. Learning deep neural network representations for Koopman operators of nonlinear dynamical systems. In American Control Conference. IEEE, 4832-4839.</p>
<p>Enforcing deterministic constraints on Generative Adversarial Networks for emulating physical systems. Y Zeng, arXiv:1911.06671Y Zeng et al. 2019. Enforcing deterministic constraints on Generative Adversarial Networks for emulating physical systems. arXiv:1911.06671 (2019).</p>
<p>L Zepeda-Núñez, Y Chen, W Zhang, Jia, L Zhang, Lin, arXiv:1912.00775Deep Density: circumventing the Kohn-Sham equations via symmetry preserving neural networks. L Zepeda-Núñez, Y Chen, J Zhang, W Jia, L Zhang, and L Lin. 2019. Deep Density: circumventing the Kohn-Sham equations via symmetry preserving neural networks. arXiv:1912.00775 (2019).</p>
<p>Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. L Zhang, Phys. Rev. Lett. 120143001L Zhang et al. 2018. Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. Phys. Rev. Lett. 120, 14 (2018), 143001.</p>
<p>DeePCG: Constructing coarse-grained models via deep neural networks. Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, Weinan E , J. Chem. Phys. 14934101Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E. 2018. DeePCG: Constructing coarse-grained models via deep neural networks. J. Chem. Phys 149, 3 (2018), 034101.</p>
<p>Real-time power system state estimation and forecasting via deep unrolled neural networks. L Zhang, G B Wang, Giannakis, IEEE Trans. Signal Process. 67L Zhang, G Wang, and GB Giannakis. 2019. Real-time power system state estimation and forecasting via deep unrolled neural networks. IEEE Trans. Signal Process 67, 15 (2019), 4069-4077.</p>
<p>R Zhang, H Liu, Sun, arXiv:1909.08118Physics-guided Convolutional Neural Network (PhyCNN) for Data-driven Seismic Response Modeling. R Zhang, Y Liu, and H Sun. 2019. Physics-guided Convolutional Neural Network (PhyCNN) for Data-driven Seismic Response Modeling. arXiv:1909.08118 (2019).</p>
<p>Estimating uncertainty of streamflow simulation using Bayesian neural networks. X Zhang, Water Resour. Res. X Zhang et al. 2009. Estimating uncertainty of streamflow simulation using Bayesian neural networks. Water Resour. Res. (2009).</p>
<p>Facial landmark detection by deep multi-task learning. Z Zhang, ECCV. SpringerZ Zhang et al. 2014. Facial landmark detection by deep multi-task learning. In ECCV. Springer.</p>
<p>Physics-informed semantic inpainting: Application to geostatistical modeling. Q Zheng, arXiv:1909.09459Q Zheng et al. 2019. Physics-informed semantic inpainting: Application to geostatistical modeling. arXiv:1909.09459 (2019).</p>
<p>Yd Zhong, A Dey, Chakraborty, arXiv:1909.12077Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control. YD Zhong, B Dey, and A Chakraborty. 2019. Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control. arXiv:1909.12077 (2019).</p>
<p>Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Y Zhu, J. Comput. Phys. Y Zhu et al. 2019. Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. J. Comput. Phys (2019).</p>
<p>Bayesian deep convolutional encoder-decoder networks for surrogate modeling and uncertainty quantification. Y Zhu, Zabaras, J. Comput. Phys. Y Zhu and N Zabaras. 2018. Bayesian deep convolutional encoder-decoder networks for surrogate modeling and uncertainty quantification. J. Comput. Phys (2018).</p>
<ol>
<li>A physics-informed machine learning approach for solving heat transfer equation in advanced manufacturing and engineering applications. N Zobeiry, Humfeld, Eng. Appl. Artif. Intell. 101104232N Zobeiry and KD Humfeld. 2021. A physics-informed machine learning approach for solving heat transfer equation in advanced manufacturing and engineering applications. Eng. Appl. Artif. Intell. 101 (2021), 104232.</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4746 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4746</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4746</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-269004939</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.05221v1.pdf" target="_blank">LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4746.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4746.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT (Llama-2 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting with Llama-2 70B (quantized)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive chain-of-thought (CoT) decoding using Llama-2-70B to produce a single step-by-step reasoning chain per query; treated as a 'similar' reasoning method because it produces one main chain without explicit multi-path search or ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 70B (quantized with GPT-Q)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama 2 70B model used in experiments, quantized with GPT-Q for efficiency; used as the student LLM across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Autoregressively decodes a single reasoning chain (sequence of steps) using a CoT prompt; essentially greedy or single-chain sampling without explicit tree search or ensemble aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple: GSM8K, AQuA, StrategyQA, Game-of-24, PrOntoQA, Blocksworld</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of mathematical, commonsense, logical, and embodied-planning reasoning benchmarks used to evaluate step-by-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: CoT is consistently outperformed by search-based methods (ToT and RAP) across datasets in this paper; CoT produces more false-positive reasoning chains (i.e., correct final answer with incorrect chain). Exact per-dataset accuracies with CoT are reported in the paper's tables (see Table 3 / Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>ToT and RAP achieve higher overall accuracy and notably fewer false-positive reasoning chains; RAP > ToT > CoT overall (RAP outperforms ToT by 42% on Blocksworld).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT (single-chain, similar reasoning) is weaker than search-based (diverse) methods: it cannot 'regret' earlier steps and therefore more often produces false-positive chains even when final answers are correct. Search-based approaches produce higher AutoRace scores (fewer false positives).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No dataset in the paper shows CoT outperforming ToT or RAP overall; CoT does avoid some failure modes (e.g., excessive detail leading to factual errors in RAP on StrategyQA), meaning in some cases simpler CoT chains may avoid introducing extraneous incorrect details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4746.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4746.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT (Llama-2 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT) with Llama-2 70B (BFS/DFS variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-search based reasoning method that explores multiple reasoning branches (diverse) using an LLM to propose and evaluate actions; implemented with breadth-first (BFS) or depth-first (DFS) variants and self-evaluation / likelihood-based rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 70B (quantized with GPT-Q)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as above: Llama 2 70B used as the student LLM for ToT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Uses a tree-search (BFS or DFS) to explore multiple reasoning branches; each candidate step/action is scored by reward functions (self-evaluation and/or likelihood) and the search selects among branches rather than committing to a single chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple: GSM8K, AQuA, StrategyQA, Game-of-24, PrOntoQA, Blocksworld</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-domain benchmark suite for reasoning used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: ToT consistently outperforms vanilla CoT across tasks in this study (higher AutoRace and answer-based metrics in most cases). ToT (BFS) shows particularly strong gains on complex math, logical, and embodied planning tasks compared to DFS. Exact numeric accuracies are reported in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (worse); RAP (often better than ToT overall). For embodied Blocksworld RAP > ToT by a large margin (RAP 42% higher than ToT on Blocksworld reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Search-based (diverse) methods like ToT reduce false positives and improve final accuracy relative to CoT by exploring alternative reasoning branches and discarding low-reward branches; breadth (BFS) is generally more important than depth (DFS) for complex tasks with large search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>DFS variant of ToT can perform worse than BFS on large search-space tasks (math, logic, embodied planning) because DFS can get trapped in inferior subspaces; for constrained tasks with small search spaces (Game-of-24) DFS does not hurt exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4746.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4746.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP (Llama-2 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning-via-Planning (RAP) with Llama-2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search-based reasoning approach that augments the LLM with an explicit world model and uses Monte-Carlo Tree Search (MCTS) with reward guidance to plan multi-step reasoning (diverse).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 70B (quantized with GPT-Q)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same Llama-2-70B backbone used; RAP leverages the model both as proposer/evaluator and as a world model to predict state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RAP (Reasoning-via-Planning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Incorporates an explicit world model representing task state and uses a search algorithm (MCTS in experiments) guided by rewards (likelihood, self-evaluation, and task heuristics) to plan sequences of actions; retains state to inform future decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple with emphasis on embodied Blocksworld (and math/commonsense tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard reasoning benchmarks plus embodied planning (Blocksworld) where explicit state tracking is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative/numeric highlights: RAP is the most effective method across most datasets in this study; notably, on Blocksworld RAP outperforms ToT by ~42% (reported in text). RAP also achieves higher AutoRace scores (fewer false positives) and higher final accuracy in many tasks (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>ToT (worse); CoT (worse). On StrategyQA RAP shows higher false-positive rate in some cases due to prompt-format-induced excessive detail.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An explicit world model combined with reward-guided search substantially improves performance, especially for embodied planning tasks that require tracking state across many actions. RAP reduces reasoning errors and increases robustness in stateful domains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On StrategyQA, RAP produced more false-positive reasoning chains than expected (higher AutoRace-detected error rate) because its iterative sub-questioning prompt led to extraneous details and factual errors; in that domain, CoT sometimes avoids such pitfalls by remaining concise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4746.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4746.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search-based vs CoT (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search-based (diverse multi-path) methods compared to single-chain CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general comparison showing that reasoning methods that explore multiple reasoning paths (diverse) via search and reward guidance yield higher accuracy and fewer false-positive chains than single-chain CoT (similar).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 70B (representative student LLM used across search methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2-70B used as the base LLM for implementing CoT, ToT, RAP, and other search-based algorithms in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Search-based methods (ToT, RAP, beam/MCTS) vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse (search-based) vs similar (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Search-based methods sample or search multiple candidate reasoning chains (via BFS, DFS, beam, MCTS, or random shooting) and score/aggregate them using reward functions (self-eval, likelihood, heuristics), whereas CoT generates a single chain via autoregressive decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple reasoning benchmarks (GSM8K, StrategyQA, AQuA, Game-of-24, PrOntoQA, Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-domain suite covering mathematical, commonsense, logical, and embodied planning reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Search-based methods (ToT, RAP) achieve higher AutoRace and answer-based metrics and detect fewer false positive reasoning chains; AutoRace shows search-based methods have higher correctness on chains that reach correct answers (i.e., they truly are correct). AutoRace detected 70.4% of false positive chains overall (paper-level statistic about detection), and search methods reduced false positives relative to CoT (figures and tables in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT: lower accuracy and higher false-positive rate; exact per-benchmark numbers are provided in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reward-guided, multi-path search reduces false positives and improves final accuracy; breadth of search is often more important than depth; explicit world models further benefit stateful/embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Search-based methods can increase verbosity and introduce additional factual errors in some prompt formats (e.g., RAP on StrategyQA), showing that diversity/exploration can sometimes harm reasoning chain faithfulness if prompts induce unnecessary details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4746.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4746.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Breadth-vs-Depth (ToT BFS vs DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Breadth-first (BFS) vs Depth-first (DFS) variants of Tree-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental finding comparing BFS and DFS search strategies: BFS (greater breadth) is generally preferable for large search-space reasoning tasks, while DFS can be acceptable in tightly constrained search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 70B (used to run ToT BFS and DFS variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2-70B backbone used to generate candidate actions and self-evaluations under both BFS and DFS search strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>ToT (BFS) vs ToT (DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Two search strategies for ToT: BFS explores many parallel branches (high breadth) up to a limit; DFS goes deep along branches (high depth) possibly exploring fewer alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, AQuA, StrategyQA, Game-of-24, PrOntoQA, Blocksworld</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-domain benchmarks; the effect depends on the effective size of the reasoning search space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: BFS (breadth) is relatively better on complex math, logical, and embodied tasks; DFS can get stuck in poor subspaces on large tasks. DFS does not harm performance in small/limited search-space tasks like Game-of-24.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DFS (worse in large search-space tasks; comparable in small ones)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Breadth of exploration is generally more important than depth for efficient discovery of correct reasoning chains in complex tasks; set search breadth limits accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>In tasks with constrained/small search spaces (Game-of-24), DFS performs adequately and breadth is less critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4746.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4746.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Leading LLMs (CoT comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of leading LLMs on CoT reasoning chains (GPT-4, Claude-3, Gemini, InternLM-2, Mistral, Mixtral, Llama-2, Qwen, Gemma)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of multiple popular LLMs using CoT prompting to compare their step-by-step reasoning abilities; reveals differences in chain quality that are not captured by final-answer accuracy alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Claude-3 Opus, Gemini Pro, InternLM-2, Mistral, Mixtral, Llama-2, Qwen, Gemma (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mix of closed-source and open models evaluated under CoT prompting across multiple datasets; some models are very large (GPT-4, Claude-3), others are smaller (InternLM-2 7B, Mistral 7B), yet performance varies nonlinearly with size.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) across LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Apply the CoT prompting paradigm to each LLM and evaluate the resulting chains with AutoRace (and answer-based metrics) to measure both final answer correctness and reasoning chain faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, AQuA, StrategyQA (AutoRace applied to the latter three in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical and commonsense reasoning benchmarks where step-by-step reasoning is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported highlights: GPT-4 turbo and Claude-3 Opus are top-performing across tasks; InternLM-2 7B surprisingly outperforms larger Llama-2 70B on average; on StrategyQA answer accuracy across top models is similar (0.63–0.79) whereas AutoRace chain-quality scores vary widely (0.28–0.91), indicating model differences in reasoning chain faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model choice strongly affects the quality of reasoning chains under the same (similar) CoT method; AutoRace reveals that models with similar answer accuracy can have widely different reasoning-chain fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some smaller models (e.g., InternLM-2 7B) outperform some much larger models under CoT in these experiments; final-answer accuracy can mask large variability in chain correctness as measured by AutoRace.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models <em>(Rating: 2)</em></li>
                <li>Reasoning with Language Model is Planning with World Model <em>(Rating: 2)</em></li>
                <li>Self-evaluation guided beam search for reasoning <em>(Rating: 2)</em></li>
                <li>A Study of Automatic Metrics for the Evaluation of Natural Language Explanations <em>(Rating: 1)</em></li>
                <li>A Suite of Metrics for Scoring Step-by-Step Reasoning (ROSCOE) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4746",
    "paper_id": "paper-269004939",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "CoT (Llama-2 70B)",
            "name_full": "Chain-of-Thought prompting with Llama-2 70B (quantized)",
            "brief_description": "Autoregressive chain-of-thought (CoT) decoding using Llama-2-70B to produce a single step-by-step reasoning chain per query; treated as a 'similar' reasoning method because it produces one main chain without explicit multi-path search or ensemble.",
            "citation_title": "New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama-2 70B (quantized with GPT-Q)",
            "model_description": "Open-source Llama 2 70B model used in experiments, quantized with GPT-Q for efficiency; used as the student LLM across tasks.",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Autoregressively decodes a single reasoning chain (sequence of steps) using a CoT prompt; essentially greedy or single-chain sampling without explicit tree search or ensemble aggregation.",
            "task_name": "Multiple: GSM8K, AQuA, StrategyQA, Game-of-24, PrOntoQA, Blocksworld",
            "task_description": "A suite of mathematical, commonsense, logical, and embodied-planning reasoning benchmarks used to evaluate step-by-step reasoning.",
            "performance": "Qualitative: CoT is consistently outperformed by search-based methods (ToT and RAP) across datasets in this paper; CoT produces more false-positive reasoning chains (i.e., correct final answer with incorrect chain). Exact per-dataset accuracies with CoT are reported in the paper's tables (see Table 3 / Table 9).",
            "comparison_with_other_method": true,
            "performance_other_method": "ToT and RAP achieve higher overall accuracy and notably fewer false-positive reasoning chains; RAP &gt; ToT &gt; CoT overall (RAP outperforms ToT by 42% on Blocksworld).",
            "key_findings": "CoT (single-chain, similar reasoning) is weaker than search-based (diverse) methods: it cannot 'regret' earlier steps and therefore more often produces false-positive chains even when final answers are correct. Search-based approaches produce higher AutoRace scores (fewer false positives).",
            "counter_examples_or_negative_results": "No dataset in the paper shows CoT outperforming ToT or RAP overall; CoT does avoid some failure modes (e.g., excessive detail leading to factual errors in RAP on StrategyQA), meaning in some cases simpler CoT chains may avoid introducing extraneous incorrect details.",
            "uuid": "e4746.0",
            "source_info": {
                "paper_title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ToT (Llama-2 70B)",
            "name_full": "Tree-of-Thoughts (ToT) with Llama-2 70B (BFS/DFS variants)",
            "brief_description": "A tree-search based reasoning method that explores multiple reasoning branches (diverse) using an LLM to propose and evaluate actions; implemented with breadth-first (BFS) or depth-first (DFS) variants and self-evaluation / likelihood-based rewards.",
            "citation_title": "New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama-2 70B (quantized with GPT-Q)",
            "model_description": "Same as above: Llama 2 70B used as the student LLM for ToT experiments.",
            "reasoning_method_name": "Tree-of-Thoughts (ToT)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Uses a tree-search (BFS or DFS) to explore multiple reasoning branches; each candidate step/action is scored by reward functions (self-evaluation and/or likelihood) and the search selects among branches rather than committing to a single chain.",
            "task_name": "Multiple: GSM8K, AQuA, StrategyQA, Game-of-24, PrOntoQA, Blocksworld",
            "task_description": "Same multi-domain benchmark suite for reasoning used in the paper.",
            "performance": "Qualitative: ToT consistently outperforms vanilla CoT across tasks in this study (higher AutoRace and answer-based metrics in most cases). ToT (BFS) shows particularly strong gains on complex math, logical, and embodied planning tasks compared to DFS. Exact numeric accuracies are reported in the paper's tables.",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (worse); RAP (often better than ToT overall). For embodied Blocksworld RAP &gt; ToT by a large margin (RAP 42% higher than ToT on Blocksworld reported).",
            "key_findings": "Search-based (diverse) methods like ToT reduce false positives and improve final accuracy relative to CoT by exploring alternative reasoning branches and discarding low-reward branches; breadth (BFS) is generally more important than depth (DFS) for complex tasks with large search spaces.",
            "counter_examples_or_negative_results": "DFS variant of ToT can perform worse than BFS on large search-space tasks (math, logic, embodied planning) because DFS can get trapped in inferior subspaces; for constrained tasks with small search spaces (Game-of-24) DFS does not hurt exploration.",
            "uuid": "e4746.1",
            "source_info": {
                "paper_title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RAP (Llama-2 70B)",
            "name_full": "Reasoning-via-Planning (RAP) with Llama-2 70B",
            "brief_description": "A search-based reasoning approach that augments the LLM with an explicit world model and uses Monte-Carlo Tree Search (MCTS) with reward guidance to plan multi-step reasoning (diverse).",
            "citation_title": "New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama-2 70B (quantized with GPT-Q)",
            "model_description": "Same Llama-2-70B backbone used; RAP leverages the model both as proposer/evaluator and as a world model to predict state transitions.",
            "reasoning_method_name": "RAP (Reasoning-via-Planning)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Incorporates an explicit world model representing task state and uses a search algorithm (MCTS in experiments) guided by rewards (likelihood, self-evaluation, and task heuristics) to plan sequences of actions; retains state to inform future decisions.",
            "task_name": "Multiple with emphasis on embodied Blocksworld (and math/commonsense tasks)",
            "task_description": "Standard reasoning benchmarks plus embodied planning (Blocksworld) where explicit state tracking is critical.",
            "performance": "Qualitative/numeric highlights: RAP is the most effective method across most datasets in this study; notably, on Blocksworld RAP outperforms ToT by ~42% (reported in text). RAP also achieves higher AutoRace scores (fewer false positives) and higher final accuracy in many tasks (see paper tables).",
            "comparison_with_other_method": true,
            "performance_other_method": "ToT (worse); CoT (worse). On StrategyQA RAP shows higher false-positive rate in some cases due to prompt-format-induced excessive detail.",
            "key_findings": "An explicit world model combined with reward-guided search substantially improves performance, especially for embodied planning tasks that require tracking state across many actions. RAP reduces reasoning errors and increases robustness in stateful domains.",
            "counter_examples_or_negative_results": "On StrategyQA, RAP produced more false-positive reasoning chains than expected (higher AutoRace-detected error rate) because its iterative sub-questioning prompt led to extraneous details and factual errors; in that domain, CoT sometimes avoids such pitfalls by remaining concise.",
            "uuid": "e4746.2",
            "source_info": {
                "paper_title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Search-based vs CoT (general)",
            "name_full": "Search-based (diverse multi-path) methods compared to single-chain CoT",
            "brief_description": "A general comparison showing that reasoning methods that explore multiple reasoning paths (diverse) via search and reward guidance yield higher accuracy and fewer false-positive chains than single-chain CoT (similar).",
            "citation_title": "New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama-2 70B (representative student LLM used across search methods)",
            "model_description": "Llama-2-70B used as the base LLM for implementing CoT, ToT, RAP, and other search-based algorithms in the experiments.",
            "reasoning_method_name": "Search-based methods (ToT, RAP, beam/MCTS) vs CoT",
            "reasoning_method_type": "diverse (search-based) vs similar (CoT)",
            "reasoning_method_description": "Search-based methods sample or search multiple candidate reasoning chains (via BFS, DFS, beam, MCTS, or random shooting) and score/aggregate them using reward functions (self-eval, likelihood, heuristics), whereas CoT generates a single chain via autoregressive decoding.",
            "task_name": "Multiple reasoning benchmarks (GSM8K, StrategyQA, AQuA, Game-of-24, PrOntoQA, Blocksworld)",
            "task_description": "Multi-domain suite covering mathematical, commonsense, logical, and embodied planning reasoning.",
            "performance": "Qualitative: Search-based methods (ToT, RAP) achieve higher AutoRace and answer-based metrics and detect fewer false positive reasoning chains; AutoRace shows search-based methods have higher correctness on chains that reach correct answers (i.e., they truly are correct). AutoRace detected 70.4% of false positive chains overall (paper-level statistic about detection), and search methods reduced false positives relative to CoT (figures and tables in paper).",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT: lower accuracy and higher false-positive rate; exact per-benchmark numbers are provided in the paper tables.",
            "key_findings": "Reward-guided, multi-path search reduces false positives and improves final accuracy; breadth of search is often more important than depth; explicit world models further benefit stateful/embodied tasks.",
            "counter_examples_or_negative_results": "Search-based methods can increase verbosity and introduce additional factual errors in some prompt formats (e.g., RAP on StrategyQA), showing that diversity/exploration can sometimes harm reasoning chain faithfulness if prompts induce unnecessary details.",
            "uuid": "e4746.3",
            "source_info": {
                "paper_title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Breadth-vs-Depth (ToT BFS vs DFS)",
            "name_full": "Breadth-first (BFS) vs Depth-first (DFS) variants of Tree-of-Thoughts",
            "brief_description": "Experimental finding comparing BFS and DFS search strategies: BFS (greater breadth) is generally preferable for large search-space reasoning tasks, while DFS can be acceptable in tightly constrained search spaces.",
            "citation_title": "New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama-2 70B (used to run ToT BFS and DFS variants)",
            "model_description": "Llama-2-70B backbone used to generate candidate actions and self-evaluations under both BFS and DFS search strategies.",
            "reasoning_method_name": "ToT (BFS) vs ToT (DFS)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Two search strategies for ToT: BFS explores many parallel branches (high breadth) up to a limit; DFS goes deep along branches (high depth) possibly exploring fewer alternatives.",
            "task_name": "GSM8K, AQuA, StrategyQA, Game-of-24, PrOntoQA, Blocksworld",
            "task_description": "Same multi-domain benchmarks; the effect depends on the effective size of the reasoning search space.",
            "performance": "Qualitative: BFS (breadth) is relatively better on complex math, logical, and embodied tasks; DFS can get stuck in poor subspaces on large tasks. DFS does not harm performance in small/limited search-space tasks like Game-of-24.",
            "comparison_with_other_method": true,
            "performance_other_method": "DFS (worse in large search-space tasks; comparable in small ones)",
            "key_findings": "Breadth of exploration is generally more important than depth for efficient discovery of correct reasoning chains in complex tasks; set search breadth limits accordingly.",
            "counter_examples_or_negative_results": "In tasks with constrained/small search spaces (Game-of-24), DFS performs adequately and breadth is less critical.",
            "uuid": "e4746.4",
            "source_info": {
                "paper_title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Leading LLMs (CoT comparison)",
            "name_full": "Comparison of leading LLMs on CoT reasoning chains (GPT-4, Claude-3, Gemini, InternLM-2, Mistral, Mixtral, Llama-2, Qwen, Gemma)",
            "brief_description": "Evaluation of multiple popular LLMs using CoT prompting to compare their step-by-step reasoning abilities; reveals differences in chain quality that are not captured by final-answer accuracy alone.",
            "citation_title": "New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4, Claude-3 Opus, Gemini Pro, InternLM-2, Mistral, Mixtral, Llama-2, Qwen, Gemma (various sizes)",
            "model_description": "A mix of closed-source and open models evaluated under CoT prompting across multiple datasets; some models are very large (GPT-4, Claude-3), others are smaller (InternLM-2 7B, Mistral 7B), yet performance varies nonlinearly with size.",
            "reasoning_method_name": "Chain-of-Thought (CoT) across LLMs",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Apply the CoT prompting paradigm to each LLM and evaluate the resulting chains with AutoRace (and answer-based metrics) to measure both final answer correctness and reasoning chain faithfulness.",
            "task_name": "GSM8K, AQuA, StrategyQA (AutoRace applied to the latter three in the paper)",
            "task_description": "Mathematical and commonsense reasoning benchmarks where step-by-step reasoning is critical.",
            "performance": "Reported highlights: GPT-4 turbo and Claude-3 Opus are top-performing across tasks; InternLM-2 7B surprisingly outperforms larger Llama-2 70B on average; on StrategyQA answer accuracy across top models is similar (0.63–0.79) whereas AutoRace chain-quality scores vary widely (0.28–0.91), indicating model differences in reasoning chain faithfulness.",
            "comparison_with_other_method": null,
            "performance_other_method": null,
            "key_findings": "Model choice strongly affects the quality of reasoning chains under the same (similar) CoT method; AutoRace reveals that models with similar answer accuracy can have widely different reasoning-chain fidelity.",
            "counter_examples_or_negative_results": "Some smaller models (e.g., InternLM-2 7B) outperform some much larger models under CoT in these experiments; final-answer accuracy can mask large variability in chain correctness as measured by AutoRace.",
            "uuid": "e4746.5",
            "source_info": {
                "paper_title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Reasoning with Language Model is Planning with World Model",
            "rating": 2,
            "sanitized_title": "reasoning_with_language_model_is_planning_with_world_model"
        },
        {
            "paper_title": "Self-evaluation guided beam search for reasoning",
            "rating": 2,
            "sanitized_title": "selfevaluation_guided_beam_search_for_reasoning"
        },
        {
            "paper_title": "A Study of Automatic Metrics for the Evaluation of Natural Language Explanations",
            "rating": 1,
            "sanitized_title": "a_study_of_automatic_metrics_for_the_evaluation_of_natural_language_explanations"
        },
        {
            "paper_title": "A Suite of Metrics for Scoring Step-by-Step Reasoning (ROSCOE)",
            "rating": 1,
            "sanitized_title": "a_suite_of_metrics_for_scoring_stepbystep_reasoning_roscoe"
        }
    ],
    "cost": 0.014791499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models
8 Apr 2024</p>
<p>Shibo Hao 
University of Florida
3 MBZUAI</p>
<p>Yi Gu 
University of Florida
3 MBZUAI</p>
<p>Haotian Luo 
University of Florida
3 MBZUAI</p>
<p>Tianyang Liu 
University of Florida
3 MBZUAI</p>
<p>Xiyan Shao 
University of Florida
3 MBZUAI</p>
<p>Xinyuan Wang 
University of Florida
3 MBZUAI</p>
<p>Shuhua Xie 
University of Florida
3 MBZUAI</p>
<p>Haodi Ma 
University of Florida
3 MBZUAI</p>
<p>Adithya Samavedhi 
University of Florida
3 MBZUAI</p>
<p>Qiyue Gao 
University of Florida
3 MBZUAI</p>
<p>Zhen Wang 
University of Florida
3 MBZUAI</p>
<p>Zhiting Hu 
University of Florida
3 MBZUAI</p>
<p>Uc San Diego 
University of Florida
3 MBZUAI</p>
<p>New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models
8 Apr 2024F83113C7AA2A21711C5EC23124436650arXiv:2404.05221v1[cs.CL]
Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability.Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge.The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison.This paper aims to close the gap:(1) We introduce AutoRace for fully automated reasoning chain evaluation.Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks.In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria.(2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward and world model components.With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP).The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.</p>
<p>Introduction</p>
<p>A central topic in Large Language Model (LLM) research is to enhance their ability of complex reasoning on diverse problems (e.g., logical reasoning, mathematical derivations, and embodied planning).Rich research has been done to generate multi-step reasoning chains with LLMs, such as Chain-of-Thoughts (CoT, Wei et al., 2022), Tree-of-Thoughts (ToT, Yao et al., 2023), Reasoning-via-Planning (RAP, Hao et al., 2023), among others (Zhu et al., 2022;Xie et al., 2023;Zhuang et al., 2023;Khalifa et al., 2023;Creswell &amp; Shanahan, 2022).However, despite the burgeoning body of literature, there lacks a systematic analysis and understanding of the diverse approaches, mainly due to two key challenges:</p>
<p>First, automatic evaluation of multi-step reasoning chains is difficult.Previous studies mostly rely on the accuracy of the final answers as a proxy for assessing the reasoning processes.However, as LLMs tend to produce unfaithful outputs or hallucinate, a correct final answer does not necessarily imply a logically sound reasoning chain (Figure 1, a) (Golovneva et al., 2022;Prasad et al., 2023;Tyen et al., 2023;Lyu et al., 2023).Indeed, by manually evaluating 100 reasoning chains generated by Llama-2-70B on the StrategyQA questions (Geva et al., 2021), we found up to 39% of such false-positive cases that contain reasoning errors despite having correct final answers.Recent efforts have attempted to evaluate the reasoning chains directly, but often require non-trivial human efforts, such as human-written reasoning chains as references (Celikyilmaz et al., 2020), or manuallyannotated datasets for training evaluation models (Golovneva et al., 2022;Prasad et al., 2023).He et al. (2023); Tyen et al. (2023) use GPT-4 to alleviate human cost, but still require demonstration questions and in-depth error analyses by human experts before applying to each new task.In addition, their instructions that prompt GPT-4 for evaluation are not adaptive to different tasks, leading to suboptimal performance (Section 3.2).</p>
<p>Second, the varied reasoning approaches present distinct formalisms and implementations (Figure 1, c).The disparity makes it difficult to analyze the nuanced differences of their reasoning chain generation and compare their critical design elements.Therefore, it is desirable to have a more holistic formulation and unified implementation.This would reveal the underlying connections among different approaches, and facilitate a more systematic comparison when combined with automatic reasoning evaluation discussed above.</p>
<p>To tackle the challenges, this paper proposes an automatic method for reasoning chain evaluation, develops a cohesive library for various latest reasoning approaches, and on this basis, performs extensive analysis of LLM step-by-step reasoning.More specifically, we first present AutoRace (Automatic Reasoning Chain Evaluation), a fully automated approach for evaluating reasoning chains that adapts to different tasks without human efforts (Figure 1,  b).For each reasoning task (e.g., math reasoning), AutoRace autonomously constructs a detailed evaluation criteria list by summarizing errors in LLM-generated reasoning chains.The criteria list is then used to instruct GPT-4 to evaluate any given reasoning chains on the task.Compared to the prefixed human-written prompts (Tyen et al., 2023;He et al., 2023), the AutoRace criteria lists are automatically customized for each task with GPT-4 to ensure accurate evaluation.On a wide range of tasks, AutoRace shows strong correlation with human evaluation, and manages to detect 70.4% of incorrect reasoning chains that cannot be captured by the conventional final-answer-based evaluation.</p>
<p>We then introduce a unified perspective of reasoning algorithms, formulating them as a search process towards maximizing accumulated rewards (Figure 1,d).A wide range of existing reasoning algorithms can be interpreted as specific choices of the components in the unified formulation, including a reward function r to decide preferences on different reasoning steps, world model P to specify the state transition, and search algorithm (e.g., beam search, Monte-Carlo tree search) to explore the expansive reasoning space.Based on the unified perspective, we further develop the LLM Reasoners library that provides standardized implementation of these components with configurable options, plus rich LLM APIs and intuitive visualizations.As a result, LLM Reasoners allows us to easily reproduce the existing reasoning algorithms, and also compose new algorithms and apply to new tasks with minimal efforts.</p>
<p>With the new evaluation method and library, we conduct extensive analysis of reasoning chain generation of diverse LLMs and reasoning algorithms.We collect 6 challenging reasoning tasks that cover different reasoning skills (logical deduction, math, and embodied planning).Using a standardized evaluation protocol, we compare various most popular reasoning algorithms (e.g., CoT, ToT, RAP).The results offer a number of new insights into reasoning algorithm design-for example: (1) Reasoning as reward-guided search helps not only improve final accuracy, but also effectively alleviate false-positive reasoning chains; (2) For efficient search in the reasoning space, the breadth of search is generally more important than the depth for most tasks; (3) incorporating a world model that explicitly infers reasoning state would effectively improve the LLM reasoning ability, particularly for tasks in embodied environments; (4) inappropriate prompt format design might inadvertently lead to falsepositive reasoning chains.We also compare across diverse LLMs (GPT-4, Claude-3, Gemini, etc.) on their CoT reasoning chains.We release all code and experiments of AutoRace and LLM Reasoners at https://www.llm-reasoners.net/,hoping to spur the progress of research on LLM complex reasoning.</p>
<p>Related Work</p>
<p>Evaluation of Reasoning Chains.Traditionally, to evaluate the reasoning process, generated reasoning chains are compared with human-written explanations, which is known as reference-based reasoning evaluation.Conventional natural language generation (NLG) metrics were applied to calculate the similarity between machine-generated chains and human-crafted ones (Celikyilmaz et al., 2020;Clinciu et al., 2021;Welleck et al., 2022).Towards reference-free reasoning evaluation, Dalvi et al. (2021); Saparov &amp; He (2022); Han et al. (2022) designed structured reasoning tasks so that the reasoning process can be checked by a program automatically.Recently, ROSCOE (Golovneva et al., 2022) and ReCEval (Prasad et al., 2023) proposed reference-free metrics on general domains, measuring similarity, informativeness and correctness among steps.With the rapid development of LLM, He et al. (2023) and Tyen et al. (2023) have experimented to evaluate reasoning chains with LLMs.Tyen et al. (2023) proposed to prompt GPT-4 with few-shot demonstrations, but the results are not satisfactory, and the authors concluded that it is still challenging for LLMs to evaluate reasoning chains.He et al. (2023) crafted a detailed instruction inspired by the Socratic method, but the method requires GPT-4 to generate a reference chain at first, limiting its performance for challenging reasoning tasks that GPT-4 fails to solve.Besides, the fixed prompt template is not adjustable to different tasks, which also leads to suboptimal evaluation accuracy.In this work, we focus on LLM-based reference-free reasoning chain evaluation.Our method is generally more accurate and robust than existing metrics, while also saving any additional human efforts.</p>
<p>Step-by-step Reasoning with LLMs.A common practice to enhance the reasoning with LLMs is to generate intermediate reasoning steps, employing methods such as chain-ofthought prompting (Wei et al., 2022;Kojima et al., 2022) or question decomposition (Zhou et al., 2022;Li et al., 2023).Inspired by the deliberate reasoning of humans, recent research has focused on searching for better reasoning chains guided by reward (Zhu et al., 2022;Xie et al., 2023;Yao et al., 2023;Zhuang et al., 2023;Khalifa et al., 2023;Creswell &amp; Shanahan, 2022).Hao et al. (2023) proposed to incorporate a world model into reasoning, which simulates the state of the world.This enables LLMs to reason in a manner close to humans' Figure 2: For any reasoning tasks (e.g., commonsense reasoning), AutoRace automatically constructs an evaluation criteria list to help itself evaluate reasoning chains in this domain.conscious planning.Hu &amp; Shu (2023) presented the LAW formulation that connects the concepts of language models, agent models, and world models for more advanced and robust reasoning.We include a more systematic summary of reasoning algorithms in Section 4.</p>
<p>AutoRace: Automatic Reasoning Chain Evaluation</p>
<p>In this section, we present AutoRace that offers more insights into the LLM reasoning process than final answer correctness (Figure 1).Compared to previous works (Tyen et al., 2023;He et al., 2023) that prompt GPT-4 with fixed human-written instructions, AutoRace involves a "learning" process, which helps it to adapt to any problem domains.Specifically, for each task, AutoRace automatically collects LLM-generated incorrect reasoning chains, and summarizes evaluation criteria from them (Figure 2).With the criteria, GPT-4 can pay more attention to common errors for this certain domain, and make a more accurate evaluation.Compared to previous works that train an evaluation model (Golovneva et al., 2022;Prasad et al., 2023) by optimization model parameters, AutoRace effectively leverages GPT-4's strong prior knowledge, so that it can learn from only incorrect reasoning chains, which can be collected automatically, instead of human labels of reasoning chains.</p>
<p>Evaluation Method</p>
<p>To formulate the problem, we consider a reasoning question x, and LLM-generated reasoning chains z, and the predicted answer y.Additionally, we have the reference answer y r , accompanied by a reference reasoning chain z r , which are available in the training set D train of most reasoning datasets.Our goal is to develop an automatic evaluation metric for the reasoning chain, s(z) ∈ {0, 1}, which is better aligned with human evaluation of the reasoning chains.</p>
<p>As the first step to criteria list construction, one needs to find out what kinds of errors are common for a task.Therefore, AutoRace is designed to condense the criteria from real mistakes in LLM-generated reasoning chains (Figure 2, I).Here, we make use of the fact that a reasoning chain reaching a wrong answer must include an intermediate mistake.Given a sub-sampled training set D = {(x, y r , z r )} ⊂ D train , we run Chain-of-Thoughts reasoning with an LLM (referred as the student LLM) to expand the dataset to D ′ = {x, y r , z r , y, z}, where z is the reasoning chain generated by the student and y is the predicted answer extracted from z.Then, we can filter out a subset where the generated answers disagree with the reference answers, D error = {(x, y r , z r , y, z) ∈ D ′ | y r ̸ = y}.</p>
<p>Having these reasoning chains with errors, the next goal is to compile a criteria list.To reduce the difficulty, we divide it into two simple steps: Detection and Summarization.The Detection step identifies the specific errors in a reasoning chain.GPT-4 is presented with the question, the reference reasoning chain, and the student reasoning chain.It is then instructed  1: Evaluation accuracy of various reasoning chain evaluation metrics.We also list the accuracy of answer-based evaluation as a reference.Note that AutoRace is the only metric that does not take any human inputs specific to reasoning tasks (i.e., fully automated).We highlight the best reasoning chain metrics (dark green) and metrics within 5% of the best performance (light green) for each task.AutoRace achieves the best average accuracy and is robust across datasets.</p>
<p>to point to the mistake in the student reasoning chain (Figure 2, II).The underlying rationale is that, even if the question x might be challenging for GPT-4 to solve on its own, it has a good chance of understanding the question and identifying the mistakes once it has access to the reference reasoning chain.</p>
<p>After collecting the errors in reasoning chains, GPT-4 is prompted to summarize these specific instances into a criteria list (Figure 2, III).Eventually, GPT-4 is able to evaluate any new reasoning chain z given a question x, by checking each criteria on each reasoning step.</p>
<p>The prompt template of each phrase is in Appendix A.5.</p>
<p>Experiments</p>
<p>To measure the efficacy of reasoning chain evaluation metrics, we use human-annotated binary correctness labels of reasoning chains as the ground truth, and calculate their accuracy.</p>
<p>Datasets.We experiment on 6 datasets covering mathematical, commonsense and logical reasoning.5 of them are from previous works (Golovneva et al., 2022;Tyen et al., 2023), originating from GSM8K (Cobbe et al., 2021), Multistep-Arithmetics (Srivastava et al., 2023), DROP (Dua et al., 2019), COSMOS-QA (Huang et al., 2019), Logical-Deduction (Srivastava et al., 2023) and Word-Sorting (Srivastava et al., 2023).We additionally sample and manually label reasoning chains from StrategyQA (Geva et al., 2021).The detailed statistics of these datasets can be found in Appendix A.</p>
<p>Baselines.We compare AutoRace with other LLM-based evaluation metrics for reasoning chains.(a) SocREval (He et al., 2023) crafted a detailed instruction prompt for GPT-4 through the Socratic method, which includes asking it to generate a reference reasoning chain before evaluation.This method also requires a one-shot demonstration written by humans for each task.Tyen et al. (2023) proposed three methods: (b) Direct (trace) asks GPT-4 to directly evaluate a reasoning chain; (c) Direct (step) asks GPT-4 to check the reasoning step by step; (d) CoT (step) asks GPT-4 to generate a reasoning process before evaluating each reasoning step.All these methods require 3-shot demonstrations written by humans.We don't experiment with metrics based on fine-tuned small models (Golovneva et al., 2022;Prasad et al., 2023), as existing literature has already indicated a substantial performance gap between these methods and LLM-based metrics (He et al., 2023).</p>
<p>Results.The results to compare various evaluation methods are presented in Table 1.We can observe that among all metrics for reasoning chains, AutoRace achieves the best overall performance.It excels in 3 out of 6 tasks and exhibits robustness, maintaining performance levels within 5% of the best results across the board.Note that different from all baseline, which requires human-written demonstrations, AutoRace does not need any human input specific to reasoning tasks.Indicated by the confusion matrix (Figure 3, left), AutoRace is good at detecting incorrect reasoning chains, without sacrificing the performance in correct reasoning chains.On the contrary, SocREval mistakenly classifies many correct reasoning chains to be incorrect.Since SocREval asks GPT-4 to generate its own response as the reference, whenever GPT-4 fails to solve the problem itself, it's very likely to evaluate the reasoning chain to be incorrect, misled by the wrong reference.We even find that AutoRace enables GPT-4 to evaluate reasoning chains on problems challenging to GPT-4 itself, as a case study shown in Figure 7, Specifically, in a problem from MultiArith, a task for testing multi-digit arithmetic, SocREval fails because GPT-4 generates the reference with the same mistakes as the reasoning chain to be evaluated, but AutoRace identifies the tricky errors with the detailed criteria.We include more detailed results, and list the comparison of cost in Appendix A.</p>
<p>Moreover, when compared with the answer-based metric, AutoRace also outperforms it in 3 of 6 tasks with better overall performance.We additionally calculate the accuracy of AutoRace on reasoning chains with mistakes but reaching a correct answer (false positive reasoning chains), and it turns out that AutoRace managed to detect 70.4% of the false positive reasoning chains averaged across different tasks (Figure 3, right).We examine some false positive reasoning chains detected by AutoRace, and find the explanation given by AutoRace is mostly reasonable.The false positive reasoning chains can be classified into 3 types (Table 8).Based on these results, we believe AutoRace would be a useful metric complementary to answer-based evaluation.</p>
<p>LLM Reasoners: A Unified Formulation and Library</p>
<p>Besides reasoning chain evaluation, another difficulty in the analysis of reasoning algorithms lies in their distinct formulations and implementations.To investigate the critical design elements that affect the nuanced performance, we aim to deliver a more holistic formulation (Section 4.1) and unified implementation (Section 4.2) in this section.</p>
<p>Unified Formulation</p>
<p>There has been rich research on constructing reasoning chains to solve problems using LLMs, from the simplest CoT prompting (Wei et al., 2022), to tree search algorithms guided by a reward function (Yao et al., 2022;Xie et al., 2023;Hao et al., 2023) and a world model (Hao et al., 2023).These methods, among many others, can be formulated as a search process that maximizes the accumulated reward argmax (a 0 ,...,a T ) ∑ T t=0 r(s t , a t ), with a world model that predicts state transition s t ∼ T (•|s t−1 , a t−1 ), and a search algorithm to optimize the objective.This section elaborates on these three crucial components and demonstrates how recent reasoning algorithms can be interpreted as special cases within this framework, with specific choices of these three components ( Search Algorithm.The expansive reasoning space makes exhaustive search infeasible and calls for the use of more efficient search algorithms.For example, CoT implicitly applies greedy decoding for the reasoning step with the highest reward1 .Another common technique is to sample multiple reasoning chains, and return the one with the highest accumulated reward (Cobbe et al., 2021;Lightman et al., 2023;Wang et al., 2023), which in essence is a random shooting algorithm (Kothare et al., 1996).Other widely used search algorithms include DFS (Yao et al., 2023), beam search (Xie et al., 2023), A* (Zhuang et al., 2023), and MCTS (Hao et al., 2023;Zhao et al., 2024).These discrete searching algorithms usually need a finite action space sampled from an LLM.</p>
<p>Library Design</p>
<p>LLM Reasoners implements our unified formulation for multi-step reasoning with a modular design.As illustrated in Figure 4, users can easily set up a reasoning method by defining the WorldModel and SearchConfig, and importing a SearchAlgorithm.Besides these three main base classes, we have also integrated rich LLM APIs in LanguageModel class, standard evaluation pipelines in benchmark, and interactive visualization tool visualizer.Search Configuration.The SearchConfig class mainly includes two important functions: get_actions to decide the action space under each state to facilitate searching, and reward to assess the quality of each reasoning step.</p>
<p>Search Algorithm.The SearchAlgorithm specifies the strategy to explore the reasoning space.We have implemented several search algorithms in our library, e.g., Greedy Decoding (Khalifa et al., 2023), Beam Search (Xie et al., 2023;Creswell &amp; Shanahan, 2022), Depth-first Search (Yao et al., 2023), and Monte-Carlo Tree Search (Hao et al., 2023).These algorithms are designed to work seamlessly with any world model and search configuration on any reasoning task.</p>
<p>Other Features.LLM Reasoners has integrated To power these modules above with LLMs conveniently, we offer a standardized interface LanguageModel that supports a range of LLM libraries, including HuggingFace transformers (Wolf et al., 2020), facebookresearch/llama (Touvron et al., 2023a;b), APIs of GPT (OpenAI, 2023), Claude and Gemini (Team et al., 2023).We also integrated libraries specialized in quantization, like Exllama (Frantar et al., 2022), to reduce the hardware requirements.Additionally, the Benchmark class provides a standard platform (e.g., standard prompts, evaluation methods) for a collection of widely recognized reasoning tasks, such as GSM8k (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), and Blocksworld (Valmeekam et al., 2023).We also include Visualizer, an interactive visualization tool that allows for the straightforward depiction of the search trees.This tool significantly lowers the complexity of developing and analyzing the complicated reasoning process.More details with an example are shown in Appendix B.1.</p>
<p>Analysis of LLM Step-by-step Reasoning</p>
<p>To better understand multi-step reasoning algorithms and analyze the design elements critical to better reasoning performance, we evaluate them on diverse reasoning datasets, utilizing our AutoRace metric and LLM Reasoners library.Table 3: Experimental results of various reasoning methods on every dataset.On three datasets marked with * , we evaluate with AutoRace, and also show the answer-based (in brackets) for reference.On other datasets, we evaluate the reasoning chain with oracle verifiers (e.g., a rule-based program, or a simulator) due to their nature of close domains.The best method in every metric is highlighted in bold.</p>
<p>Datasets</p>
<p>For a comprehensive evaluation, we first collect reasoning tasks of several categories, where each category requires different reasoning skills.</p>
<p>Mathematical Reasoning.We select (1) GSM8k (Cobbe et al., 2021), a popular dataset of math word problems that requires understanding the relationship between numbers and multiple steps of mathematical calculation; (2) AQuA (Ling et al., 2017), which additionally requires the skill to perform algebra operations.Both answer-based and AutoRace metrics are employed on these two datasets.We also include the famous (3) Game of 24, following the settings in Yao et al. (2023).This task requires constructing an equation with four given numbers and basic arithmetic operations.We evaluate the reasoning chain on Game of 24 with a program.</p>
<p>Commonsense Reasoning.We take StrategyQA (Geva et al., 2021) as the commonsense reasoning dataset.Each sample is an open-domain yes-no question that requires raising related commonsense knowledge and multiple steps of inference.We evaluate the reasoning chain using both answer-based and AutoRace metrics.</p>
<p>Logical Reasoning.The tasks involve a set of logical principles, initial statements, and a concluding hypothesis.The challenge is to perform multi-step deductions to determine if the final hypothesis is true.We use PrOntoQA (Saparov &amp; He, 2022) in this category.The evaluation is based on a rule-based program, since the problems are from a close domain.</p>
<p>Embodied Planning.The capability of LLMs to power embodied agents presents an interesting area of study, as it involves the understanding of the physical world, and requires a strong planning ability toward the goal.To assess this capacity for embodied planning, we employ the Blocksworld benchmark (Valmeekam et al., 2023), where an agent must reach a specific block stacking arrangement through moving operations such as PickUp and Stack.</p>
<p>For evaluation, we examine whether the generated chain is valid and can lead to the target state with a simulator.</p>
<p>Evaluating Reasoning Algorithms</p>
<p>Compared methods.To analyze the connections between recent step-by-step reasoning methods, we pick three representative methods, CoT (Wei et al., 2022), ToT (Yao et al., 2023), and RAP (Hao et al., 2023).Different CoT which autoregressively decodes the reasoning chain, ToT and RAP define the reward and include tree search algorithms.RAP additionally incorporates an explicit world model.</p>
<p>Configurations.</p>
<p>For ToT and RAP, we mainly apply the combination of two rewards: (1) Self-evaluation: Prompting the LLMs to evaluate the new action, and use the logits of "good" as the reward, P θ ("Good" | s, a).</p>
<p>(2) Likelihood: Calculating the log-likelihood of predicting the next action given the current state, i.e., P θ (a | s).The definition of states and the world model for RAP depends on the reasoning tasks.Benefiting from the explicit world model, there are also other rewards available for RAP on certain tasks.More details about RAP implementation are described in Appendix C.1.</p>
<p>Implementation details.To ensure reproducibility and accessibility, we use one of the leading open-sourced LLMs, Llama 2 70B (Touvron et al., 2023b), quantized with GPT-Q (Frantar et al., 2022) for all tasks and methods.To make a fair comparison, we restrict search-based methods to explore up to 10 reasoning chains: The breadth limit is 10 for ToT (BFS), the maximum number of visited terminal nodes is 10 for ToT (DFS), and the maximum number of iterations is 10 in RAP, which is based on Monte-Carlo Tree Search.We manually crafted 10 examples of reasoning chains for each task, and all methods share this same example pool.For each test case, 4 examples are randomly sampled to form the demonstrations in the prompt, resulting in a 4-shot learning setting.</p>
<p>Results</p>
<p>Table 1 shows a comparative analysis of step-by-step reasoning algorithms.Overall, ToT consistently outperforms the vanilla CoT, and RAP further improves upon ToT.Notably, the evaluation metric (AutoRace) is generally lower than the answer-based metric, especially in the AQuA and StrategyQA datasets.This discrepancy suggests significant potential for enhancements in future reasoning algorithms.We outline several key findings below.</p>
<p>Reward-guided Search Reduces False Positives.Enhanced exploration in the reasoning space, facilitated by effective reward functions, naturally leads to the superior performance of search-based methods (ToT and RAP) over the autoregressive decoding method CoT.However, a noteworthy observation is that these search-based methods also yield fewer false positive reasoning chains, indicated by the smaller gap between AutoRace and answerbased metric (Table 3), and higher AutoRace score of reasoning chains with correct answers (Figure 5).Further examination of examples with ToT (BFS) reveals that it effectively avoids some false positives by discarding reasoning steps with low rewards.In contrast, CoT lacks this mechanism to "regret".E.g., in type-A false positive chains made by CoT (Table 8), while some reasoning mistakes are identifiable by the LLM itself, CoT fails to amend errors from previous steps, only able to overlook them in the following steps.</p>
<p>Importance of Search Breadth Over Depth.</p>
<p>By comparing two variants of ToT, ToT (BFS) and ToT (DFS), our results show that BFS is relatively better on two math word problems, logical and embodied tasks.This indicates that when the search space is expansive, such as in these complex math, logic, and embodied planning tasks, DFS may sink into inappropriate reasoning subspace with the first steps, thereby failing to explore the full space.In contrast, for tasks with a limited search space, such as Game-24, DFS doesn't hamper the exploration.</p>
<p>Crucial Role of World Model in Embodied</p>
<p>Tasks.RAP stands out as the most effective method across most datasets, thanks to its explicit world model.This enables the LLM to predict and track state changes during reasoning, allowing for decisions based on the current state.Specifically, it outperforms ToT by 42% on Blocksworld.As previous research suggests (Xiang et al., 2023), LLMs miss essential embodied knowledge and skills, e.g., tracking objects over long action sequences.Thus, an explicit world model that maintains the current state would greatly reduce the difficulties in memorizing or reasoning about previous actions in an embodied environment.</p>
<p>Impact of Prompt Format on False Positives.Interestingly, StrategyQA witnesses a higher false positive rate of RAP (Figure 5).Based on errors identified by AutoRace, we discovered a common failure mode from the reasoning chains generated by RAP: The prompt design that guides LLMs to iteratively ask and answer sub-questions encourages LLMs to generate excessive details.This makes it easier to introduce factual errors but does not necessarily affect the accuracy of final answers.For example, for the problem presented in Appendix C.2, RAP raises the incorrect number 7,000 in the explanation, which is identified as an error by AutoRace.Conversely, CoT avoids this pitfall by not delving into unnecessary details.It's worth noting that, this prompt format is not a problem for math reasoning tasks, including GSM8k and AQuA, because every detail needs to be accurate to solve a math problem.This suggests that prompt design should be tailored to the task domain.</p>
<p>Evaluating Leading LLMs</p>
<p>We also use the same experimental setting to compare the step-by-step reasoning ability of multiple popular LLMs, including GPT-4 (OpenAI, 2023), Claude-3 Opus2 , Gemeni pro (Team et al., 2023), InternLM-2 (Cai et al., 2024), Mistral (Jiang et al., 2023), Mixtral (Jiang et al., 2024), Llama-2 (Touvron et al., 2023b), Qwen (Bai et al., 2023), and Gemma (Team et al., 2024).The overall results are shown in Figure 6, with more details in Table 9 Overall Rankings.GPT-4 turbo and Claude-3 Opus are the two with the strongest reasoning abilities, and they lead on almost every reasoning task.Surprisingly, InternLM-2 7B surpasses much larger models (e.g., Llama-2 70B) on average performance.We also notice the ranking of Top-3 models is aligned with ChatArena leaderboard3 , which indicates that the reasoning ability is indeed crucial to power the SOTA chatbot.</p>
<p>Reasoning Tasks.Top models have achieved remarkable performance on math word problems (GSM8k) and commonsense reasoning (StrategyQA), but reasoning tasks that require strong planning abilities (e.g., Game-24 and Blocksworld) remain unsolved, which leaves room for future research.Interestingly, on StrategyQA, the answer accuracy of different models is similar (0.63 -0.79), but the AutoRace results differ a lot, ranging from 0.28 to 0.91, with a totally different ranking.Further examination reveals the questions in StrategyQA are often ambiguous and overly simplified.GPT-4, Claude-3, and Gemini demonstrate a more thorough consideration of these problems, unlike other models (and sometimes even the ground truth reasoning chains) which can suffer from baseless assumptions and flawed logic.The difference might be attributed to the RLHF process, which aligns model response to human preference.</p>
<p>Conclusion</p>
<p>We propose AutoRace, LLM-powered automated evaluation of reasoning chains, and LLM Reasoners, a unified formulation and library for diverse step-by-step reasoning algorithms.</p>
<p>On this basis, we conducted comprehensive experiments to analyze the factors contributing to reasoning.We are interested in extending LLM Reasoners by including more algorithms and supporting multi-modalities, and applying AutoRace for broader comparison.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi.Least-to-most prompting enables complex reasoning in large language models.arXiv preprint arXiv:2205.10625,2022.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang.Solving math word problem via cooperative reasoning induced language models.arXiv preprint arXiv:2210.16257,2022.</p>
<p>Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, and Chao Zhang.Toolchain<em>: Efficient action space navigation in large language models with a</em> search.arXiv preprint arXiv:2310.13227,2023.</p>
<p>Method</p>
<p>A Additional Details of AutoRace</p>
<p>A.1 Dataset Statistics</p>
<p>For all datasets we experiment on, we list the statistics by answer labels and human labels in Table 4.We also include the false positive (FP) rate, defined as the proportion of instances with correct answers but labeled as incorrect by human evaluators, relative to the total number of instances with correct answers.</p>
<p>A.2 Results on Reasoning Chains of Different Human Labels</p>
<p>We calculate the accuracy on two subsets: (1) reasoning chains labeled correct by human, and (2) reasoning chains labeled incorrect by human.The results are listed in Table 5 and Table 6.Generally, AutoRace balances the performance on both datasets, while SocREval suffers from a severe drop on the reasoning chains labeled incorrect.Figure 7: A case study on MultiArith (Srivastava et al., 2023).SocREval (He et al., 2023) requires GPT-4 to generate its own response to the problem, and then use it as the reference to evaluate the reasoning chain.In this case, it makes the same mistake as the reasoning chain.AutoRace successfully recognized the calculation error, guided by the criterion list learned for this task.</p>
<p>Method</p>
<p>A.3 Case Study</p>
<p>In Figure 7, we show a case study of AutoRace on MultiArith (Srivastava et al., 2023), in comparison with SocREval (He et al., 2023).</p>
<p>A.4 Comparison on the Cost of Reasoning Chain Evaluation Metrics</p>
<p>Table 7 shows the average input token number, output token number, and the cost per question using different evaluation methods.We list the statistics on two tasks: Word-Sort, which usually has long reasoning chains, and StrategyQA, which has shorter reasoning chains.</p>
<p>A.5 AutoRace Prompt</p>
<p>In this section, we demonstrate the pipeline of AutoRace for GSM8k (Cobbe et al., 2021) as examples.For other reasoning tasks, the template is the same, and only the questions, student answers, and extracted criteria need to be replaced.</p>
<p>Prompt for detection and summarization</p>
<p>You are a teacher.Below are some questions, reference answers and the answers from students.</p>
<p>Question: Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.She sells the remainder at the farmers' market daily for $2 per fresh duck egg.How much in dollars does she make every day at the farmers' market?</p>
<p>Reference answer: Janet sells 16 -3 -4 = 9 duck eggs a day.She makes 9 * 2 = 18 every day at the farmer's market.</p>
<p>Student answer: Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.This means she uses 3 + 4 = 7 eggs every day.She sells the remainder at the farmers' market daily for $2 per fresh duck egg.So she sells (16 -7) * $2 = $6 worth of eggs every day.The answer is 6.</p>
<p>Question: Claire makes a 3 egg omelet every morning for breakfast.How many dozens of eggs will she eat in 4 weeks?Reference answer: She eats 3 eggs every day and there are 7 days in a week so she eats 3<em>7 = 21 eggs a week After 4 weeks she will have eaten 4</em>21 = 84 eggs There are 12 eggs in 1 dozen and she'll eat 84 eggs so that's 84/12 = 7 dozen eggs.</p>
<p>Student answer: Claire makes a 3 egg omelet every morning.In one week she will eat 3 * 7 = 21 eggs.In 4 weeks she will eat 4 * 21 = 84 eggs.The answer is 84.</p>
<p>Question:</p>
<p>Gloria is shoe shopping when she comes across a pair of boots that fit her shoe budget.However, she has to choose between the boots and two pairs of high heels that together cost five dollars less than the boots.If one pair of heels costs $33 and the other costs twice as much, how many dollars are the boots?Reference answer:</p>
<p>The second pair of heels costs 33 * 2 = $66.The heels together cost 66 + 33 = $99.The boots cost $5 more than both pairs of heels together, so the boots cost 99 + 5 = $104.</p>
<p>Student answer:</p>
<p>We know that one pair of heels costs $33 and the other pair costs twice as much.This means that the other pair costs $33 * 2 = $66.Together, the two pairs of heels cost $33 + $66 = $99.The boots cost five dollars less than the heels, so the boots cost $99 -$5 = $94.</p>
<p>The answer is $94.</p>
<p>Question: Mark's car breaks down and he needs to get a new radiator.The cost for a new radiator is $400 but he goes to get it at a junk shop and gets it for 80% off.He then hires a mechanic to install it and it takes 3 hours at $50 an hour.How much did he pay?</p>
<p>Reference answer:</p>
<p>The discount on the radiator was 400<em>.8=$320So he paid 400-320=$80 The mechanic charges 3</em>50=$150 So in total he paid 80+150=$230 Student answer:</p>
<p>The cost for a new radiator is $400.He got it at a junk shop for 80% off, so he paid $400 * 0.8 = $320 for the radiator.The mechanic charged him $50 per hour for 3 hours, so he paid $50 * 3 = $150 for the labor.In total, he paid $320 + $150 = $470.The answer is 470.</p>
<p>Please summarize the mistakes in a short sentence for the question.At the end, please make a brief list of criteria.Make sure they are general and not specific to these questions so that others can grade the answers for other answers by following these criteria.</p>
<p>Prompt for Evaluation</p>
<p>Below is a question and an answer from a student.Please check the answer through each criterion, and make sure you carefully examine each reasoning step.Finally, if there is any step that fails the verification, output an INCORRECT, or else output a CORRECT.</p>
<p>The criteria are provided by the GPT in the Summarization step.</p>
<p>A.6 False Positive Reasoning Chains Detected by AutoRace</p>
<p>AutoRace can serve as a scalable way to detect false positive reasoning chains.Here, we summarize several types of false positive reasoning chains in Table 8.</p>
<p>B Additional Details of LLM Reasoners</p>
<p>B.1 Visualizer in LLM Reasoners</p>
<p>We show a screenshot of the visualization module of LLM Reasoners in Figure 8. Users have the option to upload the log file to our server with one line of code and use the web-based visualization to debug the reasoning algorithms.It allows users to interact with the log by selecting different reasoning trees, and checking the details of each node and edge in the tree.This tool significantly lowers the difficulty of applying a complex reasoning algorithm to new tasks.</p>
<p>C Additional Details of Experiment Analysis</p>
<p>C.1 Details of RAP Implementation</p>
<p>We explicitly define states and world models in RAP.For free-form question-answering tasks, including GSM8k, AQuA, and StrategyQA, a state is considered as the set of known variables.An action involves posing a sub-question about an unknown variable.The world model then responds to this sub-question, thereby augmenting the state with the Type Question</p>
<p>Reasoning Chain Explanation</p>
<p>A Henry made two stops during his 60-mile bike trip.He first stopped after 20 miles.His second stop was 15 miles before the end of the trip.How many miles did he travel between his first and second stops?</p>
<p>Step 1 -Henry traveled 20 miles + 15 miles = 35 miles between his first and second stops.</p>
<p>Step 2 -Henry traveled 60 miles -35 miles = 25 miles without stopping.</p>
<p>Step 3 -The answer is 25</p>
<p>There is a hallucination in an early reasoning step, but the following reasoning steps ignore the mistake and reach the correct answer.</p>
<p>B Is "A Tale of Two Cities" a parody of the Bible?</p>
<p>"A Tale of Two Cities" was published in 1859, and the Bible was published much earlier.Thus, "A Tale of Two Cities" is not a parody of the Bible.So the answer is no.</p>
<p>The reasoning chain has obvious or multiple mistakes, but hits the correct answer by chance.</p>
<p>C Is clementine pith highly sought after?</p>
<p>Clementine pith is not highly sought after.So the answer is no.</p>
<p>The reasoning chain is not informative at all, though the answer is correct.</p>
<p>Table 8: Common types of false positive reasoning chains detected by AutoRace.The example questions are from StrategyQA (Geva et al., 2021) and GSM8k (Cobbe et al., 2021), and the reasoning chains are generated by Llama-2 70B.An explanation for each type of false positive reasoning chain is attached.value of a new intermediate variable.For tasks with a clearly defined intermediate state, such as the remaining numbers in Game of 24, the latest facts in PrOntoQA, and the current block arrangement in Blocksworld, we directly employ these intermediate states in natural language description.Actions in these scenarios are defined respectively as a basic arithmetic operation, a one-hop logic reasoning, and a single block movement.The world model is prompted to predict the subsequent state as per the task's definition.</p>
<p>In GSM8k, AQuA, and StrategyQA, we use the rewards from the confidence of state transition and self-evaluation of usefulness of each step.In Game of 24, we use the rewards from self-evaluation of the possibility to achieve 24 for the state after each step.In PrOntoQA, we use the rewards of action likelihood and self-evaluation of the correctness of each step.</p>
<p>In Blocksworld, we use the rewards from the action likelihood and task-specific heuristics, i.e., the number of achieved subgoals.</p>
<p>C.2 Case Study on Prompt formats</p>
<p>Below, we present the reasoning chain of RAP and CoT to a question in StrategyQA.</p>
<p>---Question--Did either Kublai Khan or his grandfather practice monogamy?---RAP-----</p>
<p>Step 1: Did Kublai Khan have a harem?Kublai Khan had a harem of 7,000 women.So the answer is yes.</p>
<p>Step 2: Did Genghis Khan have a harem?Genghis Khan had a harem of 500 women.So the answer is yes.</p>
<p>Step 3: Does having a harem of women mean practicing polygamy?Having a harem of women means practicing polygamy.So the answer is yes.</p>
<p>Step 4: Now we can answer the question: Did either Kublai Khan or hisgrandfather practice monogamy?Kublai Khan had a harem of 7,000 women.Genghis Khan had a harem of 500 women.Having a harem of women means practicing polygamy.Thus, neither Kublai Khan nor his grandfather practiced monogamy.So the answer is no.</p>
<p>---CoT-----</p>
<p>Step 1: Both Kublai Khan and his grandfather, Genghis Khan, were polygamous.</p>
<p>Step 2: Thus, neither Kublai Khan nor his grandfather practiced monogamy.</p>
<p>Step 3: So the answer is no.</p>
<p>---reference--</p>
<p>Step 1: Kublai Khan was married multiple times and was said by some to have thousands of concubines.</p>
<p>Step 2: Kublai Khan's grandfather was Genghis Khan.</p>
<p>Step 3: Genghis Khan had six wives and was said to have over 500 concubines.</p>
<p>Step 4: So the answer is no.</p>
<p>C.3 AutoRace Leaderboard</p>
<p>We show the detailed evaluation results on leading LLMs in Table 9.</p>
<p>Figure 1 :
1
Figure 1: (a) The first challenge for analyzing step-by-step reasoning with LLMs: correct final answer may be derived from incorrect reasoning chains (false-positive chains), making it necessary to evaluate the reasoning chains directly.(b) Our proposed AutoRace for fully automated evaluation.(c) The second challenge stems from the diverse reasoning algorithms with seemingly distinct designs.(d) Our LLM Reasoners provides a unified formulation and standardized implementation.</p>
<p>Figure 3 :
3
Figure 3: Analysis on different reasoning chain evaluation methods: (Left) The macroaveraged confusion matrix of these methods.SocREval and AutoRace are both good at detecting incorrect reasoning chains, while SocREval mistakenly classifies correct reasoning chains as wrong more frequently.(Right) AutoRace can recognize 70.4% of the false positive reasoning chains, showing the promise to be a great complement to answer accuracy.</p>
<p>Figure 4 :
4
Figure 4: The three key components in a reasoning algorithm, reward function, world model, and search algorithm in the formulation (top), correspond to three classes in LLM Reasoners.To implement a reasoning algorithm for a certain domain (a Reasoner object), a user may inherit the SearchConfig and WorldModel class, and import any SearchAlgorithm.World Model.The WorldModel class is responsible for managing all the state changes during reasoning.It includes init_state to create the initial state, step to predict the next states, and is_terminal to identify terminal states.Utilizing our consistent API, users can effortlessly implement a world model for a specific task, or adapt the default world model recording previous actions, as done in CoT(Wei et al., 2022), ToT(Yao et al., 2023), etc.</p>
<p>Figure 5 :
5
Figure 5: AutoRace metric of different reasoning methods on answer-correct chains.We find search-based methods, ToT and RAP have higher AutoRace scores, indicating fewer false positive reasoning chains.</p>
<p>Figure 6 :
6
Figure6: Results of various LLMs using CoT on every dataset.We apply AutoRace on three datasets with *, and oracle verifiers on other datasets.On three datasets marked with * , LLMs are ordered by average performance.</p>
<p>Figure 8 :
8
Figure 8: A screenshot of the visualization tool to diagnose the RAP algorithms on the Blocksworld task.</p>
<p>Table 2
2
). CoT p LLM (a t |a 0 , ..., a t−1 , P CoT ) s t = (a 0 , ..., a t−1 ) Gready ToT p LLM ("correct" | a 0 , ..., a t , P sel f _eval ) s t = (a 0 , ..., a t−1 ) BFS/DFS Self-Eval p LLM ("correct" | a 0 , ..., a t , P sel f _eval ) s t = (a 0 , ..., a t−1 )
MethodReward rWorld ModelSearch Alg.Beam searchToolchain  <em>LST, self-consistency, etc.s t = f tool (s t−1 , a t−1 )A</em> searchORMf ORM (s t , a t ) if t = T, else 0s t = (a 0 , ..., a t−1 )Rand. Shoot.PRMf PRM (s t , a t )s t = (a 0 , ..., a t−1 )Rand. Shoot.SitSupf f inetuned (s t , a t )s t ∼ p f inetuned (• | s t−1 , a t−1 )GreedyRAPLikelihood, self-eval., etc.s t ∼ p LLM (• | s t−1 , a t−1 )MCTS</p>
<p>Table 2 :
2
Wang et al., 2023)2023)g algorithms, including CoT(Wei et al., 2022), ToT(Yao et al., 2023), Self-Eval(Xie et al., 2023), Toolchain * (Zhuang et al., 2023), ORM(Cobbe et al., 2021), PRM(Lightman et al., 2023), SitSup(Li et al., 2023), RAP(Hao et al., 2023), summarized in terms of the reward function, world model, and search algorithm.Wang et al., 2023)train step-by-step reward models with process supervision (PRM), to provide a more accurate reward for every step.Task-specific heuristic functions can also be applied as rewards, e.g.,Zhuang et al. (2023)use the longest common sub-sequence (LCS) score between the current generated action sequence and successful plans in the memory as a reward.</p>
<p>(Khalifa et al., 2023;Lightman et al., 2023;Sun et al., 2024;ribution T (s t |s t−1 , a t−1 ).For example, we can formulate the reasoning state of CoT(Wei et al., 2022)as the list of all previous actions, i.e., s t = (a 0 , a 1 , ..., a t−1 ), and thus the world model represents a deterministic transition which always appends an action to the action history.Beyond this trivial definition, recent studies seek a more substantive depiction of the reasoning state, e.g., the description of the physical environment, or the set of known variables, etc.To track the reasoning state,Liu et al. (2022);Guan et al. (2023)augment LLMs with a physical engine or PDDL domain model.Li et al. (2023)train a model to predict the entity states as a latent variable, and RAP(Hao et al., 2023)apply the LLM as a general world model for reasoning.When the LLM can interact with the external environment, e.g., calling tools(Zhuang et al.,  2023), the environment is the world model.Reward function.The reward function r(s t , a t ) decides whether a reasoning step is desired.CoT implicitly employs the likelihood predicted by the language models as the reward, as it generates the next reasoning step with high likelihood conditioned on previous steps and a CoT prompt, i.e., r(s t , a t ) = p LLM (a t |a 0 , ..., a t−1 , P CoT ).Yao et al. (2023);Hao et al. (2023);Xie et al. (2023)propose to use self-evaluation as the reward, asking the LLM to choose if the last action is "correct" or "wrong" with a self-evaluation prompt, i.e., r(s t , a t ) = p LLM ("correct" | s t , a t , P sel f −eval ).Cobbe et al. (2021)train an outcome-supervised reward model (ORM) to evaluate a reasoning chain, which predicts the reward only at the last reasoning step.More recent works(Khalifa et al., 2023;Lightman et al., 2023;Sun et al., 2024;</p>
<p>Table 4 :
4
The distribution of datasets for evaluation metrics.
MathCommonLogicalDataset GSM8k Arith Strategy Cosmos Logic SortCorrect ans11045100824545Incorrect ans9025510097255255Correct human101627131634Incorrect human99238129148294266FP rate0.1600.390.670.870.33MethodMathCommonLogicalDataset GSM8k Arith Strategy Cosmos Logic SortAnswer-based0.890.990.760.630.870.94SocREval0.890.920.960.910.890.76Direct (trace)0.900.340.780.740.210.33Direct (step)0.960.440.850.870.750.32CoT (step)0.930.880.780.841.000.87AutoRace (Ours)0.910.900.820.840.970.87</p>
<p>Table 5 :
5
Evaluation accuracy for reasoning chains labeled incorrect by humans.</p>
<p>Table 6 :
6
Evaluation accuracy for reasoning chains labeled correct by humans.
MathCommonLogicalDataset GSM8k Arith Strategy Cosmos Logic SortAnswer-based0.990.710.760.871.000.88SocREval0.890.550.240.291.000.85Direct(trace)0.890.580.830.581.000.68Direct(step)0.740.400.780.030.830.38CoT(step)0.820.110.780.131.000.14AutoRace (Ours)0.900.850.720.451.000.74</p>
<p>Table 7 :
7
Average cost on one problem for different evaluation methods, in the form of (input token number / output token number / cost per question).
MethodWord-Sort (Long)StrategyQA (Short)SocREval1686 / 427 / $0.030357 / 269 / $0.012Direct (trace)2265 / 1 / $0.023507 / 1 / $0.005Direct (step)17517 / 12 / $0.1761349 / 4 / $0.014CoT (step) 26839 / 5504 / $0.434 2688 / 575 / $0.044AutoRace (Ours)1382 / 435 / $0.027270 / 389 / $0.014</p>
<p>You are required to check the correctness of the reasoning chains step by step.The criteria are as follows:
<strong>Accuracy in Mathematical Operations:</strong> Ensure calculations arecorrect and follow logical mathematical principles.<strong>Understanding the Problem Statement:</strong> Comprehend the details andconditions of the question accurately.<strong>Correct Application of Mathematical Concepts:</strong> Apply the rightmathematical formulas, operations, or concepts to solve the problem.<strong>Unit Conversion and Appropriateness:</strong> When required, correctlyconvert units and use appropriate units in the answer.<strong>Final Answer Relevance:</strong> Ensure the final answer directly addressesthe question asked, and is presented clearly and concisely.<strong>Logical Reasoning and Step-by-Step Explanation:</strong> The answer shouldinclude a logical, step-by-step explanation that demonstrates how thefinal answer was reached.Question:[QUESTION]Student answer:[INPUT]</p>
<p>Table 9 :
9
The comparison of different models across various tasks.GSM8k, AQuA and Strategy are evaluated with AutoRace (The answer accuracy is shown as a reference in the bracket).Other results use the oracle verifier.</p>
<p>It's usually implemented as token-level greedy decoding.
https://www.anthropic.com/news/claude-3-family
https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.172972024Internlm2 technical report. arXiv preprint</p>
<p>Evaluation of text generation: A survey. Asli Celikyilmaz, Elizabeth Clark, Jianfeng Gao, arXiv:2006.147992020arXiv preprint</p>
<p>A study of automatic metrics for the evaluation of natural language explanations. Miruna Clinciu, Arash Eshghi, Helen Hastie, arXiv:2103.085452021arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2208.14271Faithful reasoning using large language models. 2022arXiv preprint</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, arXiv:2104.086612021arXiv preprint</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, arXiv:1903.00161Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. 2019arXiv preprint</p>
<p>Gptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.173232022arXiv preprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, Roscoe, arXiv:2212.07919A suite of metrics for scoring step-by-step reasoning. 2022arXiv preprint</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Zhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for machine reasoning and planning. Hangfeng He, Hongming Zhang, Dan Roth, arXiv:2310.00074arXiv:2312.052302023. 2023arXiv preprintSocreval: Large language models with the socratic method for reference-free reasoning evaluation</p>
<p>Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, arXiv:1909.002772019arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Grace: Discriminator-guided chain-of-thought reasoning. Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>Robust constrained model predictive control using linear matrix inequalities. Venkataramanan Mayuresh V Kothare, Manfred Balakrishnan, Morari, Automatica. 32101996</p>
<p>Language modeling with latent situations. Belinda Z Li, Maxwell Nye, Jacob Andreas, 10.18653/v1/2023.findings-acl.795Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.041462017arXiv preprint</p>
<p>Mind's eye: Grounded language model reasoning through simulation. Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M Dai, arXiv:2210.053592022arXiv preprint</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-of-thought reasoning. 2023arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Archiki Prasad, Swarnadeep Saha, Xiang Zhou, Mohit Bansal, arXiv:2304.10703arXiv:2210.012402023. 2022arXiv preprintReceval: Evaluating reasoning chains via correctness and informativeness</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, Chuang Gan, arXiv:2403.09472Easy-to-hard generalization: Scalable alignment beyond human supervision. 2024arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, Victor Cȃrbune, arXiv:2311.08516Llms cannot find reasoning errors, but can correct them!. 2023arXiv preprint</p>
<p>On the planning abilities of large language models (a critical investigation with a proposed benchmark. Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, Subbarao Kambhampati, arXiv:2302.067062023arXiv preprint</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, arXiv:2312.08935Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. 2023arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022arXiv preprint</p>
<p>Naturalprover: Grounded mathematical proof generation with language models. Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, Yejin Choi, Advances in Neural Information Processing Systems. 202235</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsOctober 2020</p>
<p>Language models meet world models: Embodied experiences enhance language models. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu, Advances in neural information processing systems. 202336</p>
<p>Self-evaluation guided beam search for reasoning. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
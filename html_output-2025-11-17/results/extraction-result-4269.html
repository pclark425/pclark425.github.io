<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4269 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4269</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4269</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-263908992</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.07984v1.pdf" target="_blank">Large Language Models for Scientific Synthesis, Inference and Explanation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of"knowledge", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this"knowledge"by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4269.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4269.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4SD_synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM4SD — Knowledge Synthesis from Scientific Literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A component of the LLM4SD pipeline that prompts pretrained large language models to synthesize domain-specific, measurable rules (qualitative laws/principles) from the scientific literature encoded in the LLMs' pretraining, producing feature-like rules usable for interpretable predictive models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B, 30B, 7B, 40B</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM4SD — Knowledge Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LLMs (Galactica and Falcon series) are prompted in a role-play persona (e.g., 'assume you are an experienced chemist') to enumerate 20 or 30 rules important for predicting a target molecular property. Rules are required to have numeric or categorical measures so they can be transcribed into computable functions. After generation, the LLM's outputs are summarized and deduplicated (using the LLM's summarization ability) into a final rule list; each rule is then implemented as a feature-extraction function that maps each SMILES instance to a numeric/categorical value. These rule-based features are used to train interpretable models (random forest or linear classifiers). The pipeline explicitly leverages the LLMs' pretraining on scientific corpora (arXiv, Wikipedia, domain literature encoded in Galactica) rather than performing an explicit document retrieval-and-extract pipeline; validation of synthesized rules is done by statistical tests (Mann-Whitney U for classification, linear-regression t-test for regression) and literature review with domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>molecular sciences (physiology, biophysics, physical chemistry, quantum mechanics); drug discovery / computational chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Feature-focused empirical/qualitative rules and design principles (empirical generalizations and mechanistic heuristics) that relate molecular structure/properties to target labels (e.g., determinants of BBB permeability such as molecular weight, lipophilicity, TPSA and hydrogen bonding), described as measurable rules rather than formal symbolic theories.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) For BBBP: lower topological polar surface area (TPSA), moderate molecular weight, higher lipophilicity (logP), fewer hydrogen bond donors/acceptors predict increased blood-brain barrier permeability. 2) Galactica inferred importance of carbonyl functional groups and fragment rings as determinants of BBBP (hypothesized to affect cross-sectional area and membrane partitioning). 3) Rules relating obscure molecular substructures to Gibbs free energy (ΔG°) in QM tasks (second-order features).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Statistical significance testing of each rule (Mann-Whitney U for classification, linear-regression t-test for regression) with p<0.05, cross-referencing rules with existing literature via expert literature review, and measuring downstream predictive performance improvements (interpretable models trained on rule features).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pipeline-level predictive improvements: Physiology AUC-ROC improved from best baseline 74.43% to 76.60% (+2.8%); Biophysics AUC-ROC improved from 81.7% to 83.4% (+2.0%); Quantum mechanics MAE improved from baseline 11.2450 to 5.8233 (≈48.2% relative improvement); Physical chemistry MAE improved from 1.57 to 1.28 (≈18.5% relative improvement). Rule-level statistics: ~85% of literature-synthesized rules were statistically significant across tasks; inferred rules: average 91.3% statistically significant; of inferred rules, ~74% documented in literature and ~17.3% not previously identified by reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to specialized GNN baselines (AttrMask, GraphCL, MolCLR, 3DInfomax, GraphMVP, MoleBERT) and a Random Forest baseline using ECFP4 fingerprints, the LLM4SD pipeline (using synthesized + inferred rule features with interpretable models) outperformed baselines across 58 tasks (reported SOTA gains in multiple domains as above).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs pretrained on scientific literature can reliably summarize domain-established rules into measurable features that are both statistically significant and supported by literature; combining literature-synthesized rules with data-inferred rules yields better downstream performance than either alone; domain-specific pretraining (Galactica) enables stronger synthesis even at smaller parameter scale; LLMs can produce plausible 'second-order' features (non-obvious but mechanistically plausible heuristics) that merit expert follow-up; rule-based features enable interpretable models to match or exceed black-box SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The synthesis relies on knowledge encoded in LLM pretraining rather than explicit document-level extraction—the pipeline does not report processing a curated set of scholarly papers directly (number_of_papers is unspecified). Some inferred rules are dataset-specific and not present in literature, requiring careful validation to avoid spurious/overfitted heuristics. Performance varies with LLM scale and pretraining corpus (Falcon vs Galactica trade-offs). The authors note the need for statistical testing and expert review to mitigate memorization or hallucination; ethical concerns and limits of coverage/generalizability remain.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4269.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4269.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4SD_inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM4SD — Knowledge Inference from Data (rule mining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A component of LLM4SD where LLMs analyze labeled molecular datasets (samples with SMILES and labels/values) in a stepwise prompt-driven manner to infer empirical rules/patterns that discriminate classes or predict continuous properties; outputs are summarized and converted into computable features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B, 30B, 7B, 40B</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM4SD — Knowledge Inference</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LLMs are given instructions plus sampled batches of labeled instances (SMILES + labels/values) and prompted (e.g., 'Please infer step-by-step to come up with 3 rules that directly relate properties/structures to [Task Description]'). For each batch the LLM outputs candidate rules; outputs across batches are deduplicated and summarized by the LLM to form final inferred rules. Rules must be numeric/categorical so they can be implemented as functions. These features are vectorized for each instance and used to train interpretable models. Validation is by statistical tests and literature cross-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>molecular sciences / cheminformatics (physiology, biophysics, physical chemistry, quantum mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical regularities and structural-substructure-to-property associations (dataset-specific heuristics), including non-trivial 'second-order' features that relate structure to property through intermediate mechanistic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) Carbonyl functional groups and fragment rings flagged as predictive of BBB permeability (inferred from BBBP dataset). 2) Obscure molecular substructures influencing Gibbs free energy (ΔG°) in QM9-derived tasks. 3) Typical features like TPSA, hydrogen bond donors/acceptors, molecular weight and logP surfaced via inference as discriminative.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Statistical significance testing (Mann-Whitney U for classification, linear-regression t-test for regression), literature review by domain experts to check whether inferred rules are documented, and measuring the effect of inferred features on downstream model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Across tasks the inferred-rule set had ~91.3% statistically significant rules on average; 74% of inferred rules were documented in literature while 17.3% were not identified by reviewers; combining inferred rules with literature-synthesized rules improved domain-averaged predictive metrics (see LLM4SD_synthesis entry for domain-level gains).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared the contribution of inference-only features, synthesis-only features, and combined features; combined features consistently outperformed single-source features and other baselines (GNNs, RandomForest+ECFP4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Inference from data yielded a slightly higher proportion of statistically significant rules than literature-synthesis, and produced some novel but plausible rules absent from literature; combining inferred and synthesized rules produces the best downstream predictive performance; LLMs can infer dataset-specific, mechanistically interpretable features when prompted with labeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Inferred rules can be dataset-specific and not previously reported, necessitating careful validation to rule out spurious correlations; converting natural-language rules into precise, computable feature functions requires engineering and domain expertise; potential for overfitting to sampled batches; variability across LLM backbones and scales.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Can ChatGPT be used to generate scientific hypotheses? <em>(Rating: 2)</em></li>
                <li>BioBERT: a pre-trained biomedical language representation model for biomedical text mining <em>(Rating: 1)</em></li>
                <li>SciBERT: A pretrained language model for scientific text <em>(Rating: 1)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4269",
    "paper_id": "paper-263908992",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "LLM4SD_synthesis",
            "name_full": "LLM4SD — Knowledge Synthesis from Scientific Literature",
            "brief_description": "A component of the LLM4SD pipeline that prompts pretrained large language models to synthesize domain-specific, measurable rules (qualitative laws/principles) from the scientific literature encoded in the LLMs' pretraining, producing feature-like rules usable for interpretable predictive models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b",
            "model_size": "6.7B, 30B, 7B, 40B",
            "method_name": "LLM4SD — Knowledge Synthesis",
            "method_description": "LLMs (Galactica and Falcon series) are prompted in a role-play persona (e.g., 'assume you are an experienced chemist') to enumerate 20 or 30 rules important for predicting a target molecular property. Rules are required to have numeric or categorical measures so they can be transcribed into computable functions. After generation, the LLM's outputs are summarized and deduplicated (using the LLM's summarization ability) into a final rule list; each rule is then implemented as a feature-extraction function that maps each SMILES instance to a numeric/categorical value. These rule-based features are used to train interpretable models (random forest or linear classifiers). The pipeline explicitly leverages the LLMs' pretraining on scientific corpora (arXiv, Wikipedia, domain literature encoded in Galactica) rather than performing an explicit document retrieval-and-extract pipeline; validation of synthesized rules is done by statistical tests (Mann-Whitney U for classification, linear-regression t-test for regression) and literature review with domain experts.",
            "number_of_papers": null,
            "domain_or_field": "molecular sciences (physiology, biophysics, physical chemistry, quantum mechanics); drug discovery / computational chemistry",
            "type_of_laws_extracted": "Feature-focused empirical/qualitative rules and design principles (empirical generalizations and mechanistic heuristics) that relate molecular structure/properties to target labels (e.g., determinants of BBB permeability such as molecular weight, lipophilicity, TPSA and hydrogen bonding), described as measurable rules rather than formal symbolic theories.",
            "example_laws_extracted": "1) For BBBP: lower topological polar surface area (TPSA), moderate molecular weight, higher lipophilicity (logP), fewer hydrogen bond donors/acceptors predict increased blood-brain barrier permeability. 2) Galactica inferred importance of carbonyl functional groups and fragment rings as determinants of BBBP (hypothesized to affect cross-sectional area and membrane partitioning). 3) Rules relating obscure molecular substructures to Gibbs free energy (ΔG°) in QM tasks (second-order features).",
            "evaluation_method": "Statistical significance testing of each rule (Mann-Whitney U for classification, linear-regression t-test for regression) with p&lt;0.05, cross-referencing rules with existing literature via expert literature review, and measuring downstream predictive performance improvements (interpretable models trained on rule features).",
            "performance_metrics": "Pipeline-level predictive improvements: Physiology AUC-ROC improved from best baseline 74.43% to 76.60% (+2.8%); Biophysics AUC-ROC improved from 81.7% to 83.4% (+2.0%); Quantum mechanics MAE improved from baseline 11.2450 to 5.8233 (≈48.2% relative improvement); Physical chemistry MAE improved from 1.57 to 1.28 (≈18.5% relative improvement). Rule-level statistics: ~85% of literature-synthesized rules were statistically significant across tasks; inferred rules: average 91.3% statistically significant; of inferred rules, ~74% documented in literature and ~17.3% not previously identified by reviewers.",
            "comparison_baseline": "Compared to specialized GNN baselines (AttrMask, GraphCL, MolCLR, 3DInfomax, GraphMVP, MoleBERT) and a Random Forest baseline using ECFP4 fingerprints, the LLM4SD pipeline (using synthesized + inferred rule features with interpretable models) outperformed baselines across 58 tasks (reported SOTA gains in multiple domains as above).",
            "key_findings": "LLMs pretrained on scientific literature can reliably summarize domain-established rules into measurable features that are both statistically significant and supported by literature; combining literature-synthesized rules with data-inferred rules yields better downstream performance than either alone; domain-specific pretraining (Galactica) enables stronger synthesis even at smaller parameter scale; LLMs can produce plausible 'second-order' features (non-obvious but mechanistically plausible heuristics) that merit expert follow-up; rule-based features enable interpretable models to match or exceed black-box SOTA.",
            "challenges_limitations": "The synthesis relies on knowledge encoded in LLM pretraining rather than explicit document-level extraction—the pipeline does not report processing a curated set of scholarly papers directly (number_of_papers is unspecified). Some inferred rules are dataset-specific and not present in literature, requiring careful validation to avoid spurious/overfitted heuristics. Performance varies with LLM scale and pretraining corpus (Falcon vs Galactica trade-offs). The authors note the need for statistical testing and expert review to mitigate memorization or hallucination; ethical concerns and limits of coverage/generalizability remain.",
            "uuid": "e4269.0"
        },
        {
            "name_short": "LLM4SD_inference",
            "name_full": "LLM4SD — Knowledge Inference from Data (rule mining)",
            "brief_description": "A component of LLM4SD where LLMs analyze labeled molecular datasets (samples with SMILES and labels/values) in a stepwise prompt-driven manner to infer empirical rules/patterns that discriminate classes or predict continuous properties; outputs are summarized and converted into computable features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b",
            "model_size": "6.7B, 30B, 7B, 40B",
            "method_name": "LLM4SD — Knowledge Inference",
            "method_description": "LLMs are given instructions plus sampled batches of labeled instances (SMILES + labels/values) and prompted (e.g., 'Please infer step-by-step to come up with 3 rules that directly relate properties/structures to [Task Description]'). For each batch the LLM outputs candidate rules; outputs across batches are deduplicated and summarized by the LLM to form final inferred rules. Rules must be numeric/categorical so they can be implemented as functions. These features are vectorized for each instance and used to train interpretable models. Validation is by statistical tests and literature cross-checking.",
            "number_of_papers": null,
            "domain_or_field": "molecular sciences / cheminformatics (physiology, biophysics, physical chemistry, quantum mechanics)",
            "type_of_laws_extracted": "Empirical regularities and structural-substructure-to-property associations (dataset-specific heuristics), including non-trivial 'second-order' features that relate structure to property through intermediate mechanistic reasoning.",
            "example_laws_extracted": "1) Carbonyl functional groups and fragment rings flagged as predictive of BBB permeability (inferred from BBBP dataset). 2) Obscure molecular substructures influencing Gibbs free energy (ΔG°) in QM9-derived tasks. 3) Typical features like TPSA, hydrogen bond donors/acceptors, molecular weight and logP surfaced via inference as discriminative.",
            "evaluation_method": "Statistical significance testing (Mann-Whitney U for classification, linear-regression t-test for regression), literature review by domain experts to check whether inferred rules are documented, and measuring the effect of inferred features on downstream model performance.",
            "performance_metrics": "Across tasks the inferred-rule set had ~91.3% statistically significant rules on average; 74% of inferred rules were documented in literature while 17.3% were not identified by reviewers; combining inferred rules with literature-synthesized rules improved domain-averaged predictive metrics (see LLM4SD_synthesis entry for domain-level gains).",
            "comparison_baseline": "Compared the contribution of inference-only features, synthesis-only features, and combined features; combined features consistently outperformed single-source features and other baselines (GNNs, RandomForest+ECFP4).",
            "key_findings": "Inference from data yielded a slightly higher proportion of statistically significant rules than literature-synthesis, and produced some novel but plausible rules absent from literature; combining inferred and synthesized rules produces the best downstream predictive performance; LLMs can infer dataset-specific, mechanistically interpretable features when prompted with labeled examples.",
            "challenges_limitations": "Inferred rules can be dataset-specific and not previously reported, necessitating careful validation to rule out spurious correlations; converting natural-language rules into precise, computable feature functions requires engineering and domain expertise; potential for overfitting to sampled batches; variability across LLM backbones and scales.",
            "uuid": "e4269.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Can ChatGPT be used to generate scientific hypotheses?",
            "rating": 2,
            "sanitized_title": "can_chatgpt_be_used_to_generate_scientific_hypotheses"
        },
        {
            "paper_title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "rating": 1,
            "sanitized_title": "biobert_a_pretrained_biomedical_language_representation_model_for_biomedical_text_mining"
        },
        {
            "paper_title": "SciBERT: A pretrained language model for scientific text",
            "rating": 1,
            "sanitized_title": "scibert_a_pretrained_language_model_for_scientific_text"
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 1,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        }
    ],
    "cost": 0.0139955,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Scientific Synthesis, Inference and Explanation</p>
<p>Yizhen Zheng yizhen.zheng1@monash.edu 
Department of Data Science and Artificial Intelligence
Monash University
VictoriaAustralia</p>
<p>indicates equal contribution</p>
<p>Huan Yee Koh 
Department of Data Science and Artificial Intelligence
Monash University
VictoriaAustralia</p>
<p>Drug Discovery Biology
Monash Institute of Pharmaceutical Sciences
Monash University
VictoriaAustralia</p>
<p>indicates equal contribution</p>
<p>Jiaxin Ju 
School of Information and Communication Technology and Institute for Integrated and Intelligent Systems
Griffith University
QueenslandAustralia</p>
<p>indicates equal contribution</p>
<p>Anh T N Nguyen 
Drug Discovery Biology
Monash Institute of Pharmaceutical Sciences
Monash University
VictoriaAustralia</p>
<p>Lauren T May 
Drug Discovery Biology
Monash Institute of Pharmaceutical Sciences
Monash University
VictoriaAustralia</p>
<p>Victorian Heart Institute
Monash University
VictoriaAustralia</p>
<p>Geoffrey I Webb geoff.webb@monash.edu 
Department of Data Science and Artificial Intelligence
Monash University
VictoriaAustralia</p>
<p>Shirui Pan s.pan@griffith.edu.au 
School of Information and Communication Technology and Institute for Integrated and Intelligent Systems
Griffith University
QueenslandAustralia</p>
<p>Large Language Models for Scientific Synthesis, Inference and Explanation
FB149F66E1424304933513260ABB2681
Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language 1 .Despite their limited forms of 'knowledge,' these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation 2,3 .However, they have yet to demonstrate advanced applications in natural science 4,5 .Here we show how large language models can perform scientific synthesis, inference, and explanation.We present a method for using general purpose large language models to make inferences from scientific datasets of the form usually associated with special purpose machine learning algorithms.We show that the large language model can augment this 'knowledge' by synthesizing from the scientific literature.When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties.This approach has the further advantage that the large language model can explain the machine learning system's predictions.We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
<p>Introduction</p>
<p>Scientific productivity is in precipitous decline, with the rate of progress in many fields approximately halving every 13 years 6 .As scientific discovery becomes increasingly complex and challenging, traditional methodologies struggle to keep pace, necessitating innovative approaches.Meanwhile, Large Language Model (LLM) Artificial Intelligence systems have shown remarkable capabilities in a wide range of tasks.From creative writing to translating languages, from answering intricate queries 7,8 to code generation 9 , their capabilities have been transformative in various domains 1,2,3,10,11 .In this work we show that these LLMs have similar transformational potential in the natural sciences.Particularly, we demonstrate that LLMs can synthesize postulates from the scientific literature, make inferences from scientific data, and elucidate their conclusions with explanations.</p>
<p>LLMs are trained on large corpuses of text, including much of the scientific literature.Notable models like BioBert 12 , SciBERT 13 , Med-PALM 11 , and Galactica 14 are specifically tailored to the scientific domain.Meanwhile, general-purpose LLMs like Falcon 15 integrate extensive scientific literature in their pretraining, including sources such as arXiv and Wikipedia.We demonstrate that these systems have acquired deep abilities to interpret and manipulate the formal scientific language for describing molecules, SMILES strings, along with capability to apply information from the scientific literature in their interpretation.We present a scientific discovery pipeline LLM4SD (Large Language Models for Scientific Discovery) designed to tackle complex molecular property prediction tasks.LLM4SD operates by specifying rules for deriving features from SMILES strings that are relevant to predicting a target feature.Some of these rules are synthesized from the scientific literature that the LLMs encode.Others are inferred from training sets of SMILES strings each labelled with the relevant classes or property values.A standard machine learning model can then be learned from the training data using the rule-based features.Finally, our pipeline utilizes LLMs to produce interpretable outcomes, allowing human experts to ascertain the specific factors influencing the final predictions.We show that this pipeline achieves the current state of the art across 58 benchmark tasks spanning four domains -Physiology, Biophysics, Physical Chemistry and Quantum Mechanics.Despite these auspicious outcomes, we acknowledge the vastness and intricacy of the scientific discovery landscape; our endeavours have merely scratched the surface.Nonetheless, the strides made by LLM4SD pave the way for deeper exploration, heralding an era where AI-driven insights interweave with human ingenuity to redress the current decline in scientific productivity.Looking ahead, we are optimistic about AI's potential role as a linchpin in the future of scientific discovery, revolutionizing processes and expediting breakthroughs.</p>
<p>Large Language Models for Scientific Discovery</p>
<p>Our scientific discovery pipeline, LLM4SD, shown in Fig. 1 consists of 4 main components: Knowledge Synthesis from the Scientific Literature, Knowledge Inference from Data, Interpretable Model Training and Interpretable Explanation Generation.We demonstrate the application of our pipeline to 58 specialized property prediction tasks across four scientific domains: Physiology, Biophysics, Physical Chemistry, and Quantum Mechanics.</p>
<p>In the Knowledge Synthesis from Literature phase (Fig. 1a), LLMs use pre-trained knowledge from an extensive literature amassed from LLMs' pretraining 14,15 to synthesize domain-specific molecular property prediction rules.Then, in the Knowledge Inference from Data phase (Fig. 1b), LLMs harness their inferential and analytical skills to infer molecular property prediction rules from the patterns in the datasets.These rules can generate features that effectively distinguish between different class instances or predict specific properties, such as a molecule's lipophilicity.This process mirrors how human scientists formulate hypotheses based on observation.In both the knowledge synthesis and inference stages, we require that the rules have either a numerical or categorical measure associated with them.This ensures that the rules can be readily transformed into corresponding functions, which in turn can convert each data instance into a vector of values.</p>
<p>Rules, independently defined by LLMs, transform data instances into vectorized representations, i.e., features.These rule-based features facilitate the training of an interpretable model, e.g., random forest or linear classifier (Fig. 1c).Our preference for training these interpretable models stems from a desire to enhance transparency during predictions.Remarkably, we noted that when enhanced with LLM4SD, traditional interpretable models like random forests can surpass stateof-the-art baselines.These interpretable models, once trained, are adeptly employed for downstream application, encompassing both classification and regression scientific tasks.This entire workflow draws parallels with the methodical approach of human scientists-designing experiments to validate their proposed hypotheses.Please summarise the following information and generate response to explain why "C1=CC=C(C=C1)CCNN" is BBBP:</p>
<p>Step c result</p>
<p>Step a, b output</p>
<p>Step c output</p>
<p>Step c analysis</p>
<p>Classification Task</p>
<p>Regression Task</p>
<p>Linear Classifier Random Forest</p>
<p>Models</p>
<p>LLMs</p>
<p>In the final stages (Fig. 1d), we tap into the LLMs' adeptness at information summarization.They are tasked to demystify the decision-making mechanism, illuminating how these interpretable models arrive at prediction outcomes based on instance representations, rules, and their respective significance.This clarity and transparency positions LLMs as intuitive partners, enabling scientists to seamlessly interface with and derive insights from the system's decisionmaking processes.To improve usability for researchers, we have created a web-based application that offers knowledge synthesis, inference, and prediction with explanation functions (see Supplementary Information 3).</p>
<p>By fostering this symbiotic relationship, we not only amplify the efficacy of scientific investigations but also elevate the confidence and trust in AI-assisted conclusions, driving forward the frontier of collaborative research.</p>
<p>Experiment Results</p>
<p>In this section, we offer a synopsis of LLM4SD's pivotal results spanning the 4 domains of physiology, biophysics, quantum mechanics and physical chemistry.Notably, all results of LLM4SD are obtained based on open-source LLM backbones to ensure reproducibility.Subsequently, we delved into an ablation study of LLM4SD, examining its performance across various LLM backbones 14,15 of differing scales and pretraining datasets.</p>
<p>Overall Performance on Four Domains</p>
<p>To evaluate the versatility of LLM4SD's application, we conducted a comprehensive analysis of its performance across 58 molecular prediction tasks across the 4 domains (Fig. 2).Specifically, the physiology domain comprised (Blood-Brain Barrier Penetration) BBBP 16 , ClinTox 17 , Tox21 18 with 12 tasks, and SIDER 19 with 27 tasks.Biophysics had two tasks, BACE 20 and HIV 18 , while physical chemistry had three regression tasks: ESOL 21 , FreeSolv 22 and Lipophilicity 18 .Quantum mechanics presented 12 regression tasks under QM9 23 .The detailed description of these tasks is illustrated in the method section (see Methods, 'Datasets').We compared LL4SD's performance with specialized, state-of-the-art supervised machine learning techniques.These are advanced Graph Neural Networks (GNNs), namely AttrMask 24 , GraphCL 25 , MolCLR 26 , 3DInfomax 27 , GraphMVP 28 , and MoleBERT 29 .Each model was pretrained on large datasets with diverse molecular knowledge and then fine-tuned for specific tasks (see Methods).As a standard baseline, we implemented Random Forest 30 with ECFP4 31 as input set features.Benchmarking LLM4SD against the baseline, LLM4SD demonstrated its superior efficacy and performance (Fig 2).This exemplary performance spanned 58 diverse tasks, from physiology (Extended Data Fig. 1-3) and biophysics (Extended Data Fig. 4) to physical chemistry (Extended Data Fig. 5) and quantum mechanics (Extended Fig. 6).</p>
<p>In both physiology and biophysics, our model outperformed all existing baselines (Fig. 2a).Notably, we attained state-of-the-art (SOTA) results in Physiology, raising the AUC-ROC from a previous best of 74.43% to 76.60%, a gain of 2.8%.In Biophysics, our model further enhanced performance, advancing the AUC-ROC from 81.7% to 83.4%, marking a 2.0% improvement.These advancements in physiology and biophysics emphasize the robustness and precision of LLM4SD in tasks that demand intricate biological understanding and modeling.On tasks in quantum mechanics and physical chemistry, LLM4SD demonstrated substantial advancements (Fig. 2b).In the domain of quantum mechanics, it showed a profound improvement of 48.2% over the best performed baseline, registering an average MAE of 5.8233 across 12 tasks as opposed to 11.2450.Similarly, in physical chemistry, LLM4SD observed a noteworthy enhancement, with the model reaching a MAE of 1.28 marking an 18.5% advancement over the baseline MAE of 1.57.These significant improvements in regression tasks affirm the refined capability of our approach in continuous prediction.</p>
<p>Overall, LLM4SD's marked improvements not only affirm its supremacy over specialized, and often black-box, state-of-the-art models but also highlight its unparalleled ability to synthesize postulates, infer scientific data, and provide insightful explanations.This offers a fresh perspective in computational research and heralds a new direction in scientific endeavors.</p>
<p>Ablation Study</p>
<p>To delve deeper into the intricacies of the LLM4SD pipeline, we conducted an ablation study, focusing on discerning the influence of scale and pretraining datasets on the performance of Large Language Models (LLMs).In addition, we assessed the relative contributions of knowledge synthesis and inference.Our evaluation spanned across a spectrum of foundational LLM backbones, notably the Falcon 7b 15 , Falcon 40b 15 , Galactica-6.7b 14, and Galactica-30b 14 .</p>
<p>Here, we selected open-source LLM backbones to ensure the reproducibility of our work.It is worth noting the distinct differences between the Falcon and Galactica series of LLMs.In particular, the Falcon models are trained for a broad range of applications, imbibing a more general context during their pretraining phase, while the Galactica models are pretrained on mainly scientific literature, making them particularly suitable for science.</p>
<p>Effect of Scale</p>
<p>The ablation study of LLM4SD, which compared four open-source LLM backbones, revealed substantial differences among the different LLMs (Fig. 3a, b).Particularly within the Falcon series, performance disparities were conspicuous.The Falcon 7b, a smaller model, fell short compared to the Falcon 40b in its range of domain expertise.Notably, it failed to conduct tasks in two key areas: physiology and quantum mechanics, indicating a weaker understanding of scientific challenges and data interpretation.Conversely, the Galactica series painted a more nuanced picture.Unlike with the Falcon series, a larger model did not necessarily translate to superior performance.In disciplines such as Physiology, Biophysics, and Physical Chemistry, Galactica 6.7b rivaled the performance of Galactica-30b, despite the latter having more than 4 times the number of parameters.However, a b c Fail Fail in the domain of Quantum Mechanics, the larger Galactica 30b surged ahead, outperforming Galactica 6.7b by a margin of 14%.This variance could be attributed to the intricate and abstract nature of Quantum Mechanics, where the depth and breadth of knowledge encapsulated in the larger model might offer a discernible advantage.</p>
<p>Effect of Pretraining Datasets of LLMs</p>
<p>From these observations it becomes evident that an LLM steeped in scientific literature, even if smaller in scale, exhibits a commendable prowess in scientific tasks (Fig. 3a, b).Conversely, the Falcon series, designed for general utility, necessitates a more substantial scale to effectively navigate scientific challenges.We postulate that this phenomenon is underpinned by the emergent capabilities 32 inherent to large-scale LLMs.These capabilities empower the more expansive Falcon-40b to bridge the knowledge gap and adapt to scientific tasks.In a broader perspective, despite their relatively modest scale, the Galactica models consistently outperformed the Falcon series, underscoring the pivotal role of domain-specific pretraining.</p>
<p>Contributions of knowledge synthesis and inference</p>
<p>In our exploration of LLM4SD with respect to various knowledge sources, we discerned the performance variance arising from the use of rule-based features synthesized from literature, rule-based features inferred from data, and a combined approach.Overall values for these categories were obtained by averaging over results for all tasks in a domain (Fig. 3c).</p>
<p>In a comprehensive assessment of various scientific domains, the combination of synthesis and inference features consistently outperformed individual methods.Specifically, in the field of physiology, an AUC-ROC of 76.38 was achieved using both methods, compared to 72.15 with synthesis alone and 72.12 with inference.Similarly, in biophysics, combining both methods yielded an AUC-ROC of 80.95, surpassing the scores of 75.62 and 77.23 obtained from synthesis and inference features, respectively.In physical chemistry, the combined approach resulted in an RMSE of 1.38, which is notably better than the 1.72 from synthesis features and 1.92 from inference features.Lastly, in Quantum Mechanics, the use of both synthesis and inference features produced a MAE of 6.82, improving upon the values of 9.81 and 7.18 recorded with synthesis and inference alone.Notably, comparing just synthesis with just inference, each outperformed the other in 2 out of the 4 domains.</p>
<p>These observations highlight the value of combining knowledge synthesis from scientific literature with inference from data.Literature imparts foundational theoretical insights, while empirical data identifies further regularities.The fusion of these knowledge facets equips the models with a comprehensive understanding, empowering them to excel across varied tasks and domains.</p>
<p>Statistical Analysis and Literature Review: Validating Established Rules</p>
<p>With LLM4SD outperforming specialized, state-of-the-art methods, we further validated the rules generated by Galactica-6.7bdue to its superior performance and ease of reproducibility.The rules were validated in two ways: statistical tests to confirm the significance of these rules, and literature review to assess whether the rules are discussed in existing scientific literature.</p>
<p>For statistical tests of rules, we employed the Mann-Whitney U test 33 for classification tasks and the linear regression t-test for regression tasks.The Mann-Whitney U test 33 compared the distributions of chosen rule across the two classes of the target variable, thereby evaluating the statistical relevance of the rule's ability to split and distinguish classes.Conversely, the linear regression t-test treated the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to regression prediction.Chemistry.In the statistical analysis, the significance of a rule is determined based on the task type: for classification, the Mann-Whitney U test 33 compares the difference in distributions of chosen rule across the two classes of the target variable; and for regression, the linear regression t-test 26 treats the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to prediction.In both cases, we used a 0.05 p-value threshold to determine rule significance.In the literature review, we assessed the prevalence of a rule in existing literature.With statistical analysis and literature review, each rule is categorized into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant.Across all tasks, literaturesynthesized knowledge rules were generally both prevalent in existing literature and statistically significant.In contrast, empirically inferred data rules yielded mixed results, with some easily found in existing literature and others not identified by the researchers.We further carried out a comprehensive review with in-domain experts to evaluate the prevalence of a rule in existing literature (see Supplementary Information 4: Literature Review(Example)).After cross-referencing the rules with scientific literature, we categorized each rule into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant (Fig. 4).</p>
<p>Knowledge Synthesis from Scientific Literature</p>
<p>We discovered that most of the synthesized rules we examined are readily available in existing scholarly works.Notably, an overwhelming majority (85%) of these rules were statistically significant in indicating the target labels across all selected tasks, affirming our pipeline's ability to summarize rules from scientific literature that were the most important for different tasks and domains.</p>
<p>Importantly, except for BACE 20 and Tox21-NR-Ahr 18 , we found no instances where statistically significant rules were absent from existing literature (Fig. 4).This aligns with the design of our pipeline: without analyzing the data, LLMs tend to aggregate and summarize existing knowledge.To illustrate, in the context of BBBP, the rules generated by our pipeline were consistent with well-established determinants such as molecular weight, lipophilicity, distribution coefficient, topological polar surface area, and hydrogen bonds [34][35][36] .These findings validate the robustness and reliability of our pipeline in leveraging LLMs to summarize existing scientific literature.</p>
<p>Knowledge Inference from Data</p>
<p>We found that an average of 91.3% of the inferred rules were statistically significant, higher than synthesized rules (Fig. 4).Of these, an average of 74% rules were already documented in existing scientific literature, while 17.3% were not identified by researchers.These latter rules were primarily associated with data patterns that are not widely discussed in the literature but can be inferred from our task dataset, such as obscure molecular substructures that influence target labels like the Gibbs free energy (ΔG°) of a molecule.</p>
<p>In contrast, to the knowledge synthesized from literature, we found that 6 out of 8 tasks have statistically significant rules that were absent from existing literature.This suggests that the rules produced by LLM4SD are not merely a result of the LLM's textual memorization during pretraining.Instead, the inferred rules reflect a genuine capability to derive meaningful rules from data based on the specific task.</p>
<p>Our case studies further substantiated the utility of these unidentified but significant rules.For instance, in BBBP where 38% of rules are significant but unidentified, Galactica 6.7B pinpointed the carbonyl functional group and fragment rings as key determinants of a molecule's BBBP.We hypothesize that these features are crucial for calculating a molecule's cross-sectional area, which in turn influences its orientation in lipid-water interfaces-factors vital for membrane partitioning and permeation 37 .Intriguingly, this suggests that our pipeline enables LLMs to infer what we term as second-order features.These are features that may not be immediately obvious or widely recognized but are consistent with established scientific principles in literature.In doing so, LLMs not only corroborate existing knowledge but also apply existing knowledge in interpreting data, thereby enriching the current scientific discourse.</p>
<p>By leveraging LLMs, our pipeline not only validates well-established scientific principles but also uncovers less documented and even potentially novel rules.This facilitates a more effective and transparent interaction between scientists and the AI system, enhancing both the quality and trustworthiness of the research output.Moreover, the statistically robust but underrepresented rules we identified could serve as promising avenues for future scientific exploration, thereby advancing the frontier of collaborative, AI-assisted research.</p>
<p>Discussion</p>
<p>In our exploration, we unveil the capabilities of LLM4SD through our specially designed pipeline, enabling LLMs to excel in scientific synthesis, inference, and explanation.Through seamless integration with our proposed architecture, LLMs exhibit state-of-the-art (SOTA) performance across a vast expanse of four domains.The inherent versatility of LLM4SD stands as a testament to its potential, making it poised for broader applications across varied domains, thus magnifying its relevance in the current scientific landscape.</p>
<p>Scientific discovery, vast in scope, is constantly evolving with our expanding understanding of the universe.Our study, ambitious in its intent, captures 58 tasks across four distinct domains, providing a glimpse into the immense reservoir of scientific knowledge.While this study serves as a pioneering beacon, demonstrating LLMs' transformative capabilities, it also signals the beginning of a broader exploration.We envision further expansion, integrating more diverse tasks and domains, pushing LLMs to their full potential and reshaping the boundaries of scientific inquiry.</p>
<p>Harnessing the immense power of AI-driven models for scientific discovery brings along its ethical challenges.The vast capabilities of such models, while revolutionizing our understanding, also raise concerns of potential misuse, especially in sensitive domains like biophysics and quantum mechanics.The reliance on machine synthesis and interpretation might overshadow the indispensable human element of scrutiny and ethics in research.As we plunge deeper into the AI era, it's crucial to tread with caution, balancing advancements with rigorous oversight and an unwavering commitment to ethical rigor.</p>
<p>As we gaze towards the horizon, the potential trajectory for LLM4SD is compelling.We anticipate a future where the nexus between LLMs and advanced scientific toolkits deepens.As computational capabilities grow and scientific knowledge expands, our pipeline stands poised for evolutionary enhancements.Our steadfast goal is to harmoniously fuse artificial intelligence with myriad scientific arenas, unlocking novel insights and pioneering avenues previously unimagined.</p>
<p>Methods</p>
<p>Datasets:</p>
<p>We conducted a thorough evaluation of LLM4SD, covering 58 subtasks across four unique domains for a robust assessment.The physiology domain included 41 tasks like BBBP, ClinTox, and the 12-task Tox21, ranging from NR-AR to SR-p53, along with the 27-task SIDER suite covering various medical conditions.Biophysics offered 2 classification tasks: BACE and HIV.</p>
<p>In physical chemistry, we addressed three regression tasks: ESOL, FreeSolv, and Lipophilicity, while the Quantum mechanics domain presented 12 regression tasks within the QM9 dataset, exploring properties from mu to G, providing a comprehensive insight into LLM4SD's capabilities.</p>
<p>Physiology.</p>
<p>BBBP:</p>
<p>The BBBP dataset contains 2,039 instances, each representing unique compounds labeled based on their permeability properties.Predicting which molecules can cross this barrier is paramount for drug development, especially for neurological conditions.</p>
<p>ClinTox:</p>
<p>The ClinTox dataset, with 1,478 instances, provides comprehensive information on the toxicological properties of various compounds.</p>
<p>Biophysics.</p>
<p>HIV: With a collection of 17,930 instances, the HIV dataset offers a comprehensive repository of molecules, represented in the SMILES format.This dataset is instrumental in the classification of compounds based on their potential inhibitory effects against HIV.</p>
<p>BACE:</p>
<p>The BACE dataset, comprising 11,908 instances, is a curated collection of molecules, each represented in the SMILES format.This dataset is tailored for classification tasks, aiming to discern molecules that can inhibit the BACE-1 enzyme.By analyzing the molecules within this dataset, researchers can glean insights into the structural features that confer inhibitory properties against BACE-1.</p>
<p>Physical Chemistry.</p>
<p>ESOL:</p>
<p>The ESOL dataset, comprising 1,128 instances, is a curated collection that delves into the solubility of molecules in water.By analyzing the ESOL dataset, researchers can gain a deeper understanding of the molecular features that dictate solubility, thereby aiding in the design of compounds with optimal solubility profiles.Each entry in this dataset is represented using the SMILES notation, a universal language for describing the structure of chemical species.</p>
<p>FreeSolv: With 642 instances, the FreeSolv dataset provides comprehensive data on the hydration free energy of molecules.This dataset is pivotal for researchers aiming to predict how molecules interact with water, which has implications for drug solubility and stability.Each molecule in the FreeSolv dataset is also represented using the SMILES notation.</p>
<p>Lipophilicity: Lipophilicity is a fundamental property that influences the absorption, distribution, metabolism, and excretion of drugs.The Lipophilicity dataset with 4200 compounds offers a rich resource for understanding this property.Analyzing this dataset allows researchers to discern the molecular attributes that contribute to a compound's lipophilicity, guiding the synthesis of molecules with desired pharmacokinetic properties.Like the other datasets in this domain, each entry is denoted using the SMILES notation.</p>
<p>Quantum</p>
<p>Baselines:</p>
<p>We rigorously assessed our pipeline in comparison to specialized, state-of-the-art supervised machine learning methods.For conventional approaches, we employed Random Forest 30 , using ECFP4 31 as the input feature set.We also considered state-of-the-art Graph Neural Networks (GNNs), including Attribute Masking (AttrMask) 24 , GraphCL 25 , MolCLR 26 , 3DInfomax 27 , GraphMVP 28 , and MoleBERT 29 .Each of these models was initialized with pre-trained weights and subsequently fine-tuned for specific tasks.</p>
<p>In summary, AttrMask pre-training involves teaching the GNN to predict randomly masked atom and bond attributes within molecular graphs.GraphCL and MolCLR use graph augmentations for a contrastive learning objective, aimed at maximizing the similarity between augmentations originating from the same molecule while minimizing similarity between augmentations from different molecules.GraphMVP and 3DInfomax leverage existing 3D molecular datasets to pretrain models capable of deducing 3D molecular geometry from 2D graphs, by optimizing mutual information between 3D summary vectors and GNN graph representations.Finally, MoleBERT, the recent state-of-the-art method, employs a VQ-VAE-based tokenizer to diversify atom vocabulary, thereby balancing dominant and rare atoms.It uses Masked Atoms Modeling (MAM) and Triplet Masked Contrastive Learning (TMCL) for node and graph-level pre-training, respectively.</p>
<p>LLM for Scientific Discovery Pipeline</p>
<p>In this section, we detail the proposed pipeline and the techniques used to align them with the requirements of molecular property prediction tasks.Instead of merely prompting LLMs to generate scientific hypotheses 38 or training them for direct predictions 39 , LLM4SD emulates how human experts conduct scientific research.This includes synthesizing knowledge from literature, inferring hypotheses from datasets, validating findings through experiments, and elucidating the rationale behind predictions.</p>
<p>Knowledge Synthesis from the Scientific Literature</p>
<p>LLMs are usually pretrained on large corpora of text data that include books, articles, websites and other written content.This extensive pretraining helps LLMs to learn the structure of the language, recognize patterns, understand context and acquire a wide-ranging knowledge of facts and concepts.Thus, the goal of the knowledge synthesis process is to extract relevant features from the vast pool of the knowledge that a LLM possesses from the pretraining stage.</p>
<p>To achieve this, we first instruct the LLM to adopt the persona of an experienced chemist, and then engage it to identify pertinent features based on its existing knowledge.This form of roleplaying prompt facilitates the knowledge mining process to mimic how human experts solve real-world challenges.For example, when a chemist needs to predict the bioactivity or BBBP of a molecule, they often apply feature-related rules such as number of hydrogen bond donors/acceptors, molecular weight, and logP.We require that the features identified by LLMs can be measured with a numerical or categorical measure to enable their transcription into corresponding functions.</p>
<p>Knowledge Inference from Data</p>
<p>The objective of knowledge inference form data is to harness the powerful reasoning skills of LLMs to identify relevant features by analyzing the given data.Given their impressive ability to solve mathematical problems and identify patterns, we conjecture that LLMs have the capacity to discern common patterns within groups of molecules based on its scientific understanding.To validate this hypothesis, we provide LLMs with an instruction and several batches of sampled instances with their corresponding class labels or instance property values.In the instruction, the LLM is tasked with analyzing patterns from provided data to identify features that effectively discriminate between two classes of instances or predict their property values.As a result, LLMs will come up with rules distilled from the analysis for each batch.Since the generated rules in different batches may contain duplicates, we ultimately employ the LLMs' summarization capability to condense the rules and eliminate duplicates, resulting in the final list of features.</p>
<p>Interpretable Model Training</p>
<p>In this stage, all the features identified in the first two stages are transcribed into corresponding functions.All these functions take a scientific instance as input, e.g., a SMILES string for molecules, and return a feature value.Consequently, the final representation of an instance resides in an r-dimensional space, where r is the number of features that have been identified.These vector representations function as the feature vectors for the model training.Employing interpretable models like a linear layer or random forest enables quantification of each rule's importance in prediction, thus elucidating their contribution to the model's final decision.This transparency fosters an intuitive comprehension of the decision-making process, enhancing trust and usability among domain experts.</p>
<p>Interpretable Explanation Generation</p>
<p>The final stage in our pipeline involves generating interpretable explanations for the predictions.Specifically, we furnish the LLMs with salient information, including the model prediction, the vector representation, important rules, and their importance scores derived from the random forest or linear layer.Utilizing the inference and summarization ability of the LLMs, the provided information is transformed into a text-based explanation.This stage is pivotal in rendering the results in an accessible manner.It ensures that users can seamlessly understand the decision-making process and each rule's contribution to the overall prediction, thereby enhancing trust and transparency.This accessibility not only facilitates user interaction with the model but also empowers experts in the field to utilize the generated insights for further analysis and decision-making.</p>
<p>Metrics</p>
<p>We assessed LLM4SD across 58 molecular property prediction tasks spanning four domains, utilizing distinct evaluation metrics tailored to each task's nature.For the domains of physiology and biophysics, the Area Under the Receiver Operating Characteristic curve (AUC-ROC) metric was employed.AUC-ROC, measures the ability of the model to distinguish between classes, with a range from 0 to 1, where a higher value indicates better performance.In the domain of physical chemistry, the Root Mean Square Error (RMSE) was used.RMSE quantifies the difference between predicted and observed values, with a lower value indicating a closer fit to the true data.For quantum mechanics, we utilized the Mean Absolute Error (MAE) metric.MAE measures the average magnitude of errors between predicted and true values, with smaller values denoting better accuracy.</p>
<p>Experiment setting</p>
<p>In our experimental setting, we partitioned the data into an 80/10/10 split for training, validation, and test sets.For the domains of physiology, biophysics, and physical chemistry, we employed a scaffold split for molecular compounds.The scaffold split method in these three domains ensures that molecules with similar structures are grouped together, providing a more challenging and realistic evaluation of model generalization.To ensure reproducibility and facilitate further research, the datasets split using this scaffold method are made available in our open-source GitHub repository.In the realm of quantum mechanics, we opted for a random split."without access to 3D information" is added only for QM9 dataset.Extended Data Table 3: Regression task descriptions for the general prompt in Extended Data Table 1.</p>
<p>Extended Data</p>
<p>Extended Data</p>
<p>Regression Task Name Task Description for Table X General Prompt</p>
<p>Fig. 1 .
1
Fig.1.LLM for Scientific Discovery Pipeline.(a) Knowledge synthesis from the scientific literature.In this phase, LLMs leverage their pre-trained understanding, amassed from pretraining on a massive body of literature, to synthesize domain-specific rules.(b) Knowledge inference from data.LLMs analyse the intrinsic patterns in the task-specific datasets to identify labelled patterns and infer empirical rules.(c) Interpretable Model Training.Rules formulated from LLMs are harnessed to convert data instances into features, enabling the development of models that are both effective and readily interpretable.(d) Interpretable Explanation Generation.Culminating the process, LLMs assimilate insights from the preceding steps to articulate comprehensive textual explanations, elucidating the rationales behind predictions.In the figure, the percentage value for each factor indicates its importance in concluding the prediction.</p>
<p>Prompt</p>
<p>C=C1)CCNN", is predicted to be blood-brain barrier permeable (BBBP) based on several key factors:1.TPSA (Topological Polar Surface Area): 8.29% The TPSA of this molecule is found to be 38.05, which is moderately low.A lower TPSA generally means better BBB permeability.2. Number of Heteroatoms: 5.78% This molecule possesses 2 heteroatoms, which suggests a moderate capacity to establish diverse chemical interactions.......... LLMs can perform Explanation for the prediction a. Knowledge Synthesis from the Scientific Literature LLMs LLMs can perform Synthesis from Literature Knowledge Assume you are an experienced Chemist.Please come up with 20/30 rules that are important to predict [Task Description] Assume you are a very experienced chemist.The following data includes molecules and their corresponding value [Task Description].Please infer step-by-step to come up with 3 rules that directly relate the properties/structures of a molecule to predict [Task Description].</p>
<p>Presence of Halogens Presence of Sulfur groups Presence of Nitro groups Vectorization Presence of Alcohol groups c.Interpretable Model Training Assume you are a very experienced Chemist.In the following data, with label 1, it means the smiles string is [Task Description].With label 0, it means the smiles string is not [Task Description].Please infer stepby-step to come up with 3 rules that directly relate the properties/ structures of a molecule to predict [Task Description].</p>
<p>Fig. 2 .
2
Fig.2.Comparison between LLM4SD and baselines across 4 domains.The red dotted line represents the average performance of all baselines.(a) Comparative analysis of model performance versus baselines in physiology and biophysics.(b) Comparative analysis of regression performance: LLM4SD vs. baselines in quantum mechanics and physical chemistry.</p>
<p>Fig. 3 .
3
Fig.3.Ablation study of LLM4SD.(a) Performance comparison of four open-source LLM backbones for physiology and biophysics.The vertical dashed line in the figure separates the two LLM series: Falcon on the left and Galactica series on the right.(b) Performance comparison of four open-source LLM backbones for physical chemistry and quantum mechanics.(c) Examining the influence of both synthesized and inferred knowledge on the average model performance across four domains.The triangle's color signifies the metric employed for domain-specific tasks.A (+) next to the metric name indicates higher values yield better results, while a (-) suggests the contrary.</p>
<p>Fig. 4 .
4
Fig.4.Literature Review and Statistical Analysis of LLM Rules.a-d.We conducted statistical analysis and a comprehensive literature review on rules generated by Galactica 6.7b across all four scientific domains, with two tasks evaluated for each domain: (a) Physiology, (b) Biophysics, (c) Quantum Mechanics, and (d) Physical Chemistry.In the statistical analysis, the significance of a rule is determined based on the task type: for classification, the Mann-Whitney U test33 compares the difference in distributions of chosen rule across the two classes of the target variable; and for regression, the linear regression t-test26 treats the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0, reflecting whether the rule contributes to prediction.In both cases, we used a 0.05 p-value threshold to determine rule significance.In the literature review, we assessed the prevalence of a rule in existing literature.With statistical analysis and literature review, each rule is categorized into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant.Across all tasks, literaturesynthesized knowledge rules were generally both prevalent in existing literature and statistically significant.In contrast, empirically inferred data rules yielded mixed results, with some easily found in existing literature and others not identified by the researchers.</p>
<p>Tox21: With 7 ,
7
831 instances, the Tox21 dataset is a collaborative effort to identify environmental toxicants.Its 12 classification tasks focus on specific biological targets or pathways.The Nuclear Receptor (NR) tasks, namely NR-AhR, NR-AR, NR-AR-LBD, NR-Aromatase, NR-ER, NR-ER-LBD, and NR-PPAR-gamma, examine interactions with intracellular proteins influencing gene expression and potential toxic effects.The Stress Response (SR) tasks, including SR-ARE, SR-ATAD5, SR-HSE, SR-MMP, and SR-p53, explore the impact of chemicals on stress-related cellular pathways.SIDER:The SIDER dataset, with 1,427 instances, offers detailed data on medication side effects.Each task in this dataset relates to a specific adverse drug reaction, aiding researchers in understanding and predicting potential drug side effects.All 27 classification tasks are: 1) Hepatobiliary disorders 2) Metabolism and nutrition disorders 3) Product issues 4) Eye disorders 5) Investigations 6) Musculoskeletal and connective tissue disorders 7) Gastrointestinal disorders 8) Social circumstances 9) Immune system disorders 10) Reproductive system and breast disorders 11) Neoplasms benign, malignant and unspecified (incl cysts and polyps).12) General disorders and administration site conditions 13) Endocrine disorders 14) Surgical and medical procedures 15) Vascular disorders 16) Blood and lymphatic system disorders 17) Skin and subcutaneous tissue disorders 18) Congenital, familial and genetic disorders 19) Infections and infestations 20) Respiratory, thoracic and mediastinal disorders 21) Psychiatric disorders 22) Renal and urinary disorders 23) Pregnancy, puerperium and perinatal conditions 24) Ear and labyrinth disorders 25) Cardiac disorders 26) Nervous system disorders 27) Injury, poisoning and procedural complications.</p>
<p>Mechanics. QM9 :
QM9
The Quantum Mechanics domain, central to understanding the fundamental properties of matter, is exemplified in our evaluation through the QM9 dataset.Comprising 133,885 instances, the QM9 dataset provides a comprehensive exploration of molecules' quantum mechanical attributes, essential for diverse applications from material science to pharmaceuticals.It includes 12 tasks: (Dipole Moment), (Polarizability), (Squared Radius), ZPVE (Zero-Point Vibrational Energy), (Heat Capacity at Constant Volume), (Energy Gap), (Highest Occupied Molecular Orbital Energy), (Lowest Unoccupied Molecular Orbital Energy), (Internal Energy at 0 Kelvin), U (Internal Energy at Standard State), H (Enthalpy), G (Gibbs Free Energy).</p>
<p>Prompts for Knowledge Inference from Data Classification Tasks (General):Assume you are a very experienced biologist/Chemist.In the following data, with label 1, it means [Task Description].With label 0, it means it is not.Please infer step-by-step to come up with 3 rules that directly relate the properties/structures of a molecule.Regression Tasks (General):Assume you are a very experienced biologist/chemist.The following data includes molecules and their corresponding value [Task Description].Please infer step-by-step to come up with 3 rules that directly relate the properties/structures of a molecule (without access to 3D information).Note:</p>
<p>ESOL the water solubility data (log solubility in mols per litre) FreeSolv the octanol/water distribution coefficient (logD at pH 7.4) Lipophilicity the hydration free energy of a molecule in water Quantum Mechanics dipole moment (Mu) of a molecule Isotropic polarizability of a molecule electronic spatial extent of a molecule Zero-Point Vibrational Energy (ZPVE) of a molecule the heat capacity at constant volume of a molecule the HUMO-LUMO gap of a molecule the highest occupied molecular orbital (HOMO) energy of a molecule Lowest Unoccupied Molecular Orbital (LUMO) energy of a molecule internal energy at absolute zero temperature (0 Kelvin), U0, of a molecule U internal energy of a molecule at a specific temperature, specifically at 298.15 Kelvin (approximately room temperature), (U) of a molecule H enthalpy of the molecule at a specific temperature, specifically at 298.15 Kelvin (approximately room temperature), (U) of a molecule G Gibbs free energy of the molecule at a specific temperature, specifically at 298.15 Kelvin (approximately room temperature), (U) of a molecule</p>
<p>Table 1 :
1
General prompt for Knowledge Synthesis from the Scientific Literature and Knowledge Inference from Data.
Prompts</p>
<p>for Knowledge Synthesis from the Scientific Literature Classification Tasks (General):</p>
<p>Assumed you are an experienced biologist/chemist.Please come up with [20 rules/30 rules] that you think are very important to predict [Task Description].Each rule is either about the structure or property of a molecule.Assumed you are an experienced biologist/chemist.Please come up with [20 rules/30 rules] that you think are very important to predict [Task Description].Each rule is either about the structure or property of a molecule (
Regression Tasks (General):</p>
<p>without access to 3D information).</p>
<p>is for smaller LLMs prompt while "30 rules" is for larger LLMs prompt."without access to 3D information" is added only for QM9 dataset.
Note:"20 rules"</p>
<p>Table 2 :
2
Classification task descriptions for the general prompt in Extended Data Table1.
Classification</p>
<p>Task Name Task Description for Table X General Prompt Note: add 'if' for Knowledge Synthesis from the Scientific Literature prompt
BBBP(if) a molecule is blood brain barrier permeable (BBBP)ClinTox(if) a molecule will be approved by the FDABACE(if) a molecule can inhibit human β-secretase 1(BACE-1)HIV(if) a molecule can inhibit HIV replication.
Note:</p>
<p>add 'it is related to' for Knowledge Inference from Data prompt Tox21 (it is related to</p>
<p>) the toxicity activity of a molecule against the [subtask description] in the [nuclear receptor (NR)/ stress response (SR)] signalling pathway.
nr-arandrogen receptornr-ar-lbdandrogen receptor ligand-binding domainnr-ahraryl hydrocarbon receptorSubtask Descriptionnr-aromatase nr-er nr-er-lbd nr-ppar-gamma sr-are sr-atad5 sr-hsearomatase estrogen receptor estrogen receptor ligand-binding domain peroxisome proliferator activated receptor nuclear factor (erythroid-derived 2)-like 2 antioxidant responsive element genotoxicity indicated by ATAD5 heat shock factor response elementsr-mmpmitochondrial membrane potentialsr-p53DNA damage p53-pathwaySider(it is related to) the side-effect activity of a molecule in causing [subtask name].respiratory, thoracic and mediastinal disordersmetabolism and nutrition disordersproduct issues eye disordersinvestigationsnames Subtaskmusculoskeletal and connective tissue disorders general disorders and administration site conditions skin and subcutaneous tissue disorders pregnancy, puerperium and perinatal conditionsblood and lymphatic system disorders surgical and medical procedures congenital, familial and genetic disorders reproductive system and breast disordersimmune system disorders cardiac disorders infections and infestations ear and labyrinth disorderssocial circumstances vascular disorders renal and urinary disorders gastrointestinal disordershepatobiliary disorders endocrine disorders psychiatric disorders nervous system disordersinjury, poisoning andproceduralneoplasms benign, malignant and unspecified (incl cysts and polyps)complications
Acknowledgements:H.Y.K. scholarship is supported by the Australian Government Research Training Program (RTP) Scholarship and Monash University as a co-contribution to Australian Research Council grant ARC DP210100072.L.T.M, G.W and A.T.N.N research into artificial intelligence applications for drug discovery is supported by a National Health and Medical Research Council (NHMRC) of Australia Ideas grant (APP2013629).Computational resources were generously provided by the Nectar Research Cloud, a collaborative Australian research platform supported by the NCRIS-funded Australian Research Data Commons (ARDC) and the MASSIVE HPC facility.We also gratefully acknowledge the support of the Griffith University eResearch Service &amp; Specialized Platforms Team and the use of the High-Performance Computing Cluster "Gowonda".S.R.P is supported by ARC Future Fellowship (No. FT210100097).Data availabilityThe datasets utilized in this study are entirely open-source and have been made publicly available to ensure straightforward replication of our findings.For research related to quantum mechanics, physical chemistry, biophysics, and physiology, the datasets can be accessed at https://moleculenet.org/datasets-1.Code availabilityIn our commitment to transparency and reproducibility, we will release our code showing our implementation in https://github.com/zyzisastudyreallyhardguy/LLM4SD.This encompasses methodologies for literature knowledge mining, knowledge inference rule mining, interpretable model training, and interpretable explanation generation.Throughout this work, we have employed several open-source libraries, including Hugging Face, numpy, rdkit, pytorch, scipy, bitsandbytes, and accelerate.Furthermore, we are in the process of deploying a website to facilitate scientists in utilizing LLM4SD.The site features three core functionalities for scientific users: knowledge synthesis, knowledge inference, and prediction with explanations.Examples of user interactions with the website can be found in the supplementary information.As part of our ongoing commitment, we anticipate the inclusion of additional tasks in the future development phases.Author Contribution:These authors contributed equally: Y.Z.Z., H.Y.K., J.X.J.These authors jointly supervised this work: S.R.P., G.I.W. S.R.P. and G.I.W. supervised the project.Y.Z.Z., H.Y.K., J.X.J. contributed to the conception and design of the work.Y.Z.Z., H.Y.K., J.X.J. contributed to the technical implementation.Y.Z.Z., H.Y.K., J.X.J. prepared the figures.Y.Z.Z.contributed to the design of the web-based application.A.T.N.N. and L.T.M. provided domain expertise for the literature review and validation of rules.Y.Z.Z., H.Y.K., A.T.N.N. and L.T.M. contributed to the design of the rule validation test.All authors edited and revised the manuscript.Competing Interests:The authors declare no competing interests.Extended Figures and TablesExtended Data Fig.1|Detailedperformance comparison between "LLM4SD" and eight baselines in the physiology domain.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD outperformed other models in 3 out of 4 datasets using the AUC-ROC metric, and consistently surpassing the average across all datasets.The results for Tox21 and SIDER are average scores from 12 and 27 tasks respectively (see Extended Data Fig.2 and 3for detailed breakdown).Extended Data Fig. 2|Detailed performance comparison between "LLM4SD" and eight baselines on Tox21Dataset.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD ranks among the top three methods in 10 out of 12 tasks, and consistently outperformed the average in all tasks.Extended Data Fig.3|Detailedperformance comparison between "LLM4SD" and eight baselines on Sider Dataset.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD ranks among the top three methods in 22 out of 27 tasks, and consistently outperforms the average in all tasks with the exception of the "Psychiatric disorders" task.Extended Data Fig.4|Detailed performance comparison between "LLM4SD" and eight baselines in the biophysics domain.The red dashed line shows the average result across all methods, in terms of AUC-ROC.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD outperformed the top-performing baseline by roughly 2% on the HIV dataset and closely matched the best performing method, RandomForest.In both cases, LLM4SD delivered a visibly superior outcome compared to the average performance.Extended Data Fig.5| Detailed performance comparison between "LLM4SD" and eight baselines in the physical chemistry domain.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD significantly outperformed all baseline methods on ESOL, demonstrating a 56% improvement over the average outcome for that dataset, and achieved stateof-the-art results on the additional datasets, FreeSolv and Lipophilicity.Extended Data Fig.6|Detailed performance comparison between "LLM4SD" and eight baselines in the quantum mechanics domain.The red dashed line shows the average result across all methods.Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs.LLM4SD excelled in predicting properties such as U0, U, H, and G, showing significant enhancements.In other tasks, the results from LLM4SD were comparable to the average of all methods.
Baby steps in evaluating the capacities of large language models. M C Frank, Nat Rev Psychol. 2023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, Adv Neural Inf Process Syst. 332020</p>
<p>GPT-4. 2023OpenAITechnical Report. preprint</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, D Leslie, Nat Rev Phys. 52023</p>
<p>Large language model AI chatbots require approval as medical devices. S Gilbert, H Harvey, T Melvin, Nat Med. 2023</p>
<p>Are ideas getting harder to find?. Nicholas Bloom, Charles I Jones, John Van Reenen, Michael Webb, American Economic Review. 11042020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, Advances in Neural Information Processing Systems. 352022</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, ICLR. 2023</p>
<p>Code Llama: Open Foundation Models for Code. Rozière, Jonas Baptiste, Fabian Gehring, Sten Gloeckle, Itai Sootla, Xiaoqing Gat, Yossi Ell En Tan, Adi, arXiv:2308.129502023arXiv preprint</p>
<p>Health system-scale language models are all-purpose prediction engines. Lavender Jiang, Xujin Yao, Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Nature. 2023</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Nature. 2023</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 3642020</p>
<p>SciBERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, ACL. 2019</p>
<p>Galactica: A large language model for science. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, 2022Preprint</p>
<p>Falcon-40B: an open large language model with state-of-the-art performance. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Technology Innovation Institute. 2023Technical report</p>
<p>A Bayesian approach to in silico blood-brain barrier penetration modeling. Ines Martins, Ana L Filipa, Luis Teixeira, Andre O Pinheiro, Falcao, Journal of Chemical Information and Modeling. 5262012</p>
<p>A data-driven approach to predicting successes and failures of clinical trials. Kaitlyn M Gayvert, S Neel, Olivier Madhukar, Elemento, Cell Chemical Biology. 23102016</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, . . Pande, V , Chemical Science. 922018</p>
<p>The SIDER database of drugs and side effects. Michael Kuhn, Ivica Letunic, Lars Juhl Jensen, Peer Bork, Nucleic Acids Research. 44D12016</p>
<p>Computational modeling of β-secretase 1 (BACE-1) inhibitors using ligand based approaches. Govindan Subramanian, Bharath Ramsundar, Vijay Pande, Rajiah Aldrin Denny, Journal of Chemical Information and Modeling. 5610252016</p>
<p>ESOL: estimating aqueous solubility directly from molecular structure. John S Delaney, Journal of Chemical Information and Computer Sciences. 4432004</p>
<p>FreeSolv: a database of experimental and calculated hydration free energies, with input files. David L Mobley, J Peter Guthrie, Journal of Computer-Aided Molecular Design. 282014</p>
<p>Quantum chemistry structures and properties of 134 kilo molecules. Ramakrishnan, Pavlo O Raghunathan, Matthias Dral, O Rupp, Von Anatole, Lilienfeld, Scientific Data. 112014</p>
<p>Strategies for Pre-training Graph Neural Networks. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec, International Conference on Learning Representations. 2019</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Advances in Neural Information Processing Systems. 332020</p>
<p>Molecular contrastive learning of representations via graph neural networks. Yuyang Wang, Jianren Wang, Zhonglin Cao, Amir Barati, Farimani , Nature Machine Intelligence. 432022</p>
<p>3d infomax improves gnns for molecular property prediction. Hannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, Pietro Liò, International Conference on Machine Learning. PMLR2022</p>
<p>Pre-training Molecular Graph Representation with 3D Geometry. Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, Jian Tang, International Conference on Learning Representations. 2021</p>
<p>Mole-bert: Rethinking pre-training graph neural networks for molecules. Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, Stan Z Li, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Random forests. Leo Breiman, Machine Learning. 452001</p>
<p>Extended-connectivity fingerprints. David Rogers, Mathew Hahn, Journal of Chemical Information and Modeling. 5052010</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, TMLR. 2022</p>
<p>Mann-Whitney U Test. Patrick E Mcknight, Julius Najab, The Corsini Encyclopedia of Psychology. 2010</p>
<p>Defining desirable central nervous system drug space through the alignment of molecular properties, in vitro adme, and safety attributes. T T Wager, ACS Chemical Neuroscience. 12010</p>
<p>Moving beyond rules: the development of a central nervous system multiparameter optimization (cns mpo) approach to enable alignment of druglike properties. T T Wager, X Hou, P R Verhoest, A Villalobos, ACS Chemical Neuroscience. 12010</p>
<p>Molecular determinants of blood-brain barrier permeation. W J Geldenhuys, A S Mohammad, C E Adkins, P R Lockman, Therapetuic. Delivery. 62015</p>
<p>In silico prediction of blood-brain barrier permeation using the calculated molecular cross-sectional area as main parameter. G Gerebtzoff, A Seelig, J. Chemical Information Modeling. 462006</p>
<p>Can ChatGPT be used to generate scientific hypotheses?. Yang Park, Daniel Jeong, Zhichu Kaplan, Chia-Wei Ren, Changhao Hsu, Haowei Li, Sipei Xu, Ju Li, Li, arXiv:2304.122082023arXiv preprint</p>
<p>Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. Shion Honda, Shoi Shi, Hiroki R Ueda, arXiv:1911.047382019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
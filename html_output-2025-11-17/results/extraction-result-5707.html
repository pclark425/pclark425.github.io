<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5707 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5707</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5707</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-261530384</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.01940v4.pdf" target="_blank">CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. Evaluating the programming capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream applications. In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs. Programming comprehension task tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning, and multi-hop reasoning. The code generation task evaluates LLMs through completing C++ functions based on provided descriptions and prototypes. The code correction task asks LLMs to fix real-world erroneous code segments with different error messages. We evaluate 12 widely used LLMs, including both general-purpose and specialized models. GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to human performance, there is still significant room for improvement in LLM programming. We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5707.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5707.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT_vs_AnswerOnly_ProgComp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought versus Answer-only prompting in Programming Comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of answer-only prompts (model outputs only final choice) versus chain-of-thought (CoT) prompts (model outputs reasoning then final choice) on a 250-question multiple-choice programming comprehension task across 0-/2-/5-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (e.g., GPT-4, GPT-3.5-turbo, ChatGLM2, EduChat, CodeT5 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Programming Comprehension (multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>250 multiple-choice exam questions (conceptual understanding, commonsense reasoning, multi-hop reasoning) drawn from university finals, evaluated under 0-shot/2-shot/5-shot; accuracy extracted from final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Answer-only: prompt asks model to output only the final choice (e.g., 'output only one option'). Chain-of-thought (CoT): prompt elicits step-by-step reasoning followed by final choice (e.g., 'Let's think step by step'), tested under 0/2/5-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Answer-only vs Chain-of-Thought (CoT); each evaluated in 0-shot, 2-shot, and 5-shot scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by model; GPT-4 achieves the highest absolute accuracy on this task (paper reports GPT-4 as best; aggregate programming-comprehension accuracy for top models cited in text), while many smaller models have accuracy <50% under answer-only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports that for most evaluated models CoT produced comparable or lower accuracy than answer-only; exception: GPT-3.5-turbo shows improved accuracy under CoT, and some models (ChatGLM2, EduChat, GPT-3.5-turbo) show noticeable CoT improvements specifically on multi-hop reasoning questions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (mostly no effect or reduced accuracy; some models improved, especially on multi-hop reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize two causes: (1) CoT emergent ability appears only at large model scale (~>=60B parameters); smaller models generate noisy/unstable CoT outputs that hurt final-answer extraction. (2) For many questions (conceptual and commonsense), multi-step reasoning is unnecessary so CoT provides no benefit; CoT does help multi-hop problems requiring chained reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Most models did not benefit from CoT and in many cases accuracy decreased; only models that have reached the 'CoT emergence' (e.g., GPT-3.5-turbo per paper) showed clear improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5707.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5707.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FuncContext_CodeGen_GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Function-with-context versus Function-only prompting for code generation (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative comparison showing that providing the calling-context/main-function (function-with-context) increases GPT-4's acceptance and AC@1 on C++ function-completion questions compared to function-only prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Generation (C++ function completion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>476 C++ programming questions where models must implement a target function; evaluated by compiling concatenated function with main() and running 5–10 testcases; metrics: AC@1, AC@all, AC Rate.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Function-only: only the function prototype and description are given; Function-with-context: the prompt additionally includes the main-function context and calling code (the calling code / examples that exercise the target function).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Function-only vs Function-with-context</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>English-table values reported: Function-only AC@1 = 0.5839, AC Rate = 0.5176; Function-with-context AC@1 = 0.7032, AC Rate = 0.5992.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Function-with-context improved GPT-4 AC@1 by +0.1193 (11.93 percentage points) and AC Rate by +0.0816 (8.16 percentage points) relative to function-only in the reported experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.1193 AC@1; +0.0816 AC Rate (function-with-context minus function-only for GPT-4, English table values).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing the calling context gives the model information about input/output conventions and how the function is used, which helps the model generate compilable and correctly interfacing function bodies; authors also report higher proportion of compilable outputs with context.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5707.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5707.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FuncContext_CodeGen_GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Function-with-context versus Function-only prompting for code generation (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same comparison as above for GPT-3.5-turbo; context yields modest improvements in acceptance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Generation (C++ function completion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>476 C++ function-completion questions evaluated by compilation and testcases; metrics AC@1 and AC Rate reported.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Function-only vs Function-with-context (as above).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Function-only vs Function-with-context</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>English-table values reported: Function-only AC@1 = 0.6134, AC Rate = 0.5105; Function-with-context AC@1 = 0.6597, AC Rate = 0.5606.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Function-with-context improved GPT-3.5-turbo AC@1 by +0.0463 and AC Rate by +0.0501 (absolute increases).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.0463 AC@1; +0.0501 AC Rate (function-with-context minus function-only for GPT-3.5-turbo, English table values).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Context helps align generated function with expected calling conventions and reduces extraneous output, improving compilability and test-pass rates.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5707.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5707.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explicit_vs_Narrative</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit-question versus Narrative-question format in code generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing that explicit, atomic programming prompts (directly stating required functionality) yield higher acceptance than narrative, scenario-based prompts that hide requirements in text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Examples: GPT-4, GPT-3.5-turbo (reported comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Generation (explicit vs narrative prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subset of code generation questions split into 'explicit' (specify function directly) and 'narrative' (scenario descriptions requiring extraction of program logic).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Explicit prompts: direct specification of algorithmic goal (e.g., 'implement quicksort'); Narrative prompts: scenario/description embedding tasks and requirements in text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Explicit vs Narrative</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>English-table examples: GPT-4 explicit AC@1 = 0.7276, AC Rate = 0.6025; GPT-4 narrative AC@1 = 0.6743, AC Rate = 0.5959. GPT-3.5-turbo explicit AC@1 = 0.7054 vs narrative AC@1 = 0.6193.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4: explicit > narrative by +0.0533 AC@1; GPT-3.5-turbo: explicit > narrative by +0.0861 AC@1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example effect sizes: GPT-4 AC@1 +0.0533; GPT-3.5 AC@1 +0.0861 (explicit minus narrative, English table values).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved for explicit prompts</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicit prompts remove the extra step of extracting program logic from prose, reducing failures; narrative prompts require deep NL understanding and mapping to program logic which is harder for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5707.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5707.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeCorr_PromptTypes_GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code-only vs Code-with-Error-Message vs Code-with-Standard-Code in Code Correction (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of three prompt styles for code correction (erroneous code only; code plus coarse error type; code plus standard reference code) showing small changes in performance and some counterintuitive effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Correction (fix student erroneous code)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>1330 erroneous code segments (WA/TLE/RE) drawn from student submissions; LLMs must correct code under three prompt scenarios; evaluated by AC@1, AC@all, AC Rate.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Code-only: task description + erroneous code. Code-with-Error-Message: same + one of {WA, TLE, RE}. Code-with-Standard-Code: same + a provided standard (correct) reference solution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Code-only vs Code-with-Error-Message vs Code-with-Standard-Code</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Chinese-table values for GPT-4: Code-only AC@1 = 0.6970, AC Rate = 0.6453; Code-with-Error-Message AC@1 = 0.7143, AC Rate = 0.6677; Code-with-Standard-Code AC@1 = 0.6038, AC Rate = 0.5269.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Adding the coarse error type produced a small absolute increase (AC@1 +0.0173, AC Rate +0.0224); providing standard code reduced AC@1 by -0.0932 and AC Rate by -0.1184 for GPT-4 in reported tables (likely because identical outputs to standard code are disallowed and penalized in scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.0173 AC@1 (error-message vs code-only); -0.0932 AC@1 (standard-code vs code-only) — Chinese table values for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>error-message: slight improvement; standard-code: reduced (in reported numeric scores, but authors note standard-code can help some models conceptually if outputs are not exact copies).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note the error-type labels provided by the online judge (WA/TLE/RE) are too coarse (no line numbers or stack traces), so they give limited debugging signal. Standard code can provide a helpful template, but evaluation forbids exact reproduction of the standard answer and thus may lower measured scores; some models do use standard code as guidance and improve when not penalized for copying.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Providing error types did not significantly help most models; in some cases code-with-standard-code led to lower measured performance due to evaluation checks (identical output to standard code is marked incorrect).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5707.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5707.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QuestionDescriptionOnly_BootOutput</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-Description-Only prompt format causing extraneous interactive outputs (example: GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Feeding only the textual problem description (no function template/context) sometimes causes LLMs to emit additional prompt-like I/O lines or 'boot' outputs that break automated test harnesses despite logically correct algorithmic code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (example shown)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Generation (Question-description-only)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt contains only the natural-language problem statement; model must generate full code. Evaluated by compiling and running tests.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Question-Description-Only / free-form code generation (no function signature or harness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared qualitatively to function-with-context and function-only templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative failure mode: generated code contained extra human-facing prompts (e.g., 'Enter the value of a:') or extraneous output; code logic was correct but testcases failed because extra output did not match expected I/O.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced reliability of automated evaluation (introduces false negatives)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>When no harness is provided, models often generate user-interaction print statements or explanatory text; these extra outputs cause mismatch with strict input/output testcases. Authors mitigated this by using GPT-4 to generate a stable main() harness and auxiliary functions (functional form) and by instructing models 'only return the function' to avoid extra text.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5707.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5707.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bilingual_Prompt_LangEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of prompt language / bilingual format (Chinese vs English) on LLM performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison across Chinese and English versions of all three tasks: programming comprehension, code generation, and code correction; shows task- and model-dependent language effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (Chinese- and English-trained models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Programming comprehension, Code generation, Code correction (bilingual versions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Each dataset is provided in aligned Chinese and English versions; models evaluated on both.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same problems presented in Chinese vs English (translated); prompts follow the same templates but differ in language.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Chinese-version prompts vs English-version prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Programming comprehension: Chinese version scored higher overall than English (authors cite two causes: (1) original questions were Chinese exam items so English translations contain wording artifacts; (2) many evaluated models are primarily trained on Chinese data). Code generation: authors report no noticeable difference between Chinese and English versions for most models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>language-dependent (Chinese > English for programming comprehension; no clear difference for code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Translation artifacts and training-data language bias cause decreased performance on the translated English programming comprehension items; models trained mainly on Chinese corpora underperform on English prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some English-trained models (e.g., CodeT5-based models, GPT-3.5-turbo) show similar performance across languages, indicating model pretraining language mix modulates the effect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5707.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5707.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FewShot_Format_Use</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of few-shot (0/2/5-shot) presentation in programming comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of zero-shot, 2-shot, and 5-shot in-context learning settings across programming comprehension multiple-choice questions; shot number used as a prompt-format variable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (full list in paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Programming Comprehension (multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>250 multiple-choice questions; models tested in 0-shot, 2-shot, and 5-shot settings; best results per setting reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>N-shot in-context learning: prompts include 0, 2, or 5 exemplar question-answer pairs before the test question; used for both answer-only and CoT styles.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>0-shot vs 2-shot vs 5-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-model effects vary; the paper reports detailed per-model per-shot tables. Some models benefit from few-shot exemplars while others see modest changes; GPT-family models often perform strongly across shots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>variable (task- and model-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot exemplars can help guide model output formatting and disambiguate expected answer style, but magnitude of improvement depends on model capability and match between exemplars and test distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5707.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5707.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT_MultiHop_Benefit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought specifically helps multi-hop reasoning questions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors observed that while CoT generally does not improve simple conceptual or single-step commonsense programming questions, it noticeably improves performance on multi-hop reasoning items for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM2, EduChat, GPT-3.5-turbo (examples cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Programming Comprehension (multi-hop reasoning subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subset of multiple-choice questions requiring multi-step inference over code semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chain-of-Thought prompting versus answer-only.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>CoT vs answer-only on multi-hop subset</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report a 'noticeable improvement' in accuracy for some models on multi-hop reasoning questions in CoT setting (specific model-level gains shown in appendix tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved for multi-hop items (for some models)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Multi-hop problems require stepwise decomposition which CoT elicits; large-enough models that can produce reliable internal chains of thought gain on these items.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>CoT did not improve and sometimes reduced accuracy on conceptual and commonsense single-step items for many evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Teaching Large Language Models to Self-Debug <em>(Rating: 2)</em></li>
                <li>DocPrompting: Generating code by retrieving the docs <em>(Rating: 1)</em></li>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5707",
    "paper_id": "paper-261530384",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "CoT_vs_AnswerOnly_ProgComp",
            "name_full": "Chain-of-Thought versus Answer-only prompting in Programming Comprehension",
            "brief_description": "Comparison of answer-only prompts (model outputs only final choice) versus chain-of-thought (CoT) prompts (model outputs reasoning then final choice) on a 250-question multiple-choice programming comprehension task across 0-/2-/5-shot settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (e.g., GPT-4, GPT-3.5-turbo, ChatGLM2, EduChat, CodeT5 variants)",
            "model_size": null,
            "task_name": "Programming Comprehension (multiple-choice)",
            "task_description": "250 multiple-choice exam questions (conceptual understanding, commonsense reasoning, multi-hop reasoning) drawn from university finals, evaluated under 0-shot/2-shot/5-shot; accuracy extracted from final answer.",
            "problem_format": "Answer-only: prompt asks model to output only the final choice (e.g., 'output only one option'). Chain-of-thought (CoT): prompt elicits step-by-step reasoning followed by final choice (e.g., 'Let's think step by step'), tested under 0/2/5-shot.",
            "comparison_format": "Answer-only vs Chain-of-Thought (CoT); each evaluated in 0-shot, 2-shot, and 5-shot scenarios.",
            "performance": "Varies by model; GPT-4 achieves the highest absolute accuracy on this task (paper reports GPT-4 as best; aggregate programming-comprehension accuracy for top models cited in text), while many smaller models have accuracy &lt;50% under answer-only.",
            "performance_comparison": "Paper reports that for most evaluated models CoT produced comparable or lower accuracy than answer-only; exception: GPT-3.5-turbo shows improved accuracy under CoT, and some models (ChatGLM2, EduChat, GPT-3.5-turbo) show noticeable CoT improvements specifically on multi-hop reasoning questions.",
            "format_effect_size": null,
            "format_effect_direction": "mixed (mostly no effect or reduced accuracy; some models improved, especially on multi-hop reasoning)",
            "explanation_or_hypothesis": "Authors hypothesize two causes: (1) CoT emergent ability appears only at large model scale (~&gt;=60B parameters); smaller models generate noisy/unstable CoT outputs that hurt final-answer extraction. (2) For many questions (conceptual and commonsense), multi-step reasoning is unnecessary so CoT provides no benefit; CoT does help multi-hop problems requiring chained reasoning.",
            "counterexample_or_null_result": "Most models did not benefit from CoT and in many cases accuracy decreased; only models that have reached the 'CoT emergence' (e.g., GPT-3.5-turbo per paper) showed clear improvement.",
            "uuid": "e5707.0",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "FuncContext_CodeGen_GPT4",
            "name_full": "Function-with-context versus Function-only prompting for code generation (GPT-4)",
            "brief_description": "Quantitative comparison showing that providing the calling-context/main-function (function-with-context) increases GPT-4's acceptance and AC@1 on C++ function-completion questions compared to function-only prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Code Generation (C++ function completion)",
            "task_description": "476 C++ programming questions where models must implement a target function; evaluated by compiling concatenated function with main() and running 5–10 testcases; metrics: AC@1, AC@all, AC Rate.",
            "problem_format": "Function-only: only the function prototype and description are given; Function-with-context: the prompt additionally includes the main-function context and calling code (the calling code / examples that exercise the target function).",
            "comparison_format": "Function-only vs Function-with-context",
            "performance": "English-table values reported: Function-only AC@1 = 0.5839, AC Rate = 0.5176; Function-with-context AC@1 = 0.7032, AC Rate = 0.5992.",
            "performance_comparison": "Function-with-context improved GPT-4 AC@1 by +0.1193 (11.93 percentage points) and AC Rate by +0.0816 (8.16 percentage points) relative to function-only in the reported experiment.",
            "format_effect_size": "+0.1193 AC@1; +0.0816 AC Rate (function-with-context minus function-only for GPT-4, English table values).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Providing the calling context gives the model information about input/output conventions and how the function is used, which helps the model generate compilable and correctly interfacing function bodies; authors also report higher proportion of compilable outputs with context.",
            "counterexample_or_null_result": null,
            "uuid": "e5707.1",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "FuncContext_CodeGen_GPT3.5",
            "name_full": "Function-with-context versus Function-only prompting for code generation (GPT-3.5-turbo)",
            "brief_description": "Same comparison as above for GPT-3.5-turbo; context yields modest improvements in acceptance metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "task_name": "Code Generation (C++ function completion)",
            "task_description": "476 C++ function-completion questions evaluated by compilation and testcases; metrics AC@1 and AC Rate reported.",
            "problem_format": "Function-only vs Function-with-context (as above).",
            "comparison_format": "Function-only vs Function-with-context",
            "performance": "English-table values reported: Function-only AC@1 = 0.6134, AC Rate = 0.5105; Function-with-context AC@1 = 0.6597, AC Rate = 0.5606.",
            "performance_comparison": "Function-with-context improved GPT-3.5-turbo AC@1 by +0.0463 and AC Rate by +0.0501 (absolute increases).",
            "format_effect_size": "+0.0463 AC@1; +0.0501 AC Rate (function-with-context minus function-only for GPT-3.5-turbo, English table values).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Context helps align generated function with expected calling conventions and reduces extraneous output, improving compilability and test-pass rates.",
            "counterexample_or_null_result": null,
            "uuid": "e5707.2",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Explicit_vs_Narrative",
            "name_full": "Explicit-question versus Narrative-question format in code generation",
            "brief_description": "Comparison showing that explicit, atomic programming prompts (directly stating required functionality) yield higher acceptance than narrative, scenario-based prompts that hide requirements in text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Examples: GPT-4, GPT-3.5-turbo (reported comparisons)",
            "model_size": null,
            "task_name": "Code Generation (explicit vs narrative prompts)",
            "task_description": "Subset of code generation questions split into 'explicit' (specify function directly) and 'narrative' (scenario descriptions requiring extraction of program logic).",
            "problem_format": "Explicit prompts: direct specification of algorithmic goal (e.g., 'implement quicksort'); Narrative prompts: scenario/description embedding tasks and requirements in text.",
            "comparison_format": "Explicit vs Narrative",
            "performance": "English-table examples: GPT-4 explicit AC@1 = 0.7276, AC Rate = 0.6025; GPT-4 narrative AC@1 = 0.6743, AC Rate = 0.5959. GPT-3.5-turbo explicit AC@1 = 0.7054 vs narrative AC@1 = 0.6193.",
            "performance_comparison": "GPT-4: explicit &gt; narrative by +0.0533 AC@1; GPT-3.5-turbo: explicit &gt; narrative by +0.0861 AC@1.",
            "format_effect_size": "Example effect sizes: GPT-4 AC@1 +0.0533; GPT-3.5 AC@1 +0.0861 (explicit minus narrative, English table values).",
            "format_effect_direction": "improved for explicit prompts",
            "explanation_or_hypothesis": "Explicit prompts remove the extra step of extracting program logic from prose, reducing failures; narrative prompts require deep NL understanding and mapping to program logic which is harder for LLMs.",
            "counterexample_or_null_result": null,
            "uuid": "e5707.3",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CodeCorr_PromptTypes_GPT4",
            "name_full": "Code-only vs Code-with-Error-Message vs Code-with-Standard-Code in Code Correction (GPT-4)",
            "brief_description": "Evaluation of three prompt styles for code correction (erroneous code only; code plus coarse error type; code plus standard reference code) showing small changes in performance and some counterintuitive effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Code Correction (fix student erroneous code)",
            "task_description": "1330 erroneous code segments (WA/TLE/RE) drawn from student submissions; LLMs must correct code under three prompt scenarios; evaluated by AC@1, AC@all, AC Rate.",
            "problem_format": "Code-only: task description + erroneous code. Code-with-Error-Message: same + one of {WA, TLE, RE}. Code-with-Standard-Code: same + a provided standard (correct) reference solution.",
            "comparison_format": "Code-only vs Code-with-Error-Message vs Code-with-Standard-Code",
            "performance": "Chinese-table values for GPT-4: Code-only AC@1 = 0.6970, AC Rate = 0.6453; Code-with-Error-Message AC@1 = 0.7143, AC Rate = 0.6677; Code-with-Standard-Code AC@1 = 0.6038, AC Rate = 0.5269.",
            "performance_comparison": "Adding the coarse error type produced a small absolute increase (AC@1 +0.0173, AC Rate +0.0224); providing standard code reduced AC@1 by -0.0932 and AC Rate by -0.1184 for GPT-4 in reported tables (likely because identical outputs to standard code are disallowed and penalized in scoring).",
            "format_effect_size": "+0.0173 AC@1 (error-message vs code-only); -0.0932 AC@1 (standard-code vs code-only) — Chinese table values for GPT-4.",
            "format_effect_direction": "error-message: slight improvement; standard-code: reduced (in reported numeric scores, but authors note standard-code can help some models conceptually if outputs are not exact copies).",
            "explanation_or_hypothesis": "Authors note the error-type labels provided by the online judge (WA/TLE/RE) are too coarse (no line numbers or stack traces), so they give limited debugging signal. Standard code can provide a helpful template, but evaluation forbids exact reproduction of the standard answer and thus may lower measured scores; some models do use standard code as guidance and improve when not penalized for copying.",
            "counterexample_or_null_result": "Providing error types did not significantly help most models; in some cases code-with-standard-code led to lower measured performance due to evaluation checks (identical output to standard code is marked incorrect).",
            "uuid": "e5707.4",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "QuestionDescriptionOnly_BootOutput",
            "name_full": "Question-Description-Only prompt format causing extraneous interactive outputs (example: GPT-3.5-turbo)",
            "brief_description": "Feeding only the textual problem description (no function template/context) sometimes causes LLMs to emit additional prompt-like I/O lines or 'boot' outputs that break automated test harnesses despite logically correct algorithmic code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (example shown)",
            "model_size": null,
            "task_name": "Code Generation (Question-description-only)",
            "task_description": "Prompt contains only the natural-language problem statement; model must generate full code. Evaluated by compiling and running tests.",
            "problem_format": "Question-Description-Only / free-form code generation (no function signature or harness).",
            "comparison_format": "Compared qualitatively to function-with-context and function-only templates.",
            "performance": "Qualitative failure mode: generated code contained extra human-facing prompts (e.g., 'Enter the value of a:') or extraneous output; code logic was correct but testcases failed because extra output did not match expected I/O.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced reliability of automated evaluation (introduces false negatives)",
            "explanation_or_hypothesis": "When no harness is provided, models often generate user-interaction print statements or explanatory text; these extra outputs cause mismatch with strict input/output testcases. Authors mitigated this by using GPT-4 to generate a stable main() harness and auxiliary functions (functional form) and by instructing models 'only return the function' to avoid extra text.",
            "counterexample_or_null_result": null,
            "uuid": "e5707.5",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Bilingual_Prompt_LangEffect",
            "name_full": "Effect of prompt language / bilingual format (Chinese vs English) on LLM performance",
            "brief_description": "Comparison across Chinese and English versions of all three tasks: programming comprehension, code generation, and code correction; shows task- and model-dependent language effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (Chinese- and English-trained models)",
            "model_size": null,
            "task_name": "Programming comprehension, Code generation, Code correction (bilingual versions)",
            "task_description": "Each dataset is provided in aligned Chinese and English versions; models evaluated on both.",
            "problem_format": "Same problems presented in Chinese vs English (translated); prompts follow the same templates but differ in language.",
            "comparison_format": "Chinese-version prompts vs English-version prompts",
            "performance": "Programming comprehension: Chinese version scored higher overall than English (authors cite two causes: (1) original questions were Chinese exam items so English translations contain wording artifacts; (2) many evaluated models are primarily trained on Chinese data). Code generation: authors report no noticeable difference between Chinese and English versions for most models.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "language-dependent (Chinese &gt; English for programming comprehension; no clear difference for code generation)",
            "explanation_or_hypothesis": "Translation artifacts and training-data language bias cause decreased performance on the translated English programming comprehension items; models trained mainly on Chinese corpora underperform on English prompts.",
            "counterexample_or_null_result": "Some English-trained models (e.g., CodeT5-based models, GPT-3.5-turbo) show similar performance across languages, indicating model pretraining language mix modulates the effect.",
            "uuid": "e5707.6",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "FewShot_Format_Use",
            "name_full": "Effect of few-shot (0/2/5-shot) presentation in programming comprehension",
            "brief_description": "Evaluation of zero-shot, 2-shot, and 5-shot in-context learning settings across programming comprehension multiple-choice questions; shot number used as a prompt-format variable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (full list in paper tables)",
            "model_size": null,
            "task_name": "Programming Comprehension (multiple-choice)",
            "task_description": "250 multiple-choice questions; models tested in 0-shot, 2-shot, and 5-shot settings; best results per setting reported in tables.",
            "problem_format": "N-shot in-context learning: prompts include 0, 2, or 5 exemplar question-answer pairs before the test question; used for both answer-only and CoT styles.",
            "comparison_format": "0-shot vs 2-shot vs 5-shot",
            "performance": "Per-model effects vary; the paper reports detailed per-model per-shot tables. Some models benefit from few-shot exemplars while others see modest changes; GPT-family models often perform strongly across shots.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "variable (task- and model-dependent)",
            "explanation_or_hypothesis": "Few-shot exemplars can help guide model output formatting and disambiguate expected answer style, but magnitude of improvement depends on model capability and match between exemplars and test distribution.",
            "counterexample_or_null_result": null,
            "uuid": "e5707.7",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoT_MultiHop_Benefit",
            "name_full": "Chain-of-Thought specifically helps multi-hop reasoning questions",
            "brief_description": "Authors observed that while CoT generally does not improve simple conceptual or single-step commonsense programming questions, it noticeably improves performance on multi-hop reasoning items for some models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGLM2, EduChat, GPT-3.5-turbo (examples cited)",
            "model_size": null,
            "task_name": "Programming Comprehension (multi-hop reasoning subset)",
            "task_description": "Subset of multiple-choice questions requiring multi-step inference over code semantics.",
            "problem_format": "Chain-of-Thought prompting versus answer-only.",
            "comparison_format": "CoT vs answer-only on multi-hop subset",
            "performance": "Authors report a 'noticeable improvement' in accuracy for some models on multi-hop reasoning questions in CoT setting (specific model-level gains shown in appendix tables).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved for multi-hop items (for some models)",
            "explanation_or_hypothesis": "Multi-hop problems require stepwise decomposition which CoT elicits; large-enough models that can produce reliable internal chains of thought gain on these items.",
            "counterexample_or_null_result": "CoT did not improve and sometimes reduced accuracy on conceptual and commonsense single-step items for many evaluated models.",
            "uuid": "e5707.8",
            "source_info": {
                "paper_title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Teaching Large Language Models to Self-Debug",
            "rating": 2,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "DocPrompting: Generating code by retrieving the docs",
            "rating": 1,
            "sanitized_title": "docprompting_generating_code_by_retrieving_the_docs"
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        }
    ],
    "cost": 0.02318075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models
11 Mar 2024</p>
<p>Yong Yu 
Lingyue Fu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#102;&#117;&#108;&#105;&#110;&#103;&#121;&#117;&#101;&#64;&#115;&#106;&#116;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#102;&#117;&#108;&#105;&#110;&#103;&#121;&#117;&#101;&#64;&#115;&#106;&#116;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> 
Huacan Chai 
Kounianhua Du 
Weiming Zhang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#110;&#122;&#104;&#97;&#110;&#103;&#64;&#115;&#106;&#116;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#119;&#110;&#122;&#104;&#97;&#110;&#103;&#64;&#115;&#106;&#116;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> 
Shuang Luo 
Jianghao Lin 
Yuchen Fang 
Renting Rui 
Hao Guan 
Jianxing Liu 
Siyuan Qi 
Longteng Fan 
Jiayi Lei 
Yifan Liu 
Jingkuan Wang 
Kangning Zhang 
Weinan Zhang 
C U C R M H R Total </p>
<p>Apex Data &amp; Knowledge Management Lab
Shanghai Jiao Tong University
China</p>
<p>Apex Data &amp; Knowledge Management Lab
Shanghai Jiao Tong University
Weinan Zhang; Yong YuShanghaiChina</p>
<p>2815 0.3535</p>
<p>2459 0, 3013 0.3667</p>
<p>CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models
11 Mar 202465DCCD8F228B0D032BA9B311AD3032BBarXiv:2309.01940v4[cs.CL]Large Language Model, Benchmark
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers.Evaluating the programming capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream applications.In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs.Programming comprehension task tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning, and multi-hop reasoning.The code generation task evaluates LLMs through completing C++ functions based on provided descriptions and prototypes.The code correction task asks LLMs to fix real-world erroneous code segments with different error messages.We evaluate 12 widely used LLMs, including both general-purpose and specialized models.GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of 69%, 54%, and 66% on the three tasks, respectively.Compared to human performance, there is still significant room for improvement in LLM programming.We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth.CCS Concepts: • Computing methodologies → Natural language processing.</p>
<p>INTRODUCTION</p>
<p>Due to the widespread adaption of the Transformer [51] architecture and advancements in computational power, Large Language Models (LLMs) have been widely employed in various tasks, 111:4 Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu is used as the initial model and further trained on specific code generation datasets.Codex [9], PaLMCoder [11], CodeGeeX2 [65] are further trained on GPT-3, PaLM [11], ChatGLM2 [19,61] with extensive public code datasets, demonstrating better comprehension and generation abilities on programming languages.With the development of instruction tuning techniques, a series of models, like WizardCoder [40], Code Llama-Instruct [47], PanguCoder [12] demonstrate powerful capabilities in multilingual code generation and debugging tasks.Another prevalent approach is prompt engineering, which involves designing suitable prompts or guiding statements to direct the model in completing specific code generation tasks.By providing insightful inputs, models can generate more accurate and expected code outputs.CodeGen [44] converts natural language questions into code generation tasks and improves generation effectiveness through appropriate prompt engineering techniques.DocPrompting [66] leverages keyword retrieval to integrate information from code repositories into prompt phrases, guiding the LLMs to enhance the performance of LLMs in code generation tasks.</p>
<p>Programming Evaluation</p>
<p>Evaluating large language models' (LLMs) capabilities can be effectively achieved through multichoice formats, where models are tasked with selecting the correct answer from given options.Multi-choice RACE transforms the original reading comprehension dataset RACE [32] into a set of questions with answer choices, facilitating the evaluation of the model's score.AI2 Reasoning Challenge (ARC) [13] serves as a benchmark to measure how well language models can reason and draw conclusions within the domain of science.C-Eval [27] provides multiple-choice questions from multiple subjects, enabling us to evaluate the competency of LLMs in different academic domains.CodeApex employs multiple choice question formats to evaluate LLM's programming comprehension ability.Another crucial dimension in assessing LLMs' capabilities involves their performance on coderelated tasks.The development of specialized benchmark datasets offers nuanced insights into LLMs' programming comprehension and generation skills.In the earlier days, FlashFill++ [8] provides a benchmark dataset for inferring string transformation programs from input-output examples.CodeSearchNet [28] serves code search tasks by offering a vast collection of code snippets for code retrieval and documentation generation.With the development of deep learning, NLP tasks have become more diverse.Benchmark datasets have started combining code-related tasks with natural language.CodeXGLUE [26] is a benchmark that encompasses natural language understanding of code, covering various tasks such as code summarization, code translation, code completion, and more.APPS [24] is a benchmark designed to evaluate program synthesis systems, with a focus on probabilistic programs and their ability to generalize to new tasks.With the emergence of LLMs, [9] proposed the HumanEval benchmark, specifically designed to assess the quality of generated code.HumanEval-X [65] benchmark introduces support for multiple programming languages, including C++, Python, Go, and Java, aiming to facilitate research in cross-language code generation and understanding.CoderEval [60] benchmark specifically focuses on evaluating large language models' abilities to tackle complex, industrial-level coding challenges through a series of practical programming tasks.DebugBench [50] implants bugs into standard code by GPT4 and evaluates five LLMs in a zero-shot scenario.CodeApex focuses on evaluating the reasoning capabilities of LLMs by presenting them with algorithmic challenges and associated test cases.</p>
<p>EVALUATION PROTOCOL</p>
<p>In this section, we introduce the evaluation framework of CodeApex for three code tasks: (1) programming comprehension, (2) code generation, and (3) code correction.The overall experiment scenarios of CodeApex are listed in Figure 1.• The first task is programming comprehension, whose test set includes 250 multiple-choice exam questions, which are categorized into conceptual understanding, commonsense reasoning, and multi-hop reasoning questions.The questions are selected from the final exams of different classes (Programming, Data Structure, Algorithm) at the university, which reduces the risk that the test data is already in the training corpus of LLMs.LLMs are asked to choose the correct option under the 0-shot, 2-shot, and 5-shot in-context learning scenarios.Due to the effective application of Chain-of-Thought (CoT), we also compare the performances of LLMs under the answer-only and CoT settings.• The second task is code generation, whose test set includes 476 C++-based coding questions, covering explicit questions and narrative questions.The description of the question and the prototype of the function to implement the question are given, and LLMs are asked to complete the main part of the function.We also provide both the function-only and functionwith-context scenarios, which indicate whether the calling code of the objective function is given in addition to the description of the objective function (i.e., the context information of the code).• The third task is code correction, whose test set includes 1330 erroneous code segments with three types of error messages (Wrong Answer, Time Limit Exceeded, and Runtime Error).We introduce three types of prompts to simulate code-only, code-with-error message, and code-with-standard code scenarios.</p>
<p>For each task, we first provide an overview and then discuss about the data processing, prompting strategy, and evaluation metric.To facilitate a fine-grained comparison of the differences in the 111:6</p>
<p>Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu</p>
<p>The storage space of the circular queue is an array A [21], where "front" points to the position before the front element, and "rear" points to the rear element.Assuming the current values of "front" and "rear" are 8 and 3 respectively, the length of the queue is ____.A. abilities of LLMs across different natural languages, we provide aligned Chinese and English versions for all test data in the three tasks.</p>
<p>Programming Comprehension Task</p>
<p>3.1.1Overview.In the programming comprehension task, LLMs have to answer multiple-choice questions that are closely related to code &amp; programming topics.This evaluates the basic capabilities of LLMs in understanding the code from various dimensions such as syntax, execution flow and code structure.The ability of programming comprehension lays the foundation for LLMs to accomplish other code-related tasks, and is therefore indispensable to thoroughly integrate LLMs into real-world code production scenarios.</p>
<p>Data</p>
<p>Processing.We evaluate the programming comprehension capability of LLMs via multiple-choice questions, which generally cover the key knowledge points of programming, data structure and algorithms.The multiple-choice question data is obtained from the final exams of real-world college courses under strict confidentiality, greatly decreasing the possibility of the test data overlapping with the training data of LLMs crawled online (i.e., test data leakage).Questions with multiple correct answers are manually modified into single-choice questions to facilitate testing consistency.Similar to C-Eval [27], we format the options of the multiple-choice questions for alignment.The question prompts are provided in Markdown format, i.e., code blocks are marked using triple quotes.All questions are manually cross-validated to ensure the unambiguity of the question descriptions and the correctness of the answers.</p>
<p>In order to facilitate further exploration of the fine-grained abilities of LLMs, we manually classify the questions into three categories: (1) conceptual understanding, (2) commonsense reasoning, and (3) multi-hop reasoning.Example questions of three categories are demonstrated in Figure 2. Conceptual understanding questions assess LLMs' basic understanding of programming concepts, typically involving selecting the correct concept or natural language description without requiring specific code writing or comprehension.Commonsense Reasoning questions primarily evaluate LLMs' ability to read and understand code.These two types of questions focus on a single knowledge point, with relatively simple problem-solving steps that can be deduced through single-step reasoning.Multi-hop reasoning questions evaluate LLMs' ability to reason about code.These questions often involve chains of thought and require multiple steps of inference to get the correct answer.These three categories of questions correspond to LLMs' elementary grasp of programming concepts, basic understanding of code, and advanced comprehension of code, respectively.The categorized results could provide us with deeper insights into the fine-grained levels of programming comprehension capabilities of LLMs.(b) Answer-only setting (English).3.1.3Prompting Strategy.We evaluate the code comprehension abilities of LLMs under two different prompting strategies, i.e., answer-only and chain-of-thought.In the answer-only setting, LLMs are required to generate only a single option (i.e., the selected choice), and the correctness of the generated option contributes to the final accuracy score.In the chain-of-thought setting, LLMs would first generate a piece of analytical text followed by a final answer, and the accuracy of the final answer serves as the measure of model performance.We illustrate the prompt examples under both settings in Figure 3.</p>
<p>Chain-of-Thought Scenario (Chinese Version)
以下是一次编程考试的单项选择题，请选出其中的正确答案。 若森林F有15条边、25个结点，则F包含树的个数是____ A. 8 B. 9 C. 10 D. 11 答案：让我们一步一步思考。 1，15条边、25个结点，即有25个点构成15条边，且25个点只能 围成有限的森林 2，因此求森林中树的个数等价于求这25个点可以组成多少个孤 立的组，即有多少颗独立的树。 3，森林中有m颗树，则结点数量为25-m，边数为15-m，从而可 得：25 -m = 2 × (15 -m),即 m = 10，因此森林F包含10棵树，答 案为C。 因此这道题的正确答案是C。 …[x-shots] 若有定义：```char str[] = {'h','1','2','0','a',</p>
<p>Evaluation Metric.</p>
<p>We adopt the accuracy (ACC) as the evaluation metric for the programming comprehension task.After LLMs provide a response under either answer-only or chain-ofthought settings, we extract the final choice using regular expressions.If the choice cannot be extracted via regular expressions, it is regarded as an incorrect answer.Most LLMs are unable to generate code that meets the given input-output conditions based solely on question descriptions.When we directly input the original question description into LLMs, it is highly likely that the generated code will contain additional prompt-related output.Although the logic of the code itself may be correct, these extra outputs are not included in the original test cases, causing the generated code to fail the tests.Figure 5 illustrates an example of this situation, which occurs during code generation with GPT-3.5-turbo.In such cases, in order to more accurately evaluate code generation capability of LLMs itself, we utilize GPT-4 to assist in generating functional form test examples.With an in-context learning approach, we guide the LLMs in rewritting the template code and dividing it into a main function and some auxiliary functions.The main function includes (i) library references, (ii) the main function itself (with input and output code), and (iii) calls to auxiliary functions.The auxiliary functions contain the main algorithmic logic with input parameters and return the output to the main function.</p>
<p>To further explore the different types of code generation abilities of LLMs, we divide the questions into two categories: explicit questions and narrative questions.Explicit questions directly specify the functionality that the program needs to implement, such as "Implement a quicksort" or "Calculate the number of characters in the input." Narrative questions consist of more complex natural language descriptions, with the code requirements hidden within a scenario.For example, given a class of students, you need to inquire about the grade of the k-th student.This question requires storing the grades of the students in an array, sorting them, and then outputting the k-th largest element.Previous benchmarks [9] focus on explicit questions, and do not pay attention to the importance of evaluating narrative questions.</p>
<p>Prompting Strategy.</p>
<p>We describe our approach for generating target functions that adhere to function templates by designing prompts.These prompts consist of three components: question description, function description, and data range specification.In the 'function-only' scenario, no additional information is included in the prompts.In this case, LLMs are asked to generate a runnable function code that conforms to the function prototype and the question description.In the function-with-context scenario, we incorporate the context of the main function as an additional (a) Function-only scenario (Chinese).</p>
<p>Function-only Scenario (English)</p>
<p>I will provide you with a problem that needs to be solved using C++ programming, along with a C++ function declaration.Your task is to generate the corresponding C++ function based on this function declaration to solve the given problem.You only need to return the C++ function, without any additional explanatory text.Problem: input integers a and bm output the value of a+b.Function Declaration: int sum_a_b(int a, int b);</p>
<p>(b) Function-only scenario (English).</p>
<p>Function-with-context Scenario (Chinese)   input to guide the LLM to generate the target function.The specific design of the prompts is illustrated in Figure 6.
我会给你一个用于解决某个问题但缺少了函数定义的C++代 码，这个缺失函数定义的函数声明，这个C++代码解决的问题 及输入输出示例，你的任务是用这个函数声明生成函数定义去 填补所给的C++代码缺少的部分，并满足时间限制和空间限 制。你只需要返回C++代码的函数定义,不要返回额外的说明文 字。 问题：输入两个整数a，b，输出a+b的值。 函数声明：int sum_a_b(int</p>
<p>Evaluation Metric.</p>
<p>To evaluate the effectiveness of the code, we use the original question's test cases to measure its correctness.Each question provides 5-10 test cases that cover various possible input scenarios and edge conditions.CodeApex benchmark aligns with real-world scenarios of human programming by using the number of passed test cases as the model's score.All test case scores are equally weighted, regardless of their difficulty.</p>
<p>CodeApex introduces an evaluation metric called accept rate as the evaluation function for the generated code by the model.When the language model generates a response, we extract the runnable C++ code function from it and concatenate it into the main function in the dataset.The concatenated code remains executable, and the target input-output pairs are aligned with the test cases in the original dataset.By comparing the output of the test cases with the actual output, we can get the number of the test cases the generated code passes, i.e., #passes().We calculate the @, which represents the quality of the model-generated code
𝐴𝐶@𝑘 = 1 |𝑄 | ∑︁ 𝑞 ∈𝑄 [I(#passes(𝑞) ≥ 𝑘)] ,
where  is the question set, and the indicator function I presents
I(condition) = 1, if condition is true, 0, otherwise.
When  is set to 1, the code can pass at least one test case.@1 reflects the functional correctness of the model-generated code.In this case, some test cases may not pass, which may result from inappropriate algorithm selection in the generation process or exceeding the time/space limits of the question itself.When  is set to , @ represents the proportion of questions that have passed all test cases.This is a relatively strict metric for evaluating performance.In addition, we introduce the Accept Rate (AC Rate) metric to measure how well the generated code meets.The AC Rate is calculated as follows:
AC Rate = 1 |𝑄 | ∑︁ 𝑞 ∈𝑄</p>
<h1>passes(𝑞) # testcases(𝑞)</h1>
<p>.</p>
<p>It reflects the overall performance of LLMs in the code generation task, including both algorithm selection and implementation.</p>
<p>Code Correction Task</p>
<p>Overview.</p>
<p>In code correction task, LLMs need to revise the generated code with the question description, error message and standard code.Code correction is an essential stage in the programming procedure, the mastering of which could offer practical assistance in software engineering.In addition, utilizing the self-inspiration ability of LLMs to correct codes has emerged as a principle way for improving the accuracy of code generation [10].Therefore, we include the evaluation for code correction, which is a fundamental ability of programming by LLMs.</p>
<p>Data</p>
<p>Processing.The test data is originated from real students' submission of exercises on the internal Online Judge platform.Each submission contains the exercise ID, submission code, error type, and submission time.The types of error include: Wrong Answer (WA), Time Limit Exceeded (TLE), and Runtime Error (RE).If the submission answers all testcases correctly, then the error type is empty.</p>
<p>To ensure that erroneous code can be corrected within a certain number of steps rather than rewritten, we sample examples from submissions of the same users, where both the incorrect version and the correct version of the same problem exist.The incorrect version is then sampled to be the test sample, where we restrict the edit distance [33] between the incorrect version and the correct version to be less or equal to 50.From the generated code pairs, we select code pairs for each error type.Due to varying frequencies of difference error types, the resulting error codes primarily consist of WA error, along with a portion of other error types.We also ensure that the number of code pairs for each question is roughly equal.</p>
<p>Prompting Strategy.</p>
<p>We design three types of prompts for simulating different code correction scenarios.The first type of prompt, Code-only, contains only the erroneous code and the question description without any additional information.This prompt is used to evaluate code correction capability of LLMs, simulating humans directly reading code and identifying errors.The second type of prompt, Code-with-Error Message, includes the erroneous code, task description, and the  type of error.This prompt simulates engineers debugging their code based on actual feedback from the compiler, aligning with real-world development scenarios.The third type of prompt is called Code-with-Standard Code.It builds upon the Code-with-Error Message prompt by adding standard code as additional input.In the development process, engineers can refer to existing code for correction.Evaluating code correction ability of LLMs in this scenario can assess its comprehensive programming capabilities, providing a guidance for LLM-assisted programming.The specific design of three types of prompts is demonstrated in Figure 7.</p>
<p>Evaluation Metric.</p>
<p>Similar to code generation, we use the same test cases and same metrics (AC@1, AC@all, AC Rate) to measure the correctness of modified code.LLMs should realize code correction based on the input erroneous code.If the difference between the input and output code is too large (edit distance greater than 50), the output code is considered invalid.The score for invalid code is recorded as 0, regardless of whether it passes the test cases.In code-with-standard code scenario, in order to prevent the LLM from directly outputting standard code, we validate the generated code in the Code-with-Standard Code scenario.Any output code that is identical to the standard code is directly marked as an incorrect answer.</p>
<p>EVALUATION RESULTS</p>
<p>Experimental Setup</p>
<p>Dataset Statistics.</p>
<p>In the programming comprehension task, CodeApex releases a total of 250 multiple-choice questions, which are divided into three categories: conceptual understanding (90 questions), commonsense reasoning (99 questions), and multi-hop reasoning (61 questions).In the code generation task, CodeApex releases 476 algorithm questions in total.These questions are divided into 258 functional questions and 218 descriptive questions.In the code correction task, CodeApex releases 1330 segments of erroneous code, corresponding to solving 116 questions in the code generation task.Each erroneous code has an error message, with a total of three types of error messages: Wrong Answer (1175 code segments), Time Limit Exceeded (69 code segments) and Runtime Error (86 code segments).</p>
<p>The test data is available on GitHub1 for users to download and run.The standard answers are not publicly disclosed to ensure the appropriate usage of CodeApex, as its data might unintentionally include web-crawled content.Users are required to submit their model predictions to the provided webpage 2 to obtain an automated accuracy assessment, and a public leaderboard is maintained on the webpage.Users can choose whether to publish their models' scores on the leaderboard.2. The variants of the models encompass the chatbot mode and the completion mode, as well as different parameter sizes and versions within the same base model.In the programming comprehension task, we conduct experiments on 10 LLMs, including both open-source models and API-based models.In the code generation task, we evaluate 11 accessible LLMs, including both general-purpose and code-specialized models.In the code correction task, due to the inability of some LLMs to output recognizable code, our experiments showcase the results of 7 models.The largest open-source models have 13 billion parameters, while the largest parameterized model based on API is GPT4.</p>
<p>Parameter Settings.When LLMs generate responses, we choose two distinct temperatures to cater to different application scenarios.For the programming comprehension task, we set tempera-ture=0.01 to generate more conservative results, preventing the model from producing responses</p>
<p>Model</p>
<p>Model Size Form Creator ChatGLM [38] 6B open, general-purpose ChatGLM2 [38] 6B open, general-purpose ChatGLM3 [38] 6B open, general-purpose</p>
<p>Tsinghua &amp; Zhipu MOSS [48] 16B open, general-purpose Fudan University Chinese-Alpaca [15] 7B open, general-purpose Cui et al. [15] BELLE [29,30] 7B open, general-purpose Beike BELLE-LLaMA [29,30] 7B, 13B open, general-purpose Beike InternLM-Chat [49] 7B open, general-purpose Shanghai AI Lab &amp; Sense Time Baichuan [5] 7B open, general-purpose Baichuan Inc. EduChat-base [16] 7B, 13B open, Edu-Use EduChat-sft [16] 7B open, Edu-Use East China Normal University CodeT5-plus [53] 6B, 16B open, code-specialized Salesforce AI WizardCoder [40] 15B open, code-specialized Microsoft StarCoder [34] undisclosed api, code-specialized BigCode Vicuna [64] 13B open, general-purpose LM-SYS GPT-3.5-turbo[63] undisclosed api, general-purpose GPT-4 undisclosed api, general-purpose OpenAI that do not align with the desired answers.For the code generation and code correction task, we opted for a temperature of 0.7 and a top-p value of 0.8 to encourage the model to generate a broader range of possible codes, introducing more randomness in each code generation instance.</p>
<p>Programming Comprehension Results</p>
<p>The overall performance of the programming comprehension task in the answer-only and CoT scenarios are presented in Table 3 and Table 4.The tables display the best results achieved in the 0-shot, 2-shot, and 5-shot settings.Detailed results for the N-shot ( = 0, 2, 5) settings are provided in the Appendix A. GPT4 obtains the highest average accuracy among all the models, followed by GPT3.5-turbo.The third best model in Answer-only scenario is InternLM-Chat-7B, which has an accuracy of 37%, which still have a huge gap between GPT.It is noteworthy that the accuracy of most LLMs is below 50%, highlighting that CodeApex Benchmark is challenging in the programming comprehension task.</p>
<p>Bilingual Accuracy Comparison.The Chinese version scored higher than the English version.There are two main reasons: (1) The source question descriptions are obtained from final exams in Chinese universities, and thus the exam questions are initially presented in Chinese.Even after translating them into English version, they still contain some language habits specific to Chinese speakers.As a result, when inputting these English questions with biases into the LLM, some noise might be introduced into the model's encoding results.(2) Most evaluated models are primarily trained on Chinese data, which leads to a poor understanding of English.In Table 3 and 4, LLM trained primarily on English corpus, such as codeT5 and GPT3.5-turbo, tend to have approximate performance on Chinese and English versions.</p>
<p>Accuracy Comparison on Different Question Categories.Among the three question categories, approximately half of the models perform best in conceptual comprehension, followed by commonsense reasoning, with the lowest accuracy on multi-hop reasoning.This result indicates that LLMs are likely to incorporate knowledge of programming concepts during training.Most models scored higher in commonsense reasoning compared to multi-hop reasoning, suggesting that the capability of LLMs significantly decreases with an increase in the number of reasoning steps.</p>
<p>Effects of Chain-of-Thought Prompting.Most of the models achieve approximate or lower accuracy than the answer-only setting.The accuracy results of the CoT setting are depicted in Table 4.The reasons for this observation are two folds: (1) Models we evaluate do not reach model sizes that have the emergent ability of CoT.According to [54], the emergence of CoT requires LLM to have at least 60B parameters.When the parameter number is not enough, the CoT setting might introduce additional noise, and the generated response of LLM would be unstable.That's why GPT3.5-turbo, which has reached the emergence point, achieves higher accuracy in the CoT setting.(2) When answering conceptual understanding and commonsense reasoning questions, we do not require multi-step reasoning.Thus, the CoT ability of LLMs does not provide assistance for these categories of questions.However, for multi-hop reasoning questions, there is a noticeable improvement in accuracy in the CoT scenario for some models (such as ChatGLM2, educhat, and GPT3.5-turbo).Since CodeT5 fails to generate a response with chain-of-thought, we exclude it in the CoT setting.</p>
<p>Human Performance.Novice programmers perform similarly to GPT-4 in closed-book tests after learning, while human performance in open-book exams is significantly better than all LLMs.The performance of human testers is shown in Table 3.Note that programming comprehension tasks in CodeApex is considered a semi-open-book exam for LLMs, i.e., they have limited offline knowledge base.</p>
<p>Code Generation Results</p>
<p>Code generation task results of all the models are shown in Table 5 and Table 6.Two prompt strategies (function-only and function-with-context) are employed for each language version.The evaluation metrics include AC@1, AC@all, and AC Rate.GPT outperforms the other LLMs, with the best accepted rate over 66% (GPT4).WizardCoder and StarCoder ranked second and third, highlighting the significant improvement in code generation capability through code-based finetuning.There is no noticeable performance difference between the Chinese and English versions.</p>
<p>Effects of Contexts in Prompt.</p>
<p>As shown in Table 5 and Table 6, providing the context of function calls for LLM can effectively enhance the accuracy of generating target function code.Meanwhile, Table 7 shows the proportion of compilable code in each scenario.The majority of models are capable of generating over 50% of compilable code, which demonstrates the ability of LLM to adhere to the function prototype.After concatenating the generated function and the main function, the code that could be compiled can be checked by testcases.Generally, providing contextual information about the function can assist LLMs in generating compilable code.</p>
<p>Question Category Comparison.Table 8 shows the performance on explicit and narrative questions.The data is selected from function-only and function-with-context scenarios, with the betterperforming data chosen.The table demonstrates that LLMs perform better on explicit questions than narrative questions.This is because explicit questions only require atomic code generation ability of LLMs, while narrative questions require LLM to first understand the question description, convert natural language into program logic, and finally generate code.It is indeed a significant challenge to extract program logic from natural language, requiring a deep understanding of language semantics and context.Meanwhile, three metrics of LLM on narrative questions are mostly consistent, indicating that the generated code is either entirely correct or entirely incorrect.This may be due to the fact that narrative questions in the dataset are designed for real-world problem-solving, with relatively low requirements for code time and space complexity.</p>
<p>Human Performance.We extract real student learning records from online platforms to evaluate human performance in code generation tasks.We calculated the pass rates for human submissions in n times (n=1, 3, 5), and the average results are shown in the Table 5, Table 6 and Table 8.Experimental results suggest that human performance can outperform all LLMs, and they have a better performance on narrative questions.This is mainly due to the ease with which human programmers can translate between natural language and code languages.</p>
<p>Code Correction Results</p>
<p>The overall performance of LLMs in the code correction task is shown in Table 9 and 10.There is a significant difference in debugging ability among different models.GPT-4 performs the best among all LLMs with the best accuracy of 66%, followed by GPT3.5-turbo,StarCoder and WizardCoder.Baichuan and ChatGLM3 could correct over 25% erroneous code, which is equipped with code correction capability.Other models perform poorly in code correction tasks, with a success rate of less than 20%.The code correction capability of LLMs still has a lot of room for improvement.</p>
<p>Feedback Analysis.Providing error types does not significantly help LLMs in the code correction task.This may be because the online judge platform only provides three types of error without specific line numbers or other detailed error messages, which is not sufficient to assist LLMs in code correction.LLMs that are unable to complete code correction tasks can   the standard code serves as a reference point or template, guiding the model to make corrections based on the provided code structure and logic.</p>
<p>Error Type Analysis.We compare the performance on three types of error (WA, TLE, and RE) in Table 11 and Table 12.Results shows correcting these three types of errors has different emergence ability of LLMs.Correcting wrong answer codes is the hardest task for LLMs since errors might occur everywhere.TLE usually occurs in closed loops or too many loops, which is relatively easy CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:21 Table 10.Performance on AC@1, AC@all, and AC Rate metrics of code generation task in English version.The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined.</p>
<p>Code Correction (English) Code-only</p>
<p>Code-with-Error Message Code-with-Standard Code Model AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate ChatGLM-6B  11.Best performance comparison between correcting three types of code error (WA, TLE, RE) on AC@1, AC@all and AC Rate metric of code correction task in Chinese version.The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined.</p>
<p>Models</p>
<p>Code Correction (Chinese) WA TLE RE AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate ChatGLM-6B to locate.RE is also easy to locate, usually because the divisor is 0, the subscript is out of bounds, and so on.Thus when parameter size is limited (StarCoder, GPT3.5),RE and TLE are easy to find, while the ability to find WA error emergents until the parameter size of GPT4.</p>
<p>CONCLUSION</p>
<p>We present CodeApex, a bilingual benchmark focusing on the programming capabilities of LLMs.First, we evaluate LLMs in the programming comprehension task by assessing their accuracy in three categories of multiple-choice questions.We design two types of prompts, answer-only and chain-of-thought, with three in-context learning scenarios, to compare LLMs' performance.Second, we evaluate LLMs' code generation capabilities by testing the acceptance rate of test cases.The test set includes 476 C++-based coding questions, categorized as either explicit questions or narrative questions.We also provide a comparison on function-only and function-with-context scenarios, in order to compare the impact of context information of the target code.Third, we introduce 1332 real-world erroneous code segments to evaluate LLMs' code correction capability.We compare 111:22</p>
<p>Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu Table 12.Best performance comparison between correcting three types of code error (WA, TLE, RE) on AC@1, AC@all and AC Rate metric of code correction task in English version.The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined.</p>
<p>Models</p>
<p>Code Correction (English) WA TLE RE AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate ChatGLM-6B the performance in code-only, code-with-error message, and code-with-standard code scenarios, and analysis the result of different code error types.We organize experiments on 12 famous LLMs, including general-purpose LLMs and specialized models based on code fine-tuning.Currently, GPT achieves top-tier performance in programming abilities, but human tests indicate that there is still significant room for improvement.Furthermore, specialized LLMs demonstrate competitiveness and show their potential for further enhancement.We also incorporate human performance into the leaderboard, comparing the coding abilities of the CodeApex with human-level performance.The experiments indicate that the programming capabilities of LLMs still have some distance from human performance.In the future, the test set within CodeApex will undergo continuous updates, encompassing an expanded range of question types and additional programming languages.We hope that CodeApex could provide a reference for later study on the programming capabilities of LLMs, fostering their development and prosperity together.
- - - - - - - - - - - - CodeT5-plus-16B<em> - - - - - - - - - - - - CodeT5-plus-6B - - - - - - - - - - - - CodeT5-plus-6B</em> - - - - - - - - - - - - GPT-3.
Fig. 1 .
1
Fig. 1.Overview diagram of CodeApex benchmark.</p>
<p>'b'};const char <em>p = str;<code>下列语句正确的是：____。 A. p[2] = 's' B. strcpy(str,"123456") C. strcpy(p,"abc") D. strcpy(str,"abc") Chain-of-Thought setting (Chinese).The following is a single choice question about a programming exam, please select the correct answer among them.If forest F has 15 edges and 25 nodes, the number of trees contained in F is ____.'s think step by step. 1, 15 edges and 25 nodes, that is, 25 points form 15 edges and 25 points can only form a limited forest.2, … 3, …[explanations to the example question] Therefore, the correct answer to this question is C. …[x-shots] If there is a definition:</code>char str[] = {'h','1','2','0','a','b'}; const char </em>p = str;```.Which of the following statements is correct:____.A. p[2] = 's' B. strcpy(str,"123456") C. strcpy(p,"abc") D. strcpy(str,"abc") Answer: Let's think step by step. 1, …… 2, …… Therefore, the correct answer to this question is D. Chain-of-Thought Scenario (English Version) (d) Chain-of-Thought setting (English).</p>
<p>Fig. 3 .
3
Fig. 3. Examples illustrating the programming comprehension task in answer-only and chain-of-thought scenarios, presented in both English and Chinese versions.The completed responses from LLMs are highlighted in red, while the input prompts are shown in black text.</p>
<p>Fig. 5 .
5
Fig. 5. Illustration of the Question-Description-Only mode, where only the code question description is fed into LLMs for generation.The generated code by GPT3.5-turbo is demonstrated below.The code contains additional boot outputs, causing this logically correct code to fail the testcases.</p>
<p>a, int b); 代码： #include <iostream> //function start //function end int main() { int a, b; std::cin &gt;&gt; a &gt;&gt; b; //calling start int sum = sum_a_b(int a, int b); //calling end std::cout &lt;&lt; sum &lt;&lt; std::endl; return 0; } 输入输出示例： 输入： <code>1 2</code>，输出：<code>`3</code>时 间限制：1000 ms 空间限制：1 MB (c) Function-with-Context scenario (Chinese).</p>
<p>Function</p>
<p>-with-context Scenario (English) I will give you a C++ code that solves a problem but is missing a function definition, a function declaration that is missing a function definition, a problem that the C++ code solves and an input/output example.Your task is to use this function declaration to generate a function definition that fills in the missing parts of the given C++ code and satisfies the time and space constraints.You only need to return the C++ function, without any additional explanatory text.Problem: input integers a and bm output the value of a+b.Function Declaration: int sum_a_b(int a, int b); Code: #include <iostream> //function start //function end int main() { int a, b; std::cin &gt;&gt; a &gt;&gt; b; //calling start int sum = sum_a_b(int a, int b); //calling end std::cout &lt;&lt; sum &lt;&lt; std::endl; return 0; } Input-Output Examples: Input: <code>1 2</code>, Output: <code>`3</code>T ime Limit: 1000 ms Space Limit: 1 MB (d) Function-with-Context scenario (English).</p>
<p>Fig. 6 .
6
Fig. 6.Examples of specific design of the prompts for code generation task.The model generates the text highlighted in yellow.</p>
<p>Fig. 7 .
7
Fig. 7. Examples of specific design of the prompts for code correction task.</p>
<ol>
<li>1 . 2
12
LLMs to Evaluate.CodeApex totally evaluates 12 LLMs, comprising a total of 41 variants.The tested LLMs and their variants are listed in Table</li>
</ol>
<p>CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:17</p>
<p>CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:19</p>
<p>Examples of the three categories of questions in the programming comprehension task.
Conceptual UnderstandingCommonsense ReasoningMulti-hop ReasoningIf the most frequently used operations for aIf there are definitions: char *p =linear table are to access the value of the i-th"h12\0ab", str[] = "h12\0ab"; , the valueselement and its predecessor and successor,of sizeof(p), sizeof(str), and strlen(p)in order to improve efficiency, the <strong><em>_are</em></strong>_.storage method should be adopted.A. 4，4，3A. Singly linked listB. 7，7，7B. Doubly linked list5C. 4，8，7C. Circular linked listB. 6D. 4，7，3D. Sequential listC. 16D. 17Fig. 2.</p>
<p>only Scenario (English Version)
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models111:7Answer-only Scenario (Chinese Version)Answer-以下是一次编程考试的单项选择题，请选出其中的正确答案。The following is a single choice question about a programming若森林F有15条边、25个结点，则F包含树的个数是<strong><em>_exam, please select the correct answer among them, please outputA. 8only one answer.B. 9If forest F has 15 edges and 25 nodes, the number of trees containedC. 10in F is </em></strong><em>.D. 11A. 8答案：CB. 9…[x-shots]C. 10若有定义：<code>char str[] = {'h','1','2','0','a','b'};const char *p =D. 11str;</code> 下列语句正确的是：<strong><em>_。Answer: CA. p[2] = 's'…[x-shots]B. strcpy(str,"123456")If there is a definition: <code>char str[] = {'h','1','2','0','a','b'}; constC. strcpy(p,"abc")char *p = str;</code>. Which of the following statements is correct:</em></strong></em>.D. strcpy(str,"abc")A. p[2] = 's'答案:DB. strcpy(str,"123456")C. strcpy(p,"abc")D. strcpy(str,"abc")Answer: D(a) Answer-only setting (Chinese).</p>
<p>Fig. 4. Illustration of testcases of different difficulty levels.If the complexity of the code is  ( 2 ), it could pass at least the first six testcases.If the tester uses the prefix sum algorithms with  () complexity, he can pass all test points.
Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, HaoGuan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,111:8and Yong YuProblem: Given an integer array nums whose size is n and a targetinteger, write a code to determine if there exists a contiguoussubarray in nums that sums up to the target. If such a subarray exists,return True; otherwise, return False. The first number of input is n,and then input n numbers consecutively to represent array[1] toarray[n]. Here 1 ≤ # ≤ 1$7.Example input: <code>3 1 2 3 5``Èxample output:</code>True``Testcase 1: 3 1 2 3 5…Testcase 5: 1000 … …Testcase 6: 10000 … ……Testcase 9: 10000000 … …Testcase 10: 10000000 … …</p>
<p>Code Generation (Question-Description-Only)
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models111:9Prompt: I will provide you with a specific question description. Yourtask is to generate the code that accomplishes this target task. Youonly need to provide the C++ code; please do not include additionalexplanatory text.Problem: input integer a and b, output the value of a+b.Time Limit: 1000 msSpace Limit: 1 MBGPT3.5-turbo:#include <iostream>int main() {int a, b;std::cout &lt;&lt; "Enter the value of a: ";std::cin &gt;&gt; a;std::cout &lt;&lt; "Enter the value of b: ";std::cin &gt;&gt; b;int sum = a + b;std::cout &lt;&lt; "The sum of a and b is: " &lt;&lt;sum &lt;&lt; std::endl;return 0;}The final accuracy of theLLM is calculated as:ACC =#Correct Answers #Questions.3.2 Code Generation Task3.2.1 Overview. In the code generation task, LLMs have to extract relevant algorithm requirementsfrom natural language descriptions, and then, in conjunction with a given code function framework,generate executable and accurate codes to pass the test cases. Such a task demands that largelanguage models possess the comprehension ability for both natural languages and code structures,as well as the capability to analyze, design, and ultimately generate functional code snippets.3.2.2 Data Processing. The test data is sourced from basic exercises on an internal Online Judgeplatform, which provides question descriptions, sample inputs/outputs, code templates, and testcases of varying difficulty levels. The question descriptions are typically provided in naturallanguage, some of which also contain scenario-based descriptions. The sample inputs/outputsillustrate the format of the test case inputs and outputs, represented in Markdown format. Thecode templates represent fully functional code that passes all the tests and is used to generate testexamples for CodeApex. The difficulty levels of the test cases are illustrated in Figure 4. Takingbinary search as an example, the first five test cases have small input sizes 𝑛 and can be solvedusing a regular loop-based search. However, the last five test cases have larger input sizes, and if aregular loop-based search is used, it will exceed the time limit. The privacy of data sources and thediversity of question descriptions decrease the possibility of test data appearing in the trainingcorpus of LLMs. This promotes the fairness of evaluation and contributes to rational cognition ofthe programming abilities of different LLMs.</p>
<p>Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu
111:10Function-only Scenario (Chinese)我会给你一个需要用C++编程解决的问题和一个C++的函数声明，你的任务是按照这个函数声明生成对应的C++函数去解决这个问题，你只需要返回C++函数,不要返回额外的说明文字。问题：输入两个整数a，b，输出a+b的值。函数声明：int sum_a_b(int a, int b);</p>
<p>-with-Error Message (Chinese) Code-with-Error Message Scenario (English)</p>
<p>Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu Below you are given a task description and the student's erroneous code, please correct the existing code so that it can pass the test cases of the task.Task description: Enter a character, check if the character is an uppercase or lowercase letter.Error type is： Partially Accepted Erroneous code is: …… Please output only the correct code after the correction and do not output redundant information.
111:12 Lingyue Fu, 下面给你一道题目描述以及学生的错误代码，请你将在 Code-only Scenario (Chinese)Below you are given a task description and the student's Code-only Scenario (English)现有代码的基础上修正，使其可以通过原有题目的测试erroneous code, please correct the existing code so that it can点。pass the test cases of the task.题目描述：输入一个字符，判断该字符是大写或小写字Task description: Enter a character, check if the character is母。an uppercase or lowercase letter.错误代码为：<code>#Erroneous code is：</code>#include <bits/stdc++.h> using namespace std; int main() { char aa; if( ( aa&gt;='a' &amp;&amp; aa&lt;='z' ) || ( aa&gt;='A' &amp;&amp; aa&lt;='Z' ) ) cout&lt;&lt;"yes"; else cout&lt;&lt;"no"; return 0; }<code>请 仅仅输出修正之后的正确代码，不要输出多余信息。include &lt;bits/stdc++.h&gt; using namespace std; int main() { char aa; if( ( aa&gt;='a' &amp;&amp; aa&lt;='z' ) || ( aa&gt;='A' &amp;&amp; aa&lt;='Z' ) ) cout&lt;&lt;"yes"; else cout&lt;&lt;"no"; return 0; }</code>P lease output only the correct code after the correction anddo not output redundant information.(b) Code-only scenario (English).下面给你一道题目描述以及学生的错误代码，请你将在 现有代码的基础上修正，使其可以通过原有题目的测试 点。 题目描述：输入一个字符，判断该字符是大写或小写字 母。 错误原因为：Partially Accepted 错误代码为： … … 请仅仅输出修正之后的正确代码，不要输出多余信息。 Code-with-Error Message Scenario (Chinese) (c) Code-with-Standard Code scenario (Chinese). Code下面给你一道题目描述以及学生的错误代码，请你将在 Code-with-Standard Code Scenario (Chinese)现有代码的基础上修正，使其可以通过原有题目的测试点。题目描述：输入一个字符，判断该字符是大写或小写字母。错误原因为： Partially AcceptedError type is： Partially Accepted错误代码为：Erroneous code is:……… …标准代码为：<code>#Standard code is:</code>#include <bits/stdc++.h>include <bits/stdc++.h>using namespace std;using namespace std;int main() {int main() {char aa;char aa;if( ( aa&gt;='a' &amp;&amp; aa&lt;='z' ) ||if( ( aa&gt;='a' &amp;&amp; aa&lt;='z' ) ||( aa&gt;='A' &amp;&amp; aa&lt;='Z' ) )( aa&gt;='A' &amp;&amp; aa&lt;='Z' ) )cout&lt;&lt;"yes";cout&lt;&lt;"yes";else cout&lt;&lt;"no";else cout&lt;&lt;"no";return 0;return 0;} <code>请}</code>P仅仅输出修正之后的正确代码，不要输出多余信息。lease output only the correct code after the correction anddo not output redundant information.(e) Code-with-Error Message scenario (Chinese).
(a) Code-only scenario (Chinese).(d)Code-with-ErrorMessagescenario (English).Below you are given a task description and the student's erroneous code, please correct the existing code so that it can pass the test cases of the task.Task description: Enter a character, check if the character is an uppercase or lowercase letter.Code-</p>
<p>with-Standard Code (English) Code-with-Standard Code Scenario (English)
(f) Code-with-Standard Code scenario (English).</p>
<p>Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu
Lingyue Fu,111:14</p>
<p>Table 2 .
2
CodeApex evaluates 14 LLMs along with their variants.</p>
<p>Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu
Lingyue Fu,111:18</p>
<p>Table 5 .
5
Performance on AC@1, AC@all, and AC Rate metrics of code generation task in Chinese version.The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined.General-purpose LLMs that are not able to generate code are removed from the table.
Code Generation (Chinese)ModelFunction-onlyFunction-with-ContextAC@1 AC@all AC Rate AC@1 AC@all AC RateChatGLM-6B0.2143 0.0924 0.13710.1576 0.06930.1063ChatGLM2-6B0.2143 0.1197 0.15600.1996 0.07770.1274ChatGLM3-6B0.1387 0.10920.13300.0903 0.06510.0854MOSS-16B*0.2374 0.1492 0.18790.2458 0.12820.1770Chinese-Alpaca-plus-13B0.2773 0.1387 0.18670.2710 0.13660.1886BELLE-7B-1M0.1639 0.0588 0.09540.1513 0.06510.0941BELLE-LLaMA-13B-2M0.1912 0.0903 0.12830.1996 0.08400.1274InternLM-Chat-7B0.1450 0.0735 0.10390.1513 0.09240.1128Baichuan-Chat-13B0.3130 0.1786 0.23030.1723 0.09870.1263WizardCoder-15B::::: 0.4496 :::::: 0.2773 ::::: 0.3468::::: 0.4328::::: 0.2668::::: 0.3329StarCoder0.2227 0.1366 0.16790.1870 0.09240.1340Vicuna-13B0.2689 0.1261 0.17900.3046 0.14920.2045GPT3.5-turbo0.6429 0.4265 0.52400.6660 0.48530.5644GPT40.6216 0.4968 0.5411 0.7672 0.6336 0.6659Human (one-submission)---0.7099 0.70990.7099Human (three-submission)---0.9288 0.92880.9288Human (five-submission)---0.9766 0.97660.9766</p>
<p>Table 6 .
6
Performance on AC@1, AC@all, and AC Rate metrics of code generation task in English version.The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined.General-purpose LLMs that are not able to generate code are removed from the table.
Code Generation (English)ModelFunction-onlyFunction-with-ContextAC@1 AC@all AC Rate AC@1 AC@all AC RateChatGLM-6B0.2080 0.0693 0.12030.1828 0.06720.1031ChatGLM2-6B0.1891 0.0819 0.12430.1029 0.04620.0668ChatGLM3-6B0.1450 0.11760.13630.1324 0.09870.1232MOSS-16B*0.2626 0.1513 0.20020.1534 0.08820.1178Chinese-Alpaca-plus-13B0.2878 0.1345 0.19630.2542 0.10920.1682BELLE-7B-1M0.2038 0.0651 0.11610.1618 0.05250.0863BELLE-LLaMA-13B-2M0.1870 0.0777 0.11970.2227 0.08610.1434InternLM-Chat-7B0.0945 0.0525 0.07070.3025 0.15970.2126Baichuan-Chat-13B0.3298 0.1618 0.23100.3319 0.16810.2273WizardCoder-15B StarCoder::::: 0.4244 :::::: 0.2500 ::::: 0.3248 0.3382 0.1891 0.2494::::: 0.4391 0.2794 0.1723 ::::: 0.27520.3444 ::::: 0.2116Vicuna-13B0.1870 0.0861 0.12360.2983 0.12180.1861GPT3.5-turbo0.6134 0.4286 0.51050.6597 0.48320.5606GPT40.5839 0.4623 0.5176 0.7032 0.5705 0.5992Human (one-submission)---0.7099 0.70990.7099Human (three-submission)---0.9288 0.92880.9288Human (five-submission)---0.9766 0.97660.9766</p>
<p>Table 7 .
7
Compilable code proportion on each scenario.All models except MOSS-16B<em> use the completion pattern to generate code.The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined.Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu
Compilable Code ProportionModelZHENFunction-only Function-with-Context Function-only Function-with-ContextChatGLM-6B0.56930.52310.64290.5336ChatGLM2-6B0.51050.53990.53990.4034ChatGLM3-6B0.68280.46010.73530.6261MOSS-16B</em>0.52310.51470.60920.3739Chinese-Alpaca-plus-13B0.71640.67440.70170.6723BELLE-7B-1M0.42440.42440.52730.4307BELLE-LLaMA-13B-2M0.51050.46010.45800.5357InternLM-Chat-7B0.34660.42650.20170.7626Baichuan-Chat-13B0.62180.39080.76050.7437WizardCoder-15B0.8529::::: 0.86340.8361::::: 0.7899StarCoder0.46850.48530.67650.5945Vicuna-13B0.71220.79830.44960.7815GPT3.5-turbo0.91180.89500.89290.8929GPT4::::: 0.78650.9159::::: 0.78830.8674
exhibit code correction capability after being provided with standard code.This is because J. ACM, Vol.37, No. 4, Article 111.Publication date: August 2018.</p>
<p>Table 8 .
8
Best performance comparison between explicit and narrative questions on AC@1, AC@all and AC Rate metric of code generation in English version.The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined.
Code Generation (English)ModelExplicit QuestionsNarrative QuestionsAC@1 AC@all AC Rate AC@1 AC@all AC RateChatGLM-6B0.2558 0.10080.12400.1560 0.15600.1560ChatGLM2-6B0.2326 0.12020.11530.1376 0.13760.1376ChatGLM3-6B0.1387 0.10920.13300.0903 0.06510.0854MOSS-16B*0.3488 0.24030.25620.1606 0.16060.1606Chinese-Alpaca-plus-13B0.3876 0.20540.23840.1835 0.18350.1835BELLE-7B-1M0.2674 0.09300.13350.1330 0.13300.1330BELLE-LLaMA-13B-2M0.2946 0.13950.19330.1422 0.14220.1422InternLM-Chat-7B0.3992 0.22870.25020.1927 0.19270.1927Baichuan-Chat-13B0.4147 0.24810.26800.2523 0.25230.2523Wizard-Coder-15B StarCoder::::: 0.5388 0.4264 0.2597 ::::: 0.3488::::: 0.3943 ::::: 0.3440 0.2744 0.2385 0.2385 ::::: 0.34400.3440 ::::: 0.2385Vicuna-13B0.3953 0.18220.23930.1881 0.18810.1881GPT3.5-turbo0.7054 0.52710.57700.6193 0.6193 0.6193GPT40.7276 0.5837 0.6025 0.6743 0.55500.5959Human (one-submission)0.6970 0.69700.69700.7457 0.74570.7457Human (three-submissions) 0.9169 0.91690.91690.9618 0.96180.9618Human (five-submissions)0.9731 0.97310.97310.9865 0.98650.9865</p>
<p>Table 9 .
9
Performance on AC@1, AC@all, and AC Rate metrics of code generation task in Chinese version.The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined.
Code Correction (Chinese)ModelCode-onlyCode-with-Error Message Code-with-Standard CodeAC@1 AC@n AC Rate AC@1 AC@n AC RateAC@1 AC@n AC RateChatGLM-6B0.1323 0.05190.08990.1286 0.05110.08510.16990.15660.1086ChatGLM2-6B0.2549 0.09020.15740.2316 0.08870.14020.20220.16540.1476ChatGLM3-6B0.4293 0.24360.29440.4414 0.23760.29180.29170.28050.2220BELLE-7B-1M0.0526 0.02260.03450.0256 0.00600.01560.08800.04960.0724BELLE-LLaMA-13B-2M 0.0647 0.02180.03860.0496 0.01650.02760.04290.03980.0241InternLM-Chat-7B0.1150 0.06770.07850.1271 0.06620.08310.14740.13380.1334Baichuan-Chat-13B0.3654 0.25340.28870.3782 0.25560.27620.23760.20830.1803WizardCoder-15B0.5767 :::::: 0.43310.45680.5556 :::::: 0.41200.44900.25560.24290.1963StarCoder0.6639 0.4090::::: 0.48690.6421 0.3767::::: 0.46380.4737::::: 0.30300.3452GPT3.5-turbo::::: 0.5699 0.54660.4909::::: 0.5722 0.54810.5015::::: 0.34360.3316::::: 0.2908GPT40.6970 0.6594 0.6453 0.7143 0.6767 0.6677 0.6038 0.5940 0.5269</p>
<p>Table 1 ,
1
Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu A BREAKDOWN PERFORMANCE IN PROGRAMMING COMPREHENSION Table 2, Table 3, and Table 4 present the breakdown of performance of LLMs in programming comprehension task in the 5-shot, 2-shot, and 0-shot scenarios.
Lingyue Fu,111:26</p>
<p>Table 1 .
1
5-shot, 2-shot, and 0-shot accuracy results of Chinese version Programming Comprehension Task in the answer-only setting.C.U., C.R, and M.H.R indicate conceptual understanding, commonsense reasoning, and multi-hop reasoning questions, respectively.<em> represents chat-bot mode of the LLM.-indicates that the model does not have the ability to complete the test in that setting.
Programming Comprehension (Answer-only)-ZHModelC.U.C.R.M.H.R.Total5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shotChatGLM-6B0.320.300.320.160.120.260.300.260.310.250.220.30ChatGLM-6B</em>0.310.330.310.270.250.160.250.310.230.280.300.23ChatGLM2-6B0.320.330.370.250.270.210.300.280.340.290.300.30ChatGLM2-6B<em>0.340.310.320.260.260.190.310.310.230.300.290.25ChatGLM3-6B0.290.300.340.240.260.240.380.340.340.290.300.30ChatGLM3-6B</em>0.230.350.460.210.270.190.250.170.280.230.280.31MOSS-16B<em>0.310.300.280.240.280.340.330.250.320.290.280.31Chinese-Alpaca-7B0.210.260.350.270.290.250.180.270.260.230.270.29Chinese-Alpaca-7B</em>0.240.240.300.290.290.270.210.150.280.250.240.28Chinese-Alpaca-plus-7B0.210.190.230.300.340.300.140.160.260.230.240.27Chinese-Alpaca-plus-7B<em>0.330.200.280.290.300.160.210.250.250.290.250.22Chinese-Alpaca-13B0.240.210.260.230.200.290.150.200.260.220.200.27Chinese-Alpaca-13B</em>0.230.270.270.230.290.240.210.200.180.230.260.24Chinese-Alpaca-plus-13B0.270.240.240.340.280.200.210.200.190.280.250.21Chinese-Alpaca-plus-13B<em>0.270.230.270.270.300.280.130.200.200.230.250.26BELLE-7B-1M0.270.260.330.320.250.280.280.250.260.290.250.29BELLE-7B-1M</em>0.350.310.260.310.290.260.210.210.250.300.280.26BELLE-7B-2M0.200.220.240.320.340.260.280.260.190.270.280.24BELLE-7B-2M<em>0.200.210.200.320.300.250.180.150.280.240.230.24BELLE-LLaMA-7B-0.6M0.290.240.140.330.340.270.280.320.210.310.300.21BELLE-LLaMA-7B-0.6M</em>0.280.280.280.300.310.310.300.310.280.290.300.29BELLE-LLaMA-7B-2M0.170.220.200.300.290.280.160.170.200.220.240.23BELLE-LLaMA-7B-2M<em>0.190.280.230.210.300.310.090.260.200.170.280.25BELLE-LLaMA-13B-2M0.190.210.290.310.310.330.180.190.200.240.250.28BELLE-LLaMA-13B-2M</em>0.340.240.190.270.290.270.220.210.210.280.250.23InternLM-Chat-7B0.460.420.440.220.270.340.340.330.310.340.340.37Baichuan-7B0.340.240.340.260.240.340.260.250.240.290.250.31EduChat-base-002-7B<em>0.190.230.180.310.220.140.230.200.160.250.220.16EduChat-base-002-13B</em>0.280.250.170.350.340.030.250.220.130.300.280.10EduChat-sft-002-7B<em>0.220.240.170.190.240.150.250.300.270.210.260.19CodeT5-plus-16B0.160.170.260.280.290.300.320.260.210.250.240.26CodeT5-plus-16B</em>0.280.18-0.370.26-0.270.23-0.320.22-CodeT5-plus-6B0.200.190.230.350.350.190.240.230.180.270.260.20CodeT5-plus-6B<em>0.230.21-0.280.23-0.260.18-0.260.21-GPT-3.5-turbo0.580.560.610.300.310.330.610.590.570.470.470.49GPT-3.5-turbo</em>0.600.570.570.360.350.270.610.640.540.510.500.44GPT-40.670.640.630.550.540.490.840.820.800.660.640.62GPT-4*0.700.670.660.550.480.550.820.820.770.670.630.64B BREAKDOWN PERFORMANCE IN CODE GENERATION</p>
<p>Table 5 and
5
Table 6present the breakdown of performance of LLMs in code generation task in the code-only and code-with-context scenarios.The breakdown performance of explicit questions and narrative questions is also shown in the table.
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models111:27</p>
<p>Table 2 .
2
5-shot, 2-shot, and 0-shot accuracy results of Chinese version Programming Comprehension Task in CoT setting.C.U., C.R, and M.H.R indicate conceptual understanding, commonsense reasoning, and multi-hop reasoning questions, respectively.<em> represents chat-bot mode of the LLM.-indicates that the model does not have the ability to complete the test in that setting.
Programming Comprehension (Chain-of-Thought)-ZHModelC.U.C.R.M.H.R.Total5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shotChatGLM-6B0.310.270.220.210.160.180.380.260.230.290.220.21ChatGLM-6B</em>0.330.260.270.200.220.170.180.280.280.240.250.23ChatGLM2-6B0.480.360.300.230.250.200.250.310.280.320.300.26ChatGLM2-6B<em>0.410.320.300.240.290.260.380.230.210.340.290.26ChatGLM3-6B0.330.340.360.260.340.240.250.240.220.280.280.28ChatGLM3-6B</em>0.340.260.350.220.260.230.270.350.220.270.250.27MOSS-16B<em>0.090.270.270.090.120.170.230.330.200.120.220.21Chinese-Alpaca-7B0.280.290.210.250.300.100.200.180.210.250.270.17Chinese-Alpaca-7B</em>0.280.240.270.240.320.120.160.200.260.240.260.21Chinese-Alpaca-plus-7B0.200.370.230.270.310.200.180.280.180.220.320.21Chinese-Alpaca-plus-7B<em>0.270.270.220.260.240.240.150.200.230.240.240.23Chinese-Alpaca-13B0.290.230.270.290.200.200.180.230.250.260.220.24Chinese-Alpaca-13B</em>0.280.270.310.270.230.170.230.160.130.260.230.21Chinese-Alpaca-plus-13B0.240.260.310.250.290.180.130.210.180.220.260.23Chinese-Alpaca-plus-13B<em>0.290.240.360.250.240.270.150.230.230.240.240.29BELLE-7B-1M0.330.300.260.230.290.190.250.200.180.270.270.21BELLE-7B-1M</em>0.330.280.180.220.250.100.210.160.150.260.240.14BELLE-7B-2M0.200.200.290.240.260.180.230.180.110.220.220.20BELLE-7B-2M<em>0.260.200.270.230.240.230.150.210.210.220.220.24BELLE-LLaMA-7B-0.6M0.270.230.230.210.300.280.160.300.180.220.280.24BELLE-LLaMA-7B-0.6M</em>0.280.280.340.200.360.280.210.310.200.230.320.28BELLE-LLaMA-7B-2M0.240.190.120.300.190.140.230.210.150.260.200.14BELLE-LLaMA-7B-2M<em>0.280.280.170.260.300.120.250.250.130.260.280.14BELLE-LLaMA-13B-2M0.220.290.200.190.270.190.200.180.160.200.260.19BELLE-LLaMA-13B-2M</em>0.210.270.170.220.280.150.180.210.110.210.260.15InternLM-Chat-7B0.360.320.340.180.180.140.360.200.230.290.240.24Baichuan-7B0.040.060.070.110.020.040.050.020.070.070.030.06EduChat-base-002-7B<em>0.280.270.110.230.220.210.180.260.150.240.250.16EduChat-base-002-13B</em>0.300.290.180.230.240.180.180.330.160.240.280.18EduChat-sft-002-7B*0.310.240.270.200.210.190.250.230.260.250.230.24CodeT5-plus-16B</p>
<p>Table 7 ,
7
9, 11, 8, 10, and 12 present the breakdown of performance of LLMs in code correction task with wrong answer error, time limited error and runtime error.The breakdown performance of code-only, code-with-error message and code-with-standard code scenarios are also shown in the table.Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu
5-turbo0.590.550.500.370.410.350.680.630.520.530.520.45GPT-3.5-turbo<em>0.570.560.540.390.420.330.650.570.560.520.510.46GPT-40.710.710.630.520.520.510.660.660.62GPT-4</em>0.720.680.570.590.520.450.840.850.790.700.660.58C BREAKDOWN PERFORMANCE IN CODE CORRECTIONReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009</p>
<p>Table 3 .
3
5-shot, 2-shot, and 0-shot accuracy results of English version Programming Comprehension Task in the answer-only setting.C.U., C.R, and M.H.R indicate conceptual understanding, commonsense reasoning, and multi-hop reasoning questions, respectively.<em> represents chat-bot mode of the LLM.-indicates that the model does not have the ability to complete the test in that setting.
Programming Comprehension (Answer-Only)-ENModelC.U.C.R.M.H.R.Total5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shotChatGLM-6B0.320.340.360.160.210.240.300.310.300.290.280.30ChatGLM-6B</em>0.320.250.390.270.240.230.250.380.300.320.280.23ChatGLM2-6B0.340.330.410.250.260.320.300.300.230.290.300.30ChatGLM2-6B<em>0.360.320.280.260.280.230.310.280.210.300.300.25ChatGLM3-6B0.310.310.370.330.320.300.280.300.300.310.310.32ChatGLM3-6B</em>0.240.300.340.230.170.200.230.190.300.230.220.27MOSS-16B<em>0.260.250.260.240.270.350.330.330.150.280.280.31Chinese-Alpaca-7B0.310.300.280.270.260.250.180.210.330.290.260.29Chinese-Alpaca-7B</em>0.170.260.290.290.290.190.210.160.230.170.250.28Chinese-Alpaca-plus-7B0.340.240.280.300.290.280.140.220.330.300.260.27Chinese-Alpaca-plus-7B<em>0.240.260.210.290.210.200.210.200.230.260.230.22Chinese-Alpaca-13B0.260.200.310.230.210.230.150.160.210.230.200.27Chinese-Alpaca-13B</em>0.100.230.280.230.280.210.210.250.160.100.250.24Chinese-Alpaca-plus-13B0.240.210.240.340.360.270.210.260.310.280.280.21Chinese-Alpaca-plus-13B<em>0.210.270.290.270.360.190.130.250.220.260.300.26BELLE-7B-1M0.330.320.330.320.310.260.280.280.230.310.310.29BELLE-7B-1M</em>0.390.360.330.310.270.280.210.230.240.300.290.26BELLE-7B-2M0.260.240.240.320.260.300.280.200.260.260.240.24BELLE-7B-2M<em>0.230.210.270.320.230.190.180.200.250.230.220.24BELLE-LLaMA-7B-0.6M0.240.320.290.330.230.270.280.330.200.260.290.21BELLE-LLaMA-7B-0.6M</em>0.270.310.340.300.280.270.300.310.250.290.300.29BELLE-LLaMA-7B-2M0.300.280.210.300.270.330.160.140.260.270.240.23BELLE-LLaMA-7B-2M<em>0.280.260.270.210.320.290.090.260.250.260.280.25BELLE-LLaMA-13B-2M0.180.200.220.310.220.250.180.180.210.240.200.28BELLE-LLaMA-13B-2M</em>0.240.270.260.270.270.230.220.200.150.270.250.23InternLM-Chat-7B0.430.410.440.220.330.360.340.260.310.370.340.37Baichuan-7B0.260.290.270.260.270.310.260.270.300.220.280.31EduChat-base-002-7B<em>0.330.240.260.310.340.250.230.310.330.310.300.16EduChat-base-002-13B</em>0.370.300.210.350.360.120.250.250.180.330.310.10EduChat-sft-002-7B<em>0.290.260.230.190.340.250.250.250.150.290.290.19CodeT5-plus-16B0.190.200.210.280.300.290.320.240.220.250.250.26CodeT5-plus-16B</em>0.200.20-0.370.27-0.270.28-0.240.25-CodeT5-plus-6B0.280.270.240.350.290.190.240.290.160.320.280.20CodeT5-plus-6B<em>0.260.27-0.280.28-0.260.25-0.300.27-GPT3.5-turbo0.510.510.520.300.300.350.610.550.510.420.440.49GPT3.5-turbo</em>0.530.520.530.360.320.250.610.500.550.440.440.44GPT-40.620.640.630.480.440.480.750.750.800.600.590.62GPT-4*0.610.620.620.480.420.480.820.770.800.610.580.61
J. ACM, Vol.37, No. 4, Article 111.Publication date: August 2018.</p>
<p>Table 4 .
4
5-shot, 2-shot, and 0-shot accuracy results of English version Programming Comprehension Task in CoT setting.C.U., C.R, and M.H.R indicate conceptual understanding, commonsense reasoning, and multi-hop reasoning questions, respectively.<em> represents chat-bot mode of the LLM.-indicates that the model does not have the ability to complete the test in that setting.Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang, and Yong Yu
Programming Comprehension(Chain-of-Thought)-ENModelC.U.C.R.M.H.R.Total5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shotChatGLM-6B0.310.340.230.160.160.180.250.310.260.240.260.22ChatGLM-6B</em>0.280.290.200.250.170.250.310.280.280.280.240.24ChatGLM2-6B0.370.360.330.250.240.150.230.300.210.290.300.23ChatGLM2-6B<em>0.300.290.400.250.230.250.230.280.210.260.260.30ChatGLM3-6B0.350.330.400.200.300.170.280.210.270.270.290.27ChatGLM3-6B</em>0.340.280.310.210.210.200.270.190.280.270.230.26MOSS-16B<em>0.130.270.260.100.190.200.280.260.180.160.240.22Chinese-Alpaca-7B0.230.260.230.330.300.140.130.210.130.250.260.17Chinese-Alpaca-7B</em>0.230.270.220.300.320.140.110.230.100.230.280.16Chinese-Alpaca-plus-7B0.200.270.280.230.300.090.280.300.130.230.290.17Chinese-Alpaca-plus-7B<em>0.220.260.280.260.180.200.260.210.160.250.220.22Chinese-Alpaca-13B0.310.260.260.230.210.140.200.230.200.250.230.20Chinese-Alpaca-13B</em>0.200.240.220.210.160.190.180.260.210.200.220.21Chinese-Alpaca-plus-13B0.240.310.240.220.190.250.300.280.100.250.260.21Chinese-Alpaca-plus-13B<em>0.300.300.210.220.150.200.250.300.180.260.240.20BELLE-7B-1M0.230.170.240.220.160.180.160.080.210.210.140.21BELLE-7B-1M</em>0.210.190.120.190.200.110.110.150.070.180.180.10BELLE-7B-2M0.130.140.260.220.190.200.110.130.130.160.160.20BELLE-7B-2M<em>0.190.190.140.220.220.080.130.070.070.190.170.10BELLE-LLaMA-7B-0.6M0.220.200.080.220.230.130.260.180.030.230.210.09BELLE-LLaMA-7B-0.6M</em>0.330.220.200.190.210.240.260.210.250.260.220.23BELLE-LLaMA-7B-2M0.220.070.090.210.080.100.100.050.130.190.070.10BELLE-LLaMA-7B-2M<em>0.210.110.210.200.110.150.130.030.200.190.090.18BELLE-LLaMA-13B-2M0.260.210.110.210.220.130.150.130.200.210.200.14BELLE-LLaMA-13B-2M</em>0.280.210.160.170.100.090.180.110.100.210.140.12InternLM-Chat-7B0.420.320.370.250.200.210.260.250.250.320.260.28Baichuan-7B0.120.100.080.090.110.020.080.020.300.100.080.04EduChat-base-002-7B<em>0.240.260.130.190.240.120.260.200.150.230.240.13EduChat-base-002-13B</em>0.310.240.120.240.220.120.250.280.130.270.240.12EduChat-sft-002-7B*0.290.310.240.220.200.170.260.250.150.260.250.19</p>
<p>Table 5 .
5
Best performance comparison between explicit questions and narrative questions on AC@1, AC@all and AC Rate metric of code generation in Chinese version.
Code Generation-ZHCode-onlyCode-with-ContextModelExplicit QuestionsNarrative QuestionsExplicit QuestionsNarrative QuestionsAC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Ratebaichuan0.420.260.290.180.180.180.240.160.160.090.090.09BELLE0.270.150.160.100.100.100.270.140.150.120.120.12BELLE-7B-1M0.200.090.100.120.120.120.190.100.110.100.100.10chatglm0.270.140.170.150.150.150.210.110.120.100.100.10chatglm20.290.170.200.120.120.120.270.120.200.120.120.12cn-alpaca-plus-13B0.360.200.220.190.190.190.370.210.240.160.160.16gpt3.5-turbo0.700.480.570.580.580.580.720.520.600.610.610.61gpt-40.660.530.570.570.460.510.770.640.670.770.620.66intern0.190.110.110.090.090.090.210.140.130.080.080.08moss0.340.220.230.110.110.110.340.200.240.140.140.14starcoder0.280.180.210.160.160.160.230.120.150.150.150.15vicuna0.350.180.230.170.170.170.370.210.260.230.230.23wizard0.540.350.370.340.340.340.530.360.390.320.320.32</p>
<p>Table 6 .
6
Best performance comparison between explicit questions and narrative questions on AC@1, AC@all and AC Rate metric of code generation in English version.AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate
Code Generation-ENCode-onlyCode-with-ContextModelExplicit QuestionsNarrative QuestionsExplicit QuestionsNarrative QuestionsAC@1 baichuan 0.410.230.270.240.240.240.400.250.250.250.250.25BELLE0.280.130.160.080.080.080.290.140.190.140.140.14BELLE-7B-1M0.270.090.130.130.130.130.190.080.100.120.120.12chatglm0.260.100.120.160.160.160.220.090.110.140.140.14chatglm20.230.120.120.140.140.140.140.070.070.060.060.06cn-alpaca-plus-13B0.390.210.240.170.170.170.320.170.200.180.180.18gpt3.5-turbo0.690.500.560.530.530.530.710.530.580.620.620.62gpt-40.600.470.520.560.450.510.730.580.600.670.560.60intern0.140.090.080.040.040.040.400.230.250.190.190.19moss0.350.240.260.160.160.160.220.130.150.080.080.08starcoder0.430.260.270.240.240.240.350.230.240.190.190.19vicuna0.250.120.140.110.110.110.400.180.240.190.190.19wizard0.540.340.390.290.290.290.520.350.390.340.340.34</p>
<p>Table 7 .
7
Performance on AC@1, AC@all, and AC Rate metrics of code correction task (Wrong Answer Error) in Chinese version.</p>
<p>J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
https://github.com/APEXLAB/CodeApex.git
https://apex.sjtu.edu.cn/codeapex/
ACKNOWLEDGMENTWe would like to thank Qinxiang Cao and Pengfei Liu for helpful discussions.We also thank Shanghai Boyu Education Technology Co., Ltd for granting us data authorization and providing valuable support in establishing the testing framework.The work is partially supported by National Natural Science Foundation of China (62177033).
indicate conceptual understanding, commonsense reasoning, and multi-hop reasoning questions, respectively. Each column displays the best accuracy among 5-shot, 2-shot, and 0-shot scenarios. The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance is wave underlined. * represents chat-bot mode of the LLM. ZH and EN represent the results of Chinese and English test versions. J Acm ; Lingyue, Huacan Fu, Kounianhua Chai, Weiming Du, Shuang Zhang, Jianghao Luo, Yuchen Lin, Renting Fang, Rui, Jianxing Hao Guan, Siyuan Liu, Longteng Qi, Jiayi Fan, Yifan Lei, Jingkuan Liu, Kangning Wang, Weinan Zhang, Yong Zhang, ; C U Yu, C R , M H R , August 201837Accuracy results of Programming Comprehension Task in CoT setting. Since CodeT5 fails to generate a response with chain-of-thought, we exclude it in this setting. Programming Comprehension (Chain-of-Thought</p>
<p>. Chinese-Alpaca, 7B 0.2889 0.3030 0.2131 0.2680 0.2556 0.3333 0.2131 0.2640</p>
<p>. Chinese-Alpaca, 7B* 0.2778 0.3232 0.2623 0.2640 0.2667 0.3232 0.2295 0.2800</p>
<p>Chinese-Alpaca-plus-7B. </p>
<p>. Chinese-Alpaca, 7B* 0.2667 0.2626 0.2295 0.2400 0.2778 0.2626 0.2623 0.2480</p>
<p>. Chinese-Alpaca, 13B 0.2889 0.2929 0.2459 0.2640 0.3111 0.2323 0.2295 0.2520</p>
<p>. Chinese-Alpaca, 13B* 0.3111 0.2727 0.2295 0.2640 0.2444 0.2121 0.2623 0.2160</p>
<p>Chinese-Alpaca-plus-13B. </p>
<p>. Chinese-Alpaca, 13B* 0.3556 0.2727 0.2295 0.2920 0.3000 0.2222 0.2951 0.2560 BELLE-7B-1M</p>
<p>. Belle-Llama , 7B-0.6M 0.2667 0.3030 0.2951 0.2760 0.2222 0.2323 0.2623 0.2320</p>
<p>. Belle-Llama , 7B-0.6M* 0.3444 0.3636 0.3115 0.3200 0.3333 0.2424 0.2623 0.2600</p>
<p>. Belle-Llama , 7B-2M 0.2444 0.3030 0.2295 0.2640 0.2222 0.2121 0.1311 0.1880</p>
<p>. Belle-Llama , 7B-2M* 0.2778 0.3030 0.2459 0.2800 0.2111 0.2020 0.1967 0.1880</p>
<p>. Belle-Llama , 13B-2M 0.2889 0.2727 0.1967 0.2560 0.2556 0.2222 0.1967 0.2120</p>
<p>. Belle-Llama , 13B-2M* 0.2667 0.2828 0.2131 0.2600 0.2778 0.1717 0.1803 0.2120</p>
<p>. Baichuan-7b , </p>
<p>Minh-Thang Daniel K Adiwardana, Robert Lu, David Fergus, Chen, arXiv:2001.09977Meena: Open-domain conversational agent. 2020. 2020arXiv preprint</p>
<p>Uri Alon, Meital Zilberstein, Omer Levy, arXiv:1803.09473Code2Vec: Learning Distributed Representations of Code. 2018. 2018arXiv preprint</p>
<p>Daman Arora, Himanshu Gaurav Singh, Mausam , arXiv:2305.15074[cs.CL]Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. 2023</p>
<p>Program Synthesis with Large Language Models. Jacob Austin, Augustus Odena, Maxwell I Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, arXiv:2108.077322021. 2021</p>
<p>B Inc, Baichuan, Baichuan 7b &amp; 13b. 2023</p>
<p>ABCDM: An attention-based bidirectional CNN-RNN deep model for sentiment analysis. Shahla Mohammad Ehsan Basiri, Moloud Nemati, Erik Abdar, Cambria, Acharya Rajendra, Future Generation Computer Systems. 1152021. 2021</p>
<p>Copilot-a hard real-time runtime monitor. Ananda Basu, International Conference on Embedded Software. Springer2004</p>
<p>FlashFill++: Scaling Programming by Example by Cutting to the Chase. José Cambronero, Sumit Gulwani, Vu Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon, Ashish Tiwari, 10.1145/3571226Proc. ACM Program. Lang. 72023. jan 2023POPL, Article</p>
<p>Publication date. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. 37423August 2018J. ACM</p>
<p>Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, arXiv:2107.03374[cs.LG]Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Andrew N. Carr; Bob McGrew, Dario Amodei, Sam McCandlishJan Leike</p>
<p>Teaching Large Language Models to Self-Debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.05128[cs.CL]2023</p>
<p>Palm: Scaling language modeling with pathways. Chowdhery, Narang, Devlin, Journal of Machine Learning Research. 242023. 2023</p>
<p>Pangu-coder: Program synthesis with function-level language modeling. Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, arXiv:2207.112802022. 2022arXiv preprint</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.05457[cs.AI]Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. 2018</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training Verifiers to Solve Math Word Problems. CoRR abs/2110. 2021. 202114168</p>
<p>Yiming Cui, Ziqing Yang, Xin Yao, arXiv:2304.08177Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. 2023. 2023arXiv preprint</p>
<p>Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tiea, Yougen Zhou, Yilei Wang, Aimin Zhou, Ze Zhouand Qin Chen, Jie Zhou, arXiv:2308.02773Liang He, and Xipeng Qiu. 2023. EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education. 2023arXiv preprint</p>
<p>Can ChatGPT pass high school exams on English language comprehension. Joost Cf De Winter, 2023. 2023Researchgate. Preprint</p>
<p>How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. Aniket Deroy, Kripabandhu Ghosh, Saptarshi Ghosh, arXiv:2306.01248[cs.CL]2023</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Baby steps in evaluating the capacities of large language models. Frank Michael, Nature Reviews Psychology. 22023. 2023</p>
<p>SAIL: A Situated Artificial Intelligence Language Model for Sentiment Analysis in Literary Texts. Deepayan Ghosal, Koustav Ghosh, Abir Chakraborty, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2020Asif Ekbal, and Pushpak Bhattacharyya</p>
<p>How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment. Aidan Gilson, Conrad Safraneck, Thomas Huang, Vimig Socrates, Ling Chi, Richard Taylor, David Chartash, 10.2196/45312JMIR medical education. 9e453122023. 02 2023</p>
<p>Exploring the Responses of Large Language Models to Beginner Programmers. Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja Kujanpää, Juha Sorva, arXiv:2306.057152023. 2023Help Requests. arXiv preprint</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, arXiv:2105.09938Measuring coding challenge competence with apps. 2021. 2021arXiv preprint</p>
<p>CodeBERT: A Pretrained Model for Programming and Natural Languages. Linqing Huang, Pengcheng Yin, Wenpeng Guo, Xiaoya Zhang, Mo Yu, Yinxuan Wang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). the 58th Annual Meeting of the Association for Computational Linguistics (ACL)2020</p>
<p>. Rui Huang, Xiaoya Zhang, Furu Wei, 2021</p>
<p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, arXiv:2305.08322Maosong Sun, and Junxian He. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. 2023arXiv preprint</p>
<p>Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1909.09436[cs.LG]CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. 2020</p>
<p>BELLE: Be Everyone's Large Language model Engine. Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, Xiangang Li, 2023</p>
<p>Publication date. J Acm ; Lingyue, Huacan Fu, Kounianhua Chai, Weiming Du, Shuang Zhang, Jianghao Luo, Yuchen Lin, Renting Fang, Rui, Jianxing Hao Guan, Siyuan Liu, Longteng Qi, Jiayi Fan, Yifan Lei, Jingkuan Liu, Kangning Wang, Weinan Zhang, Yong Zhang, Yu, August 201837Article 111</p>
<p>Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases. Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, Xiangang Li, arXiv:2303.147422023. 2023arXiv preprint</p>
<p>Adi Lahat, Eyal Shachar, Benjamin Avidan, Zina Shatz, Benjamin S Glicksberg, Eyal Klang, Evaluating the use of large language model in identifying top research questions in gastroenterology. 2023. 2023134164</p>
<p>RACE: Large-scale ReAding Comprehension Dataset From Examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy, arXiv:1704.04683[cs.CL]2017</p>
<p>Binary codes capable of correcting deletions, insertions, and reversals. Vladimir I Levenshtein, Soviet physics doklady. 101966. 1966</p>
<p>. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Sankalp Siva, Dmitry Patel, Marco Abulkhanov, Manan Zocca, Zhihan Dey, Nour Zhang, Urvashi Fahmy, Wenhao Bhattacharyya, Swayam Yu, Sasha Singh, Paulo Luccioni, Maxim Villegas, Fedor Kunakov, Manuel Zhdanov, Tony Romero, Nadav Lee, Jennifer Timor, Claire Ding, Hailey Schlesinger, Schoelkopf, Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva ReddyJanArjun GuhaDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas WolfLeandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you! arXiv:2305.06161 [cs.CL</p>
<p>Competition-level code generation with alphacode. Li, Choi, Chung, Science. 2022. 2022</p>
<p>Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, arXiv:2306.05817How Can Recommender Systems Benefit from Large Language Models: A Survey. 2023. 2023arXiv preprint</p>
<p>Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation. Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang, arXiv:2308.111312023. 2023arXiv preprint</p>
<p>P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsShort Papers20222</p>
<p>Fine-tune BERT for extractive summarization. Yang Liu, Mirella Lapata, Association for Computational Linguistics (ACL). 2019</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, WizardCoder: Empowering Code Large Language Models with Evol-Instruct. 2023</p>
<p>Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, Shanshan Li, arXiv:2309.16298At Which Training Stage Does Code Data Help LLMs Reasoning?. 2023. 2023arXiv preprint</p>
<p>Convolutional Neural Networks over Tree Structures for Programming Language Processing. Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Zhi Jin, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI). the Thirtieth AAAI Conference on Artificial Intelligence (AAAI)2016</p>
<p>Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, Brad Myers, arXiv:2307.08177[cs.SE]Using an LLM to Help With Code Understanding. 2024</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.13474Codegen: An open large language model for code with multi-turn program synthesis. 2022. 2022arXiv preprint</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research (JMLR). 212020. 2020</p>
<p>LLAMA: Leveraging Language Models for Zero-shot Semantic Parsing with Utterance Re-Writing. Michael Robb, arXiv:2109.026452021. 2021arXiv preprint</p>
<p>B Roziere, Gehring, Gloeckle, arXiv:2308.12950Code llama: Open foundation models for code. 2023. 2023arXiv preprint</p>
<p>Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023. MOSS: Training Conversational Language Models from Synthetic Data. 2023</p>
<p>CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. J Acm, August 20183725Publication date</p>
<p>InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities. InternLM Team. 2023</p>
<p>Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Zhiyuan Liu, Maosong Sun, arXiv:2401.04621[cs.SE]DebugBench: Evaluating Debugging Capability of Large Language Models. 2024</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems (NeurIPS). 2017</p>
<p>Yue Wang, Weishi Wang, Shafiq Joty, Steven C H Hoi, arXiv:2109.00859[cs.CL]CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. 2021</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-label Text Classification. Kui Xu, Lei Wu, Zhiguang Wang, Fei Tian, Tong Liu, Compling Pu, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD). the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)2019</p>
<p>Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, arXiv:2401.00812A survey on how code empowers large language models to serve as intelligent agents. 2024. 2024arXiv preprintIf llm is the wizard, then code is the wand</p>
<p>Graph2Code: Generating Code Snippets from Graphical User Interfaces Using Graph Neural Networks. Pengcheng Yao, Yiding Mao, Zijun Luo, Xiwei Li, Ling Qin, Proceedings of the 28th ACM International Conference on Multimedia (MM). the 28th ACM International Conference on Multimedia (MM)2020</p>
<p>CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, Tao Xie, 10.1145/3597503.3623316Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software EngineeringNew York, NY, USAAssociation for Computing Machinery202437<conf-loc>, <city>Lisbon</city>, <coun-try>Portugal</country>, </conf-loc>) (ICSE '24</p>
<p>Codereval: A benchmark of pragmatic code generation with generative pre-trained models. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, Tao Xie, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022. 2022arXiv preprint</p>
<p>PEGASUS: Pre-training with extracted gapsentences for abstractive summarization. Jingqing Zhang, Yaoqing Yang, Chris Tar, Jason Baldridge, arXiv:1912.087772020. 2020arXiv preprint</p>
<p>DialoGPT: Large-scale generative pre-training for conversational response generation. Yizhe Zhang, Eric Sun, Zhiyuan Liu, Xin Li, Minlie Deng, Yang Ji, arXiv:1911.005362019. 2019arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, arXiv:2306.05685[cs.CL]and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. </p>
<p>Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang, arXiv:2303.17568[cs.LG]CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X. 2023</p>
<p>Docprompting: Generating code by retrieving the docs. Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, Graham Neubig, The Eleventh International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
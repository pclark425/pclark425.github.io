<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1781 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1781</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1781</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-411408008cb3f5347b3fcb8b4631b3c6b0c2ff2b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/411408008cb3f5347b3fcb8b4631b3c6b0c2ff2b" target="_blank">TnT-LLM: Text Mining at Scale with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Knowledge Discovery and Data Mining</p>
                <p><strong>Paper TL;DR:</strong> TnT-LLM is proposed, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case, and generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines.</p>
                <p><strong>Paper Abstract:</strong> Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1781",
    "paper_id": "paper-411408008cb3f5347b3fcb8b4631b3c6b0c2ff2b",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00685825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TnT-LLM: Text Mining at Scale with Large Language Models</h1>
<p>Mengting Wan ${ }^{\text {T}, \text { Tara Safavi }}{ }^{\text {T }}$, Sujay Kumar Jauhar<em>, Yujin Kim</em>, Scott Counts<em>, Jennifer Neville</em>, Siddharth Suri<em>, Chirag Shah ${ }^{\dagger}$, Ryen W. White</em>, Longqi Yang<em>, Reid Andersen</em>, Georg Buscher<em>, Dhruv Joshi</em>, Nagu Rangan<em><br></em>Microsoft Corporation, ${ }^{\dagger}$ University of Washington<br>{mengting.wan,tarasafavi}@microsoft.com</p>
<h4>Abstract</h4>
<p>Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and timeconsuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications.</p>
<h2>1 INTRODUCTION</h2>
<p>Text mining is the process of extracting useful information and insights from a large collection of textual data [10, 27]. Two central and interrelated tasks in text mining are taxonomy generation, which involves finding and organizing a set of structured, canonical labels that describe aspects of the corpus, and text classification, or the labeling of instances in the corpus using said taxonomy. Many use cases of interest to practitioners can be framed as the sequential</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of the existing human-in-the-loop and unsupervised text clustering approaches as well as the proposed LLM-powered end-to-end label taxonomy generation and text classification framework (TnT-LLM).
application of these two tasks, especially when the label space is not well-defined or when exploring a new corpus: For example, sentiment analysis consists of devising a sentiment taxonomy (e.g., "happy", "sad") and classifying text content (e.g., social media posts, product reviews) with labels in this taxonomy. Likewise, intent detection consists of defining a set of intents (e.g., "book a flight", "buy a product") and classifying text content (e.g., chatbot transcripts, search queries) with the intent labels.</p>
<p>An established approach to these two tasks is to first employ domain experts to curate a label taxonomy [4, 20, 31], then gather human annotations on a small set of corpus sample using the handcrafted taxonomy in order to train a machine learning model for text classification. While such human-in-the-loop approaches offer high interpretability, they face significant scalability challenges: They demand domain expertise and careful consideration of the granularity, coverage, and consistency of the labels [4], and manual annotation is time-consuming and costly, not to mention prone to errors and biases [28]. Moreover, the process must be repeated for each downstream use-case (e.g., sentiment analysis, intent detection, etc). Another line of work aims to solve these issues of scale via machine learning techniques like text clustering, topic modeling, and keyphrase mining. Such approaches flip the ordering of taxonomy generation and classification by first organizing the corpus sample into clusters in an unsupervised or semi-supervised fashion, then deriving the label taxonomy thereafter by describing the learned clusters. Such approaches scale better with the corpus size and use-cases, but describing text clusters in an interpretable and consistent way has proved challenging, so much so that is has been likened to "reading tea leaves" [5].</p>
<p>To address these challenges, in this paper we propose TnT-LLM, a novel framework that combines the interpretability of manual approaches with the scale of automatic text clustering and topic modeling. TnT-LLM is an end-to-end two-phase framework for joint Taxonomy Generation and Text Classification that relies on the unique strengths of instruction following Large Language Models (LLMs) in both phases. First, in the taxonomy generation phase, we devise a zero-shot multi-stage reasoning approach that prompts an LLM to produce and refine a label taxonomy iteratively with respect to the corpus for a given use-case (e.g., intent detection). Second, in the text classification phase, we adopt LLMs as data augmentors to scale up the creation of training data, which in turn is used to train lightweight classifiers capable of large-scale labeling. This framework is adaptable and modular, and can be customized to different use cases, text corpora, LLMs, and classifiers, while requiring little human intervention or input. In summary, our main contributions are as follows:</p>
<ul>
<li>We introduce TnT-LLM, an end-to-end two-phase framework to automate and scale the process of taxonomy generation and text classification with representative and interpretable labels.</li>
<li>We present a series of quantifiable and traceable evaluation strategies to validate each stage of this framework, including deterministic automatic metrics, human evaluation metrics, as well as LLM-based evaluations.</li>
<li>We use TnT-LLM to analyze conversations from Bing Copilot (formerly Bing Chat), a web-scale, multilingual, and open-domain conversational agent. Our results show that the proposed framework can produce more accurate and relevant label taxonomies compared to the state-of-the-art text clustering approaches. We also demonstrate that the lightweight label classifiers trained on LLM annotations can achieve comparable (and sometimes better) performance than directly using LLMs as classifiers, but with much higher scalability and model transparency. Through quantitative and qualitative analysis, we provide insights and recommendations for applying LLMs on large-scale text mining.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>Taxonomy Generation. Prior work in taxonomy generation falls into manual and automatic approaches. Handcrafted taxonomies, beyond being expensive to construct, tend to be developed for specific downstream tasks (e.g., web search intent analysis [4, 20], chatbot intent detection [31]), or tied to the development of specific datasets [22, 25]. On the other hand, automated approaches scale better but either rely on term extraction from corpora to obtain labels, which may hinder interpretability and/or coverage [24, 33], or else require a set of seeds for the taxonomy in order to generate new labels [32]. TnT-LLM, in contrast, is automatic, abstractive (i.e., labels describe the corpus but need not be directly extracted from it), and does not require any seed labels. Moreover, TnT-LLM treats taxonomy generation and text classification as interrelated problems in an end-to-end pipeline, whereas prior work has tended to focus mainly on the quality of the taxonomy produced, without considering its downstream utility for classification.
Text Clustering and Topic Modeling. Text clustering and topic modeling "invert" the traditional approach of defining a label set, then applying the labels on the corpus. Given a set of documents,
such approaches first group the documents into topical clusters using various definitions of textual similarity, then post-hoc label or summarize the clusters [1, 29]. While traditional approaches in theory accomplish the same goals as TnT-LLM, they suffer due to a lack of interpretability [5], as they typically do not assign intelligible labels to clusters. More recently, attempts have been made to overcome these problems by using LLMs for topic modeling [18, 30], though these approaches still require supervision through either a predefined taxonomy [30] or a seed set of topics [18].
LLMs as Annotators. Recent work has explored using LLMs to replace human annotation for labor-intensive tasks such as search relevance quality labeling [28], topic and stance detection [8], and various computational social science labeling tasks [34]. These studies have found that, in general, LLMs perform on par or even better than crowd-workers [15], often at a fraction of the cost. In the same vein, we explore using LLMs as annotators for text classification, although our main goal is to scale the process by distilling LLMs' label-specific capabilities into more efficient, lightweight classifiers.</p>
<h2>3 METHOD</h2>
<p>We begin with a high-level overview of TnT-LLM, our proposed two-phase framework for 1) LLM-powered taxonomy generation and 2) LLM-augmented text classification. In the first phase, we sample a small-scale representative subset of a corpus and perform zero-shot multi-stage taxonomy generation in an iterative manner inspired by stochastic gradient descent [3]. In the second phase, we sample a larger dataset and leverage LLMs with the taxonomy produced by Phase 1 to classify each instance. These LLM labels are then treated as "pseudo-labels" for training a lightweight text classifier. Once training is complete, the lightweight classifier is deployed to label the entire corpus offline, and may also serve for online real-time classification.</p>
<h3>3.1 Phase 1: Taxonomy Generation</h3>
<p>Phase 1 of TnT-LLM is inspired by the classic mixture model clustering process [17], but implemented in a prompt-based manner. We leverage a "stochastic optimization" approach [19] to iteratively update the intermediate taxonomy outcome, so that a large and dynamic corpus sample can be effectively handled. Depending on the desired granularity of the taxonomy, we suggest using a "small-to-medium" corpus sample that is representative of the corpus in this phase, such that the sample size is sufficient to capture the diversity of the corpus, but not too large to incur unnecessary costs.</p>
<ul>
<li>Stage 1: Summarization. In order to normalize all text samples and extract their most salient information, we first generate concise and informative summaries of each document in the sample. Specifically, we prompt an LLM to summarize each document by providing a short blurb about the intended use-case for the summary (e.g., intent detection) and a target summary length (e.g., 20 words); the full prompt template is provided in Figure 8 in the supplemental details. This stage helps reduce the size and variability of the input documents while also extracting the aspects of the document most relevant to the use-case, which we find is especially important for label spaces that are not evident from surface-level semantics (e.g., user intent). Note that this stage is</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of the LLM-powered taxonomy generation phase (Phase 1).
relatively fast, as it may be executed concurrently for each input document with a cost-efficient LLM like GPT-3.5-Turbo.</p>
<ul>
<li>Stage 2: Taxonomy Creation, Update, and Review. We next create and refine a label taxonomy using the summaries from the previous stage. Similar to SGD, we divide the summaries into equal-sized minibatches. We then process these minibatches with three types of zero-shot LLM reasoning prompts in sequence. The first, an initial generation prompt, takes the first minibatch and produces an initial label taxonomy as output. The second, a taxonomy update prompt, iteratively updates the intermediate label taxonomy with new minibatches, performing three main tasks in each step: 1) evaluating the given taxonomy on the new data; 2) identifying issues and suggestions based on the evaluation; and 3) modifying the taxonomy accordingly. Finally, after the taxonomy has been updated a specified number of times, we apply a review prompt that checks the formatting and quality of the output taxonomy, of which the output is regarded as the final taxonomy output by Stage 1. In all three prompts, we provide the use-case instruction, which specifies the goal and the format of the desired label taxonomy (e.g., the desired number of labels, the target number of words per label), alongside the minibatch. The full prompt templates are provided in Figure 10 in the supplemental details.
Notice that this process naturally lends itself to hierarchy: After a first round of taxonomy generation, we can rerun Stage 2 for each subgroup of categorized samples to create new, more granular levels in the taxonomy. An overview of our proposed approach is presented in Figure 2.
Connection to Mixture Models \&amp; Stochastic Optimization. Here we present an analogy between our pipeline and the Mixture Model family (e.g., Gaussian Mixture Model) for text clustering. We assume each text data point $\left(x_{i}\right)$ follows a mixture distribution $x_{i} \sim \sum w_{k} \mathcal{N}\left(\mu_{k}, \Sigma_{k}\right)$, where $\mathcal{N}\left(\mu_{k}, \Sigma_{k}\right)$ defines the distribution of the $k$-th component, i.e., a Gaussian distribution with a mean $\mu_{k}$ and variance $\Sigma_{k}$. Given a corpus sample $\left{x_{i}\right}$, this mixture model can be learned through Maximum Likelihood Estimation (MLE), equivalent to minimizing the negative of the log-likelihood loss,
i.e.,</li>
</ul>
<p>$$
\begin{aligned}
&amp; \max \prod_{i}\left(\sum w_{k} \mathcal{N}\left(\mu_{k}, \Sigma_{k} ; x_{i}\right)\right) \
\Leftrightarrow &amp; \min -\sum_{i} \log \left(\sum w_{k} \mathcal{N}\left(\mu_{k}, \Sigma_{k} ; x_{i}\right)\right) \Leftrightarrow \min \sum_{i} \mathcal{L}\left(\Theta, x_{i}\right)
\end{aligned}
$$</p>
<p>Mapping back to our prompt-based approach, we take a corpus sample and a use-case instruction as input. Our goal is to "learn" a taxonomy that is relevant to the instruction and best fits the input corpus sample; this taxonomy must consist of category labels with names and brief descriptions. We can represent our desired label taxonomy as a parameter set $\boldsymbol{\Theta}={\boldsymbol{\mu}, \Sigma}$, following the definition of the mixture model, where $\boldsymbol{\mu}=\left{\mu_{k}\right}$ are the names of labels $k$ which represent the "cluster centroids," and $\Sigma=\left{\Sigma_{k}\right}$ are the descriptions that specify the "shape" of cluster $k$. We assume the mixture weights $\left(w_{k}\right)$ are implicitly captured by the LLM that generates the label taxonomy in this study. We can then map our taxonomy creation and refinement stages to stochastic optimization as follows:</p>
<ul>
<li>Stage 1: Feature Representation. Our summarization stage is analogous to the featurization step in classic machine learning, where raw text inputs are projected onto a vector space via a feature transformation such as an embedding model. In our case, the output summary of each data point can be viewed as a concise and informative feature representation of the original text $\left(\boldsymbol{x}_{i}\right)$.</li>
<li>Stage 2: Stochastic Gradient Descent. The main taxonomy creation and update stage resembles prompt optimization with Stochastic Gradient Descent (SGD) [19], where the generation prompt is used to initialize the taxonomy (i.e., the parameters $\Theta_{0}$ ), which is then optimized via SGD through the update promptchain. In each update prompt, we assess how the current taxonomy $\left(\Theta_{m}\right)$ fits the given batch of data (i.e., calculating the loss function defined in Eq. (1)), then analyze and "backpropagate" the errors to update the taxonomy, i.e., $\Theta_{m+1}=\Theta_{m}-\eta \nabla \mathcal{L}\left(\Theta_{m}\right)$, where $\eta$ refers to the learning rate which we assume is implicitly adjusted by the LLM.</li>
</ul>
<h3>3.2 Phase 2: LLM-Augmented Text Classification</h3>
<p>After the taxonomy is finalized, we next train a text classifier that can be reliably deployed to perform label assignments at very large-scale and in real-time. Following recent work that shows the strengths of LLMs as annotators of training data [8, 15], we propose to leverage LLMs to obtain a "pseudo-labeled" corpus set</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An illustration of the LLM-augmented text classification phase (Phase 2).
using the taxonomy yielded in Phase 1, then use these labels to train more efficient classifiers at scale. Specifically, we prompt an LLM to infer the primary label (as a multiclass classification task) and all applicable labels (as a multilabel classification task) on a "medium-to-large" scale corpus sample that covers the range of labels in the taxonomy, creating a representative training dataset that can be used to build a lightweight classifier, such as a Logistic Regression model or a Multilayer Perceptron classifier. In this way, we can induce "pseudo labels" from the LLM classifier and transfer its knowledge to a more efficient and manageable model that can be deployed and served at scale. An illustrative figure of this phase is presented in Figure 3.</p>
<h2>4 EVALUATION SUITE</h2>
<p>Due to the unsupervised nature of the problem we study and the lack of a benchmark standard, performing quantitative evaluation on end-to-end taxonomy generation and text classification can be challenging. We therefore design a suite of strategies to evaluate TnT-LLM. Our evaluation strategies may be categorized into three buckets, depending on the type and source of the evaluation criteria. The three categories are as follows:</p>
<ul>
<li>Deterministic automatic evaluation: This type of approach is scalable and consistent, but requires well-defined, gold standard rules and annotations. It is less applicable for evaluating the abstract aspects studied in this paper, such as the quality and usefulness of a label taxonomy.</li>
<li>Human evaluation: These approaches are useful for evaluating the abstract aspects that the automatic evaluations cannot address. However, they are also time-consuming, expensive, and may encounter data privacy and compliance constraints.</li>
<li>LLM-based evaluations: Here, LLMs are used to perform the same or similar tasks as human evaluators. This type of evaluation is more scalable and cost-effective than human evaluation, albeit potentially subject to biases and errors if not applied properly. We therefore aim to combine and validate LLM-based evaluation with human evaluation metrics on small corpora so that we can extrapolate conclusions with sufficient statistical power.</li>
</ul>
<h3>4.1 Phase 1 Evaluation Strategies</h3>
<p>Following prior studies [23, 30], we evaluate a label taxonomy on three criteria: Coverage, accuracy, and relevance to the use-case instruction. Note that we require implementing the native primary label assignment to apply these metrics. For clustering-based methods, this is instantiated through the clustering algorithm. For TnTLLM, this is done by a label assignment prompt as described in Section 3.2. We also note that the label accuracy and use-case relevance metrics discussed here are applicable to both human and LLM raters.
Taxonomy Coverage. This metric measures the comprehensiveness of the generated label taxonomy for the corpus. Conventional text clustering approaches (e.g., embedding-based k-means) often achieve $100 \%$ coverage by design. In our LLM-based taxonomy generation pipeline, we add an 'Other' or 'Undefined' category in the label assignment prompt by design and measure the proportion of data points assigned to this category. The lower this proportion, the higher the taxonomy coverage.
Label Accuracy. This metric quantifies how well the assigned label reflects the text data point, relative to other labels in the same taxonomy. Analogous to mixture model clustering, the primary label should be the most probable one given the text. We assume human and LLM raters can assess the label fit by its name and description. We treat accuracy as a pairwise comparison task: for each text, we obtain the primary label and a random negative label from the same taxonomy, and ask a rater to choose the more accurate label based on their names and descriptions. ${ }^{1}$ If the rater correctly identifies the positive label, we consider it as a "Hit" and report the average hit rate as the label accuracy metric. We do not explicitly evaluate the overlap across category labels and rather expect it to be implicitly reflected in the pairwise label accuracy metric.
Relevance to Use-case Instruction. This metric measures how relevant the generated label taxonomy is to the use-case instruction. For example, "Content Creation" is relevant to an instruction to "understand user intent in a conversation", while "History and Culture" is not. We operationalize this as a binary rating task: for each instance, we provide its primary label name and description to a human or LLM rater, and ask them to decide if the label is relevant to the given use-case instruction or not. Note that we instruct the rater to use the presented instance as the context, and rate the relevance conditioned on the label's ability to accurately describe some aspect of the text input. The goal of this metric is not to evaluate the label accuracy, but rather to rule out the randomness introduced by taxonomies that are seemingly relevant to the use-case instruction, but irrelevant to the corpus sample - and therefore useless for downstream applications.</p>
<h3>4.2 Phase 2 Evaluation Strategies</h3>
<p>To quantitatively evaluate text classification, we create a benchmark dataset with reliable ground-truth annotations as follows:
Task and Annotation Reliability. We first assess the reliability of the label assignment task and the human annotations by</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>involving multiple human annotators and calculating the interrater agreement (Cohen's Kappa [6] between two raters and Fleiss' Kappa [7] among multiple raters). We then resolve disagreements between human annotations by either voting or deliberation, and obtain a consensus human annotation for each instance. Then we use an LLM as an additional annotator to perform the same label assignment task, and measure the agreement between the LLM annotation and the consensus human label. Intuitively, this agreement captures how well the LLM is aligned with (the majority of) human annotators and how reliable it is for this label assignment task.
Classification Metrics. We apply both human and LLM annotations on a small-scale corpus sample and calculate the conventional multiclass and multilabel classification metrics (e.g., Accuracy, F1) with human annotations as the ground truth. These metrics evaluate how the label classifier is aligned with human preferences on a small subset of the corpus. We then apply the LLM annotator on a larger-scale corpus sample and leverage the resulting annotations as the oracle to calculate the same classification metrics. These metrics enable a comprehensive diagnosis of the label classifier performance at scale on different aspects of the corpus, such as domains, languages, and time ranges.</p>
<p>In practice, we recommend leveraging both human evaluation and LLM-based metrics as a holistic evaluation suite, while also taking into account the task and annotation reliability. This approach can help us identify and mitigate the possible bias that may arise from either method or be affected by the task complexity, and enable us to scale up the evaluation and annotation to a large corpus sample with confidence, thus obtaining more robust and informative evaluation results.</p>
<h2>5 EXPERIMENTS</h2>
<p>We showcase the utility of TnT-LLM for two text mining tasks of special interest in today's LLM era: User intent detection and conversational domain labeling over human-AI chat transcripts.</p>
<h3>5.1 Data</h3>
<p>Our conversation transcripts are taken from Microsoft's Bing Consumer Copilot system, which is a multilingual, open-domain generative search engine that assists users through a chat experience. We randomly sample 10 weeks of conversations from 8/6/2023 to 10/14/2023, with 1 k conversations per week for Phase 1, where we perform a random $60 \%-20 \%-20 \%$ split for "learning" the label taxonomy, validation, and testing respectively. We then sample another 5 k conversations per week from the same time range for Phase 2, and apply the same train/validation/test data split.</p>
<p>We perform two steps of filtering to ensure the quality and privacy of the data. First, we apply an in-house privacy filter that scrubs all personal information (e.g., addresses, phone numbers) from the original conversation content. Second, we apply a content filter that removes all conversations that contain harmful or inappropriate content that should not be exposed to annotators or downstream analyses. After applying these filters, we obtain 9,592 conversations for Phase 1 and 48,160 conversations for Phase 2. We leverage the FastText language detector $[11,12]$ to identify the primary language of each conversation, where we find around half of the conversations in our corpus are in English.</p>
<p>In the remainder of this section, we will report results on the following datasets:</p>
<ul>
<li>BingChat-Phase 1-L-Multi: The test set used in the taxonomy generation phase, which includes around 2 k conversations.</li>
<li>BingChat-Phase2-L-Multi: The test set used in the label assignment phase, which includes around 10k conversations.
Besides the above datasets, we also reserve two separate Englishonly conversation datasets to perform human evaluations, with the same privacy and content filter applied.</li>
<li>BingChat-Phase 1-S-Eng includes 200 English conversations to evaluate label taxonomy.</li>
<li>BingChat-Phase2-S-Eng includes 400 English conversations to evaluate label assignment.</li>
</ul>
<h3>5.2 Taxonomy Generation</h3>
<p>5.2.1 Methods. To evaluate the effectiveness of TnT-LLM, we compare it with baseline methods that rely on embedding-based clustering to group conversations and then assigns LLM-generated labels to each cluster. We use two state-of-the-art LLMs, GPT4 (0613) and GPT-3.5-Turbo (0613), as label generators and evaluators, and two different embedding methods, ADA2 ${ }^{2}$ and Instructor-XL [26], to represent the conversations. The methods considered in our experiments are as follows:</p>
<ul>
<li>GPT-4 (TnT-LLM): the proposed TnT-LLM with GPT-4 to perform label taxonomy generation and assignment.</li>
<li>GPT-3.5 (TnT-LLM): the proposed TnT-LLM with GPT-3.5Turbo to perform label taxonomy generation and assignment.</li>
<li>ADA2 + GPT-4: the embedding-based clustering approach where conversations are represented via ADA2 and K-means algorithm is applied to generate clusters. We randomly sample 200 conversations within each cluster, prompt GPT-4 to summarize each conversation, then ask it to produce a label name and description from these summaries, conditioned on the use-case instruction.</li>
<li>ADA2 + GPT-3.5-Turbo: similar to the above method, with GPT-3.5-Turbo as the label generator.</li>
<li>Instructor-XL + GPT-4: similar to the above embeddingbased methods, with InSTructor-XL and GPT-4 as the underlying embedding and the label generator respectively.</li>
<li>Instructor-XL + GPT-3.5-Turbo: similar to the above method, with GPT-3.5-Turbo as the label generator.
Note that all the taxonomies evaluated in this section are fully automatic and do not involve any human intervention.
5.2.2 Implementation Details. We instruct our LLMs to generate 10 intent categories and 25 domain categories for taxonomy generation. Likewise, we learn 10 intent clusters and 25 domain clusters with our embedding-based baselines. We use a minibatch size of 200 for our proposed taxonomy generation pipeline. We also apply a minibatch version of the K-means algorithm in all embeddingbased clustering approaches, where the same batch size is used with a K-means++ [2] initialization. We run 10 different trials of the clustering algorithm and select the best one based on the Silhouette coefficient [21] on the validation set. We also devise a "model" selection prompt, which takes a batch of conversation summaries,</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Use Case</th>
<th style="text-align: center;">Among Humans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLM vs. Human</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall <br> (Fleiss)</td>
<td style="text-align: center;">Avg. pairwise <br> (Cohen)</td>
<td style="text-align: center;">GPT-3.5-Turbo <br> (Cohen)</td>
<td style="text-align: center;">GPT-4 <br> (Cohen)</td>
</tr>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Intent</td>
<td style="text-align: center;">$0.476^{*}$</td>
<td style="text-align: center;">$0.477^{*}$</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">$0.558^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">$0.478^{*}$</td>
<td style="text-align: center;">$0.484^{*}$</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">$0.578^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">Intent</td>
<td style="text-align: center;">$0.466^{*}$</td>
<td style="text-align: center;">$0.481^{*}$</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">$0.520^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.288</td>
</tr>
</tbody>
</table>
<p>Table 1: Inter-rater reliability (Fleiss' Kappa and Cohen's Kappa) among human raters and between LLM raters and the resolved human rating through majority voting. Agreement considered as moderate and above ( $&gt;0.4$ ) are highlighted with *. Evaluation is performed on BingChat-Phase 1-S-Eng.
multiple label taxonomies, a use-case instruction as input, then outputs the index of the taxonomy that best fits the data and the instructional desiderata. We then run TnT-LLM 10 trials and select the best outcome based on its performance on the validation set.
Human Evaluation. To evaluate the quality of generated taxonomies from methods listed above, three of the authors performed the label accuracy and use-case relevance tasks; each conversation was evaluated by all three raters. While raters possessed a high degree of familiarity with the Bing Copilot system, as well as the desired use-cases, they were unaware of the correspondence between methods and their generated labels. The position of the options in the pairwise comparison label accuracy task is also fully randomized. We also use two LLM systems, GPT-4 and GPT-3.5-Turbo, to perform the same evaluation tasks as the human raters. However, we notice that the LLM systems tend to exhibit a position bias [16] for the pairwise comparison task, where they favor one option over another based on its position in the prompt. This bias is more evident when the taxonomy quality is low and the task is more challenging. To mitigate this, we average the results over multiple runs with randomized positions of the options in our experiments.
5.2.3 Results. We first calculate the coverage of the LLM-generated taxonomies on the BingChat-Phase 1-L-Multi dataset, where both LLM systems achieve very high coverage ( $&gt;99.5 \%$ ) on both user intent and conversational domain taxonomies.</p>
<p>We then conduct the accuracy and relevance evaluation tasks to assess the quality of the taxonomies generated by different methods on the small English-only evaluation dataset BingChat-Phase 1S-Eng. We report the inter-rater agreement (Cohen's Kappa [6] between two raters and Fleiss' Kappa [7] among multiple raters) in Table 1. The agreement is moderate ( $\kappa&gt;0.4$ ) on intent and domain accuracy as well as intent relevance, while the agreement on domain relevance is fair (Fleiss' $\kappa=0.379$ ). ${ }^{3}$ Interestingly, for the tasks with moderate agreement, the GPT-4 evaluator agrees more with the human majority than the humans do among themselves. This suggests that GPT-4 can be a consistent and reliable evaluator.</p>
<p>Figure 4a shows the main results on label accuracy and use case relevance from human evaluations on BingChat-Phase1S-Eng. We observe our TnT-LLM using GPT-4 outperforms other</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Human evaluation results on BingChat-Phase1-S-Eng.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) GPT-4 evaluation results on BingChat-Phase 1-L-Multi.</p>
<p>Figure 4: Taxonomy evaluation results on BingChat-Phase 1-S-Eng from human raters and the GPT-4 rater, where error bars indicate $95 \%$ confidence intervals.
methods in most cases. Compared to GPT4, we find that GPT-3.5Turbo tends capture conversation topics (domains) well, but often fails to generate labels that are aligned with the user intent instruction. Likewise, we notice that some embedding methods (Ada2 + GPT-4, instructor-XL + GPT-4) perform well in terms of producing accurate domain labels, on par with TnT-LLM instantiated with GPT-3.5-Turbo, but fail to capture the user intent behind the conversations. This is likely because the domain labels reflect the topical theme of the conversations, which can be easily derived from the semantic information captured by unsupervised embeddings, while intent labels require deeper reasoning and understanding of the use-case instruction.</p>
<p>With regard to our baselines, we find that GPT-4 consistently outperforms GPT-3.5-Turbo in producing more accurate labels when using the same embedding method for clustering. For the intent use-case, GPT-4 generates more relevant labels than GPT-3.5-Turbo, while the difference is less noticeable for the domain use case; again, this may be because GPT-3.5-Turbo is better at capturing topical information in conversations than reasoning about user intent.</p>
<p>Finally, given the high agreement between GPT-4 and human raters on the label accuracy task, we use GPT-4 to evaluate the label accuracy on the larger multilingual dataset BingChat-Phase 1-L-Multi (Figure 4b). We observe similar patterns as those in our</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Use Case</th>
<th style="text-align: center;">Among Humans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLM vs. Human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall <br> (Fleiss)</td>
<td style="text-align: center;">Avg. pairwise <br> (Cohen)</td>
<td style="text-align: center;">GPT-4 <br> (Cohen)</td>
</tr>
<tr>
<td style="text-align: center;">Primary Label</td>
<td style="text-align: center;">Intent <br> Domain</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.553^{<em>} \ &amp; 0.624^{</em> *} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.559^{<em>} \ &amp; 0.624^{</em> *} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.572^{<em>} \ &amp; 0.695^{</em> *} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">All Labels (exact match)</td>
<td style="text-align: center;">Intent <br> Domain</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.422^{<em>} \ &amp; 0.467^{</em>} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.427^{<em>} \ &amp; 0.467^{</em>} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.271 \ &amp; 0.102 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Inter-rater reliability (Fleiss' Kappa and Cohen's Kappa) among human annotators and between LLM annotations and the resolved human annotations. Agreement considered as moderate ( $(0.4,0.6]$ ) are highlighted with * , substantial and above ( $&gt;0.6$ ) are highlighted with ${ }^{<em> </em>}$.
human evaluation, where our TnT-LLM achieves the highest accuracy, and in particular the instantation that uses GPT-4.</p>
<h3>5.3 LLM-Augmented Text Classification</h3>
<p>At the end of the label taxonomy generation phase, we conduct a lightweight human calibration [23] on the intent taxonomy and domain taxonomy generated from TnT-LLM with GPT-4 to improve their clarity. These calibrated taxonomies are then utilized in the label assignment phase. The full label and description texts of each taxonomy are provided in Table 5 and Table 6. As a reminder, our main goal in this section is to compare how distilled lightweight classifiers trained on LLM labels compare to a full LLM classifier; our goal is to achieve a favorable tradeoff of accuracy and efficiency compared to a more expensive but potentially more powerful LLM.
5.3.1 Methods. We apply GPT-4 as an automated annotator to assign both the primary label and any other relevant labels to each conversation in the corpus. We then train classifiers based on the GPT-4 annotated training and validation sets. We extract features from each conversation using two embedding methods: ADA2 and Instructor-XL. For each embedding method, we train three types of classifiers with the GPT-4 labels: Logistic Regression, the gradient boosting LightGBM [13], and a two-layer MultiLayer Perceptron (MLP) [9]. We use multinomial logit in logistic regression for the primary label classification, and a standard 'one-vs-all' scheme for the multilabel classification with all three classifiers.</p>
<p>Additionally, four of the authors manually labeled 400 English conversations (BingChat-Phase2-S-Eng) with the given intent and domain taxonomy. Each conversation was labeled by three annotators, and the majority vote determined the final labels. For a few conversations ( $&lt;10 \%$ ), where all three annotators disagreed on the primary label the fourth annotator was used as a tie-breaker.</p>
<p>We thus obtain two annotated test sets: BingChat-Phase2-SEng with 400 English conversations with both human and GPT-4 annotations, and BingChat-Phase2-L-Multi with around 10k conversations with GPT-4 annotations only.
5.3.2 Results. We first evaluate the agreement between annotators to assess the task complexity and reliability. As Table 2 shows, human annotators have substantial agreement on the primary domain label $(\kappa&gt;0.6)$, and moderate agreement on the primary intent label (Fleiss' $\kappa=0.553$ ). Both of these values indicate a high degree of
mutual understanding among raters and clarity in the instructions and taxonomies. We also note that the domain taxonomy has more categories (25) than the intent taxonomy (10). One might expect a larger taxonomy to be more difficult to comprehend, but we find the smaller intent taxonomy to be more challenging for humans to agree on. We attribute this to the task complexity and ambiguity, as it requires more reasoning; this observation aligns well with our observation in the previous evaluation that GPT4 greatly outperforms GPT-3.5-Turbo on intent detection, as GPT4 is generally considered to be a stronger reasoner.</p>
<p>Similar to the label accuracy evaluation (Table 1), GPT-4 agrees more with the resolved human labels than humans do among themselves on the primary label assignment. We observe that human agreement on all applicable labels is moderate ( $\kappa&gt;0.4$ ) with both intent and domain taxonomies, which is surprisingly good considering such an agreement is calculated based on exact match (i.e., an agreement is counted only if all selected labels are matched). However, the agreement between GPT-4 and human annotations on this task is much lower. A closer inspection reveals that GPT-4 tends to be more liberal than humans on label assignment, applying all relevant categories, resulting in a low precision but high recall.</p>
<p>We then evaluate the classification performance of the distilled embedding-based classifiers on two datasets: BingChat-Phase2-S-Eng, where human annotations are the oracle, and BingChat-Phase2-L-Multi, where GPT-4 annotations are the oracle. The results for the primary label classification are presented in Table 3, where we observe that lightweight embedding-based classifiers can achieve promising results. In particular, ADA2 embeddings achieve strong results with logistic regression; nonlinearity does not seem to improve performance significantly in most cases. When using human annotations as the gold standard, we find that the performance of these lightweight models are comparable to, and sometimes slightly better than, directly using GPT-4 as a classifier on BingChat-Phase2-S-Eng. We also perform evaluation on the multilingual test set BingChat-Phase2-L-Multi, where GPT-4 annotations are considered as oracle. We observe the performance on non-English conversations is lower than that on English conversations (Table 3), especially on the Instructor embedding, indicating the importance of choosing an appropriate embedding method that suits the characteristics of the corpus.</p>
<p>On the multilabel classification task (Table 4), we observe that the distilled classifiers achieve higher precision at the expense of some recall compared to GPT-4. Here, nonlinearity also seems to help more, as MLP-based classifiers achieve the highest accuracy and precision.</p>
<h3>5.4 Summary of Findings and Suggestions</h3>
<p>We have shown that our novel TnT-LLM framework is capable of generating high-quality label taxonomies from unstructured text corpora with very little human instruction or intervention. In our evaluation of this approach on real-world AI chat conversations, we demonstrated that it can be used to find structure and organization in unstructured text. Our method outperforms the conventional embedding-based clustering approach, especially when deeper reasoning beyond surface-level semantics is required. Finally we found that while embedding-based clustering can still be effective, it is</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Oracle</th>
<th style="text-align: center;">Human Annot.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4 Annot.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accur.</td>
<td style="text-align: center;">F1 macro</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">English</td>
<td style="text-align: center;">Non-Eng.</td>
</tr>
<tr>
<td style="text-align: center;">User Intent</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ADA2 +</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LogisticReg</td>
<td style="text-align: center;">$0.658^{2)}$</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">$0.763^{+2.3 \%}$</td>
<td style="text-align: center;">$0.725^{-2.7 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">$0.642^{2)}$</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">$0.716^{+2.8 \%}$</td>
<td style="text-align: center;">$0.686^{-2.3 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.658^{2)}$</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.744</td>
<td style="text-align: center;">$0.762^{+2.4 \%}$</td>
<td style="text-align: center;">$0.722^{-2.9 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">Instructor-XL +</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LogisticReg</td>
<td style="text-align: center;">$0.655^{2)}$</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">$0.745^{+8.4 \%}$</td>
<td style="text-align: center;">$0.619^{-9.9 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">$0.602^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">$0.705^{+8.1 \%}$</td>
<td style="text-align: center;">$0.589^{-9.6 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.650^{2)}$</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">$0.750^{+8.8 \%}$</td>
<td style="text-align: center;">$0.621^{-10.1 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">Conversation Domain</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ADA2 +</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LogisticReg</td>
<td style="text-align: center;">$0.640^{2)}$</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">$0.733^{+3.9 \%}$</td>
<td style="text-align: center;">$0.673^{-4.6 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">$0.560^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">$0.656^{+3.8 \%}$</td>
<td style="text-align: center;">$0.605^{-4.4 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.650^{2)}$</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">$0.731^{+4.1 \%}$</td>
<td style="text-align: center;">$0.669^{-4.8 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">Instructor-XL +</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LogisticReg</td>
<td style="text-align: center;">$0.622^{2)}$</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">$0.711^{+11.3 \%}$</td>
<td style="text-align: center;">$0.553^{-13.3 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">$0.588^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">$0.646^{+10.9 \%}$</td>
<td style="text-align: center;">$0.508^{-12.8 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.648^{2)}$</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">$0.712^{+11.4 \%}$</td>
<td style="text-align: center;">$0.553^{-13.4 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Lightweight distilled classifiers achieve competitive performance compared to a full GPT-4 classifier on Phase 2: Primary label classification results on 1) BingChat-Phase2-S-Eng with human annotations as the oracle and 2) BingChat-Phase2-L-Multi with GPT-4 annotations as the oracle. For BingChat-Phase2-S-Eng, we mark whether the classifier results are significantly higher ( $\hat{\text { ( }}$ ), lower ( $\hat{\text { ) }}$ ), or insignificant ( $\hat{\text { ) }}$ ) than GPT4 by paired t-test ( $p&lt;0.05$ ). For BingChat-Phase2-L-Multi, we indicate the percentage changes for English and non-English conversations compared to the overall result for each classifier.
more susceptible to modeling choices or artifacts, such as cluster granularity and alignment of use-case with inputs.</p>
<p>We further explored the use of LLMs as raters or evaluators, demonstrating that they effectively approximate the collective opinion of humans on some evaluation tasks. Additionally, we found that LLMs excel at single-choice questions (e.g., pairwise label accuracy evaluation task) where they are forced to indicate preference on one option over another, but they can struggle on multiplechoice questions that involve subjective and nuanced judgments with implicit standards. We suggest using LLMs as an alternative strategy for human evaluation, but with caution and verification by measuring agreement with human preferences.</p>
<p>Lastly, we proposed a perspective of using LLMs as "annotators" rather than classifiers, harnessing their ability to create abundant data. By utilizing LLMs to generate pseudo labels for unlabeled data, we can distill a lightweight classifier that can be reliably deployed at scale. In our experiments, such a classifier achieved competitive results, and matched or even surpassed the performance of GPT-4</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accur.</th>
<th style="text-align: center;">Micro</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Macro</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">User Intent</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.743</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.537</td>
</tr>
<tr>
<td style="text-align: center;">ADA2 +</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LogisticReg</td>
<td style="text-align: center;">$0.388^{\dagger}$</td>
<td style="text-align: center;">$0.574^{\dagger}$</td>
<td style="text-align: center;">$0.736^{2)}$</td>
<td style="text-align: center;">0.645</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.537</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">$0.380^{\dagger}$</td>
<td style="text-align: center;">$0.587^{\dagger}$</td>
<td style="text-align: center;">$0.669^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.456</td>
</tr>
<tr>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.418^{\dagger}$</td>
<td style="text-align: center;">$0.599^{\dagger}$</td>
<td style="text-align: center;">$0.657^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.499</td>
</tr>
<tr>
<td style="text-align: center;">Instructor-XL +</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LogisticReg</td>
<td style="text-align: center;">$0.358^{\dagger}$</td>
<td style="text-align: center;">$0.559^{\dagger}$</td>
<td style="text-align: center;">$0.688^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">$0.335^{2)}$</td>
<td style="text-align: center;">$0.557^{\dagger}$</td>
<td style="text-align: center;">$0.644^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.465</td>
</tr>
<tr>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.410^{\dagger}$</td>
<td style="text-align: center;">$0.606^{\dagger}$</td>
<td style="text-align: center;">$0.642^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.495</td>
</tr>
<tr>
<td style="text-align: center;">Conversation Domain</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.110</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.576</td>
</tr>
<tr>
<td style="text-align: center;">ADA2 +</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LogisticReg</td>
<td style="text-align: center;">$0.188^{\dagger}$</td>
<td style="text-align: center;">$0.493^{\dagger}$</td>
<td style="text-align: center;">$0.732^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.589</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.585</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">$0.182^{\dagger}$</td>
<td style="text-align: center;">$0.469^{\dagger}$</td>
<td style="text-align: center;">$0.576^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.452</td>
</tr>
<tr>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.242^{\dagger}$</td>
<td style="text-align: center;">$0.532^{\dagger}$</td>
<td style="text-align: center;">$0.625^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.509</td>
</tr>
<tr>
<td style="text-align: center;">Instructor-XL +</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LogisticReg</td>
<td style="text-align: center;">$0.210^{\dagger}$</td>
<td style="text-align: center;">$0.495^{\dagger}$</td>
<td style="text-align: center;">$0.714^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.574</td>
</tr>
<tr>
<td style="text-align: center;">LightGBM</td>
<td style="text-align: center;">$0.172^{\dagger}$</td>
<td style="text-align: center;">$0.479^{\dagger}$</td>
<td style="text-align: center;">$0.592^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.469</td>
</tr>
<tr>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.262^{\dagger}$</td>
<td style="text-align: center;">$0.550^{\dagger}$</td>
<td style="text-align: center;">$0.602^{\frac{1}{4}}$</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">0.511</td>
</tr>
</tbody>
</table>
<p>Table 4: Lightweight distilled classifiers perform on par with or better than GPT-4 on multilabel classification: Results on BingChat-Phase2-S-Eng using human-annotated gold labels.
as a classifier. We advocate for a careful assessment of the potential use cases of LLMs, balancing performance and efficiency, while exploiting both their power to generalize with the maturity, speed, and cost of conventional machine learning classifiers.</p>
<h2>6 DISCUSSION AND FUTURE WORK</h2>
<p>This work has the potential to create significant impact for research and application of AI technologies in text mining. Our framework has demonstrated the ability to use LLMs as taxonomy generators, as well as data labelers and evaluators. These automations could lead to significant efficiency gains and cost savings for a variety of domains and applications that rely on understanding, structuring and analyzing massive volumes of unstructured text. It could also broadly democratize the process of mining knowledge from text, empowering non-expert users and enterprises to interact with and interpret their data through natural language, thereby leading to better insights and data-driven decision making for a range of industries and sectors. Additionally, our framework and research findings relate to other work that leverages LLMs for taxonomy creation and text clustering, and has important empirical lessons for the efficient use of instruction-following models in these scenarios.</p>
<p>Despite these initial successes, there are some important challenges and future directions that are worth exploring. As we have already noted, LLMs are expensive and slow. In future work, we hope to explore ways to improve the speed, efficiency and robustness of our framework, through hybrid approaches that further</p>
<p>explore the combination of LLMs with embedding-based methods, or model distillation that fine-tunes a smaller model through instructions from a larger one. Evaluation continues to be a crucial and open challenge for future work, and we plan to explore ways of performing more robust LLM-aided evaluations in future work, for example by fine-tuning a model to expand its reasoning capabilities beyond pairwise judgement tasks. While this work has focused largely on text mining in the conversational domain, we also hope to explore the extensibility of our framework to other domains as well. Finally, many domains have ethical considerations from the perspective of privacy and security that must be taken into account when performing large-scale automated text mining, and we hope to engage with these challenges more deeply in future work.</p>
<h2>REFERENCES</h2>
<p>[1] Charu C Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. Mining text data (2012), 77-128.
[2] David Arthur, Sergei Vassilvitskii, et al. 2007. k-means++: The advantages of careful seeding. In Soda, Vol. 7. 1027-1035.
[3] Lon Bottou. 1998. Online algorithms and stochastic approximations. Online learning in neural networks (1998).
[4] B Barla Cambazoglu, Leila Tavakoli, Falk Scholer, Mark Sanderson, and Bruce Croft. 2021. An intent taxonomy for questions asked in web search. In Proceedings of the 2021 Conference on Human Information Interaction and Retrieval. 85-94.
[5] Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. Advances in neural information processing systems 22 (2009).
[6] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement 20, 1 (1960), 37-46.
[7] Joseph L Fleiss and Jacob Cohen. 1973. The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability. Educational and psychological measurement 33, 3 (1973), 613-619.
[8] Fabrizio Gilardi, Meysam Alizadeh, and Mait Kubli. 2023. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056 (2023).
[9] Simon Haykin. 1998. Neural networks: a comprehensive foundation. Prentice Hall PTR.
[10] Andreas Hotho, Andreas Nrnberger, and Gerhard Paa. 2005. A brief survey of text mining. Journal for Language Technology and Computational Linguistics 20, 1 (2005), 19-62.
[11] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hrve Jgou, and Tomas Mikolov. 2016. FastText.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651 (2016).
[12] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of Tricks for Efficient Text Classification. arXiv preprint arXiv:1607.01759 (2016).
[13] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems 30 (2017).
[14] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. CoRR abs/1412.6980 (2014). https://api.semanticscholar.org/CorpusID: 6628106
[15] Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, and Sujay Jauhar. 2023. Making Large Language Models Better Data Creators. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 15349-15360. https://doi.org/10.18653/v1/2023.emnlp-main. 948
[16] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172 (2023).
[17] Geoffrey J McLachlan and Kaye E Basford. 1988. Mixture models: Inference and applications to clustering. Vol. 38. M. Dekker New York.
[18] Chau Minh Pham, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. 2023. TopicGPT: A Prompt-based Topic Modeling Framework. arXiv preprint arXiv:2311.01449 (2023).
[19] Reid Prysant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic Prompt Optimization with "Gradient Descent" and Beam Search. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for</p>
<p>Computational Linguistics, Singapore, 7957-7968. https://doi.org/10.18653/v1/ 2023.emnlp-main. 494
[20] Daniel E Rose and Danny Levinson. 2004. Understanding user goals in web search. In Proceedings of the 13th international conference on World Wide Web. $13-19$.
[21] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics 20 (1987), 53-63.
[22] Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia 6, 12 (2008), e26752.
[23] Chirag Shah, Ryen W White, Reid Andersen, Georg Buscher, Scott Counts, Sarkar Snigdha Sarathi Das, Ali Montazer, Sathish Manivannan, Jennifer Neville, Xiaochuan Ni, et al. 2023. Using large language models to generate, validate, and apply user intent taxonomies. arXiv preprint arXiv:2309.13063 (2023).
[24] Jingbo Shang, Xinyang Zhang, Liyuan Liu, Sha Li, and Jiawei Han. 2020. Nettaxo: Automated topic taxonomy construction from text-rich network. In Proceedings of the Web Conference 2020. 1908-1919.
[25] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2015 conference on empirical methods in natural language processing. 1631-1642.
[26] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022. One Embedder, Any Task: Instruction-Finetuned Text Embeddings. https://arxiv.org/abs/2212.0 9741
[27] Ah-Hwee Tan et al. 1999. Text mining: The state of the art and the challenges. In Proceedings of the PAKDD 1999 workshop on knowledge discovery from advanced databases, Vol. 8. 65-70.
[28] Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2023. Large language models can accurately predict searcher preferences. arXiv preprint arXiv:2309.10621 (2023).
[29] Iler Vayansky and Sathish AP Kumar. 2020. A review of topic modeling methods. Information Systems 94 (2020), 101582.
[30] Zihan Wang, Jingbo Shang, and Ruiqi Zhong. 2023. Goal-Driven Explainable Clustering via Language Descriptions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 10626-10649. https://doi.org/10.18653/v1/2023.emnlp-main. 657
[31] Anuradha Welivita and Pearl Pu. 2020. A Taxonomy of Empathetic Response Intents in Human Social Conversations. In Proceedings of the 28th International Conference on Computational Linguistics. 4886-4899.
[32] Qingkai Zeng, Jinfeng Lin, Weizhao Yu, Jane Cleland-Huang, and Meng Jiang. 2021. Enhancing taxonomy completion with concept generation via fusing relational representations. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining. 2104-2113.
[33] Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler, Michelle Vanni, and Jiawei Han. 2018. Taxogen: Unsupervised topic taxonomy construction by adaptive term embedding and clustering. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining. 2701-2709.
[34] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2023. Can Large Language Models Transform Computational Social Science? arXiv preprint arXiv:2305.03514 (2023).</p>
<h2>A TAXONOMIES</h2>
<p>The user intent taxonomy and conversation domain taxonomy used in the label assignment phase are provided in Tables 5 and 6 . Note although the label name and the majority of label description are automatically generated through our TnT-LLM framework, we did perform a lightweight human calibration on these generated taxonomies and added artificial examples. These examples are purely for illustration purpose and do not link to any particular data point in our corpus.</p>
<h2>B ADDITIONAL RESULTS</h2>
<p>We present additional results from the experiments conducted for taxonomy generation phase and the label assignment phase.</p>
<h2>B. 1 Phase 1: Taxonomy Generation</h2>
<p>In addition to the taxonomy evaluation results on BingChat-Phase 1-S-Eng, we also investigate how the label taxonomy outcome from our proposed TnT-LLM framework perform across different languages. We present the label accuracy results from the GPT-4 rator in Figure 5, where we generally do not find significant differences of its performance on English conversations and non-English conversations.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Taxonomy evaluation results by language on multilingual conversations (BingChat-Phase 1-L-Multi) from the GPT-4 rater.</p>
<h2>B. 2 Phase 2: Label Assignment</h2>
<p>B.2.1 Annotation Agreement Analysis. We conduct in-depth investigation on the agreement results among human annotators and the LLM annotator for the label assignment task. The agreement results between different pairs of human annotators are presented in Figure 6. The confusion matrix between the GPT-4 annotations and (resolved) human annotations for the primary label on BingChat-Phase2-S-Eng dataset is provided in Figure 7. We notice that for user intent, most disagreements occur at the boundary between "Fact-based information seeking" and "Clarification and concept explanation", "General solution and advice seeking" and "Technical assistance and problem solving". This suggests that human annotators and the GPT-4 annotator have different judgments on how "technical" or how much elaboration a user query requires. Note all our human annotators have high technical expertise, which may lead them to apply different implicit standards than the general population, resulting in potentially biased annotations. We observe similar patterns in the domain label assignment task, where "General digital support" and "Software development and hardware issues" are often confused, and the GPT-4 annotator has a high false positive rate on the "Software development and hardware
issues" if human annotations are considered as oracle. We argue that this kind of analysis can help us identify and reduce potential biases in both human annotations and LLM annotations, and thus improve the clarity of the label description in the taxonomy and the consistency of label annotations.
B.2.2 Full Classification Results. We present the full multiclass classification results from predicting the primary label of a conversation in Figure 11, the full multilabel classification results from predicting all applicable labels in Figure 12, and the by language classification results in Figure 13. We confirm that the conclusions in Section 5.3 still hold.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Pairwise agreement (in Cohen's Kappa) between human annotators on the label assignment task.</p>
<h2>C IMPLEMENTATION DETAILS</h2>
<h2>C. 1 Pipeline Design and Detailed Techniques</h2>
<p>We discuss the details of our LLM-based framework in this section. The rationale of these design details is to ensure that our proposed framework is executable, robust, and can be validated via quantitative metrics.
Executability and Robustness. A key challenge is how to reliably execute the framework, especially when a prompt chain is involved where the states are dependent on the previous outputs. To address this, we explicitly state the output format in our prompts using predefined xml tags, such as "<output>output taxonomy in markdown table format</output>". This allows us to parse the outcomes from each step of the prompt chain and feed them to the next step. Moreover, we instruct the LLMs to format the taxonomy as a markdown table with a predefined schema, which includes the name, description, and index of each label. By asking the LLMs to output the name and the index of the assigned label together, we improve the consistency of the label assignment outputs and reduce the potential post-processing effort.</p>
<p>However, we acknowledge that LLMs may not always follow the format instruction perfectly. Therefore, we propose the following strategy to increase the robustness of the pipeline execution. Specifically, we design a few guardrail tests for each type of LLM prompts. These tests include: 1) checking whether the output from a prompt adheres to the specified format that can be successfully parsed; 2) verifying whether the output is in the correct language (English) specified in the prompt, especially for the summarization prompt; 3) ensuring whether the output satisfies a key verifiable requirement given in the prompt instruction, such as the maximum number of labels in the output taxonomy. These metrics not only measure the instruction-following ability of an LLM system, but also provide</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: The confusion matrix of the primary labels assigned by human annotators and the GPT-4 annotator.
a quality assurance test suite to enhance the executability of the framework.</p>
<p>We also specify a maximum number of retries ( 5 in our experiments) and a base temperature for each LLM call. If the outcome from an LLM call cannot pass the guardrail tests, we increase the temperature by 0.1 and allow it to try again until reaching the limit. Although there are still cases where LLMs fail to follow the instruction after exhausting the retry quota, empirically we find that this strategy largely increases the executability of our LLM-based framework.
"Model" Selection. We draw inspiration from the practice of applying stochastic gradient descent in classic machine learning optimization. Our taxonomy generation and update pipeline does not guarantee the convergence to a global, but we can leverage an external validation step to perform 'model' selection in a more principled way. To this end, we devise an evaluation prompt that takes as input a pair of or multiple taxonomies, a batch of text summaries, a use case instruction along with the taxonomy requirements, and then outputs the index of the taxonomy that best fits the data and complies with the requirements. ${ }^{4}$ We apply the evaluation prompt on a validation set that is separate from the training set used by the update prompts. After each or every few update steps, we compare the updated taxonomy and the best taxonomy that has been tracked on the validation set using the evaluation prompt. Once the update prompt chain is completed, the best taxonomy is passed to the final review step. This process simulates the conventional stochastic optimization practices, where the 'early stopping' criteria can also be applied.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Efficiency Analysis and Sample Size Suggestion. The efficiency of our pipeline depends on the choice of the corpus sample size and the LLM system for each phase. For the taxonomy generation phase (Phase 1), we suggest using a 'small-to-medium' size corpus sample that is representative of the whole corpus. The sample size $(N)$ should be large enough to capture the diversity of the corpus, but not too large to incur unnecessary computational costs. In our experiments, we found that a sample size around 10k was sufficient to produce a high quality taxonomy with no more than 100 labels. The most computationally intensive stage of this phase is the summarization stage, which requires calling an LLM at least $N$ times to generate summaries for the entire corpus sample. This stage can be skipped if the input texts are short and normative, or replaced by a cheaper or more specialized summarization model. The generation and update prompt chain requires an LLM system with high reasoning capacity and large context window. We used GPT-4 (with 32k context window) and GPT-3.5-Turbo (with 16k context window) in our experiments, and was able to achieve efficiency with proper batching (with a batch size of 200). We observed that GPT-3.5-Turbo was $5 \mathrm{x}-10 \mathrm{x}$ faster than GPT-4, but may compromise the quality of the final label taxonomy outcome.</p>
<p>For the label assignment and classifier development phase (Phase 2), we recommend using a 'medium-to-large' size corpus sample that covers the range of labels in the taxonomy. The sample size needed also depends on the difficulty of the classification task and the effectiveness of the representation model used. Since this phase involves applying an LLM on the entire sample, we suggest starting with a 'medium' size sample for model development, and increasing it as needed.</p>
<p># Instruction
## Context</p>
<ul>
<li><strong>Goal</strong>: Summarize the input text for the given use case.</li>
<li><strong>Data</strong>: Your input data is a conversation history between a User and an AI agent.</li>
<li><strong>Use case</strong>: [use_case]
# Data
{{input_text}}
# Questions
## Q1. Provide a summary of the input text <strong>in ((summary_length))
words or less</strong> for the use case.
Tips</li>
<li>The summary will represent the input data for clustering in the next step.</li>
<li>Be concise and clear. Do not add phrases like "This is the summary of the data ..." or "Summarized text: ...".</li>
<li>Within ((summary_length)) words, include the relevant information for the use case in the summary as much as possible.</li>
<li>Do not include any line breaks in the summary.</li>
<li>Provide your answer in <strong>English</strong> only.
## Q2. Explain how you wrote the summary <strong>in ((explanation_length)) words or less</strong>.
## Provide your answers between the tags: <summary>your answer to Q1</summary>. <explanation>your answer to Q2</explanation>.
# Output</li>
</ul>
<h2>Figure 8: Conversation summarization prompt (Stage 1 in Phase 1).</h2>
<p># Instruction
## Context</p>
<ul>
<li><strong>Goal</strong>: Your goal is to classify the input data using the provided reference table.</li>
<li><strong>Reference table</strong>: The input reference table is a markdown table with each row as a category, with the following columns:</li>
<li><strong>id</strong>: category index.</li>
<li><strong>name</strong>: category name.</li>
<li><strong>description</strong>: category description used to classify data points.</li>
<li><strong>Data</strong>: Your input data is a conversation history between a User and an AI agent.
# Reference table
{{reference_table}}
# Data
{{input_text}}
# Questions
## Please classify the input data using the reference table. Your output should include the following information:</li>
<li><strong>category-id</strong>: <strong>id</strong> of a category in the reference table; if unable to classify using the reference table, please output "-1".</li>
<li><strong>category-name</strong>: <strong>name</strong> of a category in the reference table that corresponds to the <strong>category-id</strong>; if unable to classify using the reference table, please output "Undefined".</li>
<li><strong>explanation</strong>: a short explanation of why you think the input data belongs to the category or you cannot classify the data into any of the given categories. You explanation should be within [[explanation_length]] words.
Tips</li>
<li>You should only output the <strong>primary</strong> category for the input data. If it can be classified into multiple categories, please output <strong>the most relevant category</strong>.</li>
<li>Your output should be in "English" only.
## Please provide your answers between the tags: <category-id>your identified category id</category-id>. <category-name>your identified category name</category-name>. <explanation>your explanation</explanation>.
# Output</li>
</ul>
<p>Figure 9: Label assignment prompt (Phase 2).</p>
<h2>C. 2 Experiment Details</h2>
<p>LLM Configurations. We used the following fixed parameter configurations for all prompts applied in this work: frequency_penalty=0,
presence_penalty=0, top_p=0.5. We purposely apply a higher temperature for the taxonomy generation prompt chain to elicit the generation power of LLMs. The base temperature is set to 0.5 for the "generation" prompt, and 0.2 for the "update" prompt. Base temperature is set to 0 for all other prompts in our experiments.
Hyperparameter Selection. For classifiers presented in Section 5.3, we perform grid search based on their accuracy performance on the validation set as the follows.</p>
<ul>
<li>Logistic Regression: An $\ell_{2}$ regularizor is applied and $\lambda$ is selected from $[0.01,0.1,1,10]$.</li>
<li>LightGBM: We use the default number of leaves (31) in the official LightGBM package and the maximum depth is selected from $[3,5,7,9]$.</li>
<li>MLP: We apply an Adam [14] optimizer with weight decay set to $1 \mathrm{e}-5$ and a learning rate 0.001 . The size of the hidden layer is selected from $[32,64,128,256]$.
Instruction Following Results. In addition to the results reported in Sections 5.2 and 5.3, we also evaluate the instruction following ability of the two LLM systems applied in our experiments. For the first summarization stage of our proposed taxonomy generation pipeline (Stage 1 in Section 3.1), we primarily evaluate 1) if the output can be successfully parsed based on the predefined format in the prompt (i.e., format check) and 2) if the output complies with the language specified in the prompt (i.e., English). We found that GPT4 performed flawlessly, passing $100 \%$ of the format and language checks. GPT-3.5-Turbo, on the other hand, had a very low failure rate for the format check ( $&lt;0.01 \%$ ) and a slightly higher failure rate for the language check (around 2\%). However, we also notice that $0.3 \%$ of GPT-3.5-Turbo outputs passed the strict format check, but copied the instruction into the XML tags. Given the overall instruction following success rate is high and our taxonomy generation pipeline is relatively robust to minor perturbations of the input batch, we discard the conversations that did not pass the instruction following test for GPT-3.5-Turbo in the subsequent stage. For the taxonomy generation and update stage (Stage 2 in Section 3.1), we evaluate if the prompt chain can successfully complete for each of 10 epoch runs, which requires that all the intermediate taxonomy outcomes 1) can be successfully parsed (i.e., format check) and 2) comply with the predefined taxonomy size limit (i.e., max number of generated labels). GPT-4 again performed flawlessly, completing 10 out of 10 epochs for both taxonomies. GPT-3.5-Turbo, however, struggled on this stage, primarily because of it persistently exceeding the taxonomy size limit in the 'Update' step. At the end, it only completed 4 out of 10 epochs for the intent taxonomy and 1 out of 10 epochs for the domain taxonomy. For the native label assignment stage, we find both GPT-4 and GPT-3.5-Turbo are able to pass the format check close to $100 \%$.</li>
</ul>
<h2>D PROMPT TEMPLATES</h2>
<p>In this section, we present the prompt templates that were used for conversation summarization (Figure 8), label assignment (Figure 9), and taxonomy generation (Figure 10a), updation (Figure 10b) and review (Figure 10c).</p>
<h2>Instruction</h2>
<h2>Context</h2>
<ul>
<li><strong>Goal</strong>: You goal is to cluster the input data into meaningful categories for the given use case.</li>
<li><strong>Data</strong>: The input data will be a markdown table with summaries for a list of human-dl conversations, including the following columns:</li>
<li><strong>id</strong>: conversation index.</li>
<li><strong>text</strong>: conversation summary.</li>
<li><strong>Use case</strong>: [use_case]</li>
</ul>
<h2>Requirements</h2>
<h2>Format</h2>
<ul>
<li>Output clusters as a ** markdown table** with each row as a category, with the following columns:</li>
<li><strong>id</strong>: category index starting from 1 in an incremental manner.</li>
<li><strong>name</strong>: category name should be <strong>within [cluster_name_length]</strong>
words<em>. It can be either </em>verb phrase<em> or </em>noun phrase*, whichever is more appropriate.</li>
<li><strong>description</strong>: category description should be <strong>within [cluster_description_length] words</strong>.
Here is an example of your output:</li>
<li>markdown
[filename]description]
$+[+\mid+]$
[category id]category name[category description]</li>
<li>Total number of categories should be <strong>no more than
[man_num_clusters]</strong>.</li>
<li>Output table should be in <strong>English</strong> only.</li>
</ul>
<h2>Quality</h2>
<ul>
<li><strong>No overlap or contradiction</strong> among the categories.</li>
<li><strong>Name</strong> in a concise and clear label for the category. One only phrases that are specific to each category and avoid those that are common to all categories.</li>
<li><strong>Description</strong> differentiates one category from another.</li>
<li><strong>Name</strong> and <strong>description</strong> can <strong>accurately</strong> and <strong>consistently</strong> classify new data points <strong>without ambiguity</strong>.</li>
<li><strong>Name</strong> and <strong>description</strong> are <em>consistent with each other</em>. Output clusters match the data as closely as possible, without missing important categories or adding unnecessary ones.</li>
<li>Output clusters serve the given use case well.</li>
<li>Output clusters should be specific and meaningful. Do not invent categories that are not in the data.</li>
</ul>
<h2>Data</h2>
<p>[data_table]
# Questions
## Q1. Please generate a cluster table from the input data that meets the requirements.
Tips
- The cluster table should be a <strong>flat list</strong> of <strong>mutually exclusive</strong> categories. Sort them based on their semantic relatedness.
- You can have <em>lower than [man_num_clusters] categories</em> in the cluster table, but <strong>do not exceed the limit.</strong>
- On <strong>specific</strong> about each category, <strong>Do not include vague
categories</strong> such as "Other", "General", "Unclear", "Miscellaneous" or "Undefined" in the cluster table.
- You can ignore low quality or ambiguous data points.
## Q2. Why did you cluster the data the way you did? Explain your reasoning <strong>within [implanation_length] words</strong>.
## Provide your answers between the tags: <cluster_table>your generated cluster table with no more than [man_num_clusters] categories&gt;[cluster_table&gt;. <explanation>explanation of your reasoning process within [implanation_length] words</explanation>.
# Output</p>
<h2>Instruction</h2>
<h2>Context</h2>
<ul>
<li><strong>Goal</strong>: You goal is to review the given reference table based on the input data for the specified use case, then update the reference table if needed.</li>
<li>You will be given a reference cluster table, which is built on existing data. The reference table will be used to classify new data points.</li>
<li>You will compare the input data with the reference table, output a rating score of the quality of the reference table, suggest potential edits, and update the reference table if needed.</li>
<li><strong>Reference cluster table</strong>: The input cluster table is a markdown table with each row as a category, with the following columns:</li>
<li><strong>id</strong>: category index.</li>
<li><strong>name</strong>: category name.</li>
<li><strong>description</strong>: category description used to classify data points.</li>
<li><strong>Data</strong>: The input data will be a markdown table with summaries for a list of human-dl conversations, including the following columns:</li>
<li><strong>id</strong>: conversation index.</li>
<li><strong>text</strong>: conversation summary.</li>
<li><strong>Use case</strong>: [use_case]</li>
</ul>
<h2>Requirements</h2>
<p>[name as in the generation prompt]
# Reference cluster table
[cluster_table]
# Data
[data_table]</p>
<h2>Questions</h2>
<p>## Q1: Review the given reference table and the input data and provide a rating score of the reference table. The rating score should be an integer between 0 and 100. higher rating score means better quality. You should consider the following factors when rating the reference table:
- <strong>Intrinsic quality</strong>: - [if the cluster table meets the "Requirements" section, with clear and consistent category names and descriptions, and no overlap or contradiction among the categories]
- [if the categories in the cluster table are relevant to the the given use case]
- [if the cluster table includes any vague categories such as "Other", "General", "Unclear", "Miscellaneous" or "Undefined".
- <strong>Extrinsic quality</strong>: - [if the cluster table can accurately and consistently classify the input data without ambiguity]
- [if there are missing categories in the cluster table but appear in the input data]
- [if there are unnecessary categories in the cluster table that do not appear in the input data.]</p>
<p>## Q2: Explain your rating score in Q1 <strong>within [explanation_length] words</strong>.
## Q3: Based on your review, decide if you need to edit the reference table to improve its quality. If yes, suggest potential edits <strong>within [suggestion_length] words</strong>. If no, please output 'Wik'
Tips
- You can edit the category name, description, or remove a category. You can also merge or add new categories if needed. Your edits should meet the "Requirements" section.
- The cluster table should be a <strong>flat list</strong> of <strong>mutually exclusive</strong> categories. Sort them based on their semantic relatedness.
- You can have <em>lower than [man_num_clusters] categories</em> in the cluster table, but <strong>do not exceed the limit.</strong>
- On <strong>specific</strong> about each category, <strong>Do not include vague
categories</strong> such as "Other", "General", "Unclear", "Miscellaneous" or "Undefined" in the cluster table.
- You can ignore low quality or ambiguous data points.</p>
<p>## Q4: If you decide to edit the reference table, please provide your updated reference table. If you decide not to edit the reference table, please output the original reference table.
(b) Taxonomy update prompt.</p>
<h2>(c) Taxonomy review prompt.</h2>
<p>Figure 10: Label taxonomy generation, update and review prompts (Stage 2 in Phase 1).</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" />
(a) English only dataset BingChat-Phase2-5-Eng with human annotations as the oracle. Dashed line indicates the result from the GPT-4 classifier.
<img alt="img-9.jpeg" src="img-9.jpeg" />
(b) Multilingual dataset BingChat-Phase2-L-Multi with GPT-4 annotations as the oracle.</p>
<p>Figure 11: Results from predicting the primary label.
<img alt="img-10.jpeg" src="img-10.jpeg" />
(a) English only dataset BingChat-Phase2-5-Eng with human annotations as the oracle. Dashed line indicates the result from the GPT-4 classifier.
<img alt="img-11.jpeg" src="img-11.jpeg" />
(b) Multilingual dataset BingChat-Phase2-L-Multi with GPT-4 annotations as the oracle.</p>
<p>Figure 12: Results from predicting all applicable labels.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" />
(b) All applicable labels by language, GPT-4 annotation as oracle</p>
<p>Figure 13: Results by language (English vs. non-English conversations) from predicting both the primary label and all applicable with GPT-4 annotations as the oracle on the large multilingual test set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Label Name</th>
<th style="text-align: center;">Label Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Website Navigation Requests</td>
<td style="text-align: center;">User seeks to visit a very specific website or web page by providing a URL, or keywords that "indicate the name or domain of the website", e.g., "amazon.com", "gmail login".</td>
</tr>
<tr>
<td style="text-align: center;">Fact-Based Information Seeking</td>
<td style="text-align: center;">User seeks factual and descriptive information on a specific topic, product, or service. These user queries can be answered by retrieving the factual information that "already exists in the sources" and require "a high level of specificity" and "low level of subjectivity", e.g., "What is the capital of France?".</td>
</tr>
<tr>
<td style="text-align: center;">Clarification and Concept Explanation</td>
<td style="text-align: center;">User asks AI to explain various topics or concepts, or seeks clarification or confirmation on a matter, by providing a question that requires more than a factual or a descriptive answer, but rather "an interpretation, definition, or elaboration", e.g., "What is the difference between AI and machine learning?".</td>
</tr>
<tr>
<td style="text-align: center;">General Solution and Advice Seeking</td>
<td style="text-align: center;">User seeks general solutions, advice, instructions, or steps on a "non-technical" topic, product, or service, by providing a problem, goal, or scenario that requires more than a factual or descriptive answer, but rather "a recommendation, suggestion, or guidance", e.g., "What should I buy for my friend's birthday?".</td>
</tr>
<tr>
<td style="text-align: center;">Technical Assistance and Problem Solving</td>
<td style="text-align: center;">User seeks help with "technical" issues or problem-solving related to a product, service, or system, by providing a description of the issue, error, or challenge that requires more than a factual or descriptive answer, but rather "a diagnosis, solution, or workaround", e.g., "How to fix the bug in my code?".</td>
</tr>
<tr>
<td style="text-align: center;">Language Translation Requests</td>
<td style="text-align: center;">User requests translation or interpretation of a phrase or sentence "from one language to another", e.g., "Hello' in Spanish".</td>
</tr>
<tr>
<td style="text-align: center;">Content Creation and Storytelling Requests</td>
<td style="text-align: center;">User requests the "creation of original content" such as images, stories, instructions, summaries, or narratives on a specific topic or theme, e.g., "Create an image of a unicorn in a forest".</td>
</tr>
<tr>
<td style="text-align: center;">Planning and Scheduling</td>
<td style="text-align: center;">User seeks assistance with planning an event, trip, or schedule, e.g., "Plan a birthday party for my mom".</td>
</tr>
<tr>
<td style="text-align: center;">Data Analysis and Calculation Requests</td>
<td style="text-align: center;">User asks for quantitative data analysis, calculations, or statistical interpretations, by providing the source of the data and the desired operation or result, e.g., "Calculate the average of these numbers", "Analyze the sales data for last quarter".</td>
</tr>
<tr>
<td style="text-align: center;">Greetings and Social Interactions</td>
<td style="text-align: center;">User greets the AI agent or engages in social interactions, by providing a salutation, expression, or remark, or requesting to play games with the AI, which "does not require a factual, descriptive, or technical answer", but rather an engaging, polite or humorous response, e.g., "Hello, how are you?", "You're very smart".</td>
</tr>
</tbody>
</table>
<p>Table 5: The user intent taxonomy used in the label assignment experiments. Note all presented examples are artificial and do not link to any particular data point in our corpus.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Label Name</th>
<th style="text-align: center;">Label Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Academic Resources</td>
<td style="text-align: center;">Requests for educational resources, explanations of "general" academic concepts, and academic advice, e.g., "Best college for computer science", "How to prepare for the SAT?".</td>
</tr>
<tr>
<td style="text-align: center;">Linguistics and Language Learning</td>
<td style="text-align: center;">Requests for translations, text editing, or discussions about grammar, syntax, and other linguistic concepts, e.g., "Translate 'Hello' to French", "What is the difference between 'affect' and 'effect'?".</td>
</tr>
<tr>
<td style="text-align: center;">Mathematics, Logics and Data Science</td>
<td style="text-align: center;">Queries and discussions related to concepts, theories, and problems in the fields of mathematics and logics, or related to machine learning and data science, e.g., "How to calculate standard deviation?", "What is the difference between boosting and bagging?".</td>
</tr>
<tr>
<td style="text-align: center;">Physics and Chemistry</td>
<td style="text-align: center;">Queries and discussions related to concepts, theories, and problems in the fields of physics and chemistry, e.g., "What is the speed of light?", "What is the atomic number of carbon?".</td>
</tr>
<tr>
<td style="text-align: center;">Business and Industry</td>
<td style="text-align: center;">Discussions about "business operations", "industry developments", and related information, e.g., "What is the best business strategy for a startup?", "Generate a FAQ page for a healthcare product website".</td>
</tr>
<tr>
<td style="text-align: center;">Economics and Finance</td>
<td style="text-align: center;">Discussions about economic concepts and theories, financial products, investment advice, and related queries, e.g., "What is the current inflation rate?", "What is the best investment strategy in 2024?".</td>
</tr>
<tr>
<td style="text-align: center;">Job and Career Advice</td>
<td style="text-align: center;">Requests for job applications, career advice, and related information, e.g., "What is the best career path for a data scientist?".</td>
</tr>
<tr>
<td style="text-align: center;">Legal and Regulatory Information</td>
<td style="text-align: center;">Queries about legal terms, regulations, and related information, e.g., "What is the legal drinking age in the US?", "What are the regulations for AI development in EU countries?".</td>
</tr>
<tr>
<td style="text-align: center;">Art, Design and Creativity</td>
<td style="text-align: center;">Requests for "image creation and creative writing", or discussions about "art, design and creative concepts", e.g., "Create a logo for my company", "What is the difference between modern art and contemporary art?".</td>
</tr>
<tr>
<td style="text-align: center;">Entertainment, Media, and Gaming</td>
<td style="text-align: center;">Discussions about movies, music, games, game development, and other forms of entertainment, e.g., "Who is the director of the movie 'Oppenheimer'?".</td>
</tr>
<tr>
<td style="text-align: center;">Interactive Activities with AI</td>
<td style="text-align: center;">Requests for playing games, or engaging in "interactive activities with the AI", e.g., "Play a game with me", "Tell me a joke".</td>
</tr>
<tr>
<td style="text-align: center;">Personal Lifestyle and Hobbies</td>
<td style="text-align: center;">Conversations about "personal" hobbies, lifestyle choices, and individual interests, e.g., "How to learn to play the guitar as a beginner?".</td>
</tr>
<tr>
<td style="text-align: center;">Sports and Fitness</td>
<td style="text-align: center;">Conversations about "sports events", "fitness advice", and related topics, e.g., "Who will play in the NBA finals?", "Training tips for marathon".</td>
</tr>
<tr>
<td style="text-align: center;">Food and Nutrition</td>
<td style="text-align: center;">Conversations about food recommendations, nutritional information, and cooking advice., e.g., "How to make a pizza?".</td>
</tr>
<tr>
<td style="text-align: center;">Health and Wellness</td>
<td style="text-align: center;">Discussions about health conditions, treatments, and wellness information, e.g., "Is cancer curable?", "Best practices to improve sleep quality".</td>
</tr>
<tr>
<td style="text-align: center;">General Digital Support</td>
<td style="text-align: center;">Conversations related to the AI's abilities, limitations, functionality, task requests, and technical support for "general" digital products or services, e.g., "What can Bing Chat do?", "How to take a screenshot on macbook?".</td>
</tr>
<tr>
<td style="text-align: center;">Software Development and Hardware Issues</td>
<td style="text-align: center;">Conversations about "coding", "software configuration", "development tools", and specific software or "hardware issues" and their solutions, e.g., "How to install python on macbook?", "How to fix a broken external hard drive?".</td>
</tr>
<tr>
<td style="text-align: center;">Home and Household Issues</td>
<td style="text-align: center;">Queries about home maintenance, household issues, and related advice, e.g., "How to clean a microwave oven?".</td>
</tr>
<tr>
<td style="text-align: center;">Animals and Nature</td>
<td style="text-align: center;">Queries about animals, nature, and related information, e.g., "What is the pH value of water?", "What is the average lifespan of a cat?".</td>
</tr>
<tr>
<td style="text-align: center;">Geography, Climate and Environment</td>
<td style="text-align: center;">Queries about geographical facts, weather conditions, climate information, environmental issues and related topics, e.g., "What is the most populous city in the world?", "What is the weather like in Seattle?".</td>
</tr>
<tr>
<td style="text-align: center;">History and Culture</td>
<td style="text-align: center;">Queries about historical events, cultural practices, and related topics, e.g., "What is the date of the French Revolution?".</td>
</tr>
<tr>
<td style="text-align: center;">Personal Counseling and Emotional Support</td>
<td style="text-align: center;">Conversations seeking emotional support, personal relationship advice, and life guidance, e.g., "How to deal with a breakup?".</td>
</tr>
<tr>
<td style="text-align: center;">Social and Political Issues</td>
<td style="text-align: center;">Conversations about social issues, political events, and related topics, e.g., "Who are the candidates for the 2024 US presidential election?".</td>
</tr>
<tr>
<td style="text-align: center;">Product and Shopping Queries</td>
<td style="text-align: center;">Requests for product suggestions, comparisons, online shopping, product availability, and other related information about consumer goods, e.g., "What is the best laptop for gaming?", "Compare different models of iPhone".</td>
</tr>
<tr>
<td style="text-align: center;">Travel and Tourism</td>
<td style="text-align: center;">Queries about travel plans, tourist destinations, travel-oriented cultural tips and related information, e.g., "Plan a 5-day trip to Hawaii", "Best place to visit in Paris".</td>
</tr>
</tbody>
</table>
<p>Table 6: The conversation domain taxonomy used in the label assignment experiments. Note all presented examples are artificial and do not link to any particular data point in our corpus.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Note to mitigate the potential position bias [16] in such kind of single-choice or pairwise selection evaluation, we always randomize the position of each option and run the evaluation multiple times in all of our experiments.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
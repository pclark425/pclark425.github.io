<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3214 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3214</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3214</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e" target="_blank">End-To-End Memory Networks</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A neural network with a recurrent attention model over a possibly large external memory that is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3214.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3214.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemN2N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-End Memory Networks (MemN2N)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable neural network that reads from a separate external memory with a recurrent attention mechanism (multiple soft attention 'hops') and is trained end-to-end by backpropagation; supports position & temporal encodings and weight-tying schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemN2N</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture stores inputs as embedded memory vectors m_i and uses a query embedding u to compute content-based attention p_i = softmax(u^T m_i); produces an output o = sum_i p_i c_i and can perform K sequential 'hops' where u^{k+1}=u^k + o^k (or u^{k+1}=H u^k + o^k). Supports adjacent or layer-wise weight tying, position encoding (PE), temporal encodings T_A/T_C, linear-start (LS) training and random empty-memory injection (RN).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external differentiable memory (content-addressable soft attention over stored input vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Inputs x_i are embedded into memory vectors m_i (via A) and associated output vectors c_i (via C); a query q is embedded to u (via B); attention p_i = Softmax(u^T m_i) retrieves a weighted sum o = Σ_i p_i c_i; multiple hops repeat this process, updating u by adding o (or via learned linear H) so later hops condition on prior retrieved content; temporal row vectors and position encodings augment m_i/c_i; entire system is differentiable and trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI synthetic QA (20 tasks) and language modeling (Penn Treebank, Text8)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>bAbI: synthetic question-answering tasks targeting multi-step reasoning and temporal/relational inference, answers typically single-word; Language modeling: next-word prediction on corpora (Penn Treebank, Text8) measuring perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering; language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI (1k training): example best MemN2N variants: mean test error ≈ 13.9% (PE+LS+RN, joint) and 12.4% (PE LS SN joint) depending on variant; bAbI (10k): best ≈ 6.6% mean error (PE+LS+RN). Language modeling (Penn Treebank): test perplexity improved from baselines to as low as 111 (best reported MemN2N config: 7 hops, memory size 200, hidden 150). Text8: test perplexity improved to ~147 (7 hops) from LSTM baseline 154.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multiple soft-attention hops over an external memory materially improve task performance (more hops reduce bAbI error and language-model perplexity); MemN2N trained end-to-end closes much of the gap with strongly supervised Memory Networks and outperforms weakly-supervised baselines; position and temporal encodings and training tricks (linear start, random empty memories) further boost results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still below strongly supervised MemNN on some bAbI tasks; high variance across random initializations on some tasks (authors repeat trainings and pick best); smooth soft attention may not scale well to very large memories (authors propose hashing/multiscale attention as future work); occasional failure on specific 1k tasks (several tasks have >5% error).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'End-To-End Memory Networks', 'publication_date_yy_mm': '2015-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3214.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3214.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Networks (MemNN, Weston et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier memory-augmented model that uses discrete/hard selection (max) over memories and required supervision of supporting facts per training example; used here as a strongly-supervised baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memory networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory Network (MemNN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stores sentences in memory and uses a scoring mechanism with hard max selection to pick supporting sentences; training in original formulation uses explicit labels of supporting facts (strong supervision) and can adaptively choose number of hops.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory with hard max retrieval over stored facts (supervised supporting-fact labels used in training)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory slots hold sentence representations; during each hop the model selects the top-scoring memory (hard max) as supporting facts; supporting facts are provided during training to supervise which memories should be selected.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI synthetic QA (20 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic multi-step QA tasks designed to probe reasoning types (supporting facts identification, temporal reasoning, deduction, induction, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI (1k training, strongly supervised): mean test error 6.7% (reported in paper's Table 1); bAbI (10k): mean error 3.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Strong supervision of supporting facts plus hard selection yields better performance than weakly supervised models; compared to MemN2N, strongly supervised MemNN achieves lower mean error on bAbI when supporting facts are available.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires supporting-fact supervision and uses hard max (non-differentiable) per layer, making end-to-end training from input-output pairs difficult; less generally applicable to tasks without per-hop supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'End-To-End Memory Networks', 'publication_date_yy_mm': '2015-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3214.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3214.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemNN-WSH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemNN-WSH (Weakly-Supervised Heuristic Memory Network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic weakly-supervised variant of Memory Network used as a baseline that enforces simple word-overlap constraints to approximate supporting-fact selection when labels are not available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemNN-WSH</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A MemNN-style baseline without supporting-fact labels: enforce heuristics that first hop must share a word with the question, and the second hop must share a word with the first hop and the answer; train by ranking valid memories above invalid ones.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory with heuristic hard-selection constraints (weak supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory candidates are filtered by simple lexical overlap rules to produce 'valid' memories; objective trains the model to rank valid memories higher than invalid ones using MemNN ranking criteria rather than backpropagating through soft attention.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI synthetic QA (20 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same synthetic QA tasks; here used to evaluate weakly-supervised, heuristic memory selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI (1k): mean error 40.2% (Table 1); bAbI (10k): mean error 39.2% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Heuristic weak supervision performs substantially worse than end-to-end differentiable MemN2N variants and much worse than strongly supervised MemNN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on brittle heuristics (lexical overlap) to identify memories; poor performance on many tasks compared to learned attention approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'End-To-End Memory Networks', 'publication_date_yy_mm': '2015-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3214.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3214.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Turing Machine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Turing Machine (NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural model with a continuous external memory that supports both content- and address-based read/write operations; cited in related work as closely related but more complex.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Turing Machine (NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Continuous memory architecture with learned read/write heads that perform content- and location-based addressing (including operations like sharpening); designed for algorithmic tasks like copying, sorting, and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external differentiable memory (content and address-based read/write)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory represented as a matrix with differentiable read and write heads that attend by content similarity and by positional addressing; includes specialized mechanisms (e.g., sharpening) to control access.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>algorithmic toy tasks (sorting, recall) - mentioned contextually</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Abstract algorithmic tasks used in NTM literature to illustrate learned read/write behavior (not evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>algorithmic reasoning / toy tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NTM is related conceptually (continuous memory + differentiable access), but differs in providing explicit address-based operations; paper notes temporal features in MemN2N approximate some address-based access but MemN2N is simpler and applied to textual QA/language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'End-To-End Memory Networks', 'publication_date_yy_mm': '2015-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3214.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3214.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNNsearch / Attention (Bahdanau et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural machine translation by jointly learning to align and translate (RNNsearch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention-based encoder-decoder model for machine translation that computes soft attention weights over encoder hidden states to condition the decoder; cited as related prior work on attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation by jointly learning to align and translate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RNNsearch (attention-based encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Bidirectional RNN encoder and gated RNN decoder with a learned soft-attention scoring network that computes weights over encoder hidden states and conditions decoder outputs on attended context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit encoder-state memory with attention (soft attention over encoder hidden states)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Encoder produces a set of hidden-state vectors; decoder at each step computes attention scores as a small neural net function of decoder state and each encoder state, then forms a context vector as weighted sum to produce next outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>machine translation (sequence-to-sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translating sentences between languages requiring alignment of source tokens to target outputs; attention enables conditioning on variable-length source.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>sequence-to-sequence / translation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites Bahdanau et al. as conceptually similar because both use soft attention over stored representations; MemN2N extends the idea to multiple hops and larger sets of memories (many sentences) rather than a single sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'End-To-End Memory Networks', 'publication_date_yy_mm': '2015-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3214.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3214.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory network (LSTM) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard gated recurrent neural network architecture used as a baseline for both bAbI QA (weakly supervised) and language modeling; maintains internal recurrent memory cells.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Gated recurrent network with internal memory (cell state and gates) designed to capture longer-term dependencies compared to vanilla RNNs; used here as a baseline trained only on question/answer pairs for bAbI and as a tuned LM baseline for Penn Treebank and Text8.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal recurrent state (not an external memory beyond the model's context/state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Uses gated recurrent cell state (memory cells) to store information across time steps; no external content-addressable memory is used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI synthetic QA (baseline) and language modeling (Penn Treebank, Text8)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As baseline: for bAbI trained only on QA pairs (weak supervision); for LM baseline tuned LSTM used for perplexity comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering; language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI (1k) mean error 51.3% (Table 1) when trained weakly on QA; Language modeling (Penn Treebank): test perplexity 115 (LSTM baseline in Table 2); Text8: test perplexity 154.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LSTM baselines perform poorly on bAbI QA in weak supervision setting compared to MemN2N; for language modeling MemN2N slightly outperforms tuned LSTM on Text8/Penn in reported configs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LSTM internal state can be unstable over long timescales and in this paper performs worse on synthetic multi-step QA tasks lacking direct supervision of supporting facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'End-To-End Memory Networks', 'publication_date_yy_mm': '2015-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memory networks <em>(Rating: 2)</em></li>
                <li>Towards AI-complete question answering: A set of prerequisite toy tasks <em>(Rating: 2)</em></li>
                <li>Neural turing machines <em>(Rating: 2)</em></li>
                <li>Neural machine translation by jointly learning to align and translate <em>(Rating: 1)</em></li>
                <li>Learning longer memory in recurrent neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3214",
    "paper_id": "paper-4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "MemN2N",
            "name_full": "End-to-End Memory Networks (MemN2N)",
            "brief_description": "A differentiable neural network that reads from a separate external memory with a recurrent attention mechanism (multiple soft attention 'hops') and is trained end-to-end by backpropagation; supports position & temporal encodings and weight-tying schemes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemN2N",
            "agent_description": "Architecture stores inputs as embedded memory vectors m_i and uses a query embedding u to compute content-based attention p_i = softmax(u^T m_i); produces an output o = sum_i p_i c_i and can perform K sequential 'hops' where u^{k+1}=u^k + o^k (or u^{k+1}=H u^k + o^k). Supports adjacent or layer-wise weight tying, position encoding (PE), temporal encodings T_A/T_C, linear-start (LS) training and random empty-memory injection (RN).",
            "memory_used": true,
            "memory_type": "external differentiable memory (content-addressable soft attention over stored input vectors)",
            "memory_mechanism_description": "Inputs x_i are embedded into memory vectors m_i (via A) and associated output vectors c_i (via C); a query q is embedded to u (via B); attention p_i = Softmax(u^T m_i) retrieves a weighted sum o = Σ_i p_i c_i; multiple hops repeat this process, updating u by adding o (or via learned linear H) so later hops condition on prior retrieved content; temporal row vectors and position encodings augment m_i/c_i; entire system is differentiable and trained end-to-end.",
            "task_name": "bAbI synthetic QA (20 tasks) and language modeling (Penn Treebank, Text8)",
            "task_description": "bAbI: synthetic question-answering tasks targeting multi-step reasoning and temporal/relational inference, answers typically single-word; Language modeling: next-word prediction on corpora (Penn Treebank, Text8) measuring perplexity.",
            "task_type": "question answering; language modeling",
            "performance_with_memory": "bAbI (1k training): example best MemN2N variants: mean test error ≈ 13.9% (PE+LS+RN, joint) and 12.4% (PE LS SN joint) depending on variant; bAbI (10k): best ≈ 6.6% mean error (PE+LS+RN). Language modeling (Penn Treebank): test perplexity improved from baselines to as low as 111 (best reported MemN2N config: 7 hops, memory size 200, hidden 150). Text8: test perplexity improved to ~147 (7 hops) from LSTM baseline 154.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Multiple soft-attention hops over an external memory materially improve task performance (more hops reduce bAbI error and language-model perplexity); MemN2N trained end-to-end closes much of the gap with strongly supervised Memory Networks and outperforms weakly-supervised baselines; position and temporal encodings and training tricks (linear start, random empty memories) further boost results.",
            "limitations_or_challenges": "Still below strongly supervised MemNN on some bAbI tasks; high variance across random initializations on some tasks (authors repeat trainings and pick best); smooth soft attention may not scale well to very large memories (authors propose hashing/multiscale attention as future work); occasional failure on specific 1k tasks (several tasks have &gt;5% error).",
            "uuid": "e3214.0",
            "source_info": {
                "paper_title": "End-To-End Memory Networks",
                "publication_date_yy_mm": "2015-03"
            }
        },
        {
            "name_short": "MemNN",
            "name_full": "Memory Networks (MemNN, Weston et al.)",
            "brief_description": "An earlier memory-augmented model that uses discrete/hard selection (max) over memories and required supervision of supporting facts per training example; used here as a strongly-supervised baseline.",
            "citation_title": "Memory networks",
            "mention_or_use": "use",
            "agent_name": "Memory Network (MemNN)",
            "agent_description": "Stores sentences in memory and uses a scoring mechanism with hard max selection to pick supporting sentences; training in original formulation uses explicit labels of supporting facts (strong supervision) and can adaptively choose number of hops.",
            "memory_used": true,
            "memory_type": "external memory with hard max retrieval over stored facts (supervised supporting-fact labels used in training)",
            "memory_mechanism_description": "Memory slots hold sentence representations; during each hop the model selects the top-scoring memory (hard max) as supporting facts; supporting facts are provided during training to supervise which memories should be selected.",
            "task_name": "bAbI synthetic QA (20 tasks)",
            "task_description": "Synthetic multi-step QA tasks designed to probe reasoning types (supporting facts identification, temporal reasoning, deduction, induction, etc.).",
            "task_type": "question answering",
            "performance_with_memory": "bAbI (1k training, strongly supervised): mean test error 6.7% (reported in paper's Table 1); bAbI (10k): mean error 3.2%.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Strong supervision of supporting facts plus hard selection yields better performance than weakly supervised models; compared to MemN2N, strongly supervised MemNN achieves lower mean error on bAbI when supporting facts are available.",
            "limitations_or_challenges": "Requires supporting-fact supervision and uses hard max (non-differentiable) per layer, making end-to-end training from input-output pairs difficult; less generally applicable to tasks without per-hop supervision.",
            "uuid": "e3214.1",
            "source_info": {
                "paper_title": "End-To-End Memory Networks",
                "publication_date_yy_mm": "2015-03"
            }
        },
        {
            "name_short": "MemNN-WSH",
            "name_full": "MemNN-WSH (Weakly-Supervised Heuristic Memory Network)",
            "brief_description": "A heuristic weakly-supervised variant of Memory Network used as a baseline that enforces simple word-overlap constraints to approximate supporting-fact selection when labels are not available.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "MemNN-WSH",
            "agent_description": "A MemNN-style baseline without supporting-fact labels: enforce heuristics that first hop must share a word with the question, and the second hop must share a word with the first hop and the answer; train by ranking valid memories above invalid ones.",
            "memory_used": true,
            "memory_type": "external memory with heuristic hard-selection constraints (weak supervision)",
            "memory_mechanism_description": "Memory candidates are filtered by simple lexical overlap rules to produce 'valid' memories; objective trains the model to rank valid memories higher than invalid ones using MemNN ranking criteria rather than backpropagating through soft attention.",
            "task_name": "bAbI synthetic QA (20 tasks)",
            "task_description": "Same synthetic QA tasks; here used to evaluate weakly-supervised, heuristic memory selection.",
            "task_type": "question answering",
            "performance_with_memory": "bAbI (1k): mean error 40.2% (Table 1); bAbI (10k): mean error 39.2% (Table 3).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Heuristic weak supervision performs substantially worse than end-to-end differentiable MemN2N variants and much worse than strongly supervised MemNN.",
            "limitations_or_challenges": "Relies on brittle heuristics (lexical overlap) to identify memories; poor performance on many tasks compared to learned attention approaches.",
            "uuid": "e3214.2",
            "source_info": {
                "paper_title": "End-To-End Memory Networks",
                "publication_date_yy_mm": "2015-03"
            }
        },
        {
            "name_short": "Neural Turing Machine",
            "name_full": "Neural Turing Machine (NTM)",
            "brief_description": "A neural model with a continuous external memory that supports both content- and address-based read/write operations; cited in related work as closely related but more complex.",
            "citation_title": "Neural turing machines",
            "mention_or_use": "mention",
            "agent_name": "Neural Turing Machine (NTM)",
            "agent_description": "Continuous memory architecture with learned read/write heads that perform content- and location-based addressing (including operations like sharpening); designed for algorithmic tasks like copying, sorting, and recall.",
            "memory_used": true,
            "memory_type": "external differentiable memory (content and address-based read/write)",
            "memory_mechanism_description": "Memory represented as a matrix with differentiable read and write heads that attend by content similarity and by positional addressing; includes specialized mechanisms (e.g., sharpening) to control access.",
            "task_name": "algorithmic toy tasks (sorting, recall) - mentioned contextually",
            "task_description": "Abstract algorithmic tasks used in NTM literature to illustrate learned read/write behavior (not evaluated in this paper).",
            "task_type": "algorithmic reasoning / toy tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "NTM is related conceptually (continuous memory + differentiable access), but differs in providing explicit address-based operations; paper notes temporal features in MemN2N approximate some address-based access but MemN2N is simpler and applied to textual QA/language modeling.",
            "limitations_or_challenges": null,
            "uuid": "e3214.3",
            "source_info": {
                "paper_title": "End-To-End Memory Networks",
                "publication_date_yy_mm": "2015-03"
            }
        },
        {
            "name_short": "RNNsearch / Attention (Bahdanau et al.)",
            "name_full": "Neural machine translation by jointly learning to align and translate (RNNsearch)",
            "brief_description": "An attention-based encoder-decoder model for machine translation that computes soft attention weights over encoder hidden states to condition the decoder; cited as related prior work on attention.",
            "citation_title": "Neural machine translation by jointly learning to align and translate",
            "mention_or_use": "mention",
            "agent_name": "RNNsearch (attention-based encoder-decoder)",
            "agent_description": "Bidirectional RNN encoder and gated RNN decoder with a learned soft-attention scoring network that computes weights over encoder hidden states and conditions decoder outputs on attended context.",
            "memory_used": true,
            "memory_type": "implicit encoder-state memory with attention (soft attention over encoder hidden states)",
            "memory_mechanism_description": "Encoder produces a set of hidden-state vectors; decoder at each step computes attention scores as a small neural net function of decoder state and each encoder state, then forms a context vector as weighted sum to produce next outputs.",
            "task_name": "machine translation (sequence-to-sequence)",
            "task_description": "Translating sentences between languages requiring alignment of source tokens to target outputs; attention enables conditioning on variable-length source.",
            "task_type": "sequence-to-sequence / translation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Paper cites Bahdanau et al. as conceptually similar because both use soft attention over stored representations; MemN2N extends the idea to multiple hops and larger sets of memories (many sentences) rather than a single sentence.",
            "limitations_or_challenges": null,
            "uuid": "e3214.4",
            "source_info": {
                "paper_title": "End-To-End Memory Networks",
                "publication_date_yy_mm": "2015-03"
            }
        },
        {
            "name_short": "LSTM (baseline)",
            "name_full": "Long Short-Term Memory network (LSTM) baseline",
            "brief_description": "Standard gated recurrent neural network architecture used as a baseline for both bAbI QA (weakly supervised) and language modeling; maintains internal recurrent memory cells.",
            "citation_title": "Long short-term memory",
            "mention_or_use": "use",
            "agent_name": "LSTM",
            "agent_description": "Gated recurrent network with internal memory (cell state and gates) designed to capture longer-term dependencies compared to vanilla RNNs; used here as a baseline trained only on question/answer pairs for bAbI and as a tuned LM baseline for Penn Treebank and Text8.",
            "memory_used": false,
            "memory_type": "internal recurrent state (not an external memory beyond the model's context/state)",
            "memory_mechanism_description": "Uses gated recurrent cell state (memory cells) to store information across time steps; no external content-addressable memory is used.",
            "task_name": "bAbI synthetic QA (baseline) and language modeling (Penn Treebank, Text8)",
            "task_description": "As baseline: for bAbI trained only on QA pairs (weak supervision); for LM baseline tuned LSTM used for perplexity comparison.",
            "task_type": "question answering; language modeling",
            "performance_with_memory": "bAbI (1k) mean error 51.3% (Table 1) when trained weakly on QA; Language modeling (Penn Treebank): test perplexity 115 (LSTM baseline in Table 2); Text8: test perplexity 154.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "LSTM baselines perform poorly on bAbI QA in weak supervision setting compared to MemN2N; for language modeling MemN2N slightly outperforms tuned LSTM on Text8/Penn in reported configs.",
            "limitations_or_challenges": "LSTM internal state can be unstable over long timescales and in this paper performs worse on synthetic multi-step QA tasks lacking direct supervision of supporting facts.",
            "uuid": "e3214.5",
            "source_info": {
                "paper_title": "End-To-End Memory Networks",
                "publication_date_yy_mm": "2015-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memory networks",
            "rating": 2
        },
        {
            "paper_title": "Towards AI-complete question answering: A set of prerequisite toy tasks",
            "rating": 2
        },
        {
            "paper_title": "Neural turing machines",
            "rating": 2
        },
        {
            "paper_title": "Neural machine translation by jointly learning to align and translate",
            "rating": 1
        },
        {
            "paper_title": "Learning longer memory in recurrent neural networks",
            "rating": 1
        }
    ],
    "cost": 0.01527225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>End-To-End Memory Networks</h1>
<p>Sainbayar Sukhbaatar<br>Dept. of Computer Science<br>Courant Institute, New York University<br>sainbar@cs.nyu.edu</p>
<p>Arthur Szlam Jason Weston Rob Fergus<br>Facebook AI Research<br>New York<br>{aszlam, jase, robfergus}@fb.com</p>
<h4>Abstract</h4>
<p>We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.</p>
<h2>1 Introduction</h2>
<p>Two grand challenges in artificial intelligence research have been to build models that can make multiple computational steps in the service of answering a question or completing a task, and models that can describe long term dependencies in sequential data.</p>
<p>Recently there has been a resurgence in models of computation using explicit storage and a notion of attention [23, 8, 2]; manipulating such a storage offers an approach to both of these challenges. In [23, 8, 2], the storage is endowed with a continuous representation; reads from and writes to the storage, as well as other processing steps, are modeled by the actions of neural networks.</p>
<p>In this work, we present a novel recurrent neural network (RNN) architecture where the recurrence reads from a possibly large external memory multiple times before outputting a symbol. Our model can be considered a continuous form of the Memory Network implemented in [23]. The model in that work was not easy to train via backpropagation, and required supervision at each layer of the network. The continuity of the model we present here means that it can be trained end-to-end from input-output pairs, and so is applicable to more tasks, i.e. tasks where such supervision is not available, such as in language modeling or realistically supervised question answering tasks. Our model can also be seen as a version of RNNsearch [2] with multiple computational steps (which we term "hops") per output symbol. We will show experimentally that the multiple hops over the long-term memory are crucial to good performance of our model on these tasks, and that training the memory representation can be integrated in a scalable manner into our end-to-end neural network model.</p>
<h2>2 Approach</h2>
<p>Our model takes a discrete set of inputs $x_{1}, \ldots, x_{n}$ that are to be stored in the memory, a query $q$, and outputs an answer $a$. Each of the $x_{i}, q$, and $a$ contains symbols coming from a dictionary with $V$ words. The model writes all $x$ to the memory up to a fixed buffer size, and then finds a continuous representation for the $x$ and $q$. The continuous representation is then processed via multiple hops to output $a$. This allows backpropagation of the error signal through multiple memory accesses back to the input during training.</p>
<p>2.1 Single Layer</p>
<p>We start by describing our model in the single layer case, which implements a single memory hop operation. We then show it can be stacked to give multiple hops in memory.</p>
<p>Input memory representation: Suppose we are given an input set $x_{1},..,x_{i}$ to be stored in memory. The entire set of ${x_{i}}$ are converted into memory vectors $\left{m_{i}\right}$ of dimension $d$ computed by embedding each $x_{i}$ in a continuous space, in the simplest case, using an embedding matrix $A$ (of size $d\times V$ ). The query $q$ is also embedded (again, in the simplest case via another embedding matrix $B$ with the same dimensions as $A$ ) to obtain an internal state $u$. In the embedding space, we compute the match between $u$ and each memory $m_{i}$ by taking the inner product followed by a softmax:</p>
<p>$p_{i}=\text{Softmax}\left(u^{T} m_{i}\right).$ (1)</p>
<p>where $\operatorname{Softmax}\left(z_{i}\right)=e^{z_{i}} / \sum_{j} e^{z_{j}}$. Defined in this way $p$ is a probability vector over the inputs.</p>
<p>Output memory representation: Each $x_{i}$ has a corresponding output vector $c_{i}$ (given in the simplest case by another embedding matrix $C$ ). The response vector from the memory $o$ is then a sum over the transformed inputs $c_{i}$, weighted by the probability vector from the input:</p>
<p>$$
o=\sum_{i} p_{i} c_{i}
$$</p>
<p>Because the function from input to output is smooth, we can easily compute gradients and backpropagate through it. Other recently proposed forms of memory or attention take this approach, notably Bahdanau et al. [2] and Graves et al. [8], see also [9].</p>
<p>Generating the final prediction: In the single layer case, the sum of the output vector $o$ and the input embedding $u$ is then passed through a final weight matrix $W$ (of size $V \times d$ ) and a softmax to produce the predicted label:</p>
<p>$$
\hat{a}=\operatorname{Softmax}(W(o+u))
$$</p>
<p>The overall model is shown in Fig. 1(a). During training, all three embedding matrices $A, B$ and $C$, as well as $W$ are jointly learned by minimizing a standard cross-entropy loss between $\hat{a}$ and the true label $a$. Training is performed using stochastic gradient descent (see Section 4.2 for more details).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a): A single layer version of our model. (b): A three layer version of our model. In practice, we can constrain several of the embedding matrices to be the same (see Section 2.2).</p>
<h1>2.2 Multiple Layers</h1>
<p>We now extend our model to handle $K$ hop operations. The memory layers are stacked in the following way:</p>
<ul>
<li>The input to layers above the first is the sum of the output $o^{k}$ and the input $u^{k}$ from layer $k$ (different ways to combine $o^{k}$ and $u^{k}$ are proposed later):</li>
</ul>
<p>$$
u^{k+1}=u^{k}+o^{k}
$$</p>
<ul>
<li>Each layer has its own embedding matrices $A^{k}, C^{k}$, used to embed the inputs $\left{x_{i}\right}$. However, as discussed below, they are constrained to ease training and reduce the number of parameters.</li>
<li>At the top of the network, the input to $W$ also combines the input and the output of the top memory layer: $\hat{a}=\text { Softmax }\left(W u^{K+1}\right)=\text { Softmax }\left(W\left(o^{K}+u^{K}\right)\right)$.</li>
</ul>
<p>We explore two types of weight tying within the model:</p>
<ol>
<li>Adjacent: the output embedding for one layer is the input embedding for the one above, i.e. $A^{k+1}=C^{k}$. We also constrain (a) the answer prediction matrix to be the same as the final output embedding, i.e $W^{T}=C^{K}$, and (b) the question embedding to match the input embedding of the first layer, i.e. $B=A^{1}$.</li>
<li>Layer-wise (RNN-like): the input and output embeddings are the same across different layers, i.e. $A^{1}=A^{2}=\ldots=A^{K}$ and $C^{1}=C^{2}=\ldots=C^{K}$. We have found it useful to add a linear mapping $H$ to the update of $u$ between hops; that is, $u^{k+1}=H u^{k}+o^{k}$. This mapping is learnt along with the rest of the parameters and used throughout our experiments for layer-wise weight tying.</li>
</ol>
<p>A three-layer version of our memory model is shown in Fig. 1(b). Overall, it is similar to the Memory Network model in [23], except that the hard max operations within each layer have been replaced with a continuous weighting from the softmax.</p>
<p>Note that if we use the layer-wise weight tying scheme, our model can be cast as a traditional RNN where we divide the outputs of the RNN into internal and external outputs. Emitting an internal output corresponds to considering a memory, and emitting an external output corresponds to predicting a label. From the RNN point of view, $u$ in Fig. 1(b) and Eqn. 4 is a hidden state, and the model generates an internal output $p$ (attention weights in Fig. 1(a)) using $A$. The model then ingests $p$ using $C$, updates the hidden state, and so on . Here, unlike a standard RNN, we explicitly condition on the outputs stored in memory during the $K$ hops, and we keep these outputs soft, rather than sampling them. Thus our model makes several computational steps before producing an output meant to be seen by the "outside world".</p>
<h1>3 Related Work</h1>
<p>A number of recent efforts have explored ways to capture long-term structure within sequences using RNNs or LSTM-based models [4, 7, 12, 15, 10, 1]. The memory in these models is the state of the network, which is latent and inherently unstable over long timescales. The LSTM-based models address this through local memory cells which lock in the network state from the past. In practice, the performance gains over carefully trained RNNs are modest (see Mikolov et al. [15]). Our model differs from these in that it uses a global memory, with shared read and write functions. However, with layer-wise weight tying our model can be viewed as a form of RNN which only produces an output after a fixed number of time steps (corresponding to the number of hops), with the intermediary steps involving memory input/output operations that update the internal state.</p>
<p>Some of the very early work on neural networks by Steinbuch and Piske[19] and Taylor [21] considered a memory that performed nearest-neighbor operations on stored input vectors and then fit parametric models to the retrieved sets. This has similarities to a single layer version of our model.</p>
<p>Subsequent work in the 1990's explored other types of memory [18, 5, 16]. For example, Das et al. [5] and Mozer et al. [16] introduced an explicit stack with push and pop operations which has been revisited recently by [11] in the context of an RNN model.</p>
<p>Closely related to our model is the Neural Turing Machine of Graves et al. [8], which also uses a continuous memory representation. The NTM memory uses both content and address-based access, unlike ours which only explicitly allows the former, although the temporal features that we will introduce in Section 4.1 allow a kind of address-based access. However, in part because we always write each memory sequentially, our model is somewhat simpler, not requiring operations like sharpening. Furthermore, we apply our memory model to textual reasoning tasks, which qualitatively differ from the more abstract operations of sorting and recall tackled by the NTM.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Our model is also related to Bahdanau et al. [2]. In that work, a bidirectional RNN based encoder and gated RNN based decoder were used for machine translation. The decoder uses an attention model that finds which hidden states from the encoding are most useful for outputting the next translated word; the attention model uses a small neural network that takes as input a concatenation of the current hidden state of the decoder and each of the encoders hidden states. A similar attention model is also used in Xu et al. [24] for generating image captions. Our "memory" is analogous to their attention mechanism, although [2] is only over a single sentence rather than many, as in our case. Furthermore, our model makes several hops on the memory before making an output; we will see below that this is important for good performance. There are also differences in the architecture of the small network used to score the memories compared to our scoring approach; we use a simple linear layer, whereas they use a more sophisticated gated architecture.</p>
<p>We will apply our model to language modeling, an extensively studied task. Goodman [6] showed simple but effective approaches which combine $n$-grams with a cache. Bengio et al. [3] ignited interest in using neural network based models for the task, with RNNs [14] and LSTMs [10, 20] showing clear performance gains over traditional methods. Indeed, the current state-of-the-art is held by variants of these models, for example very large LSTMs with Dropout [25] or RNNs with diagonal constraints on the weight matrix [15]. With appropriate weight tying, our model can be regarded as a modified form of RNN, where the recurrence is indexed by memory lookups to the word sequence rather than indexed by the sequence itself.</p>
<h1>4 Synthetic Question and Answering Experiments</h1>
<p>We perform experiments on the synthetic QA tasks defined in [22] (using version 1.1 of the dataset). A given QA task consists of a set of statements, followed by a question whose answer is typically a single word (in a few tasks, answers are a set of words). The answer is available to the model at training time, but must be predicted at test time. There are a total of 20 different types of tasks that probe different forms of reasoning and deduction. Here are samples of three of the tasks:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sam walks into the kitchen.</th>
<th style="text-align: center;">Brian is a lion.</th>
<th style="text-align: center;">Mary journeyed to the den.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Sam picks up an apple.</td>
<td style="text-align: center;">Julius is a lion.</td>
<td style="text-align: center;">Mary went back to the kitchen.</td>
</tr>
<tr>
<td style="text-align: center;">Sam walks into the bedroom.</td>
<td style="text-align: center;">Julius is white.</td>
<td style="text-align: center;">John journeyed to the bedroom.</td>
</tr>
<tr>
<td style="text-align: center;">Sam drops the apple.</td>
<td style="text-align: center;">Bernhard is green.</td>
<td style="text-align: center;">Mary discarded the milk.</td>
</tr>
<tr>
<td style="text-align: center;">Q: Where is the apple?</td>
<td style="text-align: center;">Q: What color is Brian?</td>
<td style="text-align: center;">Q: Where was the milk before the den?</td>
</tr>
<tr>
<td style="text-align: center;">A. Bedroom</td>
<td style="text-align: center;">A. White</td>
<td style="text-align: center;">A. Hallway</td>
</tr>
</tbody>
</table>
<p>Note that for each question, only some subset of the statements contain information needed for the answer, and the others are essentially irrelevant distractors (e.g. the first sentence in the first example). In the Memory Networks of Weston et al. [22], this supporting subset was explicitly indicated to the model during training and the key difference between that work and this one is that this information is no longer provided. Hence, the model must deduce for itself at training and test time which sentences are relevant and which are not.</p>
<p>Formally, for one of the 20 QA tasks, we are given example problems, each having a set of $I$ sentences $\left{x_{i}\right}$ where $I \leq 320$; a question sentence $q$ and answer $a$. Let the $j$ th word of sentence $i$ be $x_{i j}$, represented by a one-hot vector of length $V$ (where the vocabulary is of size $V=177$, reflecting the simplistic nature of the QA language). The same representation is used for the question $q$ and answer $a$. Two versions of the data are used, one that has 1000 training problems per task and a second larger one with 10,000 per task.</p>
<h3>4.1 Model Details</h3>
<p>Unless otherwise stated, all experiments used a $K=3$ hops model with the adjacent weight sharing scheme. For all tasks that output lists (i.e. the answers are multiple words), we take each possible combination of possible outputs and record them as a separate answer vocabulary word.</p>
<p>Sentence Representation: In our experiments we explore two different representations for the sentences. The first is the bag-of-words (BoW) representation that takes the sentence $x_{i}=\left{x_{i 1}, x_{i 2}, \ldots, x_{i n}\right}$, embeds each word and sums the resulting vectors: e.g $m_{i}=\sum_{j} A x_{i j}$ and $c_{i}=\sum_{j} C x_{i j}$. The input vector $u$ representing the question is also embedded as a bag of words: $u=\sum_{j} B q_{j}$. This has the drawback that it cannot capture the order of the words in the sentence, which is important for some tasks.</p>
<p>We therefore propose a second representation that encodes the position of words within the sentence. This takes the form: $m_{i}=\sum_{j} l_{j} \cdot A x_{i j}$, where $\cdot$ is an element-wise multiplication. $l_{j}$ is a</p>
<p>column vector with the structure $l_{k j}=(1-j / J)-(k / d)(1-2 j / J)$ (assuming 1-based indexing), with $J$ being the number of words in the sentence, and $d$ is the dimension of the embedding. This sentence representation, which we call position encoding (PE), means that the order of the words now affects $m_{i}$. The same representation is used for questions, memory inputs and memory outputs.</p>
<p>Temporal Encoding: Many of the QA tasks require some notion of temporal context, i.e. in the first example of Section 2, the model needs to understand that Sam is in the bedroom after he is in the kitchen. To enable our model to address them, we modify the memory vector so that $m_{i}=\sum_{j} A x_{i j}+T_{A}(i)$, where $T_{A}(i)$ is the $i$ th row of a special matrix $T_{A}$ that encodes temporal information. The output embedding is augmented in the same way with a matrix $T_{c}$ (e.g. $c_{i}=\sum_{j} C x_{i j}+T_{C}(i)$ ). Both $T_{A}$ and $T_{C}$ are learned during training. They are also subject to the same sharing constraints as $A$ and $C$. Note that sentences are indexed in reverse order, reflecting their relative distance from the question so that $x_{1}$ is the last sentence of the story.</p>
<p>Learning time invariance by injecting random noise: we have found it helpful to add "dummy" memories to regularize $T_{A}$. That is, at training time we can randomly add $10 \%$ of empty memories to the stories. We refer to this approach as random noise (RN).</p>
<h1>4.2 Training Details</h1>
<p>$10 \%$ of the bAbI training set was held-out to form a validation set, which was used to select the optimal model architecture and hyperparameters. Our models were trained using a learning rate of $\eta=0.01$, with anneals every 25 epochs by $\eta / 2$ until 100 epochs were reached. No momentum or weight decay was used. The weights were initialized randomly from a Gaussian distribution with zero mean and $\sigma=0.1$. When trained on all tasks simultaneously with 1 k training samples ( 10 k training samples), 60 epochs ( 20 epochs) were used with learning rate anneals of $\eta / 2$ every 15 epochs ( 5 epochs). All training uses a batch size of 32 (but cost is not averaged over a batch), and gradients with an $\ell_{2}$ norm larger than 40 are divided by a scalar to have norm 40 . In some of our experiments, we explored commencing training with the softmax in each memory layer removed, making the model entirely linear except for the final softmax for answer prediction. When the validation loss stopped decreasing, the softmax layers were re-inserted and training recommenced. We refer to this as linear start (LS) training. In LS training, the initial learning rate is set to $\eta=0.005$. The capacity of memory is restricted to the most recent 50 sentences. Since the number of sentences and the number of words per sentence varied between problems, a null symbol was used to pad them all to a fixed size. The embedding of the null symbol was constrained to be zero.</p>
<p>On some tasks, we observed a large variance in the performance of our model (i.e. sometimes failing badly, other times not, depending on the initialization). To remedy this, we repeated each training 10 times with different random initializations, and picked the one with the lowest training error.</p>
<h3>4.3 Baselines</h3>
<p>We compare our approach ${ }^{2}$ (abbreviated to MemN2N) to a range of alternate models:</p>
<ul>
<li>MemNN: The strongly supervised AM+NG+NL Memory Networks approach, proposed in [22]. This is the best reported approach in that paper. It uses a max operation (rather than softmax) at each layer which is trained directly with supporting facts (strong supervision). It employs $n$-gram modeling, nonlinear layers and an adaptive number of hops per query.</li>
<li>MemNN-WSH: A weakly supervised heuristic version of MemNN where the supporting sentence labels are not used in training. Since we are unable to backpropagate through the max operations in each layer, we enforce that the first memory hop should share at least one word with the question, and that the second memory hop should share at least one word with the first hop and at least one word with the answer. All those memories that conform are called valid memories, and the goal during training is to rank them higher than invalid memories using the same ranking criteria as during strongly supervised training.</li>
<li>LSTM: A standard LSTM model, trained using question / answer pairs only (i.e. also weakly supervised). For more detail, see [22].</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.4 Results</h1>
<p>We report a variety of design choices: (i) BoW vs Position Encoding (PE) sentence representation; (ii) training on all 20 tasks independently vs jointly training (joint training used an embedding dimension of $d=50$, while independent training used $d=20$ ); (iii) two phase training: linear start (LS) where softmaxes are removed initially vs training with softmaxes from the start; (iv) varying memory hops from 1 to 3 .</p>
<p>The results across all 20 tasks are given in Table 1 for the 1 k training set, along with the mean performance for 10 k training set $^{3}$. They show a number of interesting points:</p>
<ul>
<li>The best MemN2N models are reasonably close to the supervised models (e.g. 1k: $6.7 \%$ for MemNN vs $12.6 \%$ for MemN2N with position encoding + linear start + random noise, jointly trained and 10k: $3.2 \%$ for MemNN vs $4.2 \%$ for MemN2N with position encoding + linear start + random noise + non-linearity ${ }^{4}$, although the supervised models are still superior.</li>
<li>All variants of our proposed model comfortably beat the weakly supervised baseline methods.</li>
<li>The position encoding (PE) representation improves over bag-of-words (BoW), as demonstrated by clear improvements on tasks $4,5,15$ and 18, where word ordering is particularly important.</li>
<li>The linear start (LS) to training seems to help avoid local minima. See task 16 in Table 1, where PE alone gets $53.6 \%$ error, while using LS reduces it to $1.6 \%$.</li>
<li>Jittering the time index with random empty memories (RN) as described in Section 4.1 gives a small but consistent boost in performance, especially for the smaller 1 k training set.</li>
<li>Joint training on all tasks helps.</li>
<li>Importantly, more computational hops give improved performance. We give examples of the hops performed (via the values of eq. (1)) over some illustrative examples in Fig. 2 and in Appendix B.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MemN2N</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Strongly <br> Supervised <br> MemNN [22]</td>
<td style="text-align: center;">LSTM <br> [22]</td>
<td style="text-align: center;">MemNN <br> WSH</td>
<td style="text-align: center;">BoW</td>
<td style="text-align: center;">PE</td>
<td style="text-align: center;">PE <br> LS</td>
<td style="text-align: center;">$\begin{gathered} \text { PE } \ \text { LS } \ \text { RN } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { 1 hop } \ \text { PE LS } \ \text { joint } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { 2 hops } \ \text { PE LS } \ \text { joint } \end{gathered}$</td>
<td style="text-align: center;">3 hops <br> PE LS <br> joint</td>
<td style="text-align: center;">$\begin{gathered} \text { PE } \ \text { LS } \ \text { SN } \ \text { joint } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { PE LS } \ \text { LW } \ \text { joint } \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">1: 1 supporting fact</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">2: 2 supporting facts</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">18.8</td>
</tr>
<tr>
<td style="text-align: center;">3: 3 supporting facts</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">31.7</td>
</tr>
<tr>
<td style="text-align: center;">4: 2 argument relations</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">17.5</td>
</tr>
<tr>
<td style="text-align: center;">5: 3 argument relations</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">12.9</td>
</tr>
<tr>
<td style="text-align: center;">6: yes/no questions</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">7: counting</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">10.1</td>
</tr>
<tr>
<td style="text-align: center;">8: lists/sets</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">6.1</td>
</tr>
<tr>
<td style="text-align: center;">9: simple negation</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: center;">10: indefinite knowledge</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: center;">11: basic coreference</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">3.3</td>
</tr>
<tr>
<td style="text-align: center;">12: conjunction</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">13: compound coreference</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">14: time reasoning</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">15: basic deduction</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: center;">16: basic induction</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">51.0</td>
</tr>
<tr>
<td style="text-align: center;">17: positional reasoning</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">42.6</td>
</tr>
<tr>
<td style="text-align: center;">18: size reasoning</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">9.2</td>
</tr>
<tr>
<td style="text-align: center;">19: path finding</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">90.6</td>
</tr>
<tr>
<td style="text-align: center;">20: agent's motivation</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">Mean error (\%)</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">15.2</td>
</tr>
<tr>
<td style="text-align: center;">Failed tasks (err. $&gt;5 \%$ )</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">On 10k training data</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mean error (\%)</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">11.0</td>
</tr>
<tr>
<td style="text-align: center;">Failed tasks (err. $&gt;5 \%$ )</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
</tr>
</tbody>
</table>
<p>Table 1: Test error rates (\%) on the 20 QA tasks for models using 1 k training examples (mean test errors for 10 k training examples are shown at the bottom). Key: BoW = bag-of-words representation; PE = position encoding representation; LS = linear start training; RN = random injection of time index noise; $\mathrm{LW}=$ RNN-style layer-wise weight tying (if not stated, adjacent weight tying is used); joint $=$ joint training on all tasks (as opposed to per-task training).</p>
<h2>5 Language Modeling Experiments</h2>
<p>The goal in language modeling is to predict the next word in a text sequence given the previous words $x$. We now explain how our model can easily be applied to this task.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example predictions on the QA tasks of [22]. We show the labeled supporting facts (support) from the dataset which MemN2N does not use during training, and the probabilities $p$ of each hop used by the model during inference. MemN2N successfully learns to focus on the correct supporting sentences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Penn Treebank</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Text8</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"># of hidden</td>
<td style="text-align: center;"># of hops</td>
<td style="text-align: center;">memory size</td>
<td style="text-align: center;">Valid. perp.</td>
<td style="text-align: center;">Test perp.</td>
<td style="text-align: center;"># of hidden</td>
<td style="text-align: center;"># of hops</td>
<td style="text-align: center;">memory size</td>
<td style="text-align: center;">Valid. perp.</td>
<td style="text-align: center;">Test perp.</td>
</tr>
<tr>
<td style="text-align: center;">RNN [15]</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">133</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">184</td>
</tr>
<tr>
<td style="text-align: center;">LSTM [15]</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">154</td>
</tr>
<tr>
<td style="text-align: center;">SCRN [15]</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">161</td>
</tr>
<tr>
<td style="text-align: center;">MemN2N</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">121</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">187</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">142</td>
<td style="text-align: center;">178</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">127</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">162</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">127</td>
<td style="text-align: center;">118</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">154</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">124</td>
<td style="text-align: center;">155</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">118</td>
<td style="text-align: center;">147</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">118</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">163</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">121</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">132</td>
<td style="text-align: center;">166</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">126</td>
<td style="text-align: center;">158</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">124</td>
<td style="text-align: center;">155</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">157</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">121</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">154</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">118</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: The perplexity on the test sets of Penn Treebank and Text8 corpora. Note that increasing the number of memory hops improves performance.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Average activation weight of memory positions during 6 memory hops. White color indicates where the model is attending during the $k^{\text {th }}$ hop. For clarity, each row is normalized to have maximum value of 1. A model is trained on (left) Penn Treebank and (right) Text8 dataset.</p>
<p>We now operate on word level, as opposed to the sentence level. Thus the previous $N$ words in the sequence (including the current) are embedded into memory separately. Each memory cell holds only a single word, so there is no need for the BoW or linear mapping representations used in the QA tasks. We employ the temporal embedding approach of Section 4.1.</p>
<p>Since there is no longer any question, $q$ in Fig. 1 is fixed to a constant vector 0.1 (without embedding). The output softmax predicts which word in the vocabulary (of size $V$ ) is next in the sequence. A cross-entropy loss is used to train model by backpropagating the error through multiple memory layers, in the same manner as the QA tasks. To aid training, we apply ReLU operations to half of the units in each layer. We use layer-wise (RNN-like) weight sharing, i.e. the query weights of each layer are the same; the output weights of each layer are the same. As noted in Section 2.2, this makes our architecture closely related to an RNN which is traditionally used for language</p>
<p>modeling tasks; however here the "sequence" over which the network is recurrent is not in the text, but in the memory hops. Furthermore, the weight tying restricts the number of parameters in the model, helping generalization for the deeper models which we find to be effective for this task. We use two different datasets:
Penn Tree Bank [13]: This consists of 929k/73k/82k train/validation/test words, distributed over a vocabulary of 10 k words. The same preprocessing as [25] was used.
Text8 [15]: This is a a pre-processed version of the first 100M million characters, dumped from Wikipedia. This is split into $93.3 \mathrm{M} / 5.7 \mathrm{M} / 1 \mathrm{M}$ character train/validation/test sets. All word occurring less than 5 times are replaced with the $&lt;\mathrm{UNK}&gt;$ token, resulting in a vocabulary size of $\sim 44 \mathrm{k}$.</p>
<h1>5.1 Training Details</h1>
<p>The training procedure we use is the same as the QA tasks, except for the following. For each mini-batch update, the $\ell_{2}$ norm of the whole gradient of all parameters is measured ${ }^{5}$ and if larger than $L=50$, then it is scaled down to have norm $L$. This was crucial for good performance. We use the learning rate annealing schedule from [15], namely, if the validation cost has not decreased after one epoch, then the learning rate is scaled down by a factor 1.5. Training terminates when the learning rate drops below $10^{-5}$, i.e. after 50 epochs or so. Weights are initialized using $\mathcal{N}(0,0.05)$ and batch size is set to 128. On the Penn tree dataset, we repeat each training 10 times with different random initializations and pick the one with smallest validation cost. However, we have done only a single training run on Text8 dataset due to limited time constraints.</p>
<h3>5.2 Results</h3>
<p>Table 2 compares our model to RNN, LSTM and Structurally Constrained Recurrent Nets (SCRN) [15] baselines on the two benchmark datasets. Note that the baseline architectures were tuned in [15] to give optimal perplexity ${ }^{6}$. Our MemN2N approach achieves lower perplexity on both datasets ( 111 vs 115 for RNN/SCRN on Penn and 147 vs 154 for LSTM on Text8). Note that MemN2N has $\sim 1.5 \mathrm{x}$ more parameters than RNNs with the same number of hidden units, while LSTM has $\sim 4 \mathrm{x}$ more parameters. We also vary the number of hops and memory size of our MemN2N, showing the contribution of both to performance; note in particular that increasing the number of hops helps. In Fig. 3, we show how MemN2N operates on memory with multiple hops. It shows the average weight of the activation of each memory position over the test set. We can see that some hops concentrate only on recent words, while other hops have more broad attention over all memory locations, which is consistent with the idea that succesful language models consist of a smoothed $n$-gram model and a cache [15]. Interestingly, it seems that those two types of hops tend to alternate. Also note that unlike a traditional RNN, the cache does not decay exponentially: it has roughly the same average activation across the entire memory. This may be the source of the observed improvement in language modeling.</p>
<h2>6 Conclusions and Future Work</h2>
<p>In this work we showed that a neural network with an explicit memory and a recurrent attention mechanism for reading the memory can be successfully trained via backpropagation on diverse tasks from question answering to language modeling. Compared to the Memory Network implementation of [23] there is no supervision of supporting facts and so our model can be used in a wider range of settings. Our model approaches the same performance of that model, and is significantly better than other baselines with the same level of supervision. On language modeling tasks, it slightly outperforms tuned RNNs and LSTMs of comparable complexity. On both tasks we can see that increasing the number of memory hops improves performance.</p>
<p>However, there is still much to do. Our model is still unable to exactly match the performance of the memory networks trained with strong supervision, and both fail on several of the 1 k QA tasks. Furthermore, smooth lookups may not scale well to the case where a larger memory is required. For these settings, we plan to explore multiscale notions of attention or hashing, as proposed in [23].</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank Armand Joulin, Tomas Mikolov, Antoine Bordes and Sumit Chopra for useful comments and valuable discussions, and also the FAIR Infrastructure team for their help and support.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>[1] C. G. Atkeson and S. Schaal. Memory-based neural networks for robot learning. Neurocomputing, 9:243-269, 1995.
[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR), 2015.
[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137-1155, Mar. 2003.
[4] J. Chung, Ç. Gülçehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint: 1412.3555, 2014.
[5] S. Das, C. L. Giles, and G.-Z. Sun. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society, 1992.
[6] J. Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001.
[7] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint: 1308.0850, 2013.
[8] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint: 1410.5401, 2014.
[9] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623, 2015.
[10] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997.
[11] A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. NIPS, 2015.
[12] J. Koutník, K. Greff, F. J. Gomez, and J. Schmidhuber. A clockwork RNN. In ICML, 2014.
[13] M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of english: The Penn Treebank. Comput. Linguist., 19(2):313-330, June 1993.
[14] T. Mikolov. Statistical language models based on neural networks. Ph. D. thesis, Brno University of Technology, 2012.
[15] T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. Ranzato. Learning longer memory in recurrent neural networks. arXiv preprint: 1412.7753, 2014.
[16] M. C. Mozer and S. Das. A connectionist symbol manipulator that discovers the structure of context-free languages. NIPS, pages 863-863, 1993.
[17] B. Peng, Z. Lu, H. Li, and K. Wong. Towards Neural Network-based Reasoning. ArXiv preprint: 1508.05508, 2015.
[18] J. Pollack. The induction of dynamical recognizers. Machine Learning, 7(2-3):227-252, 1991.
[19] K. Steinbuch and U. Piske. Learning matrices and their applications. IEEE Transactions on Electronic Computers, 12:846-862, 1963.
[20] M. Sundermeyer, R. Schlüter, and H. Ney. LSTM neural networks for language modeling. In Interspeech, pages 194-197, 2012.
[21] W. K. Taylor. Pattern recognition by means of automatic analogue apparatus. Proceedings of The Institution of Electrical Engineers, 106:198-209, 1959.
[22] J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv preprint: 1502.05698, 2015.
[23] J. Weston, S. Chopra, and A. Bordes. Memory networks. In International Conference on Learning Representations (ICLR), 2015.
[24] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ArXiv preprint: 1502.03044, 2015.
[25] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.</p>
<h1>Appendix A Results on 10k QA dataset</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MemN2N</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Strongly Supervised</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemNN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PE</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { PE } \ &amp; \text { LS } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { PE LS } \ &amp; \text { LW } \ &amp; \text { RN } \end{aligned}$</td>
<td style="text-align: center;">1 hop <br> PE LS <br> joint</td>
<td style="text-align: center;">2 hops <br> PE LS <br> joint</td>
<td style="text-align: center;">3 hops <br> $\begin{aligned} &amp; \text { PE LS } \ &amp; \text { joint } \end{aligned}$</td>
<td style="text-align: center;">PE LS <br> LS RN</td>
<td style="text-align: center;">PE LS <br> LW</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemNN</td>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">WSH</td>
<td style="text-align: center;">BoW</td>
<td style="text-align: center;">PE</td>
<td style="text-align: center;">LS</td>
<td style="text-align: center;">RN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1: 1 supporting fact</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">2: 2 supporting facts</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3: 3 supporting facts</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4: 2 argument relations</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5: 3 argument relations</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">6: yes/no questions</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7: counting</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8: Intersets</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9: simple negation</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10: indefinite knowledge</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11: basic coreference</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">12: conjunction</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">13: compound coreference</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">14: time reasoning</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">15: basic deduction</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">16: basic induction</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">17: positional reasoning</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">18: size reasoning</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">19: path finding</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">20: agent's motivation</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Mean error (\%)</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Failed tasks (err. $&gt;5 \%$ )</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Test error rates (\%) on the 20 bAbl QA tasks for models using 10k training examples. Key: BoW = bag-of-words representation; $\mathrm{PE}=$ position encoding representation; $\mathrm{LS}=$ linear start training; $\mathrm{RN}=$ random injection of time index noise; $\mathrm{LW}=$ RNN-style layer-wise weight tying (if not stated, adjacent weight tying is used); joint $=$ joint training on all tasks (as opposed to per-task training); $*=$ this is a larger model with non-linearity (embedding dimension is $d=100$ and ReLU applied to the internal state after each hop. This was inspired by [17] and crucial for getting better performance on tasks 17 and 19).</p>
<p>Appendix B Visualization of attention weights in QA problems
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Examples of attention weights during different memory hops for the hAbi tasks. The model is PE+LS+RN with 3 memory hops that is trained separately on each task with 10k training data. The support column shows which sentences are necessary for answering questions. Although this information is not used, the model succesfully learns to focus on the correct support sentences on most of the tasks. The hop columns show where the model put more weight (indicated by values and blue color) during its three hops. The mistakes made by the model are highlighted by red color.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ In the QA tasks, the gradient of each weight matrix is measured separately.
${ }^{6}$ They tuned the hyper-parameters on Penn Treebank and used them on Text8 without additional tuning, except for the number of hidden units. See [15] for more detail.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
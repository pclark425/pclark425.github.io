<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2467 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2467</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2467</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-ef54be60095535f378039730e920da69a2331053</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ef54be60095535f378039730e920da69a2331053" target="_blank">Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This report presents a comprehensive evaluation of coordination and routing capabilities in a novel multi-agent collaboration framework and offers valuable guidance for enterprise deployments of multi-agent systems and advance the development of scalable, efficient multi-agent collaboration frameworks.</p>
                <p><strong>Paper Abstract:</strong> AI agents powered by large language models (LLMs) have shown strong capabilities in problem solving. Through combining many intelligent agents, multi-agent collaboration has emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents. However, designing the collaboration protocols and evaluating the effectiveness of these systems remains a significant challenge, especially for enterprise applications. This report addresses these challenges by presenting a comprehensive evaluation of coordination and routing capabilities in a novel multi-agent collaboration framework. We evaluate two key operational modes: (1) a coordination mode enabling complex task completion through parallel communication and payload referencing, and (2) a routing mode for efficient message forwarding between agents. We benchmark on a set of handcrafted scenarios from three enterprise domains, which are publicly released with the report. For coordination capabilities, we demonstrate the effectiveness of inter-agent communication and payload referencing mechanisms, achieving end-to-end goal success rates of 90%. Our analysis yields several key findings: multi-agent collaboration enhances goal success rates by up to 70% compared to single-agent approaches in our benchmarks; payload referencing improves performance on code-intensive tasks by 23%; latency can be substantially reduced with a routing mechanism that selectively bypasses agent orchestration. These findings offer valuable guidance for enterprise deployments of multi-agent systems and advance the development of scalable, efficient multi-agent collaboration frameworks.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2467.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2467.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Collaboration (MAC) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical multi-agent collaboration framework for enterprise applications that uses a supervisor (root) agent to plan, delegate, and coordinate specialist agents, with two operational modes (coordination and routing), payload-referencing optimization, and assertion-based automatic benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Agent Collaboration (MAC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Centralized hierarchical multi-agent system where a Supervisor Agent decomposes tasks, assigns sub-tasks to specialist agents, orchestrates parallel and synchronized communications, and collects results. Two operational modes are implemented: (1) Coordination mode — supervisor performs planning and parallel messaging among specialists, supports payload referencing to avoid retransmitting large content blocks; (2) Routing mode — a fast classifier decides whether to bypass full orchestration and route requests directly to a specialist agent. Interactions are instrumented for assertion-based benchmarking using LLM-driven user and action simulators and an LLM-based assertion judge.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (multiple specialists per domain, up to multi-layer hierarchies; experiments used teams of ~5-10 agents per domain and multi-layer setups)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Supervisor agent (task planning, delegation, orchestration); domain specialist agents vary by dataset: Travel (Flight agent, Hotel agent, Car rental, Weather, Location search, Travel budget, Restaurant, Local expert, Airbnb), Mortgage (Mortgage agent, Property agent, Credit agent, Income agent, Payment agent, Closing agent), Software (Software agent, Design agent, Code agent, Test agent, Review agent, Deploy agent, Infrastructure agent, Application agent). Specialists may each have access to domain-specific action groups/tools.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Planning/task decomposition, implementation (code generation in Software), execution (tool/API invocation simulations), testing and review (Test/Review agents), deployment (Deploy/Infrastructure), and evaluation (assertion-based success judgment).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Hierarchical, centralized supervisor-based coordination (tree-like hierarchy). Supervisor performs task planning, decomposition, assigns sub-tasks to leaf specialist agents; hierarchy can be multi-layer (leaf agents can be supervisors of lower-level specialists). Also supports a routing mode where a classifier decides to bypass orchestration for direct routing.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Message-passing modeled as a 'send_message' tool (parameters: recipient, content). Messages are natural-language with structured tagging for provenance: incoming messages are wrapped as <message from="$SOURCE_AGENT">... </message>. Communication is integrated with function-calling capability of underlying LLMs. Payloads (large structured content like code blocks) are detected, assigned unique IDs and wrapped with special tags; supervisor can include compact reference tags which the system expands to full payloads when delivering to recipients.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Synchronized request-response messaging (sender blocks until reply) with possibility of asynchronous extension; system memory stores message history and payloads; assertion-based benchmarking provides automated feedback via an LLM judge that evaluates user-side and system-side assertions; a user simulator (LLM) acts as human-in-the-loop for clarifications. Routing classifier provides binary decision feedback and fallback to full orchestration if uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand and turn-based: supervisor may communicate in parallel to multiple specialists within a user turn; each send_message call awaits a response by default (synchronized). Experiments show ~7-9 supervisor communications per session on average (varies by domain).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Enterprise applications (Travel planning, Mortgage financing, Software development); methods generalize to multi-step tasks requiring tool use and long-form payload exchange.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include Goal Success Rate (GSR) variants and latency: Coordination mode overall GSR up to 0.90 when supervisor and specialists use Claude 3.5 Sonnet (20241022); single-agent baselines: Travel 0.60, Mortgage 0.80, Software 0.53. Latency: avg. communication overhead per supervisor turn 13.39–35.44 s (domain dependent); avg. user-perceived turn latency 24.42–168.73 s. Payload referencing ablation (Software): Overall GSR improved from 0.73 (without) to 0.90 (with) — a 23% relative improvement; avg. communication overhead per turn reduced from 48.78 s to 35.44 s (27% relative reduction); avg. output tokens per communication reduced from 539.21 to 373.77 (≈30% reduction). Routing mode: routing classification accuracy ≈0.90–0.92, false agent switch rate 0.00–0.03, classification latency ≈344–378 ms, turn-level routing overhead ≈630–750 ms; routing-mode end-to-end GSR: Travel 0.85, Mortgage 0.95 (supervisor GSR 1.00).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to single-agent baseline (one agent given all tools/instructions), MAC substantially outperformed single-agent in GSR (absolute regressions up to 0.37 observed in single-agent settings, and paper claims multi-agent improved goal success by up to 70% in benchmarks). Also compared with an open-source automation framework (OSF) where MAC showed materially higher GSR across domains (e.g., Travel: MAC 0.87 vs OSF 0.50; Mortgage: MAC 0.90 vs OSF 0.63; Software: MAC 0.77 vs OSF 0.40 under particular model settings).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Improved goal success rates (overall GSR up to 0.90); specialization reduces hallucinations in tool parameterization and tool-choice; payload referencing improves code-heavy task effectiveness (+23% relative GSR in Software) and reduces supervisor token/output and communication overhead (~27% lower avg. communication time and ~30% fewer tokens); dynamic routing reduces latency by bypassing orchestration for simple requests (classification latency ≈350 ms, turn-level overhead ≈600–800 ms), and supervisor GSR remains high in routing experiments (1.00 in presented domains).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Coordination and messaging introduce communication overhead and higher latency, particularly in complex domains (Software user-perceived latency up to ≈169 s). Synchronized blocking message-passing can delay supervisor responses. Payload referencing can increase specialist-side activity leading to higher user-perceived latency in some settings. Routing requires an accurate low-latency classifier (≥90% accuracy needed) and has a small false-switch risk. Benchmarking multi-agent interactions is complex; LLM-based judges and simulators can disagree with humans in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Payload referencing ablation (Software domain): With referencing — Overall GSR 0.90, avg. communication overhead per turn 35.44 s, avg. output tokens/comm 373.77; Without referencing — Overall GSR 0.73, avg. communication overhead per turn 48.78 s, avg. output tokens/comm 539.21. Comparison vs OSF (sequential one-time assignment framework) shows MAC substantially higher GSR across domains (see baseline_comparison). Routing classifier intrinsic eval: ≈0.92 accuracy, <3% false switch, classification latency ≈350 ms.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper recommends hierarchical supervisor + specialist design for manageability; use payload referencing for domains with large structured payloads (e.g., code) to reduce tokens and improve GSR; employ dynamic routing classifier to bypass orchestration for simple single-specialist requests (classifier target ≥90% accuracy, latency ≈350 ms); utilize stronger LLM models for specialists (Claude 3.5 Sonnet produced best overall results in experiments); allow parallel supervisor-to-specialist communications where subtasks are independent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2467.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-Source Task Automation Framework (referred in paper as OSF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely adopted open-source automation framework used for comparison; it implements multi-agent collaboration as a sequential task-automation workflow with one-time sub-task assignments and ReAct-style prompting for tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OSF (open-source framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sequential task-automation workflow: supervisor receives user task, breaks it into one-time sub-tasks for sub-agents; agents in OSF use ReAct-style prompting for tool use (rather than native function-calling). The authors extended OSF to support multiple sub-agent interactions for comparative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (sequential supervisor + multiple sub-agents; in experiments mirrored MAC domains)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Supervisor and sub-agents corresponding to domain roles (not detailed in paper); OSF agents tend to be verbose due to prompting style.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Task decomposition, tool invocation (sequential sub-task execution), limited conversational back-and-forth unless extended.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Sequential one-time sub-task assignment (supervisor breaks tasks and issues one-time assignments to sub-agents).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language prompts with ReAct-style prompting for tool use; agents communicate via implicit prompt-based workflows rather than a native send_message/tool-call interface.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not explicitly described in this paper; OSF original design relies on sequential execution outputs and ReAct signalings. In experiments, extensions allowed multiple sub-agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Primarily one-time/subtask assignment; extended experiments allowed back-and-forth but native design is sequential one-shot assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used for same enterprise domains (Travel, Mortgage, Software) in comparative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported comparative GSRs in this paper under specific model settings: e.g., Travel OSF 0.50 vs MAC 0.87 (Supervisor Sonnet 3.5 (20240620) / Specialists Sonnet 3.0), Mortgage OSF 0.63 vs MAC 0.90, Software OSF 0.40 vs MAC 0.77. OSF agents were found to be more verbose and token-heavy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Direct comparison to MAC in coordination mode; MAC outperformed OSF across evaluated domains and model settings.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>OSF supports straightforward sequential automation and ReAct-style tool usage; creative prompt engineering enables task automation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>One-time assignment design can be insufficient for conversational back-and-forth between specialist agents; agents are verbose and use more tokens, which increases cost and may reduce performance in conversational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper reports comparative experiments (MAC vs OSF) but does not present internal OSF ablations beyond its standard vs extended conversational setup.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper suggests OSF requires extension to support multiple sub-agent interactions for conversational tasks; otherwise sequential one-shot assignment is native but suboptimal for multi-turn conversational collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2467.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT: Meta programming for a multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM project that attempts to mimic a software company by assigning roles to agents according to an SOP (standard operating procedure), enabling role-based multi-agent collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MetaGPT: Meta programming for a multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Framework that mimics a software company: developers provide SOPs and MetaGPT assigns roles to various agents and organizes agent collaboration for software development tasks (as described in the related work of this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (role-based teams; not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Role-based agents (e.g., developer roles) assigned per SOP; specific roles not enumerated in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Software development lifecycle tasks (planning, implementation, code generation) as per the mimicry of a software company.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Role assignment and team-based coordination guided by SOPs; characterized by centralized hierarchy in the paper's related-work summary.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in detail in this paper; described as assigning roles and coordinating via prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development / task automation (mentioned as an example of multi-agent LLM usage).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned in related work; no experimental comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Facilitates role assignment and modularization of software development tasks (as described in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2467.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAMEL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CAMEL: Communicative Agent Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework promoting independent collaboration between LLM agents using 'inception prompting' to steer conversational agents toward task completion and to collect/analyze conversational data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Camel: Communicative agents for "mind" exploration of large language model society</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CAMEL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Communicative Agent Framework that encourages decentralized collaboration among LLM agents; introduces 'inception prompting' to influence agent behavior and produces conversational data for research into agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (multiple agents in a communicative society; not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not enumerated in this paper's summary; CAMEL focuses on communicative roles and independent collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Conversational collaboration, task-solving via agent-to-agent dialogs (general purpose).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Decentralized agent interactions (paper notes CAMEL adopts decentralized hierarchies and decision-making processes).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Conversational natural-language messages driven by inception prompting (as described in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Conversational and iterative; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent tasks, research platform for agent interaction analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as related work; no direct comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Enables independent collaboration and research into agent society behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2467.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGen: Multi-agent conversation framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework enabling multi-agent systems to engage in sophisticated conversations, allowing agents to share information and iteratively refine outputs through simulated dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoGen: Enabling next-gen llm applications via multi-agent conversation framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Conversation-centric multi-agent framework that enables agents to negotiate, plan, and execute tasks collaboratively via multi-turn dialogues, simulating human-like interactions to refine outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (designed for multiple conversational agents; not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified here; AutoGen supports role specialization depending on user configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Conversational planning, negotiation, iterative refinement of solutions; tool-use dialog simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Conversation-driven coordination where agents iteratively exchange messages and refine outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language dialogues, leveraging multi-turn conversation constructs (details not expanded in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Iterative refinement through conversational exchange; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Continuous multi-turn dialogue as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General LLM multi-agent applications; used for collaborative planning and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as related work; no direct experimental comparison here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Enables sophisticated conversational negotiation and iterative improvement of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2467.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CrewAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CrewAI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform that organizes agents into specialized roles to enhance task execution via task decomposition, treating agents like members of a crew to solve subtasks efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CrewAI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CrewAI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Role-based agent organization that emphasizes task decomposition, assigning specialized agents to sub-tasks analogous to a crew, improving modularity and parallelization of work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (role-based teams; unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Specialist agents assigned to decomposed subtasks (paper references role specialization but does not list specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Task decomposition and execution; applicable to complex tasks requiring multiple specialized roles.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Role-based team coordination (centralized assignment implied in the paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General task automation and multi-agent problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned in related work; no comparison results here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Improves task decomposition and specialist assignment for efficient problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2467.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangGraph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework organizing agent interactions using directed acyclic graphs (DAGs) to manage dependencies between tasks and agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LangGraph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LangGraph</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Organizes agent interactions using DAGs to represent task dependencies and optimize information flow; helps visualize and manage dependencies so each agent's action is informed by prior steps.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (graph nodes represent agents or tasks; unspecified here)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified in this paper; framework focuses on dependency organization rather than particular agent roles.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Dependency management across multi-step tasks and pipeline-style workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>DAG-structured coordination (explicit dependency graph between tasks/agents).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in this paper; implicit exchange following graph edges.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Depends on DAG execution ordering; not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent task orchestration with interdependent subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned in related work; no experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Clarifies dependencies and optimizes flow among agents for interdependent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2467.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolSandbox</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolSandbox</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stateful, conversational, interactive evaluation benchmark for LLM tool use; includes a user simulator, environment executor, milestones (must-happen events), and minefields (forbidden events).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolsandbox: A stateful, conversational, interactive evaluation benchmark for llm tool use capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ToolSandbox</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation framework that simulates user and environment interactions, pre-defines milestones and minefields for sessions, and stores execution context (world state) for tool-using agent evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>Not a multi-agent system per se; an evaluation platform used to benchmark multi-agent or single-agent tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>N/A — focuses on evaluation of tool-using agents via simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Execution and evaluation of agent tool use in interactive sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>N/A for Task orchestration — it's an evaluation platform; supports simulated tool calls and milestones to judge correct behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Simulated function/tool calls plus conversation logging; specifics not deeply described here.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Milestones and minefields provide structured evaluation feedback; judge compares trajectories to expected events.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Determined by simulated interactions; not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation of LLM tool use across interactive tasks (general-purpose).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (paper references ToolSandbox as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned for comparison of evaluation approaches; no direct experimental comparison reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Provides structured and stateful evaluation for tool-using agents and can emulate environment effects.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2467.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentEval: multi-agent evaluation setup</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation setup with a multi-agent judge trio (CriticAgent, QuantifierAgent, VerifierAgent) that decomposes evaluation into criteria generation, quantification, and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and verifying task utility in LLMpowered applications</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentEval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent evaluation pipeline: CriticAgent derives coarse-grained success criteria from a task description, QuantifierAgent measures whether trajectories meet criteria (possibly producing scalar scores), and VerifierAgent checks accuracy/completeness of the evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>3 (CriticAgent, QuantifierAgent, VerifierAgent) in the evaluation pipeline described.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Critic (criteria generation), Quantifier (measurement/scoring), Verifier (validation of evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Evaluation and verification of agent trajectories and utility.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Sequential evaluation pipeline among the three judge agents.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in detail in this paper beyond LLM-based agent interactions for judging.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Quantified scores and verification results that can be used to accept/reject trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>One-time per trajectory evaluation; specifics depend on evaluation workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation across LLM-powered application tasks (general-purpose).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Decomposes evaluation into specialized judge roles enabling scalable, structured judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Paper notes CriticAgent criteria can be coarse-grained; detailed limitations in original work are not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2467.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentQuest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentQuest: modular benchmark framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular benchmark that measures agent success by progress rate based on milestone completion; milestones are environment hidden states needed to reach final solution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AgentQuest: A modular benchmark framework to measure progress and improve LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentQuest</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmarking framework that defines milestones (environment hidden states) and measures agent progress rate by counting completed milestones; supports externally annotated or programmatically defined milestones for game-like environments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>N/A (benchmarking framework rather than a multi-agent system itself).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>N/A (framework for evaluating agents).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Evaluation/progress measurement in task-solving trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>N/A for agent coordination; provides milestone-based measurement for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Progress-rate feedback derived from milestone completion.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Game-like datasets and environments (e.g., ALFWorld, Sudoku) and general agent evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Provides fine-grained progress assessment via milestones.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Limited to environments where milestones are well-defined or annotatable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2467.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDev: Communicative agents for software development</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent approach for software development where agents coordinate in a centralized hierarchy and decision-making mode to mimic software workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatDev: Communicative agents for software development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent system designed for software development tasks with centralized team hierarchy and centralized decision-making; assigns communicative roles to agents to complete software engineering workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (team of role-specialized agents; details not expanded in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Role-oriented agents for software development (implied: design, implement, test roles), but specifics not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Software development phases (design, implementation, possibly test/review).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized hierarchy and centralized decision-making (as noted in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Conversational natural-language prompts between agents (details not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as related work; no experimental comparison presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Centralized coordination maps well to a software-company metaphor for role assignment and task control.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2467.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2467.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Framework for creating interactive agent-based simulations of human-like behavior; emphasizes social behavior and psychology-inspired interactions among agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agent simulation framework where agents exhibit human-like memory and behavior, used to study social behavior and interactions; mentioned as part of related work emphasizing decentralized behaviors and psychology/social science influences.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (agent populations in simulated environments; exact counts dependent on simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents modeled as human-like actors with memory and behavior; not specialized by computational role in the context cited here.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Behavioral simulation and interaction studies; not focused on scientific research workflows in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Agent interactions guided by simulated memory and social behaviors (decentralized interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language simulation and internal memory representations (details in original work, not expanded here).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Social/interaction-based feedback mechanisms intrinsic to simulation; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Continuous interactions as driven by simulation events.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Interactive social simulations and behavioral research.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Referenced as related work; no comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Useful for studying emergent social behaviors and psychology-inspired agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MetaGPT: Meta programming for a multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>Camel: Communicative agents for "mind" exploration of large language model society <em>(Rating: 2)</em></li>
                <li>AutoGen: Enabling next-gen llm applications via multi-agent conversation framework <em>(Rating: 2)</em></li>
                <li>Toolsandbox: A stateful, conversational, interactive evaluation benchmark for llm tool use capabilities <em>(Rating: 2)</em></li>
                <li>Assessing and verifying task utility in LLMpowered applications <em>(Rating: 2)</em></li>
                <li>AgentQuest: A modular benchmark framework to measure progress and improve LLM agents <em>(Rating: 1)</em></li>
                <li>ChatDev: Communicative agents for software development <em>(Rating: 1)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2467",
    "paper_id": "paper-ef54be60095535f378039730e920da69a2331053",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "MAC",
            "name_full": "Multi-Agent Collaboration (MAC) framework",
            "brief_description": "A hierarchical multi-agent collaboration framework for enterprise applications that uses a supervisor (root) agent to plan, delegate, and coordinate specialist agents, with two operational modes (coordination and routing), payload-referencing optimization, and assertion-based automatic benchmarking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Multi-Agent Collaboration (MAC)",
            "system_description": "Centralized hierarchical multi-agent system where a Supervisor Agent decomposes tasks, assigns sub-tasks to specialist agents, orchestrates parallel and synchronized communications, and collects results. Two operational modes are implemented: (1) Coordination mode — supervisor performs planning and parallel messaging among specialists, supports payload referencing to avoid retransmitting large content blocks; (2) Routing mode — a fast classifier decides whether to bypass full orchestration and route requests directly to a specialist agent. Interactions are instrumented for assertion-based benchmarking using LLM-driven user and action simulators and an LLM-based assertion judge.",
            "number_of_agents": "variable (multiple specialists per domain, up to multi-layer hierarchies; experiments used teams of ~5-10 agents per domain and multi-layer setups)",
            "agent_specializations": "Supervisor agent (task planning, delegation, orchestration); domain specialist agents vary by dataset: Travel (Flight agent, Hotel agent, Car rental, Weather, Location search, Travel budget, Restaurant, Local expert, Airbnb), Mortgage (Mortgage agent, Property agent, Credit agent, Income agent, Payment agent, Closing agent), Software (Software agent, Design agent, Code agent, Test agent, Review agent, Deploy agent, Infrastructure agent, Application agent). Specialists may each have access to domain-specific action groups/tools.",
            "research_phases_covered": "Planning/task decomposition, implementation (code generation in Software), execution (tool/API invocation simulations), testing and review (Test/Review agents), deployment (Deploy/Infrastructure), and evaluation (assertion-based success judgment).",
            "coordination_mechanism": "Hierarchical, centralized supervisor-based coordination (tree-like hierarchy). Supervisor performs task planning, decomposition, assigns sub-tasks to leaf specialist agents; hierarchy can be multi-layer (leaf agents can be supervisors of lower-level specialists). Also supports a routing mode where a classifier decides to bypass orchestration for direct routing.",
            "communication_protocol": "Message-passing modeled as a 'send_message' tool (parameters: recipient, content). Messages are natural-language with structured tagging for provenance: incoming messages are wrapped as &lt;message from=\"$SOURCE_AGENT\"&gt;... &lt;/message&gt;. Communication is integrated with function-calling capability of underlying LLMs. Payloads (large structured content like code blocks) are detected, assigned unique IDs and wrapped with special tags; supervisor can include compact reference tags which the system expands to full payloads when delivering to recipients.",
            "feedback_mechanism": "Synchronized request-response messaging (sender blocks until reply) with possibility of asynchronous extension; system memory stores message history and payloads; assertion-based benchmarking provides automated feedback via an LLM judge that evaluates user-side and system-side assertions; a user simulator (LLM) acts as human-in-the-loop for clarifications. Routing classifier provides binary decision feedback and fallback to full orchestration if uncertain.",
            "communication_frequency": "On-demand and turn-based: supervisor may communicate in parallel to multiple specialists within a user turn; each send_message call awaits a response by default (synchronized). Experiments show ~7-9 supervisor communications per session on average (varies by domain).",
            "task_domain": "Enterprise applications (Travel planning, Mortgage financing, Software development); methods generalize to multi-step tasks requiring tool use and long-form payload exchange.",
            "performance_metrics": "Reported metrics include Goal Success Rate (GSR) variants and latency: Coordination mode overall GSR up to 0.90 when supervisor and specialists use Claude 3.5 Sonnet (20241022); single-agent baselines: Travel 0.60, Mortgage 0.80, Software 0.53. Latency: avg. communication overhead per supervisor turn 13.39–35.44 s (domain dependent); avg. user-perceived turn latency 24.42–168.73 s. Payload referencing ablation (Software): Overall GSR improved from 0.73 (without) to 0.90 (with) — a 23% relative improvement; avg. communication overhead per turn reduced from 48.78 s to 35.44 s (27% relative reduction); avg. output tokens per communication reduced from 539.21 to 373.77 (≈30% reduction). Routing mode: routing classification accuracy ≈0.90–0.92, false agent switch rate 0.00–0.03, classification latency ≈344–378 ms, turn-level routing overhead ≈630–750 ms; routing-mode end-to-end GSR: Travel 0.85, Mortgage 0.95 (supervisor GSR 1.00).",
            "baseline_comparison": "Compared to single-agent baseline (one agent given all tools/instructions), MAC substantially outperformed single-agent in GSR (absolute regressions up to 0.37 observed in single-agent settings, and paper claims multi-agent improved goal success by up to 70% in benchmarks). Also compared with an open-source automation framework (OSF) where MAC showed materially higher GSR across domains (e.g., Travel: MAC 0.87 vs OSF 0.50; Mortgage: MAC 0.90 vs OSF 0.63; Software: MAC 0.77 vs OSF 0.40 under particular model settings).",
            "coordination_benefits": "Improved goal success rates (overall GSR up to 0.90); specialization reduces hallucinations in tool parameterization and tool-choice; payload referencing improves code-heavy task effectiveness (+23% relative GSR in Software) and reduces supervisor token/output and communication overhead (~27% lower avg. communication time and ~30% fewer tokens); dynamic routing reduces latency by bypassing orchestration for simple requests (classification latency ≈350 ms, turn-level overhead ≈600–800 ms), and supervisor GSR remains high in routing experiments (1.00 in presented domains).",
            "coordination_challenges": "Coordination and messaging introduce communication overhead and higher latency, particularly in complex domains (Software user-perceived latency up to ≈169 s). Synchronized blocking message-passing can delay supervisor responses. Payload referencing can increase specialist-side activity leading to higher user-perceived latency in some settings. Routing requires an accurate low-latency classifier (≥90% accuracy needed) and has a small false-switch risk. Benchmarking multi-agent interactions is complex; LLM-based judges and simulators can disagree with humans in some cases.",
            "ablation_studies": "Payload referencing ablation (Software domain): With referencing — Overall GSR 0.90, avg. communication overhead per turn 35.44 s, avg. output tokens/comm 373.77; Without referencing — Overall GSR 0.73, avg. communication overhead per turn 48.78 s, avg. output tokens/comm 539.21. Comparison vs OSF (sequential one-time assignment framework) shows MAC substantially higher GSR across domains (see baseline_comparison). Routing classifier intrinsic eval: ≈0.92 accuracy, &lt;3% false switch, classification latency ≈350 ms.",
            "optimal_configurations": "Paper recommends hierarchical supervisor + specialist design for manageability; use payload referencing for domains with large structured payloads (e.g., code) to reduce tokens and improve GSR; employ dynamic routing classifier to bypass orchestration for simple single-specialist requests (classifier target ≥90% accuracy, latency ≈350 ms); utilize stronger LLM models for specialists (Claude 3.5 Sonnet produced best overall results in experiments); allow parallel supervisor-to-specialist communications where subtasks are independent.",
            "uuid": "e2467.0",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OSF",
            "name_full": "Open-Source Task Automation Framework (referred in paper as OSF)",
            "brief_description": "A widely adopted open-source automation framework used for comparison; it implements multi-agent collaboration as a sequential task-automation workflow with one-time sub-task assignments and ReAct-style prompting for tool use.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "OSF (open-source framework)",
            "system_description": "Sequential task-automation workflow: supervisor receives user task, breaks it into one-time sub-tasks for sub-agents; agents in OSF use ReAct-style prompting for tool use (rather than native function-calling). The authors extended OSF to support multiple sub-agent interactions for comparative experiments.",
            "number_of_agents": "variable (sequential supervisor + multiple sub-agents; in experiments mirrored MAC domains)",
            "agent_specializations": "Supervisor and sub-agents corresponding to domain roles (not detailed in paper); OSF agents tend to be verbose due to prompting style.",
            "research_phases_covered": "Task decomposition, tool invocation (sequential sub-task execution), limited conversational back-and-forth unless extended.",
            "coordination_mechanism": "Sequential one-time sub-task assignment (supervisor breaks tasks and issues one-time assignments to sub-agents).",
            "communication_protocol": "Natural-language prompts with ReAct-style prompting for tool use; agents communicate via implicit prompt-based workflows rather than a native send_message/tool-call interface.",
            "feedback_mechanism": "Not explicitly described in this paper; OSF original design relies on sequential execution outputs and ReAct signalings. In experiments, extensions allowed multiple sub-agent interactions.",
            "communication_frequency": "Primarily one-time/subtask assignment; extended experiments allowed back-and-forth but native design is sequential one-shot assignments.",
            "task_domain": "Used for same enterprise domains (Travel, Mortgage, Software) in comparative experiments.",
            "performance_metrics": "Reported comparative GSRs in this paper under specific model settings: e.g., Travel OSF 0.50 vs MAC 0.87 (Supervisor Sonnet 3.5 (20240620) / Specialists Sonnet 3.0), Mortgage OSF 0.63 vs MAC 0.90, Software OSF 0.40 vs MAC 0.77. OSF agents were found to be more verbose and token-heavy.",
            "baseline_comparison": "Direct comparison to MAC in coordination mode; MAC outperformed OSF across evaluated domains and model settings.",
            "coordination_benefits": "OSF supports straightforward sequential automation and ReAct-style tool usage; creative prompt engineering enables task automation pipelines.",
            "coordination_challenges": "One-time assignment design can be insufficient for conversational back-and-forth between specialist agents; agents are verbose and use more tokens, which increases cost and may reduce performance in conversational tasks.",
            "ablation_studies": "Paper reports comparative experiments (MAC vs OSF) but does not present internal OSF ablations beyond its standard vs extended conversational setup.",
            "optimal_configurations": "Paper suggests OSF requires extension to support multiple sub-agent interactions for conversational tasks; otherwise sequential one-shot assignment is native but suboptimal for multi-turn conversational collaboration.",
            "uuid": "e2467.1",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT: Meta programming for a multi-agent collaborative framework",
            "brief_description": "A multi-agent LLM project that attempts to mimic a software company by assigning roles to agents according to an SOP (standard operating procedure), enabling role-based multi-agent collaboration.",
            "citation_title": "MetaGPT: Meta programming for a multi-agent collaborative framework",
            "mention_or_use": "mention",
            "system_name": "MetaGPT",
            "system_description": "Framework that mimics a software company: developers provide SOPs and MetaGPT assigns roles to various agents and organizes agent collaboration for software development tasks (as described in the related work of this paper).",
            "number_of_agents": "variable (role-based teams; not specified in this paper)",
            "agent_specializations": "Role-based agents (e.g., developer roles) assigned per SOP; specific roles not enumerated in this paper's summary.",
            "research_phases_covered": "Software development lifecycle tasks (planning, implementation, code generation) as per the mimicry of a software company.",
            "coordination_mechanism": "Role assignment and team-based coordination guided by SOPs; characterized by centralized hierarchy in the paper's related-work summary.",
            "communication_protocol": "Not specified in detail in this paper; described as assigning roles and coordinating via prompts.",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "Software development / task automation (mentioned as an example of multi-agent LLM usage).",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Mentioned in related work; no experimental comparison provided here.",
            "coordination_benefits": "Facilitates role assignment and modularization of software development tasks (as described in cited work).",
            "coordination_challenges": "Not detailed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.2",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CAMEL",
            "name_full": "CAMEL: Communicative Agent Framework",
            "brief_description": "A framework promoting independent collaboration between LLM agents using 'inception prompting' to steer conversational agents toward task completion and to collect/analyze conversational data.",
            "citation_title": "Camel: Communicative agents for \"mind\" exploration of large language model society",
            "mention_or_use": "mention",
            "system_name": "CAMEL",
            "system_description": "Communicative Agent Framework that encourages decentralized collaboration among LLM agents; introduces 'inception prompting' to influence agent behavior and produces conversational data for research into agent interactions.",
            "number_of_agents": "variable (multiple agents in a communicative society; not specified in this paper)",
            "agent_specializations": "Not enumerated in this paper's summary; CAMEL focuses on communicative roles and independent collaboration.",
            "research_phases_covered": "Conversational collaboration, task-solving via agent-to-agent dialogs (general purpose).",
            "coordination_mechanism": "Decentralized agent interactions (paper notes CAMEL adopts decentralized hierarchies and decision-making processes).",
            "communication_protocol": "Conversational natural-language messages driven by inception prompting (as described in related work).",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Conversational and iterative; specifics not provided here.",
            "task_domain": "General multi-agent tasks, research platform for agent interaction analysis.",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Mentioned as related work; no direct comparison in this paper.",
            "coordination_benefits": "Enables independent collaboration and research into agent society behaviors.",
            "coordination_challenges": "Not detailed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.3",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AutoGen",
            "name_full": "AutoGen: Multi-agent conversation framework",
            "brief_description": "A framework enabling multi-agent systems to engage in sophisticated conversations, allowing agents to share information and iteratively refine outputs through simulated dialogues.",
            "citation_title": "AutoGen: Enabling next-gen llm applications via multi-agent conversation framework",
            "mention_or_use": "mention",
            "system_name": "AutoGen",
            "system_description": "Conversation-centric multi-agent framework that enables agents to negotiate, plan, and execute tasks collaboratively via multi-turn dialogues, simulating human-like interactions to refine outputs.",
            "number_of_agents": "variable (designed for multiple conversational agents; not specified in this paper)",
            "agent_specializations": "Not specified here; AutoGen supports role specialization depending on user configuration.",
            "research_phases_covered": "Conversational planning, negotiation, iterative refinement of solutions; tool-use dialog simulation.",
            "coordination_mechanism": "Conversation-driven coordination where agents iteratively exchange messages and refine outputs.",
            "communication_protocol": "Natural-language dialogues, leveraging multi-turn conversation constructs (details not expanded in this paper).",
            "feedback_mechanism": "Iterative refinement through conversational exchange; specifics not provided here.",
            "communication_frequency": "Continuous multi-turn dialogue as needed.",
            "task_domain": "General LLM multi-agent applications; used for collaborative planning and execution.",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Mentioned as related work; no direct experimental comparison here.",
            "coordination_benefits": "Enables sophisticated conversational negotiation and iterative improvement of outputs.",
            "coordination_challenges": "Not detailed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.4",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CrewAI",
            "name_full": "CrewAI",
            "brief_description": "A platform that organizes agents into specialized roles to enhance task execution via task decomposition, treating agents like members of a crew to solve subtasks efficiently.",
            "citation_title": "CrewAI",
            "mention_or_use": "mention",
            "system_name": "CrewAI",
            "system_description": "Role-based agent organization that emphasizes task decomposition, assigning specialized agents to sub-tasks analogous to a crew, improving modularity and parallelization of work.",
            "number_of_agents": "variable (role-based teams; unspecified in this paper)",
            "agent_specializations": "Specialist agents assigned to decomposed subtasks (paper references role specialization but does not list specifics).",
            "research_phases_covered": "Task decomposition and execution; applicable to complex tasks requiring multiple specialized roles.",
            "coordination_mechanism": "Role-based team coordination (centralized assignment implied in the paper's mention).",
            "communication_protocol": "Not specified in detail in this paper.",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "General task automation and multi-agent problem solving.",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Mentioned in related work; no comparison results here.",
            "coordination_benefits": "Improves task decomposition and specialist assignment for efficient problem solving.",
            "coordination_challenges": "Not detailed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.5",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LangGraph",
            "name_full": "LangGraph",
            "brief_description": "A framework organizing agent interactions using directed acyclic graphs (DAGs) to manage dependencies between tasks and agents.",
            "citation_title": "LangGraph",
            "mention_or_use": "mention",
            "system_name": "LangGraph",
            "system_description": "Organizes agent interactions using DAGs to represent task dependencies and optimize information flow; helps visualize and manage dependencies so each agent's action is informed by prior steps.",
            "number_of_agents": "variable (graph nodes represent agents or tasks; unspecified here)",
            "agent_specializations": "Not specified in this paper; framework focuses on dependency organization rather than particular agent roles.",
            "research_phases_covered": "Dependency management across multi-step tasks and pipeline-style workflows.",
            "coordination_mechanism": "DAG-structured coordination (explicit dependency graph between tasks/agents).",
            "communication_protocol": "Not specified in this paper; implicit exchange following graph edges.",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Depends on DAG execution ordering; not specified here.",
            "task_domain": "General multi-agent task orchestration with interdependent subtasks.",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Mentioned in related work; no experiments reported here.",
            "coordination_benefits": "Clarifies dependencies and optimizes flow among agents for interdependent tasks.",
            "coordination_challenges": "Not detailed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.6",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ToolSandbox",
            "name_full": "ToolSandbox",
            "brief_description": "A stateful, conversational, interactive evaluation benchmark for LLM tool use; includes a user simulator, environment executor, milestones (must-happen events), and minefields (forbidden events).",
            "citation_title": "Toolsandbox: A stateful, conversational, interactive evaluation benchmark for llm tool use capabilities",
            "mention_or_use": "mention",
            "system_name": "ToolSandbox",
            "system_description": "Evaluation framework that simulates user and environment interactions, pre-defines milestones and minefields for sessions, and stores execution context (world state) for tool-using agent evaluations.",
            "number_of_agents": "Not a multi-agent system per se; an evaluation platform used to benchmark multi-agent or single-agent tool use.",
            "agent_specializations": "N/A — focuses on evaluation of tool-using agents via simulation.",
            "research_phases_covered": "Execution and evaluation of agent tool use in interactive sessions.",
            "coordination_mechanism": "N/A for Task orchestration — it's an evaluation platform; supports simulated tool calls and milestones to judge correct behavior.",
            "communication_protocol": "Simulated function/tool calls plus conversation logging; specifics not deeply described here.",
            "feedback_mechanism": "Milestones and minefields provide structured evaluation feedback; judge compares trajectories to expected events.",
            "communication_frequency": "Determined by simulated interactions; not specified here.",
            "task_domain": "Evaluation of LLM tool use across interactive tasks (general-purpose).",
            "performance_metrics": "Not reported in this paper (paper references ToolSandbox as related work).",
            "baseline_comparison": "Mentioned for comparison of evaluation approaches; no direct experimental comparison reported here.",
            "coordination_benefits": "Provides structured and stateful evaluation for tool-using agents and can emulate environment effects.",
            "coordination_challenges": "Not discussed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.7",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AgentEval",
            "name_full": "AgentEval: multi-agent evaluation setup",
            "brief_description": "An evaluation setup with a multi-agent judge trio (CriticAgent, QuantifierAgent, VerifierAgent) that decomposes evaluation into criteria generation, quantification, and verification.",
            "citation_title": "Assessing and verifying task utility in LLMpowered applications",
            "mention_or_use": "mention",
            "system_name": "AgentEval",
            "system_description": "Multi-agent evaluation pipeline: CriticAgent derives coarse-grained success criteria from a task description, QuantifierAgent measures whether trajectories meet criteria (possibly producing scalar scores), and VerifierAgent checks accuracy/completeness of the evaluation.",
            "number_of_agents": "3 (CriticAgent, QuantifierAgent, VerifierAgent) in the evaluation pipeline described.",
            "agent_specializations": "Critic (criteria generation), Quantifier (measurement/scoring), Verifier (validation of evaluation).",
            "research_phases_covered": "Evaluation and verification of agent trajectories and utility.",
            "coordination_mechanism": "Sequential evaluation pipeline among the three judge agents.",
            "communication_protocol": "Not specified in detail in this paper beyond LLM-based agent interactions for judging.",
            "feedback_mechanism": "Quantified scores and verification results that can be used to accept/reject trajectories.",
            "communication_frequency": "One-time per trajectory evaluation; specifics depend on evaluation workflow.",
            "task_domain": "Evaluation across LLM-powered application tasks (general-purpose).",
            "performance_metrics": "Not reported in this paper (referenced as related work).",
            "baseline_comparison": "Not provided here.",
            "coordination_benefits": "Decomposes evaluation into specialized judge roles enabling scalable, structured judgment.",
            "coordination_challenges": "Paper notes CriticAgent criteria can be coarse-grained; detailed limitations in original work are not reproduced here.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.8",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AgentQuest",
            "name_full": "AgentQuest: modular benchmark framework",
            "brief_description": "A modular benchmark that measures agent success by progress rate based on milestone completion; milestones are environment hidden states needed to reach final solution.",
            "citation_title": "AgentQuest: A modular benchmark framework to measure progress and improve LLM agents",
            "mention_or_use": "mention",
            "system_name": "AgentQuest",
            "system_description": "Benchmarking framework that defines milestones (environment hidden states) and measures agent progress rate by counting completed milestones; supports externally annotated or programmatically defined milestones for game-like environments.",
            "number_of_agents": "N/A (benchmarking framework rather than a multi-agent system itself).",
            "agent_specializations": "N/A (framework for evaluating agents).",
            "research_phases_covered": "Evaluation/progress measurement in task-solving trajectories.",
            "coordination_mechanism": "N/A for agent coordination; provides milestone-based measurement for agents.",
            "communication_protocol": "N/A.",
            "feedback_mechanism": "Progress-rate feedback derived from milestone completion.",
            "communication_frequency": "N/A.",
            "task_domain": "Game-like datasets and environments (e.g., ALFWorld, Sudoku) and general agent evaluation tasks.",
            "performance_metrics": "Not reported in this paper (referenced as related work).",
            "baseline_comparison": "Not provided here.",
            "coordination_benefits": "Provides fine-grained progress assessment via milestones.",
            "coordination_challenges": "Limited to environments where milestones are well-defined or annotatable.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.9",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ChatDev",
            "name_full": "ChatDev: Communicative agents for software development",
            "brief_description": "A multi-agent approach for software development where agents coordinate in a centralized hierarchy and decision-making mode to mimic software workflow.",
            "citation_title": "ChatDev: Communicative agents for software development",
            "mention_or_use": "mention",
            "system_name": "ChatDev",
            "system_description": "Multi-agent system designed for software development tasks with centralized team hierarchy and centralized decision-making; assigns communicative roles to agents to complete software engineering workflows.",
            "number_of_agents": "variable (team of role-specialized agents; details not expanded in this paper)",
            "agent_specializations": "Role-oriented agents for software development (implied: design, implement, test roles), but specifics not enumerated here.",
            "research_phases_covered": "Software development phases (design, implementation, possibly test/review).",
            "coordination_mechanism": "Centralized hierarchy and centralized decision-making (as noted in related work).",
            "communication_protocol": "Conversational natural-language prompts between agents (details not provided here).",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "Software development.",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Mentioned as related work; no experimental comparison presented here.",
            "coordination_benefits": "Centralized coordination maps well to a software-company metaphor for role assignment and task control.",
            "coordination_challenges": "Not detailed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.10",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents: Interactive simulacra of human behavior",
            "brief_description": "Framework for creating interactive agent-based simulations of human-like behavior; emphasizes social behavior and psychology-inspired interactions among agents.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "system_name": "Generative Agents",
            "system_description": "Agent simulation framework where agents exhibit human-like memory and behavior, used to study social behavior and interactions; mentioned as part of related work emphasizing decentralized behaviors and psychology/social science influences.",
            "number_of_agents": "variable (agent populations in simulated environments; exact counts dependent on simulation).",
            "agent_specializations": "Agents modeled as human-like actors with memory and behavior; not specialized by computational role in the context cited here.",
            "research_phases_covered": "Behavioral simulation and interaction studies; not focused on scientific research workflows in this paper.",
            "coordination_mechanism": "Agent interactions guided by simulated memory and social behaviors (decentralized interactions).",
            "communication_protocol": "Natural-language simulation and internal memory representations (details in original work, not expanded here).",
            "feedback_mechanism": "Social/interaction-based feedback mechanisms intrinsic to simulation; not detailed in this paper.",
            "communication_frequency": "Continuous interactions as driven by simulation events.",
            "task_domain": "Interactive social simulations and behavioral research.",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Referenced as related work; no comparison in this paper.",
            "coordination_benefits": "Useful for studying emergent social behaviors and psychology-inspired agent interactions.",
            "coordination_challenges": "Not discussed in this paper.",
            "ablation_studies": "Not provided in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2467.11",
            "source_info": {
                "paper_title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MetaGPT: Meta programming for a multi-agent collaborative framework",
            "rating": 2,
            "sanitized_title": "metagpt_meta_programming_for_a_multiagent_collaborative_framework"
        },
        {
            "paper_title": "Camel: Communicative agents for \"mind\" exploration of large language model society",
            "rating": 2,
            "sanitized_title": "camel_communicative_agents_for_mind_exploration_of_large_language_model_society"
        },
        {
            "paper_title": "AutoGen: Enabling next-gen llm applications via multi-agent conversation framework",
            "rating": 2,
            "sanitized_title": "autogen_enabling_nextgen_llm_applications_via_multiagent_conversation_framework"
        },
        {
            "paper_title": "Toolsandbox: A stateful, conversational, interactive evaluation benchmark for llm tool use capabilities",
            "rating": 2,
            "sanitized_title": "toolsandbox_a_stateful_conversational_interactive_evaluation_benchmark_for_llm_tool_use_capabilities"
        },
        {
            "paper_title": "Assessing and verifying task utility in LLMpowered applications",
            "rating": 2,
            "sanitized_title": "assessing_and_verifying_task_utility_in_llmpowered_applications"
        },
        {
            "paper_title": "AgentQuest: A modular benchmark framework to measure progress and improve LLM agents",
            "rating": 1,
            "sanitized_title": "agentquest_a_modular_benchmark_framework_to_measure_progress_and_improve_llm_agents"
        },
        {
            "paper_title": "ChatDev: Communicative agents for software development",
            "rating": 1,
            "sanitized_title": "chatdev_communicative_agents_for_software_development"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 1,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        }
    ],
    "cost": 0.0231435,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications</h1>
<p>Raphael Shu, Nilaksh Das, Michelle Yuan, Monica Sunkara, Yi Zhang<br>AWS Bedrock</p>
<h4>Abstract</h4>
<p>AI agents powered by large language models (LLMs) have shown strong capabilities in problem solving. Through combining many intelligent agents, multi-agent collaboration has emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents. However, designing the collaboration protocols and evaluating the effectiveness of these systems remains a significant challenge, especially for enterprise applications. This report addresses these challenges by presenting a comprehensive evaluation of coordination and routing capabilities in a novel multi-agent collaboration framework. We evaluate two key operational modes: (1) a coordination mode enabling complex task completion through parallel communication and payload referencing, and (2) a routing mode for efficient message forwarding between agents. We benchmark on a set of handcrafted scenarios from three enterprise domains, which are publicly released with the report. For coordination capabilities, we demonstrate the effectiveness of inter-agent communication and payload referencing mechanisms, achieving end-toend goal success rates of $90 \%$. Our analysis yields several key findings: multi-agent collaboration enhances goal success rates by up to $70 \%$ compared to single-agent approaches in our benchmarks; payload referencing improves performance on code-intensive tasks by $23 \%$; latency can be substantially reduced with a routing mechanism that selectively bypasses agent orchestration. These findings offer valuable guidance for enterprise deployments of multi-agent systems and advance the development of scalable, efficient multi-agent collaboration frameworks.</p>
<h2>1 Introduction</h2>
<p>The rapid advancement of AI agents driven by large language models (LLMs) [3] has opened new frontiers towards solving complex problems. Based on the strong reasoning and tool-use capabilities, an agent can plan and execute multiple steps for actions until the goal of problem solving is reached [23]. However, as the complexity of real-world challenges continues to grow, there is an increasing need for scaling up the agent-based systems by coordinating with multiple agents with diverse specializations [17].
Towards tackling multi-faceted real-world problems, multi-agent system (MAS) research emerged in the mid-1980s to early-1990s as a critical sub-field of artificial intelligence focused on developing computational systems composed of multiple interacting intelligent agents [18]. Researchers sought to create frameworks where autonomous entities could communicate, coordinate, and solve problems collectively [20]. With the rise of LLM-based AI agents in recent years, the key challenges in MAS research regained focal attention in the new Generative AI (GenAI) era [8]. While earlier MAS work drew inspirations heavily from fields like distributed computing and game theory, new LLM-based GenAI agent research looks further into inter-disciplinary influence from psychology and social science as the AI agents start to demonstrate human-like intelligence as well as social behavior [16].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the hierarchical agents approach for multi-agent collaboration. In a centralized hierarchy, a supervisor agent oversees and assigns tasks to specialist agents. The figure demonstrates a multi-layer hierarchy, where an agent can function as both a specialist agent and a supervisor agent.</p>
<p>One particularly fruitful research avenue in GenAI MAS research is the exploration of multi-agent collaboration (MAC) [9]. Operating under the "collaborative assumption" [13] - a premise that agents are fundamentally motivated to achieve shared or compatible goals and prioritize collective problem solving over individual self-interest - multi-agent collaboration aims to address the key challenges such as communication protocols, goal alignment, group decision-making, scalability, and trust and reliability [4, 21, 25]. In this paper, we present a particular design of MAC framework targeting the development of MAC applications for real-world enterprise use cases. Three research questions arise for having multiple LLM-based agents to work together: 1) how to define the collaboration mechanism, 2) how to facilitate efficient knowledge exchange between agents, and 3) how to evaluate the effectiveness and efficiency of collaboration. In this technical report, we propose strategies to address these research questions. Based on collected evaluation datasets, we report the evaluation results and detailed analysis on the effectiveness and efficiency of multi-agent collaboration.</p>
<p>Collaboration Mechanism Collaboration mechanisms define how agents interact with each other to achieve a common goal [24]. Two key aspects are important: team hierarchy and decision-making mechanisms. Both hierarchy and decision-making can be either centralized or decentralized [7]. In a centralized hierarchy, a central authority assigns or delegates tasks to agents, whereas in a decentralized hierarchy, agents take their own initiatives. On the other hand, centralized decisionmaking involves a single agent making the final decision, while decentralized decision-making often relies on consensus or voting among multiple agents.</p>
<p>The literature presents various approaches to these mechanisms. For example, ChatDev [17] employs a centralized team hierarchy and decision-making mechanism, while MAD [19] and Generative Agents [16] adopt decentralized hierarchies and decision-making processes.</p>
<p>In MAC, we start by exploring centralized approaches which we refer to as hierarchical agents, and expand to decentralized mechanisms as future work. In hierarchical agents, each team has a tree-like hierarchy, with the root agent responsible for the the team goal and the "leaf" agents responsible for sub-tasks. We refer to the root agent as supervisor Agent. The supervisor Agent is required to perform task planning, break down the task, assign sub-tasks, and facilitate communication between specialist agents.</p>
<p>As shown in Figure 1, such a hierarchy can extend to multiple levels, with leaf agents acting as supervisors for other specialist agents. This hierarchical approach allows each leaf agent to focus on their specialized tasks, while the Supervisor Agent manages planning, delegation, and coordination. Unlike building a monolithic agent capable of solving a wide range of tasks, this approach enables the LLM behind each agent to maintain a limited context relevant to their specific role. Additionally, developing and benchmarking specialist agents becomes more manageable, and the development</p>
<p>process can be distributed across multiple agent developers. Note that although the supervisor agent can delegate tasks to other agents, it is still the responsibility of the supervisor agent to complete the task and bring the results back to the user.</p>
<p>Inter-Agent Knowledge Exchange Knowledge exchange between agents is a fundamental aspect of multi-agent collaboration. In this technical report, we focus on enabling the most basic form of knowledge exchange: message passing. Each agent can send direct messages to other visible agents. Within the context of hierarchical agents, an agent can send and receive messages from its supervisor and, potentially, from specialist agents. We initially support synchronized communication, where message passing temporarily blocks the execution of the sender agent until a response is received. This protocol can be extended to support asynchronous communication, allowing the sender agent to continue execution while waiting for a response.</p>
<p>Multi-Agent Evaluation Benchmarking single AI agents is already difficult [10] and increasing the number of agents to benchmark only complicates the problem. While an ideal approach is to have a human try out the multi-agent system and evaluate whether every conversation is successful or not, this kind of online evaluation is too expensive and not scalable for fast prototyping. To rapidly evaluate our multi-agent systems, we introduce assertion-based benchmarking as a way to leverage model-based evaluation and avoid dependency on collecting ground-truth conversation trajectories. For assertion-based benchmarking, we collect 90 scenarios from three different enterprise application domains. We open-source the benchmarking dataset and conversation evaluation script. ${ }^{2}$
For enterprise applications, efficient collaboration is crucial, as many are latency-sensitive. To address this, we further optimize the multi-agent collaboration framework by introducing payload referencing and a dynamic routing mechanism. The remainder of this report provides details on methodologies and optimizations in Section 3 Evaluation results and analysis are presented in Sections 4 to 6. Finally, we discuss limitations and future work in Sections 7 to 8.</p>
<h1>2 Related Work</h1>
<p>Multi-agent GenAI Systems An agent is defined as "an entity which is placed in an environment and senses different parameters that are used to make a decision based on the goal of the entity." [10]. The motivation for multi-agent systems is to have agents collaborate on a complex task that could not have been accomplished by a single agent. With the rise of LLMs, researchers have proposed leveraging the reasoning and planning capabilities of these models to develop more sophisticated multi-agent systems. Examples of such multi-agent LLM systems include MetaGPT [9] and CAMEL [12]. MetaGPT is one of the first multi-agent LLM projects that tries to mimic a software company. Developers can provide a standard operating procedure and MetaGPT tries to assign roles to various agents. CAMEL, or Communicative Agent Framework, promotes independent collaboration between LLM agents. Its key innovation is the use of "inception prompting," a method that steers conversational agents to complete tasks. Beyond its practical applications, CAMEL also functions as a research platform. It enables the creation and analysis of conversational data, offering valuable insights into the behavior and interactions of communicative agents. Across these works, much of the emphasis is on customization and coordination, which was much less prominent in traditional multi-agent systems [18, 20].</p>
<p>Multi-agent Frameworks and Platforms CrewAI [5] is designed to enhance task execution by organizing agents into specialized roles, similar to team members in a crew. This approach emphasizes task decomposition, where a complex problem is broken down into smaller, manageable subtasks. Each agent is assigned a specific role based on its expertise, allowing for efficient problemsolving. AutoGen [22] represents a significant advancement in enabling multi-agent systems to engage in sophisticated conversations. This framework allows agents to communicate and collaborate by sharing information and refining their outputs through iterative interactions. By simulating human-like dialogues, AutoGen enables agents to negotiate, plan, and execute tasks collaboratively.
LangGraph [11] introduces an innovative framework for organizing agent interactions using directed acyclic graphs (DAGs). This structure allows for clear visualization and management of dependencies</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>between tasks and agents. By leveraging DAGs, LangGraph optimizes the flow of information and decision-making processes among agents. This approach enhances the system's ability to handle complex, interdependent tasks by ensuring that each agent's actions are informed by the outcomes of preceding steps.</p>
<p>End-to-end Agent Evaluation Frameworks While our work on assertion-based benchmarking is ongoing, there are other works in the literature that are similar. AgentEval [2] has a multi-agent setup where there are three agents to evaluate conversation trajectories: 1) CriticAgent, 2) QuantifierAgent, 3) VerifierAgent. The CriticAgent takes the task description as input and outputs a list of criteria for task success. The QuantifierAgent then assesses whether the trajectories meet the criteria and the outputs here can be a scalar value. The VerifierAgent will finally verify that the evaluation is accurate and complete. Note that the criteria proposed by the CriticAgent is much more coarse-grained like "clarity" and "efficiency".
AgentQuest [6] proposes to measure success with a "progress rate" that is based on a set of milestones. The progress rate measures how many milestones are completed and milestones are defined to be "environment hidden states the agent needs to reach to get the final solution of the task". AgentQuest focuses benchmarking agents for game-like datasets like ALFWorld and Sudoku, so state naturally comes with the environment when the agent plays the game. These milestones can either be externally annotated or programmatically defined within the simulation.
Most recently, ToolSandbox [14] is the work in literature most similar to our evaluation setup. ToolSandbox also includes a user simulator and environment executor for tools. Before the simulations, they also pre-define "milestones" and "minefields" for each session. Milestones are events that must occur during the conversation and minefields are those that should not happen. These milestones and minefields are similar to our assertions that cover actions and agent behaviors. In addition, their implementation stores an "Execution Context" that contains the "world state" to mimic tasks that manipulate a resource like a database.</p>
<h1>3 Modeling</h1>
<p>Multi-agent collaboration enables developers to combine specialized agents to solve complex problems. Each agent can be independently developed, optimized, and configured to leverage its unique strengths. Compared to single-agent workflows, multi-agent collaboration integrates the complementary capabilities and expertise of agents with different specializations, making it highly effective for addressing complex tasks. Developers can achieve amplified capability, flexibility, and scalability by deploying a team of agents. Moreover, multi-agent solutions can have higher robustness and fault tolerance by using redundant agents in the team. For complex tasks, multi-agent solutions can improve efficiency by distributing the tasks to multiple agents and parallelizing the execution.
From a developer experience perspective, the development process is simplified by dividing functionalities among multiple agents. Developers can potentially reuse and compose existing agents for different multi-agent solutions. In some cases, cost-effective solutions can be built by utilizing low-cost orchestrating LLMs for specific agents. In this section, we review the primary features of MAC, which include inter-agent communication, payload referencing, and dynamic agent routing.</p>
<h3>3.1 Inter-Agent Communication</h3>
<p>We model the inter-agent communication capability as a specialized tool that can be leveraged by the supervisor agent. This approach allows us to seamlessly extend the agents' communication abilities by integrating it with the existing function calling capability. The key aspects of this approach are:</p>
<ol>
<li>Unified Communication Interface: The user is treated as another agent in the system, allowing for a consistent communication interface across all interactions - whether it's between the user and supervisor agent, or between the supervisor agent and specialist agents.</li>
<li>Parallel Communication: The supervisor agent can engage in parallel communication with multiple specialist agents simultaneously, enabling more efficient task completion through concurrent information exchange (Figure 2).</li>
<li>Leveraging Existing Function Calling Capability: By modeling communication as a tool, the supervisor agent can utilize the same underlying mechanisms for function calling to</li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of parallel agent communication. In this example, the supervisor agent simultaneously communicates with multiple agents as the tasks can be completed independently.
facilitate inter-agent messaging. This ensures a cohesive integration with the foundational model's existing tool-use capabilities.</p>
<p>We provide the supervisor agent with a tool called send_message, which has two parameters: recipient and content. This tool allows the supervisor agent to send messages to other agents. Additionally, the incoming messages from specialist agents are tagged in the following format:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;message</span><span class="w"> </span><span class="na">from=</span><span class="s">&quot;$SOURCE_AGENT&quot;</span><span class="nt">&gt;</span>
...
<span class="nt">&lt;/message&gt;</span>
</code></pre></div>

<p>Overall, this approach to inter-agent communication helps to create a more unified and extensible multi-agent collaboration framework.</p>
<h1>3.2 Payload Referencing</h1>
<p>Payload referencing is a specialized mechanism designed to handle the exchange of large content blocks, particularly code snippets (Figure 3). This mechanism aims to reduce the latency of the supervisor agent by allowing direct injection of text extracted from past multi-party communication into the message content. This is an important optimization for inter-agent communication, as the supervisor agent often needs to provide relevant context from previous interactions when communicating with specialist agents.
For example, let's say the supervisor agent (Agent A) needs to ask a specialist agent (Agent B) to perform a specific task based on the output of another specialist agent (Agent C). Instead of having Agent A regenerate the full context and details of the message from Agent C, the payload referencing mechanism allows Agent A to simply reference the relevant text from its past interactions. This can significantly reduce the number of output tokens required in the message to Agent B, leading to faster communication and reduced latency.
When a specialist agent generates a message containing structured content (e.g., code blocks), the system automatically detects these sections. For each incoming message to the supervisor agent, the detected content blocks, referred to as payloads, are assigned unique identifiers and wrapped with special tags that include these identifiers before being sent to the supervisor agent. Instead of repeatedly regenerating large static payloads in its outgoing messages to other specialist agents, the supervisor agent is instructed to reference previously detected payloads using the assigned identifiers. This allows the supervisor agent to use a simplified reference tag when sending messages. For every outgoing message from the supervisor agent, the system detects these reference tags and replaces</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example of payload referencing mechanism. In this example, the Coder agent delivers code which is then detected and tagged. The supervisor agent can then use the tag as a reference which would then be expanded to the original content for the Test agent.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Dynamic agent routing, where an incoming request can be routed directly to a specialist agent, with their messages relayed back to the user.
them with the corresponding payloads before sending them to the other specialist agents. This technique enables the supervisor agent to reference payloads by generating a significantly reduced number of tokens compared to generating the entire payload itself.
Our ablation experiments with the payload referencing capability demonstrated a $27 \%$ relative reduction in the average communication overhead per turn of the supervisor agent. This is discussed in more detail in Section 6.1.</p>
<h1>3.3 Dynamic Agent Routing</h1>
<p>Given a complex problem, a supervisor agent often needs to communicate with multiple specialist agents across several rounds to complete the task. However, when the incoming request is simple and relevant to only a single specialized agent, triggering the full coordination capability of the supervisor agent introduces unnecessary overhead, slowing down the collaboration. In such cases, the supervisor</p>
<p>agent only needs to route the incoming message to the appropriate specialist agent, avoiding the inefficiency of the full orchestration process.</p>
<p>To address this communication overhead, we introduce a dynamic agent routing mechanism that selectively bypasses the supervisor agent's orchestration when the incoming message only requires simple routing, such as the example shown in Figure 4. The routing decision is made using a fast classifier that predicts whether the incoming message can be directly routed without additional processing. If the classifier is uncertain, it can still trigger the full orchestration process as a fallback. Note that the supervisor agent is always aware of the routing actions, and the communication between the requester and specialist agents will be available in the agent memory even the orchestration process is bypassed.
Applying dynamic agent routing can substantially improve the efficiency of multi-agent collaboration, particularly for latency-sensitive use cases. However, the success of dynamic agent routing relies on an accurate classifier capable of determining when a request requires the supervisor agent's processing with low latency. In our experiments, we demonstrate that this classification step can achieve $\geq 90 \%$ accuracy with a latency of approximately 350 ms .</p>
<h1>4 End-to-end Automatic Evaluation</h1>
<p>Recent literature have noted the challenges associated with LLM agents benchmarking, which are often due to the dynamic and complex nature of the problem. The definition of success is often unclear as it can refer to either from the user perspective or from the environment/system, as users may not fully know what happens "behind-the-scenes". Moreover, benchmarks may not consider that user feedback can help agents achieve their goals. If the user is not properly simulated, then the evaluation undermines the agent's capability to orchestrate tasks with a human in the loop.</p>
<p>Prior single-agent benchmarking is more static where user inputs and follow-up responses are predefined before evaluation. The gold-truth actions would be collected along with the conversations, which would then be compared to the actions generated by the agent. This static setup already has some issues because it assumes that the user goals can only be fulfilled through executing a certain set of actions. In reality, there may be multiple trajectories that enable the agent to fulfill user requests. If those trajectories are not captured in the gold-truth, then the agent is incorrectly penalized.</p>
<p>To generalize evaluation metrics for agents, we formally define the success of agent for a given user $u$ and scenario $s$ as $X_{u}^{s}$, a Bernoulli random variable that represents success or failure for the user-agent session. A scenario is defined as the setting for a session which includes user goals and task domain. Since user profiles and scenarios vary, we are interested in the expected value of success for any user-agent session:</p>
<p>$$
\mathbb{E}<em u="u">{U, S}\left[X</em>
$$}^{s}\right]=p_{\text {success }</p>
<p>where $p_{\text {success }}$ is the success rate of an agent with a sampled user from user pool $U$ and scenario sampled from collection $S$. The objective of benchmarking is to approximate $p_{\text {success }}$.
We can easily extend this formulation to multi-agent systems. However, with more agents added, the complexity grows and $p_{\text {success }}$ is more difficult to approximate. Several, different trajectories could be considered correct and sometimes no actions need to be executed to achieve user goals. We propose assertion-based benchmarking as a way to better approximate likelihood of agent success.
The assertion-based evaluation framework relies on three components: 1) benchmarking data collection, 2) environment simulators, 3) automatic assertion judge (Figure 5). First, a set of scenarios need to be collected according to pre-defined agent profiles and tool schemas for a particular domain. With each scenario, a list of assertions needs to be included. These assertions are statements that must hold true for a conversation to be considered successful. This is similar to debugging software with test cases. Second, the user simulator is initialized from the scenario and input problem and begins to interact with the multi-agent system. An action simulator is also used to simulate the tool calls from the multi-agent system. The trajectories are recorded for further evaluation. Finally, a judge determines the validity of each assertion. Based on the number of correct assertions, goal success rates can be computed to help measure the success of the multi-agent system. In the following subsections, we cover each component of the framework in depth.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Overview of end-to-end assertion-based benchmarking with scenarios and assertions</p>
<h1>4.1 Benchmarking Data</h1>
<p>There are three artifacts that need to be collected for each scenario. First, the scenario description itself must list the user goals and any background information. The information in the scenario description is critical because the user simulator will be grounded on the description. Then, the second artifact is the input problem, which is the first turn of the conversation from the user. The input problem will begin the benchmarking simulation.
The final artifact is a list of assertions that need to be satisfied during the simulation. Assertions are categorized two types: user-side and system-side. User-side assertions cover behaviors of the multi-agent system that can be observed by the user. On the other hand, system-side assertions cover behaviors of the multi-agent system that cannot be observed by the user. These assertions may include tool calling correctness, parameter correctness, inter-agent behavior, or rule compliance. Appendix A shows example artifacts from our publicly released benchmarking data collection.</p>
<h3>4.2 Simulation</h3>
<p>For each session, we feed the scenario description and the input problem to a user simulator. The user simulator is a LLM that is prompted to follow the scenario description. The simulation begins with the input problem as the first turn of the session. The input problem is delivered to the supervisor agent who then begins to work the specialist agents. If the specialist agents need to invoke an action, the function calls are passed to the action simulator, which is a LLM grounded on the provided tool schemas. The action simulator also has access to past tool invocations so that it can generate results that is aligned to past observations. After the action simulator returns the simulated action, the specialist agents continue to carry out the task.
During the simulation, the user simulator will continue to interact with the supervisor agents. The user simulator can either help clarify any questions from the supervisor agent or keep sending new task requests. Any requests or answers given by the user simulator should be aligned with the information in the scenario. Once the user simulator determines that all the goals in the scenario are met, the user simulator generates a </stop> token to end the simulation. Otherwise, we set a maximum number of user simulation turns to 5 to prevent simulations that fail to end.</p>
<h3>4.3 Assertion Judgements</h3>
<p>After simulations are completed, we pass the trajectories to a LLM judge to help automate the assertion evaluation. Along with the simulation trajectories, we also pass the scenario and the assertions to the judge. The judge returns whether each assertion is True or False, and includes the reason for their judgement. The reason often exhibits evidence from the conversation, which can</p>
<p>Table 1: Definitions of Success Metrics</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Definition</th>
<th style="text-align: left;">Implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overall GSR</td>
<td style="text-align: left;">Overall goal success rate covering both <br> the user-side and the system-side</td>
<td style="text-align: left;">Use LLM to judge user-side and system- <br> side assertions. For a conversation, score <br> is 1 if all assertions are True; else 0.</td>
</tr>
<tr>
<td style="text-align: left;">Supervisor <br> GSR</td>
<td style="text-align: left;">Goal success rate of the supervisor agent <br> without any dependence on sub-agent <br> and tool behavior</td>
<td style="text-align: left;">If overall GSR is 1 or supervisor agent <br> is reliable (see reliability metrics), then <br> score for the conversation is 1; else 0.</td>
</tr>
<tr>
<td style="text-align: left;">User-side GSR</td>
<td style="text-align: left;">Goal success rate in the perspective of <br> the user</td>
<td style="text-align: left;">Use LLM to judge user-side assertions. <br> For a conversation, score is 1 if all user- <br> side assertions are True; else 0.</td>
</tr>
<tr>
<td style="text-align: left;">System-side <br> GSR</td>
<td style="text-align: left;">Goal success rate in the perspective of <br> the system developers</td>
<td style="text-align: left;">Use LLM to judge system-side asser- <br> tions. For a conversation, score is 1 if all <br> system-side assertions are True; else 0.</td>
</tr>
</tbody>
</table>
<p>Table 2: Definitions of Latency Metrics</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Avg. communication <br> overhead per turn</td>
<td style="text-align: left;">Average number of seconds that the supervisor agent spends communi- <br> cating with other agents before getting back to the user. This time does <br> not take into account the duration of agents other than the supervisor <br> agent.</td>
</tr>
<tr>
<td style="text-align: left;">Avg. latency per com- <br> munication</td>
<td style="text-align: left;">Average number of seconds that the supervisor agent spends to deliver <br> each message to communicate with other agents.</td>
</tr>
<tr>
<td style="text-align: left;">Avg. user-perceived <br> turn latency per session</td>
<td style="text-align: left;">Average number of seconds it takes for the supervisor agent to get back <br> to the user. This time does take into account the duration of all agents in <br> the system.</td>
</tr>
<tr>
<td style="text-align: left;">Avg. communications <br> per session</td>
<td style="text-align: left;">Average count of messages sent by the supervisor agent over the entire <br> session.</td>
</tr>
<tr>
<td style="text-align: left;">Avg. output tokens per <br> communication</td>
<td style="text-align: left;">Average number of total output tokens from the supervisor agent for <br> each message.</td>
</tr>
</tbody>
</table>
<p>easily show why the assertion has succeeded or failed. This is important as multi-agent conversations can be very lengthy and difficult for people to pinpoint the causes of failures.</p>
<h1>4.4 Metrics</h1>
<p>A conversation is considered overall successful if all assertions, both user-side and system-side, are True. We then measure Goal Success Rate (GSR) as the percentage of conversations that have all assertions evaluated as True. This overall GSR is our primary measure of success. We also compute User GSR and System GSR, which are the variants of GSR where the assertions being evaluated are limited to one type (either user-side or system-side). These metrics are useful to understand whether multi-agent systems are failing from the perspective of the user or the system (Table 1).
Beyond goal success, latency is a critical metric in multi-agent systems. As these systems involve multiple agents interacting and collaborating to perform complex tasks, the time delay between agent communications and actions can significantly impact user experience. Minimizing latency is crucial for ensuring the system can operate efficiently, especially in time-sensitive applications. Table 2 lists the various latency metrics that are included in this report.
Lastly, we also included metrics to evaluate routing mode. For routing mode, we are concerned with routing classification accuracy, false agent switch rates, turn-level routing overhead, and routing classification latency (Table 3).</p>
<p>Table 3: Definitions of Routing Metrics</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Classification Accuracy</td>
<td style="text-align: left;">Accuracy of routing decisions, calculated based on human anno- <br> tated ground-truth labels.</td>
</tr>
<tr>
<td style="text-align: left;">False Agent Switch Rate</td>
<td style="text-align: left;">Ratio of routing decisions causing the handling agent to be <br> switched to a wrong agent</td>
</tr>
<tr>
<td style="text-align: left;">Turn-level Routing Overhead <br> (ms)</td>
<td style="text-align: left;">Time taken in the primary agent (including routing classification <br> and orchestration) within a user turn.</td>
</tr>
<tr>
<td style="text-align: left;">Classification Latency (ms)</td>
<td style="text-align: left;">Time taken for classifying a single routing decision.</td>
</tr>
</tbody>
</table>
<p>Table 4: Dataset statistics, including average number of goals per scenario, average number of assertions per scenario, total number of action groups, and total number of APIs/tools.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Avg. Goals per Scenario</th>
<th style="text-align: center;">Avg. Assertions per Scenario</th>
<th style="text-align: center;">Action Groups</th>
<th style="text-align: right;">APIs/Tools</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">11</td>
<td style="text-align: right;">52</td>
</tr>
<tr>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">1.91</td>
<td style="text-align: center;">3.99</td>
<td style="text-align: center;">10</td>
<td style="text-align: right;">35</td>
</tr>
<tr>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">1.69</td>
<td style="text-align: center;">7.47</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;">4</td>
</tr>
</tbody>
</table>
<h1>5 Experimental Results</h1>
<p>To understand how MAC performs for enterprise usage, we choose three business domains to experiment with. For each domain, we set up a set of agents and tools. Then, we manually collect benchmarking data for each domain. In this report, we show experiments on thirty scenarios from each domain. This dataset is also publicly released for others to benchmark their own multi-agent systems (Section 1). The three domains are as follows:</p>
<ol>
<li>Travel planning: agents help user plan a trip, which includes booking flights, booking hotels, finding local events, getting information about the weather, etc.</li>
<li>Mortgage financing: agents help user with mortgage issues, e.g., submitting loan applications, querying information about properties, retrieving banking information, etc.</li>
<li>Software development: agents help user design, implement, test, review, and/or deploy code.</li>
</ol>
<p>The first two domains are conversational, whereas the third domain is more about automation and requires minimal interaction with user. For the agent profile design, we have made sure to cover the following conditions to showcase adoption of multi-agent collaboration for various developer setups:</p>
<ol>
<li>Supervisor agents with and without tools: The supervisor agent for Mortgage has access to MortgageLoans tools, whereas the supervisor agents for other domains do not have tools.</li>
<li>Specialist agents with and without tools: Many specialist agents have their own set of tools. However, some specialist agents in Software do not have tools as they should have the capability to complete the tasks without them.</li>
<li>Agent hierarchy with depth more than one: In Software, the supervisor agent can call on Deploy agent, which can then call on Infrastructure agent and Application agent.</li>
</ol>
<p>Table 4 shows an overview of the statistics for the three datasets. Action groups refer to group of tools (e.g. "BookFlight" action group is a set of tools for searching flights, booking flights, getting flight details, etc.). Travel and Mortgage each have more than thirty tools, whereas Software has only two tools. Software tends to have more assertions on average than Travel and Mortgage. Appendix B shows the agents for each domain and their associated action groups, which are the set of tools that the agents have access to.</p>
<h3>5.1 Coordination Mode Experiments</h3>
<p>We report the goal success metrics for coordination mode across a variety of settings in Table 5. For evaluating the assertions, we leverage OpenAI's GPT-4o model [15] for providing LLM-based</p>
<p>Table 5: End-to-end evaluation of Coordination Mode</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Overall GSR</th>
<th style="text-align: center;">Supervisor GSR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">Specialists: Sonnet 3.5 (20241022)</td>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">Specialists: Sonnet 3.0</td>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">Specialists: Haiku 3.5</td>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">Single-agent: Sonnet 3.5 (20241022)</td>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 6: Latency Performance of Coordination Mode</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Avg. communication overhead per turn (s)</th>
<th style="text-align: center;">Avg. user-perceived turn latency per session (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">13.75</td>
<td style="text-align: center;">31.46</td>
</tr>
<tr>
<td style="text-align: center;">Specialists: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">13.39</td>
<td style="text-align: center;">24.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">35.44</td>
<td style="text-align: center;">168.73</td>
</tr>
<tr>
<td style="text-align: center;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">15.43</td>
<td style="text-align: center;">42.12</td>
</tr>
<tr>
<td style="text-align: center;">Specialists: Sonnet 3.0</td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">15.97</td>
<td style="text-align: center;">29.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">53.48</td>
<td style="text-align: center;">137.31</td>
</tr>
<tr>
<td style="text-align: center;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">12.95</td>
<td style="text-align: center;">23.98</td>
</tr>
<tr>
<td style="text-align: center;">Specialists: Haiku 3.5</td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">11.03</td>
<td style="text-align: center;">18.13</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">36.65</td>
<td style="text-align: center;">125.31</td>
</tr>
<tr>
<td style="text-align: center;">Single-agent: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">52.61</td>
</tr>
</tbody>
</table>
<p>judgements. The results demonstrate that multi-agent collaboration achieves the best overall GSR of $90 \%$ across the three evaluated domains when the Claude 3.5 Sonnet (20241022) model [1] is utilized as both the supervisor agent and the specialist agents.
We further examine the impact of using different agent models. Switching the specialist agent model from Claude 3.5 Sonnet to Claude 3.0 Sonnet shows a significant regression in the performance for the Software development domain. This is also apparent when the specialist agent is switched to Claude 3.5 Haiku, as the performance in the Software domain is mostly recovered with this newer generation of models.
We also compare the performance of a single agent with the multi-agent collaboration approach. In this experiment, a single agent is given the tools from all specialist agents combined and is responsible for assisting the user. We reuse the assertions collected for the multi-agent setup but replace any mentions of specialist agents with supervisor agent (e.g. "flight agent books tickets" becomes "travel agent books tickets"). For any assertions about inter-agent behaviors, we would replace mentions of the primary agent with "user" and mentions of the specialist agent with the supervisor agent (e.g. "code agent implements code and delivers back to software agent" becomes "software agent implements code and delivers back to user").</p>
<p>In the single-agent setting, we observe an absolute regression of up to $37 \%$. MAC allows each specialist agent to be provided with instructions for the specific subset of tasks it is supposed to handle. This specialized task assignment may not be achievable by a single agent, which may struggle to manage the multitude of instructions required to complete the complex tasks. In the single-agent trajectories, we observe more hallucination in tool parameters and incorrect tool choice.</p>
<p>Table 7: Routing Mode Performance</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Classification <br> accuracy</th>
<th style="text-align: center;">False <br> switch <br> rate</th>
<th style="text-align: center;">Turn-level <br> routing <br> overhead (ms)</th>
<th style="text-align: right;">Classification <br> latency (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mortgage Routing (First Layer)</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">630</td>
<td style="text-align: right;">344</td>
</tr>
<tr>
<td style="text-align: left;">Mortgage Routing (Second Layer)</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">630</td>
<td style="text-align: right;">378</td>
</tr>
<tr>
<td style="text-align: left;">Travel Routing</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">750</td>
<td style="text-align: right;">360</td>
</tr>
</tbody>
</table>
<p>Table 8: End-to-end evaluation of Routing Mode</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: center;">Overall GSR</th>
<th style="text-align: center;">Supervisor GSR</th>
<th style="text-align: center;">Avg. Routing Overhead <br> per Turn (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">751</td>
</tr>
<tr>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">627</td>
</tr>
</tbody>
</table>
<p>We report the latency metrics for our experiments in Table 6. When using Claude 3.5 Sonnet (20241022) for both supervisor agent and specialist agents, the average communication overhead per turn ranges from 13.39 s to 35.44 s , with the Software domain showing significantly higher overhead due to its complexity. The Software domain consistently demonstrates higher latency metrics across all settings, with user-perceived turn latency reaching 168.73 s compared to 31.46 s for Travel and 24.42s for Mortgage domains. While the single-agent setting shows lower user-perceived latencies, this comes at the cost of reduced goal success rates as shown in the previous results. See Appendix C for full results on coordination mode.
We also performed ablation experiments with the payload referencing capability of the supervisor agent, which is discussed in more detail in Section 6.1.</p>
<h1>5.2 Routing Mode Experiments</h1>
<p>To benchmark dynamic agent routing, we curated two additional datasets for evaluation from our original benchmarking data: 1) Mortgage Routing (3 agent layers) and 2) Travel Routing (2 agent layers), and manually annotated around 100 routing classification labels for each datasets. Intrinsic evaluation shows that the LLM-based routing solution with Claude 3.0 Haiku achieves more than $90 \%$ routing classification accuracy and less than $3 \%$ false agent switching rate (Table 7). Average latency of routing classification is about 350 ms . With end-to-end evaluation, we observe turn-level routing overhead (time taken by the supervisor agents) in 600 ms to 800 ms range (Table 8).</p>
<h3>5.3 Agreement with Human Evaluation</h3>
<p>Throughout this report, we have used an LLM to provide automatic judgements on assertions. This has helped scale our experiments so that we can quickly prototype and develop improved multi-agent systems. What if we ask a human to judge conversation success purely from their own perspective without any reference to assertions? To understand how well assertion-based evaluation compares with human evaluation, we deliver a batch of ninety trajectories to human annotators during one of our milestone checkpoints. For each conversation, we ask three annotators to provide binary judgements on a different set of guidelines. During this human evaluation, they are never shown assertions or LLM judgements and only instructed to determine success and efficiency from their own judgements. Table 9 show the instructions given to human annotators for each success metric.</p>
<p>After the human annotators finish their evaluation, the aggregated majority judgement is used to compare against LLM evaluations. Note that for this milestone, we use Claude 3.5 Sonnet (20240620) for supervisor agent and Sonnet 3.0 for specialist agents. See Appendix D for the automatic and human evaluation results for this set of experiments. We then compute agreement between human and LLM judges. Since the judgements for automatic and human evaluation are binary, we use agreement ratio to measure alignment between human and automatic evaluation. The agreement ratio is the number of conversations with matching judgements (either both 1 or both 0 ) over the total number of conversations evaluated.</p>
<p>Table 9: The instructions given to annotators when performing human evaluation of conversation trajectories. Note that humans do not judge assertions and only determine success purely from their own judgements.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Human annotator instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">User GSR</td>
<td style="text-align: left;">Evaluate from user's perspective, whether the conversation with the primary <br> agent meets the user's goals and successfully address user's requests or not. <br> From the perspective of the environment, evaluate whether the actual impacts <br> of all agent actions accurately address and resolve all of the user's stated tasks, <br> requests, and expectations within that specific environment or context.</td>
</tr>
<tr>
<td style="text-align: left;">Supervisor <br> GSR</td>
<td style="text-align: left;">Evaluate from user's perspective whether the primary agent has tried its best to <br> help the User, regardless of whether task was completed or the agent actions <br> correctly impacted the environment.</td>
</tr>
</tbody>
</table>
<p>Table 10: The agreement ratios between human annotators and assertion-based benchmarking on success metrics across 90 conversation trajectories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Overall GSR</th>
<th style="text-align: center;">Supervisor GSR</th>
<th style="text-align: center;">User-side GSR</th>
<th style="text-align: center;">System-side GSR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.90</td>
</tr>
</tbody>
</table>
<p>For success metrics, the agreement is generally above $85 \%$ (Table 10). The only exception is on primary agent success for software, where the agreement is $77 \%$. Here, the automatic evaluation has overall GSR as $90 \%$ but the human annotators only believe the supervisor agent is successful in $87 \%$ of the conversations. On the trajectories where the humans disagree with LLM judgements, we observe a mix of mistakes from both ends. For some trajectories, the human evaluation would mark that supervisor agent has not make a mistake but LLM judge has captured when the supervisor agent repeatedly wanting to follow-up with the user rather than delegating tasks to sub-agents. There is also the reversed case where the LLM judge does not observe any issues with the supervisor agent but the human annotators detect that the supervisor agent neglects to conduct thorough code review with the Review agent.</p>
<h1>6 Communication Mechanism Ablations</h1>
<p>Section 5 shows the main results of our approach with different models. In this section, we provide additional ablations to quantify the impact of MAC communication mechanisms. This includes experiments with and without payload referencing, as well as comparison with open-source frameworks. The additional experiments provide more justification on the utility of MAC for enterprise applications.</p>
<h3>6.1 Impact of Payload Referencing</h3>
<p>In Table 11, we report the results of an ablation experiment with Payload Referencing for Software domain with Claude 3.5 Sonnet (20241022) as the supervisor agent as well as specialist agents. This experiment reveals that payload referencing significantly improves both the efficiency as well as effectiveness of multi-agent collaboration, particularly in the code-heavy Software domain. The impact is most pronounced here as large code blocks are frequently exchanged between agents.
Enabling payload referencing results in a $23 \%$ relative improvement in overall GSR as well as a $27 \%$ relative reduction in the average communication overhead per turn. The latter can be attributed to a $30 \%$ relative reduction in the average output tokens per communication of the supervisor agent, as it is able to leverage the payload referencing mechanism to more efficiently reduce the number of generated tokens required for sharing code blocks, while improving the goal success rate.
When enabling payload referencing, we also observe an increase in the average user-perceived turn latency. As this also includes the latency of the specialist agents, it suggests that the specialist agents operating for a smaller number of turns may be detrimental to the overall goal success.</p>
<p>Table 11: Impact of payload referencing on GSR and latency for Software</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Overall <br> GSR</th>
<th style="text-align: center;">Avg. <br> communication <br> overhead per <br> turn (s)</th>
<th style="text-align: center;">Avg. <br> user-perceived <br> turn latency <br> per session (s)</th>
<th style="text-align: center;">Avg. <br> output <br> tokens per <br> communication</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">With Payload Ref.</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">35.44</td>
<td style="text-align: center;">168.73</td>
<td style="text-align: center;">373.77</td>
</tr>
<tr>
<td style="text-align: left;">Without Payload Ref.</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">48.78</td>
<td style="text-align: center;">159.75</td>
<td style="text-align: center;">539.21</td>
</tr>
</tbody>
</table>
<p>Table 12: Comparison with a widely adopted open-source framework for task automation (OSF). Note that the version of Sonnet 3.5 used below refers to an older version.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Overall GSR</th>
<th style="text-align: center;">Supervisor GSR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">[MAC Coordination Mode]</td>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">Supervisor: Sonnet 3.5 (20240620)</td>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: left;">Specialists: Sonnet 3.0</td>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">[OSF]</td>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: left;">Supervisor: Sonnet 3.5 (20240620)</td>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">Specialists: Sonnet 3.0</td>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">[OSF]</td>
<td style="text-align: left;">Travel</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">Supervisor: GPT-4o (mini)</td>
<td style="text-align: left;">Mortgage</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Specialists: GPT-4o (mini)</td>
<td style="text-align: left;">Software</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.77</td>
</tr>
</tbody>
</table>
<p>In conclusion, payload referencing proves particularly valuable in domains such as Software, as it:</p>
<ul>
<li>Enables precise referencing of payloads without expensive regeneration of tokens</li>
<li>Reduces the likelihood of payload corruption during agent-to-agent transmission</li>
<li>Maintains formatting and structure of the payload across agent communications</li>
<li>Facilitates more efficient parallel inter-agent communication</li>
</ul>
<p>Our experiments suggest that payload referencing is a crucial optimization for multi-agent systems, particularly in domains involving structured content exchange. The mechanism not only improves system performance metrics but also enhances the quality and reliability of agent interactions.</p>
<h1>6.2 Comparison with Task Automation Framework</h1>
<p>We also conducted a comparative evaluation of our implementation against a widely adopted opensource framework (OSF) for task automation. The OSF implements multi-agent collaboration as a sequential task-automation workflow, wherein the supervisor agent receives a task from the user and is responsible for breaking it up into one-time sub-tasks for its sub-agents. However, as our evaluation is primarily based on more conversational back-and-forth between the specialist agents, the one-time task assignment may not be sufficient. Therefore, we also expanded the OSF to support multiple sub-agent interactions. The OSF also enables tool-use for its agents using ReAct-style prompting [23], as opposed to native function-calling capability of our implementation.
In Table 12, we present a comparative analysis with the OSF. Due to the prompting style of the agents in the OSF, they tend to be highly verbose and utilize a much higher number of tokens. To constrain the experiment to a reasonable budget, we employ Claude 3.5 Sonnet (20240620) as the supervisor agent and Claude 3.0 Sonnet as the specialist agents. Since the original prompts in the OSF may have been optimized for GPT-4o (mini), as suggested by the defaults in the code implementation, we also report the performance of GPT-4o (mini) with the OSF in the conversational setting.</p>
<h2>7 Discussion</h2>
<p>The results presented in this technical report demonstrate the effectiveness of MAC in enabling coordinated problem-solving through multiple specialized agents. Across the evaluated domains</p>
<p>of Travel, Mortgage, and Software Development, the multi-agent collaboration approach achieved impressive goal success rates, with the overall GSR reaching as high as $90 \%$ when utilizing the Claude 3.5 Sonnet (20241022) models for both the supervisor agent and specialist agents. This strong performance is particularly notable when compared to the single-agent setting, where a significant regression of up to $37 \%$ was observed in the Software Development domain. The ability of the multi-agent collaboration to leverage the specialized expertise of individual agents appears to be a key factor in this improved effectiveness.</p>
<p>Further analysis of the results highlights the importance of optimizing the inter-agent communication mechanisms. The impact of the payload referencing feature is most pronounced in the Software Development domain, where it led to a $23 \%$ relative improvement in overall GSR as well as a $27 \%$ relative reduction in the average communication overhead per turn. By allowing the supervisor agent to more efficiently reference and share large content blocks, such as code snippets, this specialized mechanism enhances the reliability and speed of the multi-agent interactions. However, the results also reveal some limitations of the current system. The higher latency observed in the more complex Software Development scenarios suggests that further optimizations may be needed to reduce the overhead of multi-agent coordination, particularly in time-sensitive applications.</p>
<h1>8 Conclusion</h1>
<p>This paper presents a comprehensive evaluation of the multi-agent collaboration framework, demonstrating its effectiveness in enabling coordinated problem-solving across a variety of enterprise domains. The results highlight the system's ability to leverage specialized agents to tackle complex tasks, achieving impressive goal success rates of up to $90 \%$ in the evaluated scenarios. A key aspect of MAC is the focus on optimizing the inter-agent communication mechanisms. The introduction of the payload referencing feature, for example, was shown to provide significant benefits, particularly in code-heavy tasks, by allowing the supervisor agent to more efficiently share and reference large content blocks. This optimization has led to remarkable improvements in both the overall goal success rate and the communication efficiency of the system.
Furthermore, the evaluation framework that is employed in this study, which combines assertionbased benchmarking with automated LLM-based judgements, has proven to be a reliable and scalable approach for assessing the performance of multi-agent collaborative systems. During development, we observe high agreement rates on goal success between human and automated evaluation. This validates our evaluation framework, which can enable faster prototyping and development of MAC.
One key focus for future work will be on further reducing the latency observed in more complex scenarios, such as those in the Software Development domain. Exploring additional optimizations to the inter-agent coordination mechanisms may help to address this challenge and ensure the system can operate efficiently, even in time-sensitive applications. Additionally, expanding the detection and handling of different forms of static long-form payloads could lead to additional performance gains. Investigating automatic prompt optimization techniques may also prove valuable in enhancing the collaboration capabilities of the individual agents.
Finally, automating the dataset curation process for the benchmarking framework could enable more scalable and iterative development of the multi-agent system, allowing for faster prototyping and deployment of improvements. By building on the solid foundations laid out in this technical report, the future work in these areas promises to further strengthen MAC and its ability to tackle increasingly complex, real-world challenges.</p>
<h2>Acknowledgments</h2>
<p>First, we want to thank the AWS Bedrock Agents Science team for their help in the project, notably Tamer Alkhouli, Renato Negrinho, Veera Raghavendra Elluru, and Yassine Benajiba. We thank Claudia Zaghi and Nina Rondoni for their substantial support in data collection. We thank the AWS Bedrock Product team and solution architects for their crucial feedback on enterprise needs and usecases. Finally, we want to acknowledge the AWS Bedrock Engineering team for diligently deploying the multi-agents collaboration service and delivering our work on enterprise applications to AWS customers.</p>
<h1>References</h1>
<p>[1] Anthropic. Claude 3.5 Sonnet, 2024. https://www.anthropic.com/news/ claude-3-5-sonnet, Accessed on 2024-12-06.
[2] Negar Arabzadeh, Siqing Huo, Nikhil Mehta, Qingyun Wu, Chi Wang, Ahmed Hassan Awadallah, Charles L. A. Clarke, and Julia Kiseleva. Assessing and verifying task utility in LLMpowered applications. In Empirical Methods of Natural Language Processing, 2024.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020.
[4] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023.
[5] CrewAI. CrewAI, 2024. https://www.crewai.com/, Accessed on 2024-12-06.
[6] Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, and Carolin Lawrence. AgentQuest: A modular benchmark framework to measure progress and improve LLM agents. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2024.
[7] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024.
[8] Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, and Chaoyang He. Llm multi-agent systems: Challenges and open problems. arXiv preprint arXiv:2402.03578, 2024.
[9] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In International Conference on Learning Representations, 2024.
[10] Sayash Kapoor, Benedikt Stroebl, Zachary S Siegel, Nitya Nadgir, and Arvind Narayanan. AI Agents That Matter. arXiv preprint arXiv:2407.01502, 2024.
[11] LangChain. LangGraph, 2024. https://www.langchain.com/langgraph, Accessed on 2024-12-06.
[12] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for "mind" exploration of large language model society. In Neural Information Processing Systems, 2023.
[13] Huao Li, Yu Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Charles Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration via large language models. In Empirical Methods in Natural Language Processing, 2023.
[14] Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, et al. Toolsandbox: A stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682, 2024.
[15] OpenAI. Hello GPT-4o, 2024. https://openai.com/index/hello-gpt-4o/, Accessed on 2024-12-06.
[16] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In ACM Symposium on User Interface Software and Technology, 2023.</p>
<p>[17] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. ChatDev: Communicative agents for software development. In Annual Meeting of the Association for Computational Linguistics, 2024.
[18] Munindar P Singh. Multiagent systems. Springer, 1994.
[19] Andries Petrus Smit, Nathan Grinsztajn, Paul Duckworth, Thomas D Barrett, and Arnu Pretorius. Should we be going mad? a look at multi-agent debate strategies for llms. In International Conference on Machine Learning, 2024.
[20] Katia P Sycara. Multiagent systems. AI magazine, 19(2):79-79, 1998.
[21] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023.
[22] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. AutoGen: Enabling next-gen llm applications via multi-agent conversation framework. In COLM, 2024.
[23] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations, 2023.
[24] Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, and Shumin Deng. Exploring collaboration mechanisms for LLM agents: A social psychology view. In Annual Meeting of the Association for Computational Linguistics, 2024.
[25] Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, and Xuelong Li. Towards efficient LLM grounding for embodied multi-agent collaboration. arXiv preprint arXiv:2405.14314, 2024.</p>
<h1>Appendix</h1>
<h2>A Benchmarking Data Artifacts</h2>
<p>Table 13: Example artifacts from benchmarking data collection</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Scenario</td>
<td style="text-align: center;">Goals: <br> - User needs to book a ticket for a round-trip economy flight from DEN to RST, departing on June 23, and returning on June 30. <br> - User needs to book a room in Rochester, Minnesota from June 23 to June 30. <br> - User needs to obtain total estimated cost of flight, hotel, food, and local transportation for their 7-day trip, given their $\$ 2,500$ weekly budget.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Background: <br> - User's full name is Gregory James Anderson. <br> - Gregory is 36 years old. <br> - Gregory resides in Denver, Colorado. <br> - Gregory's preferred class for flights is economy.</td>
</tr>
<tr>
<td style="text-align: center;">Input Problem</td>
<td style="text-align: center;">Please book a ticket for a round-trip economy flight from DEN to RST, departing on June 23.</td>
</tr>
<tr>
<td style="text-align: center;">Assertions</td>
<td style="text-align: center;">User-side assertions: <br> - User is informed that a ticket for an economy flight from DEN to RST departing on June 23 have been booked. <br> - User is informed that a ticket for an economy flight from RST to DEN have been booked. The flight from DEN to RST departs on June 30. <br> - User is informed that a room in a hotel in Rochester, Minnesota from June 23 to June 30 has been booked <br> - User is informed of the total estimated cost including flight, hotel, food, and local transportation for their 7-day trip from Denver to Minnesota, given their \$2,500 weekly budget.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">System-side assertions: <br> - book_flight is executed to book two tickets for a round-trip economy flight from DEN to RST on June 23 <br> - book_hotel is executed to book a hotel in Minnesota from June 23 to June 30 <br> - search_flight is executed before book_flight to provide the user with flight options before performing the booking <br> - search_hotel is executed before book_hotel to provide the user with options before booking <br> - calculate is executed to get the total estimated cost for flight, hotel, food, and local transportation for his 7-day trip from Denver to Minnesota for one person, given their $\$ 2,500$ weekly budget</td>
</tr>
</tbody>
</table>
<h1>B Agent Profiles</h1>
<p>Table 14: List of agents for each benchmarking domain. Supervisor agents are bolded. We also list their action groups, which are sets of tools that the agents have access to.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain/Usecase</th>
<th style="text-align: center;">Agent Name</th>
<th style="text-align: center;">Action Groups</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Travel Planning</td>
<td style="text-align: center;">Travel agent</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Weather agent</td>
<td style="text-align: center;">Weather</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Location search agent</td>
<td style="text-align: center;">LocationService</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Car rental agent</td>
<td style="text-align: center;">CarRental</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Flight agent</td>
<td style="text-align: center;">BookFlight</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hotel agent</td>
<td style="text-align: center;">BookHotel</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Travel budget agent</td>
<td style="text-align: center;">Calculator</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Restaurant agent</td>
<td style="text-align: center;">RestaurantSearch, FoodDelivery</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Local expert agent</td>
<td style="text-align: center;">Eventbrite, NewsSeartch</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Airbnb agent</td>
<td style="text-align: center;">BookAirbnb</td>
</tr>
<tr>
<td style="text-align: center;">Mortgage Financing</td>
<td style="text-align: center;">Mortgage agent</td>
<td style="text-align: center;">MortgageLoans</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Property agent</td>
<td style="text-align: center;">LocationService, RealEstateManagement</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Credit agent</td>
<td style="text-align: center;">Banking, CreditReport</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Income agent</td>
<td style="text-align: center;">HRPayrollBenefits, Calculator</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Payment agent</td>
<td style="text-align: center;">Calculator</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Closing agent</td>
<td style="text-align: center;">Calculator, RealEstateManagement</td>
</tr>
<tr>
<td style="text-align: center;">Software Development</td>
<td style="text-align: center;">Software agent</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Design agent</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code agent</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test agent</td>
<td style="text-align: center;">SoftwareDevelopment</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Review agent</td>
<td style="text-align: center;">SoftwareDevelopment</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Deploy agent</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Infrastructure agent</td>
<td style="text-align: center;">CodeDeployment</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Application agent</td>
<td style="text-align: center;">CodeDeployment</td>
</tr>
</tbody>
</table>
<h2>C Full results of Coordination Mode</h2>
<p>Table 15: Full End-to-end evaluation of Coordination Mode</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Overall GSR</th>
<th style="text-align: center;">Supervisor GSR</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { User- } \ &amp; \text { side } \ &amp; \text { GSR } \end{aligned}$</th>
<th style="text-align: center;">System- <br> side <br> GSR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;">Specialists: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;">Specialists: Sonnet 3.0</td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">Supervisor: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">Specialists: Haiku 3.5</td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: center;">Single-agent: Sonnet 3.5 (20241022)</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.60</td>
</tr>
</tbody>
</table>
<p>Table 16: Full Latency Performance of Coordination Mode</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Dataset</th>
<th>Avg. communication overhead per turn (s)</th>
<th>Avg. user-perceived turn latency per session (s)</th>
<th>Avg. communications per session</th>
<th>Avg. output tokens per communication</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervisor: Sonnet 3.5 (20241022) Specialists: Sonnet 3.5 (20241022)</td>
<td>Travel</td>
<td>13.75</td>
<td>31.46</td>
<td>8.63</td>
<td>225.88</td>
</tr>
<tr>
<td></td>
<td>Mortgage</td>
<td>13.39</td>
<td>24.42</td>
<td>7.27</td>
<td>201.66</td>
</tr>
<tr>
<td></td>
<td>Software</td>
<td>35.44</td>
<td>168.73</td>
<td>7.59</td>
<td>373.77</td>
</tr>
<tr>
<td>Supervisor: Sonnet 3.5 (20241022) Specialists: Sonnet 3.0</td>
<td>Travel</td>
<td>15.43</td>
<td>42.12</td>
<td>7.51</td>
<td>276.18</td>
</tr>
<tr>
<td></td>
<td>Mortgage</td>
<td>15.97</td>
<td>29.90</td>
<td>6.69</td>
<td>286.21</td>
</tr>
<tr>
<td></td>
<td>Software</td>
<td>53.48</td>
<td>137.31</td>
<td>9.11</td>
<td>490.78</td>
</tr>
<tr>
<td>Supervisor: Sonnet 3.5 (20241022) Specialists: Haiku 3.5</td>
<td>Travel</td>
<td>12.95</td>
<td>23.98</td>
<td>9.73</td>
<td>236.27</td>
</tr>
<tr>
<td></td>
<td>Mortgage</td>
<td>11.03</td>
<td>18.13</td>
<td>6.93</td>
<td>202.87</td>
</tr>
<tr>
<td></td>
<td>Software</td>
<td>36.65</td>
<td>125.31</td>
<td>8.07</td>
<td>388.79</td>
</tr>
<tr>
<td>Single-agent: Sonnet 3.5 (20241022)</td>
<td>Travel</td>
<td>-</td>
<td>14.12</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Mortgage</td>
<td>-</td>
<td>9.12</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Software</td>
<td>-</td>
<td>52.61</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<h1>D Human Evaluation Results</h1>
<p>Table 17: The automatic and human evaluation results. Note that these set of experiments were from an intermediate milestone checkpoint of MAC.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Overall GSR</th>
<th style="text-align: center;">Supervisor GSR</th>
<th style="text-align: center;">User-side GSR</th>
<th style="text-align: center;">System-side GSR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Automatic Evaluation</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">Human Evaluation</td>
<td style="text-align: center;">Travel</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mortgage</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Software</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.73</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/aws-samples/multiagent-collab-scenario-benchmark&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
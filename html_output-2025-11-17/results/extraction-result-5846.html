<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5846 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5846</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5846</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-59641c10ed7431a3cf841f308367dc2dc0281b74</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/59641c10ed7431a3cf841f308367dc2dc0281b74" target="_blank">What Makes Good In-Context Examples for GPT-3?</a></p>
                <p><strong>Paper Venue:</strong> Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out</p>
                <p><strong>Paper TL;DR:</strong> This work proposes to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt, and evaluates the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline.</p>
                <p><strong>Paper Abstract:</strong> GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3’s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3’s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5846.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5846.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Randomly sampled in-context examples (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The standard baseline format where a few task-relevant examples are randomly sampled from the training set and concatenated into the prompt for GPT-3; used in the original GPT-3 paper and as the main baseline in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks (Sentiment analysis / Table-to-text / QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot in-context learning where k example (input, target) pairs are concatenated as context and GPT-3 generates the target for a test input. Evaluated on IMDB sentiment (examples from SST-2), ToTTo table-to-text, and open-domain QA (NQ, WQ, TriviaQA).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt formed by concatenating k randomly sampled training (x_i, y_i) pairs using the task-specific prompt templates; examples separated by a special token/word (paper inserts 'in' between adjacent examples). Temperature set to 0 for deterministic generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared primarily against retrieval-based in-context example selection (KATE) and a kNN direct-prediction baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>IMDB accuracy: 87.95 ± 2.74; ToTTo BLEU: 28.4 ± 2.1, PARENT: 39.3 ± 2.6; QA EM (NQ): 28.6 ± 0.3, (WQ): 41.0 ± 0.5, (TriviaQA): 59.2 ± 0.4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to KATE_roberta: IMDB 91.99; ToTTo BLEU 41.0 / PARENT 50.6; QA NQ 40.0 / WQ 47.7 / TriviaQA 57.5. Compared to kNN_roberta (direct label): IMDB 50.20; ToTTo: BLEU 14.1 / PARENT 12.6; QA NQ 24.0 / WQ 23.9 / TriviaQA 26.2.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>IMDB: KATE_roberta +4.04 accuracy over Random; ToTTo: KATE_roberta +12.6 BLEU, +11.3 PARENT over Random; QA NQ: KATE_roberta +11.4 EM over Random; WQ: +6.7 EM; TriviaQA: -1.7 EM (KATE_roberta slightly lower than Random on TriviaQA in one setting, but other KATE variants improve).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (retrieval-based formats generally improved performance over random; random sampling can lead to lower performance and higher variance)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Randomly chosen examples give inconsistent, variable contextual signals; irrelevant or distantly related examples can induce hallucination or incorrect priors. The paper hypothesizes that more semantically relevant in-context examples provide better templates and detailed cues that GPT-3 can copy/adapt, hence improving outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Random baseline sometimes performs comparably (e.g., TriviaQA where some KATE variants initially underperform KATE_roberta), and random selection shows high variance (Table 1 SST-2 trials show accuracy ranging 86.9–95.8), indicating that not all tasks or samplings produce large differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5846.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5846.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KATE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KNN-Augmented In-Context Example Selection (KATE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based prompt construction method that selects the k nearest training examples (by sentence-embedding similarity) to a test input and concatenates them as the few-shot context for GPT-3, improving in-context learning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks (Sentiment analysis / Table-to-text / QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same few-shot in-context learning setup but with in-context examples selected by nearest-neighbor retrieval in a sentence-embedding space (various encoders tested: RoBERTa-large, RoBERTa fine-tuned on NLI and STS-B, RoBERTa fine-tuned on SST-2 for sentiment).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt where k examples are the top-k nearest neighbors to the test source x in an embedding space; examples are ordered (by similarity) and concatenated as (x1,y1,...,xk,yk) with the same prompt templates used across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared vs random sampling, kNN direct-prediction baseline, and different retrieval encoders (RoBERTa, NLI-finetuned, NLI+STS-B, SST-2-finetuned). Also compared performance as k varies and as training-set retrieval pool size varies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>IMDB accuracy (examples from SST-2): KATE_roberta 91.99; KATE_sst-2 93.43 (best); ToTTo dev: KATE_roberta BLEU 41.0 / PARENT 50.6 (comparable to fine-tuned T5 BLEU 41.2 / PARENT 53.0); QA: NQ KATE_roberta 40.0 EM, KATE_nli 40.8, KATE_nli+sts-b 41.6; WQ KATE_roberta 47.7, KATE_nli 50.6, KATE_nli+sts-b 50.2; TriviaQA KATE_roberta 57.5, KATE_nli 60.9, KATE_nli+sts-b 62.4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Versus Random: IMDB +4.04 (KATE_roberta), +5.48 (KATE_sst-2); ToTTo BLEU +12.6, PARENT +11.3 (KATE_roberta over Random); QA NQ +11.4 EM (KATE_roberta vs Random), WQ +6.7 EM. Versus fine-tuned T5: KATE comparable on ToTTo (BLEU 41.0 vs 41.2) and outperforms T5 on some QA benchmarks (e.g., KATE_nli+sts-b NQ 41.6 > T5 closed-book 34.5 as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large improvements: +~11 EM on NQ, +~12.6 BLEU on ToTTo, +~4–5 percentage points accuracy on IMDB compared to random sampling. Gains depend on encoder and task (fine-tuned encoders yield additional gains for task-aligned retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Semantically similar in-context examples provide better templates and detailed factual cues that GPT-3 can follow or copy (reducing hallucination); retrieval supplies informative, task-aligned context that complements GPT-3's generative priors, enabling substantial performance gains without model fine-tuning. Fine-tuning the sentence encoder on task-related data improves the relevance of retrieved examples and therefore performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Fine-tuning the retrieval encoder on dissimilar tasks can hurt performance (e.g., KATE_nli and KATE_nli+sts-b slightly underperform KATE_roberta on IMDB sentiment; conversely, NLI/STS-B fine-tuning helps on QA). Also KATE's performance can vary with the choice of encoder and dataset; ordering of examples had only small, data-dependent effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5846.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5846.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN (direct prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-Nearest Neighbor direct-prediction baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-parametric baseline that retrieves nearest neighbors by the same embedding space and predicts the test label by majority voting of the neighbors' targets (or uses top-1 target directly), used to test whether retrieval alone (without GPT-3) explains gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>kNN classifier (as baseline) / GPT-3 for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment analysis / Table-to-text / QA (classification/regression where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For classification (sentiment, QA label voting) the kNN baseline outputs nearest neighbor labels via majority vote; for generation tasks it is evaluated by using the top neighbor's target as a predicted output.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>No prompt to GPT-3 in the kNN direct prediction baseline — prediction is determined solely by retrieved training labels (top-k majority vote or top-1). When compared fairly, the same embedding space (RoBERTa-large) is used for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Directly compared vs KATE (retrieval used to form GPT-3 prompt) and random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>IMDB accuracy kNN_roberta: 50.20; ToTTo BLEU 14.1 / PARENT 12.6; QA NQ 24.0 EM / WQ 23.9 EM / TriviaQA 26.2 EM (poor relative to GPT-3-based approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Much worse than KATE and worse than Random prompts. E.g., IMDB KATE_roberta 91.99 vs kNN_roberta 50.20; ToTTo KATE_roberta BLEU 41.0 vs kNN_roberta BLEU 14.1; QA NQ KATE_roberta 40.0 vs kNN_roberta 24.0.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large negative effect when using retrieval only as direct predictions instead of as in-context prompt: e.g., IMDB about -41.79 absolute accuracy vs KATE_roberta; ToTTo BLEU -26.9 vs KATE_roberta.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (kNN direct prediction performs much worse than using retrieved examples as in-context prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Retrieval alone (majority-vote labels) does not capture the generative adaptation GPT-3 can perform when given examples; the combination of retrieved examples with GPT-3's generative ability is critical — retrieved labels by themselves are insufficient and often degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5846.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5846.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Number of in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of in-context examples (k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of how varying the count of in-context examples k affects GPT-3 performance for both random and retrieval-based selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Questions (representative), also general across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain QA (Exact Match metric) evaluated with various numbers of in-context examples (k = 5, 10, 20, 35, 64) retrieved from training pool.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompts with k nearest neighbors (KATE) vs k randomly sampled examples (Random); metrics observed as k varies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>KATE_nli+sts-b vs KATE_roberta vs Random sampling across k values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative trend: Both KATE and Random benefit from more examples, but KATE consistently outperforms Random at all examined ks; specific numbers not tabulated in main text, but reported that KATE outperforms Random even with as few as 5 examples, and EM improves as k increases up to 64.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>KATE > Random across all k; performance improves with larger k for both methods, but effect size remains with KATE maintaining superiority.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not given as single scalar; paper reports consistent gains at all tested k and that increasing k from small (5) up to 64 increases EM scores (e.g., KATE gains are large even at k=5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (more examples generally improve performance; retrieval yields larger gains at each k)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More in-context examples provide richer task exemplars and more informational cues; retrieval ensures those additional examples are relevant, hence more helpful. Using fewer but relevant examples (KATE) can be almost as effective as larger numbers of random examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Token-length constraints limit maximum usable k (GPT-3 token limit 2048); beyond that, more examples cannot be added. No explicit null effect reported but improvements plateau depending on task and token capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5846.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5846.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Order of in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ordering of in-context examples in the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of whether the ordering (most-similar-first, reverse, or random permutations) of retrieved in-context examples affects GPT-3 performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Questions (NQ) primarily; also WQ and TriviaQA tested</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain QA evaluated for effect of example ordering using KATE_nli+sts-b with multiple permutations and reverse order.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Examples ordered by descending similarity to the test (default), reverse order (least-similar first), and several random permutations; examples concatenated in that order into the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Default (most similar first) vs reverse vs random permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>NQ EM: trials 42.0, 42.5, 42.0 for three random permutations; default 41.6; reverse 42.8 (reverse performed best on this NQ subset). Variation small: differences within ~1.2 EM points.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Order effects are small relative to difference between Random and KATE; on WQ and TriviaQA the default order slightly outperformed reverse, indicating data-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Small (order changes produced differences on the order of ~0.0–1.2 EM in the reported NQ experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (data-dependent; sometimes reverse slightly better, other times default better; overall small effect)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Order provides a minor inductive bias; GPT-3 is somewhat sensitive to example sequence but the dominant factor is example relevance. The small magnitude suggests ordering is less critical than which examples are chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Variation among orders is small; order does not significantly impact performance compared with choice of examples (relevance) or number of examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5846.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5846.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentence-encoder fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of fine-tuning sentence embeddings used for retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using sentence encoders fine-tuned on task-related datasets (e.g., NLI, STS-B, SST-2) alters retrieval quality and thus changes KATE's downstream performance when those retrieved examples are used as in-context prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (with retrieval using RoBERTa variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment analysis (IMDB using SST-2 examples), Table-to-text (ToTTo), QA (NQ/WQ/TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieval uses embeddings from various encoders: pre-trained RoBERTa-large, RoBERTa fine-tuned on SNLI+MultiNLI (nli), RoBERTa fine-tuned further on STS-B (nli+sts-b), and RoBERTa fine-tuned on SST-2 (sst-2).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same KATE retrieval pipeline but using different sentence encoders to compute nearest neighbors for in-context example selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>KATE_roberta vs KATE_nli vs KATE_nli+sts-b vs KATE_sst-2 (encoder variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>IMDB: KATE_roberta 91.99, KATE_nli 90.40, KATE_nli+sts-b 90.20, KATE_sst-2 93.43 (best). ToTTo: slight drop from KATE_roberta to KATE_nli and KATE_nli+sts-b (RoBERTa best for ToTTo). QA: KATE_nli and KATE_nli+sts-b outperform KATE_roberta (e.g., NQ: 40.8 and 41.6 vs 40.0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Fine-tuning the encoder on a task-similar dataset improves retrieval usefulness (e.g., SST-2 fine-tuned encoder helped IMDB sentiment; NLI/STS-B fine-tuned encoders helped QA retrieval). Fine-tuning on dissimilar tasks can hurt (e.g., NLI+STS-B hurt IMDB).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Task-dependent: IMDB +~1.44 accuracy (KATE_sst-2 93.43 vs KATE_roberta 91.99); QA NQ +1.6 EM (KATE_nli+sts-b 41.6 vs KATE_roberta 40.0); ToTTo saw small decreases when using NLI/STS-B fine-tuned encoders versus RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved or reduced depending on alignment between encoder fine-tuning objective and downstream task (task-aligned fine-tuning improves retrieval effectiveness)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Encoders fine-tuned on datasets with objectives similar to the downstream task produce embeddings that retrieve more semantically relevant examples, thereby improving the quality of the in-context prompt; conversely, fine-tuning on dissimilar tasks can misalign similarity signals and hurt retrieval usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Fine-tuning on dissimilar tasks (e.g., NLI+STS-B for IMDB sentiment) reduced performance compared to the generic RoBERTa retrieval encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5846.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5846.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt template & formatting (delimiter 'in')</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt concatenation format and template engineering (including special 'in' delimiter and token-limit considerations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The concrete prompt templates and formatting choices (how examples are concatenated, inclusion of a delimiter token 'in' between examples, task-specific phrasing, and token-length pre-processing) affect generation and feasibility due to token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All evaluated tasks (SST-2/IMDB, ToTTo, QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt templates specify example presentation (e.g., 'Sentence: ... Label: Positive' for sentiment; 'Table: ... Sentence:' for table-to-text; 'Q: ... A:' for QA). Templates and delimiters define context structure passed to GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Examples concatenated into a single prompt string with a special character/word 'in' inserted between adjacent examples (as described and illustrated); task-specific templates provided in Appendix Table 11; for ToTTo some html-like tags were removed to meet the 2048 token limit.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as a single metric but formatting choices are necessary to produce the reported numbers; token-limit-driven preprocessing (deleting closing angle brackets like </cell>) allowed inclusion of more context and 2 retrieved examples for ToTTo.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>affects feasibility and content fidelity (formatting enables or constrains which examples and how much information can be included)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Well-designed prompt templates and consistent delimiters give GPT-3 clearer structure to follow; insufficient or malformed formatting can cause the model to hallucinate or omit required details. Token limits force compromises (e.g., deleting tags) that influence how many examples fit in the prompt and therefore performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>No direct ablation comparing different delimiter tokens reported; however, hallucination examples (Table 5/10) show that random prompts with the same template still produce incorrect details, indicating template alone does not guarantee correctness — relevance of examples matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Examples for GPT-3?', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Sentence-BERT: Sentence embeddings using siamese bert-networks <em>(Rating: 2)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 1)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 1)</em></li>
                <li>Making pre-trained language models better few-shot learners (LM-BFF) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5846",
    "paper_id": "paper-59641c10ed7431a3cf841f308367dc2dc0281b74",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Random in-context examples",
            "name_full": "Randomly sampled in-context examples (baseline)",
            "brief_description": "The standard baseline format where a few task-relevant examples are randomly sampled from the training set and concatenated into the prompt for GPT-3; used in the original GPT-3 paper and as the main baseline in this study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Multiple tasks (Sentiment analysis / Table-to-text / QA)",
            "task_description": "Few-shot in-context learning where k example (input, target) pairs are concatenated as context and GPT-3 generates the target for a test input. Evaluated on IMDB sentiment (examples from SST-2), ToTTo table-to-text, and open-domain QA (NQ, WQ, TriviaQA).",
            "problem_format": "Few-shot prompt formed by concatenating k randomly sampled training (x_i, y_i) pairs using the task-specific prompt templates; examples separated by a special token/word (paper inserts 'in' between adjacent examples). Temperature set to 0 for deterministic generation.",
            "comparison_format": "Compared primarily against retrieval-based in-context example selection (KATE) and a kNN direct-prediction baseline.",
            "performance": "IMDB accuracy: 87.95 ± 2.74; ToTTo BLEU: 28.4 ± 2.1, PARENT: 39.3 ± 2.6; QA EM (NQ): 28.6 ± 0.3, (WQ): 41.0 ± 0.5, (TriviaQA): 59.2 ± 0.4.",
            "performance_comparison": "Compared to KATE_roberta: IMDB 91.99; ToTTo BLEU 41.0 / PARENT 50.6; QA NQ 40.0 / WQ 47.7 / TriviaQA 57.5. Compared to kNN_roberta (direct label): IMDB 50.20; ToTTo: BLEU 14.1 / PARENT 12.6; QA NQ 24.0 / WQ 23.9 / TriviaQA 26.2.",
            "format_effect_size": "IMDB: KATE_roberta +4.04 accuracy over Random; ToTTo: KATE_roberta +12.6 BLEU, +11.3 PARENT over Random; QA NQ: KATE_roberta +11.4 EM over Random; WQ: +6.7 EM; TriviaQA: -1.7 EM (KATE_roberta slightly lower than Random on TriviaQA in one setting, but other KATE variants improve).",
            "format_effect_direction": "varied (retrieval-based formats generally improved performance over random; random sampling can lead to lower performance and higher variance)",
            "explanation_or_hypothesis": "Randomly chosen examples give inconsistent, variable contextual signals; irrelevant or distantly related examples can induce hallucination or incorrect priors. The paper hypothesizes that more semantically relevant in-context examples provide better templates and detailed cues that GPT-3 can copy/adapt, hence improving outputs.",
            "counterexample_or_null_result": "Random baseline sometimes performs comparably (e.g., TriviaQA where some KATE variants initially underperform KATE_roberta), and random selection shows high variance (Table 1 SST-2 trials show accuracy ranging 86.9–95.8), indicating that not all tasks or samplings produce large differences.",
            "uuid": "e5846.0",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "KATE",
            "name_full": "KNN-Augmented In-Context Example Selection (KATE)",
            "brief_description": "A retrieval-based prompt construction method that selects the k nearest training examples (by sentence-embedding similarity) to a test input and concatenates them as the few-shot context for GPT-3, improving in-context learning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Multiple tasks (Sentiment analysis / Table-to-text / QA)",
            "task_description": "Same few-shot in-context learning setup but with in-context examples selected by nearest-neighbor retrieval in a sentence-embedding space (various encoders tested: RoBERTa-large, RoBERTa fine-tuned on NLI and STS-B, RoBERTa fine-tuned on SST-2 for sentiment).",
            "problem_format": "Few-shot prompt where k examples are the top-k nearest neighbors to the test source x in an embedding space; examples are ordered (by similarity) and concatenated as (x1,y1,...,xk,yk) with the same prompt templates used across experiments.",
            "comparison_format": "Compared vs random sampling, kNN direct-prediction baseline, and different retrieval encoders (RoBERTa, NLI-finetuned, NLI+STS-B, SST-2-finetuned). Also compared performance as k varies and as training-set retrieval pool size varies.",
            "performance": "IMDB accuracy (examples from SST-2): KATE_roberta 91.99; KATE_sst-2 93.43 (best); ToTTo dev: KATE_roberta BLEU 41.0 / PARENT 50.6 (comparable to fine-tuned T5 BLEU 41.2 / PARENT 53.0); QA: NQ KATE_roberta 40.0 EM, KATE_nli 40.8, KATE_nli+sts-b 41.6; WQ KATE_roberta 47.7, KATE_nli 50.6, KATE_nli+sts-b 50.2; TriviaQA KATE_roberta 57.5, KATE_nli 60.9, KATE_nli+sts-b 62.4.",
            "performance_comparison": "Versus Random: IMDB +4.04 (KATE_roberta), +5.48 (KATE_sst-2); ToTTo BLEU +12.6, PARENT +11.3 (KATE_roberta over Random); QA NQ +11.4 EM (KATE_roberta vs Random), WQ +6.7 EM. Versus fine-tuned T5: KATE comparable on ToTTo (BLEU 41.0 vs 41.2) and outperforms T5 on some QA benchmarks (e.g., KATE_nli+sts-b NQ 41.6 &gt; T5 closed-book 34.5 as reported).",
            "format_effect_size": "Large improvements: +~11 EM on NQ, +~12.6 BLEU on ToTTo, +~4–5 percentage points accuracy on IMDB compared to random sampling. Gains depend on encoder and task (fine-tuned encoders yield additional gains for task-aligned retrieval).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Semantically similar in-context examples provide better templates and detailed factual cues that GPT-3 can follow or copy (reducing hallucination); retrieval supplies informative, task-aligned context that complements GPT-3's generative priors, enabling substantial performance gains without model fine-tuning. Fine-tuning the sentence encoder on task-related data improves the relevance of retrieved examples and therefore performance.",
            "counterexample_or_null_result": "Fine-tuning the retrieval encoder on dissimilar tasks can hurt performance (e.g., KATE_nli and KATE_nli+sts-b slightly underperform KATE_roberta on IMDB sentiment; conversely, NLI/STS-B fine-tuning helps on QA). Also KATE's performance can vary with the choice of encoder and dataset; ordering of examples had only small, data-dependent effects.",
            "uuid": "e5846.1",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "kNN (direct prediction)",
            "name_full": "k-Nearest Neighbor direct-prediction baseline",
            "brief_description": "A non-parametric baseline that retrieves nearest neighbors by the same embedding space and predicts the test label by majority voting of the neighbors' targets (or uses top-1 target directly), used to test whether retrieval alone (without GPT-3) explains gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "kNN classifier (as baseline) / GPT-3 for comparison",
            "model_size": null,
            "task_name": "Sentiment analysis / Table-to-text / QA (classification/regression where applicable)",
            "task_description": "For classification (sentiment, QA label voting) the kNN baseline outputs nearest neighbor labels via majority vote; for generation tasks it is evaluated by using the top neighbor's target as a predicted output.",
            "problem_format": "No prompt to GPT-3 in the kNN direct prediction baseline — prediction is determined solely by retrieved training labels (top-k majority vote or top-1). When compared fairly, the same embedding space (RoBERTa-large) is used for retrieval.",
            "comparison_format": "Directly compared vs KATE (retrieval used to form GPT-3 prompt) and random sampling.",
            "performance": "IMDB accuracy kNN_roberta: 50.20; ToTTo BLEU 14.1 / PARENT 12.6; QA NQ 24.0 EM / WQ 23.9 EM / TriviaQA 26.2 EM (poor relative to GPT-3-based approaches).",
            "performance_comparison": "Much worse than KATE and worse than Random prompts. E.g., IMDB KATE_roberta 91.99 vs kNN_roberta 50.20; ToTTo KATE_roberta BLEU 41.0 vs kNN_roberta BLEU 14.1; QA NQ KATE_roberta 40.0 vs kNN_roberta 24.0.",
            "format_effect_size": "Large negative effect when using retrieval only as direct predictions instead of as in-context prompt: e.g., IMDB about -41.79 absolute accuracy vs KATE_roberta; ToTTo BLEU -26.9 vs KATE_roberta.",
            "format_effect_direction": "reduced (kNN direct prediction performs much worse than using retrieved examples as in-context prompts)",
            "explanation_or_hypothesis": "Retrieval alone (majority-vote labels) does not capture the generative adaptation GPT-3 can perform when given examples; the combination of retrieved examples with GPT-3's generative ability is critical — retrieved labels by themselves are insufficient and often degrade performance.",
            "counterexample_or_null_result": null,
            "uuid": "e5846.2",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Number of in-context examples",
            "name_full": "Effect of number of in-context examples (k)",
            "brief_description": "Analysis of how varying the count of in-context examples k affects GPT-3 performance for both random and retrieval-based selection strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Natural Questions (representative), also general across tasks",
            "task_description": "Open-domain QA (Exact Match metric) evaluated with various numbers of in-context examples (k = 5, 10, 20, 35, 64) retrieved from training pool.",
            "problem_format": "Few-shot prompts with k nearest neighbors (KATE) vs k randomly sampled examples (Random); metrics observed as k varies.",
            "comparison_format": "KATE_nli+sts-b vs KATE_roberta vs Random sampling across k values.",
            "performance": "Qualitative trend: Both KATE and Random benefit from more examples, but KATE consistently outperforms Random at all examined ks; specific numbers not tabulated in main text, but reported that KATE outperforms Random even with as few as 5 examples, and EM improves as k increases up to 64.",
            "performance_comparison": "KATE &gt; Random across all k; performance improves with larger k for both methods, but effect size remains with KATE maintaining superiority.",
            "format_effect_size": "Not given as single scalar; paper reports consistent gains at all tested k and that increasing k from small (5) up to 64 increases EM scores (e.g., KATE gains are large even at k=5).",
            "format_effect_direction": "improved (more examples generally improve performance; retrieval yields larger gains at each k)",
            "explanation_or_hypothesis": "More in-context examples provide richer task exemplars and more informational cues; retrieval ensures those additional examples are relevant, hence more helpful. Using fewer but relevant examples (KATE) can be almost as effective as larger numbers of random examples.",
            "counterexample_or_null_result": "Token-length constraints limit maximum usable k (GPT-3 token limit 2048); beyond that, more examples cannot be added. No explicit null effect reported but improvements plateau depending on task and token capacity.",
            "uuid": "e5846.3",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Order of in-context examples",
            "name_full": "Ordering of in-context examples in the prompt",
            "brief_description": "Investigation of whether the ordering (most-similar-first, reverse, or random permutations) of retrieved in-context examples affects GPT-3 performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "Natural Questions (NQ) primarily; also WQ and TriviaQA tested",
            "task_description": "Open-domain QA evaluated for effect of example ordering using KATE_nli+sts-b with multiple permutations and reverse order.",
            "problem_format": "Examples ordered by descending similarity to the test (default), reverse order (least-similar first), and several random permutations; examples concatenated in that order into the prompt.",
            "comparison_format": "Default (most similar first) vs reverse vs random permutations.",
            "performance": "NQ EM: trials 42.0, 42.5, 42.0 for three random permutations; default 41.6; reverse 42.8 (reverse performed best on this NQ subset). Variation small: differences within ~1.2 EM points.",
            "performance_comparison": "Order effects are small relative to difference between Random and KATE; on WQ and TriviaQA the default order slightly outperformed reverse, indicating data-dependence.",
            "format_effect_size": "Small (order changes produced differences on the order of ~0.0–1.2 EM in the reported NQ experiments).",
            "format_effect_direction": "mixed (data-dependent; sometimes reverse slightly better, other times default better; overall small effect)",
            "explanation_or_hypothesis": "Order provides a minor inductive bias; GPT-3 is somewhat sensitive to example sequence but the dominant factor is example relevance. The small magnitude suggests ordering is less critical than which examples are chosen.",
            "counterexample_or_null_result": "Variation among orders is small; order does not significantly impact performance compared with choice of examples (relevance) or number of examples.",
            "uuid": "e5846.4",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Sentence-encoder fine-tuning",
            "name_full": "Effect of fine-tuning sentence embeddings used for retrieval",
            "brief_description": "Using sentence encoders fine-tuned on task-related datasets (e.g., NLI, STS-B, SST-2) alters retrieval quality and thus changes KATE's downstream performance when those retrieved examples are used as in-context prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (with retrieval using RoBERTa variants)",
            "model_size": "175B",
            "task_name": "Sentiment analysis (IMDB using SST-2 examples), Table-to-text (ToTTo), QA (NQ/WQ/TriviaQA)",
            "task_description": "Retrieval uses embeddings from various encoders: pre-trained RoBERTa-large, RoBERTa fine-tuned on SNLI+MultiNLI (nli), RoBERTa fine-tuned further on STS-B (nli+sts-b), and RoBERTa fine-tuned on SST-2 (sst-2).",
            "problem_format": "Same KATE retrieval pipeline but using different sentence encoders to compute nearest neighbors for in-context example selection.",
            "comparison_format": "KATE_roberta vs KATE_nli vs KATE_nli+sts-b vs KATE_sst-2 (encoder variants).",
            "performance": "IMDB: KATE_roberta 91.99, KATE_nli 90.40, KATE_nli+sts-b 90.20, KATE_sst-2 93.43 (best). ToTTo: slight drop from KATE_roberta to KATE_nli and KATE_nli+sts-b (RoBERTa best for ToTTo). QA: KATE_nli and KATE_nli+sts-b outperform KATE_roberta (e.g., NQ: 40.8 and 41.6 vs 40.0).",
            "performance_comparison": "Fine-tuning the encoder on a task-similar dataset improves retrieval usefulness (e.g., SST-2 fine-tuned encoder helped IMDB sentiment; NLI/STS-B fine-tuned encoders helped QA retrieval). Fine-tuning on dissimilar tasks can hurt (e.g., NLI+STS-B hurt IMDB).",
            "format_effect_size": "Task-dependent: IMDB +~1.44 accuracy (KATE_sst-2 93.43 vs KATE_roberta 91.99); QA NQ +1.6 EM (KATE_nli+sts-b 41.6 vs KATE_roberta 40.0); ToTTo saw small decreases when using NLI/STS-B fine-tuned encoders versus RoBERTa.",
            "format_effect_direction": "improved or reduced depending on alignment between encoder fine-tuning objective and downstream task (task-aligned fine-tuning improves retrieval effectiveness)",
            "explanation_or_hypothesis": "Encoders fine-tuned on datasets with objectives similar to the downstream task produce embeddings that retrieve more semantically relevant examples, thereby improving the quality of the in-context prompt; conversely, fine-tuning on dissimilar tasks can misalign similarity signals and hurt retrieval usefulness.",
            "counterexample_or_null_result": "Fine-tuning on dissimilar tasks (e.g., NLI+STS-B for IMDB sentiment) reduced performance compared to the generic RoBERTa retrieval encoder.",
            "uuid": "e5846.5",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Prompt template & formatting (delimiter 'in')",
            "name_full": "Prompt concatenation format and template engineering (including special 'in' delimiter and token-limit considerations)",
            "brief_description": "The concrete prompt templates and formatting choices (how examples are concatenated, inclusion of a delimiter token 'in' between examples, task-specific phrasing, and token-length pre-processing) affect generation and feasibility due to token limits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "All evaluated tasks (SST-2/IMDB, ToTTo, QA)",
            "task_description": "Prompt templates specify example presentation (e.g., 'Sentence: ... Label: Positive' for sentiment; 'Table: ... Sentence:' for table-to-text; 'Q: ... A:' for QA). Templates and delimiters define context structure passed to GPT-3.",
            "problem_format": "Examples concatenated into a single prompt string with a special character/word 'in' inserted between adjacent examples (as described and illustrated); task-specific templates provided in Appendix Table 11; for ToTTo some html-like tags were removed to meet the 2048 token limit.",
            "comparison_format": null,
            "performance": "Not reported as a single metric but formatting choices are necessary to produce the reported numbers; token-limit-driven preprocessing (deleting closing angle brackets like &lt;/cell&gt;) allowed inclusion of more context and 2 retrieved examples for ToTTo.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "affects feasibility and content fidelity (formatting enables or constrains which examples and how much information can be included)",
            "explanation_or_hypothesis": "Well-designed prompt templates and consistent delimiters give GPT-3 clearer structure to follow; insufficient or malformed formatting can cause the model to hallucinate or omit required details. Token limits force compromises (e.g., deleting tags) that influence how many examples fit in the prompt and therefore performance.",
            "counterexample_or_null_result": "No direct ablation comparing different delimiter tokens reported; however, hallucination examples (Table 5/10) show that random prompts with the same template still produce incorrect details, indicating template alone does not guarantee correctness — relevance of examples matters.",
            "uuid": "e5846.6",
            "source_info": {
                "paper_title": "What Makes Good In-Context Examples for GPT-3?",
                "publication_date_yy_mm": "2021-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Sentence-BERT: Sentence embeddings using siamese bert-networks",
            "rating": 2
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 1
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 1
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners (LM-BFF)",
            "rating": 1
        }
    ],
    "cost": 0.01721475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What Makes Good In-Context Examples for GPT-3?</h1>
<p>Jiachang Liu ${ }^{1}$, Dinghan Shen ${ }^{2}$, Yizhe Zhang ${ }^{3}$, Bill Dolan ${ }^{4}$, Lawrence Carin ${ }^{1}$, Weizhu Chen ${ }^{2}$<br>${ }^{1}$ Duke University ${ }^{2}$ Microsoft Dynamics $365 \mathrm{AI} \quad{ }^{3}$ Meta AI ${ }^{4}$ Microsoft Research<br>${ }^{1}$ {jiachang.liu, lcarin}@duke.edu<br>${ }^{3}$ yizhe.zhang@hotmail.com<br>${ }^{2,4}$ {dishen, billdol, wzchen}@microsoft.com</p>
<h4>Abstract</h4>
<p>GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting incontext examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities. Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders finetuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-totext generation ( $44.3 \%$ on the ToTTo dataset) and open-domain question answering ( $45.5 \%$ on the NQ dataset).</p>
<h2>1 Introduction</h2>
<p>GPT-3 (Brown et al., 2020) is a new breakthrough in NLP research. Previously, NLP models are firstly pre-trained and then fine-tuned on a specific task. What sets GPT-3 apart from other models is its impressive "in-context" learning ability. Provided with a few in-context examples, GPT-3 can generalize to unseen cases without further finetuning. This opens up many new technological possibilities that are previously considered unique</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>${ }^{4}$ Work was done when Jiachang (intern) and Yizhe were at Microsoft.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Trial</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">86.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of GPT-3 on the SST-2 sentiment analysis dataset. Five different examples are randomly selected from the training set for each trial. Different contexts induce different accuracies on the test set.
to human. Future NLP systems can be developed to expand emails, extract entities from text, generate code based on natural language instructions with a few demonstration examples.</p>
<p>Despite its powerful and versatile in-context learning ability, GPT-3 has some practical challenges. The original paper utilizes task-relevant examples that are randomly sampled from the training set. However, we observe that the performance of GPT-3 tends to fluctuate with different choices of in-context examples. As shown in Table 1, the variance with distinct in-context examples can be significant. Our work aims to carefully examine this issue to gain a deeper understanding on how to better select in-context examples to improve GPT3 's performance without fine-tuning. Note that our approach requires a training set to select examples. With such a training dataset, it is possible to fine-tune GPT-3 to take full advantage of the model's strength. However, currently GPT-3 has not been released to public for fine-tuning. Even if it is available, fine-tuning GPT-3 requires hundreds of GPUs to load the 175B model, which is prohibitively expensive and time-consuming for ordinary research labs. Another issue is that storing large fine-tuned model checkpoints require huge storage space. Consequently, we resort to prompt/example engineering strategy. Nevertheless, the fine-tuning results using T5 are provided for reference.</p>
<p>A brute-force approach for selecting the optimal in-context instances would be to perform combinatorial search over the entire dataset. Unfortunately, this strategy is computationally impractical. To this</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: In-context example selection for GPT-3. White dots: unused training samples; grey dots: randomly sampled training samples; red dots: training samples selected by the k-nearest neighbors algorithm in the embedding space of a sentence encoder.
end, we empirically investigate the influences of employing different in-context examples. Interestingly, we find that the in-context examples that are closer to the test sample in the embedding space consistently give rise to stronger performance (relative to the farther ones). Inspired by this observation and the recent success of retrieval-augmented models Hashimoto et al., 2018), we propose to utilize nearest neighbors of a given test sample (among all the training instances available) as the in-context examples.</p>
<p>To verify the effectiveness of the proposed method, we evaluate it on several natural language understanding and generation tasks, including sentiment analysis, table-to-text generation and opendomain question answering. It is observed that the retrieval-based in-context examples unleash the in-context learning capabilities of GPT-3 much more effectively than the random sampling baseline, even when the number of examples is small. Moreover, we find that the specific sentence encoders employed for the retrieval procedure play a critical role. Thus, an extensive exploration is conducted and shows that encoders fine-tuned on natural language matching tasks serve as more effective in-context examples selector on the QA task. In summary, our contributions are as follows:
i) to the best of our knowledge, we take a first step towards understanding the sensitivity of GPT3's in-context learning ability with respect to the choice of in-context examples;
ii) to alleviate the sensitivity issue, an additional retrieval module is introduced to find semanticallysimilar in-context examples of a test instance, which greatly outperforms the baseline based on
randomly sampled in-context examples;
iii) empirically, the better selected examples lead GPT-3 to achieve comparable performance to a fine-tuned T5 model on the table-to-text task and outperforms the T5 model on the QA tasks;
iv) fine-tuning the retrieval model on task-related dataset(s) leads to stronger empirical results;
v) the performance of GPT-3 improves as the number of examples for retrieval increases.</p>
<h2>2 Method</h2>
<h3>2.1 GPT-3 for In-Context Learning</h3>
<p>The in-context learning scenario of GPT-3 can be regarded as a conditional text generation problem. Concretely, the probability of generating a target $y$ is conditioned on the context $C$, which includes $k$ examples, and the source $x$. Therefore, the probability can be expressed as:</p>
<p>$$
p_{\mathrm{LM}}(y \mid C, x)=\prod_{t=1}^{T} p\left(y_{t} \mid C, x, y_{&lt;t}\right)
$$</p>
<p>where LM denotes the parameters of the language model, and $C=\left{x_{1}, y_{1}, x_{2}, y_{2}, \ldots, x_{k}, y_{k}\right}$ is a context string concatenating $k$ training instances with the special character "in". A concrete illustration can be found in the Appendix.</p>
<p>For GPT-3, this generation process is implemented through a giant transformer-based architecture Vaswani et al., 2017). Due to the computational burden of fine-tuning, GPT-3 is leveraged in an in-context learning manner as described above. Unfortunately, as shown in Table 1, the results of GPT-3 tend to fluctuate significantly with different in-context examples. We aim to alleviate this issue via judicious in-context example selection.</p>
<h3>2.2 The Impact of In-Context Examples</h3>
<p>We start the investigation by looking at the role of in-context examples from an empirical perspective. Previous retrieve-and-edit literature usually retrieve prototypes that are close to the test source $x$ in some embedding space. These examples and the test source $x$ often share semantic or lexical similarities. This hints on how we may select incontext examples for GPT-3.</p>
<p>To this end, we examine the impact of the distance between the in-context example and the test sample on GPT-3's performance. Concretely, a comparison is made on the the Natural Questions (NQ) dataset between two selection strategies. Given a test example, the first method utilizes the 10 farthest training instances as the in-context examples, while the second employs the 10 closest neighbors. We use the CLS embeddings of a pre-trained RoBERTa-large model as sentence representations to measure the proximity of two sentences (using the Euclidean distance).</p>
<p>For evaluation, 100 test questions are randomly sampled and the average Exact Match (EM) scores with the two distinct strategies are reported in Table 2. It can be observed that the nearest neighbors, used as the in-context examples, give rise to much better results relative to the farthest ones. Moreover, the pre-trained RoBERTa model serves as effective sentence embeddings for the retrieval procedure.</p>
<h3>2.3 kNN-augmented Example Selection</h3>
<p>Based on the findings above, we propose KATE $^{1}$, a strategy to select good examples for in-context learning. The process is visualized in Figure 1. Specifically, we first use a sentence encoder to convert sources in both the training set and test set to vector representations. For online prediction, we can convert the training set first and encode each test source on the fly. Then, for each test source $x$, we retrieve its nearest $k$ neighbors $x_{1}, x_{2}, \ldots, x_{k}$ from the training set (according to the distances in the embedding space). Given some pre-defined similarity measure $s$ such as the negative Euclidean distance or the cosine similarity, the neighbors are ordered so that $s\left(x_{i}, x\right) \geq s\left(x_{j}, x\right)$ when $i&lt;j$.</p>
<p>The $k$ sources are concatenated with their targets to form the context $C=$ $\left{x_{1}, y_{1}, x_{2}, y_{2}, \ldots, x_{k}, y_{k}\right}$, which is sent to GPT-3 along with the test input. The algorithm is presented in Algorithm 1. Note that different</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Comparison of the EM score on the closest 10 neighbors and farthest 10 neighbors on a subset of 100 test samples of the NQ dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">kNN</span><span class="w"> </span><span class="nt">In-context</span><span class="w"> </span><span class="nt">Example</span><span class="w"> </span><span class="nt">Selection</span>
<span class="nt">Given</span><span class="o">:</span><span class="w"> </span><span class="nt">test</span><span class="w"> </span><span class="nt">prompt</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">),</span><span class="w"> </span><span class="nt">training</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\boldsymbol{x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">i=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">N</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">sentence</span><span class="w"> </span><span class="nt">encoder</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="o">)</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">number</span>
<span class="nt">of</span><span class="w"> </span><span class="nt">in-context</span><span class="w"> </span><span class="nt">examples</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">(</span><span class="nt">hyperparameter</span><span class="o">).</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">=</span><span class="nt">-</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="nt">-</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">.</span><span class="err">\</span><span class="nt">frac</span><span class="p">{</span><span class="err">\boldsymbol{v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">}</span><span class="p">{</span><span class="err">\left\|\boldsymbol{v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">v</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">Select</span><span class="w"> </span><span class="nt">largest</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">similarities</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="err">&#39;</span><span class="nt">s</span><span class="w"> </span><span class="o">(</span><span class="nt">in</span><span class="w"> </span><span class="nt">descending</span>
<span class="w">    </span><span class="nt">order</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">indices</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="p">{</span><span class="err">\sigma(1),</span><span class="w"> </span><span class="err">\ldots,</span><span class="w"> </span><span class="err">\sigma(k)\</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">C</span><span class="o">=</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">sigma</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">sigma</span><span class="p">(</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">ldots</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">sigma</span><span class="p">(</span><span class="nx">k</span><span class="p">)}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">sigma</span><span class="p">(</span><span class="nx">k</span><span class="p">)}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">dot</span><span class="p">{</span><span class="err">\boldsymbol{y</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">GPT</span><span class="p">}</span><span class="nt">-3</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">C</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">test</span><span class="w"> </span><span class="p">}}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<p>numbers of examples can be employed, and we conduct study on its impact in a later section.</p>
<p>Choices of Retrieval Module A core step for our context selection approach is mapping sentences into a latent semantic space, leaving a question as what sentence encoders we should choose. We compared among existing pre-trained text encoders and found them sufficient to retrieve semantically similar sentences. The sentence encoders can be divided into two categories.</p>
<p>The first category includes generally pre-trained sentence encoders such as the BERT, RoBERTa, and XLNet models. These models have been trained on large quantities of unsupervised tasks and achieved good performance on many natural language tasks. The corresponding embeddings contain rich semantic information from the original sentences.</p>
<p>The second category includes sentence encoders fine-tuned on specific tasks or datasets. For example, a sentence encoder trained on the STS dataset should be able to assess similarities among different questions better than a generally pre-trained sentence encoder. Sentence-BERT (Wolf et al., 2019; Reimers and Gurevych, 2019, 2020) shows that these fine-tuned encoders have achieved great performance on tasks such as sentence clustering, paraphrase mining, and information retrieval.</p>
<h2>3 Experimental Setup</h2>
<p>We apply our proposed method to the following three tasks: sentiment analysis, table-to-text generation, and question answering. Dataset split setups and prompt templates are shown in Table 9 and 11 in the Appendix. For the hyper-parameters in the GPT-3 API, we set the temperature to 0 .</p>
<h3>3.1 Sentence Embeddings for Retrieval</h3>
<p>To retrieve semantically-similar training instances, we consider two types of sentence embeddings.</p>
<ul>
<li>The original RoBERTa-large model (Liu et al., 2019), which is abbreviated as $\mathrm{KATE}_{\text {roberta }}$;</li>
<li>The RoBERTa-large models which are: $i$ ) finetuned on the SNLI and MultiNLI datasets ( $\mathrm{KATE}<em _nli_sts-b="{nli+sts-b" _text="\text">{\text {nli }}$ ) (Bowman et al., 2015; Williams et al., 2017); ii) first fine-tuned on the SNLI and MultiNLI dataset and then on the STS-B datasets ( $\mathrm{KATE}</em>$ ) (Cer et al., 2017).}</li>
</ul>
<p>All sentence encoders share the same architecture. The only differences are the specific datasets used for fine-tuning. The negative Euclidean distance is used for $\mathrm{KATE}<em _nli="{nli" _text="\text">{\text {roberta }}$, while the cosine similarity is employed for $\mathrm{KATE}</em>$.}}$ and $\mathrm{KATE}_{\text {nli+sts-b }</p>
<p>Sentiment Analysis For this task, we conduct experiments under the dataset-transfer setting. Incontext examples are selected from one dataset, and the evaluation is made on another dataset. This setting is designed to simulate a real-world scenario where we want to leverage an existing labeled dataset for a unlabeled one (of a similar task).</p>
<p>Specifically, we select examples from the SST2 training set (Socher et al., 2013; Wang et al., 2018) and ask GPT-3 to predict on the IMDB test set (Maas et al., 2011). To explore whether a sentence encoder fine-tuned on a similar task would benefit KATE, we also employ a pre-trained RoBERTa-large model fine-tuned on the SST-2 training set (dubbed as $\mathrm{KATE}_{\text {sst-2 }}$ ). The number of examples is chosen to be 3 since adding more examples does not further improve the performance.</p>
<p>Table-to-Text Generation Given a Wikipedia table and a set of highlighted cells, this task focuses on producing human-readable texts as descriptions. ToTTo (Parikh et al., 2020) ${ }^{2}$ is utilized for evaluation due to its popularity. We use BLEU (Papineni</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2002) and PARENT (Dhingra et al., 2019) metrics for evaluation. Because the token length limit of GPT-3 is 2048, we add a preprocessing step by deleting the closing angle brackets such as $&lt;/$ cell $&gt;$ and $&lt;/$ table $&gt;$ to save space. The number of in-context examples is set as 2 so that the input length is within the token limit.</p>
<p>Question Answering We conduct experiments on three QA benchmarks: Natural Questions (NQ) (Kwiatkowski et al., 2019), Web Questions (WQ) (Berant et al., 2013), and TriviaQA (Joshi et al., 2017). For evaluation, we use the Exact Match (EM) score, which is defined as the proportion of the number of predicted answers being exactly one of the ground-truth answers. The matching is performed after string normalization, which includes article and punctuation removal. The number of examples is set to be 64 for NQ and WQ and 10 for TriviaQA (The retrieved 64 examples exceed the token limit). We evaluate on the test sets of NQ and WQ and the dev set of TriviaQA.</p>
<h3>3.2 Baseline Methods</h3>
<p>Random Sampling For each test sentence, we randomly select in-context examples from the training set. We refer to this method as Random in the experimental results. On the test set, the random baseline is repeated for five times to obtain the average score and corresponding standard deviation.
$k$-Nearest Neighbor Additionally, to investigate whether the retrieval module is complementary to GPT-3's in-context learning ability, we further consider a $k$-nearest neighbor baseline. Specifically, the target $y_{1}$ associated with the first retrieved example is considered as the predicted target for the test sample. For the sentiment analysis and QA tasks, the top $k$ retrieved examples $\left{y_{1}, \ldots, y_{k}\right}$ are utilized, where the final prediction is determined by majority voting among the $k$ examples' targets. If there is a tie case, we use the target of the example most similar to the test sentence. To ensure fair comparison, we compare the baseline $k \mathrm{NN}$ and KATE under the same embedding space of a pre-trained RoBERTa-large model. This baseline is abbreviated as $k \mathrm{NN}_{\text {roberta }}$.
Fine-tuned T5 Although this work aims at improving the in-context learning abilities of GPT-3, we include a fine-tuned T5 (3B) model as a baseline. This comparison informs us where GPT-3 performs comparably or surpasses a fine-tuned model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T5 (fine-tuned)</td>
<td style="text-align: center;">95.2</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$87.95 \pm 2.74$</td>
</tr>
<tr>
<td style="text-align: center;">$k \mathrm{NN}_{\text {roberta }}$</td>
<td style="text-align: center;">50.20</td>
</tr>
<tr>
<td style="text-align: center;">KATE roberta</td>
<td style="text-align: center;">91.99</td>
</tr>
<tr>
<td style="text-align: center;">KATE nli</td>
<td style="text-align: center;">90.40</td>
</tr>
<tr>
<td style="text-align: center;">KATE nli+sts-b</td>
<td style="text-align: center;">90.20</td>
</tr>
<tr>
<td style="text-align: center;">KATE ${ }_{\text {sst-2 }}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 4 3}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on the IMDB dataset. In-context examples are from the SST-2 dataset.</p>
<h2>4 Experimental Results</h2>
<h3>4.1 Sentiment Analysis</h3>
<p>We first evaluate KATE on the sentiment analysis task. The results are in Table 3. KATE consistently produces better performance relative to the random selection baseline. Notably, there is no variance with the obtained results since the fixed retrieved in-context examples are employed. For KATE, when the pre-trained sentence encoder is fine-tuned on NLI or NLI+STS-B datasets, the performance slightly decreases. Since the objectives of the IMDB and the NLI+STS-B datasets are different, this shows that fine-tuning on a dissimilar task hurts KATE's performance. In contrast, KATE $<em _roberta="{roberta" _text="\text">{\text {sst-2 }}$ obtains the best accuracy, showing that fine-tuning on a similar task improves KATE's performance. To verify that the gains are not merely from the retrieval step, we further compare $\mathrm{KATE}</em>}}$ with the $k \mathrm{NN<em _roberta="{roberta" _text="\text">{\text {roberta }}$. It turns out that the performance of $k \mathrm{NN}</em>}}$ is close to random guessing. This observation is consistent when one neighbor or three neighbors are retrieved. Notably, with the sentence encoder fine-tuned on the SST-2 dataset, the accuracy of $k \mathrm{NN<em _sst-2="{sst-2" _text="\text">{\text {sst-2 }}$ is 92.46 , which is lower than that of $\mathrm{KATE}</em>$. These results suggest that GPT-3 is critical to the final results, and the retrieval module is complementary to GPT-3.}</p>
<p>The fine-tuned T5 model works better since its parameters has been adapted to this specific task. However, fine-tuning requires access to model parameters, lots of memory storage, and time. The fine-tuning result here is just for reference. Through KATE, the performance of GPT-3 has increased significantly without fine-tuning.</p>
<h3>4.2 Table-to-text Generation</h3>
<p>We next evaluate KATE on the ToTTo dataset and present results in Table 4. KATE gives rise to considerable gains over the random baseline, according to both the BLEU and PARENT scores. Notably,</p>
<p>KATE enables GPT-3 to achieve performance comparable to a fine-tuned T5 model. On a finer scale, the evaluation can be done on the overlap subset and the nonoverlap subset. The overlap dev subset shares a significant number of header names with the training set, while the nonoverlap one does not. KATE improves results on both subsets, meaning that the retrieval module is helpful even when the dev set is out of distribution of the training set. Similar to sentiment analysis, there is a slight drop in performance from $\mathrm{KATE}<em _nli="{nli" _text="\text">{\text {roberta }}$ to $\mathrm{KATE}</em>}}$ and $\mathrm{KATE<em _nli="{nli" _text="\text">{\text {nli+sts-b }}$. This is due to the difference between the objectives of the ToTTo dataset and NLI+STSB datasets. The drop from $\mathrm{KATE}</em>$ baseline, it performs much worse than the random selection method and KATE, suggesting that the retrieval process and GPT-3 work collaboratively to achieve better results.}}$ to $\mathrm{KATE}_{\text {nli+sts-b }}$ further validates the idea that fine-tuning on a dissimilar task can hurt KATE's performance. For the $k \mathrm{NN</p>
<p>To understand how the retrieval mechanism helps GPT-3, we conduct a case study on the retrieved examples (see Table 5). By retrieving relevant examples from the training set, KATE provides useful detailed information within the table, e.g., the number of points, rebounds, and assists, to GPT-3 for more accurate description. On the other hand, the random selection method has the issue of hallucination, where the generated sequences contain information (i.e., "senior year" and "University of Texas") not present in the table.</p>
<h3>4.3 Questing Answering</h3>
<p>Lastly, we evaluate KATE on the open-domain QA tasks, as shown in Table 6. We compare with some state-of-the-art fine-tuned methods such as RAG (Lewis et al., 2020) and T5 (Raffel et al., 2019). The T5 results were reported in (Brown et al., 2020) using the 11B model, which needs specialized TPUs to do fine-tuning. KATE again improves GPT-3's performance substantially across various benchmarks. Moreover, KATE helps GPT3 to even outperform the fine-tuned T5 model. It is worth noting that this time both $\mathrm{KATE}<em _nli_sts-b="{nli+sts-b" _text="\text">{\text {nli }}$ and $\mathrm{KATE}</em>}}$ improve upon $\mathrm{KATE<em _roberta="{roberta" _text="\text">{\text {roberta }}$ because fine-tuning on NLI or STS-B datasets is helpful for retrieving semantically similar questions from the QA datasets. Moreover, on the NQ and TriviaQA datasets, further fine-tuning on the STS-B dataset improves KATE's results. We evaluate the baseline $k \mathrm{NN}</em>$ baseline results again suggest that}}$ by using the top-1 nearest neighbor. The $k \mathrm{NN</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overlap Subset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Nonoverlap Subset</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">PARENT</td>
</tr>
<tr>
<td style="text-align: center;">T5 (fine-tuned)</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$28.4 \pm 2.1$</td>
<td style="text-align: center;">$39.3 \pm 2.6$</td>
<td style="text-align: center;">$31.2 \pm 2.5$</td>
<td style="text-align: center;">$41.8 \pm 3.0$</td>
<td style="text-align: center;">$25.6 \pm 1.8$</td>
<td style="text-align: center;">$37.0 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;">$k \mathrm{NN}_{\text {roberta }}$</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">7.52</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {roberta }}$</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">45.5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {nli }}$</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {nli+sts-b }}$</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">43.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Table-to-text generation results on the ToTTo dev dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Table</th>
<th style="text-align: left;">Table: <page_title $>$ Trey Johnson <section_title $>$ College <table $>&lt;$ cell $&gt;32&lt;$ col_header $&gt;$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GP <cell $>4.8$ <col_header $>$ RPG $&lt;$ cell $&gt;2.3$ <col_header $>$ APG $&lt;$ cell $&gt;23.5$ <col_header $>$ PPG</td>
</tr>
<tr>
<td style="text-align: left;">Retrieved</td>
<td style="text-align: left;">Table: <page_title $>$ Dedric Lawson <section_title $>$ College <table $>&lt;$ cell $&gt;9.9$ <col_header $>$</td>
</tr>
<tr>
<td style="text-align: left;">Examples</td>
<td style="text-align: left;">RPG $&lt;$ cell $&gt;3.3$ <col_header $>$ APG $&lt;$ cell $&gt;19.2$ <col_header $>$ PPG</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sentence: Dedric Lawson averaged 19.2 points, 9.9 rebounds and 3.3 assists per game.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Table: <page_title $>$ Carsen Edwards <section_title $>$ College <table $>&lt;$ cell $&gt;3.8$ <col_header $>$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">RPG $&lt;$ cell $&gt;2.8$ <col_header $>$ APG $&lt;$ cell $&gt;18.5$ <col_header $>$ PPG</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sentence: Edwards averaged 18.5 points, 3.8 rebounds and 2.8 assists per game.</td>
</tr>
<tr>
<td style="text-align: left;">Predictions</td>
<td style="text-align: left;">Ground-truth: Trey Johnson averaged 23.5 points, 4.8 rebounds, and 2.3 assists in 32 games.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Random: Trey Johnson averaged 23.5 points per game in his senior year at the University of Texas.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KATE: Johnson averaged 23.5 points, 4.8 rebounds and 2.3 assists per game.</td>
</tr>
</tbody>
</table>
<p>Table 5: A sample of retrieved in-context examples from the ToTTo dataset. For the KATE method, GPT-3 pays more attention to detailed information such as the number of points, rebounds, and assists. In contrast, the random selection method leads GPT-3 to generate details which do not exist in the original table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">WQ</th>
<th style="text-align: center;">TriviaQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RAG (Open-Domain)</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;">T5+SSM (Closed-Book)</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">60.5</td>
</tr>
<tr>
<td style="text-align: center;">T5 (Closed-Book)</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">50.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 (64 examples)</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$28.6 \pm 0.3$</td>
<td style="text-align: center;">$41.0 \pm 0.5$</td>
<td style="text-align: center;">$59.2 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">$k \mathrm{NN}_{\text {roberta }}$</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">26.2</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {roberta }}$</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">57.5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {nli }}$</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">60.9</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{KATE}_{\text {nli+sts-b }}$</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">62.4</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on QA datasets. (*) We used 10 examples for TriviaQA and 64 examples for NQ and WQ.
the retrieval module and GPT-3 work together to achieve better performance. We also explore using 64 nearest neighbors ( 10 for TriviaQA) to determine the answer (by majority voting explained in Section 3.2). The EM score are similar to retrieving the top-1 nearest neighbor.</p>
<p>To investigate why the retrieved examples are helpful, we present a case study. Concretely, the retrieval examples from the NQ dataset are shown in Table 7. For the first and second cases, the random baseline provides wrong answers because GPT-3 is unable to recall the exact detail. However, the in-context examples selected by KATE contain the correct details, which facilitate GPT-3 to answer questions. For the third case, the random baseline
leads GPT-3 to misinterpret the question as asking for a specific location. In contrast, KATE selects similar types of questions asking for the origins of objects. Using these in-context examples, GPT-3 is able to interpret and answer the question correctly.</p>
<h2>5 Analysis of Different Factors</h2>
<h3>5.1 Number of In-context Examples</h3>
<p>We first investigate the impact of the number of examples on KATE's performance. Concretely, on the NQ dataset, we choose the number of examples to be $5,10,20,35$, and 64 , and $\mathrm{KATE}<em _roberta="{roberta" _text="\text">{\text {nli+sts-b }}$ is compared with the random baseline and $\mathrm{KATE}</em>$ across different settings. As shown in the left plot of Figure 2, both KATE and the random baseline benefit from utilizing more examples. However, KATE consistently outperforms the random selection method, even when the number of in-context examples is as few as 5 . This result is interesting because in practice, employing less examples leads to more efficient inference with GPT-3.}</p>
<h3>5.2 Size of Training Set for Retrieval</h3>
<p>We further examine how the size of the training set may influence the KATE method. On the NQ dataset, we create new subsets from the original training set, with sizes of $1 \mathrm{k}, 2 \mathrm{k}, 5 \mathrm{k}, 10 \mathrm{k}, 30 \mathrm{k}$, and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">In-Context Examples</th>
<th style="text-align: center;">Predictions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question: The Mughal Gardens of Rashtrapati Bhavan is modelled on which garden?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The Mughal Garden of Rashtrapati Bhavan is modelled on? The Persian style of architecture</td>
<td style="text-align: center;">Ground-truth: Persian garden</td>
</tr>
<tr>
<td style="text-align: center;">Who built the first Mughal Garden in India? Babur</td>
<td style="text-align: center;">KATE: The Persian gardens</td>
</tr>
<tr>
<td style="text-align: center;">The landscape design of the Gardens of Versailles is known as which style? French garden</td>
<td style="text-align: center;">Random Baseline: Shalimar gardens</td>
</tr>
<tr>
<td style="text-align: center;">Question: What city was Zeus the patron god of?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">What is the symbol of Zeus the Greek God? Bull</td>
<td style="text-align: center;">Ground-truth: Olympia</td>
</tr>
<tr>
<td style="text-align: center;">Where did Zeus spend most of his time? Mount Olympus</td>
<td style="text-align: center;">KATE: Olympia</td>
</tr>
<tr>
<td style="text-align: center;">Where was the statue of Zeus at Olympia located? In the Temple of Zeus</td>
<td style="text-align: center;">Random Baseline Athens</td>
</tr>
<tr>
<td style="text-align: center;">Question: Where did the Dewey decimal system come from?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Where did the formula for area of a circle come from? Archimedes</td>
<td style="text-align: center;">Ground-truth: Melvil Dewey</td>
</tr>
<tr>
<td style="text-align: center;">Where did the name jack russell come from? Reverend John Russell</td>
<td style="text-align: center;">KATE: Melvil Dewey</td>
</tr>
<tr>
<td style="text-align: center;">Where did the letters of the alphabet come from? The Phoenician alphabet</td>
<td style="text-align: center;">Random Baseline: the library of Congress</td>
</tr>
</tbody>
</table>
<p>Table 7: Three samples of retrieved in-context examples from the NQ dataset. Three retrieved Q-A pairs are shown on the left. Predictions by the KATE method and useful details from in-context examples are shown in Green. Gold-standard references are shown in Blue. Predictions by the random baseline are shown in Red.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left: Effect of number of in-context examples for different selection methods. Right: Effect of the size of training set for retrieval on KATE. Two representative sentence encoders are used in these studies.</p>
<p>70k, respectively. In-context examples are retrieved from these subsets instead of the original training set. The number of nearest neighbors is set to 64 . We compare $\mathrm{KATE}<em _roberta="{roberta" _text="\text">{\text {nli+sts-b }}$ with the random selection method and $\mathrm{KATE}</em>}}$, and the results are shown in the right plot of Figure 2. For $\mathrm{KATE<em _nli_sts-b="{nli+sts-b" _text="\text">{\text {roberta }}$ and $\mathrm{KATE}</em>$, as the size of the training set increases, the EM scores also increase. In contrast, the result of the random sampling baseline does not change much. Intuitively, as the training size gets larger, it is more likely for KATE to retrieve relevant in-context examples to help GPT-3 answer a question correctly. As we have shown previously in Table 7, the retrieved in-context examples could provide critical detailed information to GPT-3, thus helping GPT-3 to better answer the questions.}</p>
<h3>5.3 Order of In-context Examples</h3>
<p>Moreover, we explore how the order of in-context examples may affect KATE's results. As mentioned in Section 2.3, under the standard setting, the retrieved in-context examples are ordered such that $s\left(x_{i}, x\right) \geq s\left(x_{j}, x\right)$ whenever $i&lt;j$. Here, we ran-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Trial</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">Reverse</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">EM Score</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">42.8</td>
</tr>
</tbody>
</table>
<p>Table 8: Analysis on the effect of orders of in-context example on the NQ dataset using $\mathrm{KATE}<em _nli_sts-b="{nli+sts-b" _text="\text">{\text {nli+sts-b }}$. The default order puts the most similar example in the front, and the reverse order does the opposite.
domly permute the order of in-context examples in the NQ dataset for the proposed $\mathrm{KATE}</em>, x\right)$ whenever $i&lt;j$. The results are presented in Table 8. On this particular NQ dataset, the reverse order performs the best. However, we also did the experiments on the WQ and TriviaQA and find that the default order performs slightly better than the reverse order. Hence,}}$ method, and conduct the experiments for 3 different orders. Additionally, we explore the reverse order where $s\left(x_{i}, x\right) \leq s\left(x_{j</p>
<p>the choice of orders is data-dependent. Additionally, it can be observed that the variation among the NQ results tends to be quite small (compared with the difference between the random baseline and KATE), indicating that the example order does not have a significant impact on KATE's performance.</p>
<h2>6 Related Work</h2>
<p>Pre-trained Language Models NLP systems have made tremendous progress by pre-training models on unlabeled text (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Raffel et al., 2019; Xue et al., 2020; Lample and Conneau, 2019; Radford et al., 2018, 2019). These models can be fine-tuned for a wide range of downstream tasks. GPT-3 (Brown et al., 2020), however, can perform in-context learning without fine-tuning. People have just started trying to understand GPT3 from different perspectives. (Hendrycks et al., 2020) studies which categories of questions GPT-3 is more capable of answering. (Zhao et al., 2021) proposes to improve the model by contextual calibration. However, their method is limited to predicting very few tokens because for long sequence generation, the contextual calibration step needs to be repeatedly performed after each newly generated token. In contrast, our work, KATE, only calls the API once and is suitable for both text classification and generation tasks. Another related work is LM-BFF (Gao et al., 2020), which uses a smaller language model (RoBERTa-large) to demonstrate that prompt-based fine-tuning can outperform standard fine-tuning on text classification tasks. Our work differs by showing that, without fine-tuning, relevant examples can still substantially improve the performance of GPT-3 for both text classification and generation tasks. Finally, AutoPrompt (Shin et al., 2020) explores adding some additional tokens to smaller language models to improve performance on classification tasks.</p>
<p>Retrieval-based Text Generation There is a long history of applying information retrieval to text generation (Sumita and Hitoshi, 1991). It is very related to the exemplar-based learning (Jäkel et al., 2008; Ziyadi et al., 2020). Some representative applications in the field of deep learning include machine translation (Gu et al., 2018), sentiment transfer (Li et al., 2018; Guu et al., 2018), QA (Karpukhin et al., 2020; Mao et al., 2020), dialogue generation (Yan et al., 2016; Cai et al., 2018; Song et al., 2016; Pandey et al., 2018; We-
ston et al., 2018; Wu et al., 2019), text summarization (Cao et al., 2017; Peng et al., 2019), data-to-text generation (Peng et al., 2019), and text-tocode generation (Hashimoto et al., 2018). All these retrieve-and-edit frameworks require their editors to be trained or fine-tuned on specific tasks. In contrast, our work uniquely examines how to better use GPT-3 as a universal editor without fine-tuning. We find that the more semantically similar context we provide to GPT-3, the better results the model can generate.</p>
<p>Improve NLP Systems with $k$ NN Some recent works try to incorporate non-parametric methods to improve a given model's performance. For example, the newly introduced $k$ NN-LM (Khandelwal et al., 2019), $k$ NN-MT (Khandelwal et al., 2020), and BERT- $k$ NN (Kassner and Schütze, 2020) generate the next token by retrieving the nearest $k$ neighbors from the datastore. Another related work $k$ NN classification model (Rajani et al., 2020) uses $k \mathrm{NN}$ as backoff when the confidence is low from the classification model. There are two key differences between our work and other approaches. First, we retrieve the nearest $k$ neighbors to modify the conditional context instead of the prediction. Second, we do not have access to the parameters of GPT-3. Instead, we rely on some independently pre-trained models to get the sentence embeddings to retrieve the nearest $k$ neighbors.</p>
<h2>7 Conclusion</h2>
<p>This work presented a first step towards investigating the sensitivity of GPT-3 to in-context examples. To this end, we proposed KATE, a non-parametric selection approach that retrieves in-context examples according to their semantic similarity to the test samples. On several natural language understanding and generation tasks, the proposed method improves GPT-3's performance, over the random sampling baseline, by a significant margin. Particularly, KATE enables GPT-3 to achieve performance comparable to a fine-tuned T5 model on the table-to-text generation task and outperforms T5 on the QA task. Moreover, we found that fine-tuning the sentence embeddings for retrieval on task-related datasets gave rise to further empirical gains. Detailed analysis was conducted to explore the robustness of KATE to different hyperprameters, such as the number of in-context examples, examples' order, etc. One limitation we notice is that despite the improved performance on sentiment analysis,</p>
<p>GPT-3 still lags behind the fine-tuned T5 model by a small margin. This suggests that our proposed method is more suitable and effective on long text generation tasks. We hope this work could provide insights for better understanding the behaviors of GPT-3 and represents a helpful step towards further improving its in-context learning capabilities.</p>
<h2>8 Ethical and Broader Impacts</h2>
<p>Risk Our proposed KATE method significantly improves the in-context learning ability of GPT-3 and makes long-text generation more easily without fine-tuning the pre-trained model. However, one risk implication is that our proposed method will benefit the research groups which are financially capable of using such huge models. For individual or small-group researchers, they cannot apply our proposed method to their specific applications since they don't have access to the model. Our work has suggested researchers should focus more on investigating the in-context learning of pretrained models. One potential future direction is for researchers to scale-down the sizes of pre-trained models to find a balance between model performance and model size. Once a smaller model is obtained with comparable performance (enhanced by KATE), our proposed method can become more widely accessible to individual researchers.</p>
<p>Potential Bias During the experiment on table-to-text generation, we have pointed out that large pre-trained language models could be susceptible to hallucination (case study in Table 5). This problem is more pronounced when we use randomly sampled examples. This happens because the language model is biased toward the training dataset. As shown in Table 5, when random examples are used, the sentence generated by GPT-3 is grammatically correct, but some details never exist in the given table. In contrast, our proposed method, KATE, can significantly alleviate this problem by guiding GPT-3 to look for and generate the correct information. For similar reasons, large pretrained models could be potentially susceptible to gender and racial bias. Since our KATE method shows that in-context examples are crucial for highquality long-text generations, one way to alleviate the racial and gender bias is to incorporate an additional module to filter out offensive in-context examples. Since racial and gender bias are not our main research focus, a full investigation goes beyond the scope of our work. However, we believe
this is an exciting opportunity for future work.</p>
<h2>Code Availability</h2>
<p>Implementations of the proposed KATE method discussed in this paper are available at https: //github.com/jiachangliu/KATEGPT3.</p>
<h2>References</h2>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533-1544.</p>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. 2018. Skeleton-to-response: Dialogue generation guided by retrieval memory. arXiv preprint arXiv:1809.05296.</p>
<p>Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2017. Faithful to the original: Fact aware neural abstractive summarization. arXiv preprint arXiv:1711.04434.</p>
<p>Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezGazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, MingWei Chang, Dipanjan Das, and William W Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. arXiv preprint arXiv:1906.01081.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2018. Search engine guided neural machine translation. In AAAI, pages 5133-5140.</p>
<p>Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. 2018. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437-450.</p>
<p>Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. 2018. A retrieve-and-edit framework for predicting structured outputs. In Advances in Neural Information Processing Systems, pages $10052-10062$.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.</p>
<p>Frank Jäkel, Bernhard Schölkopf, and Felix A Wichmann. 2008. Generalization and similarity in exemplar models of categorization: Insights from machine learning. Psychonomic Bulletin \&amp; Review, 15(2):256271.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>Nora Kassner and Hinrich Schütze. 2020. Bertknn: Adding a knn search component to pretrained language models for better qa. arXiv preprint arXiv:2005.00766.</p>
<p>Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Nearest neighbor machine translation. arXiv preprint arXiv:2010.00710.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466.</p>
<p>Guillaume Lample and Alexis Conneau. 2019. Crosslingual language model pretraining. arXiv preprint arXiv:1901.07291.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401.</p>
<p>Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: A simple approach to sentiment and style transfer. arXiv preprint arXiv:1804.06437.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142-150.</p>
<p>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020. Generation-augmented retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553.</p>
<p>Gaurav Pandey, Danish Contractor, Vineet Kumar, and Sachindra Joshi. 2018. Exemplar encoder-decoder for neural conversation generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1329-1338.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Ankur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-totext generation dataset. In Proceedings of EMNLP.</p>
<p>Hao Peng, Ankur P Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. 2019. Text generation with exemplar-based adaptive decoding. arXiv preprint arXiv:1904.04428.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Nazneen Fatema Rajani, Ben Krause, Wengpeng Yin, Tong Niu, Richard Socher, and Caiming Xiong. 2020. Explaining and improving model behavior with k nearest neighbor representations. arXiv preprint arXiv:2010.09030.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.</p>
<p>Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. arXiv preprint arXiv:2004.09813.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages $1631-1642$.</p>
<p>Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. 2016. Two are better than one: An ensemble of retrieval-and generation-based dialog systems. arXiv preprint arXiv:1610.07149.</p>
<p>Eiichiro Sumita and HDA Hitoshi. 1991. Experiments and prospects of example-based machine translation. In 29th Annual Meeting of the Association for Computational Linguistics, pages 185-192.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30:5998-6008.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.</p>
<p>Jason Weston, Emily Dinan, and Alexander H Miller. 2018. Retrieve and refine: Improved sequence generation models for dialogue. arXiv preprint arXiv:1808.04776.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. 2019. Response generation by context-aware prototype editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7281-7288.</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.</p>
<p>Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to respond with deep neural networks for retrieval-based human-computer conversation system. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, pages 55-64.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages 5753-5763.</p>
<p>Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.</p>
<p>Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, and Weizhu Chen. 2020. Examplebased named entity recognition. arXiv preprint arXiv:2008.10570.</p>
<h2>A An Example of In-context Learning</h2>
<p>As shown in the illustration of Figure 3, GPT-3 is asked to translate "mountain" to its German version based on the three examples given as part of the input.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The figure above shows how to perform in-context learning with a language model. Three incontext examples and the test prompt are concatenated as a single string input for GPT-3, with a special character "in" inserted between two adjacent examples. GPT-3 keeps generating tokens until there is a special character "in".</p>
<h2>B Data Split</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">67 k</td>
<td style="text-align: center;">872</td>
<td style="text-align: center;">1.8 k</td>
</tr>
<tr>
<td style="text-align: center;">IMDB</td>
<td style="text-align: center;">25 k</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25 k</td>
</tr>
<tr>
<td style="text-align: center;">ToTTo</td>
<td style="text-align: center;">120 k</td>
<td style="text-align: center;">7.7 k</td>
<td style="text-align: center;">7.7 k</td>
</tr>
<tr>
<td style="text-align: center;">NQ</td>
<td style="text-align: center;">79 k</td>
<td style="text-align: center;">8.8 k</td>
<td style="text-align: center;">3.6 k</td>
</tr>
<tr>
<td style="text-align: center;">WQ</td>
<td style="text-align: center;">3.4 k</td>
<td style="text-align: center;">361</td>
<td style="text-align: center;">2 k</td>
</tr>
<tr>
<td style="text-align: center;">TriviaQA</td>
<td style="text-align: center;">78.8 k</td>
<td style="text-align: center;">8.8 k</td>
<td style="text-align: center;">11.3 k</td>
</tr>
</tbody>
</table>
<p>Table 9: Data split for different datasets. In-context examples are selected from the training set. Because ToTTo and TriviaQA require submitting to their leaderboards, the evaluation is done on the dev sets. For all other datasets, the evaluation is done on the test sets.</p>
<h2>C Complete ToTTo Case Study</h2>
<p>Due to the length limit of the main paper, we present in the appendix the full ToTTo case study comparing the random sampling baseline and our proposed KATE method. We present the case study in Table 10.</p>
<p>As we have discussed in the main paper, the in-context examples retrieved by KATE facilitates GPT-3 to effectively extract key information from the given table. Detailed numbers such as the number of points, rebounds, and assists have all been included in the sentence.</p>
<p>In contrast, the sentence generated by GPT-3 using randomly sampled in-context examples only
extract partial information from the table. Only the number of points is included while the numbers of rebounds and assists are ignored. Moreover, the random sampling baseline could lead to the issue of hallucination. Both "senior year" and "University of Texas" are not present in the given table. One may wonder whether these wrong phrases were present in the randomly sampled in-context examples, which might have caused this issue. However, if we look at the randomly sampled in-context examples in the second block of the table, such information do not exist. This suggests such hallucinated phrases are generated by the language model itself.</p>
<p>This comparison provides some key insights on why KATE works better than the random sampling baseline. By retrieving semantically/syntactically similar in-context examples, KATE provides GPT3 with a much more accurate template/structure to do text generation. Without such structure, GPT-3 can generate sentences that are fluent but do not meet the goal of a particular task.</p>
<h2>D On Prompt Engineering vs. Fine-tuning</h2>
<p>As we mentioned in the main paper, given a training dataset, we could take the full advantage of the GPT-3's model strength through fine-tuning. However, there are several advantages of prompt engineering over fine-tuning. First, fine-tuning requires access to the model parameters and gradients. It is impossible to access this information via the current GPT-3's API. Second, fine-tuning large models are time-consuming and costly. Ordinary research labs and individual developers do not have resources to accomplish such tasks. Third, storing large fine-tuned model checkpoints requires large storage space. Even if GPT-3 is fine-tuned and stored for many specific tasks/datasets, many finetuned checkpoints may not be frequently called. This is not energy efficient. Our proposed KATE method does not require costly fine-tuning and improves the random baseline on both text classification and generation tasks, sometimes by a significant margin. This makes it more practical to deploy the same GPT-3 model across all tasks.</p>
<h2>E T5 Baseline</h2>
<p>Although our primary goal is to improve GPT-3's in-context learning ability, we also include the finetuned T5 results as a reference (3B T5 on SST-2 and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Test Table</th>
<th style="text-align: center;">Table: <page_title $>$ Trey Johnson <section_title $>$ College <table $>&lt;$ cell $&gt;32&lt;$ col_header $&gt;$ <br> GP <cell $>4.8&lt;$ col_header $&gt;$ RPG $&lt;$ cell $&gt;2.3&lt;$ col_header $&gt;$ APG $&lt;$ cell $&gt;23.5&lt;$ col_header $&gt;$ PPG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Randomly Sampled Examples</td>
<td style="text-align: center;">Table: <page_title $>$ List of RAGBRAI overnight stops <section_title $>$ By year <table $>&lt;$ cell $&gt;$ 1986 <col_header $>&lt;$ col_header $&gt;$ Year <cell $>$ Audubon (1) <col_header $>$ Route - start to finish (number indicates occurrence) <col_header $>$ Monday <cell $>2006$ <col_header $>&lt;$ col_header $&gt;$ Year <cell $>$ Audubon (2) <col_header $>$ Route - start to finish (number indicates occurrence) <br> <col_header $>$ Monday <br> Sentence: Audubon has been an RAGBRAI overnight stop in 1986 and 2006. <br> Table: <page_title $>$ List of Administrators of British Brunei <section_title $>$ British Brunei <br> administrators <table $>&lt;$ cell $&gt;$ Malcolm Stewart Hannibal McArthur <col_header $>$ Consul <br> Generals to Brunei <col_header $>$ British Consuls in Brunei <col_header $>$ British Residents in Brunei <br> Sentence: Malcolm Stewart Hannibal McArthur was the first British resident in Brunei.</td>
</tr>
<tr>
<td style="text-align: center;">KATE- <br> Retrieved <br> Examples</td>
<td style="text-align: center;">Table: <page_title $>$ Dedric Lawson <section_title $>$ College <table $>&lt;$ cell $&gt;9.9&lt;$ col_header $&gt;$ <br> RPG $&lt;$ cell $&gt;3.3&lt;$ col_header $&gt;$ APG $&lt;$ cell $&gt;19.2&lt;$ col_header $&gt;$ PPG <br> Sentence: Dedric Lawson averaged 19.2 points, 9.9 rebounds and 3.3 assists per game. <br> Table: <page_title $>$ Carsen Edwards <section_title $>$ College <table $>&lt;$ cell $&gt;3.8&lt;$ col_header $&gt;$ <br> RPG $&lt;$ cell $&gt;2.8&lt;$ col_header $&gt;$ APG $&lt;$ cell $&gt;18.5&lt;$ col_header $&gt;$ PPG <br> Sentence: Edwards averaged 18.5 points, 3.8 rebounds and 2.8 assists per game.</td>
</tr>
<tr>
<td style="text-align: center;">Predictions</td>
<td style="text-align: center;">Ground-truth: Trey Johnson averaged 23.5 points, 4.8 rebounds, and 2.3 assists in 32 games. <br> Random: Trey Johnson averaged 23.5 points per game in his senior year at the University of Texas. <br> KATE: Johnson averaged 23.5 points, 4.8 rebounds and 2.3 assists per game.</td>
</tr>
</tbody>
</table>
<p>Table 10: A sample of retrieved in-context examples from the ToTTo dataset. For the KATE method, GPT-3 pays more attention to detailed information such as the number of points, rebounds, and assists. In contrast, the random selection method leads GPT-3 to generate details which do not exist in the original table. Information such as "senior year" and "University of Texas" also do not exist in the randomly sampled in-context examples. This suggests that the wrong information was generated by the language model itself. Although the sentence by the random sampling baseline is fluent, it does meet the goal of the table-to-text task.</p>
<p>ToTTo datasets, and 11B T5 on the QA datasets). The reason for reporting the 3B T5 results on the SST-2 and ToTTo datasets is that this is the largest T5 model we can use. For the 3B T5 model, Google Colab ${ }^{3}$ provides a free V2-8 TPU to fine-tune the 3B model. We used the Colab tutorial notebook to fine-tune the 3B T5 model on the SST-2 and ToTTo training sets. We couldn't fine-tune the 11B T5 model because the model size is too large. Finetuning such a large model requires a V3-8 TPU, which is not free of charge. Fortunately, the original GPT-3 paper (Brown et al., 2020) has already reported the finet-tuned 11B T5 results on the three QA datasets, so we reuse these results in our main paper for the QA task. Our proposed KATE method significantly improves GPT-3, performing comparably to the fine-tuned T5 model on the table-to-text task and outperforming the fine-tuned T5 model on the QA task.</p>
<h2>F Details on Retrieval Modules</h2>
<p>As we mention in the main paper, we use the pretrained RoBERTa-large model (Liu et al., 2019)</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>as the first retrieval module, which has 355M parameters and is pre-trained with the MLM (masked language modeling) objective. The result given by this module is denoted as $\mathrm{KATE}_{\text {roberta }}$. We directly download this model from the HuggingFace Model Zoo (MIT license) ${ }^{4}$. All other retrieval modules share the same architecture as the RoBERTa-large module but are fine-tuned on specific datasets.</p>
<p>For the fine-tuned retrieval modules, the first we use is the RoBERTa-large model fine-tuned on the SNLI and MultiNLI datasets ( $\mathrm{KATE}<em _mathrm_nli="\mathrm{nli">{\mathrm{nli}}$ ) (Bowman et al., 2015; Williams et al., 2017); the next we use is the RoBERTa-large model fine-tuned on the SNLI and MultiNLI dataset and then on the STS-B datasets $\left(\mathrm{KATE}</em>$.}+\mathrm{sts}-\mathrm{b}}\right)$ (Cer et al., 2017). These fine-tuned models have already been accomplished and included by the Sentence-BERT family and are publicly available, so we directly download from the Sentence-BERT Model Zoo ${ }^{5</p>
<p>Lastly, specifically for the sentiment analysis task, we include a RoBERTa-large model finetuned on the SST-2 dataset ( $\mathrm{KATE}_{\text {sst-2 }}$ ) (Socher et al., 2013; Wang et al., 2018). At the time of our</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>research, we didn't find a good publicly available fine-tuned model, so we fine-tune the pre-trained RoBERTa-large model on SST-2 by ourselves. The exact fine-tuning procedure, including the hyperparameters and learning rate, can be found at the HuggingFace website ${ }^{6}$. We fine-tune the RoBERTalarge model using a single V100 GPU.</p>
<h1>G Prompt Templates Used</h1>
<p>For reproducibility, we show the prompt templates used for all tasks in Tables 11 .</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Prompt Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2 \&amp; IMDB</td>
<td style="text-align: left;">Sentence: comes from the brave, uninhibited performances. Label: Positive <br> Sentence: This tearful movie about a sister and her battle to save as many souls as she can is very <br> moving. The film does well in picking up the characters and showing how Sister Helen deals with <br> each. A wonderful journey from life to death. Label:</td>
</tr>
<tr>
<td style="text-align: left;">ToTTo</td>
<td style="text-align: left;">Table: <page_title>Dedric Lawson <section_title>College <table><cell>9.9 <col_header>RPG <br> <cell>3.3 <col_header>APG <cell>19.2 <col_header>PPG <br> Sentence: Dedric Lawson averaged 19.2 points, 9.9 rebounds and 3.3 assists per game. <br> Table: <page_title>Trey Johnson <section_title>College <table><cell>32 <col_header>GP <br> <cell>4.8 <col_header>RPG <cell>2.3 <col_header>APG <cell>23.5 <col_header>PPG <br> Sentence:</td>
</tr>
<tr>
<td style="text-align: left;">QA</td>
<td style="text-align: left;">Q: The landscape design of the Gardens of Versailles is known as which style? <br> A: The Persian style of architecture. <br> Q: The Mughal Gardens of Rashtrapati Bhavan is modelled on which garden? <br> A:</td>
</tr>
</tbody>
</table>
<p>Table 11: The prompt templates used for all tasks discussed in the paper. We show only one in-context example per task for illustration purposes.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ The fine-tuning script we use can be found at https://huggingface.co/transformers/ v2.7.0/examples.html#glue.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ The HuggingFace Model Zoo can be found at https: //huggingface.co/models.
${ }^{5}$ The Sentence-BERT Model Zoo can be found at https: //huggingface.co/sentence-transformers.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
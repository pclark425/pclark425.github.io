<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2415 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2415</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2415</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-6bf623e772d5634e33a035a3586dbab41e29c78b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6bf623e772d5634e33a035a3586dbab41e29c78b" target="_blank">AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> It is shown that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks, by introducing a novel framework that significantly reduces human bias through a generic search space.</p>
                <p><strong>Paper Abstract:</strong> Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2415.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2415.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoML-Zero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary AutoML framework that represents entire ML algorithms as small programs (Setup, Predict, Learn) composed of simple mathematical operations and searches this generic space to automatically discover learning algorithms from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML-Zero</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoML-Zero encodes ML algorithms as programs with three component functions (Setup, Predict, Learn) operating on a small typed memory (scalars, vectors, matrices). It searches a very large, fine-grained space of programs using evolutionary search (regularized evolution) and baselines (random search). Evaluations use training/validation loops over task datasets; the framework uses proxy low-dimensional tasks, functional-equivalence checking, early-stopping 'hurdles', migration between parallel workers, and hyperparameter tuning to scale search. Discovered programs can implement model structure, initialization, forward pass and learning rule (e.g., rediscovered backprop, bilinear interactions, normalized gradients, weight averaging).</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning (AutoML, meta-learning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration / automated algorithm discovery</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Not explicitly quantified as a formal "novelty" score; novelty is inferred qualitatively by (a) occurrence of convergent code motifs across independent runs, (b) discovery of algorithmic motifs not hand-designed (e.g., bilinear interactions, normalized gradients, weight averaging). The paper does not define a numerical novelty metric (null).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility of generated algorithms is operationalized as standard ML performance metrics on tasks: mean validation loss (mean_loss returned by Evaluate), median accuracy across a set of tasks, RMS error for regression tasks. Search-space "feasibility" / difficulty is measured by the empirical density of acceptable algorithms under random search (RS success rate = ratio of acceptable algorithms to total evaluated), e.g., 1 acceptable algorithm per 1e7 evaluated for linear regression in a restricted experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Examples reported: RS density for linear regression ≈ 1 / 10^7; evolution is reported ~5x more efficient than RS in that setting. Final evolved binary-CIFAR algorithm accuracy: 84.06 ± 0.10% (evolved) vs 82.22 ± 0.17% (2-layer NN baseline) and 77.65 ± 0.22% (logistic baseline). Other dataset results: SVHN 88.12% (evolved) vs 85.14% (nonlinear baseline) vs 59.58% (linear), downsampled ImageNet 80.78% vs 78.44% vs 76.44%, Fashion MNIST 98.60% vs 98.21% vs 97.90%.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Qualitative evidence: the paper emphasizes a trade-off between reduced human bias / more generic search spaces (higher potential for innovation/novelty) and increased sparsity/harder search (lower feasibility/density of good solutions). This trade-off is described qualitatively (e.g., "genericity of the AutoML-Zero space makes it more difficult to search"; solutions can be as rare as 1 in 10^12 in the generic space). No numeric correlation or Pareto analysis between novelty and feasibility is provided (null for quantified tradeoff).</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>The framework uses (single-objective) evolutionary search (regularized evolution) optimizing task performance (median validation accuracy / mean loss). To make search tractable it employs search-engine optimizations (functional equivalence checking to avoid re-evaluating semantically equivalent programs, hurdles early-stopping based on population percentiles, migration between parallel workers for diversity), and hyperparameter tuning (random search over constants) for final evaluation. No explicit multi-objective novelty-vs-feasibility optimization is implemented (null).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Humans inspected, simplified, and interpreted evolved programs and ran ablation/knock-in studies. Ablation results for the best evolved algorithm (Section 4.2) showed accuracy drops when removing discovered components: input-noise regularizer (-0.16% absolute), bilinear multiplicative interactions (-1.46%), normalized gradients (-1.20%), and weight averaging (-4.11%). Task-adaptation experiments were repeated and analyzed statistically: noisy-ReLU/dropout-like adaptation appeared preferentially with little data (8/30 experiments vs 0/30 control, p<0.0005); learning-rate decay appeared for fast-training settings (30/30 vs 3/30 control, p<1e-14); transformed-mean-as-learning-rate appeared for multi-class tasks (24/30 vs 0/30 control, p<1e-11). The paper describes manual decoupling of hyperparameters (human intervention) to enable transfer to higher-dimensional data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Baselines include random search (RS) as a search baseline and hand-designed algorithms for final predictive baselines (logistic regression and 2-layer fully connected neural network trained by gradient descent); related prior methods are cited (learned optimizers, symbolic optimizers, genetic programming).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Evolution vs RS: for simple linear regression, evolution ~5x more efficient than RS given equal total evaluations; RS density for acceptable linear algorithms ≈ 1/10^7. Final predictive comparisons (binary CIFAR final eval): AutoML-Zero evolved best algorithm accuracy 84.06 ± 0.10% vs nonlinear baseline 82.22 ± 0.17% and linear baseline 77.65 ± 0.22%. Transfer gains reported on SVHN, downsampled ImageNet, and Fashion MNIST as summarized in feasibility_score.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Domain/task-specific emergent behaviors: (1) With few training examples, evolution produced noisy-ReLU / dropout-like injection of noise as a regularizer. (2) For fast-training scenarios (fewer epochs), evolution discovered learning-rate decay schedules (iterated arctan approximating near-exponential decay). (3) For multi-class tasks, evolved algorithms often used a transformed mean of the weight matrix as a learning-rate signal. These findings indicate automatic adaptation of algorithmic motifs to task constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-Zero: Evolving Machine Learning Algorithms From Scratch', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2415.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2415.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regularized Evolution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regularized Evolution (a.k.a. aging or tournament-regularized evolutionary algorithm for architecture search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary search algorithm that maintains a population of candidate programs/models, uses tournament selection to pick parents and replaces the oldest population member with a mutated child, emphasizing recency/aging to promote exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regularized evolution for image classifier architecture search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Regularized Evolution</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A population-based evolutionary algorithm (used as the core search driver in AutoML-Zero). Each cycle samples T individuals, picks the best as parent, copies and mutates it to create a child, inserts the child and deletes the oldest individual. Mutations are insert/remove instruction, randomize a component function, or modify an instruction argument. The paper uses this method at scale, with many worker processes, migration, and added practical improvements (FEC, hurdles).</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning (search/AutoML)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration / optimization in extremely large sparse discrete program spaces</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility of solutions found by regularized evolution is measured by the same ML metrics (mean validation loss, median accuracy across tasks). Search effectiveness is compared to random search by measuring success rate (density of acceptable programs) given equal total evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Reported empirical improvement: for linear regression example evolution produced ~5x higher success rate than RS given same evaluation budget; in more difficult tasks evolution greatly outperformed RS (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Regularized evolution enables discovery of solutions in sparse generic spaces where RS fails; the paper qualitatively reports that evolution is much more effective as task difficulty increases. No formal novelty-feasibility tradeoff analysis is provided for the search algorithm itself.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Single-objective search maximizing validation performance; practical accelerations include FEC to avoid duplicate evals, hurdles early-stopping, parallel workers with migration, and variable tournament/population hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared to Random Search in experiments; also contrasted with other potential search methods in discussion (reinforcement learning, Bayesian optimization) but not used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Empirically superior to RS in the AutoML-Zero search space: especially as problem difficulty increases (Figure 4); concrete example: 5x better efficiency for linear regression restricted-experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Regularized evolution can discover complex algorithmic constructs (e.g., backprop-like code) when the search space permits; effectiveness improves with added techniques (FEC, hurdles, migration).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-Zero: Evolving Machine Learning Algorithms From Scratch', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2415.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2415.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Search (RS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Search (baseline for AutoML search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline search strategy that samples random programs from the search space and selects the best by validation performance; used here both as a baseline and to estimate "density" of acceptable algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Random search for hyper-parameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Random search (RS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Randomly samples programs (component functions/instructions/arguments) from the AutoML-Zero search space (optionally with constraints in restricted experiments) and evaluates them on proxy tasks; used to estimate the sparsity (density) of acceptable solutions and as a baseline to compare evolutionary search.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning (AutoML, hyperparameter/model search)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>baseline sampling / density estimation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>RS success rate (density) defined as ratio of acceptable algorithms (those exceeding a hand-designed reference performance) to total evaluated; used as an empirical measure of problem difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Example: RS density for acceptable linear-regression algorithms ≈ 1 in 10^7 (i.e., one acceptable algorithm every 10^7 random samples). For more complex tasks RS failed to find solutions that evolution found.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>RS demonstrates that the generic search space is sparse: increased genericity (less bias) leads to much lower density of acceptable solutions, indicating a practical trade-off where higher novelty potential costs search feasibility. This is presented qualitatively and via RS density numbers; no quantitative novelty-feasibility frontier is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>None beyond uniform random sampling. Used as a baseline; in final hyperparameter tuning RS is used over continuous constants.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared directly to regularized evolution (main paper experiments) and to hand-designed algorithm references.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Evolution outperforms RS substantially in sparse generic search spaces; RS found acceptable solutions only rarely (e.g., 1/10^7 for a simple linear task) while evolution produced acceptable algorithms much more often for same eval budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>RS can be competitive in densely designed AutoML spaces (cited previous work) but fails in the very generic AutoML-Zero space the paper defines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-Zero: Evolving Machine Learning Algorithms From Scratch', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2415.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2415.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Functional Equivalence Checking (FEC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional Equivalence Checking for supervised ML algorithms (fingerprinting predictions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical caching technique that detects when two algorithms behave identically (on a short set of examples) to avoid redundant full evaluations by hashing truncated predictions into a fingerprint.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Functional Equivalence Checking (FEC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>FEC fingerprints a candidate algorithm by running it for 10 training and 10 validation steps on a fixed example set, truncating and hashing the resulting predictions to create a fingerprint. An LRU cache maps fingerprints to previously-computed accuracies; when a new candidate's fingerprint matches, the cached accuracy is reused instead of re-evaluating. This reduces duplicate evaluations of semantically equivalent (or effectively equivalent on the fingerprint set) programs and yielded ~4x speedup in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning (search/AutoML infrastructure)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>search efficiency / duplicate-detection</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Operational: reduces compute cost by reusing evaluations when fingerprints match; measured speedup (~4x) in the paper's infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Reported ~4x speedup from FEC (section 3.2 / S3); cache size used: 100k fingerprint-accuracy pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>FEC impacts feasibility (computational cost) positively by saving compute; it does not provide a measure of novelty and does not aim to distinguish novel from duplicate implementations except by identical behavior on the fingerprint examples.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Engineering optimization (memoization) to improve throughput of evolutionary search; complementary to search algorithm rather than an objective tradeoff controller.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared implicitly to naive re-evaluation (no FEC).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>About 4x faster search throughput when FEC is used (paper reports this speedup empirically).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>FEC is a practical mechanism when searching program spaces where many syntactic variants produce the same behavior; it reduces redundant compute but does not measure or encourage semantic novelty beyond the fingerprint test.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-Zero: Evolving Machine Learning Algorithms From Scratch', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2415.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2415.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Genetic Programming (symbolic optimizer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming for discovering symbolic optimizers / learning rules (prior work, e.g., Bengio et al., 1994)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier work that used genetic programming to evolve symbolic expressions (trees) representing local learning rules (operators combining available local signals) to discover learning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Use of genetic programming for the search of a new learning rule for neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming / Symbolic Optimizer (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced prior approaches represent update rules as symbolic trees whose leaves are local signals (inputs, activations, errors) and whose internal nodes are basic arithmetic operations; evolutionary operators evolve tree structures to discover new learning rules. The paper cites Bengio et al. (1994) as an early example that searched for a local learning rule using genetic programming.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning (symbolic optimizer discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>automated algorithm discovery / optimizer discovery</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Genetic programming (tree-based representations) evolving symbolic formulas; contrasted in the paper with the sequence-of-instructions representation used in AutoML-Zero.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>The paper positions genetic programming as prior art for symbolically discovering learning rules, but notes that AutoML-Zero differs by searching entire algorithms (not just optimizers) and using a sequence-of-instructions representation permitting richer vector/matrix ops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-Zero: Evolving Machine Learning Algorithms From Scratch', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Regularized evolution for image classifier architecture search <em>(Rating: 2)</em></li>
                <li>Learning to learn by gradient descent by gradient descent <em>(Rating: 2)</em></li>
                <li>Use of genetic programming for the search of a new learning rule for neural networks <em>(Rating: 2)</em></li>
                <li>Neural optimizer search with reinforcement learning <em>(Rating: 2)</em></li>
                <li>Random search for hyper-parameter optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2415",
    "paper_id": "paper-6bf623e772d5634e33a035a3586dbab41e29c78b",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "AutoML-Zero",
            "name_full": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
            "brief_description": "An evolutionary AutoML framework that represents entire ML algorithms as small programs (Setup, Predict, Learn) composed of simple mathematical operations and searches this generic space to automatically discover learning algorithms from scratch.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AutoML-Zero",
            "system_description": "AutoML-Zero encodes ML algorithms as programs with three component functions (Setup, Predict, Learn) operating on a small typed memory (scalars, vectors, matrices). It searches a very large, fine-grained space of programs using evolutionary search (regularized evolution) and baselines (random search). Evaluations use training/validation loops over task datasets; the framework uses proxy low-dimensional tasks, functional-equivalence checking, early-stopping 'hurdles', migration between parallel workers, and hyperparameter tuning to scale search. Discovered programs can implement model structure, initialization, forward pass and learning rule (e.g., rediscovered backprop, bilinear interactions, normalized gradients, weight averaging).",
            "research_domain": "machine learning (AutoML, meta-learning)",
            "problem_type": "open-ended exploration / automated algorithm discovery",
            "novelty_metric": "Not explicitly quantified as a formal \"novelty\" score; novelty is inferred qualitatively by (a) occurrence of convergent code motifs across independent runs, (b) discovery of algorithmic motifs not hand-designed (e.g., bilinear interactions, normalized gradients, weight averaging). The paper does not define a numerical novelty metric (null).",
            "novelty_score": null,
            "feasibility_metric": "Feasibility of generated algorithms is operationalized as standard ML performance metrics on tasks: mean validation loss (mean_loss returned by Evaluate), median accuracy across a set of tasks, RMS error for regression tasks. Search-space \"feasibility\" / difficulty is measured by the empirical density of acceptable algorithms under random search (RS success rate = ratio of acceptable algorithms to total evaluated), e.g., 1 acceptable algorithm per 1e7 evaluated for linear regression in a restricted experiment.",
            "feasibility_score": "Examples reported: RS density for linear regression ≈ 1 / 10^7; evolution is reported ~5x more efficient than RS in that setting. Final evolved binary-CIFAR algorithm accuracy: 84.06 ± 0.10% (evolved) vs 82.22 ± 0.17% (2-layer NN baseline) and 77.65 ± 0.22% (logistic baseline). Other dataset results: SVHN 88.12% (evolved) vs 85.14% (nonlinear baseline) vs 59.58% (linear), downsampled ImageNet 80.78% vs 78.44% vs 76.44%, Fashion MNIST 98.60% vs 98.21% vs 97.90%.",
            "tradeoff_evidence": "Qualitative evidence: the paper emphasizes a trade-off between reduced human bias / more generic search spaces (higher potential for innovation/novelty) and increased sparsity/harder search (lower feasibility/density of good solutions). This trade-off is described qualitatively (e.g., \"genericity of the AutoML-Zero space makes it more difficult to search\"; solutions can be as rare as 1 in 10^12 in the generic space). No numeric correlation or Pareto analysis between novelty and feasibility is provided (null for quantified tradeoff).",
            "optimization_strategy": "The framework uses (single-objective) evolutionary search (regularized evolution) optimizing task performance (median validation accuracy / mean loss). To make search tractable it employs search-engine optimizations (functional equivalence checking to avoid re-evaluating semantically equivalent programs, hurdles early-stopping based on population percentiles, migration between parallel workers for diversity), and hyperparameter tuning (random search over constants) for final evaluation. No explicit multi-objective novelty-vs-feasibility optimization is implemented (null).",
            "human_evaluation": true,
            "human_evaluation_results": "Humans inspected, simplified, and interpreted evolved programs and ran ablation/knock-in studies. Ablation results for the best evolved algorithm (Section 4.2) showed accuracy drops when removing discovered components: input-noise regularizer (-0.16% absolute), bilinear multiplicative interactions (-1.46%), normalized gradients (-1.20%), and weight averaging (-4.11%). Task-adaptation experiments were repeated and analyzed statistically: noisy-ReLU/dropout-like adaptation appeared preferentially with little data (8/30 experiments vs 0/30 control, p&lt;0.0005); learning-rate decay appeared for fast-training settings (30/30 vs 3/30 control, p&lt;1e-14); transformed-mean-as-learning-rate appeared for multi-class tasks (24/30 vs 0/30 control, p&lt;1e-11). The paper describes manual decoupling of hyperparameters (human intervention) to enable transfer to higher-dimensional data.",
            "comparative_baseline": "Baselines include random search (RS) as a search baseline and hand-designed algorithms for final predictive baselines (logistic regression and 2-layer fully connected neural network trained by gradient descent); related prior methods are cited (learned optimizers, symbolic optimizers, genetic programming).",
            "comparative_results": "Evolution vs RS: for simple linear regression, evolution ~5x more efficient than RS given equal total evaluations; RS density for acceptable linear algorithms ≈ 1/10^7. Final predictive comparisons (binary CIFAR final eval): AutoML-Zero evolved best algorithm accuracy 84.06 ± 0.10% vs nonlinear baseline 82.22 ± 0.17% and linear baseline 77.65 ± 0.22%. Transfer gains reported on SVHN, downsampled ImageNet, and Fashion MNIST as summarized in feasibility_score.",
            "domain_specific_findings": "Domain/task-specific emergent behaviors: (1) With few training examples, evolution produced noisy-ReLU / dropout-like injection of noise as a regularizer. (2) For fast-training scenarios (fewer epochs), evolution discovered learning-rate decay schedules (iterated arctan approximating near-exponential decay). (3) For multi-class tasks, evolved algorithms often used a transformed mean of the weight matrix as a learning-rate signal. These findings indicate automatic adaptation of algorithmic motifs to task constraints.",
            "uuid": "e2415.0",
            "source_info": {
                "paper_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Regularized Evolution",
            "name_full": "Regularized Evolution (a.k.a. aging or tournament-regularized evolutionary algorithm for architecture search)",
            "brief_description": "An evolutionary search algorithm that maintains a population of candidate programs/models, uses tournament selection to pick parents and replaces the oldest population member with a mutated child, emphasizing recency/aging to promote exploration.",
            "citation_title": "Regularized evolution for image classifier architecture search",
            "mention_or_use": "use",
            "system_name": "Regularized Evolution",
            "system_description": "A population-based evolutionary algorithm (used as the core search driver in AutoML-Zero). Each cycle samples T individuals, picks the best as parent, copies and mutates it to create a child, inserts the child and deletes the oldest individual. Mutations are insert/remove instruction, randomize a component function, or modify an instruction argument. The paper uses this method at scale, with many worker processes, migration, and added practical improvements (FEC, hurdles).",
            "research_domain": "machine learning (search/AutoML)",
            "problem_type": "open-ended exploration / optimization in extremely large sparse discrete program spaces",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Feasibility of solutions found by regularized evolution is measured by the same ML metrics (mean validation loss, median accuracy across tasks). Search effectiveness is compared to random search by measuring success rate (density of acceptable programs) given equal total evaluations.",
            "feasibility_score": "Reported empirical improvement: for linear regression example evolution produced ~5x higher success rate than RS given same evaluation budget; in more difficult tasks evolution greatly outperformed RS (Figure 4).",
            "tradeoff_evidence": "Regularized evolution enables discovery of solutions in sparse generic spaces where RS fails; the paper qualitatively reports that evolution is much more effective as task difficulty increases. No formal novelty-feasibility tradeoff analysis is provided for the search algorithm itself.",
            "optimization_strategy": "Single-objective search maximizing validation performance; practical accelerations include FEC to avoid duplicate evals, hurdles early-stopping, parallel workers with migration, and variable tournament/population hyperparameters.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "Compared to Random Search in experiments; also contrasted with other potential search methods in discussion (reinforcement learning, Bayesian optimization) but not used.",
            "comparative_results": "Empirically superior to RS in the AutoML-Zero search space: especially as problem difficulty increases (Figure 4); concrete example: 5x better efficiency for linear regression restricted-experiment.",
            "domain_specific_findings": "Regularized evolution can discover complex algorithmic constructs (e.g., backprop-like code) when the search space permits; effectiveness improves with added techniques (FEC, hurdles, migration).",
            "uuid": "e2415.1",
            "source_info": {
                "paper_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Random Search (RS)",
            "name_full": "Random Search (baseline for AutoML search)",
            "brief_description": "A baseline search strategy that samples random programs from the search space and selects the best by validation performance; used here both as a baseline and to estimate \"density\" of acceptable algorithms.",
            "citation_title": "Random search for hyper-parameter optimization",
            "mention_or_use": "use",
            "system_name": "Random search (RS)",
            "system_description": "Randomly samples programs (component functions/instructions/arguments) from the AutoML-Zero search space (optionally with constraints in restricted experiments) and evaluates them on proxy tasks; used to estimate the sparsity (density) of acceptable solutions and as a baseline to compare evolutionary search.",
            "research_domain": "machine learning (AutoML, hyperparameter/model search)",
            "problem_type": "baseline sampling / density estimation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "RS success rate (density) defined as ratio of acceptable algorithms (those exceeding a hand-designed reference performance) to total evaluated; used as an empirical measure of problem difficulty.",
            "feasibility_score": "Example: RS density for acceptable linear-regression algorithms ≈ 1 in 10^7 (i.e., one acceptable algorithm every 10^7 random samples). For more complex tasks RS failed to find solutions that evolution found.",
            "tradeoff_evidence": "RS demonstrates that the generic search space is sparse: increased genericity (less bias) leads to much lower density of acceptable solutions, indicating a practical trade-off where higher novelty potential costs search feasibility. This is presented qualitatively and via RS density numbers; no quantitative novelty-feasibility frontier is provided.",
            "optimization_strategy": "None beyond uniform random sampling. Used as a baseline; in final hyperparameter tuning RS is used over continuous constants.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "Compared directly to regularized evolution (main paper experiments) and to hand-designed algorithm references.",
            "comparative_results": "Evolution outperforms RS substantially in sparse generic search spaces; RS found acceptable solutions only rarely (e.g., 1/10^7 for a simple linear task) while evolution produced acceptable algorithms much more often for same eval budgets.",
            "domain_specific_findings": "RS can be competitive in densely designed AutoML spaces (cited previous work) but fails in the very generic AutoML-Zero space the paper defines.",
            "uuid": "e2415.2",
            "source_info": {
                "paper_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Functional Equivalence Checking (FEC)",
            "name_full": "Functional Equivalence Checking for supervised ML algorithms (fingerprinting predictions)",
            "brief_description": "A practical caching technique that detects when two algorithms behave identically (on a short set of examples) to avoid redundant full evaluations by hashing truncated predictions into a fingerprint.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Functional Equivalence Checking (FEC)",
            "system_description": "FEC fingerprints a candidate algorithm by running it for 10 training and 10 validation steps on a fixed example set, truncating and hashing the resulting predictions to create a fingerprint. An LRU cache maps fingerprints to previously-computed accuracies; when a new candidate's fingerprint matches, the cached accuracy is reused instead of re-evaluating. This reduces duplicate evaluations of semantically equivalent (or effectively equivalent on the fingerprint set) programs and yielded ~4x speedup in experiments.",
            "research_domain": "machine learning (search/AutoML infrastructure)",
            "problem_type": "search efficiency / duplicate-detection",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Operational: reduces compute cost by reusing evaluations when fingerprints match; measured speedup (~4x) in the paper's infrastructure.",
            "feasibility_score": "Reported ~4x speedup from FEC (section 3.2 / S3); cache size used: 100k fingerprint-accuracy pairs.",
            "tradeoff_evidence": "FEC impacts feasibility (computational cost) positively by saving compute; it does not provide a measure of novelty and does not aim to distinguish novel from duplicate implementations except by identical behavior on the fingerprint examples.",
            "optimization_strategy": "Engineering optimization (memoization) to improve throughput of evolutionary search; complementary to search algorithm rather than an objective tradeoff controller.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "Compared implicitly to naive re-evaluation (no FEC).",
            "comparative_results": "About 4x faster search throughput when FEC is used (paper reports this speedup empirically).",
            "domain_specific_findings": "FEC is a practical mechanism when searching program spaces where many syntactic variants produce the same behavior; it reduces redundant compute but does not measure or encourage semantic novelty beyond the fingerprint test.",
            "uuid": "e2415.3",
            "source_info": {
                "paper_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Genetic Programming (symbolic optimizer)",
            "name_full": "Genetic Programming for discovering symbolic optimizers / learning rules (prior work, e.g., Bengio et al., 1994)",
            "brief_description": "Earlier work that used genetic programming to evolve symbolic expressions (trees) representing local learning rules (operators combining available local signals) to discover learning algorithms.",
            "citation_title": "Use of genetic programming for the search of a new learning rule for neural networks",
            "mention_or_use": "mention",
            "system_name": "Genetic Programming / Symbolic Optimizer (prior work)",
            "system_description": "Referenced prior approaches represent update rules as symbolic trees whose leaves are local signals (inputs, activations, errors) and whose internal nodes are basic arithmetic operations; evolutionary operators evolve tree structures to discover new learning rules. The paper cites Bengio et al. (1994) as an early example that searched for a local learning rule using genetic programming.",
            "research_domain": "machine learning (symbolic optimizer discovery)",
            "problem_type": "automated algorithm discovery / optimizer discovery",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Genetic programming (tree-based representations) evolving symbolic formulas; contrasted in the paper with the sequence-of-instructions representation used in AutoML-Zero.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "The paper positions genetic programming as prior art for symbolically discovering learning rules, but notes that AutoML-Zero differs by searching entire algorithms (not just optimizers) and using a sequence-of-instructions representation permitting richer vector/matrix ops.",
            "uuid": "e2415.4",
            "source_info": {
                "paper_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Regularized evolution for image classifier architecture search",
            "rating": 2
        },
        {
            "paper_title": "Learning to learn by gradient descent by gradient descent",
            "rating": 2
        },
        {
            "paper_title": "Use of genetic programming for the search of a new learning rule for neural networks",
            "rating": 2
        },
        {
            "paper_title": "Neural optimizer search with reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Random search for hyper-parameter optimization",
            "rating": 1
        }
    ],
    "cost": 0.0194145,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</h1>
<p>Esteban Real<em>1 Chen Liang ${ }^{</em> 1}$ David R. So ${ }^{1}$ Quoc V. Le ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expertdesigned layers as building blocks-or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.</p>
<h2>1. Introduction</h2>
<p>In recent years, neural networks have reached remarkable performance on key tasks and seen a fast increase in their popularity [e.g. He et al., 2015; Silver et al., 2016; Wu et al., 2016]. This success was only possible due to decades of machine learning (ML) research into many aspects of the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>field, ranging from learning strategies to new architectures [Rumelhart et al., 1986; LeCun et al., 1995; Hochreiter \&amp; Schmidhuber, 1997, among many others]. The length and difficulty of ML research prompted a new field, named AutoML, that aims to automate such progress by spending machine compute time instead of human research time (Fahlman \&amp; Lebiere, 1990; Hutter et al., 2011; Finn et al., 2017). This endeavor has been fruitful but, so far, modern studies have only employed constrained search spaces heavily reliant on human design. A common example is architecture search, which typically constrains the space by only employing sophisticated expert-designed layers as building blocks and by respecting the rules of backpropagation (Zoph \&amp; Le, 2016; Real et al., 2017; Tan et al., 2019). Other AutoML studies similarly have found ways to constrain their search spaces to isolated algorithmic aspects, such as the learning rule used during backpropagation (Andrychowicz et al., 2016; Ravi \&amp; Larochelle, 2017), the data augmentation (Cubuk et al., 2019a; Park et al., 2019) or the intrinsic curiosity reward in reinforcement learning (Alet et al., 2019); in these works, all other algorithmic aspects remain hand-designed. This approach may save compute time but has two drawbacks. First, human-designed components bias the search results in favor of human-designed algorithms, possibly reducing the innovation potential of AutoML. Innovation is also limited by having fewer options (Elsken et al., 2019b). Indeed, dominant aspects of performance are often left out (Yang et al., 2020). Second, constrained search spaces need to be carefully composed (Zoph et al., 2018; So et al., 2019; Negrinho et al., 2019), thus creating a new burden on researchers and undermining the purported objective of saving their time.</p>
<p>To address this, we propose to automatically search for whole ML algorithms using little restriction on form and only simple mathematical operations as building blocks. We call this approach AutoML-Zero, following the spirit of previous work which aims to learn with minimal human participation [e.g. Silver et al., 2017]. In other words, AutoML-Zero aims to search a fine-grained space simultaneously for the model, optimization procedure, initialization, and so on, permitting much less human-design and even allowing the discovery of non-neural network algorithms. To demonstrate that this is possible today, we present an initial solution to this challenge that creates algorithms competitive</p>
<p>with backpropagation-trained neural networks.
The genericity of the AutoML-Zero space makes it more difficult to search than existing AutoML counterparts. Existing AutoML search spaces have been constructed to be dense with good solutions, thus deemphasizing the search method itself. For example, comparisons on the same space found that advanced techniques are often only marginally superior to simple random search (RS) (Li \&amp; Talwalkar, 2019; Elsken et al., 2019b; Negrinho et al., 2019). AutoML-Zero is different: the space is so generic that it ends up being quite sparse. The framework we propose represents ML algorithms as computer programs comprised of three component functions, Setup, Predict, and Learn, that performs initialization, prediction and learning. The instructions in these functions apply basic mathematical operations on a small memory. The operation and memory addresses used by each instruction are free parameters in the search space, as is the size of the component functions. While this reduces expert design, the consequent sparsity means that RS cannot make enough progress; e.g. good algorithms to learn even a trivial task can be as rare as 1 in $10^{12}$. To overcome this difficulty, we use small proxy tasks and migration techniques to build highly-optimized open-source infrastructure capable of searching through 10,000 models/second/cpu core. In particular, we present a variant of functional equivalence checking that applies to ML algorithms. It prevents re-evaluating algorithms that have already been seen, even if they have different implementations, and results in a 4 x speedup. More importantly, for better efficiency, we move away from RS. ${ }^{1}$</p>
<p>Perhaps surprisingly, evolutionary methods can find solutions in the AutoML-Zero search space despite its enormous size and sparsity. By randomly modifying the programs and periodically selecting the best performing ones on given tasks/datasets, we discover reasonable algorithms. We will first show that starting from empty programs and using data labeled by "teacher" neural networks with random weights, evolution can discover neural networks trained by gradient descent (Section 4.1). Next, we will minimize bias toward known algorithms by switching to binary classification tasks extracted from CIFAR-10 and allowing a larger set of possible operations. The result is evolved models that surpass the performance of a neural network trained with gradient descent by discovering interesting techniques like multiplicative interactions, normalized gradient and weight averaging (Section 4.2). Having shown that these ML algorithms are attainable from scratch, we will finally demonstrate that it is also possible to improve an existing algorithm by initializing the population with it. This way, evolution adapts the algorithm to the type of task provided. For example,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>dropout-like operations emerge when the task needs regularization and learning rate decay appears when the task requires faster convergence (Section 4.3). Additionally, we present ablation studies dissecting our method (Section 5) and baselines at various compute scales for comparisons by future work (Suppl. Section S10).</p>
<p>In summary, our contributions are:</p>
<ul>
<li>AutoML-Zero, the proposal to automatically search for ML algorithms from scratch with minimal human design;</li>
<li>A novel framework with open-sourced code ${ }^{1}$ and a search space that combines only basic mathematical operations;</li>
<li>Detailed results to show potential through the discovery of nuanced ML algorithms using evolutionary search.</li>
</ul>
<h2>2. Related Work</h2>
<p>AutoML has utilized a wide array of paradigms, including growing networks neuron-by-neuron (Stanley \&amp; Miikkulainen, 2002), hyperparameter optimization (Snoek et al., 2012; Loshchilov \&amp; Hutter, 2016; Jaderberg et al., 2017) and, neural architecture search (Zoph \&amp; Le, 2016; Real et al., 2017). As discussed in Section 1, AutoML has targeted many aspects of neural networks individually, using sophisticated coarse-grained building blocks. Mei et al. (2020), on the other hand, perform a fine-grained search over the convolutions of a neural network. Orthogonally, a few studies benefit from extending the search space to two such aspects simultaneously (Zela et al., 2018; Miikkulainen et al., 2019; Noy et al., 2019). In our work, we perform a fine-grained search over all aspects of the algorithm.</p>
<p>An important aspect of an ML algorithm is the optimization of its weights, which has been tackled by AutoML in the form of numerically discovered optimizers (Chalmers, 1991; Andrychowicz et al., 2016; Vanschoren, 2019). The output of these methods is a set of coefficients or a neural network that works well but is hard to interpret. These methods are sometimes described as "learning the learning algorithm". However, in our work, we understand algorithm more broadly, including the structure and initialization of the model, not just the optimizer. Additionally, our algorithm is not discovered numerically but symbolically. A symbolically discovered optimizer, like an equation or a computer program, can be easier to interpret or transfer. An early example of a symbolically discovered optimizer is that of Bengio et al. (1994), who optimize a local learning rule for a 4-neuron neural network using genetic programming (Holland, 1975; Forsyth et al., 1981; Koza \&amp; Koza, 1992). Our search method is similar but represents the program as a sequence of instructions. While they use the basic operations ${+,-, \times, \div}$, we allow many more, taking advantage of dense hardware computations. Risi \&amp; Stanley (2010) tackle the discovery of a biologically informed neural network learning rule too, but with a very different encoding.</p>
<p>More recently, Bello et al. (2017) also search for a symbolic optimizer, but in a restricted search space of hand-tuned operations (e.g. "apply dropout with $30 \%$ probability", "clip at 0.00001 ", etc.). Our search space, on the other hand, aims to minimize restrictions and manual design. Unlike these three studies, we do not even assume the existence of a neural network or of gradients.</p>
<p>We note that our work also relates to program synthesis efforts. Early approaches have proposed to search for programs that improve themselves (Lenat, 1983; Schmidhuber, 1987). We share similar goals in searching for learning algorithms, but focus on common machine learning tasks and have dropped the self-reflexivity requirement. More recently, program synthesis has focused on solving problems like sorting (Graves et al., 2014), string manipulation (Gulwani et al., 2017; Balog et al., 2017), or structured data QA (Liang et al., 2016). Unlike these studies, we focus on synthesizing programs that solve the problem of doing ML.</p>
<p>Suppl. Section S1 contains additional related work.</p>
<h2>3. Methods</h2>
<p>AutoML-Zero concerns the automatic discovery of algorithms that perform well on a given set of ML tasks $\mathcal{T}$. First, search experiments explore a very large space of algorithms $\mathcal{A}$ for an optimal and generalizable $a^{*} \in \mathcal{A}$. The quality of the algorithms is measured on a subset $\mathcal{T}<em _select="{select" _text="\text">{\text {search }} \subset \mathcal{T}$, with each search experiment producing a candidate algorithm. In this work, we apply random search as a baseline and evolutionary search as the main search method due to their simplicity and scalability. Once the search experiments are done, we select the best candidate by measuring their performances on another subset of tasks $\mathcal{T}</em>}} \subset \mathcal{T}$ (analogous to standard ML model selection with a validation set). Unless otherwise stated, we use binary classification tasks extracted from CIFAR-10, a collection of tiny images each labeled with object classes (Krizhevsky \&amp; Hinton, 2009), and we calculate the average accuracy across a set of tasks to measure the quality of each algorithm. To lower compute costs and achieve higher throughput, we create small proxy tasks for $\mathcal{T<em _select="{select" _text="\text">{\text {search }}$ and $\mathcal{T}</em>$ by using one random matrix for each task to project the input features to lower dimensionality. The projected dimensionality is $8 \leq F \leq 256$. Finally, we compare the best algorithm's performance against handdesigned baselines on the CIFAR-10 data in the original dimensionality (3072), holding out the CIFAR-10 test set for the final evaluation. To make sure the improvement is not specific to CIFAR-10, we further show the gain generalizes to other datasets: SVHN (Netzer et al., 2011), ImageNet (Chrabaszcz et al., 2017), and Fashion MNIST (Xiao et al., 2017). The Experiment Details paragraphs in Section 4 contain the specifics of the tasks. We now describe the search space and search method with sufficient detail to
understand the results. For reproducibility, we provide the minutiae in the Supplement and the open-sourced code.}</p>
<h3>3.1. Search Space</h3>
<p>We represent algorithms as computer programs that act on a small virtual memory with separate address spaces for scalar, vector and matrix variables (e.g. s1, v1, m1), all of which are floating-point and share the dimensionality of the task's input features $(F)$. Programs are sequences of instructions. Each instruction has an operation-or $o p$ that determines its function (e.g. "multiply a scalar with a vector"). To avoid biasing the choice of ops, we use a simple criterion: those that are typically learned by high-school level. We purposefully exclude machine learning concepts, matrix decompositions, and derivatives. Instructions have op-specific arguments too. These are typically addresses in the memory (e.g. "read the inputs from scalar address 0 and vector address 3; write the output to vector address 2"). Some ops also require real-valued constants (e.g. $\mu$ and $\sigma$ for a random Gaussian sampling op), which are searched for as well. Suppl. Section S2 contains the full list of 65 ops.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># (Setup, Predict, Learn) = input ML algorithm.</span>
<span class="c1"># Dtrain / Dvalid = training / validation set.</span>
<span class="c1"># sX/vX/mX: scalar/vector/matrix var at address X.</span>
<span class="n">def</span><span class="w"> </span><span class="n">Evaluate</span><span class="p">(</span><span class="n">Setup</span><span class="p">,</span><span class="w"> </span><span class="n">Predict</span><span class="p">,</span><span class="w"> </span><span class="n">Learn</span><span class="p">,</span><span class="w"> </span><span class="n">Dtrain</span><span class="p">,</span>
<span class="n">Dvalid</span><span class="p">):</span>
<span class="c1"># Zero-initialize all the variables (sX/vX/mX).</span>
<span class="w">    </span><span class="n">initialize_memory</span><span class="p">()</span>
<span class="w">    </span><span class="n">Setup</span><span class="p">()</span><span class="w"> </span><span class="c1"># Execute setup instructions.</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">Dtrain</span><span class="p">:</span>
<span class="w">        </span><span class="n">v0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="c1"># x will now be accessible to Predict.</span>
<span class="w">        </span><span class="n">Predict</span><span class="p">()</span><span class="w"> </span><span class="c1"># Execute prediction instructions.</span>
<span class="w">        </span><span class="c1"># s1 will now be used as the prediction.</span>
<span class="w">        </span><span class="n">s1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Normalize</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span><span class="w"> </span><span class="c1"># Normalize the prediction.</span>
<span class="w">        </span><span class="n">s0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="c1"># y will now be accessible to Learn.</span>
<span class="w">        </span><span class="n">Learn</span><span class="p">()</span><span class="w"> </span><span class="c1"># Execute learning instructions.</span>
<span class="w">    </span><span class="n">sum_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">Dvalid</span><span class="p">:</span>
<span class="w">        </span><span class="n">v0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span>
<span class="w">        </span><span class="n">Predict</span><span class="p">()</span><span class="w"> </span><span class="c1"># Only Predict(), not Learn().</span>
<span class="w">        </span><span class="n">s1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Normalize</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
<span class="w">        </span><span class="n">sum_loss</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">Loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">s1</span><span class="p">)</span>
<span class="w">    </span><span class="n">mean_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">Dvalid</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># Use validation loss to evaluate the algorithm.</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">mean_loss</span>
</code></pre></div>

<p>Figure 1: Algorithm evaluation on one task. We represent an algorithm as a program with three component functions (Setup, Predict, Learn). These are evaluated by the pseudo-code above, producing a mean loss for each task. The search method then uses the median across tasks as an indication of the algorithm's quality.</p>
<p>Inspired by supervised learning work, we represent an algorithm as a program with three component functions that we call Setup, Predict, and Learn (e.g. Figure 5). The algorithm is evaluated as in Fig 1. There, the two for-loops implement the training and validation phases, processing</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: One cycle of the evolutionary method (Goldberg \&amp; Deb, 1991; Real et al., 2019). A population of $P$ algorithms (here, $P=5$; laid out from left to right in the order they were discovered) undergoes many cycles like this one. First, we remove the oldest algorithm (step 1). Then, we choose a random subset of size $T$ (here, $T=3$ ) and select the best of them (step 2). The best is copied (step 3) and mutated (step 4).
the task's examples one-at-a-time for simplicity. The training phase alternates Predict and Learn executions. Note that Predict just takes in the features of an example (i.e. x )-its label (i.e. y ) is only seen by Learn afterward.</p>
<p>Then, the validation loop executes Predict over the validation examples. After each Predict execution, whatever value is in scalar address 1 (i.e. s1) is considered the prediction-Predict has no restrictions on what it can write there. For classification tasks, this prediction in $(-\infty, \infty)$ is normalized to a probability in $(0,1)$ through a sigmoid (binary classification) or a softmax (multi-class). This is implemented as the s1 = Normalize(s1) instruction. The virtual memory is zero-initialized and persistent, and shared globally throughout the whole evaluation. This way, Setup can initialize memory variables (e.g. the weights), Learn can adjust them during training, and Predict can use them. This procedure yields an accuracy for each task. The median across $D$ tasks is used as a measure of the algorithm's quality by the search method.</p>
<h3>3.2. Search Method</h3>
<p>Search experiments must discover algorithms by modifying the instructions in the component functions (Setup, Predict, and Learn; e.g. Figure 5). Unless otherwise
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Mutation examples. Parent algorithm is on the left; child on the right. (i) Insert a random instruction (removal also possible). (ii) Randomize a component function. (iii) Modify an argument.
stated, we use the regularized evolution search method because of its simplicity and recent success on architecture search benchmarks (Real et al., 2019; Ying et al., 2019; So et al., 2019). This method is illustrated in Figure 2. It keeps a population of $P$ algorithms, all initially empty-i.e. none of the three component functions has any instructions/code lines. The population is then improved through cycles. Each cycle picks $T&lt;P$ algorithms at random and selects the best performing one as the parent, i.e. tournament selection (Goldberg \&amp; Deb, 1991). This parent is then copied and mutated to produce a child algorithm that is added to the population, while the oldest algorithm in the population is removed. The mutations that produce the child from the parent must be tailored to the search space; we use a random choice among three types of actions: (i) insert a random instruction or remove an instruction at a random location in a component function, (ii) randomize all the instructions in a component function, or (iii) modify one of the arguments of an instruction by replacing it with a random choice (e.g. "swap the output address" or "change the value of a constant"). These are illustrated in Figure 3.</p>
<p>In order to reach a throughput of $2 \mathrm{k}-10 \mathrm{k}$ algorithms $/ \mathrm{sec}-$ $\mathrm{ond} / \mathrm{cpu}$ core, besides the use of small proxy tasks, we apply two additional upgrades: (1) We introduce a version of functional equivalence checking (FEC) that detects equivalent supervised ML algorithms, even if they have different implementations, achieving a 4 x speedup. To do this, we record</p>
<p>the predictions of an algorithm after executing 10 training and 10 validation steps on a fixed set of examples. These are then truncated and hashed into a fingerprint for the algorithm to detect duplicates in the population and reuse previous evaluation scores. (2) We add hurdles (So et al., 2019) to reach further 5x throughput. In addition to (1) and (2), to attain higher speeds through parallelism, we distribute experiments across worker processes that exchange models through migration (Alba \&amp; Tomassini, 2002); each process has its own P-sized population and runs on a commodity CPU core. We denote the number of processes by $W$. Typically, $100&lt;W&lt;1000$ (we indicate the exact numbers with each experiment ${ }^{2}$ ). Workers periodically upload randomly selected algorithms to a central server. The server replies with algorithms randomly sampled across all workers, replacing half the local population (i.e. random migration). To additionally improve the quality of the search, we allow some workers to search on projected binary MNIST tasks, in addition to projected binary CIFAR-10, to promote diversity (see e.g. (Wang et al., 2019)). More details about these techniques can be found in Suppl. Section S3. Section 5 and Suppl. Section S9 contain ablation studies showing that all these techniques are beneficial.</p>
<p>For each experimental result, we include an Experiment Details paragraph with the exact values for meta-parameters like $P$ and $T$. None of the meta-parameters were tuned in the final set of experiments at full compute scale. Most of them were either decided in smaller experiments (e.g. $P$ ), taken from previous work (e.g. $T$ ), or simply not tuned at all. In some cases, when uncertain about a parameter's appropriate value, we used a range of values instead (e.g. " $100 \leq P \leq 1000$ "); different worker processes within the experiment use different values within the range.
$\qquad$ Details: Generally, we use $T=10,100 \leq P \leq 1000$. Each child algorithm is mutated with probability $U=0.9$. Run time: 5 days. Migration rate adjusted so that each worker process has fewer than 1 migration/s and at least 100 migrations throughout the expt. Specifics for each expt. in Suppl. Section S5. Suppl. Section S3 describes additional more general methods minutiae.</p>
<h2>4. Results</h2>
<p>In the next three sections, we will perform experiments to answer the following three questions, respectively: "how difficult is searching the AutoML-Zero space?", "can we use our framework to discover reasonable algorithms with minimal human input?", and "can we discover different algorithms by varying the type of task we use during the search experiment?"</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.1. Finding Simple Neural Nets in a Difficult Space</h3>
<p>We now demonstrate the difficulty of the search space through random search (RS) experiments and we show that, nonetheless, interesting algorithms can be found, especially with evolutionary search. We will explore the benefits of evolution as we vary the task difficulty. We start by searching for algorithms to solve relatively easy problems, such as fitting linear regression data. Note that without the following simplifications, RS would not be able to find solutions.</p>
<p>Experiment Details: we generate simple regression tasks with 1000 training and 100 validation examples with random 8dim. feature vectors $\left{x_{i}\right}$ and scalar labels ${L\left(x_{i}\right)} . L$ is fixed for each task but varies between them. To get affine tasks, $L\left(x_{i}\right)=$ $w x_{i}+a$, where $u$ and $a$ are a random vector and scalar. For linear tasks, $a=0$. All random numbers were Gaussian $(\mu=0, \sigma=1)$. Evaluations use RMS error and the Normalize() instruction in Figure 1 is the identity. We restrict the search space by only using necessary ops and fixing component function lengths to those of known solutions. E.g., for a linear dataset, Learn has 4 instructions because linear SGD requires 4 instructions. To keep lengths fixed, insert/remove-instruction mutations are not allowed and component functions are initialized randomly. RS generates programs where all instructions are random (see Section 3.2) and selects the best at the end. Evolution expts. are small ( $W=1 ; D=$ 3; 10k algs./expt.); We repeat expts. until statistical significance is achieved. Full configs. in Suppl. Section S5. Note that the restrictions above apply <em>only</em> to this section (4.1).</p>
<p>We quantify a task's difficulty by running a long RS experiment. We count the number of acceptable algorithms, i.e. those with lower mean RMS error than a hand-designed reference (e.g. linear regressor or neural network). The ratio of acceptable algorithms to the total number of algorithms evaluated gives us an RS success rate. It can also be interpreted as an estimate of the "density of acceptable algorithms" in the search space. We use this density as a measure of problem difficulty. For example, in the linear regression case, we looked for all algorithms that do better than a linear regressor with gradient descent. Even in this trivial task type, we found only 1 acceptable algorithm every $10^{7}$, so we define $10^{7}$ to be the difficulty of the linear regression task. We then run the evolution experiments with the same combined total number of evaluations as for RS. We measure the ratio of acceptable algorithms to the total number of algorithms evaluated, to get an evolution success rate. However, we only count at most 1 acceptable algorithm from each experiment; this biases the results against evolution but is necessary because a single experiment may yield multiple copies of a single acceptable algorithm. Even in the simple case of linear regression, we find that evolution is 5 times more efficient than RS. This stands in contrast to many previous AutoML studies, where the solutions are dense enough that RS can be competitive (Section 1).</p>
<p>Figure 4 summarizes the result of this analysis for 4 task types: the discovery of a full-algorithm/only-the-learning</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Relative success rate of evolution and random search (RS). Each point represents a different task type and the x-axis measures its difficulty (defined in the main text). As the task type becomes more difficult, evolution vastly outperforms RS, illustrating the complexity of AutoML-Zero when compared to more traditional AutoML spaces.
for linear/affine regression data. The AutoML-Zero search space is generic but this comes at a cost: even for easy problems, good algorithms are sparse. As the problem becomes more difficult, the solutions become vastly more sparse and evolution greatly outperforms RS.</p>
<p>As soon as we advance to nonlinear data, the gap widens and we can no longer find solutions with RS. To make sure a good solution exists, we generate regression tasks using teacher neural networks and then verify that evolution can rediscover the teacher's code.</p>
<p>Experiment Details: tasks as above but the labeling function is now a teacher network: $L\left(x_{i}\right)=u \cdot \operatorname{ReLU}\left(M x_{i}\right)$, where $M$ is a random $8 \times 8$ matrix, $u$ is a random vector. Number of training examples up to 100k. Single expt. Same search space restrictions as above, but now allowing ops used in 2-layer fully connected neural nets. After searching, we select the algorithm with the smallest RMS loss. Full configs. in Suppl. Section S5. Note that the restrictions above apply <em>only</em> to this section (4.1).</p>
<p>When the search method uses only 1 task in $\mathcal{T}<em _search="{search" _text="\text">{\text {search }}$ (i.e. $D=1$ ), the algorithm evolves the exact prediction function used by the teacher and hard-codes its weights. The results become more surprising as we increase the number of tasks in $\mathcal{T}</em>$ (e.g. to $D=100$ ), as now the algorithm must find different weights for each task. In this case, evolution not only discovers the forward pass, but also "invents" backpropagation code to learn the weights (Figure 5). Despite its difficulty, we conclude that searching the AutoML-Zero space seems feasible and we should use evolutionary search instead of RS for more complex tasks.}</p>
<h3>4.2. Searching with Minimal Human Input</h3>
<p>Teacher datasets and carefully chosen ops bias the results in favor of known algorithms, so in this section we replace them with more generic options. We now search among a</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># sX/vX/mX = scalar/vector/matrix at address X.</span>
<span class="c1"># &quot;gaussian&quot; produces Gaussian IID random numbers.</span>
<span class="n">def</span><span class="w"> </span><span class="n">Setup</span><span class="p">():</span>
<span class="w">    </span><span class="c1"># Initialize variables.</span>
<span class="w">    </span><span class="n">m1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gaussian</span><span class="p">(</span><span class="o">-</span><span class="mf">1e-10</span><span class="p">,</span><span class="w"> </span><span class="mf">9e-09</span><span class="p">)</span><span class="w"> </span><span class="c1"># 1st layer weights</span>
<span class="w">    </span><span class="n">s3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">4.1</span><span class="w"> </span><span class="c1"># Set learning rate</span>
<span class="w">    </span><span class="n">v4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gaussian</span><span class="p">(</span><span class="o">-</span><span class="mf">0.033</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">)</span><span class="w"> </span><span class="c1"># 2nd layer weights</span>
<span class="n">def</span><span class="w"> </span><span class="n">Predict</span><span class="p">():</span><span class="w"> </span><span class="c1"># v0=features</span>
<span class="w">    </span><span class="n">v6</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span><span class="w"> </span><span class="n">v0</span><span class="p">)</span><span class="w"> </span><span class="c1"># Apply 1st layer weights</span>
<span class="w">    </span><span class="n">v7</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">v6</span><span class="p">)</span><span class="w"> </span><span class="c1"># Apply ReLU</span>
<span class="w">    </span><span class="n">s1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dot</span><span class="p">(</span><span class="n">v7</span><span class="p">,</span><span class="w"> </span><span class="n">v4</span><span class="p">)</span><span class="w"> </span><span class="c1"># Compute prediction</span>
<span class="n">def</span><span class="w"> </span><span class="n">Learn</span><span class="p">():</span><span class="w"> </span><span class="c1"># s0=label</span>
<span class="w">    </span><span class="n">v3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">heaviside</span><span class="p">(</span><span class="n">v6</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="w"> </span><span class="c1"># ReLU gradient</span>
<span class="w">    </span><span class="n">s1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s0</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">s1</span><span class="w"> </span><span class="c1"># Compute error</span>
<span class="w">    </span><span class="n">s2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">s3</span><span class="w"> </span><span class="c1"># Scale by learning rate</span>
<span class="w">    </span><span class="n">v2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">v3</span><span class="w"> </span><span class="c1"># Approx. 2nd layer weight delta</span>
<span class="w">    </span><span class="n">v3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">v2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="c1"># Gradient w.r.t. activations</span>
<span class="w">    </span><span class="n">m0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outer</span><span class="p">(</span><span class="n">v3</span><span class="p">,</span><span class="w"> </span><span class="n">v0</span><span class="p">)</span><span class="w"> </span><span class="c1"># 1st layer weight delta</span>
<span class="w">    </span><span class="n">m1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">m1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">m0</span><span class="w"> </span><span class="c1"># Update 1st layer weights</span>
<span class="w">    </span><span class="n">v4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">v2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">v4</span><span class="w"> </span><span class="c1"># Update 2nd layer weights</span>
</code></pre></div>

<p>Figure 5: Rediscovered neural network algorithm. It implements backpropagation by gradient descent. Comments added manually.
long list of ops selected based on the simplicity criterion described in Section 3.1. The increase in ops makes the search more difficult but allows the discovery of solutions other than neural networks. For more realistic datasets, we use binary classification tasks extracted from CIFAR-10 and MNIST.</p>
<p>Experiment Details: We extract tasks from the CIFAR-10 and MNIST training sets; each of the datasets are searched on by half of the processes. For both datasets, the 45 pairs of the 10 classes yield tasks with 8000 train / 2000 valid examples. 36 pairs are randomly selected to constitute $\mathcal{T}<em _select="{select" _text="\text">{\text {search }}$, i.e. search tasks; 9 pairs are held out for $\mathcal{T}</em>$, i.e tasks for model selection. The CIFAR-10 test set is reserved for final evaluation to report results. Features are projected to $8 \leq F \leq 256$ dim. Each evaluation is on $1 \leq D \leq 10$ tasks. $W=10 k$. From now on, we use the full setup described in Section 3.2. In particular, we allow variable component function length. Number of possible ops: T/58/58 for Setup/Predict/Learn, resp. Full config. in Suppl. Section S5...}</p>
<p>Figure 6 shows the progress of an experiment. It starts with a population of empty programs and automatically invents improvements, several of which are highlighted in the plot. These intermediate discoveries are stepping stones available to evolution and they explain why evolution outperforms RS in this space. Each experiment produces a candidate algorithm using $\mathcal{T}<em _select="{select" _text="\text">{\text {search }}$. We then evaluate these algorithms on unseen pairs of classes $\left(\mathcal{T}</em>\right)$ and compare the results to a hand-designed reference, a 2-layer fully connected neural network trained by gradient descent. The candidate algorithms perform better in 13 out of 20 experiments. To make sure the improvement is not specific to the small proxy tasks, we select the best algorithm for a final evaluation on binary classification with the original CIFAR-10 data.}</p>
<p>Since we are evaluating on tasks with different dimensional-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Progress of one evolution experiment on projected binary CIFAR-10. Callouts indicate some beneficial discoveries. We also print the code for the initial, an intermediate, and the final algorithm. The last is explained in the flow diagram. It outperforms a simple fully connected neural network on held-out test data and transfers to features 10x its size. Code notation is the same as in Figure 5. The x -axis gap is due to infrequent recording due to disk throughput limitations.
ity in the final evaluation, we treat all the constants in the best evolved algorithm as hyperparameters and tune them jointly through RS using the validation set. For comparison, we tune two hand-designed baselines, one linear and one nonlinear, using the same total compute that went into discovering and tuning the evolved algorithm. We finally evaluate them all on unseen CIFAR-10 test data. Evaluating with 5 different random seeds, the best evolved algorithm's accuracy ( $84.06 \pm 0.10 \%$ ) significantly outperforms the linear baseline (logistic regression, $77.65 \pm 0.22 \%$ ) and the nonlinear baseline (2-layer fully connected neural network, $82.22 \pm 0.17 \%$ ). This gain also generalizes to binary classification tasks extracted from other datasets: SVHN (Netzer et al., 2011) ( $88.12 \%$ for the best evolved algorithm $v s$. $59.58 \%$ for the linear baseline vs. $85.14 \%$ for the nonlinear baseline), downsampled ImageNet (Chrabaszcz et al., 2017) ( $80.78 \%$ vs. $76.44 \%$ vs. $78.44 \%$ ), Fashion MNIST (Xiao et al., 2017) ( $98.60 \%$ vs. $97.90 \%$ vs. $98.21 \%$ ). This algorithm is limited by our simple search space, which cannot currently represent some techniques that are crucial in state-of-the-art models, like batch normalization or convolution. Nevertheless, the algorithm shows interesting characteris- tics, which we describe below.</p>
<p>As a case study, we delve into the best algorithm, shown in Figure 6. The code has been cleaned for readability; we removed and rearranged instructions when this caused no difference in performance (raw code in Suppl. Section S6). The algorithm has the following notable features, whose usefulness we verified through ablations (more details in Suppl. Section S8): (1) Noise is added to the input, which, we suspect, acts as a regularizer:</p>
<p>$$
\mathbf{a}=\mathbf{x}+\mathbf{u} ; \mathbf{b}=\mathbf{x}-\mathbf{u} ; \mathbf{u} \sim \mathbf{U}(\alpha, \beta)
$$</p>
<p>where $\mathbf{x}$ is the input, $\mathbf{u}$ is a random vector drawn from a uniform distribution. (2) Multiplicative interactions (Jayakumar et al., 2020) emerge in a bilinear form: $\mathbf{o}=\mathbf{a}^{\top} \mathbf{W b}$, where $\mathbf{o}$ is the output, and $\mathbf{W}$ is the weight matrix. (3) The gradient $\mathbf{g}$ w.r.t. the weight matrix $\mathbf{W}$ is computed correctly and is then normalized to be a unit vector:</p>
<p>$$
\mathbf{g}_{\mathbf{w}}=\frac{\mathbf{g}}{|\mathbf{g}|} ; \mathbf{g}=\delta \mathbf{a b}^{\top} ; \delta=\mathbf{y}^{*}-\mathbf{y}
$$</p>
<p>where $\delta$ is the error, $y$ is the predicted probability, and $y^{*}$ is the label. Normalizing gradients is a common heuris-</p>
<p>tic in non-convex optimization (Hazan et al., 2015; Levy, 2016). (4) The weight matrix $\mathbf{W}^{\prime}$ used during inference is the accumulation of all the weight matrices $\left{\mathbf{W}<em t="t">{\mathbf{t}}\right}$ after each training step $\mathbf{t}$, i.e.: $\mathbf{W}^{\prime}=\sum</em>} \mathbf{W<em _mathbf_t="\mathbf{t">{\mathbf{t}}$. This is reminiscent of the averaged perceptron (Collins, 2002) and neural network weight averaging during training (Polyak \&amp; Juditsky, 1992; Goodfellow et al., 2016). Unlike these studies, the evolved algorithm accumulates instead of averaging, but this difference has no effect when measuring the accuracy of classification tasks (it does not change the prediction). As in those techniques, different weights are used at training and validation time. The evolved algorithm achieves this by setting the weights $\mathbf{W}$ equal to $\mathbf{W}^{\prime}$ at the end of the Predict component function and resetting them to $\mathbf{W}</em>$.}}$ right after that, at the beginning of the Learn component function. This has no effect during training, when Predict and Learn alternate in execution. However, during validation, Learn is never called and Predict is executed repeatedly, causing $\mathbf{W}$ to remain as $\mathbf{W}^{\prime</p>
<p>In conclusion, even though the task used during search is simple, the results show that our framework can discover commonly used algorithms from scratch.</p>
<h3>4.3. Discovering Algorithm Adaptations</h3>
<p>In this section, we will show wider applicability by searching on three different task types. Each task type will impose its own challenge (e.g. "too little data"). We will show that evolution specifically adapts the algorithms to meet the challenges. Since we already reached reasonable models from scratch above, now we save time by simply initializing the populations with the working neural network of Figure 5.</p>
<p>Experiment Details: The basic expt. configuration and datasets (binary CIFAR-10) are as in Section 4.2, with the following exceptions: $W=1 k ; F=16 ; 10 \leq D \leq 100$; critical
alterations to the data are explained in each task type below. Full
configs. in Suppl. Section S5.
Few training examples. We use only 80 of the training examples and repeat them for 100 epochs. Under these conditions, algorithms evolve an adaptation that augments the data through the injection of noise (Figure 7a). This is referred to in the literature as a noisy ReLU (Nair \&amp; Hinton, 2010; Bengio et al., 2013) and is reminiscent of Dropout (Srivastava et al., 2014). Was this adaptation a result of the small number of examples or did we simply get lucky? To answer this, we perform 30 repeats of this experiment and of a control experiment. The control has 800 examples/100 epochs. We find that the noisy ReLU is reproducible and arises preferentially in the case of little data (expt: $8 / 30$, control: $0 / 30, p&lt;0.0005$ ).</p>
<p>Fast training. Training on 800 examples/10 epochs leads to the repeated emergence of learning-rate decay, a wellknown strategy for the timely training of an ML model (Bengio, 2012). An example can be seen in Figure 7b. As a control, we increase the number of epochs to 100 . With overwhelming confidence, the decay appears much more often in the cases with fewer training steps (expt: $30 / 30$, control: $3 / 30, p&lt;10^{-14}$ ).</p>
<p>Multiple classes. When we use all 10 classes of the CIFAR10 dataset, evolved algorithms tend to use the transformed mean of the weight matrix as the learning rate (Figure 7c). (Note that to support multiple classes, labels and outputs are now vectors, not scalars.) While we do not know the reason, the preference is statistically significant (expt: $24 / 30$, control: $0 / 30, p&lt;10^{-11}$ ).</p>
<p>Altogether, these experiments show that the resulting algorithms seem to adapt well to the different types of tasks.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Adaptations to different task types. (a) When few examples are available, evolution creates a noisy ReLU. (b) When fast training is needed, we get a learning rate decay schedule implemented as an iterated arctan map (top) that is nearly exponential (bottom). (c) With multiple classes, the mean of the weight matrix is transformed and then used as the learning rate. Same notation as in Figure 5; full algorithms in Suppl. Section S6.</p>
<h2>5. Conclusion and Discussion</h2>
<p>In this paper, we proposed an ambitious goal for AutoML: the automatic discovery of whole ML algorithms from basic operations with minimal restrictions on form. The objective was to reduce human bias in the search space, in the hope that this will eventually lead to new ML concepts. As a start, we demonstrated the potential of this research direction by constructing a novel framework that represents an ML algorithm as a computer program comprised of three component functions (Setup, Predict, Learn). Starting from empty component functions and using only basic mathematical operations, we evolved neural networks, gradient descent, multiplicative interactions, weight averaging, normalized gradients, and the like. These results are promising, but there is still much work to be done. In the remainder of this section, we motivate future work with concrete observations.</p>
<p>The search method was not the focus of this study but to reach our results, it helped to (1) add parallelism through migration, (2) use FEC, (3) increase diversity, and (4) apply hurdles, as we detailed in Section 3.2. The effects can be seen in Figure 8. Suppl. Section S9 shows that these improvements work across compute scales (today's highcompute regime is likely to be tomorrow's low-compute regime, so ideas that do not scale with compute will be shorter-lived). Preliminary implementations of crossover and geographic structure did not help in our experiments. The silver lining is that the AutoML-Zero search space provides ample room for algorithms to distinguish themselves (e.g. Section 4.1), allowing future work to attempt more sophisticated evolutionary approaches, reinforcement learning, Bayesian optimization, and other methods that have helped AutoML before.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Search method ablation study. From left to right, each column adds an upgrade, as described in the main text.</p>
<p>Evaluating evolved algorithms on new tasks requires hyperparameter tuning, as is common for machine learning algorithms, but without inspection we may not know what each variable means (e.g. "Is s7 the learning rate?"). Tuning all constants in the program was insufficient due to hyperparameter coupling, where an expression happens to
produce a good value for a hyperparameter on a specific set of tasks but won't generalize. For example, evolution may choose $s 7=v 2 \cdot v 2$ because $v 2 \cdot v 2$ coincides with a good value for the hyperparameter s7. We address this through manual decoupling (e.g. recognizing the problematic code line and instead setting s7 to a constant that can be tuned later). This required time-consuming analysis that could be automated by future work. More details can be found in Suppl. Section S7.</p>
<p>Interpreting evolved algorithms also required effort due to the complexity of their raw code (Suppl. Section S8). The code was first automatically simplified by removing redundant instructions through static analysis. Then, to decide upon interesting code snippets, Section 4.3 focused on motifs that reappeared in independent search experiments. Such convergent evolution provided a hypothesis that a code section may be beneficial. To verify this hypothesis, we used ablations/knock-outs and knock-ins, the latter being the insertion of code sections into simpler algorithms to see if they are beneficial there too. This is analogous to homonymous molecular biology techniques used to study gene function. Further work may incorporate other techniques from the natural sciences or machine learning where interpretation of complex systems is key.</p>
<p>Search space enhancements have improved architecture search dramatically. In only two years, for example, comparable experiments went from requiring hundreds of GPUs (Zoph et al., 2018) to only one (Liu et al., 2019b). Similarly, enhancing the search space could bring significant improvements to AutoML-Zero. Also note that, despite our best effort to reduce human bias, there is still implicit bias in our current search space that limits the potential to discover certain types of algorithms. For instance, to keep our search space simple, we process one example at a time, so discovering techniques that work on batches of examples (like batch-norm) would require adding loops or higherorder tensors. As another case in point, in the current search space, a multi-layer neural network can only be found by discovering each layer independently; the addition of loops or function calls could make it easier to unlock such deeper structures.</p>
<h2>Author Contributions</h2>
<p>ER and QVL conceived the project; ER led the project; QVL provided advice; ER designed the search space, built the initial framework, and demonstrated plausibility; CL designed proxy tasks, built the evaluation pipeline, and analyzed the algorithms; DRS improved the search method and scaled up the infrastructure; ER, CL, and DRS ran the experiments; ER wrote the paper with contributions from CL; all authors edited the paper and prepared the figures; CL open-sourced the code.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Samy Bengio, Vincent Vanhoucke, Doug Eck, Charles Sutton, Yanping Huang, Jacques Pienaar, and Jeff Dean for helpful discussions, and especially Gabriel Bender, Hanxiao Liu, Rishabh Singh, Chiyuan Zhang, Hieu Pham, David Dohan and Alok Aggarwal for useful comments on the paper, as well as the larger Google Brain team.</p>
<h2>References</h2>
<p>Alba, E. and Tomassini, M. Parallelism and evolutionary algorithms. IEEE transactions on evolutionary computation, 2002.</p>
<p>Alet, F., Schneider, M. F., Lozano-Perez, T., and Kaelbling, L. P. Meta-learning curiosity algorithms. In International Conference on Learning Representations, 2019.</p>
<p>Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas, N. Learning to learn by gradient descent by gradient descent. In NIPS, 2016.</p>
<p>Angeline, P. J., Saunders, G. M., and Pollack, J. B. An evolutionary algorithm that constructs recurrent neural networks. IEEE transactions on Neural Networks, 1994.</p>
<p>Baker, B., Gupta, O., Naik, N., and Raskar, R. Designing neural network architectures using reinforcement learning. In $I C L R, 2017$.</p>
<p>Balog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., and Tarlow, D. Deepcoder: Learning to write programs. $I C L R, 2017$.</p>
<p>Bello, I., Zoph, B., Vasudevan, V., and Le, Q. V. Neural optimizer search with reinforcement learning. ICML, 2017.</p>
<p>Bengio, S., Bengio, Y., and Cloutier, J. Use of genetic programming for the search of a new learning rule for neural networks. In Evolutionary Computation, 1994.</p>
<p>Bengio, Y. Practical recommendations for gradient-based training of deep architectures. In Neural networks: Tricks of the trade. Springer, 2012.</p>
<p>Bengio, Y., Léonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint, 2013.</p>
<p>Bergstra, J. and Bengio, Y. Random search for hyperparameter optimization. JMLR, 2012.</p>
<p>Cai, H., Zhu, L., and Han, S. Proxylessnas: Direct neural architecture search on target task and hardware. ICLR, 2019.</p>
<p>Chalmers, D. J. The evolution of learning: An experiment in genetic connectionism. In Connectionist Models. Elsevier, 1991.</p>
<p>Chen, X., Liu, C., and Song, D. Towards synthesizing complex programs from input-output examples. arXiv preprint arXiv:1706.01284, 2017.</p>
<p>Chrabaszcz, P., Loshchilov, I., and Hutter, F. A downsampled variant of imagenet as an alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017.</p>
<p>Collins, M. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processingVolume 10, pp. 1-8. Association for Computational Linguistics, 2002.</p>
<p>Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation policies from data. CVPR, 2019a.</p>
<p>Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Randaugment: Practical automated data augmentation with a reduced search space. arXiv preprint arXiv:1909.13719, 2019b.</p>
<p>Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., rahman Mohamed, A., and Kohli, P. Robustfill: Neural program learning under noisy i/o. In ICML, 2017.</p>
<p>Elsken, T., Metzen, J. H., and Hutter, F. Efficient multiobjective neural architecture search via lamarckian evolution. In $I C L R, 2019$ a.</p>
<p>Elsken, T., Metzen, J. H., and Hutter, F. Neural architecture search. In Automated Machine Learning. Springer, 2019b.</p>
<p>Fahlman, S. E. and Lebiere, C. The cascade-correlation learning architecture. In NIPS, 1990.</p>
<p>Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In ICML, 2017.</p>
<p>Forsyth, R. et al. Beagle-a darwinian approach to pattern recognition. Kybernetes, 10(3):159-166, 1981.</p>
<p>Gaier, A. and Ha, D. Weight agnostic neural networks. In NeurIPS, 2019.</p>
<p>Ghiasi, G., Lin, T.-Y., and Le, Q. V. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In CVPR, 2019.</p>
<p>Goldberg, D. E. and Deb, K. A comparative analysis of selection schemes used in genetic algorithms. FOGA, 1991.</p>
<p>Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. MIT press, 2016.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
<p>Gulwani, S., Polozov, O., Singh, R., et al. Program synthesis. Foundations and Trends ${ }^{\circledR}$ in Programming Languages, 2017.</p>
<p>Hazan, E., Levy, K., and Shalev-Shwartz, S. Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems, pp. 1594-1602, 2015.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015.</p>
<p>Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 1997.</p>
<p>Holland, J. Adaptation in natural and artificial systems: an introductory analysis with application to biology. Control and artificial intelligence, 1975.</p>
<p>Hutter, F., Hoos, H. H., and Leyton-Brown, K. Sequential model-based optimization for general algorithm configuration. In LION, 2011.</p>
<p>Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., Vinyals, O., Green, T., Dunning, I., Simonyan, K., et al. Population based training of neural networks. arXiv, 2017.</p>
<p>Jayakumar, S. M., Menick, J., Czarnecki, W. M., Schwarz, J., Rae, J., Osindero, S., Teh, Y. W., Harley, T., and Pascanu, R. Multiplicative interactions and where to find them. In $I C L R, 2020$.</p>
<p>Kim, M. and Rigazio, L. Deep clustered convolutional kernels. arXiv, 2015.</p>
<p>Koza, J. R. and Koza, J. R. Genetic programming: on the programming of computers by means of natural selection. MIT press, 1992.</p>
<p>Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Master's thesis, Dept. of Computer Science, U. of Toronto, 2009.</p>
<p>Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338, 2015.</p>
<p>LeCun, Y., Bengio, Y., et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 1995.</p>
<p>LeCun, Y., Cortes, C., and Burges, C. J. The mnist database of handwritten digits, 1998.</p>
<p>Lenat, D. B. Eurisko: a program that learns new heuristics and domain concepts: the nature of heuristics iii: program design and results. Artificial intelligence, 21(1-2):61-98, 1983.</p>
<p>Levy, K. Y. The power of normalization: Faster evasion of saddle points. arXiv preprint arXiv:1611.04831, 2016.</p>
<p>Li, K. and Malik, J. Learning to optimize. ICLR, 2017.
Li, L. and Talwalkar, A. Random search and reproducibility for neural architecture search. CoRR, abs/1902.07638, 2019. URL http://arxiv.org/ abs/1902.07638.</p>
<p>Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach to hyperparameter optimization. JMLR, 2018.</p>
<p>Liang, C., Berant, J., Le, Q. V., Forbus, K. D., and Lao, N. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In $A C L, 2016$.</p>
<p>Liang, C., Norouzi, M., Berant, J., Le, Q. V., and Lao, N. Memory augmented policy optimization for program synthesis and semantic parsing. In NeurIPS, 2018.</p>
<p>Liu, C., Zoph, B., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L., Yuille, A., Huang, J., and Murphy, K. Progressive neural architecture search. ECCV, 2018.</p>
<p>Liu, C., Chen, L.-C., Schroff, F., Adam, H., Hua, W., Yuille, A. L., and Fei-Fei, L. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In CVPR, 2019a.</p>
<p>Liu, H., Simonyan, K., and Yang, Y. Darts: Differentiable architecture search. ICLR, 2019b.</p>
<p>Loshchilov, I. and Hutter, F. Cma-es for hyperparameter optimization of deep neural networks. arXiv preprint arXiv:1604.07269, 2016.</p>
<p>Mei, J., Li, Y., Lian, X., Jin, X., Yang, L., Yuille, A., and Yang, J. Atomnas: Fine-grained end-to-end neural architecture search. ICLR, 2020.</p>
<p>Mendoza, H., Klein, A., Feurer, M., Springenberg, J. T., and Hutter, F. Towards automatically-tuned neural networks. In Workshop on Automatic Machine Learning, 2016.</p>
<p>Metz, L., Maheswaranathan, N., Cheung, B., and SohlDickstein, J. Meta-learning update rules for unsupervised representation learning. In ICLR, 2019.</p>
<p>Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju, B., Shahrzad, H., Navruzyan, A., Duffy, N., et al. Evolving deep neural networks. In Artificial Intelligence in the Age of Neural Networks and Brain Computing. Elsevier, 2019.</p>
<p>Nair, V. and Hinton, G. E. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.</p>
<p>Neelakantan, A., Le, Q. V., and Sutskever, I. Neural programmer: Inducing latent programs with gradient descent. CoRR, abs/1511.04834, 2015.</p>
<p>Negrinho, R., Gormley, M., Gordon, G. J., Patil, D., Le, N., and Ferreira, D. Towards modular and programmable architecture search. In NeurIPS, 2019.</p>
<p>Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. 2011.</p>
<p>Noy, A., Nayman, N., Ridnik, T., Zamir, N., Doveh, S., Friedman, I., Giryes, R., and Zelnik-Manor, L. Asap: Architecture search, anneal and prune. arXiv, 2019.</p>
<p>Orchard, J. and Wang, L. The evolution of a generalized neural learning rule. In IJCNN, 2016.</p>
<p>Parisotto, E., rahman Mohamed, A., Singh, R., Li, L., Zhou, D., and Kohli, P. Neuro-symbolic program synthesis. ArXiv, abs/1611.01855, 2016.</p>
<p>Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. Specaugment: A simple data augmentation method for automatic speech recognition. Proc. Interspeech, 2019.</p>
<p>Pitrat, J. Implementation of a reflective system. Future Generation Computer Systems, 12(2-3):235-242, 1996.</p>
<p>Polozov, O. and Gulwani, S. Flashmeta: a framework for inductive program synthesis. In OOPSLA 2015, 2015.</p>
<p>Polyak, B. T. and Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992.</p>
<p>Ramachandran, P., Zoph, B., and Le, Q. Searching for activation functions. ICLR Workshop, 2017.</p>
<p>Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. ICLR, 2017.</p>
<p>Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y. L., Le, Q., and Kurakin, A. Large-scale evolution of image classifiers. In ICML, 2017.</p>
<p>Real, E., Aggarwal, A., Huang, Y., and Le, Q. V. Regularized evolution for image classifier architecture search. $A A A I, 2019$.</p>
<p>Reed, S. E. and de Freitas, N. Neural programmerinterpreters. CoRR, abs/1511.06279, 2015.</p>
<p>Risi, S. and Stanley, K. O. Indirectly encoding neural plasticity as a pattern of local rules. In International Conference on Simulation of Adaptive Behavior, 2010.</p>
<p>Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning representations by back-propagating errors. Nature, 1986.</p>
<p>Runarsson, T. P. and Jonsson, M. T. Evolution and design of distributed learning rules. In ECNN, 2000.</p>
<p>Schmidhuber, J. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta... hook. PhD thesis, Technische Universität München, 1987.</p>
<p>Schmidhuber, J. Optimal ordered problem solver. Machine Learning, 54(3):211-254, 2004.</p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016.</p>
<p>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. Nature, 2017.</p>
<p>Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization of machine learning algorithms. In NIPS, 2012.</p>
<p>So, D. R., Liang, C., and Le, Q. V. The evolved transformer. In ICML, 2019.</p>
<p>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 2014.</p>
<p>Stanley, K. O. and Miikkulainen, R. Evolving neural networks through augmenting topologies. Evol. Comput., 2002.</p>
<p>Stanley, K. O., Clune, J., Lehman, J., and Miikkulainen, R. Designing neural networks through neuroevolution. Nature Machine Intelligence, 2019.</p>
<p>Suganuma, M., Shirakawa, S., and Nagao, T. A genetic programming approach to designing convolutional neural network architectures. In GECCO, 2017.</p>
<p>Sun, Y., Xue, B., Zhang, M., and Yen, G. G. Evolving deep convolutional neural networks for image classification. IEEE Transactions on Evolutionary Computation, 2019.</p>
<p>Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., and Le, Q. V. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, 2019.</p>
<p>Valkov, L., Chaudhari, D., Srivastava, A., Sutton, C. A., and Chaudhuri, S. Houdini: Lifelong learning as program synthesis. In NeurIPS, 2018.</p>
<p>Vanschoren, J. Meta-learning. Automated Machine Learning, 2019.</p>
<p>Wang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. GECCO, 2019.</p>
<p>Wichrowska, O., Maheswaranathan, N., Hoffman, M. W., Colmenarejo, S. G., Denil, M., de Freitas, N., and SohlDickstein, J. Learned optimizers that scale and generalize. ICML, 2017.</p>
<p>Wilson, D. G., Cussat-Blanc, S., Luga, H., and Miller, J. F. Evolving simple programs for playing atari games. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 229-236, 2018.</p>
<p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv, 2016.</p>
<p>Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.</p>
<p>Xie, L. and Yuille, A. Genetic CNN. In ICCV, 2017.
Xie, S., Kirillov, A., Girshick, R., and He, K. Exploring randomly wired neural networks for image recognition. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1284-1293, 2019.</p>
<p>Yang, A., Esperança, P. M., and Carlucci, F. M. Nas evaluation is frustratingly hard. $I C L R, 2020$.</p>
<p>Yao, Q., Wang, M., Chen, Y., Dai, W., Yi-Qi, H., Yu-Feng, L., Wei-Wei, T., Qiang, Y., and Yang, Y. Taking human out of learning applications: A survey on automated machine learning. arXiv, 2018.</p>
<p>Yao, X. Evolving artificial neural networks. IEEE, 1999.
Ying, C., Klein, A., Real, E., Christiansen, E., Murphy, K., and Hutter, F. Nas-bench-101: Towards reproducible neural architecture search. ICML, 2019.</p>
<p>Zela, A., Klein, A., Falkner, S., and Hutter, F. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search. ICML AutoML Workshop, 2018.</p>
<p>Zhong, Z., Yan, J., Wu, W., Shao, J., and Liu, C.-L. Practical block-wise neural network architecture generation. In CVPR, 2018.</p>
<p>Zoph, B. and Le, Q. V. Neural architecture search with reinforcement learning. In $I C L R, 2016$.</p>
<p>Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning transferable architectures for scalable image recognition. In CVPR, 2018.</p>
<h1>AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</h1>
<h2>Supplementary Material</h2>
<h2>S1. Additional Related Work</h2>
<p>Because our approach simultaneously searches all the aspects of an ML algorithm, it relates to previous work that targets each aspect individually. As there are many such aspects (e.g. architecture, hyperparameters, learning rule), previous work is extensive and impossible to exhaustively list here. Many examples belong within the field of AutoML. A frequently targeted aspect of the ML algorithm is the structure of the model; this is known as architecture search. It has a long history [Fahlman \&amp; Lebiere, 1990; Angeline et al., 1994; Yao, 1999; Stanley \&amp; Miikkulainen, 2002; Bergstra \&amp; Bengio, 2012; Mendoza et al., 2016; Baker et al., 2017; Zoph \&amp; Le, 2016; Real et al., 2017; Xie \&amp; Yuille, 2017; Suganuma et al., 2017; Liu et al., 2018, and many others] and continues today [Liu et al., 2019b; Elsken et al., 2019a; Cai et al., 2019; Liu et al., 2019a; Ghiasi et al., 2019; Sun et al., 2019; Xie et al., 2019, and many others]. Reviews provide more thorough background (Elsken et al., 2019b; Stanley et al., 2019; Yao et al., 2018). Recent works have obtained accurate models by constraining the space to only look for the structure of a block that is then stacked to form a neural network. The stacking is fixed and the block is free to combine standard neural network layers into patterns that optimize the accuracy of the model (Zoph et al., 2018; Zhong et al., 2018). Mei et al. (2020) highlight the importance of finer-grained search spaces and take a step in that direction by splitting convolutions into channels that can be handled separately. Other specific architecture aspects have also been targeted, such as the hyperparameters (Snoek et al., 2012; Loshchilov \&amp; Hutter, 2016; Jaderberg et al., 2017; Li et al., 2018), activation functions (Ramachandran et al., 2017), a specific layer (Kim \&amp; Rigazio, 2015), the full forward pass (Gaier \&amp; Ha, 2019), the data augmentation (Cubuk et al., 2019a; Park et al., 2019; Cubuk et al., 2019b), etc. Beyond these narrowly targeted search spaces, more inclusive spaces are already demonstrating promise. For example, a few studies have combined two seemingly disparate algorithmic aspects into a single search space: the inner modules and the outer structure (Miikkulainen et al., 2019), the architecture and the hyperparameters (Zela et al., 2018), the layers and the weight pruning (Noy et al., 2019), and so on. We extend this to all aspects of the algorithm, including the optimization.</p>
<p>An important aspect of an ML algorithm is optimization, which has been tackled by AutoML in the form of numerically discovered optimizers. Chalmers (1991) formalizes the update rule for the weights as $w_{i, j} \leftarrow w_{i, j}+F\left(x_{1}, x_{2}, \ldots\right)$, where $x_{i}$ are local signals and $F$ combines them linearly. The coefficients of the linear combination constitute the search space and are encoded as a bit string that is searched with a genetic algorithm. This is an example of a numerically learned update rule: the final result is a set of coefficients that work very well but may not be interpretable. Numerically learned optimizers have improved since then. Studies found that Chalmers' $F$ formula above can be replaced with more advanced structures, such as a second neural network (Runarsson \&amp; Jonsson, 2000; Orchard \&amp; Wang, 2016), an LSTM (Ravi \&amp; Larochelle, 2017), a hierarchical RNN (Wichrowska et al., 2017), or even a different LSTM for each weight (Metz et al., 2019). Numerically or otherwise, some studies center on the method by which the optimizer is learned; it can vary widely from the use of gradient descent (Andrychowicz et al., 2016), to reinforcement learning (Li \&amp; Malik, 2017), to evolutionary search with sophisticated developmental encodings (Risi \&amp; Stanley, 2010). All these methods are sometimes collectively labeled as meta-learning (Vanschoren, 2019) or described as "learning the learning algorithm", as the optimizer is indeed an algorithm. However, in this work, we understand algorithm more broadly and it will include also the structure and the initialization of the model. Additionally, our algorithm is not learned numerically, but discovered symbolically. A symbolically discovered optimizer, like an equation or a computer program, can be easier to interpret or transfer.</p>
<p>An early example of a symbolically discovered optimizer is that of Bengio et al. (1994), who represent $F$ as a tree: the leaves are the possible inputs to the optimizer (i.e. the $x_{i}$ above) and the nodes are one of ${+,-, \times, \div}$. $F$ is then evolved, making this an example of genetic programming (Holland, 1975; Forsyth et al., 1981; Koza \&amp; Koza, 1992). Our search method is similar to genetic programming but we choose to represent the program as a sequence of instructions-like a programmer would type it-rather than a tree. Another similarity with Bengio et al. (1994) is that they also use simple mathematical operations as building blocks. We use many more, however, including vector and</p>
<p>matrix instructions that take advantage of dense hardware computations. More recently, Bello et al. (2017) revisited symbolically learned optimizers to apply them to a modern neural network. Their goal was to maximize the final accuracy of their models and so they restrict the search space by allowing hand-tuned operations (e.g. "apply dropout with 30\% probability", "clip at 0.00001", etc.). Our search space, on the other hand, aims to minimize restrictions and manual design. Both Bengio et al. (1994) and Bello et al. (2017) assume the existence of a neural network with a forward pass that computes the activations and a backward pass that provides the weight gradients. Thus, the search process can just focus on discovering how to use these activations and gradients to adjust the network's weights. In contrast, we do not assume the existence of a neural network model or of the gradient. They must therefore be discovered in the same way as the rest of the algorithm.</p>
<p>We note that our work also relates to program synthesis efforts. Early approaches have proposed to search for programs that improve themselves (Lenat, 1983; Schmidhuber, 1987; Pitrat, 1996). We share similar goals in searching for learning algorithms, but focus on common machine learning tasks and have dropped the self-reflexivity requirement. More recently, program synthesis has focused on solving problems like sorting, addition, counting (Schmidhuber, 2004; Graves et al., 2014; Reed \&amp; de Freitas, 2015; Valkov et al., 2018), string manipulations (Polozov \&amp; Gulwani, 2015; Parisotto et al., 2016; Devlin et al., 2017), character recognition (Lake et al., 2015), competition-style programming (Balog et al., 2017), structured data QA (Neelakantan et al., 2015; Liang et al., 2016; 2018), program parsing (Chen et al., 2017), and game playing (Wilson et al., 2018), to name a few. These studies are increasingly making more use of ML to solved the said problems (Gulwani et al., 2017). Unlike these studies, we focus on synthesizing programs that solve the problem of doing ML.</p>
<h2>S2. Search Space Additional Details</h2>
<p>Supplementary Table S1 describes all the ops in our search space. They are ordered to reflect how we chose them: we imagined a typical school curriculum up to-but not including-calculus (see braces to the right of the table). In particular, there are no derivatives so any gradient computation used for training must be evolved.</p>
<h2>S3. Search Method Additional Details</h2>
<p>The mutations that produce the child from the parent must be tailored to the search space. We use a uniformly random choice among the following three transformations: (i) add or remove an instruction; instructions are added at a random position and have a random op and random argu-
ments; to prevent programs from growing unnecessarily, instruction removal is twice as likely as addition; (ii) completely randomize all instructions in a component function by randomizing all their ops and arguments; or (iii) modify a randomly chosen argument of a randomly selected existing instruction. All categorical random choices are uniform. When modifying a real-valued constant, we multiply it by a uniform random number in $[0.5,2.0]$ and flip its sign with $10 \%$ probability.</p>
<p>We upgrade the regularized evolution search method (Real et al., 2019) to improve its performance in the following ways. These upgrades are justified empirically through ablation studies in Supplementary Section S9.</p>
<p>Functional Equivalence Checking (FEC). The lack of heavy design of the search space allows for mutations that do not have an effect on the accuracy (e.g. adding an instruction that writes to an address that is never read). When these mutations occur, the child algorithm behaves identically to its parent. To prevent these identically functioning algorithms from being repeatedly evaluated (i.e. trained and validated in full many times), we keep an LRU cache mapping evaluated algorithm fingerprints to their accuracies. Before evaluating an algorithm, we first quickly fingerprint it and consult the cache to see if it has already been evaluated. If it has, we reuse the stored accuracy instead of computing it again. This way, we can keep the different implementations of the same algorithm for the sake of diversity: even though they produce the same accuracy now, they may behave differently upon further mutation.</p>
<p>To fingerprint an algorithm, we train it for 10 steps and validate it on 10 examples. The 20 resulting predictions are then truncated and hashed to produce an integer fingerprint. The cache holds 100 k fingerprint-accuracy pairs.</p>
<p>Parallelism. In multi-process experiments, each process runs regularized evolution on its own population and the worker processes exchange algorithms through migration (Alba \&amp; Tomassini, 2002). Every 100-10000 evaluations, each worker uploads 50 algorithms (half the population) to a central server. The server replies with 50 algorithms sampled randomly across all workers that are substituted into the worker's population.</p>
<p>Dataset Diversity. While the final evaluations are on binary CIFAR-10, in the experiments in Sections 4.2 and 4.3, $50 \%$ of the workers train and evaluate on binary MNIST instead of CIFAR-10. MNIST is a dataset of labeled hand-written digits (LeCun et al., 1998). We project MNIST to 256 dimensions in the same way we do for CIFAR-10. Supplementary Section S9 demonstrates how searching on multiple MNIST-based and CIFAR-based tasks improves final performance on CIFAR-10, relative to searching only on multiple MNIST-based tasks or only on multiple CIFAR-based tasks.</p>
<p>Hurdles. We adopt the hurdles upgrade to the evolutionary algorithm. This upgrade uses statistics of the population to early-stop the training of low performing models (So et al., 2019). The early-stopping criterion is the failure to reach a minimum accuracy-the hurdle. We alter the original implementation by setting the hurdle to the $75^{\text {th }}$ percentile of unique accuracies of the evolving population on a rolling basis (as opposed to the stationary value used in the original implementation). This alteration gives us more predictability over the resource savings: we consistently save $75 \%$ of our compute, regardless of how the accuracy distribution shifts over the course of the search experiment.</p>
<p>Terminating Degenerate Algorithms. We terminate algorithms early if their calculations produce NaN or Inf values, and assign them a fixed minimum accuracy $a_{\text {min }}$ (we use $a_{\text {min }}=0$ ). Similarly, if an algorithm's error on any training
example exceeds a threshold $e_{\max } \gg 1$ (we use $e_{\max }=100$ ), we also stop that algorithm and assign it the accuracy $a_{\text {min }}$. Lastly, we time each algorithm as it is being executed and terminate it if its run-time exceeds a fixed threshold; we set this threshold to 4 x the run-time of a plain neural network trained with gradient descent.</p>
<p>The experiment's meta-parameters (e.g. $P$ and $T$ ) were either decided in smaller experiments (e.g. $P$ ), taken from previous work (e.g. $T$ ), or not tuned. Even when tuning parameters in smaller experiments, this was not done extensively (e.g. no multi-parameter grid searches); typically, we tried a handful of values independently when each feature was introduced. For each experiment, we scaled-without tuning-some meta-parameters based on compute or hardware limitations. For example, compute-heavy tasks use smaller populations in order to save frequent checkpoints in</p>
<p>Table S1: Ops vocabulary. $s, \vec{v}$ and $M$ denote a scalar, vector, and matrix, resp. Early-alphabet letters ( $a, b$, etc.) denote memory addresses. Mid-alphabet letters (e.g. $i, j$, etc.) denote vector/matrix indexes ("Index" column). Greek letters denote constants ("Consts." column). $\mathcal{U}(\alpha, \beta)$ denotes a sample from a uniform distribution in $[\alpha, \beta] . \mathcal{N}(\mu, \sigma)$ is analogous for a normal distribution with mean $\mu$ and standard deviation $\sigma . \mathbb{1}<em a="a">{X}$ is the indicator function for set $X$. Example: " $M</em>(\alpha, \beta)$ " describes the operation "assign to the $i, j$-th entry of the matrix at address $a$ a value sampled from a uniform random distribution in $[\alpha, \beta]$ ".}^{(i, j)}=\mathcal{U</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Op <br> ID</th>
<th style="text-align: center;">Code <br> Example</th>
<th style="text-align: center;">Input Args <br> Addresses <br> / types</th>
<th style="text-align: center;">Consts.</th>
<th style="text-align: center;">Output Args <br> Address <br> / type</th>
<th style="text-align: center;">Index <br> (see caption)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OP0</td>
<td style="text-align: center;">no_op</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP1</td>
<td style="text-align: center;">s2=s3*s0</td>
<td style="text-align: center;">$a, b /$ scalars</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$c /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP2</td>
<td style="text-align: center;">s4=s0-s1</td>
<td style="text-align: center;">$a, b /$ scalars</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$c /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP3</td>
<td style="text-align: center;">s8=s5*s5</td>
<td style="text-align: center;">$a, b /$ scalars</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$c /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP4</td>
<td style="text-align: center;">s7=s5/s2</td>
<td style="text-align: center;">$a, b /$ scalars</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$c /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP5</td>
<td style="text-align: center;">s8=abs(s0)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP6</td>
<td style="text-align: center;">s4=1/s8</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP7</td>
<td style="text-align: center;">s5=sin(s4)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP8</td>
<td style="text-align: center;">s1=cos(s4)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP9</td>
<td style="text-align: center;">s3=tan(s3)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP10</td>
<td style="text-align: center;">s0=arcsin(s4)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP11</td>
<td style="text-align: center;">s2=arccos(s0)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP12</td>
<td style="text-align: center;">s4=arctan(s0)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP13</td>
<td style="text-align: center;">s1=exp(s2)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP14</td>
<td style="text-align: center;">s0=log(s3)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP15</td>
<td style="text-align: center;">s3=heaviside(s0)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP16</td>
<td style="text-align: center;">v2=heaviside(v2)</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP17</td>
<td style="text-align: center;">m7=heaviside(m3)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ matrix</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP18</td>
<td style="text-align: center;">v1=s7*v1</td>
<td style="text-align: center;">$a, b /$ sc,vec</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$c /$ vector</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP19</td>
<td style="text-align: center;">v1=bcast(s3)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP20</td>
<td style="text-align: center;">v5=1/v7</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP21</td>
<td style="text-align: center;">s0=norm(v3)</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OP22</td>
<td style="text-align: center;">v3=abs(v3)</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>[Table continues on the next page.]</p>
<p>Table S1: Ops vocabulary (continued)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Op <br> ID</th>
<th style="text-align: center;">Code <br> Example</th>
<th style="text-align: center;">Input Args Address <br> / types</th>
<th style="text-align: center;">Consts</th>
<th style="text-align: center;">Output Args Address <br> / type</th>
<th style="text-align: center;">Index</th>
<th style="text-align: center;">Description (see caption)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OP23</td>
<td style="text-align: center;">v5=v0+v9</td>
<td style="text-align: center;">$a, b /$ vectors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c / vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{c}=\vec{v}</em>$}+\vec{v}_{b</td>
</tr>
<tr>
<td style="text-align: center;">OP24</td>
<td style="text-align: center;">v1=v0-v9</td>
<td style="text-align: center;">$a, b /$ vectors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c / vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{c}=\vec{v}</em>$}-\vec{v}_{b</td>
</tr>
<tr>
<td style="text-align: center;">OP25</td>
<td style="text-align: center;">v8=v1*v9</td>
<td style="text-align: center;">$a, b /$ vectors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c / vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{c}^{(i)}=\vec{v}</em> \forall i$}^{(i)} \vec{v}_{b}^{(i)</td>
</tr>
<tr>
<td style="text-align: center;">OP26</td>
<td style="text-align: center;">v9=v8/v2</td>
<td style="text-align: center;">$a, b /$ vectors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c / vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{c}^{(i)}=\vec{v}</em> \forall i$}^{(i)} / \vec{v}_{b}^{(i)</td>
</tr>
<tr>
<td style="text-align: center;">OP27</td>
<td style="text-align: center;">s6=dot(v1,v5)</td>
<td style="text-align: center;">$a, b /$ vectors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c / scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{c}=\vec{v}<em b="b">{a}^{T} \vec{v}</em>$</td>
</tr>
<tr>
<td style="text-align: center;">OP28</td>
<td style="text-align: center;">m1=outer(v6,v5)</td>
<td style="text-align: center;">$a, b /$ vectors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}=\vec{v}<em b="b">{a} \vec{v}</em>$}^{T</td>
</tr>
<tr>
<td style="text-align: center;">OP29</td>
<td style="text-align: center;">m1=s4*m2</td>
<td style="text-align: center;">$a, b /$ sc/mat</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}=s_{a} M_{b}$</td>
</tr>
<tr>
<td style="text-align: center;">OP30</td>
<td style="text-align: center;">m3=1/m0</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{b}^{(i, j)}=1 / M_{a}^{(i, j)} \forall i, j$</td>
</tr>
<tr>
<td style="text-align: center;">OP31</td>
<td style="text-align: center;">v6=dot (m1,v0)</td>
<td style="text-align: center;">$a, b /$ mat/vec</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c / vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{c}=M</em>$} \vec{v}_{b</td>
</tr>
<tr>
<td style="text-align: center;">OP32</td>
<td style="text-align: center;">m2=bcast(v0,axis=0)</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{b}^{(i, j)}=\vec{v}_{a}^{(i)} \forall i, j$</td>
</tr>
<tr>
<td style="text-align: center;">OP33</td>
<td style="text-align: center;">m2=bcast(v0,axis=1)</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{b}^{(j, i)}=\vec{v}_{a}^{(i)} \forall i, j$</td>
</tr>
<tr>
<td style="text-align: center;">OP34</td>
<td style="text-align: center;">s2=norm(m1)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{b}=\left|M_{a}\right|$</td>
</tr>
<tr>
<td style="text-align: center;">OP35</td>
<td style="text-align: center;">v4=norm(m7,axis=0)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}_{b}^{(i)}=\left</td>
</tr>
<tr>
<td style="text-align: center;">OP36</td>
<td style="text-align: center;">v4=norm(m7,axis=1)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}_{b}^{(j)}=\left</td>
</tr>
<tr>
<td style="text-align: center;">OP37</td>
<td style="text-align: center;">m9=transpose(m3)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{b}=\left</td>
</tr>
<tr>
<td style="text-align: center;">OP38</td>
<td style="text-align: center;">m1=abs(m8)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{b}^{(i, j)}=\left</td>
</tr>
<tr>
<td style="text-align: center;">OP39</td>
<td style="text-align: center;">m2=m2+m0</td>
<td style="text-align: center;">$a, b /$ matrixes</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}=M_{a}+M_{b}$</td>
</tr>
<tr>
<td style="text-align: center;">OP40</td>
<td style="text-align: center;">m2=m3+m1</td>
<td style="text-align: center;">$a, b /$ matrixes</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}=M_{a}-M_{b}$</td>
</tr>
<tr>
<td style="text-align: center;">OP41</td>
<td style="text-align: center;">m3=m2*m3</td>
<td style="text-align: center;">$a, b /$ matrixes</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}^{(i, j)}=M_{a}^{(i, j)} M_{b}^{(i, j)} \forall i, j$</td>
</tr>
<tr>
<td style="text-align: center;">OP42</td>
<td style="text-align: center;">m4=m2/m4</td>
<td style="text-align: center;">$a, b /$ matrixes</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}^{(i, j)}=M_{a}^{(i, j)} / M_{b}^{(i, j)} \forall i, j$</td>
</tr>
<tr>
<td style="text-align: center;">OP43</td>
<td style="text-align: center;">m5=matmul(m5,m7)</td>
<td style="text-align: center;">$a, b /$ matrixes</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}=M_{a} M_{b}$</td>
</tr>
<tr>
<td style="text-align: center;">OP44</td>
<td style="text-align: center;">s1=minimum(s2,s3)</td>
<td style="text-align: center;">$a, b /$ scalars</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{c}=\min \left(s_{a}, s_{b}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">OP45</td>
<td style="text-align: center;">v4=minimum(v3,v9)</td>
<td style="text-align: center;">$a, b /$ vectors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c / vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{c}^{(i)}=\min \left(\vec{v}</em>\right) \forall i$}^{(i)}, \vec{v}_{b}^{(i)</td>
</tr>
<tr>
<td style="text-align: center;">OP46</td>
<td style="text-align: center;">m2=minimum(m2,m1)</td>
<td style="text-align: center;">$a, b /$ matrixes</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}^{(i, j)}=\min \left(M_{a}^{(i, j)}, M_{b}^{(i, j)}\right) \forall i, j$</td>
</tr>
<tr>
<td style="text-align: center;">OP47</td>
<td style="text-align: center;">s8=maximum(s3,s0)</td>
<td style="text-align: center;">$a, b /$ scalars</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{c}=\max \left(s_{a}, s_{b}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">OP48</td>
<td style="text-align: center;">v7=maximum(v3,v6)</td>
<td style="text-align: center;">$a, b /$ vectors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c / vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{c}^{(i)}=\max \left(\vec{v}</em>\right) \forall i$}^{(i)}, \vec{v}_{b}^{(i)</td>
</tr>
<tr>
<td style="text-align: center;">OP49</td>
<td style="text-align: center;">m7=maximum(m1,m0)</td>
<td style="text-align: center;">$a, b /$ matrixes</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">c/matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{c}^{(i, j)}=\max \left(M_{a}^{(i, j)}, M_{b}^{(i, j)}\right) \forall i, j$</td>
</tr>
<tr>
<td style="text-align: center;">OP50</td>
<td style="text-align: center;">s2=mean(v2)</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{b}=\operatorname{mean}\left(s_{a}, s_{b}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">OP51</td>
<td style="text-align: center;">s2=mean(m8)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{b}=\operatorname{mean}\left(M_{a}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">OP52</td>
<td style="text-align: center;">v1=mean(m2,axis=0)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{b}^{(i)}=\operatorname{mean}\left(M</em>\right) \forall i$}^{(i, \cdot)</td>
</tr>
<tr>
<td style="text-align: center;">OP53</td>
<td style="text-align: center;">v3=std(m2,axis=0)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}<em a="a">{b}^{(i)}=\operatorname{stdev}\left(M</em>\right) \forall i$}^{(i, \cdot)</td>
</tr>
<tr>
<td style="text-align: center;">OP54</td>
<td style="text-align: center;">s3=std(v3)</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{b}=\operatorname{stdev}\left(\vec{v}_{a}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">OP55</td>
<td style="text-align: center;">s4=std(m0)</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$b /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{b}=\operatorname{stdev}\left(M_{a}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">OP56</td>
<td style="text-align: center;">s2=0.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{a}=\gamma$</td>
</tr>
<tr>
<td style="text-align: center;">OP57</td>
<td style="text-align: center;">v3[5]=-2.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">$i$</td>
<td style="text-align: center;">$\vec{v}_{a}^{(i)}=\gamma$</td>
</tr>
<tr>
<td style="text-align: center;">OP58</td>
<td style="text-align: center;">m2[5,1]=-0.03</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">$i, j$</td>
<td style="text-align: center;">$M_{a}^{(i, j)}=\gamma$</td>
</tr>
<tr>
<td style="text-align: center;">OP59</td>
<td style="text-align: center;">s4=uniform(-1,1)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\alpha, \beta$</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{a}=\mathcal{U}(\alpha, \beta)$</td>
</tr>
<tr>
<td style="text-align: center;">OP60</td>
<td style="text-align: center;">v1=uniform(0.4,0.8)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\alpha, \beta$</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}_{a}^{(i)}=\mathcal{U}(\alpha, \beta) \forall i$</td>
</tr>
</tbody>
</table>
<p>Table S1: Ops vocabulary (continued)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Op</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Input Args</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Output Args</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Description <br> (see caption)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ID</td>
<td style="text-align: center;">Example</td>
<td style="text-align: center;">Addresses <br> / types</td>
<td style="text-align: center;">Consts</td>
<td style="text-align: center;">Address <br> / type</td>
<td style="text-align: center;">Index</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OP61</td>
<td style="text-align: center;">m0=uniform $(-0.5,0.6)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\alpha, \beta$</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{a}^{(i, j)}=\mathcal{U}(\alpha, \beta) \forall i, j$</td>
</tr>
<tr>
<td style="text-align: center;">OP62</td>
<td style="text-align: center;">s4=gaussian $(0.1,0.7)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mu, \sigma$</td>
<td style="text-align: center;">$a /$ scalar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$s_{a}=\mathcal{N}(\mu, \sigma)$</td>
</tr>
<tr>
<td style="text-align: center;">OP63</td>
<td style="text-align: center;">v8=gaussian $(0.4,1)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mu, \sigma$</td>
<td style="text-align: center;">$a /$ vector</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\vec{v}_{a}^{(i)}=\mathcal{N}(\mu, \sigma) \forall i$</td>
</tr>
<tr>
<td style="text-align: center;">OP64</td>
<td style="text-align: center;">m2=gaussian $(-2,1.3)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mu, \sigma$</td>
<td style="text-align: center;">$a /$ matrix</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$M_{a}^{(i, j)}=\mathcal{N}(\mu, \sigma) \forall i, j$</td>
</tr>
</tbody>
</table>
<p>case of machine reboots. Additional discrepancies between experiment configurations in the different sections are due to different researchers working independently.</p>
<h2>S4. Task Generation Details</h2>
<p>Sections 4.2 and 4.3 employ many binary classification tasks grouped into two sets, $\mathcal{T}<em _select="{select" _text="\text">{\text {search }}$ and $\mathcal{T}</em>}}$. We now describe how these tasks are generated. We construct a binary classification task by randomly selecting a pair of classes from CIFAR-10 to yield positive and negative examples. We then create a random projection matrix by drawing from a Gaussian distribution with zero mean and unit variance. We use the matrix to project the features of all the examples corresponding to the class pair to a lower dimension (i.e. from the original 3072 to, for example, 16). The projected features are then standardized. This generates a proxy task that requires much less compute than the non-projected version. Each class pair and random projection matrix produce a different task. Since CIFAR-10 has 10 classes, there are 45 different pairs. For each pair we perform 100 different projections. This way we end up with 4500 tasks, each containing 8000/2000 training/validation examples. We use all the tasks from 36 of the pairs to form the $\mathcal{T<em _select="{select" _text="\text">{\text {search }}$ task set. The remaining tasks form $\mathcal{T}</em>$.}</p>
<h2>S5. Detailed Search Experiment Setups</h2>
<p>Here we present details and method meta-parameters for experiments referenced in Section 4. These complement the "Experiment Details" paragraphs in the main text.</p>
<p>Experiments in Section 4.1, Figure 4: Scalar/vector/matrix number of addresses: 4/3/1 (linear), 5/3/1 (affine). Fixed num. instructions for Setup/Predict/Learn: 5/1/4 (linear), 6/2/6 (affine). Expts. in this figure allow only minimal ops to discover a known algorithm, as follows. For "linear backprop" expts.: allowed Learn ops are {OP3, OP4, OP19, OP24}. For "linear regressor" expts.: allowed Setup ops are ${\mathrm{OP} 56, \mathrm{OP} 57}$, allowed predict ops are ${\mathrm{OP} 27}$, and allowed Learn ops are ${\mathrm{OP} 2, \mathrm{OP} 3, \mathrm{OP} 18, \mathrm{OP} 23}$. For "affine backprop" expts.: allowed Learn ops are ${\mathrm{OP} 1, \mathrm{OP} 2, \mathrm{OP} 3$,</p>
<p>OP18, OP23}. For "affine regressor" expts.: allowed Setup ops are ${\mathrm{OP} 56, \mathrm{OP} 57}$, allowed Predict ops are ${\mathrm{OP} 1$, OP27}, and allowed Learn ops are ${\mathrm{OP} 1, \mathrm{OP} 2, \mathrm{OP} 3, \mathrm{OP} 18$, OP23}. 1 process, no server. Tasks: see Experiment Details paragraph in main text. Evolution expts.: $P=1000 ; T=10$; $U=0.9$; we initialize the population with random programs; evals. per expt. for points in plot (left to right): $10 \mathrm{k}, 10 \mathrm{k}$, $10 \mathrm{k}, 100 \mathrm{k}$ (optimized for each problem difficulty to nearest factor of 10). Random search expts.: same num. memory addresses, same component function sizes, same total number of evaluations. These experiments are intended to be as simple as possible, so we do not use hurdles or additional data.</p>
<p>Experiment in Section 4.1, Figure 5: Scalar/vector/matrix number of addresses: 4/8/2. Fixed num. instructions for Setup/Predict/Learn: 21/3/9. In this figure, we only allow as ops those that appear in a two-layer neural network with gradient descent: allowed Setup ops are {OP56, OP63, OP64}, allowed Predict ops are {OP27, OP31, OP48}, and allowed Learn ops are ${\mathrm{OP} 2, \mathrm{OP} 3, \mathrm{OP} 16$, OP18, OP23, OP25, OP28, OP40}. Tasks: see Experiment Details paragraph in main text. $P=1000 . T=10 . U=0.9$. $W=1 \mathrm{k}$. Worker processes are uniformly divided into 4 groups, using parameters $T / D / P$ covering ranges in a log scale, as follows: $100 \mathrm{k} / 100 / 100,100 \mathrm{k} / 22 / 215,10 \mathrm{k} / 5 / 464$, and $100 / 1 / 1000$. Uses FEC. We initialize the population with random programs.</p>
<p>Experiments in Section 4.2: Scalar/vector/matrix number of addresses: 8/14/3. Maximum num. instructions for Setup/Predict/Learn: 21/21/45. All the initialization ops are now allowed for Setup: ${\mathrm{OP} 56, \mathrm{OP} 57, \mathrm{OP} 58$, OP59, OP60, OP61, OP62, OP63, OP64}. Predict and Learn use a longer list of 58 allowed ops: ${\mathrm{OP} 0, \mathrm{OP} 1$, OP2, OP3, OP4, OP5, OP6, OP7, OP8, OP9, OP10, OP11, OP12, OP13, OP14, OP15, OP16, OP17, OP18, OP19, OP20, OP21, OP22, OP23, OP24, OP25, OP26, OP27, OP28, OP29, OP30, OP31, OP32, OP33, OP34, OP35, OP36, OP37, OP38, OP39, OP40, OP41, OP42, OP43, OP44, OP45, OP46, OP47, OP48, OP49, OP50, OP51, OP52, OP53, OP54, OP55, OP60, OP61}-all these ops</p>
<p>are available to both Predict and Learn. We use all the optimizations described in Section 5, incl. additional projected binary MNIST data. Worker processes are uniformly divided to perform each possible combination of tasks: {projected binary CIFAR-10, projected binary MNIST} $\otimes$ ${N=800 \&amp; E=1, N=8000 \&amp; E=1, N=800 \&amp; E=10}$ $\otimes{D=1, D=10} \otimes{F=8, F=16, F=256}$; where $N$ is the number of training examples, $E$ is the number of training epochs, and other quantities are defined in Section 3. $P=100 . T=10 . U=0.9 . W=10 \mathrm{k}$ processes (commodity CPU cores). We initialize the population with empty programs.</p>
<p>Experiments in Section 4.3: Scalar/vector/matrix number of addresses: 10/16/4. Maximum num. instructions for Setup/Predict/Learn: 21/21/45. Allowed ops for Setup are {OP56, OP57, OP58, OP59, OP60, OP61, OP62, OP63, OP64}, allowed ops for Predict and Learn are {OP0, OP1, OP2, OP3, OP4, OP5, OP6, OP7, OP8, OP9, OP10, OP11, OP12, OP13, OP14, OP15, OP16, OP17, OP18, OP19, OP20, OP21, OP22, OP23, OP24, OP25, OP26, OP27, OP28, OP29, OP30, OP31, OP32, OP33, OP34, OP35, OP36, OP37, OP38, OP39, OP40, OP41, OP42, OP43, OP44, OP45, OP46, OP47, OP48, OP49, OP50, OP51, OP52, OP53, OP54, OP55, OP63, OP64}. These are the same ops as in the paragraph above, except for the minor accidental replacement of uniform for Gaussian initialization ops. We use FEC and hurdles. Workers use binary CIFAR-10 dataset projected to dimension 16. Half of the workers use $D=10$ (for faster evolution), and the other half use $D=100$ (for more accurate evaluation). $P=100$. $T=10 . U=0.9$. Section 4.3 considers three different task types: (1) In the "few training examples" task type (Figure 7a), experiments train each algorithm on 80 examples for 100 epochs for the experiments, while controls train on 800 examples for 100 epochs. (2) In the "fast training" task type (Figure 7b), experiments train on 800 examples for 10 epochs, while controls train on 800 examples for 100 epochs. (3) In the "multiple classes" task type (Figure 7c), experiments evaluate on projected 10-class CIFAR-10 classification tasks, while controls evaluate on the projected binary CIFAR-10 classification tasks described before. The 10-class tasks are generated similarly to the binary tasks, as follows. Each task contains 45K/5K training/validation examples. Each example is a CIFAR-10 image projected to 16 dimensions using a random matrix drawn from a Gaussian distribution with zero mean and unit variance. This projection matrix remains fixed for all examples within a task. The data are standardized after the projection. We use 1000 different random projection matrices to create 1000 different tasks. $80 \%$ of these tasks constitute $\mathcal{T}<em _select="{select" _text="\text">{\text {search }}$ and the rest form $\mathcal{T}</em>$. Since Section 4.2 showed that we can discover reasonable models from scratch, in Section 4.3, we initialize the population with the simple two-layer neural
network with gradient descent of Figure 5 in order to save compute.}</p>
<h2>S6. Evolved Algorithms</h2>
<p>In this section, we show the raw code for algorithms discovered by evolutionary search in Sections 4.2 and 4.3. The code in those sections was simplified and therefore has superficial differences with the corresponding code here.</p>
<p>Supplementary Figure S1a shows the raw code for the best evolved algorithm in Section 4.2. For comparison, Figure S1b shows the effect of removing redundant instructions through automated static analysis (details in Supplementary Section S8). For example, the instruction v3 = gaussian $(0.7,0.4)$ has been deleted this way.</p>
<div class="codehilite"><pre><span></span><code>def Setup():
    s4 = uniform(0.6,0.2)
    v3 = gaussian(0.7,0.4)
    v12= gaussian(0.2,0.6)
    s1 =
        uniform(-0.1,-0.2)
def Predict():
    v1 = v0 - v9
    v5 = v0 + v9
    v6 = dot(m1, v5)
    m1 = s2 <span class="gs">* m2</span>
<span class="gs">    s1 = dot(v6, v1)</span>
<span class="gs">    s6 = cos(s4)</span>
<span class="gs">def Learn():</span>
<span class="gs">    s4 = s0 - s1</span>
<span class="gs">    s3 = abs(s1)</span>
<span class="gs">    m1 = outer(v1,v0)</span>
<span class="gs">    s5 = sin(s4)</span>
<span class="gs">    s2 = norm(m1)</span>
<span class="gs">    s7 = s5 / s2</span>
<span class="gs">    s4 = s4 + s6</span>
<span class="gs">    v11= s7 *</span> v1
    m1 = heaviside(m2)
    m1 = outer(v11,v5)
    m0 = m1 + m0
    v9 = uniform(2e-3,0.7)
    s7 = log(s0)
    s4 = std(m0)
    m2 = m2 + m0
    m1 = s4 * m0
</code></pre></div>

<p>(a)
def Setup():
def Predict():
v1 = v0 - v9
v5 = v0 + v9
v6 = dot(m1,v5)
m1 = s2 * m2
s1 = dot(v6,v1)
def Learn():
s4 = s0 - s1
m1 = outer(v1,v0)
s5 = sin(s4)
s2 = norm(m1)
s7 = s5 / s2
v11= s7 * v1
m1 = outer(v11,v5)
m0 = m1 + m0
v9 =
uniform(2e-3,0.7)
s4 = std(m0)
m2 = m2 + m0
m1 = s4 * m0
(b)</p>
<p>Figure S1: (a) Raw code for the best evolved algorithm in Figure 6 (bottom-right corner) in Section 4.2. (b) Same code after redundant instructions have been removed through static analysis.</p>
<p>Finally, the fully simplified version is in the bottom right corner of Figure 6 in the main text. To achieve this simplification, we used ablation studies to find instructions that can be removed or reordered. More details can be found in Supplementary Section S8. For example, in going from Supplementary Figure S1b to Figure 6, we removed s5 = $\sin (\mathrm{s} 4)$ because its deletion does not significantly alter the accuracy. We also consistently renamed some variables (of</p>
<div class="codehilite"><pre><span></span><code>def Setup():
    s3 = 4.0e-3
def Predict():
    v7 = v5 - v0
    v3 = dot(m0, v0)
    s8 = s9 / s3
    v3 = v3 + v1
    m1 = heaviside(m3)
    v4 = maximum(v3, v7)
    s1 = dot(v4, v2)
    s4 = log(s9)
    v3 = bcast(s8)
    s7 = std(v1)
    v3 = v14 + v0
    m2 = matmul(m0, m2)
def Learn():
    v1 = gaussian(-0.50, 0.41)
    s4 = std(m0)
    s4 = s0 - s1
    v5 = gaussian(-0.48, 0.48)
    m1 = transpose(m3)
    s4 = s3 <span class="gs">* s4</span>
<span class="gs">    v15 = v15 *</span> v6
    v6 = s4 <span class="gs">* v4</span>
<span class="gs">    v2 = v2 + v6</span>
<span class="gs">    v7 = s4 *</span> v2
    v8 = heaviside(v3)
    v7 = v8 * v7
    m1 = outer(v7, v0)
    m0 = m0 + m1
</code></pre></div>

<p>(a) Raw code for the adaptation to few examples in Figure 7a.
(b) Raw code for the adaptation to fast training in Figure 7b.</p>
<div class="codehilite"><pre><span></span><code>def Setup():
    m3 = uniform(0.05, 0.11)
    s1 = uniform(0.31, 0.90)
    v18 = uniform(-0.49, 4.41)
    s1 = -0.65
    m5 = uniform(0.21, 0.22)
    v9 = gaussian(0.64, 7.8e-3)
    s1 = -0.84
def Predict():
    s1 = abs(s1)
    v15 = norm(m1, axis=1)
    v15 = dot(m0, v0)
    v8 = v19 - v0
    v15 = v8 + v15
    v7 = max(v1, v15)
    v13 = min(v5, v4)
    m2 = transpose(m2)
    v10 = v13 <span class="gs">* v0</span>
<span class="gs">    m7 = heaviside(m3)</span>
<span class="gs">    m4 = transpose(m7)</span>
<span class="gs">    v2 = dot(m1, v7)</span>
<span class="gs">    v6 = max(v2, v9)</span>
<span class="gs">    v2 = v2 + v13</span>
<span class="gs">    v11 = heaviside(v17)</span>
<span class="gs">    s1 = sin(s1)</span>
<span class="gs">    m3 = m6 - m5</span>
<span class="gs">    v19 = heaviside(v14)</span>
<span class="gs">    v10 = min(v12, v7)</span>
<span class="gs">def Learn():</span>
<span class="gs">    m5 = abs(m7)</span>
<span class="gs">    v8 = v1 - v2</span>
<span class="gs">    m2 = transpose(m2)</span>
<span class="gs">    v8 = s1 *</span> v8
    v15 = v15 - v4
    v4 = v4 + v8
    s1 = arcsin(s1)
    v15 = mean(m3, axis=1)
    v12 = v11 <span class="gs">* v11</span>
<span class="gs">    m4 = heaviside(m5)</span>
<span class="gs">    m6 = outer(v8, v7)</span>
<span class="gs">    s1 = sin(s1)</span>
<span class="gs">    s1 = exp(s0)</span>
<span class="gs">    m1 = m1 + m3</span>
<span class="gs">    m5 = outer(v15, v6)</span>
<span class="gs">    m2 = transpose(m1)</span>
<span class="gs">    s1 = exp(s0)</span>
<span class="gs">    v12 = uniform(0.30, 0.33)</span>
<span class="gs">    s1 = minimum(s0, s1)</span>
<span class="gs">    m5 = m5 *</span> m7
    v9 = dot(m2, v8)
    v9 = v10 * v9
    v3 = norm(m7, axis=1)
    s1 = mean(m1)
    m2 = outer(v9, v0)
    m0 = m0 + m2
</code></pre></div>

<p>(c) Raw code for the adaptation to multiple classes in Figure 7c.</p>
<p>Figure S2: Raw evolved code for algorithm snippets in Figure 7 in Section 4.3.
course, this has no effect on the execution of the code).
in full.
Supplementary Figure S2 shows the raw code for the algorithms in Figure 7 in Section 4.3. Note that in Figure 7, we display a code snippet containing only a few selected instructions, while Supplementary Figure S2 shows the programs</p>
<h2>S7. Algorithm Selection and Evaluation</h2>
<p>We first run search experiments evaluating algorithms on the projected binary classification tasks sampled from $\mathcal{T}<em _select="{select" _text="\text">{\text {search }}$ and collect the best performing candidate from each experiment. The measure of performance is the median accuracy across tasks. Then, we rank these candidates by evaluating them on tasks sampled from $\mathcal{T}</em>$ and we select the highest-ranking candidate (this is analogous to typical model selection practice using a validation set). The highest ranking algorithm is finally evaluated on the binary classification tasks using CIFAR-10 data with the original dimensionality (3072).}</p>
<p>Because the algorithms are initially evolved on tasks with low dimensionality (16) and finally evaluated on the full-size dimensionality (3072), their hyperparameters must be tuned on the full-size dimensionality before that final evaluation. To do this, we treat all the constants in the algorithms as hyperparameters and jointly tune them using random search. For each random search trial, each constant is scaled up or down by a random factor sampled between 0.001 and 1000 on a log-scale. We allowed up to 10 k trials to tune the hyperparameters, but only a few hundred were required to tune the best algorithm in Figure 6-note that this algorithm only has 3 constants. To make comparisons with baselines fair, we tune these baselines using the same amount of resources that went into tunining and evolving our algorithms. All hyperparameter-tuning trials use 8000 training and 2000 validation examples from the CIFAR-10 training set. After tuning, we finally run the tuned algorithms on 2000 examples from the held-out CIFAR-10 test set. We repeat this final evaluation with 5 different random seeds and report the mean and standard deviation. We stress that the CIFAR-10 test set was used only in this final evaluation, and never in $\mathcal{T}<em _select="{select" _text="\text">{\text {search }}$ or $\mathcal{T}</em>$.}</p>
<p>In our experiments, we found a hyperparameter coupling phenomenon that hinders algorithm selection and tuning. ML algorithms usually make use of hyperparameters (e.g. learning rate) that need to be tuned for different datasets (for example, when the datasets have very different input dimensions or numbers of examples). Similarly, the evolved algorithms also contain hyperparameters that need to be adjusted for different datasets. If the hyperparameters are represented as constants in the evolved algorithm, we can identify and tune them on the new dataset by using random search. However, it is harder to tune them if a hyperparameter is instead computed from other variables. For example, in some evolved algorithms, the learning rate $s_{2}$ was computed as $s_{2}=\operatorname{norm}\left(v_{1}\right)$ because the best value for $s_{2}$ coincides with the L2-norm of $v_{1}$ on $\mathcal{T}<em 1="1">{\text {search }}$. However, when we move to a new dataset with higher dimensions, the L2-norm of $v</em>$ might no longer be a good learning rate. This can cause the evolved algorithms' performance to drop
dramatically on the new dataset. To resolve this, we identify these parameters by manual inspection of the evolved code. We then manually decouple them: in the example, we would set $s_{2}$ to a constant that we can tune with random-search. This recovers the performance. Automating the decoupling process would be a useful direction for future work.</p>
<h2>S8. Interpreting Algorithms</h2>
<p>It is nontrivial to interpret the raw evolved code and decide which sections of it are important. We use the following procedures to help with the interpretation of discovered algorithms:
(a) We clean up the raw code (e.g. Figure S1a) by automatically simplifying programs. To do this, we remove redundant instructions through static analysis, resulting in code like that in Figure S1b. Namely, we analyze the computations that lead to the final prediction and remove instructions that have no effect. For example, we remove instructions that initialize variables that are never used.
(b) We focus our attention on code sections that reappear in many independent search experiments. This is a sign that such code sections may be beneficial. For example, Section 4.3 applied this procedure to identify adaptations to different tasks.
(c) Once we have hypotheses about interesting code sections, we perform ablations/knock-outs, where we remove the code section from the algorithm to see if there is a significant loss in accuracy. As an example, for Section 4.2, we identified 6 interesting code sections in the best evolved algorithm to perform ablations. For each ablation, we removed the relevant code section, then tuned all the hyperparameters / constants again, and then computed the loss in validation accuracy. 4 out of the 6 ablations caused a large drop in accuracy. These are the ones that we discussed in Section 4.2. Namely, (1) the addition of noise to the input ( $-0.16 \%$ ); (2) the bilinear model $(-1.46 \%)$; (3) the normalized gradients $(-1.20 \%)$; and (4) the weight averaging $(-4.11 \%)$. The remaining 2 code sections show no significant loss upon ablation, and so were removed for code readability. Also for readability, we reorder some instructions when this makes no difference to the accuracy either (e.g. we move related code lines closer to each other). After this procedure, the code looks like that in Figure 6.
(d) If an ablation suggests that a code section is indeed helpful to the original algorithm, we then perform a knock-in. That is, we insert the code section into simpler algorithms to see if it improves their performance too. This way we confirmed the usefulness of the 4 code sections mentioned in (c), for example.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The electricity consumption for our experiments (which were run in 2019) was matched with purchases of renewable energy.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
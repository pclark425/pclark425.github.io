<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5390 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5390</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5390</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-d58438041e8d645a3e531325d4f0aed3dbe41920</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d58438041e8d645a3e531325d4f0aed3dbe41920" target="_blank">Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a dense connection strategy, proposing a novel Densely Connected Graph Convolutional Network (DCGCN), able to integrate both local and non-local features to learn a better structural representation of a graph.</p>
                <p><strong>Paper Abstract:</strong> We focus on graph-to-sequence learning, which can be framed as transducing graph structures to sequences for text generation. To capture structural information associated with graphs, we investigate the problem of encoding graphs using graph convolutional networks (GCNs). Unlike various existing approaches where shallow architectures were used for capturing local structural information only, we introduce a dense connection strategy, proposing a novel Densely Connected Graph Convolutional Network (DCGCN). Such a deep architecture is able to integrate both local and non-local features to learn a better structural representation of a graph. Our model outperforms the state-of-the-art neural models significantly on AMR-to-text generation and syntax-based neural machine translation.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5390.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5390.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCGCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Densely Connected Graph Convolutional Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-sequence encoder that applies densely connected graph convolutional layers to directly encode graph structures (no linearization) for text generation; introduced dense connectivity, graph attention, direction-aware weights and a linear-combination layer to learn deep, parameter-efficient graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>DCGCN + Extended Levi Graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The model does not linearize graphs. Instead it (1) transforms the input graph into an extended Levi graph (edges become nodes, see separate entry) with explicit edge-type tokens (default, reverse, self, global, plus forward/backward for dependency trees); (2) concatenates learned node embeddings with positional encodings (minimum distance from root) as inputs; (3) encodes the extended Levi graph with multi-block densely connected GCNs where each layer inputs the concatenation of all preceding layer outputs (dense connectivity); (4) incorporates node-wise self-attention (graph attention) to produce per-neighbor attention coefficients and direction/edge-type-specific weight matrices; (5) aggregates direction-specific outputs by concatenation and linear projection; (6) passes the encoder outputs through a linear-combination layer (inspired by ELMo) and uses the learned global-node vector to initialize an attention LSTM decoder with coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and dependency trees (transformed into extended Levi graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves explicit graph topology and edge labels by turning edges into nodes; supports multiple edge types (direction-aware labels) and per-edge-type parameterization; adds a global node to capture non-local context; positional encodings supply root-distance information; dense connectivity enables very deep GCN stacks while remaining parameter-efficient (each dense layer small, concatenated outputs produce full-dim representations); highly parallelizable compared with recurrent encoders; aims to avoid structural information loss inherent in sequence linearization; requires transformation (Levi/extended Levi) and extra edge-type vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation (LDC2015E86 / AMR15 and LDC2017T10 / AMR17) and syntax-based neural machine translation (source dependency trees; WMT16 English-German and English-Czech subsets used)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU and CHRF++ reported. Key reported results from paper: AMR17 single DCGCN: 27.6 BLEU; AMR15 (no external) DCGCN single: 25.7 BLEU, DCGCN ensemble: 28.2 BLEU, CHRF++ 59.6 (ensemble); AMR15 with external data: DCGCN(single) 0.1M extra -> 29.0 BLEU; 0.2M extra -> 31.6 BLEU; 0.3M extra -> single 33.2 BLEU / ensemble 35.3 BLEU; Syntax-based NMT: English-German single DCGCN: 19.0 BLEU (CHRF++ 44.1); English-Czech single: 12.1 BLEU (CHRF++ 37.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Directly compared against sequence-linearization Seq2Seq models, recurrent graph encoders (Graph-state LSTM) and GGNN-based encoders: DCGCN substantially outperforms Seq2Seq baselines when trained without large external corpora (e.g., on AMR15 DCGCN(single) 25.7 vs Seq2SeqK 22.0 BLEU), outperforms GraphLSTM (e.g., AMR15 25.7 vs 23.3 BLEU) and GGNN2Seq (paper reports DCGCN single is 3.3 BLEU higher than GGNN2Seq single on AMR17). For syntax-based NMT DCGCN single (19.0 BLEU) surpasses BiRNN+GCN and GGNN2Seq baselines (e.g., GGNN2Seq 16.7 BLEU on En-De in the paper's table). DCGCN achieves these improvements with fewer parameters than many baselines (paper reports DCGCN models often have ~1/3–1/6 of parameters of competing models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires transforming graphs into extended Levi graphs (increasing node count—Levi nodes for original edges plus a global node), which increases graph size and computational cost per example; relies on careful design choices (edge-type vocabularies, positional encodings, dense-block sizes) and hyperparameter tuning; still uses an LSTM decoder (not an all-convolutional/transformer decoder) and benefits from additional external auto-parsed data for top performance; dense GCN stacks mitigate but do not directly eliminate general issues like over-smoothing in GCNs—dense connectivity is the proposed mitigation. Open questions include applying framework to other graph tasks and further reducing preprocessing/transform complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5390.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5390.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extended Levi Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extended Levi Graph (edge-to-node conversion with added global node and sequential edges)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph transformation that turns original graph edges into nodes (Levi graph) and extends it by adding a global node connected to all nodes and (for dependency trees) forward/backward sequential edges; used so node and edge labels can be encoded uniformly and to give each node global context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Extended Levi Graph (default/reverse/self/global [+ forward/backward])</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Starting from an input graph (AMR or dependency tree), every original edge becomes an explicit node (Levi transformation); edges on the Levi graph are typed as default (original), reverse, and self (as in prior Levi transformations); this paper extends the Levi graph by adding a dedicated global node with 'global' edges from the global node to every other node; for dependency trees they also add forward and backward sequential edges between token nodes to capture sentence order; the edge-type vocabulary thus includes default, reverse, self, global (and forward/backward for dependency trees).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and dependency trees (converted)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables uniform encoding of node- and edge-information since edges become nodes; preserves labeled relations explicitly; global node provides a single vector summarizing whole-graph information (used to initialize decoder); increasing node count improves expressive modeling of relations but raises graph size and computational cost; enables direction-specific parameterization because each Levi edge has explicit type.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as encoder input representation for AMR-to-text generation and syntax-based NMT experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared implicitly against linearized-sequence inputs (Seq2Seq) and against approaches that only add reverse/self edges (Marcheggiani & Titov) — extended Levi graph gives a more uniform representation and the paper shows encoders using it (with DCGCN or GGNN) outperform sequence-linearized baselines and other encodings in AMR-to-text; the added global node and explicit edge-nodes are argued and ablated in paper (global node and linear combination empirically improve BLEU by ~1.3–2.2 points in ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Transforms increase graph size (node count) which may affect computational cost and memory; requires defining and handling an expanded edge-type vocabulary; needs careful handling of positional/sequence information for sentence-like inputs; some earlier methods use simpler additions (reverse/self edges) which are cheaper computationally but less expressive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5390.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5390.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization (Seq2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation (linearized-graph input)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commonly used baseline approach where structured graphs (e.g., AMR) are serialized/linearized into a sequence and fed to a standard sequence-to-sequence model for generation, trading explicit structure for simplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / Serialization (AMR -> token sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph structures (AMR) are converted to a bracketed or token sequence representation (a linear traversal/serialization) and then provided as input tokens to a standard seq2seq encoder-decoder model; entity simplification and post-processing are commonly used alongside to reduce sparsity and restore entities.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (serialized into sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple to apply and compatible with off-the-shelf sequence models; efficient at training time and leverages large pretrained seq models; however it can lose explicit relational/topological information and long-distance structural dependencies, making it harder for seq models to learn graph semantics without large amounts of data or careful preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation benchmarks (AMR15, AMR17) used as baselines in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported baseline numbers (from paper citations): on AMR15: Seq2SeqK (no external) 22.0 BLEU; with 0.2M extra auto-parsed data Seq2SeqK 27.4 BLEU; with 2M extra 32.3 BLEU; with 20M extra 33.8 BLEU. On AMR17 (as reported in comparisons) Seq2Seq baselines are substantially lower than DCGCN when no external data is used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper shows that linearized Seq2Seq models are substantially outperformed by graph-aware encoders like DCGCN when trained on the same (limited) gold data—DCGCN single models exceed Seq2SeqK by multiple BLEU points without requiring large external auto-parsed corpora; however when Seq2Seq models are trained with very large external corpora they can reach competitive scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Loses structural fidelity and explicit labeled relations; performance often depends strongly on large amounts of extra auto-parsed data and heuristics (entity simplification); harder to capture non-local graph structure compared to graph-structured encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5390.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5390.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Levi Graph + GGNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that transforms the input graph into a Levi graph (edges turned into nodes) and encodes it with a Gated Graph Neural Network (GGNN) as encoder for sequence generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi Graph (edge->node) + GGNN encoder</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Edges from the original graph become special nodes in the Levi graph, allowing edge labels to be represented as node features; GGNN (gated recurrent-style message passing) propagates and updates node states over multiple steps; reverse and self edges are commonly added as additional edge types; encoder outputs are used with an attention decoder for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and dependency trees (via Levi graph transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Uniform representation of nodes and edge-labels; GGNN's gating can model multi-step propagation and long-range interactions but is recurrent (less parallelizable than pure convolutional GCNs) and tends to have higher parameter counts; effective at encoding labeled relations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation and syntax-based NMT (benchmarked in paper comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in paper: GGNN2Seq (from Beck et al., 2018) examples include NMT English-German single 16.7 BLEU and English-Czech single 9.8 BLEU (paper's Table 4). The DCGCN single model is reported as 3.3 BLEU higher than GGNN2Seq single on AMR17 (paper statement).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>DCGCN is reported to outperform GGNN2Seq in BLEU while using fewer parameters (paper reports DCGCN ~29.7M vs GGNN2Seq ~41.2M for some settings); GGNN-based encoders remain a strong recurrent-graph baseline but DCGCN's dense convolutional approach is more parallelizable and scales to deeper encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>GGNNs are recurrent (less parallel), can be parameter-heavy, and may require many propagation steps to capture long-range dependencies; they also do not directly exploit dense connectivity insights to ease training of very deep stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5390.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5390.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphLSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A graph-to-sequence model for AMR-to-text generation (graph-state LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent graph encoder that models graph-level semantics via graph-state LSTM transitions (message passing with gating) to learn context-aware node states for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A graph-to-sequence model for AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-state LSTM (recurrent graph encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses graph-state LSTM units that iteratively update node representations through gated message passing among neighbors (graph-state transitions), explicitly capturing multi-hop non-local information through recurrent updates; output node states are attended by a decoder for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (and other linguistic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Captures non-local interactions via state transitions and gating; more sequential in computation (recurrent) so less parallelizable; in some reported systems used with char-level representations and pretrained embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation (AMR15/AMR17), used as a strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported figures from paper: AMR15 GraphLSTM (no external) 23.3 BLEU; with 0.2M external auto-parsed data GraphLSTM 28.2 BLEU; with 2M external 33.6 BLEU. Paper reports DCGCN outperforms GraphLSTM by ~2 BLEU points in fully supervised setting and requires less external data to reach comparable scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>DCGCN (convolutional, densely connected) is reported to outperform GraphLSTM (recurrent gated encoder) in BLEU on AMR benchmarks while using fewer parameters and without char-level features or pretrained embeddings; GraphLSTM improved with large external data but DCGCN achieves competitive scores with less extra data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Recurrent message-passing incurs sequential computation cost; may require more external data or auxiliary features (char-level, pretrained embeddings) to match top performance; gating increases parameter complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5390.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5390.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree/Statistical methods (PBMT/Tree2Str/TSP/SNRG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-neural / tree-transducer / phrase-based approaches for AMR-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Older (non-neural) graph-to-text approaches transform AMR into tree-like structures or linearized forms and apply statistical machine translation or tree transducers / synchronous grammars to generate text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation from abstract meaning representation using tree transducers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Tree-to-string / spanning-tree / phrase-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Examples include transforming AMR into a spanning tree and applying a tree transducer (Tree2Str), linearizing AMR for phrase-based MT (PBMT), formulating AMR-to-text as TSP over fragments, or using synchronous node replacement grammars (SNRG); these methods often rely on separate language models trained on large text corpora (e.g., Gigaword) to improve fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (converted to trees or linear sequences for SMT/tree-transducer pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Can leverage large external corpora for fluency via separate language models; sometimes competitive BLEU without neural encoders; require careful hand-crafted conversions and search-based decoding, less end-to-end differentiable than neural methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation (AMR15 comparisons in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported numbers (from paper Table 3): TSP 22.4 BLEU (ALL external), PBMT 26.9 BLEU (ALL external), Tree2Str 23.0 BLEU (ALL external), SNRG 25.6 BLEU (ALL external). DCGCN ensemble (no external) 28.2 BLEU outperforms these non-neural baselines without using their external corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper states DCGCN (ensemble) surpasses these non-neural models even without external data, and neural graph encoders can incorporate structural information directly rather than rely on separately-trained language models; statistical methods often rely on large external corpora to reach competitive fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depend heavily on external monolingual corpora for language models; need specialized conversion heuristics from graph to tree/sequence and non-differentiable pipeline components; less flexible for end-to-end learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5390.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5390.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity simplification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entity simplification preprocessing (as used by Konstas et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing step that replaces complex named entities in AMR graphs with simplified placeholders to reduce sparsity and facilitate learning, restoring them after generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Entity simplification (AMR preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Named entities and complex entity subgraphs are replaced by canonical placeholders (simplified tokens) during training and generation; a post-processing step maps placeholders back to the original entity strings (often using the AMR graph's name and wiki attributes).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (preprocessing prior to encoding or linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Reduces vocabulary sparsity and helps models generalize; simplifies graph structure making encoding easier; requires entity recovery logic after generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation (used as preprocessing in this paper following Konstas et al. 2017).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Used by both linearized Seq2Seq baselines and by graph-encoder approaches in the literature to improve performance; it's a complementary preprocessing step rather than an encoding scheme itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Needs accurate post-processing to restore entities; errors in entity simplification or restoration can harm final output quality; may mask useful entity-level context from the encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>A graph-to-sequence model for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Generation from abstract meaning representation using tree transducers <em>(Rating: 2)</em></li>
                <li>Generating english from abstract meaning representations <em>(Rating: 2)</em></li>
                <li>Graph convolutional encoders for syntax-aware neural machine translation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5390",
    "paper_id": "paper-d58438041e8d645a3e531325d4f0aed3dbe41920",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "DCGCN",
            "name_full": "Densely Connected Graph Convolutional Network",
            "brief_description": "A graph-to-sequence encoder that applies densely connected graph convolutional layers to directly encode graph structures (no linearization) for text generation; introduced dense connectivity, graph attention, direction-aware weights and a linear-combination layer to learn deep, parameter-efficient graph representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "DCGCN + Extended Levi Graph",
            "representation_description": "The model does not linearize graphs. Instead it (1) transforms the input graph into an extended Levi graph (edges become nodes, see separate entry) with explicit edge-type tokens (default, reverse, self, global, plus forward/backward for dependency trees); (2) concatenates learned node embeddings with positional encodings (minimum distance from root) as inputs; (3) encodes the extended Levi graph with multi-block densely connected GCNs where each layer inputs the concatenation of all preceding layer outputs (dense connectivity); (4) incorporates node-wise self-attention (graph attention) to produce per-neighbor attention coefficients and direction/edge-type-specific weight matrices; (5) aggregates direction-specific outputs by concatenation and linear projection; (6) passes the encoder outputs through a linear-combination layer (inspired by ELMo) and uses the learned global-node vector to initialize an attention LSTM decoder with coverage.",
            "graph_type": "AMR graphs and dependency trees (transformed into extended Levi graphs)",
            "representation_properties": "Preserves explicit graph topology and edge labels by turning edges into nodes; supports multiple edge types (direction-aware labels) and per-edge-type parameterization; adds a global node to capture non-local context; positional encodings supply root-distance information; dense connectivity enables very deep GCN stacks while remaining parameter-efficient (each dense layer small, concatenated outputs produce full-dim representations); highly parallelizable compared with recurrent encoders; aims to avoid structural information loss inherent in sequence linearization; requires transformation (Levi/extended Levi) and extra edge-type vocabulary.",
            "evaluation_task": "AMR-to-text generation (LDC2015E86 / AMR15 and LDC2017T10 / AMR17) and syntax-based neural machine translation (source dependency trees; WMT16 English-German and English-Czech subsets used)",
            "performance_metrics": "BLEU and CHRF++ reported. Key reported results from paper: AMR17 single DCGCN: 27.6 BLEU; AMR15 (no external) DCGCN single: 25.7 BLEU, DCGCN ensemble: 28.2 BLEU, CHRF++ 59.6 (ensemble); AMR15 with external data: DCGCN(single) 0.1M extra -&gt; 29.0 BLEU; 0.2M extra -&gt; 31.6 BLEU; 0.3M extra -&gt; single 33.2 BLEU / ensemble 35.3 BLEU; Syntax-based NMT: English-German single DCGCN: 19.0 BLEU (CHRF++ 44.1); English-Czech single: 12.1 BLEU (CHRF++ 37.1).",
            "comparison_to_other_representations": "Directly compared against sequence-linearization Seq2Seq models, recurrent graph encoders (Graph-state LSTM) and GGNN-based encoders: DCGCN substantially outperforms Seq2Seq baselines when trained without large external corpora (e.g., on AMR15 DCGCN(single) 25.7 vs Seq2SeqK 22.0 BLEU), outperforms GraphLSTM (e.g., AMR15 25.7 vs 23.3 BLEU) and GGNN2Seq (paper reports DCGCN single is 3.3 BLEU higher than GGNN2Seq single on AMR17). For syntax-based NMT DCGCN single (19.0 BLEU) surpasses BiRNN+GCN and GGNN2Seq baselines (e.g., GGNN2Seq 16.7 BLEU on En-De in the paper's table). DCGCN achieves these improvements with fewer parameters than many baselines (paper reports DCGCN models often have ~1/3–1/6 of parameters of competing models).",
            "limitations_or_challenges": "Requires transforming graphs into extended Levi graphs (increasing node count—Levi nodes for original edges plus a global node), which increases graph size and computational cost per example; relies on careful design choices (edge-type vocabularies, positional encodings, dense-block sizes) and hyperparameter tuning; still uses an LSTM decoder (not an all-convolutional/transformer decoder) and benefits from additional external auto-parsed data for top performance; dense GCN stacks mitigate but do not directly eliminate general issues like over-smoothing in GCNs—dense connectivity is the proposed mitigation. Open questions include applying framework to other graph tasks and further reducing preprocessing/transform complexity.",
            "uuid": "e5390.0",
            "source_info": {
                "paper_title": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Extended Levi Graph",
            "name_full": "Extended Levi Graph (edge-to-node conversion with added global node and sequential edges)",
            "brief_description": "A graph transformation that turns original graph edges into nodes (Levi graph) and extends it by adding a global node connected to all nodes and (for dependency trees) forward/backward sequential edges; used so node and edge labels can be encoded uniformly and to give each node global context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Extended Levi Graph (default/reverse/self/global [+ forward/backward])",
            "representation_description": "Starting from an input graph (AMR or dependency tree), every original edge becomes an explicit node (Levi transformation); edges on the Levi graph are typed as default (original), reverse, and self (as in prior Levi transformations); this paper extends the Levi graph by adding a dedicated global node with 'global' edges from the global node to every other node; for dependency trees they also add forward and backward sequential edges between token nodes to capture sentence order; the edge-type vocabulary thus includes default, reverse, self, global (and forward/backward for dependency trees).",
            "graph_type": "AMR graphs and dependency trees (converted)",
            "representation_properties": "Enables uniform encoding of node- and edge-information since edges become nodes; preserves labeled relations explicitly; global node provides a single vector summarizing whole-graph information (used to initialize decoder); increasing node count improves expressive modeling of relations but raises graph size and computational cost; enables direction-specific parameterization because each Levi edge has explicit type.",
            "evaluation_task": "Used as encoder input representation for AMR-to-text generation and syntax-based NMT experiments in the paper.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Compared implicitly against linearized-sequence inputs (Seq2Seq) and against approaches that only add reverse/self edges (Marcheggiani & Titov) — extended Levi graph gives a more uniform representation and the paper shows encoders using it (with DCGCN or GGNN) outperform sequence-linearized baselines and other encodings in AMR-to-text; the added global node and explicit edge-nodes are argued and ablated in paper (global node and linear combination empirically improve BLEU by ~1.3–2.2 points in ablations).",
            "limitations_or_challenges": "Transforms increase graph size (node count) which may affect computational cost and memory; requires defining and handling an expanded edge-type vocabulary; needs careful handling of positional/sequence information for sentence-like inputs; some earlier methods use simpler additions (reverse/self edges) which are cheaper computationally but less expressive.",
            "uuid": "e5390.1",
            "source_info": {
                "paper_title": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Linearization (Seq2Seq)",
            "name_full": "Neural AMR: Sequence-to-sequence models for parsing and generation (linearized-graph input)",
            "brief_description": "A commonly used baseline approach where structured graphs (e.g., AMR) are serialized/linearized into a sequence and fed to a standard sequence-to-sequence model for generation, trading explicit structure for simplicity.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "mention",
            "representation_name": "Linearization / Serialization (AMR -&gt; token sequence)",
            "representation_description": "Graph structures (AMR) are converted to a bracketed or token sequence representation (a linear traversal/serialization) and then provided as input tokens to a standard seq2seq encoder-decoder model; entity simplification and post-processing are commonly used alongside to reduce sparsity and restore entities.",
            "graph_type": "AMR graphs (serialized into sequences)",
            "representation_properties": "Simple to apply and compatible with off-the-shelf sequence models; efficient at training time and leverages large pretrained seq models; however it can lose explicit relational/topological information and long-distance structural dependencies, making it harder for seq models to learn graph semantics without large amounts of data or careful preprocessing.",
            "evaluation_task": "AMR-to-text generation benchmarks (AMR15, AMR17) used as baselines in the paper.",
            "performance_metrics": "Reported baseline numbers (from paper citations): on AMR15: Seq2SeqK (no external) 22.0 BLEU; with 0.2M extra auto-parsed data Seq2SeqK 27.4 BLEU; with 2M extra 32.3 BLEU; with 20M extra 33.8 BLEU. On AMR17 (as reported in comparisons) Seq2Seq baselines are substantially lower than DCGCN when no external data is used.",
            "comparison_to_other_representations": "Paper shows that linearized Seq2Seq models are substantially outperformed by graph-aware encoders like DCGCN when trained on the same (limited) gold data—DCGCN single models exceed Seq2SeqK by multiple BLEU points without requiring large external auto-parsed corpora; however when Seq2Seq models are trained with very large external corpora they can reach competitive scores.",
            "limitations_or_challenges": "Loses structural fidelity and explicit labeled relations; performance often depends strongly on large amounts of extra auto-parsed data and heuristics (entity simplification); harder to capture non-local graph structure compared to graph-structured encoders.",
            "uuid": "e5390.2",
            "source_info": {
                "paper_title": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Levi Graph + GGNN",
            "name_full": "Graph-to-sequence learning using gated graph neural networks",
            "brief_description": "An approach that transforms the input graph into a Levi graph (edges turned into nodes) and encodes it with a Gated Graph Neural Network (GGNN) as encoder for sequence generation tasks.",
            "citation_title": "Graph-to-sequence learning using gated graph neural networks",
            "mention_or_use": "mention",
            "representation_name": "Levi Graph (edge-&gt;node) + GGNN encoder",
            "representation_description": "Edges from the original graph become special nodes in the Levi graph, allowing edge labels to be represented as node features; GGNN (gated recurrent-style message passing) propagates and updates node states over multiple steps; reverse and self edges are commonly added as additional edge types; encoder outputs are used with an attention decoder for generation.",
            "graph_type": "AMR graphs and dependency trees (via Levi graph transformation)",
            "representation_properties": "Uniform representation of nodes and edge-labels; GGNN's gating can model multi-step propagation and long-range interactions but is recurrent (less parallelizable than pure convolutional GCNs) and tends to have higher parameter counts; effective at encoding labeled relations.",
            "evaluation_task": "AMR-to-text generation and syntax-based NMT (benchmarked in paper comparisons).",
            "performance_metrics": "Reported in paper: GGNN2Seq (from Beck et al., 2018) examples include NMT English-German single 16.7 BLEU and English-Czech single 9.8 BLEU (paper's Table 4). The DCGCN single model is reported as 3.3 BLEU higher than GGNN2Seq single on AMR17 (paper statement).",
            "comparison_to_other_representations": "DCGCN is reported to outperform GGNN2Seq in BLEU while using fewer parameters (paper reports DCGCN ~29.7M vs GGNN2Seq ~41.2M for some settings); GGNN-based encoders remain a strong recurrent-graph baseline but DCGCN's dense convolutional approach is more parallelizable and scales to deeper encoders.",
            "limitations_or_challenges": "GGNNs are recurrent (less parallel), can be parameter-heavy, and may require many propagation steps to capture long-range dependencies; they also do not directly exploit dense connectivity insights to ease training of very deep stacks.",
            "uuid": "e5390.3",
            "source_info": {
                "paper_title": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "GraphLSTM",
            "name_full": "A graph-to-sequence model for AMR-to-text generation (graph-state LSTM)",
            "brief_description": "A recurrent graph encoder that models graph-level semantics via graph-state LSTM transitions (message passing with gating) to learn context-aware node states for generation.",
            "citation_title": "A graph-to-sequence model for AMR-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "Graph-state LSTM (recurrent graph encoder)",
            "representation_description": "Uses graph-state LSTM units that iteratively update node representations through gated message passing among neighbors (graph-state transitions), explicitly capturing multi-hop non-local information through recurrent updates; output node states are attended by a decoder for generation.",
            "graph_type": "AMR graphs (and other linguistic graphs)",
            "representation_properties": "Captures non-local interactions via state transitions and gating; more sequential in computation (recurrent) so less parallelizable; in some reported systems used with char-level representations and pretrained embeddings.",
            "evaluation_task": "AMR-to-text generation (AMR15/AMR17), used as a strong baseline.",
            "performance_metrics": "Reported figures from paper: AMR15 GraphLSTM (no external) 23.3 BLEU; with 0.2M external auto-parsed data GraphLSTM 28.2 BLEU; with 2M external 33.6 BLEU. Paper reports DCGCN outperforms GraphLSTM by ~2 BLEU points in fully supervised setting and requires less external data to reach comparable scores.",
            "comparison_to_other_representations": "DCGCN (convolutional, densely connected) is reported to outperform GraphLSTM (recurrent gated encoder) in BLEU on AMR benchmarks while using fewer parameters and without char-level features or pretrained embeddings; GraphLSTM improved with large external data but DCGCN achieves competitive scores with less extra data.",
            "limitations_or_challenges": "Recurrent message-passing incurs sequential computation cost; may require more external data or auxiliary features (char-level, pretrained embeddings) to match top performance; gating increases parameter complexity.",
            "uuid": "e5390.4",
            "source_info": {
                "paper_title": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Tree/Statistical methods (PBMT/Tree2Str/TSP/SNRG)",
            "name_full": "Non-neural / tree-transducer / phrase-based approaches for AMR-to-text",
            "brief_description": "Older (non-neural) graph-to-text approaches transform AMR into tree-like structures or linearized forms and apply statistical machine translation or tree transducers / synchronous grammars to generate text.",
            "citation_title": "Generation from abstract meaning representation using tree transducers",
            "mention_or_use": "mention",
            "representation_name": "Tree-to-string / spanning-tree / phrase-based linearization",
            "representation_description": "Examples include transforming AMR into a spanning tree and applying a tree transducer (Tree2Str), linearizing AMR for phrase-based MT (PBMT), formulating AMR-to-text as TSP over fragments, or using synchronous node replacement grammars (SNRG); these methods often rely on separate language models trained on large text corpora (e.g., Gigaword) to improve fluency.",
            "graph_type": "AMR graphs (converted to trees or linear sequences for SMT/tree-transducer pipelines)",
            "representation_properties": "Can leverage large external corpora for fluency via separate language models; sometimes competitive BLEU without neural encoders; require careful hand-crafted conversions and search-based decoding, less end-to-end differentiable than neural methods.",
            "evaluation_task": "AMR-to-text generation (AMR15 comparisons in paper).",
            "performance_metrics": "Reported numbers (from paper Table 3): TSP 22.4 BLEU (ALL external), PBMT 26.9 BLEU (ALL external), Tree2Str 23.0 BLEU (ALL external), SNRG 25.6 BLEU (ALL external). DCGCN ensemble (no external) 28.2 BLEU outperforms these non-neural baselines without using their external corpora.",
            "comparison_to_other_representations": "Paper states DCGCN (ensemble) surpasses these non-neural models even without external data, and neural graph encoders can incorporate structural information directly rather than rely on separately-trained language models; statistical methods often rely on large external corpora to reach competitive fluency.",
            "limitations_or_challenges": "Depend heavily on external monolingual corpora for language models; need specialized conversion heuristics from graph to tree/sequence and non-differentiable pipeline components; less flexible for end-to-end learning.",
            "uuid": "e5390.5",
            "source_info": {
                "paper_title": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Entity simplification",
            "name_full": "Entity simplification preprocessing (as used by Konstas et al., 2017)",
            "brief_description": "A preprocessing step that replaces complex named entities in AMR graphs with simplified placeholders to reduce sparsity and facilitate learning, restoring them after generation.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "use",
            "representation_name": "Entity simplification (AMR preprocessing)",
            "representation_description": "Named entities and complex entity subgraphs are replaced by canonical placeholders (simplified tokens) during training and generation; a post-processing step maps placeholders back to the original entity strings (often using the AMR graph's name and wiki attributes).",
            "graph_type": "AMR graphs (preprocessing prior to encoding or linearization)",
            "representation_properties": "Reduces vocabulary sparsity and helps models generalize; simplifies graph structure making encoding easier; requires entity recovery logic after generation.",
            "evaluation_task": "AMR-to-text generation (used as preprocessing in this paper following Konstas et al. 2017).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Used by both linearized Seq2Seq baselines and by graph-encoder approaches in the literature to improve performance; it's a complementary preprocessing step rather than an encoding scheme itself.",
            "limitations_or_challenges": "Needs accurate post-processing to restore entities; errors in entity simplification or restoration can harm final output quality; may mask useful entity-level context from the encoder.",
            "uuid": "e5390.6",
            "source_info": {
                "paper_title": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "A graph-to-sequence model for AMR-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Generation from abstract meaning representation using tree transducers",
            "rating": 2
        },
        {
            "paper_title": "Generating english from abstract meaning representations",
            "rating": 2
        },
        {
            "paper_title": "Graph convolutional encoders for syntax-aware neural machine translation",
            "rating": 2
        }
    ],
    "cost": 0.02250375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning</h1>
<p>Zhijiang Guo ${ }^{1 <em>}$, Yan Zhang ${ }^{1 </em>}$, Zhiyang Teng ${ }^{1,2}$, Wei Lu ${ }^{1}$<br>${ }^{1}$ Singapore University of Technology and Design 8 Somapah Road, Singapore, 487372<br>${ }^{2}$ School of Engineering, Westlake University, China<br>{zhijiang_guo,yan_zhang, zhiyang_teng}@mymail.sutd.edu.sg<br>tengzhi yang@westlake.edu.cn, luwei@sutd.edu.sg</p>
<h4>Abstract</h4>
<p>We focus on graph-to-sequence learning, which can be framed as transducing graph structures to sequences for text generation. To capture structural information associated with graphs, we investigate the problem of encoding graphs using graph convolutional networks (GCNs). Unlike various existing approaches where shallow architectures were used for capturing local structural information only, we introduce a dense connection strategy, proposing a novel Densely Connected Graph Convolutional Network (DCGCN). Such a deep architecture is able to integrate both local and non-local features to learn a better structural representation of a graph. Our model outperforms the state-of-the-art neural models significantly on AMR-to-text generation and syntax-based neural machine translation.</p>
<h2>1 Introduction</h2>
<p>Graphs play an important role in natural language processing (NLP) as they are able to capture richer structural information than sequences and trees. Generally, semantics of sentences can be encoded as graphs. For example, the abstract meaning representation (AMR) (Banarescu et al., 2013) is a directed, labeled graph as shown in Figure 1, where nodes in the graph denote semantic concepts and edges denote relations between concepts. Such graph representations can capture rich semanticlevel structural information, and are attractive representations useful for semantics-related tasks such as semantic parsing (Guo and Lu, 2018) and natural language generation (Beck et al., 2018). In this paper, we focus on the graph-to-sequence</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>learning tasks, where we aim to learn representations for graphs that are useful for text generation.</p>
<p>Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are variants of convolutional neural networks (CNNs) that operate directly on graphs, where the representation of each node is iteratively updated based on those of its adjacent nodes in the graph through an information propagation scheme. For example, the first layer of GCNs can only capture the graph's adjacency information between immediate neighbors, while with the second layer one will be able to capture second-order proximity information (neighborhood information two hops away from one node) as shown in Figure 1. Formally, $L$ layers will be needed in order to capture neighborhood information that is $L$ hops away.</p>
<p>GCNs have been successfully applied to many NLP tasks (Bastings et al., 2017; Zhang et al., 2018b). Interestingly, although deeper GCNs with more layers will be able to capture richer neighborhood information of a graph, empirically it has been observed that the best performance is achieved with a 2-layer model (Li et al., 2018).</p>
<p>Therefore, recent efforts that leverage recurrencebased graph neural networks have been explored as the alternatives to encode the structural information of graphs. Examples include graph-state long short-term memory (LSTM) networks (Song et al., 2018) and gated graph neural networks (GGNNs) (Beck et al., 2018). Deep architectures based on such recurrence-based models have been successfully built for tasks such as language generation, where rich neighborhood information captured was shown useful.</p>
<p>Compared with recurrent neural networks, convolutional architectures are highly parallelizable and are more amenable to hardware acceleration</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A 3-layer densely connected graph convolutional network. The example AMR graph here corresponds to the sentence "You guys know what I mean." Every layer encodes information about immediate neighbors and 3 layers are needed to capture thirdorder neighborhood information (nodes that are 3 hops away from the current node). Each layer concatenates all preceding outputs as the input.
(Gehring et al., 2017). It is therefore worthwhile to explore the possibility of applying deeper GCNs that are able to capture more non-local information associated with the graph for graph-to-sequence learning. Prior efforts have tried to train deep GCNs by incorporating residual connections (Bastings et al., 2017). Xu et al. (2018) show that vanilla residual connections proposed by He et al. (2016) are not effective for graph neural networks. They next attempt to resolve this issue by adding additional recurrent layers on top of graph convolutional layers. However, they are still confined to relatively shallow GCNs architectures (at most 6 layers in their experiments), which may not be able to capture the rich nonlocal interactions for larger graphs.</p>
<p>In this paper, to better address the issue of learning deeper GCNs, we introduce dense connectivity to GCNs and propose the novel densely connected graph convolutional networks (DCGCNs), inspired by DenseNets (Huang et al., 2017) that distill insights from residual connections. The dense connectivity strategy is illustrated in Figure 1 schematically. Direct connections are introduced from any layer to all its preceding layers. For example, the third layer receives the outputs of the first layer and the second layer, capturing the first-order, the second-order, and the third-order neighborhood information. With the help of dense connections, we are able to train multi-layer GCN models with a large depth, allowing rich local and non-local information to be captured for learning
a better graph representation than those learned from the shallower GCN models.</p>
<p>Experiments show that our model is able to achieve better performance for graph-to-sequence learning tasks. For the AMR-to-text generation task, our model surpasses the current state-of-the-art neural models trained on LDC2015E86 and LDC2017T10 by 2 and 4.3 BLEU points, respectively. For the syntax-based neural machine translation task, our model is also consistently better than others, showing the effectiveness of the model on a large training set. Our code is available at https://github.com/Cartus/ DCGCN. ${ }^{1}$</p>
<h2>2 Densely Connected GCNs</h2>
<p>In this section, we will present the basic components used for constructing our DCGCN model.</p>
<h3>2.1 GCNs</h3>
<p>GCNs are neural networks that operate directly on graph structures (Kipf and Welling, 2017). Here we mathematically illustrate how multi-layer GCNs work on an undirected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ and $\mathcal{E}$ are the set of nodes and edges, respectively. The convolution computation for node $v$ at the $l$-th layer, which takes the input feature representation $\mathbf{h}^{(l-1)}$ as input and outputs the induced representation $\mathbf{h}_{v}^{(l)}$, can be defined as</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">{v}^{(l)}=\rho\left(\sum</em>\right)
$$}(v)} W^{(l)} \mathbf{h}_{u}^{(l-1)}+\mathbf{b}^{(l)</p>
<p>where $W^{(l)}$ is the weight matrix, $\mathbf{b}^{(l)}$ is the bias vector, $\mathcal{N}(v)$ is the set of one-hop neighbors of node $v$, and $\rho$ is an activation function (e.g., RELU [Nair and Hinton, 2010]). $\mathbf{h}<em v="v">{v}^{(0)}$ is the initial input $\mathbf{x}</em>$ and $d$ is the input feature dimension.}$, where $\mathbf{x}_{v} \in \mathbb{R}^{d</p>
<p>GCNs with Residual Connections. Bastings et al. (2017) integrate residual connections (He et al., 2016) into GCNs to help information propagation. Specifically, each node is updated</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>according to Equation (1) first and then the resulting representation is combined with the node's representation from the last iteration:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">{v}^{(l)}=\rho\left(\sum</em>}(v)} W^{(l)} \mathbf{h<em v="v">{u}^{(l-1)}+\mathbf{b}^{(l)}\right)+\mathbf{h}</em>
$$}^{(l-1)</p>
<p>GCNs with Layer Aggregations. Xu et al. (2018) propose layer aggregations for GCNs, in which the final representation of each node is computed by combining the node's representations from all GCN layers:</p>
<p>$$
\mathbf{h}<em v="v">{v}^{\text {final }}=L A\left(\mathbf{h}</em>}^{(l)}, \mathbf{h<em v="v">{v}^{(l-1)}, \ldots, \mathbf{h}</em>\right)
$$}^{(1)</p>
<p>where the $L A$ function can be concatenation, maxpooling, or LSTM-attention operations as defined in Xu et al. (2018).</p>
<h3>2.2 Dense Connectivity</h3>
<p>Dense connectivity is the core component of the proposed DCGCN. With dense connectivity, node $v$ in the $l$-th layer not only takes inputs from $\mathbf{h}^{(l-1)}$, but also receives information from all the preceding layers, as shown in Figure 2. Mathematically, we first define $\mathbf{g}_{u}^{(l)}$ as the concatenation of the initial node representation and the node representations produced in layers $1, \cdots$, $l-1:$</p>
<p>$$
\mathbf{g}<em u="u">{u}^{(l)}=\left[\mathbf{x}</em>} ; \mathbf{h<em u="u">{u}^{(1)} ; \ldots ; \mathbf{h}</em>\right]
$$}^{(l-1)</p>
<p>Such a mechanism allows deeper layers to capture all previous information to alleviate the problem discussed in Section 1 in graph neural networks. Similar strategies are also proposed in previous work (He et al., 2016; Huang et al., 2017).</p>
<p>While dense connectivity allows training deeper neural networks, every intermediate layer is designated to be of very small size, allowing adding only a small set of feature-maps at each layer. The final classifier makes predictions based on all feature-maps, which is called "collective knowledge" (Huang et al., 2017). Such a strategy improves the parameter efficiency. In practice, the dimensions of these small hidden layers $d_{\text {hidden }}$ are decided by the number of layers $L$ and the input feature dimension $d$. In DCGCN, we use $d_{\text {hidden }}=d / L$.</p>
<p>For example, if we have a 3-layer $(L=3)$ DCGCN model and input dimension is 300 ( $d=300$ ), the hidden dimension of each layer will be $d_{\text {hidden }}=d / L=300 / 3=100$. Then
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Each DCGCN block has two sub-blocks. Both of them are densely connected graph convolutional layers with different numbers of layers. A linear transformation is used between two sub-blocks, followed by a residual connection.
we concatenate the output of each layer to form the new representation. We have 3 layers so the output dimension is $300(3 \times 100)$. Different from the GCN model whose hidden dimension is larger than or equal to the input dimension, the DCGCN model shrinks the hidden dimension as the number of layers increases in order to improve the parameter efficiency similar to DenseNets (Huang et al., 2017).</p>
<p>Accordingly, we modify the convolution computation of each layer as:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">{v}^{(l)}=\rho\left(\sum</em>\right)
$$}(v)} W^{(l)} \mathbf{g}_{u}^{(l)}+\mathbf{b}^{(l)</p>
<p>The column dimension of the weight matrix increases by $d_{\text {hidden }}$ per layer, that is, $W^{(l)} \in$ $\mathbb{R}^{d_{\text {hidden } \times d^{(l)}}}$, where $d^{(l)}=d+d_{\text {hidden }} \times(l-1)$.</p>
<h3>2.3 Graph Attention</h3>
<p>Attention mechanisms have become almost a de facto standard in many sequence-based tasks (Vaswani et al., 2017). In DCGCNs, we also incorporate the self-attention strategy by implicitly specifying different weights to different nodes in a neighborhood similar to graph attention networks (Velickovic et al., 2018).</p>
<p>In order to perform self-attention on nodes, attention coefficients are required. The input for the calculation is a set of vectors, $\tilde{\mathbf{g}}^{(l)}=$ $\left{\tilde{\mathbf{g}}<em 2="2">{1}^{(l)}, \tilde{\mathbf{g}}</em>}^{(l)}, \ldots, \tilde{\mathbf{g}<em n="n">{n}^{(l)}\right}$, after node-wise feature transformation $\tilde{\mathbf{g}}</em>}^{(l)}=W^{(l)} \mathbf{g<em a="a">{n}^{(l)}$. As an initial step, a shared linear projection parameterized by a weight matrix, $W</em>$, is applied to nodes in the graph. Attention coefficients can be computed as:} \in \mathbb{R}^{d_{\text {hidden }} \times d_{\text {hidden }}</p>
<p>$$
\alpha_{i j}^{(l)}=\frac{\exp \left(\phi\left(\mathbf{a}^{\top}\left[W_{a} \tilde{\mathbf{g}}<em a="a">{i}^{(l)} ; W</em>} \tilde{\mathbf{g}<em _in="\in" _mathcal_N="\mathcal{N" k="k">{j}^{(l)}\right]\right)\right)}{\sum</em><em a="a">{i}} \exp \left(\phi\left(\mathbf{a}^{\top}\left[W</em>} \tilde{\mathbf{g}<em a="a">{i}^{(l)} ; W</em>
$$} \tilde{\mathbf{g}}_{k}^{(l)}\right]\right)\right)</p>
<p>where $\mathbf{a} \in \mathbb{R}^{2 d_{\text {hidden }}}$ is a weight vector, $\phi$ is the activation function (here we use LeakyReLU [Girshick et al., 2014]). These coefficients are used to compute a linear combination of the node representations. Modifying the convolution computation for attention, we arrive at:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">{v}^{(l)}=\rho\left(\sum</em>\right)
$$}(v)} \alpha_{v u}^{(l)} W^{(l)} \mathbf{g}_{u}^{(l)}+\mathbf{b}^{(l)</p>
<p>where $\alpha_{v u}^{(l)}$ are normalized attention coefficients computed by the attention mechanism at $l$-th layer. Note that these coefficients will not change the dimension of the output representations.</p>
<h2>3 Graph-to-Sequence Model</h2>
<p>In the following we will explain the model architecture of the graph-to-sequence model. We leverage DCGCNs as the graph encoder, which directly models the graph structure without linearization.</p>
<h3>3.1 Graph Encoder</h3>
<p>The graph encoder is composed of DCGCN blocks, as shown in Figure 3. Within each DCGCN block, we design two types of multi-layer DCGCNs as two sub-blocks to capture graph structure at different abstract levels. As Figure 2 shows, in each block, the first sub-block has $n$-layers and the second sub-block has $m$-layers. This prototype shares the same spirit with the usage of two different-sized filters in DenseNets (Huang et al., 2017).</p>
<p>Linear Combination Layer. In addition to densely connected layers, we include a linear
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The model concatenates node embeddings and positional embeddings as inputs. The encoder contains a stack of $N$ identical blocks. The linear transformation layer combines output of all blocks into hidden representations. These are fed into an attention mechanism, generating the context vector. The decoder, a 2-layer LSTM (Hochreiter and Schmidhuber, 1997), makes predictions based on hidden representations and the context vector.
combination layer between multi-layer DCGCNs to filter the representations from different DCGCN layers, reaching a more expressive representation. This strategy is inspired by ELMo (Peters et al., 2018), which combines the hidden states from different LSTM layers. We also use a residual connection (He et al., 2016) to incorporate the initial inputs of multi-layer GCNs into the linear combination layer, see Figure 3. Formally, the output of the linear combination layer is defined as:</p>
<p>$$
\mathbf{h}<em _comb="{comb" _text="\text">{\text {comb }}=W</em>}}\left(\mathbf{h<em v="v">{\text {out }}+\mathbf{x}</em>
$$}\right)+\mathbf{b}_{\text {comb }</p>
<p>where $\mathbf{h}<em _out="{out" _text="\text">{\text {out }}$ is the output of the densely connected layers by concatenating outputs from all previous $L$ layers $\mathbf{h}</em>}}=\left[\mathbf{h}^{(1)} ; \ldots ; \mathbf{h}^{(L)}\right]$ and $\mathbf{h<em v="v">{\text {out }} \in \mathbb{R}^{d}$. $\mathbf{x}</em>}$ is the input of the DCGCN layer. $\mathbf{h<em v="v">{\text {out }}$ and $\mathbf{x}</em>}$ share the same dimension $d . W_{\text {comb }} \in \mathbb{R}^{d \times d}$ is a weight matrix and $\mathbf{b<em _comb="{comb" _text="\text">{\text {comb }}$ is a bias vector for the linear transformation. Both $W</em>$ are different according to different DCGCN layers. In addition, another linear combination layer is added to obtain the final representations as shown in Figure 3.}}$ and $\mathbf{b}_{\text {comb }</p>
<h3>3.2 Extended Levi Graph</h3>
<p>In order to improve the information propagation process in graph structures such as AMR graphs and dependency trees, previous researchers enrich the original input graphs with additional</p>
<p>transformations. Marcheggiani and Titov (2017) add reverse edges as well as self-loop edges for each node to the original graph. This strategy is similar to the bidirectional recurrent neural networks (RNNs) (Elman, 1990), which can enjoy the information propagation from two directions. Beck et al. (2018) adapt this approach and additionally transform the directed input graphs into Levi graphs (Gross et al., 2013). Basically, edges in the original graphs are turned into additional nodes in Levi graphs. With this approach, we can encode the original edge labels and node inputs in the same way. Specifically, Beck et al. (2018) define three types of edge labels on the Levi graph: default, reverse, and self, which refer to the original edges, the new virtual edges that are reverse to the original edges, and the self-loop edges.</p>
<p>Scarselli et al. (2009) add another node that is connected to all other nodes. Zhang et al. (2018a) use a global sentence-level node to assemble and back-distribute information. Motivated by these works, we propose an extended Levi graph, which adds a global node in the Levi graph. For every node $x$ in the original Levi graph, there is a new edge (global) from the global node to $x$. Figure 4 shows an example AMR graph and its corresponding extended Levi graph. The edge type vocabulary for the extended Levi graph of the AMR graph now becomes $\mathcal{T}={$ default, reverse, self, global $}$. Our motivations are three-fold. First, the global node gives each node a global view of the input graph, which can make each node more aware of the non-local information. Second, the global node can serve as a hub to help node communications, which can facilitate the node information propagation process. Third, the output vectors of the global node in the encoder can be used as the initial states of the decoder, which are crucial for sequence-to-sequence learning tasks. Prior efforts average representations of all nodes as the graph embedding to initialize the decoder. Instead, we directly use the learned representation of the global nodes, which captures the information from all nodes in the whole graph.</p>
<p>The input to the syntax-based neural machine translation task is the dependency tree. Unlike the AMR graph, the sentence contains significant sequential information. Beck et al. (2018) inject this information by adding sequential connections to each token. In our model, we also add forward and backward sequential connections, as
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An AMR graph (top) and its corresponding extended Levi graph (bottom). The extended Levi graph contains an additional global node and four different type of edges.
illustrated in Figure 5. Therefore, the edge type vocabulary for the extended Levi graph of the dependency tree becomes $\mathcal{T}={$ default, reverse, self, global, forward, backward $}$.</p>
<p>Positional encodings about the relative or absolute position of the tokens have been proved beneficial for sequence learning (Gehring et al., 2017). We also include positional encodings by concatenating them with the learned word embeddings. The positional encodings are indexed by integer values representing the minimum distance from the root node. For example, come-01 in Figure 4 is the root node of the AMR graph, so its index should be 0 , where and is the child node of come-01, its index is 1 . Notice that we denote the index of the global node as -1 .</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A dependency tree and its extended Levi graph.</p>
<h3>3.3 Direction Aggregation</h3>
<p>Directionality and edge labels play an important role in linguistic structures. Information from incoming edges, outgoing edges, and self edges should be treated differently by using separate weight matrices. Moreover, information from incoming edges that have different labels should have different weight matrices, too. Following this motivation, we incorporate the directionality of an edge directly in its label. For example, node learn01 in Figure 4 has three incoming edges, these edges have three different types: default (from node op2), self (from node learn-01), and global (from node gnode). For the AMR graph we have four types of edges while for dependency trees we have six as mentioned in Section 3.2. Thus, considering different type of edges, we modify the convolution computation as:</p>
<p>$$
\mathbf{v}<em _in="\in" _mathcal_N="\mathcal{N" _substack_u="\substack{u">{t}^{(l)}=\rho\left(\sum</em>}(v) \ \operatorname{dir}(u, v)=t}} \alpha_{v u}^{(l)} W_{t}^{(l)} \mathbf{g<em t="t">{u}^{(l)}+\mathbf{b}</em>\right)
$$}^{(l)</p>
<p>where $\operatorname{dir}(u, v)$ selects the weight matrix and bias term associated with the edge type $t$. For example, in the AMR generation task, there are four edge types: default, reverse, self, and global. Each type corresponds to a separate weight matrix and a separate bias term.</p>
<p>Now we need to aggregate representations learned from different types of edges. A simple way to do this is averaging them to get the final representations. However, Hamilton et al. (2017) show that using a mean-based function to aggregate feature information from different nodes may not be satisfactory, since information from different sources should not be treated equally. Thus we assign different weights to information from different types of edges to integrate such information. Specifically, we concatenate the learned representations from all types of edges and perform a linear transformation, mathematically represented as:</p>
<p>$$
f\left(\left[\mathbf{v}<em T="T">{1}^{(l)} ; \cdots ; \mathbf{v}</em>}^{(l)}\right]\right)=W_{f}\left[\mathbf{v<em T="T">{1}^{(l)} ; \cdots ; \mathbf{v}</em>
$$}^{(l)}\right]+\mathbf{b}_{f</p>
<p>where $W_{f} \in \mathbb{R}^{d^{\prime} \times d_{\text {hidden }}}$ is the weight matrix and $d^{\prime}=T \times d_{\text {hidden }} . T$ is the size of the edge type vocabulary and $d_{\text {hidden }}$ is the hidden dimension in DCGCN layers as described in Section 2.2. $\mathbf{b}<em _hidden="{hidden" _text="\text">{f} \in \mathbb{R}^{d</em>$ is a bias vector. Finally, the convolution computation becomes:}}</p>
<p>$$
\mathbf{h}<em 1="1">{v}^{(l)}=\rho\left(f\left(\left[\mathbf{v}</em>\right]\right)\right)
$$}^{(l)} ; \cdots ; \mathbf{v}_{T}^{(l)</p>
<h3>3.4 Decoder</h3>
<p>We use an attention-based LSTM decoder (Bahdanau et al., 2015). The initial state of the decoder is the representation of the global node described in Section 3.2. The decoder yields the natural language sequence by calculating a sequence of hidden states sequentially. Here we also include the coverage mechanism (Tu et al., 2016). Therefore, when generating the $t$-th token, the decoder considers five factors: the attention memory, the word embedding of the $(t-1)$-th token, the previous hidden state of LSTM, the previous context vector, and the previous coverage vector.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setup</h3>
<p>We assess the effectiveness of our models on two typical graph-to-sequence learning tasks,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AMR15 (LDC2015E86)</td>
<td style="text-align: center;">16,833</td>
<td style="text-align: center;">1,368</td>
<td style="text-align: center;">1,371</td>
</tr>
<tr>
<td style="text-align: left;">AMR17 (LDC2017T10)</td>
<td style="text-align: center;">36,521</td>
<td style="text-align: center;">1,368</td>
<td style="text-align: center;">1,371</td>
</tr>
<tr>
<td style="text-align: left;">English-Czech</td>
<td style="text-align: center;">181,112</td>
<td style="text-align: center;">2,656</td>
<td style="text-align: center;">2,999</td>
</tr>
<tr>
<td style="text-align: left;">English-German</td>
<td style="text-align: center;">226,822</td>
<td style="text-align: center;">2,169</td>
<td style="text-align: center;">2,999</td>
</tr>
</tbody>
</table>
<p>Table 1: The number of sentences in four datasets.
including AMR-to-text generation and syntaxbased neural machine translation (NMT). For the AMR-to-text generation task, we use two benchmarks-the LDC2015E86 dataset (AMR15) and the LDC2017T10 dataset (AMR17). In these datasets, each instance contains a sentence and an AMR graph. We follow Konstas et al. (2017) to apply entity simplification in the preprocessing steps. We then transform each preprocessed AMR graph into its extended Levi graph as described in Section 3.2. For the syntax-based NMT task, we evaluate our model on both the En-De and the En-Cs News Commentary v11 dataset from the WMT16 translation task. ${ }^{2}$ We parse English sentences after tokenization to generate the dependency trees on the source side using SyntaxNet (Alberti et al., 2017). ${ }^{3}$ We tokenize Czech and German using the Moses tokenizer. ${ }^{4}$ On the target side, we use byte-pair encodings (Sennrich et al., 2016) with 8,000 merge operations to obtain subwords. We transform the labelled dependency trees into their corresponding extended Levi graphs as described in Section 3.2. Table 1 shows the statistics of these four datasets. The AMR-to-text datasets contain about 16 K $\sim 36 \mathrm{~K}$ training instances. The NMT datasets are relatively large, consisting of around 200 K training instances.</p>
<p>We tune model hyper-parameters using random layouts based on the results of the development set. We choose the number of DCGCN blocks (Block) from ${1,2,3,4}$. We select the feature dimension $d$ from ${180,240,300,360,420}$. We do not use pretrained embeddings. The encoder and the decoder share the training vocabulary. We adopt Adam (Kingma and Ba, 2015) with an initial learning rate of 0.0003 as the optimizer. The</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Main results on AMR17. #P shows the model size in terms of parameters; ' 'S'' and ' E '' denote single and ensemble models, respectively.
batch size (Batch) candidates are ${16,20,24}$. We determine when to stop training based on the perplexity change in the development set. For decoding, we use beam search with beam size 10. Through preliminary experiments, we find that the combinations $($ Block $=4, d=360$, Batch $=16)$ and (Block $=2, d=360$, Batch $=24$ ) give best results on AMR and NMT tasks, respectively. Following previous work, we evaluate the results in terms of both BLEU (B) scores (Papineni et al., 2002) and sentence-level CHRF++ (C) scores (Popovic, 2017; Beck et al., 2018). Particularly, we use case-insensitive BLEU scores for AMR and case sensitive BLEU scores for NMT. For ensemble models, we train five models with different random seeds and then use Sockeye (Felix et al., 2017) to perform default ensemble decoding.</p>
<h3>4.2 Main Results on AMR-to-text Generation</h3>
<p>We compare the performance of DCGCNs with the other three kinds of models: (1) sequence-tosequence (Seq2Seq) models, which use linearized graphs as inputs; (2) recurrent graph encoders (GGNN2Seq, GraphLSTM); (3) models trained with external resources. For convenience, we denote the LSTM-based Seq2Seq models of Konstas et al. (2017) and Beck et al. (2018) as Seq2SeqK and Seq2SeqB, respectively. GGNN2Seq (Beck et al., 2018) is the model that leverages GGNNs as graph encoders.</p>
<p>Table 2 shows the results on AMR17. Our single model achieves 27.6 BLEU points, which is the new state-of-the-art result for single models. In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources. For example, the single DCGCN model gains</p>
<p>5.9 more BLEU points than the single models of Seq2SeqB on AMR17. These results demonstrate the importance of explicitly capturing the graph structure in the encoder.</p>
<p>In addition, our single DCGCN model obtains better results than previous ensemble models. For example, on AMR17, the single DCGCN model is 1 BLEU point higher than the ensemble model of Seq2SeqB. Our model requires substantially fewer parameters (e.g., the parameter size is only $3 / 5$ and $1 / 9$ of those in GGNN2Seq and Seq2SeqB, respectively). The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6.</p>
<p>Under the same setting, our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms. For GGNN2Seq, our single model is 3.3 and 0.1 BLEU points higher than their single and ensemble models, respectively. We also have similar observations in terms of CHRF++ scores for sentence-level evaluations. DCGCN also outperforms GraphLSTM by 2.0 BLEU points in the fully supervised setting as shown in Table 3. Note that GraphLSTM uses char-level neural representations and pretrained word embeddings, whereas our model solely relies on word-level representations with random initializations. This empirically shows that compared with recurrent graph encoders, DCGCNs can learn better representations for graphs.</p>
<p>Moreover, we compare our results with the state-of-the-art semi-supervised models on the AMR15 test set (Table 3), including non-neural methods such as TSP (Song et al., 2016), PBMT (Pourdamghani et al., 2016), Tree2Str (Flanigan et al., 2016), and SNRG (Song et al., 2017). All these non-neural models train language models on the whole Gigaword corpus. Our ensemble model gives 28.2 BLEU points without external data, which is better than these other methods.</p>
<p>Following Konstas et al. (2017) and Song et al. (2018), we also evaluate our model using external Gigaword sentences as training data. We first use the additional data to pretrain the model, then fine tune it on the gold data. Using additional 0.1 M data, the single DCGCN model achieves a BLEU score of 29.0, which is higher than Seq2SeqK (Konstas et al., 2017) and GraphLSTM (Song et al., 2018) trained with 0.2 M additional data. When using the same amount of 0.2 M data, the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">External</th>
<th style="text-align: center;">B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2SeqK (Konstas et al., 2017)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: left;">GraphLSTM (Song et al., 2018)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(single)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25.7</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(ensemble)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{2 8 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">TSP (Song et al., 2016)</td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">22.4</td>
</tr>
<tr>
<td style="text-align: left;">PBMT (Pourdamghani et al., 2016)</td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">26.9</td>
</tr>
<tr>
<td style="text-align: left;">Tree2Str (Flanigan et al., 2016)</td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">23.0</td>
</tr>
<tr>
<td style="text-align: left;">SNRG (Song et al., 2017)</td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: left;">Seq2SeqK (Konstas et al., 2017)</td>
<td style="text-align: center;">0.2 M</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: left;">GraphLSTM (Song et al., 2018)</td>
<td style="text-align: center;">0.2 M</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(single)</td>
<td style="text-align: center;">0.1 M</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(single)</td>
<td style="text-align: center;">0.2 M</td>
<td style="text-align: center;">$\mathbf{3 1 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Seq2SeqK (Konstas et al., 2017)</td>
<td style="text-align: center;">2 M</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: left;">GraphLSTM (Song et al., 2018)</td>
<td style="text-align: center;">2 M</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: left;">Seq2SeqK (Konstas et al., 2017)</td>
<td style="text-align: center;">20 M</td>
<td style="text-align: center;">33.8</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(single)</td>
<td style="text-align: center;">0.3 M</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(ensemble)</td>
<td style="text-align: center;">0.3 M</td>
<td style="text-align: center;">$\mathbf{3 5 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used.
performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM, respectively. The DCGCN model is able to achieve competitive BLEU points (33.2) by using 0.3 M external data, while GraphLSTM achieves a score of 33.6 by using 2 M data and Seq2SeqK achieves a score of 33.8 by using 20 M data. These results show that our model is more effective in terms of using automatically generated AMR graphs. Using 0.3 M additional data, our ensemble model achieves the new state-of-the-art result of 35.3 BLEU points.</p>
<h3>4.3 Main Results on Syntax-based NMT</h3>
<p>Table 4 shows the results for the English-German (En-De) and English-Czech (En-Cs) translation tasks. BoW+GCN, CNN+GCN, and BiRNN+GCN refer to utilizing the following encoders with a GCN layer on top respectively: 1) a bag-ofwords encoder, 2) a one-layer CNN, and 3) a bidirectional RNN. PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007). Our single model achieves 19.0 and 12.1 BLEU points on the En-De and En-Cs tasks, respectively, significantly outperforming all the single models. For example, compared</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">English-German</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">English-Czech</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#P</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">#P</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;">BoW+GCN (Bastings et al., 2017)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CNN+GCN (Bastings et al., 2017)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BiRNN+GCN (Bastings et al., 2017)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PB-SMT (Beck et al., 2018)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: center;">Seq2SeqB (Beck et al., 2018)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">41.4 M</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">39.1 M</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">33.8</td>
</tr>
<tr>
<td style="text-align: center;">GGNN2Seq (Beck et al., 2018)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">41.2 M</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">38.8 M</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: center;">DCGCN (ours)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">29.7M</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">28.3M</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">37.1</td>
</tr>
<tr>
<td style="text-align: center;">Seq2SeqB (Beck et al., 2018)</td>
<td style="text-align: center;">Ensemble</td>
<td style="text-align: center;">207M</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">195M</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: center;">GGNN2Seq (Beck et al., 2018)</td>
<td style="text-align: center;">Ensemble</td>
<td style="text-align: center;">206M</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">194M</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">35.9</td>
</tr>
<tr>
<td style="text-align: center;">DCGCN (ours)</td>
<td style="text-align: center;">Ensemble</td>
<td style="text-align: center;">149M</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">142M</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">37.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Main results on English-German and English-Czech datasets.
with the best GCN-based model (BiRNN+GCN), our single DCGCN model surpasses it by 2.7 and 2.5 BLEU points on the En-De and En-Cs tasks, respectively. Our models consist of full GCN layers, removing the burden of using a recurrent encoder to extract non-local contextual information in the bottom layers. Compared with non-GCN models, our single DCGCN model is 2.2 and 1.9 BLEU points higher than the current state-of-the-art single model (GGNN2Seq) on the En-De and En-Cs translation tasks, respectively. In addition, our single model is comparable to the ensemble results of Seq2SeqB and GGNN2Seq, whereas the number of parameters of our models is only about $1 / 6$ of theirs. Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively. Our ensemble results are significantly higher than those of the state-of-the-art syntax-based ensemble models reported by GGNN2Seq (En-De: 20.5 vs. 19.6; En-Cs: 13.1 vs. 11.7 in terms of BLEU).</p>
<h3>4.4 Additional Experiments</h3>
<p>Layers in the Sub-block. Table 5 shows the effect of the number of layers of each subblock on the AMR15 development set. DenseNets (Huang et al., 2017) use two kinds of convolution filters: $1 \times 1$ and $3 \times 3$. Similar to DenseNets, we choose the values of $n$ and $m$ for layers from $[1,2,3,6]$. We choose this value range by considering the scale of non-local nodes, the abstract information at different level, and the calculation efficiency. For brevity, we only show representative configurations. We first investigate DCGCN with one block. In general, the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Block</th>
<th style="text-align: center;">$n$</th>
<th style="text-align: center;">$m$</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">48.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">50.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">49.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">49.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">50.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">51.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">51.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">52.1</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">53.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">53.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">52.1</td>
</tr>
</tbody>
</table>
<p>Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.
performance increases when we gradually enlarge $n$ and $m$. For example, when $n=1$ and $m=1$, the BLEU score is 17.6; when $n=6$ and $m=6$, the BLEU score becomes 22.0. We observe that the three settings $(n=6, m=3),(n=3$, $m=6)$, and $(n=6, m=6)$ give similar results for both 1 DCGCN block and 2 DCGCN blocks. Because the first two settings contain fewer parameters than the third setting, it is reasonable to choose either $(n=6, m=3)$ or $(n=3, m=6)$. For later experiments, we use $(n=6, m=3)$.</p>
<p>Comparisons with Baselines. The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA). In general, increasing</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GCN</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">C</th>
<th style="text-align: center;">GCN</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">+RC (2)</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">+RC+LA (2)</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">47.9</td>
</tr>
<tr>
<td style="text-align: left;">+RC (4)</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">+RC+LA (4)</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">51.1</td>
</tr>
<tr>
<td style="text-align: left;">+RC (6)</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">+RC+LA (6)</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">50.8</td>
</tr>
<tr>
<td style="text-align: left;">+RC (9)</td>
<td style="text-align: center;">$\mathbf{2 1 . 1}$</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">+RC+LA (9)</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
<td style="text-align: center;">52.6</td>
</tr>
<tr>
<td style="text-align: left;">+RC (10)</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">$\mathbf{5 0 . 7}$</td>
<td style="text-align: center;">+RC+LA (10)</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">$\mathbf{5 2 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN1 (9)</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">DCGCN3 (27)</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">54.7</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN2 (18)</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">DCGCN4 (36)</td>
<td style="text-align: center;">$\mathbf{2 5 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. $+\mathrm{RC}+\mathrm{LA}$ refers to GCNs with both residual connections and layer aggregations. DCGCN $i$ represents our model with $i$ blocks, containing $i \times(n+m)$ layers. The number of layers for each model is shown in parentheses.
the number of GCN layers from 2 to 9 boosts the model performance. However, when the layer number exceeds 10 , the performance of both baseline models start to drop. For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9). In preliminary experiments, we cannot manage to train very deep GCN+RC and GCN+RC+LA models. In contrast, our DCGCN models can be trained using a large number of layers. For example, DCGCN4 contains 36 layers. When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on the AMR15 development set. We therefore choose DCGCN4 for the AMR experiments. Using a similar method, DCGCN2 is selected for the NMT tasks. When the layer numbers are 9 , DCGCN1 is better than GCN+RC in term of B/C scores (21.7/51.5 vs. 21.1/50.5). GCN+RC+LA (9) is sightly better than DCGCN1. However, when we set the number to 18, GCN+RC+LA achieves a BLEU score of 19.4, which is significantly worse than the BLEU score obtained by DCGCN2 (23.3). We also try GCN+RC+LA (27), but it does not converge. In conclusion, these results show the robustness and effectiveness of our DCGCN models.</p>
<p>Performance vs. Parameter Budget. We also evaluate the performance of DCGCN model against different number of parameters on the AMR generation task. Results are shown in Figure 6. Specifically, we try four parameter budgets, including $11.8 \mathrm{M}, 14.0 \mathrm{M}, 16.2 \mathrm{M}$, and 18.4 M . These numbers correspond to the model size (in terms of number of parameters) of DCGCN1, DCGCN2,</p>
<p>DCGCN3, and DCGCN4, respectively. For each budget, we vary both the depth of GCN models and the hidden vector dimensions of each node in GCNs in order to exhaust the entire budget. For example, $G C N(2)-512, G C N(3)-426$, $G C N(4)-372$, and $G C N(5)-336$ contain about 11.8 M parameters, where $G C N(i)-d$ indicates a GCN model with $i$ layers and the hidden size for each node is $d$. We compare DCGCN1 with these four models. DCGCN1 gives 22.9 BLEU points. For the GCN models, the best result is obtained by $G C N(5)-336$, which falls behind DCGCN1 by 2.0 BLEU points. We compare DCGCN2, DCGCN3, and DCGCN4 with their equal-sized GCN models in a similar way. The results show that DCGCN consistently outperforms GCN under the same parameter budget. When the parameter budget becomes larger, we can observe that the performance difference becomes more prominent. In particular, the BLEU margins between DCGCN models and their best GCN models are 2.0, 2.7, 2.7 , and 3.4 , respectively.</p>
<p>Performance vs. Layers. We compare DCGCN models with different layers under the same parameter budget. Table 7 shows the results. For example, when both DCGCN1 and DCGCN2 are limited to 10.9 M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9). Similarly, when DCGCN3 and DCGCN4 contain 18.6 M and 18.4 M parameters, DCGCN4 outperforms DCGCN3 by 1 BLEU point with a slightly smaller model. In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones.</p>
<p>Level of Density. Table 8 shows the ablation study of the level of density of our model. We use DCGCNs with 4 dense blocks as the full model. Then we remove dense connections gradually from the last block to the first block. In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections. The full model gives 25.5 BLEU points on the AMR15 dev set. After removing the dense connections in the last block, the BLEU score becomes 24.8. Without using the dense connections in the last two blocks, the score drops to 23.8. Furthermore, excluding the dense connections in the last three blocks only gives 23.2 BLEU points. Although these four</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of DCGCN and GCN over different number of parameters. $a-b$ means the model has $a$ layers ( $a$ blocks for DCGCN) and the hidden size is $b$ (e.g., 5-336 means a 5-layer GCN with the hidden size 336).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">D</th>
<th style="text-align: left;">#P</th>
<th style="text-align: left;">B</th>
<th style="text-align: left;">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DCGCN(1)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">10.9 M</td>
<td style="text-align: left;">20.9</td>
<td style="text-align: left;">52.0</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(2)</td>
<td style="text-align: left;">180</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{2 2 . 2}$</td>
<td style="text-align: left;">$\mathbf{5 2 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(2)</td>
<td style="text-align: left;">240</td>
<td style="text-align: left;">11.3 M</td>
<td style="text-align: left;">22.8</td>
<td style="text-align: left;">52.8</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(4)</td>
<td style="text-align: left;">180</td>
<td style="text-align: left;">11.4 M</td>
<td style="text-align: left;">$\mathbf{2 3 . 4}$</td>
<td style="text-align: left;">$\mathbf{5 3 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(1)</td>
<td style="text-align: left;">420</td>
<td style="text-align: left;">12.6 M</td>
<td style="text-align: left;">22.2</td>
<td style="text-align: left;">52.4</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(2)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">12.5 M</td>
<td style="text-align: left;">23.8</td>
<td style="text-align: left;">53.8</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(3)</td>
<td style="text-align: left;">240</td>
<td style="text-align: left;">12.3 M</td>
<td style="text-align: left;">$\mathbf{2 3 . 9}$</td>
<td style="text-align: left;">$\mathbf{5 4 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(2)</td>
<td style="text-align: left;">360</td>
<td style="text-align: left;">14.0 M</td>
<td style="text-align: left;">24.2</td>
<td style="text-align: left;">$\mathbf{5 4 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(3)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{2 4 . 4}$</td>
<td style="text-align: left;">54.2</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(2)</td>
<td style="text-align: left;">420</td>
<td style="text-align: left;">15.6 M</td>
<td style="text-align: left;">24.1</td>
<td style="text-align: left;">53.7</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(4)</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{2 4 . 6}$</td>
<td style="text-align: left;">$\mathbf{5 4 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(3)</td>
<td style="text-align: left;">420</td>
<td style="text-align: left;">18.6 M</td>
<td style="text-align: left;">24.5</td>
<td style="text-align: left;">54.6</td>
</tr>
<tr>
<td style="text-align: left;">DCGCN(4)</td>
<td style="text-align: left;">360</td>
<td style="text-align: left;">18.4 M</td>
<td style="text-align: left;">$\mathbf{2 5 . 5}$</td>
<td style="text-align: left;">$\mathbf{5 5 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparisons of different DCGCN models under almost the same parameter budget.
models have the same number of layers, dense connections allow the model to achieve much better performance. If all the dense connections are not considered, the model does not coverage at all. These results indicate dense connections do play a significant role in our model.</p>
<h2>Ablation Study for Encoder and Decoder.</h2>
<p>Following Song et al. (2018), we conduct a further ablation study for modules used in the graph encoder and LSTM decoder on the AMR15 dev set, including linear combination, global node, direction aggregation, graph attention mechanism, and coverage mechanism using the 4-block models by always keeping the dense connections.</p>
<p>Table 9 shows the results. For the encoder, we find that the linear combination and the global node have more contributions in terms of B/C</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DCGCN4</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">55.4</td>
</tr>
<tr>
<td style="text-align: left;">- ${4}$ dense block</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">54.9</td>
</tr>
<tr>
<td style="text-align: left;">$-{3,4}$ dense blocks</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">54.1</td>
</tr>
<tr>
<td style="text-align: left;">$-{2,3,4}$ dense blocks</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">53.1</td>
</tr>
</tbody>
</table>
<p>Table 8: Ablation study for density of connections on the dev set of AMR15. $-{i}$ dense block denotes removing the dense connections in the $i$-th block.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DCGCN4</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">55.4</td>
</tr>
<tr>
<td style="text-align: left;">Encoder Modules</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">-Linear Combination</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">53.2</td>
</tr>
<tr>
<td style="text-align: left;">-Global Node</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">54.6</td>
</tr>
<tr>
<td style="text-align: left;">-Direction Aggregation</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">54.6</td>
</tr>
<tr>
<td style="text-align: left;">-Graph Attention</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">54.7</td>
</tr>
<tr>
<td style="text-align: left;">-Global Node\&amp;Linear Combination</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">52.4</td>
</tr>
<tr>
<td style="text-align: left;">Decoder Modules</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">-Coverage Mechanism</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">53.0</td>
</tr>
</tbody>
</table>
<p>Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder.
scores. The results drop by $2 / 2.2$ and $1.3 / 1.2$ points, respectively, after removing them. Without these two components, our model gives a BLEU score of 22.6 , which is still better than the best GCN+RC model (21.1) and the best GCN+RC+LA model (22.1). Adding either the global node or the linear combination improves the baseline models with only dense connections. This suggests that enriching input graphs with the global node and including the linear combination can facilitate GCNs to learn better information aggregations,</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: CHRF++ scores with respect to the input graph size for three models.
producing more expressive graph representations. Results also show the linear combination is more effective than the global node. Considering them together further enhances the model performance. After removing the graph attention module, our model gives 24.9 BLEU points. Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points. The coverage mechanism is also effective in our models. Without the coverage mechanism, the result drops by $1.7 / 2.4$ points for $\square / \subset$ scores.</p>
<h3>4.5 Analysis and Discussion</h3>
<p>Graph Size. Following Bastings et al. (2017), we show in Figure 7 the CHRF++ score variations according to the graph size $|G|$ on the AMR2015 development set, where $|G|$ refers to the number of nodes in the extended Levi graph. We bin the graph size into five classes $(\leq 30,(30,40]$, $(40,50],(50,60],&gt;60)$. We average the sentencelevel CHRF++ scores of the sentences in the same bin to plot Figure 7. For small graphs (i.e., $|G| \leq 30$ ), DCGCN obtains similar results as the baselines. For large graphs, DCGCN significantly outperforms the two baselines. In general, as the graph size increases, the gap between DCGCN and the two baselines becomes larger. In addition, we can also notice that the margin between GCN and GCN+LA is quite stable, while the margin between DCGCN and GCN+LA varies according to the graph size. The trend for BLEU scores is similar to CHRF++ scores. This suggests that DCGCN can perform better for larger graphs as its deeper architecture can capture the longdistance dependencies. Dense connections facilitate information propagation in large graphs, while</p>
<div class="codehilite"><pre><span></span><code>(s / state-01
    :ARG0 (p / person
        :ARG0-of (b / have-org-role-91
            :ARG1 (i / intelligence
                :mod (c / country :wiki &quot;united_states&quot;
                :name (n/ name :op1 &quot;u.s.&quot;)))
            :ARG2 (o / official)))
:ARG1 (c2 / continue-01
    :ARG0 (p2 / person
        :ARG0-of (b2 / have-org-role-91
            :ARG2 (o2 / official
                :mod (c3 / country :wiki &quot;north_korea&quot;
                :name (n2 / name :op1 &quot;north&quot; :op2
                &quot;korea&quot;)))))
    :ARG1 (t / trade-01
        :ARG1 (i2 / technology
            :purpose (w / weapon
            :ARG2-of (d / destroy-01
                :degree (m / mass))))
            :mod (g / globe))
        :ARG2-of (i2 / include-01
        :ARG1 (i3 / instruct-01
            :ARG3 (m2 / make-01
                :ARG1 (m3 / missile
                    :ARG1-of (a / advanced-02)))))))
</code></pre></div>

<p>Reference: u.s. intelligence officials stated that north korean officials are continuing global trade in technology for weapons of mass destruction including instructions for making advanced missiles.</p>
<p>GCN+RC: a u.s. intelligence official stated that north korea officials continued the global trade for weapons of mass destruction by making advanced missiles to make advanced missiles.</p>
<p>GCN+RC+LA: a u.s. intelligence official stated that north korea officials continued global trade with weapons of mass destruction including making advanced missiles.</p>
<p>DCGCN: a u.s. intelligence official stated that north korea officials continue global trade on technology for weapons of mass destruction including instructions to make advanced missiles.</p>
<p>Table 10: Example outputs.
shallow GCNs might struggle to capture such dependencies.</p>
<p>Example Output. Table 10 shows example outputs from three models for the AMR-to-text task, together with the corresponding AMR graph as well as the text reference. The word "technology" in the reference acts as a link between "global trade" and "weapons of mass destruction", offering the background knowledge to help understand the context. The word "instructions" also plays a crucial role in the generated sentence - without the word the sentence will have a significantly different meaning. Both GCN+RC and GCN+RC+LA fail to successfully generate these two important words. The output from GCN+RC does not even appear to be grammatically correct. In contrast, DCGCN manages to generate both words. We believe this is because DCGCN is able to learn richer semantic information by capturing complex long dependencies. GCN+RC+LA does generate an output that looks similar to the reference at</p>
<p>the token level. However, the conveyed semantic information in the generated sentence largely differs from that of the reference. DCGCNs do not have this problem.</p>
<h2>5 Related Work</h2>
<p>Our work builds on a rich line of recent efforts on graph-to-sequence models, graph convolutional networks, and densely connected convolutional networks.</p>
<p>Graph-to-Sequence Learning. Early research efforts for graph-to-sequence learning are based on statistical methods. Lu et al. (2009) present a language generation model using the tree-structured meaning representation based on tree conditional random fields. Lu and Ng (2011) propose a model for language generation from lambda calculus expressions that can be represented as forest structures. Konstas and Lapata (2012, 2013) leverage hypergraphs for concept-to-text generation. Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it into a sentence using a tree-to-string transducer. Pourdamghani et al. (2016) adopt a phrase-based model for machine translation (Koehn et al., 2003) based on a linearized AMR graph. Song et al. (2017) leverage a synchronous node replacement grammar. Konstas et al. (2017) also linearize the input graph and feed it to the Seq2Seq model (Sutskever et al., 2014).</p>
<p>Sequence-based neural networks may lose structural information from the original graph because they require linearization of the input graph. Recent research efforts consider developing encoders with graph neural networks. Beck et al. (2018) use GGNNs (Li et al., 2016) as the encoder and introduce the Levi graph that allows nodes and edges to have their own hidden representations. Song et al. (2018) propose the graph-state LSTM to directly encode graph-level semantics. In order to capture non-local information, the encoder performs graph state transition by information exchange between connected nodes. Their work belongs to the family of RNNs. Our graph encoder is built based on GCNs. Recurrent graph neural networks (Li et al., 2016; Song et al., 2018) use gated operations to update node states whereas graph convolutional networks use linear transformation. The contrast between our model and theirs is reminiscent of the contrast between CNN and RNN.</p>
<p>Closest to our work, Bastings et al. (2017) stack GCNs upon a RNN or CNN encoder because 2-layer GCNs may not be able to capture nonlocal information, especially when the graph is large. Our graph encoder solely relies on the DCGCN model, whose deep network structure encodes richer local and non-local information for learning better graph representations.</p>
<h2>Densely Connected Convolutional Networks.</h2>
<p>Intuitively, neural networks should be able to learn rich representations by stacking a large number of layers. However, empirical results often do not support such an intuition-useful information captured in earlier layers may get lost after passing through subsequent layers. Many recent efforts focus on resolving such an issue. Highway Networks (Srivastava et al., 2015) use bypassing paths along with gating units to train networks. ResNets (He et al., 2016), in which identity mappings are used as bypassing paths, have achieved impressive performance on various tasks. DenseNets (Huang et al., 2017) refine this insight and propose a dense connectivity strategy, which connects all layers directly with each other to ensure maximum information flow between layers.</p>
<p>Graph Convolutional Networks. Early efforts that attempt to extend neural networks to deal with arbitrary structured graphs are introduced by Gori et al. (2005) and Scarselli et al. (2009), where the states of nodes are updated based on the states of their neighbors. Bruna (2014) then applies the convolution operation on graph Laplacians to construct efficient architectures in the spectral domain. Subsequent efforts improve its computational efficiency with local spectral convolution techniques (Henaff et al., 2015; Defferrard et al., 2016; Kipf and Welling, 2017).</p>
<p>Our approach is closely related to GCNs (Kipf and Welling, 2017), which restrict the filters to operate on a first-order neighborhood around each node. Recent improvements and extensions of GCNs include using additional aggregation methods such as vertex attention (Velickovic et al., 2018) or pooling mechanism (Hamilton et al., 2017) to better summarize neighborhood states.</p>
<p>However, the best performance of GCNs is achieved with a 2-layer model, while deeper models perform worse though they can potentially have access to more non-local information. Li et al. (2018) show that this issue is due to the</p>
<p>over-smoothed output representations that impede distinguishing nodes from different clusters. Recent attempts that try to address this issue includes the use of layer-aggregation functions (Xu et al., 2018), which combine learned features from all layers, and the use of co-training and self-training mechanisms that encourage exploration on the entire graph (Li et al., 2018).</p>
<h2>6 Conclusion</h2>
<p>We introduce the novel densely connected graph convolutional networks to learn structural graph representations. Experimental results show that DCGCNs can outperform state-of-the-art models in two tasks: AMR-to-text generation and syntaxbased neural machine translation. Unlike previous designs of GCNs, DCGCNs scale naturally to significantly more layers without suffering from performance degradation and optimization difficulties, thanks to the introduced dense connectivity mechanism. Such a deep architecture allows the encoder to better capture the rich structural information of a graph, especially when it is large.</p>
<p>There are multiple venues for future work. One natural question we would like to ask is how to make use of the proposed framework to perform improved graph representation learning for various graph related tasks (Xu et al., 2018). On the other hand, we would also like to investigate how other NLP applications such as relation extraction (Zhang et al., 2018b) and semantic role labeling (Marcheggiani and Titov, 2017) can potentially benefit from our proposed approach.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the anonymous reviewers and our Action Editor Stefan Riezler for their comments and suggestions on this work. We would also like to thank Daniel Beck, Linfeng Song, Joost Bastings, Zuozhu Liu, and Yiluan Guo for their helpful suggestions. This work is supported by Singapore Ministry of Education Academic Research Fund (AcRF) Tier 2 Project MOE2017-T2-1-156. This work is also partially supported by SUTD project PIE-SGP-AI-2018-01.</p>
<h2>References</h2>
<p>Chris Alberti, Daniel Andor, Ivan Bogatyy, Michael Collins, Daniel Gillick, Lingpeng Kong,</p>
<p>Terry Koo, Ji Ma, Mark Omernick, Slav Petrov, Chayut Thanapirom, Zora Tung, and David Weiss. 2017. Syntaxnet models for the conll 2017 shared task. arXiv preprint arxiv:1703.04929.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR.</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of LAW@ACL.</p>
<p>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima'an. 2017. Graph convolutional encoders for syntax-aware neural machine translation. In Proceedings of EMNLP.</p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of $A C L$.</p>
<p>Joan Bruna. 2014. Spectral networks and deep locally connected networks on graphs. In Proceedings of ICLR.</p>
<p>Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arxiv:1512.01274.</p>
<p>Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In Proceedings of NIPS.</p>
<p>Jeffrey L. Elman. 1990. Finding structure in time. Cognitive Science, 14(2):179-211.</p>
<p>Hieber Felix, Domhan Tobias, Denkowski Michael, Vilar David, Sokolov Artem, Clifton Ann, and Post Matt. 2017. Sockeye: A toolkit for neural machine translation. arXiv preprint arxiv:1712.05690.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime G. Carbonell. 2016. Generation from</p>
<p>abstract meaning representation using tree transducers. In Proceedings of NAACL-HLT.</p>
<p>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. 2017. Convolutional sequence to sequence learning. In Proceedings of ICML.</p>
<p>Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of CVPR.</p>
<p>Michele Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for learning in graph domains. In Proceedings of IJCNN.</p>
<p>Jonathan L. Gross, Jay Yellen, and Ping Zhang. 2013. Handbook of Graph Theory, Second Edition. Chapman \&amp; Hall/CRC.</p>
<p>Zhijiang Guo and Wei Lu. 2018. Better transitionbased AMR parsing with a refined search space. In Proceedings of EMNLP.</p>
<p>William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Proceedings of NIPS.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of CVPR.</p>
<p>Mikael Henaff, Joan Bruna, and Yann LeCun. 2015. Deep convolutional networks on graph-structured data. arXiv preprint arxiv:1506.05163.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.</p>
<p>Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. 2017. Densely connected convolutional networks. In Proceedings of CVPR.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of $I C L R$.</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In Proceedings of $I C L R$.</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL(Demo).</p>
<p>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL-HLT.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke S. Zettlemoyer. 2017. Neural amr: Sequence-to-sequence models for parsing and generation. In Proceedings of $A C L$.</p>
<p>Ioannis Konstas and Mirella Lapata. 2012. Unsupervised concept-to-text generation with hypergraphs. In Proceedings of NAACL-HLT.</p>
<p>Ioannis Konstas and Mirella Lapata. 2013. Inducing document plans for concept-to-text generation. In Proceedings of EMNLP.</p>
<p>Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of AAAI.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated graph sequence neural networks. In Proceedings of $I C L R$.</p>
<p>Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of EMNLP.</p>
<p>Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Natural language generation with tree conditional random fields. In Proceedings of EMNLP.</p>
<p>Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for semantic role labeling. In Proceedings of EMNLP.</p>
<p>Vinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of ICML.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke S. Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT.</p>
<p>Maja Popovic. 2017. chrf++: Words helping character $n$-grams. In Proceedings of WMT@ACL.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating english from abstract meaning representations. In Proceedings of INLG.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of $A C L$.</p>
<p>Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. AMR-to-text generation with synchronous node replacement grammar. In Proceedings of ACL.</p>
<p>Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo Wang, and Daniel Gildea. 2016. AMR-to-text generation as a traveling salesman problem. In Proceedings of EMNLP.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-tosequence model for AMR-to-text generation. In Proceedings of ACL.</p>
<p>Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. 2015. Training very deep networks. In Proceedings of NIPS.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of NIPS.</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neural machine translation. In Proceedings of $A C L$.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NIPS.</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In Proceedings of ICLR.</p>
<p>Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken ichi Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs with jumping knowledge networks. In Proceedings of ICML.</p>
<p>Yue Zhang, Qi Liu, and Linfeng Song. 2018a. Sentence-state LSTM for text representation. In Proceedings of ACL.</p>
<p>Yuhao Zhang, Peng Qi, and Christopher D. Manning. 2018b. Graph convolution over pruned dependency trees improves relation extraction. In Proceedings of EMNLP.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ http://www.statmt.org/wmt16/translationtask.html.
${ }^{3}$ https://github.com/tensorflow/models/tree/ master/research/syntaxnet.
${ }^{4}$ https://github.com/moses-smt/mosesdecoder.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
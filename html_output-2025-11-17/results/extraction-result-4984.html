<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4984 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4984</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4984</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264146296</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09430v4.pdf" target="_blank">Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness when performing logical reasoning has not been sufficiently assessed. To comprehensively evaluate this ability, we develop three new logical reasoning datasets named"ReClor-plus","LogiQA-plus"and"LogiQAv2-plus"that extend standard logical reasoning datasets to evaluate the robustness of the LLM's reasoning. For each, we create three subsets: the first with randomly shuffled options, the second with the correct choices replaced by"none of the other options is correct", and the third with a combination of shuffling and substitution. Experiments on these datasets show that these simple augmentations greatly hinder the models' performance. Despite their high performance on the original publicly available datasets, we find that all models perform poorly on these newly constructed datasets. We also demonstrate that introducing task variations into the training set can markedly improve the model's performance on both the original and our developed datasets. Finally, we show that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning. Source code and data are made publicly available at https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4984.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4984.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large autoregressive transformer from OpenAI (next-token prediction) evaluated zero-shot and with prompting; shows strong performance on original multiple-choice logical reasoning datasets but substantial degradation under task-structure perturbations and only limited gains from Chain-of-Thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer decoder model trained by OpenAI for next-token prediction; used in this paper in zero-shot and instruction-prompted settings (including Chain-of-Thought) and with prompt augmentation. Exact pretraining data and weights not public.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and the paper's perturbed variants (ReClor-plus, LogiQA-plus, LogiQAv2-plus using Shuffle-Order, Replace-Answer, Shuffle-RepAns)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice reading-comprehension style logical reasoning questions (GMAT/LSAT/civil service exam style); requires identifying the single logically entailed option and handling negation/implication/paraphrase.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and instruction prompting (standard instruction format), Chain-of-Thought (CoT) prompting, and prompt augmentation with logic-driven (AMR-LDA) extended context/options.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strong on original validation sets (highest among evaluated models on original sets in this paper); zeroshot accuracies reported in Table 2: ReClor ≈0.1312, LogiQA ≈0.1626, LogiQAv2 ≈0.1686. Performance dropped substantially on perturbed variants (Shuffle-Order/Replace-Answer/Shuffle-RepAns), sometimes near random. CoT prompting produced modest gains (not consistent) — e.g., GPT-4-CoT improved some Shuffle-RepAns cases (Table 2: ReClor-CoT ≈0.1816, LogiQA-CoT ≈0.2523, LogiQAv2-CoT ≈0.2177). Prompt augmentation using AMR-LDA (augment test prompts) yielded larger improvements in some settings (reported prompt-augmentation results: GPT-4-IPT-LDA ReClor ≈0.8766, LogiQA ≈0.5510, LogiQAv2 ≈0.7027 in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large drop when options are shuffled or when correct answer is replaced with 'none of the other options is correct' (Replace-Answer, Shuffle-RepAns). CoT helps on some examples but does not systematically resolve robustness failures; instruction fine-tuning could not be applied to GPT-4 in this work (no access to weights). Susceptible to dataset contamination/memorisation for datasets published prior to model pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms Alpaca-7B and Vicuna-7B in zero-shot on original datasets. When fine-tuned discriminative models merge large training sets (MERIt, AMR-LDA), those discriminative models can match or exceed GPT-4 on some tasks. Prompt augmentation with AMR-LDA helps GPT-4 more than logic-driven augmentation applied to next-token training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>CoT prompting produces mixed results (only GPT-4 shows systematic but limited improvements on Shuffle-RepAns). Prompt augmentation (augmented test prompt) yields strong gains; adding logic-driven data augmentation into next-token fine-tuning harms generative models' performance. Model-size ablation not applicable (GPT-4 single size in study).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4984.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4984.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI autoregressive next-token model evaluated zero-shot, with Chain-of-Thought prompting and with prompt augmentation; performs well on original datasets but drops on perturbed variants and shows mixed response to CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer variant fine-tuned with RLHF on top of GPT-3 family; used in zero-shot and instruction-prompted evaluations in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and their perturbed variants (Shuffle-Order, Replace-Answer, Shuffle-RepAns)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice reading-comprehension logical reasoning requiring deduction from natural language premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting, Chain-of-Thought prompting, and prompt augmentation using AMR-LDA-extended inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot accuracies reported in Table 2: ReClor ≈0.1847, LogiQA ≈0.2286, LogiQAv2 ≈0.2583. Performance degrades markedly on perturbed sets (especially Shuffle-RepAns). Prompt augmentation (AMR-LDA on test prompt) improved results in some cases (GPT-3.5-IPT-LDA reported improvements in Table 3: e.g., LogiQAv2 ≈0.4971).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performs poorly on Shuffle-RepAns and Replace-Answer variants (near random in many settings). Chain-of-Thought prompting did not consistently help and in some reported cases reduced performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms open-source Alpaca/Vicuna zero-shot on original datasets but can be surpassed by discriminative fine-tuned models when those are trained on merged datasets. Prompt augmentation benefits GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>CoT often did not help; AMR-LDA prompt augmentation helps. Including logic-driven augmented data into next-token training harms generative models' performance (table/analysis in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4984.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4984.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (7B) - instruction-finetuned LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-fine-tuned variant of LLaMA (7B) used in this paper both zero-shot and after instruction fine-tuning on logical reasoning tasks; fine-tuning substantially improves performance, approaching GPT-4 on merged-training experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following model derived from LLaMA weights; autoregressive decoder architecture (LLaMA backbone); evaluated both zero-shot and after further instruction fine-tuning (IFT) on ReClor/LogiQA/LogiQAv2 and perturbed variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and perturbed variants (Shuffle-Order, Replace-Answer, Shuffle-RepAns); transfer/finetuning experiments with varying ratios of perturbed data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice logical reading comprehension; tests for permutation invariance and understanding of 'none of the above' by replacing correct options.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Instruction fine-tuning (IFT) on the datasets, instruction prompting (zero-shot and CoT), logic-driven data augmentation (AMR-LDA) applied either into training or applied as prompt augmentation for test-time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot poor (below random on some perturbed sets). After instruction fine-tuning on individual or merged training sets, accuracy improved significantly; fine-tuned Alpaca-7B approached GPT-4 on merged training sets. Reported examples: Alpaca-7B-IFT + AMR-LDA in Table 3 shows high gains when AMR-LDA used appropriately (e.g., Alpaca-7B-IFT-LDA ReClor ≈0.4800 in prompt-augmentation setting). Performance improves when a large training set (LogiQAv2 >10k) includes substantial perturbed examples (≈50% Shuffle-RepAns) — this improved generalisation across formats.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero-shot and small-fine-tune regimes fail on task-structure perturbations. Overfitting risk observed when high proportions of perturbed data are used on small datasets — causing declines on some formats (e.g., Shuffle-Order). Logic-driven augmentation inserted directly into next-token training can harm generative models unless applied as prompt augmentation or balanced carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>After IFT and with merged data, Alpaca-7B can approach GPT-4’s performance; but on perturbed sets still trails best discriminative models in some settings. Responds better than Vicuna-7B in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Transfer learning with varying perturbation ratios shows benefits only with large training sets (>10k) and high perturbation ratio (≈50%). Smaller training sets do not benefit and may degrade performance. Logic-driven prompt augmentation helps; logic-driven training augmentation often hurts next-token models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4984.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4984.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (7B) - instruction-tuned LLaMA derivative</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source instruction-following model (LLaMA-based) evaluated zero-shot and after instruction fine-tuning; underperforms GPT-3.5/GPT-4 on original datasets and shows inconsistent improvements from fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational model based on LLaMA; used in zero-shot and IFT comparisons in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and perturbed variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice logical reasoning reading-comprehension tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot, instruction prompting, and instruction fine-tuning (using the official Vicuna/Alpaca repos); Chain-of-Thought prompting experimented.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot below GPT-3.5/GPT-4 on original sets; fine-tuning led to improvements but results variable and sometimes below random on perturbed variants in zero-shot. Table 2 entries show low base numbers (e.g., Vicuna-7B ReClor zero-shot ≈0.0640). CoT prompting had inconsistent effect (some increases in some splits).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails many perturbed variants in zero-shot; fine-tuning not consistently effective across datasets; often lags Alpaca-7B after IFT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performs worse than GPT-3.5/GPT-4 and Alpaca-7B in this paper's evaluations; similar trends to Alpaca but lower baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>CoT prompting slightly changed results but did not reliably improve robustness; instruction fine-tuning helps but variability remains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4984.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4984.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B–65B) family</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open weights family of decoder-only transformer models (LLaMA) evaluated in size-ablation experiments (7B to 65B) showing no consistent correlation between model parameter count and robustness on perturbed logical-reasoning tasks when used zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (7B–65B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer models from Meta available in multiple sizes; evaluated here in zero-shot across LLaMA-7B, up to LLaMA-65B on the Shuffle-RepAns perturbed format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–65B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Shuffle-RepAns variants of ReClor/LogiQA/LogiQAv2 (robustness-focused perturbation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Perturbed multiple-choice logical reasoning where options are shuffled and correct replaced (Shuffle-RepAns) to test permutation invariance and real reasoning vs. memorisation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation with varying base sizes (no instruction fine-tuning in size ablation described).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No significant difference observed across sizes in zero-shot on Shuffle-RepAns format (Table 5): larger parameter counts (7B→65B) did not reliably increase robustness on these perturbed logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Scaling within the LLaMA family did not confer robustness to task-structure changes in zero-shot setting; exposes that parameter count alone does not guarantee improved logical generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Contrary to typical scaling benefits, LLaMA-65B did not substantially outperform LLaMA-7B on Shuffle-RepAns; thus other factors (training data diversity, instruction fine-tuning, augmentation) more determinative.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Size ablation across LLaMA variants indicates little-to-no correlation between size and robustness on perturbed logical-reasoning inputs under zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4984.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4984.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative encoder-based model fine-tuned for logical multi-choice reasoning tasks that uses logic-driven augmentation and achieves strong leaderboard performance; shows permutation invariance and stable performance on Shuffle-Order but declines on Replace-Answer variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LReasoner (ALBERT-XXLarge-v2 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Discriminative (encoder-only) transformer model using ALBERT-XXLarge-v2 backbone, fine-tuned for classification on multiple-choice logical reasoning tasks; trained with concatenated (context + [SEP] + question + [SEP] + option) inputs per candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA (and perturbed variants), used as fine-tuned discriminative baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice logical reading-comprehension requiring selection of single correct option via classification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning on task training sets, logic-driven data augmentation methods in original LReasoner work are referenced; evaluated on original and perturbed formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strong performance on original sets (competitive with best models) and stable performance on Shuffle-Order due to input concatenation scheme (permutation invariance). However, substantial drop in performance on Replace-Answer and Shuffle-RepAns variants reported in the paper (see main results summary and Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Significant decline on Replace-Answer and Shuffle-RepAns; suggests limited ability to detect 'none of the above' or to generalise beyond memorised correlations despite permutation invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Among discriminative models, LReasoner is competitive and shows stability on Shuffle-Order; however, on some tasks MERIt and AMR-LDA approach or exceed GPT-4 when merged training sets are used.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Permutation invariance attributed to input formatting (separate concatenations per option) — explains robustness to Shuffle-Order but not to Replace-Answer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4984.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4984.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MERIt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MERIt (Meta-path guided contrastive learning for logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative encoder model that performs intermediate training via relation extraction on large corpora and achieves top performance on ReClor leaderboard; improves generalisation in logical reasoning when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MERIt (DeBERTaV2-XXLarge backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only discriminative model using DeBERTaV2-XXLarge backbone; employs relation extraction and intermediate training to construct auxiliary data before fine-tuning on downstream logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA (and perturbed variants) used as discriminative baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice logical reasoning (reading comprehension) emphasizing rule extraction and deduction from text.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Intermediate training on constructed data (relation-extraction derived) followed by fine-tuning on target logical reasoning datasets; evaluated on perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported strong performance on original datasets; MERIt achieved #1 on ReClor leaderboard at time of proposal. In this paper, MERIt performed commendably and sometimes surpassed GPT-4 when training sets were merged.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Like other discriminative models, stable on Shuffle-Order but significant performance drops on Replace-Answer and Shuffle-RepAns; shows reliance on training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Comparable or superior to GPT-4 on some fine-tuned merged-data experiments; better generalisation than vanilla generative models when fine-tuned discriminatively.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Demonstrates benefit of intermediate training and augmented/constructed training data for discriminative models; but susceptible to shifts in task structure (Replace-Answer).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4984.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4984.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-LDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-LDA (AMR-based Logic-Driven Data Augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative model and (importantly) an augmentation method that uses AMR and logical equivalence laws to generate logically equivalent/inequivalent extended options and contexts; improves discriminative fine-tuned models but can harm next-token generative training unless used for prompt augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhancing logical reasoning of large language models through logic-driven data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AMR-LDA (method and model; DeBERTaV2-XXLarge backbone for discriminative variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach that leverages AMR parsing to construct logically equivalent/inequivalent sentences (extended options/contexts) following logical equivalence laws; used both as a discriminative model (AMR-LDA) and as a logic-driven data augmentation technique for training and prompt augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and perturbed variants; both used to augment training data and as prompt-augmentation at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice logical reasoning where augmentation creates extended-option/context pairs via logical equivalence/contraposition/implication laws to expose logical structures.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>AMR parsing + logical-equivalence transformations to generate extended options and extended contexts appended to inputs; used in three ways in experiments: (a) augment training data for discriminative fine-tuning, (b) augment training data for generative next-token fine-tuning (which harmed performance), (c) prompt augmentation at test/inference time for generative models (which improved performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Improves discriminative model performance (AMR-LDA and LReasoner) on standard and some perturbed formats when used in training; however, inserting AMR-LDA augmented data into next-token training of generative LLMs degraded performance (Table 3). Prompt augmentation (adding AMR-LDA extended options/contexts to test prompt) improved generative model performance substantially in reported experiments (e.g., GPT-4-IPT-LDA high gains in Table 3 and Alpaca-7B-IFT-LDA gains).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Directly adding AMR-LDA augmented data into next-token (generative) model training distorted the training distribution and hurt generalisation. Logical augmentation must be applied carefully and is most effective for discriminative fine-tuning or as prompt augmentation for large generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>AMR-LDA fine-tuned discriminative models achieve top leaderboard performance and are more robust than vanilla generative models on some tasks; but naïve usage with generative next-token training is detrimental.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper reports that logic-driven augmentation in training helps discriminative LMs but harms generative next-token models; prompt augmentation (apply AMR-LDA only at test-time input) helps GPT-3.5/GPT-4 and fine-tuned Alpaca-7B. Also provides case studies illustrating AMR-LDA transformations (appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA 2.0 - an improved dataset for logical reasoning in natural language understanding <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Enhancing logical reasoning of large language models through logic-driven data augmentation <em>(Rating: 2)</em></li>
                <li>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning <em>(Rating: 2)</em></li>
                <li>LReasoner <em>(Rating: 1)</em></li>
                <li>RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4984",
    "paper_id": "paper-264146296",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A proprietary large autoregressive transformer from OpenAI (next-token prediction) evaluated zero-shot and with prompting; shows strong performance on original multiple-choice logical reasoning datasets but substantial degradation under task-structure perturbations and only limited gains from Chain-of-Thought prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Autoregressive transformer decoder model trained by OpenAI for next-token prediction; used in this paper in zero-shot and instruction-prompted settings (including Chain-of-Thought) and with prompt augmentation. Exact pretraining data and weights not public.",
            "model_size": null,
            "logical_reasoning_task": "ReClor, LogiQA, LogiQAv2 and the paper's perturbed variants (ReClor-plus, LogiQA-plus, LogiQAv2-plus using Shuffle-Order, Replace-Answer, Shuffle-RepAns)",
            "task_description": "Multiple-choice reading-comprehension style logical reasoning questions (GMAT/LSAT/civil service exam style); requires identifying the single logically entailed option and handling negation/implication/paraphrase.",
            "method_or_approach": "Zero-shot and instruction prompting (standard instruction format), Chain-of-Thought (CoT) prompting, and prompt augmentation with logic-driven (AMR-LDA) extended context/options.",
            "performance": "Strong on original validation sets (highest among evaluated models on original sets in this paper); zeroshot accuracies reported in Table 2: ReClor ≈0.1312, LogiQA ≈0.1626, LogiQAv2 ≈0.1686. Performance dropped substantially on perturbed variants (Shuffle-Order/Replace-Answer/Shuffle-RepAns), sometimes near random. CoT prompting produced modest gains (not consistent) — e.g., GPT-4-CoT improved some Shuffle-RepAns cases (Table 2: ReClor-CoT ≈0.1816, LogiQA-CoT ≈0.2523, LogiQAv2-CoT ≈0.2177). Prompt augmentation using AMR-LDA (augment test prompts) yielded larger improvements in some settings (reported prompt-augmentation results: GPT-4-IPT-LDA ReClor ≈0.8766, LogiQA ≈0.5510, LogiQAv2 ≈0.7027 in Table 3).",
            "limitations_or_failure_cases": "Large drop when options are shuffled or when correct answer is replaced with 'none of the other options is correct' (Replace-Answer, Shuffle-RepAns). CoT helps on some examples but does not systematically resolve robustness failures; instruction fine-tuning could not be applied to GPT-4 in this work (no access to weights). Susceptible to dataset contamination/memorisation for datasets published prior to model pretraining.",
            "comparison": "Outperforms Alpaca-7B and Vicuna-7B in zero-shot on original datasets. When fine-tuned discriminative models merge large training sets (MERIt, AMR-LDA), those discriminative models can match or exceed GPT-4 on some tasks. Prompt augmentation with AMR-LDA helps GPT-4 more than logic-driven augmentation applied to next-token training.",
            "ablation_or_analysis_results": "CoT prompting produces mixed results (only GPT-4 shows systematic but limited improvements on Shuffle-RepAns). Prompt augmentation (augmented test prompt) yields strong gains; adding logic-driven data augmentation into next-token fine-tuning harms generative models' performance. Model-size ablation not applicable (GPT-4 single size in study).",
            "uuid": "e4984.0",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-Turbo (OpenAI)",
            "brief_description": "An earlier OpenAI autoregressive next-token model evaluated zero-shot, with Chain-of-Thought prompting and with prompt augmentation; performs well on original datasets but drops on perturbed variants and shows mixed response to CoT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Autoregressive transformer variant fine-tuned with RLHF on top of GPT-3 family; used in zero-shot and instruction-prompted evaluations in this paper.",
            "model_size": null,
            "logical_reasoning_task": "ReClor, LogiQA, LogiQAv2 and their perturbed variants (Shuffle-Order, Replace-Answer, Shuffle-RepAns)",
            "task_description": "Multiple-choice reading-comprehension logical reasoning requiring deduction from natural language premises.",
            "method_or_approach": "Zero-shot prompting, Chain-of-Thought prompting, and prompt augmentation using AMR-LDA-extended inputs.",
            "performance": "Zero-shot accuracies reported in Table 2: ReClor ≈0.1847, LogiQA ≈0.2286, LogiQAv2 ≈0.2583. Performance degrades markedly on perturbed sets (especially Shuffle-RepAns). Prompt augmentation (AMR-LDA on test prompt) improved results in some cases (GPT-3.5-IPT-LDA reported improvements in Table 3: e.g., LogiQAv2 ≈0.4971).",
            "limitations_or_failure_cases": "Performs poorly on Shuffle-RepAns and Replace-Answer variants (near random in many settings). Chain-of-Thought prompting did not consistently help and in some reported cases reduced performance.",
            "comparison": "Outperforms open-source Alpaca/Vicuna zero-shot on original datasets but can be surpassed by discriminative fine-tuned models when those are trained on merged datasets. Prompt augmentation benefits GPT-3.5.",
            "ablation_or_analysis_results": "CoT often did not help; AMR-LDA prompt augmentation helps. Including logic-driven augmented data into next-token training harms generative models' performance (table/analysis in paper).",
            "uuid": "e4984.1",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Alpaca-7B",
            "name_full": "Alpaca (7B) - instruction-finetuned LLaMA",
            "brief_description": "An open-source instruction-fine-tuned variant of LLaMA (7B) used in this paper both zero-shot and after instruction fine-tuning on logical reasoning tasks; fine-tuning substantially improves performance, approaching GPT-4 on merged-training experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B",
            "model_description": "Instruction-following model derived from LLaMA weights; autoregressive decoder architecture (LLaMA backbone); evaluated both zero-shot and after further instruction fine-tuning (IFT) on ReClor/LogiQA/LogiQAv2 and perturbed variants.",
            "model_size": "7B",
            "logical_reasoning_task": "ReClor, LogiQA, LogiQAv2 and perturbed variants (Shuffle-Order, Replace-Answer, Shuffle-RepAns); transfer/finetuning experiments with varying ratios of perturbed data.",
            "task_description": "Multiple-choice logical reading comprehension; tests for permutation invariance and understanding of 'none of the above' by replacing correct options.",
            "method_or_approach": "Instruction fine-tuning (IFT) on the datasets, instruction prompting (zero-shot and CoT), logic-driven data augmentation (AMR-LDA) applied either into training or applied as prompt augmentation for test-time.",
            "performance": "Zero-shot poor (below random on some perturbed sets). After instruction fine-tuning on individual or merged training sets, accuracy improved significantly; fine-tuned Alpaca-7B approached GPT-4 on merged training sets. Reported examples: Alpaca-7B-IFT + AMR-LDA in Table 3 shows high gains when AMR-LDA used appropriately (e.g., Alpaca-7B-IFT-LDA ReClor ≈0.4800 in prompt-augmentation setting). Performance improves when a large training set (LogiQAv2 &gt;10k) includes substantial perturbed examples (≈50% Shuffle-RepAns) — this improved generalisation across formats.",
            "limitations_or_failure_cases": "Zero-shot and small-fine-tune regimes fail on task-structure perturbations. Overfitting risk observed when high proportions of perturbed data are used on small datasets — causing declines on some formats (e.g., Shuffle-Order). Logic-driven augmentation inserted directly into next-token training can harm generative models unless applied as prompt augmentation or balanced carefully.",
            "comparison": "After IFT and with merged data, Alpaca-7B can approach GPT-4’s performance; but on perturbed sets still trails best discriminative models in some settings. Responds better than Vicuna-7B in these experiments.",
            "ablation_or_analysis_results": "Transfer learning with varying perturbation ratios shows benefits only with large training sets (&gt;10k) and high perturbation ratio (≈50%). Smaller training sets do not benefit and may degrade performance. Logic-driven prompt augmentation helps; logic-driven training augmentation often hurts next-token models.",
            "uuid": "e4984.2",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Vicuna-7B",
            "name_full": "Vicuna (7B) - instruction-tuned LLaMA derivative",
            "brief_description": "Open-source instruction-following model (LLaMA-based) evaluated zero-shot and after instruction fine-tuning; underperforms GPT-3.5/GPT-4 on original datasets and shows inconsistent improvements from fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_description": "Instruction-tuned conversational model based on LLaMA; used in zero-shot and IFT comparisons in the paper.",
            "model_size": "7B",
            "logical_reasoning_task": "ReClor, LogiQA, LogiQAv2 and perturbed variants",
            "task_description": "Multiple-choice logical reasoning reading-comprehension tasks.",
            "method_or_approach": "Zero-shot, instruction prompting, and instruction fine-tuning (using the official Vicuna/Alpaca repos); Chain-of-Thought prompting experimented.",
            "performance": "Zero-shot below GPT-3.5/GPT-4 on original sets; fine-tuning led to improvements but results variable and sometimes below random on perturbed variants in zero-shot. Table 2 entries show low base numbers (e.g., Vicuna-7B ReClor zero-shot ≈0.0640). CoT prompting had inconsistent effect (some increases in some splits).",
            "limitations_or_failure_cases": "Fails many perturbed variants in zero-shot; fine-tuning not consistently effective across datasets; often lags Alpaca-7B after IFT in experiments.",
            "comparison": "Performs worse than GPT-3.5/GPT-4 and Alpaca-7B in this paper's evaluations; similar trends to Alpaca but lower baseline.",
            "ablation_or_analysis_results": "CoT prompting slightly changed results but did not reliably improve robustness; instruction fine-tuning helps but variability remains.",
            "uuid": "e4984.3",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-family",
            "name_full": "LLaMA (7B–65B) family",
            "brief_description": "Open weights family of decoder-only transformer models (LLaMA) evaluated in size-ablation experiments (7B to 65B) showing no consistent correlation between model parameter count and robustness on perturbed logical-reasoning tasks when used zero-shot.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA (7B–65B)",
            "model_description": "Decoder-only transformer models from Meta available in multiple sizes; evaluated here in zero-shot across LLaMA-7B, up to LLaMA-65B on the Shuffle-RepAns perturbed format.",
            "model_size": "7B–65B",
            "logical_reasoning_task": "Shuffle-RepAns variants of ReClor/LogiQA/LogiQAv2 (robustness-focused perturbation)",
            "task_description": "Perturbed multiple-choice logical reasoning where options are shuffled and correct replaced (Shuffle-RepAns) to test permutation invariance and real reasoning vs. memorisation.",
            "method_or_approach": "Zero-shot evaluation with varying base sizes (no instruction fine-tuning in size ablation described).",
            "performance": "No significant difference observed across sizes in zero-shot on Shuffle-RepAns format (Table 5): larger parameter counts (7B→65B) did not reliably increase robustness on these perturbed logical reasoning tasks.",
            "limitations_or_failure_cases": "Scaling within the LLaMA family did not confer robustness to task-structure changes in zero-shot setting; exposes that parameter count alone does not guarantee improved logical generalisation.",
            "comparison": "Contrary to typical scaling benefits, LLaMA-65B did not substantially outperform LLaMA-7B on Shuffle-RepAns; thus other factors (training data diversity, instruction fine-tuning, augmentation) more determinative.",
            "ablation_or_analysis_results": "Size ablation across LLaMA variants indicates little-to-no correlation between size and robustness on perturbed logical-reasoning inputs under zero-shot.",
            "uuid": "e4984.4",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LReasoner",
            "name_full": "LReasoner",
            "brief_description": "A discriminative encoder-based model fine-tuned for logical multi-choice reasoning tasks that uses logic-driven augmentation and achieves strong leaderboard performance; shows permutation invariance and stable performance on Shuffle-Order but declines on Replace-Answer variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LReasoner (ALBERT-XXLarge-v2 backbone)",
            "model_description": "Discriminative (encoder-only) transformer model using ALBERT-XXLarge-v2 backbone, fine-tuned for classification on multiple-choice logical reasoning tasks; trained with concatenated (context + [SEP] + question + [SEP] + option) inputs per candidate.",
            "model_size": null,
            "logical_reasoning_task": "ReClor, LogiQA (and perturbed variants), used as fine-tuned discriminative baseline",
            "task_description": "Multiple-choice logical reading-comprehension requiring selection of single correct option via classification.",
            "method_or_approach": "Supervised fine-tuning on task training sets, logic-driven data augmentation methods in original LReasoner work are referenced; evaluated on original and perturbed formats.",
            "performance": "Strong performance on original sets (competitive with best models) and stable performance on Shuffle-Order due to input concatenation scheme (permutation invariance). However, substantial drop in performance on Replace-Answer and Shuffle-RepAns variants reported in the paper (see main results summary and Table 1).",
            "limitations_or_failure_cases": "Significant decline on Replace-Answer and Shuffle-RepAns; suggests limited ability to detect 'none of the above' or to generalise beyond memorised correlations despite permutation invariance.",
            "comparison": "Among discriminative models, LReasoner is competitive and shows stability on Shuffle-Order; however, on some tasks MERIt and AMR-LDA approach or exceed GPT-4 when merged training sets are used.",
            "ablation_or_analysis_results": "Permutation invariance attributed to input formatting (separate concatenations per option) — explains robustness to Shuffle-Order but not to Replace-Answer.",
            "uuid": "e4984.5",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MERIt",
            "name_full": "MERIt (Meta-path guided contrastive learning for logical reasoning)",
            "brief_description": "A discriminative encoder model that performs intermediate training via relation extraction on large corpora and achieves top performance on ReClor leaderboard; improves generalisation in logical reasoning when fine-tuned.",
            "citation_title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "MERIt (DeBERTaV2-XXLarge backbone)",
            "model_description": "Encoder-only discriminative model using DeBERTaV2-XXLarge backbone; employs relation extraction and intermediate training to construct auxiliary data before fine-tuning on downstream logical reasoning tasks.",
            "model_size": null,
            "logical_reasoning_task": "ReClor, LogiQA (and perturbed variants) used as discriminative baseline",
            "task_description": "Multiple-choice logical reasoning (reading comprehension) emphasizing rule extraction and deduction from text.",
            "method_or_approach": "Intermediate training on constructed data (relation-extraction derived) followed by fine-tuning on target logical reasoning datasets; evaluated on perturbations.",
            "performance": "Reported strong performance on original datasets; MERIt achieved #1 on ReClor leaderboard at time of proposal. In this paper, MERIt performed commendably and sometimes surpassed GPT-4 when training sets were merged.",
            "limitations_or_failure_cases": "Like other discriminative models, stable on Shuffle-Order but significant performance drops on Replace-Answer and Shuffle-RepAns; shows reliance on training distribution.",
            "comparison": "Comparable or superior to GPT-4 on some fine-tuned merged-data experiments; better generalisation than vanilla generative models when fine-tuned discriminatively.",
            "ablation_or_analysis_results": "Demonstrates benefit of intermediate training and augmented/constructed training data for discriminative models; but susceptible to shifts in task structure (Replace-Answer).",
            "uuid": "e4984.6",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AMR-LDA",
            "name_full": "AMR-LDA (AMR-based Logic-Driven Data Augmentation)",
            "brief_description": "A discriminative model and (importantly) an augmentation method that uses AMR and logical equivalence laws to generate logically equivalent/inequivalent extended options and contexts; improves discriminative fine-tuned models but can harm next-token generative training unless used for prompt augmentation.",
            "citation_title": "Enhancing logical reasoning of large language models through logic-driven data augmentation",
            "mention_or_use": "use",
            "model_name": "AMR-LDA (method and model; DeBERTaV2-XXLarge backbone for discriminative variant)",
            "model_description": "Approach that leverages AMR parsing to construct logically equivalent/inequivalent sentences (extended options/contexts) following logical equivalence laws; used both as a discriminative model (AMR-LDA) and as a logic-driven data augmentation technique for training and prompt augmentation.",
            "model_size": null,
            "logical_reasoning_task": "ReClor, LogiQA, LogiQAv2 and perturbed variants; both used to augment training data and as prompt-augmentation at test time.",
            "task_description": "Multiple-choice logical reasoning where augmentation creates extended-option/context pairs via logical equivalence/contraposition/implication laws to expose logical structures.",
            "method_or_approach": "AMR parsing + logical-equivalence transformations to generate extended options and extended contexts appended to inputs; used in three ways in experiments: (a) augment training data for discriminative fine-tuning, (b) augment training data for generative next-token fine-tuning (which harmed performance), (c) prompt augmentation at test/inference time for generative models (which improved performance).",
            "performance": "Improves discriminative model performance (AMR-LDA and LReasoner) on standard and some perturbed formats when used in training; however, inserting AMR-LDA augmented data into next-token training of generative LLMs degraded performance (Table 3). Prompt augmentation (adding AMR-LDA extended options/contexts to test prompt) improved generative model performance substantially in reported experiments (e.g., GPT-4-IPT-LDA high gains in Table 3 and Alpaca-7B-IFT-LDA gains).",
            "limitations_or_failure_cases": "Directly adding AMR-LDA augmented data into next-token (generative) model training distorted the training distribution and hurt generalisation. Logical augmentation must be applied carefully and is most effective for discriminative fine-tuning or as prompt augmentation for large generative models.",
            "comparison": "AMR-LDA fine-tuned discriminative models achieve top leaderboard performance and are more robust than vanilla generative models on some tasks; but naïve usage with generative next-token training is detrimental.",
            "ablation_or_analysis_results": "Paper reports that logic-driven augmentation in training helps discriminative LMs but harms generative next-token models; prompt augmentation (apply AMR-LDA only at test-time input) helps GPT-3.5/GPT-4 and fine-tuned Alpaca-7B. Also provides case studies illustrating AMR-LDA transformations (appendix).",
            "uuid": "e4984.7",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "LogiQA 2.0 - an improved dataset for logical reasoning in natural language understanding",
            "rating": 2,
            "sanitized_title": "logiqa_20_an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Enhancing logical reasoning of large language models through logic-driven data augmentation",
            "rating": 2,
            "sanitized_title": "enhancing_logical_reasoning_of_large_language_models_through_logicdriven_data_augmentation"
        },
        {
            "paper_title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
            "rating": 2,
            "sanitized_title": "merit_metapath_guided_contrastive_learning_for_logical_reasoning"
        },
        {
            "paper_title": "LReasoner",
            "rating": 1
        },
        {
            "paper_title": "RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners",
            "rating": 1,
            "sanitized_title": "robustlr_a_diagnostic_benchmark_for_evaluating_logical_robustness_of_deductive_reasoners"
        }
    ],
    "cost": 0.017973999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning
30 Mar 2024</p>
<p>Qiming Bao 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>XtractaNew Zealand</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Gaël Gendron 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Alex Yuxuan Peng 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Wanjun Zhong 
School of Computer Science and Engineering
Sun Yat-Sen University</p>
<p>Neset Tan 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Yang Chen 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Michael Witbrock 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Jiamou Liu 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning
30 Mar 202431B5D715A00C54C336EC05DCCC43D395arXiv:2310.09430v4[cs.CL]
Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels.However, their generalisation and robustness when performing logical reasoning has not been sufficiently assessed.To comprehensively evaluate this ability, we develop three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus" that extend standard logical reasoning datasets to evaluate the robustness of the LLM's reasoning.For each, we create three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options is correct", and the third with a combination of shuffling and substitution.Experiments on these datasets show that these simple augmentations greatly hinder the models' performance.Despite their high performance on the original publicly available datasets, we find that all models perform poorly on these newly constructed datasets.We also demonstrate that introducing task variations into the training set can markedly improve the model's performance on both the original and our developed datasets.Finally, we show that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning.Source code and data are made publicly available at 1 .</p>
<p>Introduction</p>
<p>By leveraging the vast amounts of data available on the internet, large language models have achieved great performance on various tasks Brown et al. (2020); Wei et al. (2022b).However, training data that is confined to a single task or structure may lead to overfitting to specific tasks, consequently diminishing the model's generalisation capabilities.Specifically, there is a dearth of both quantity and quality of data on logical reasoning available on the internet Liu et al. (2023a).This scarcity results in limitations for large language models when tackling complex logical reasoning tasks Wang et al. (2022).Consequently, ensuring high quality and diversity in training data becomes essential, as it can significantly aid large language models in enhancing their robustness on reasoning tasks.Current logical reasoning datasets do not truly represent the reasoning abilities of large language models, as making small modifications significantly degrades performance.Many logical reasoning datasets are designed to select the precise correct option, models might adapt to choose answers that merely resemble the correct one.Moreover, since numerous public logical reasoning datasets were published prior to the training of these large language models, the models could have been trained using these datasets Li &amp; Flanigan (2023).Consequently, these models might inadvertently recall the location of the correct answer.</p>
<p>To address the above issues, We propose a data perturbation procedure and apply it to three existing logical reasoning datasets, resulting in the creation of "ReClor-plus", "LogiQAplus" and "LogiQA v2-plus".These new datasets feature three subsets to evaluate the generalisation and robustness of large language models.These new datasets contain modifications of the task structure of existing logical reasoning datasets.We do not change the semantics of the original context, and only modify its structure (orders, forms).We perform systematic experiments with our logical reasoning datasets to investigate the models' generalisation and robustness on three main aspects.First, we evaluate the in-context generalisation and robustness of large language models on logical reasoning tasks.Second, we perform instruction fine-tuning, instruction prompting and logic-driven data augmentation to evaluate whether those methods can help improve the performance of the models.Third, we investigate how different proportions of data perturbation on the training set can help models improve their generalisation and robustness.Fourth, since model with larger parameter scale demonstrate better performance, it is underexplored how model scale will influence model's performance on logical reasoning.we discuss whether the number of parameters in the model can influence the model's generalisation and robustness.</p>
<p>Our benchmark supports both discriminative large language models like LReasoner Wang et al. (2022), MERIt Jiao et al. (2022), and AMR-LDA Bao et al. (2023) and generative large language models like GPT-3.5 OpenAI (2023a), GPT-4 OpenAI (2023b), LLaMA Touvron et al. (2023), Alpaca Taori et al. (2023) and Vicuna Chiang et al. (2023).</p>
<p>Our main findings can be summarised as follows:</p>
<p>• We find that existing large language models like GPT-3.5 and GPT-4 perform well on logical reasoning tasks in the original format but their performance drops on our new formats, suggesting that the models may have seen these datasets during training and failed to acquire generalised logical reasoning capabilities.• We find that instruction fine-tuning can help large language models increase their generalisation and robustness on logical reasoning tasks.In particular, fine-tuned discriminative large language models often demonstrate permutation invariance.Furthermore, applying logic-driven data augmentation for fine-tuning, combined with prompting, can enhance the generalisation performance of both discriminative large language models and generative large language models.• We find that, for large training set sizes (more than 10,000 training samples), high ratio of perturbated data (shuffled and substituted) can help increase generative large language model's performance on most logical reasoning tasks.However, this does not work with small training sets.• Finally, we find surprisingly that there is no direct correlation between the model's size (from LLaMA-7B to LLaMA-65B) and its generalisation and robustness on logical reasoning tasks.Contrary to intuition and observations from other tasks Touvron et al. ( 2023), a larger model does not necessarily guarantee better generalisation and robustness on logical reasoning tasks.</p>
<p>Related Work</p>
<p>Research on the generalisation and robustness of large language models for logical reasoning mainly focuses on synthetic natural language reasoning.Initial findings indicate that transformers can be trained on multi-hop reasoning tasks and substantially generalise to deeper unseen reasoning depths, although it can be challenging on paraphrased synthetic test sets Clark et al. (2021).Efforts to enhance generalisation to deeper multi-step reasoning include the introduction of PARARULE-Plus, providing data augmentation on reasoning depths between 2 to 5 Bao et al. (2022).AbductionRules incorporates abductive reasoning to understand and answer the multi-step reasoning task Young et al. (2022).ROBUSTLR is a challenging dataset considering conjunction, negation, and utilising logical equivalence for paraphrasing Sanyal et al. (2022).</p>
<p>The existing synthetic multi-step reasoning datasets often lack the complexity and diversity found in real-world data.They might be generated using a limited set of rules or scenarios, which can lead to a narrower scope of logical reasoning challenges.ReClor Yu et al. (2020) and LogiQA Liu et al. (2020a) are challenging reading comprehension datasets derived from real-world examinations such as the GMAT, LSAT, and national civil servant exams Yu et al. (2020); Liu et al. (2020a;2023b).An enhanced version, LogiQAv2, incorporates additional data from Chinese civil servant examinations Liu et al. (2023b).Evaluations reveal that the implementation of discourse order-oriented loss functions, specifically Sentence Order Prediction (SOP) and Next Sentence Prediction (NSP), enhances the performance of models like ALBERT and BERT on reasoning tasks Lan et al. (2020); Kenton &amp; Toutanova (2019); Li et al. (2022).Further assessments of generative large language models, including GPT-3.5 and GPT-4, on ReClor, LogiQA, and LogiQAv2, have demonstrated their commendable performance.However, it is unclear how robust large language models are in real-world logical reasoning tasks Liu et al. (2023b).Bao et al. (2023); Jiao et al. (2022); Wang et al. (2022) utilise reading comprehension tasks requiring logical reasoning to perform experiments and evaluate the logical reasoning capabilities in existing large language models.The model needs to predict the answer by understanding the rules within the context and deducing the conclusion, which cannot be directly found through simple pattern matching from the context.This differs fundamentally from typical reading comprehension tasks.Existing work focuses on reading comprehension through task structure variation, such as shuffling options or replacing the correct one, yet no studies directly utilize task variation to assess logical reasoning.AddSent Jia &amp; Liang (2017) generates misleading text by modifying the question according to certain rules and manually proofreading; AddAny Jia &amp; Liang (2017) automatically searches for misleading texts word by word across various MRC models; AddAn-sCtx Liu et al. (2020b) generates misleading text by removing answer words from answer sentences.In this paper, we define the generalisation and robustness of models in logical reasoning that these models not only need to solve the original question but also address new questions that have been modified through task structure variations, including shuffling the order of options and replacing the correct answer.</p>
<p>Method</p>
<p>We propose a logical reasoning evaluation benchmark for evaluating the robustness and generalisation of large language models.Figure 2 illustrates the various configurations tested.We propose three task structure variations and apply them to three existing datasets to construct our logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus".The task variations are as follows: 1) Shuffle-Order: The order of all the options is shuffled.This variation evaluates whether the model is reasoning or remembers the position of the correct answer.2) Replace-Answer: The correct answer is replaced with "none of the other options is correct" and we add "You can also say there is no correct answer" at the end of the question.This variation evaluates whether the model understands that apart from the correct option, all other options are incorrect.It is used to detect models returning answers that look correct.3) Shuffle-RepAns: The third split combines the variations from 1) and 2).We apply our variations on ReClor Yu et al. (2020), LogiQA Liu et al. (2020a) and LogiQAv2 Liu et al. (2023b).The three logical reasoning datasets are all formatted as multiple-choice reading comprehension tasks.An example from ReClor can be found in Figure 1.LogiQA and LogiQAv2 are following the same format as ReClor.For each question in these datasets, there are only four options, and only one of these options is the correct answer.We propose these three datasets to validate the robustness of the model from the following three perspectives: a) We want to evaluate whether the model performs reasoning, instead of just memorizing the position of the correct answer.b) We want to evaluate whether the model understands that, aside from the correct option, all other options are incorrect.We do not want model to simply predict the answer that looks most like the correct answer.c) Building upon the second point, we shuffle the order of the options to judge whether the model can perform more complex reasoning.Figure 1: The instruction fine-tuning involves providing the model with a task description before the input.It includes the Instruction, Input, and Question.The model then gives the expected output.The correct answer is highlighted in blue with a checkmark.Each question has four choices, and only one of them is the correct answer.</p>
<p>Fine-tuned discriminative large language models have to select one answer from the answer set.In constrast, the generative large language models used in next-token prediction setting have to generate a text that matches the correct option letter.To better make a comparison between these two classes of models, we add the following instruction after the question: "Please only return the letter in front of your predict correct option, A, B, C or D." to ease the evaluation.We catch the correct answer using regular expressions from the generated prediction if the prediction does not correspond to the desired format.We use the official Alpaca2 and Vicuna3 repositories for instruction fine-tuning and instruction prompting.We perform instruction fine-tuning and instruction prompting on both models.For instruction prompting, we use Chain-of-Thought prompting to explore how it can help increase the generalisation and robustness of large language models in logical reasoning tasks.</p>
<p>Instruction Fine-Tuning/Prompting Instruction Fine-Tuning (IFT) has been proposed to enhance the performance of large language models on unseen tasks Mishra et al. (2022); Wei et al. (2022a).An example of this approach is depicted in Figure 1.An instruction, "Can you predict the correct option for the given input?" is added, encompassing the context, question, and each option from the logical reasoning datasets under evaluation.Subsequently, a sentence is introduced: "Please only return the letter in front of your predicted correct option, A, B, C, or D." The process entails training the model on pairs of instructions and corresponding responses.Contrastingly, while instruction prompting employs a similar input format, it is utilised during inference rather than training.Two methods of instruction prompting (IPT) are implemented: a zero-shot evaluation with a format identical to instruction fine-tuning, and a second approach, Chain-of-Thought Wei et al. (2022c), specifically applied to enhance performance in logical reasoning.The latter integrates the following prompt: "Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:' " into the instructions to encourage the model to reason.</p>
<p>We also investigate if adding our perturbed sets into the training of the LLMs can help increase performance, and what proportion of perturbed data is required.We perform instruction fine-tuning with ReClor, LogiQA and LogiQAv2 separately.We use different ratios of perturbed sets when shuffling and replacing the order of options.</p>
<p>Experiments</p>
<p>Experiment Setup</p>
<p>Modeling Choices We use GPT-3.5-Turbofor the GPT-3.5 experiments and GPT-4 with 8192 tokens for the GPT-4 experiments.For fine-tuned discriminative large language models, we refer to the code from the ReClor leaderboard4 .The appendix demonstrates a more detailed hyperparameter setting and model selection.</p>
<p>Result on the Original Datasets</p>
<p>We summarise our primary findings in Table 1.In this subsection, we emphasise results from the original ReClor, LogiQA, and LogiQAv2 datasets.Under the zero-shot evaluation setting, both GPT-3.5 and GPT-4 notably outperform Alpaca-7B and Vicuna-7B across the three datasets, with the latter two models underperforming.Among these, GPT-4 achieves the highest accuracy.We then fine-tuned Alpaca-7B and Vicuna-7B using training data from each individual task and subsequently evaluated them.Notably, all the fine-tuned generative large language models display a marked improvement over their zero-shot evaluation performance.</p>
<p>Simultaneously, we fine-tuned LReasoner, MERIt, and AMR-LDA on their respective training sets and found commendable performance across the board.AMR-LDA's accuracy approached that of GPT-4.To further enhance the diversity and volume of the training data, we merged training sets from ReClor, LogiQA, and LogiQA-v2.Both generative and discriminative large language models exhibited improved results in this setting.The performance of the fine-tuned Alpaca-7B approached GPT-4's, while both MERIt and AMR-LDA surpassed GPT-4.These results underscore the significance of data diversity and volume in logical reasoning tasks.</p>
<p>Assessing Models' Robustness on Logical Reasoning Tasks</p>
<p>Generative Large Language Models We evaluated the performance of GPT-3.5 and GPT-4 on various datasets, notably observing a significant performance drop on the LogiQAv2 dataset, particularly in its Shuffle-Order variant, compared to less pronounced declines on other datasets.Given that GPT-3.5 and GPT-4 were trained before 2023, and considering the publication of ReClor and LogiQA before 2022, it's plausible that these datasets were part of their training data.However, since LogiQAv2 was released after 2023, it was not included, making it a reliable measure of the models' reasoning capabilities.To address potential data leakage concerns with ReClor and LogiQA, we applied instruction fine-tuning using these datasets, both individually and combined with LogiQAv2.This approach significantly enhanced performance across the board.Furthermore, we assessed Alpaca-7B and Vicuna-7B, finding that their zero-shot evaluations were below random chance levels, and fine-tuning failed to yield consistent improvements across datasets.The observed variability in performance, with improvements on shuffled datasets when initial performance was poor and vice versa, suggests a lack of reliable logical reasoning and potential overfitting to the training data, with minor accuracy gains likely attributable to random variance rather than genuine progress.</p>
<p>Discriminative Large Language Models Fine-tuned discriminative large language models such as LReasoner, MERIt, and AMR-LDA exhibit enhanced generalisation performance on logical reasoning questions.These models demonstrate stable performance on both the original and Shuffle-Order sets; however, they experience a significant decline in performance on the Replace-Answer set and Shuffle-RepAns set.The stability observed in the Shuffle-Order set performance can be attributed to the models' handling of input structure, where the input is formed by concatenating the context, question, and each corresponding option (Context + [SEP]' + Question + [SEP]' + Option), with the output being the label for each concatenation.The special token '[SEP]' is used to separate sentences.Thus, shuffling the option order doesn't create new context, question, and option concatenations, maintaining stable performance and permutation invariance.Despite this, discriminative models do not surpass generative models on the Replace-Answer and Shuffle-RepAns sets, suggesting these models might not engage in complex logical reasoning as effectively.Therefore, the overall performance indicates that large language models, including discriminative ones, may lack strong logical reasoning abilities, as detailed in Table 1.</p>
<p>Chain-of-Thought (CoT) Prompting</p>
<p>We perform further experiments using Chain-of-Thought prompting Wei et al. (2022c).As shown in Table 2, in the zero-shot evaluation, all generative large language models do not perform well.The performance of GPT-3.5 and GPT-4 on the Shuffle-RepAns set is nearly equivalent to a random guess, while Alpaca-7B and Vicuna-7B fail the task.Employing the CoT prompting doesn't result in a significant difference in performance for these generative large language models compared to when CoT prompting is not used.Only GPT-4 exhibits systematic improvements in accuracy on the Shuffle-RepAns task.Nonetheless, this task remains challenging for these models, including GPT-4.We have included some case studies in the appendix A that illustrate the use of CoT prompting to assist GPT-4 in correctly answering questions with intermediate steps.Overall, since CoT prompting does not offer any explicitly useful information as additional input, and the model has not been trained to respond correctly when given the CoT prompting, it is reasonable to expect that it may not perform well in complex logical reasoning scenarios.Since logic-driven data augmentation performs well on logical reasoning tasks for finetuned discriminative large language models like LReasoner and AMR-LDA, it is worth considering to apply this method to generative large language models trained on nexttoken prediction task and see if there is an improvement in our logical reasoning tasks for these models.We use the provided augmented data from the authors of AMR-LDA for ReClor, LogiQA and LogiQAv2 datasets and extend the augmented option information to each option in the training set for ReClor, LogiQA and LogiQAv2.The input format for the augmented data is formed as follows: context + question + each option + extended option + extended context.The term 'extended option' refers to the use of AMR-LDA to augment the option based on the logical equivalence laws.'Extended context' means using AMR-LDA to augment the context based on the logical equivalence laws.The output is the ID of each option, which can be either A, B, C, or D. An example for the extended option and the extended context can be found in Appendix Figure 3. Two examples of how AMR has been used to perform logical equivalence sentence construction for long sentence can be found in Appendix Table 4 and 5.We perform instruction fine-tuning using the input and output formats, along with the same instructions mentioned in the experiment setup.</p>
<p>Models</p>
<p>Table 3 shows that logic-driven data augmentation is detrimental to the generalisation and robustness of large language models trained using next-token prediction for logical reasoning tasks.We make the hypothesis that logic-driven data augmentation does not directly map to the task of next-token prediction, which may disturb the training of the model.This hypothesis is corroborated by Table 3.When we use individual training sets from ReClor, LogiQA, and LogiQAv2 for separate training and testing, there is an observed improvement compared to models that did not undergo such training.However, performance significantly declines when we utilise AMR-LDA to augment data in the training set.This phenomenon is also evident when the training set is expanded to include ReClor, LogiQA, and LogiQAv2 collectively.This suggests that merely increasing the scale of the training set can offer some benefits in enabling the model to tackle more complex logical reasoning tasks.While, if there is a discrepancy between the distributions of the training and test sets, the potential improvements will be constrained.Additionally, using logic-driven data augmentation in the training set, which might distort its distribution, could further deteriorate performance on the test set.</p>
<p>To enhance the performance of models on more complex logical reasoning tasks, we employ logic-driven data augmentation for generative large language models during instruction fine-tuning and prompting.As shown in Table 3, for the experiment was conducted under "prompt augmentation using AMR-LDA" area, both the training and test sets were augmented using AMR-LDA.We discovered that two logic-driven data augmentation methods can enhance model's performance on task variation.The first method involves applying logic-driven data augmentation to both the training and test sets; this approach was effective for the fine-tuned Alpaca-7B, but not for the Alpaca-7B that hadn't been finetuned.The second method involves applying logic-driven data augmentation solely to the test set; this approach was effective for GPT-3.5 and GPT-4.To implement logic-driven data augmentation on the test set, we followed three steps:</p>
<p>Step 1: Augment each option and append the augmented text to the original option.</p>
<p>Step 2: If an option can be augmented, we then augment the context and append this augmented context to the already augmented option.</p>
<p>Step 3: The augmented prompt will be composed of the following elements: context + question + each option + extended option + extended context.This strategy helped large language models enhance their performance on our task variation.This result indicates that logic-driven data augmentation offers benefits in two aspects.The first is altering the distribution of both the training and test sets.The second is enhancing the prompt or input information for larger language models like GPT-3.5 and GPT-4, which extracts additional details to large language models, thereby enhancing their efficacy in logical reasoning tasks.</p>
<p>Transfer Learning with Data Perturbation</p>
<p>We perform transfer learning to investigate the extent to which incorporating variations of task structure into the training set can help models improve their performance on logical reasoning tasks.Table 4 shows our results.We carried out instruction fine-tuning on Alpaca-7B using individual training sets from ReClor, LogiQA, and LogiQAv2, each with varying data perturbation ratios, specifically focusing on Shuffle-RepAns.The reason we selected Alpaca-7B is that our main experiment, as shown in Table 1, involves both Alpaca-7B and Vicuna-7B.These models demonstrate similar performance in Table 1.Under the constraint of limited computational resources, we therefore chose Alpaca-7B for a deeper analysis.We observed that using a large training set of LogiQAv2 for fine-tuning (over 10,000 samples).50% of the training set is perturbed to the Shuffle-RepAns format, which improves performance on all original, Shuffle-Order, Replace-Answer, and Shuffle-RepAns format logical reasoning tasks.However, when the amount of training samples used for the fine-tuning is less than 10,000, the model does not demonstrate the above phenomenon.We also observe an intriguing phenomenon: the models' performance on the Shuffle-Order set does not improve as the proportion of the Shuffle-RepAns set in the training set increases.Instead, it declines.Particularly on the ReClor and LogiQA datasets, we can clearly see that the higher the task structure perturbation ratio, the worse the model performs on the Shuffle-Order set.We speculate that the model may overfit the data with altered task structure perturbation ratio.These results also support the claim that existing large language models are not robust when solving logical reasoning tasks.To enhance the robustness of logical reasoning in large language models, employing a more expansive training dataset and a higher perturbation ratio demonstrates beneficial.</p>
<p>Influence of the Model Size</p>
<p>We perform additional experiments to see whether, with the same model architecture, a larger model size will show better generalisation and robustness on logical reasoning tasks.</p>
<p>We use different size of LLaMA models to perform the experiments from LLaMA-7B to LLaMA-65B.This choice was made because LLaMA provides model checkpoints for various sizes, ranging from 7B to 70B, which were available during the period of our experiment.We utilised the Shuffle-RepAns sets and perform the zero-shot evaluation.Table 5 shows the results.We do not find a significant difference in the Shuffle-RepAns format logical reasoning task with different sizes of models under zero-shot evaluation, without any instruction fine-tuning.In summary, Table 5 sheds light on the nuanced relationship between model size and performance in logical reasoning tasks.While larger models might be expected to perform better due to their increased capacity, the results presented here suggest that model size does not play a significant role in determining overall performance on robust and complex logical reasoning tasks.It's worth noting that our comparisons are based on different model sizes of the same LLaMA base model.Therefore, the conclusions drawn may not necessarily apply to comparisons involving different base models.</p>
<p>Models</p>
<p>Conclusion</p>
<p>Our study examines the generalisation and robustness of large language models (LLMs) in logical reasoning tasks, revealing significant limitations.We find that instruction finetuning enhances performance, while chain-of-thought (CoT) prompting alone falls short in robust reasoning tasks.Analysis shows that datasets larger than 10,000 samples, such as LogiQAv2, require extensive data perturbation and task structure modifications (Shuffle-RepAns) for improved adaptability-a necessity not seen with smaller datasets.Model size, within the same LLaMA base model framework, does not guarantee better generalisation or robustness.We observe that logic-driven data augmentation benefits fine-tuned discriminative models by improving performance and demonstrating permutation invariance, indicating reliance on more than memorisation.Applying logic-driven data augmentation to prompts further aids larger generative models like GPT-3.5, GPT-4, and fine-tuned Alpaca-7B, enhancing their logical reasoning capabilities in novel tasks.</p>
<p>A Appendix</p>
<p>A.1 Background proposed and it, again exceeding the performance on MERIt.We utilise MERIt and AMR-LDA to perform fine-tuning on the downstream tasks using their public model weights.</p>
<p>Generative Large Language Models</p>
<p>A.2 Hyperparameter Setting</p>
<p>We determine the parameter values in Tables 6 and 7 by following the public parameter values provided by Stanford Alpaca 2 for generative large language models, as well as the default hyperparameters used by each discriminative large language model, including AMR-LDA6 , LReasoner7 , and MERIt8 .We use the model weights from the official repositories of LReasoner, MERIt and AMR-LDA on each logical reasoning reading comprehension tasks.</p>
<p>For LReasoner, we select ALBERT-XXLarge-v2 Lan et al. (2019) as the backbone model.For MERIt and AMR-LDA, we select DeBERTaV2-XXLarge He et al. ( 2021) as the backbone model.We selected ALBERT-XXLarge-v2 and DeBERTaV2-XXLarge because they are the best available models, as mentioned in their original papers, LReasoner Wang et al. (2022) and MERIt Jiao et al. (2022), respectively.The authors of LReasoner did not publicly release the model weights for LogiQAv2 and their logic-driven data augmentation are designed for ReClor and LogiQA, hence we use their model weights trained on LogiQA to evaluate the result on LogiQAv2.For fine-tuning instructions on Alpaca and Vicuna, we refer to the training script and code from the official Alpaca GitHub repository Taori et al. (2023).</p>
<p>For the generative large language models like GPT-3.5 and GPT-4.We propose a new instruction for the Shuffle-Order set.We provide an instruction that includes an example, shuffles the order of options for this example, and then request the model's response.For the Replace-Answer set, we add an instruction "You can select the option that none of the other options is correct" when we test more complex scenarios.For the Shuffle-RepAns set, we add the two instructions together.All the experiments are run on 8 NVIDIA A100 GPUs, each with 80G of VRAM.We evaluate on the validation sets of ReClor, LogiQA and LogiQAv2, as the test sets are private and used to rank models on the leaderboard.We constructed our task variations from the existing validation sets.We employed the modified validation set (Shuffle-Order, Replace-Answer, and Shuffle-RepAns) to perform the evaluation.we use accuracy to adhere to the methodologies outlined in previously published papers Bao et al. (2023); Jiao et al. (2022); Wang et al. (2022).We follow the same metrics to ensure a fair comparison when evaluating the model's performance.The datasets used are balanced and only a single choice per question is a logical entailment, so accuracy is a proper measure to assess the models' logical reasoning performance.For prompting GPT-3.5 and GPT-4, we use the default hyperparameters from OpenAI playground.</p>
<p>A.3 Analysis of Hyperparameters for GPT-4</p>
<p>We performed an experiment on GPT-4 using different hyperparameter values (temperature) as shown in Table 8.This experiment was carried out using the GPT-4 API, trained on a dataset containing information up to April 2023.For the other tables in this paper, the experiments utilised the earlier version of GPT-4, released in March, which had only learned from data available up to September 2021.However, we found that temperature did not significantly influence the model's performance across the entire evaluation set.We find that using Chain-of-Thought Prompting can show some improvements in Shuffle-RepAns logical reasoning tasks with GPT-4, while its accuracy is no different from random guessing.Therefore, we listed some examples to see what differences are made in the results generated by GPT-4 after incorporating Chain-of-Thought Prompting.Figure 6, 8 and 10 show that GPT-4 fails the example but with Chain-of-Thought Prompting, it can correctly answer the question with solution process.Figure 7, 9, and 11 show that both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on the example.</p>
<p>A.4 Limitations</p>
<p>Our paper acknowledges several limitations: Firstly, pre-training large language models (LLMs) is computationally expensive, potentially causing fine-tuning efforts to suffer from residual effects of pre-training tasks.Secondly, prevalent logical reasoning evaluation tasks, designed akin to reading comprehension exercises, may unfairly advantage discriminative LLMs over generative counterparts due to their format, highlighting the absence of a balanced dataset for fair assessment across different LLM training tasks.Lastly, the unavailability of GPT-4's source code and training data precludes us from conducting instruction fine-tuning experiments to test its logical reasoning robustness.Figure 3: Example of using AMR-LDA to augment the prompt from the ReClor dataset and their subsequent utilization as input for GPT-4.Data segments that are marked in bold italics and appear in blue were generated using the contraposition law, while those in brown were generated using the implication law.In this example, only the segments in dark blue are the extended option, and the sentences that follow constitute the extended context.</p>
<p>A.5 Ethics Statement</p>
<p>As we do not present a new model but perform an evaluation of the existing state-of-the-art, we believe our work does not present significant ethical risks.We believe that our work will contribute to a better understanding of the training mechanisms of large language models in current large models.By highlighting flaws in the capabilities of language models, this evaluation can help mitigating risks of misuses that could raise ethical concerns.</p>
<p>Long Sentence Example 1:</p>
<p>Original sentence: Sarah woke up early in the morning, and she started her day with a cup of coffee and some light yoga stretches.Original sentence's AMR graph: (a / and :op1 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning)))) :op2 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :ARG0 p :mod (y / yoga) :ARG1-of (l / light-06) :quant (s3 / some))))) Modified AMR graph using AMR-LDA: (a / and :op1 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :ARG0 p :mod (y / yoga) :ARG1-of (l / light-06) :quant (s3 / some)))) :op2 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning))))) Generated logical equivalence sentence using AMR-LDA: Sarah started her day with a cup of coffee and some light yoga stretching and woke up early in the morning.</p>
<p>Figure 4: One example uses our AMR-LDA to generate logical equivalence sentences for long sentences.In this case, a logical equivalence sentence is generated using the commutative law, and the same color represents the same argument.In this case, the order of the former and latter arguments for the conjunction word "and" has been swapped.</p>
<p>Long Sentence Example 2:</p>
<p>Original sentence: Sarah woke up early in the morning, and she started her day with a cup of coffee and some light yoga stretches that will help lose weight.Original sentence's AMR graph: (a / and (a / and :op1 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning)))) :op2 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :mod (y / yoga) :ARG0-of (h / help-01 :ARG1 (l / lose-01 :ARG1 (w2 / weight))) :ARG1-of (l2 / light-06) :quant (s3 / some))))) Modified AMR graph using AMR-LDA: (a / and :op1 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :mod (y / yoga) :ARG0-of (h / help-01 :ARG1 (l / lose-01 :ARG1 (w2 / weight))) :ARG1-of (l2 / light-06) :quant (s3 / some)))) :op2 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning))))) Generated logical equivalence sentence using AMR-LDA: Sarah started her day with a cup of coffee and some light yoga stretching to help lose weight, and woke up early in the morning.Figure 5: One example uses our AMR-LDA to generate logical equivalence sentences for long sentences.In this case, a logical equivalence sentence is generated using the commutative law, and the same color represents the same argument.AMR-LDA can understand the effect of that clause on yoga stretching.In this case, the order of the former and latter arguments for the conjunction word "and" has been swapped.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: Some biologists believe that the capacity for flight first developed in marine reptiles, claiming that feathers are clearly developed from scales.Other biologists rightly reject this suggestion, pointing out that bats have no scales and that nonmarine reptiles also have scales.Those who believe that flight first developed in tree-dwelling reptiles reject the claim that the limbs of land-dwelling reptiles might have developed into wings.They insist that it is more likely that tree-dwelling reptiles developed wings to assist their leaps from branch to branch.Instruction: Can you predict the correct option for the given input?Input: Advertisement: The dental profession knows that brushing with Blizzard toothpaste is the best way to fight cavities.We surveyed five dentists, and each agreed that the tartar control formula found in Blizzard is the most effective cavityfighting formula available in a toothpaste.Question: The flawed reasoning in which one of the following is most similar to the flawed reasoning in the advertisement?Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:'.The 'ANSWER' is the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. We know that electing Gomez would be the best course for the nation to follow because, of ten voters we surveyed, each agreed that electing Gomez would help the nation.B. Some of the nation's voters believe that Gomez is the candidate who would be best for the nation.Of the ten voters we surveyed, each agreed that the policies Gomez is committed to would be the best policies for the nation to adopt.C. The nation's voters know that Gomez is the candidate whose policies would be best for the nation.Of ten voters polled, each said that Gomez would be a very popular leader.D. None of the other options is correct.Correct Answer: "D" GPT-4: "A" Chain-of-Thought Prompting GPT-4: The reasoning in the advertisement is flawed because it is based on a small and potentially biased sample size (five dentists) to make a general claim about the dental profession's view on Blizzard toothpaste.Let's analyse each option: A. This option is flawed for the same reason as the advertisement: it's based on a small sample size (ten voters) to make a claim about the best course for the nation.B. This option is also based on a small sample size (ten voters), but it doesn't make a general claim about the nation's voters.Instead, it only states that some voters believe Gomez is the best candidate.C.This option is flawed because it makes a general claim about the nation's voters based on the popularity of Gomez among ten voters, not on the effectiveness of his policies.D. This option states that none of the other options is correct.</p>
<p>The most similar flawed reasoning to the advertisement is found in option A, as it also uses a small sample size to make a general claim.ANSWER: A Figure 7: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns ReClor dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.In this example, Chain-of-Thought try to predict an option which looks more like the correct answer.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: An organisation that elects a leadership team implements such a rule?if there are more than one candidate, the candidate must agree to be nominated, and before expressing his consent, he must be informed who his candidate is.</p>
<p>Question: If the above conclusion is true, then the following is also true? Please only return the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. None of the other options is correct.B. Implement the rule so that the number of nominated candidates is greater than not implementing the rule.C. Implementing this rule, there is at most one candidate nominated.D. Implement the rule, no candidate may be nominated.Correct Answer: "A" GPT-4: "C" Chain-of-Thought Prompting GPT-4: To analyse each option, let's break down the initial rule:</p>
<p>1.There must be more than one candidate.2. The candidate must agree to be nominated.</p>
<ol>
<li>The candidate must be informed of their opponent before agreeing to be nominated.Now let's evaluate each option: A. None of the other options is correct.We cannot determine this without evaluating the other options first.B. Implement the rule so that the number of nominated candidates is greater than not implementing the rule.This option is not necessarily true.The rule does not guarantee that there will be more nominated candidates.It only ensures that a candidate knows their opponent before agreeing to be nominated.C. Implementing this rule, there is at most one candidate nominated.This option is incorrect.The rule states that there must be more than one candidate.Therefore, there cannot be only one nominated candidate.D. Implement the rule, no candidate may be nominated.This option is also incorrect.The rule does not prevent candidates from being nominated, it only requires that they know their opponent before agreeing to be nominated.ANSWER: A. None of the other options is correct.</li>
</ol>
<p>Figure 8: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQA dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: There are three types of robots in a laboratory.Step 2: Analyse each option to determine if it aligns with the characteristics of directed thinking.</p>
<p>A. Seeing the man with braids, he concluded that the man was a woman -This statement is based on past knowledge and experience (assuming that braids are typically associated with women).-It forms a direction (concluding that the person is a woman).B. The peach blossoms in Beijing are in bloom, so spring is coming -This statement is based on past knowledge and experience (assuming that peach blossoms bloom in spring).-It forms a direction (predicting that spring is coming).C. The clothes are broken.I want to sew them with a sewing bag -This statement is based on past knowledge and experience (knowing that sewing can fix broken clothes).-It forms a method or procedure (using a sewing bag to sew the clothes).D. None of the other options is correct.-This statement contradicts the analysis of options A, B, and C, which all align with the characteristics of directed thinking.</p>
<p>Step 3: Propose a solution based on the analysis.ANSWER: D</p>
<p>Figure 10: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQAv2 dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.With the help of Chain-of-Thought Prompting, GPT-4 can find out option D is contradicts the analysis of options A, B, and C, which all align with the characteristics of directed thinking.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: In today's society, many old people become old drifters in order to take care of their children and the next generation.In a recent urban survey,52.2percent of the young people surveyed admitted that their parents are old drifters.They and their partners have just started their careers, and work pressure is high, so they have no time to take care of their children and do housework.According to experts, the group of old drifters in China's cities will further expand.Question: Which of the following, if true, best supports the views of the above experts?Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:'.The 'ANSWER' is the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. Some old people prefer to live in their hometown B. The elderly in the city have better medical conditions than in the countryside C. None of the other options is correct.D. After the implementation of the two-child policy, the two-child birth rate in urban areas is lower than that in rural areas Correct Answer: "C" GPT-4: "B" Chain-of-Thought Prompting GPT-4:</p>
<p>Step 1: Identify the claim made by experts The claim made by experts is that the group of old drifters in China's cities will further expand.</p>
<p>Step 2: Evaluate each option based on its relevance to the claim A. Some old people prefer to live in their hometown This option does not directly support the claim that the group of old drifters in China's cities will further expand.It only states a preference of some old people.B. The elderly in the city have better medical conditions than in the countryside This option could indirectly support the claim, as better medical conditions might attract more elderly people to move to cities.However, it does not directly address the issue of old drifters taking care of their children and the next generation.C. None of the other options is correct.This option is not relevant to the claim.D. After the implementation of the two-child policy, the two-child birth rate in urban areas is lower than that in rural areas This option supports the claim that the group of old drifters in China's cities will further expand.If there are more children being born in urban areas due to the two-child policy, it is likely that more elderly people will move to cities to help take care of their children and grandchildren, thus increasing the number of old drifters in cities.</p>
<p>Step 3: Choose the option that best supports the claim ANSWER: D</p>
<p>LLaMA Touvron et al. (2023), AlpacaTaori et al. (2023), VicunaChiang et al. (2023),GPT-3.5OpenAI (2023a), and GPT-4 OpenAI (2023b) are large autoregressive language models pre-trained to perform next-token prediction.They are based on the decoder-part of the transformer architectureVaswani et al. (2017).GPT-3.5 was fine-tuned fromGPT-3 Brown et al. (2020)  using reinforcement learning from human feedback (RLHF) fine-tuning OpenAI (2023a).GPT-4 shows human-level performance on LSAT, SAT Math and Graduate Record Examination (GRE) Quantitative OpenAI (2023b), all of which assess reasoning and problem solving skills in different contexts.The version of GPT-4 that we use does not include multimodal training and its training details are not publicly available.Discriminative Large Language ModelsLReasoner Wang et al. (2022), MERIt Jiao et al. (2022), and AMR-LDA Bao et al. (2023) are language models fine-tuned for downstream logical reasoning tasks.While the generative large language models perform nexttoken prediction, these models are trained to perform classification tasks, such as multi-choice question answering.These models are based on encoder-only transformers Kenton &amp; Toutanova (2019), whose last layers are replaced with classification layers fine-tuned for different tasks.LReasoner uses logic-driven data augmentation to increase their performance and achieved #1 on the ReClor Yu et al. (2020) leaderboard 5 when it was proposed.MERIt uses relation extraction to construct new data from Wikipedia and performs intermediate training on that new data.It achieved #1 5 on the ReClor leaderboard when initially proposed, exceeding the performance of LReasoner.AMR-LDA uses AMRbased logic-driven data augmentation to construct a tuple of sentences (original sentence, logical equivalence sentence, logical inequivalence sentence) and train the model and then train on the downstream tasks.It achieved #2 5 on the ReClor leaderboard when initially</p>
<p>Figure 6 :
6
Figure6: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns ReClor dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.With the help of Chain-of-Thought Prompting, GPT-4 can find out the other options are incorrect and to select "None of the other options is correct."as the predicted answer.</p>
<p>Figure 9 :
9
Figure 9: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQA dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.</p>
<p>Figure 11 :
11
Figure 11: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQAv2 dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.</p>
<p>Please only return the letter in front of your predict correct option, A, B, C or D. A.</p>
<p>Can you predict the correct option for the given input?Input: If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.Question: If the statements above are true, which one of the following must be true?
Instruction Fine-Tuning/Prompting CasesInstruction: If you are not able to write your essays using a word processing program, you haveno keyboarding skills. B. If you are able to write your essays using a word processingprogram, you have at least some keyboarding skills.C. If you are not able to writeyour essays using a word processing program, you are not able to use a computer.D. If you have some keyboarding skills, you will be able to write your essays usinga word processing program.Response: "B"</p>
<p>Table 2 :
2
Comparison between base models and models prompted using CoT prompting.
ReClorLogiQALogiQAv2ShuffleShuffleShuffleRepAnsRepAnsRepAnsZero-shot evaluationAlpaca-7B0.01200.02300.0121Alpaca-7B-CoT0.01200.03370.0152Vicuna-7B0.06400.17970.1784Vicuna-7B-CoT0.13200.16740.1593GPT-3.50.18470.22860.2583GPT-3.5-CoT0.10880.16740.1722GPT-40.13120.16260.1686GPT-4-CoT0.18160.25230.21774.5 Logic-Driven Data AugmentationReClorLogiQALogiQAv2ModelsShuffleShuffleShuffleRepAnsRepAnsRepAnsZero-shot evaluationAlpaca-7B0.01200.01210.0121GPT-3.50.18470.25830.2583GPT-40.13120.16860.1686ReClor/LogiQA/LogiQAv2 single training setAlpaca-7B-IFT0.27200.22730.1548+ AMR-LDA0.04400.05220.0548ReClor + LogiQA + LogiQAv2 merged training setAlpaca-7B-IFT0.11400.13510.1128+ AMR-LDA0.00600.02450.0197Prompt augmentation using AMR-LDAAlpaca-7B-IPT-LDA0.03000.03680.0331Alpaca-7B-IFT-LDA0.48000.36860.2237GPT-3.5-IPT-LDA0.36670.46850.4971GPT-4-IPT-LDA0.87660.55100.7027</p>
<p>Table 3 :
3
Accuracy of evaluated models when adding AMR-LDA's logic-driven augmented data into the training set.We evaluate Alpaca-7B after instruction fine-tuning.
Datasets →ReClorLogiQALogiQAv2Perturbation Ratio ↓ OriginalShuffle OrderReplace AnswerShuffle RepAnsOriginalShuffle OrderReplace AnswerShuffle RepAnsOriginalShuffle OrderReplace AnswerShuffle RepAnsReClor/LogiQA/LogiQAv2 single training set with different ratio of data perturbation (Shuffle-RepAns)0%0.16800.52800.23600.27200.11050.34860.28410.22730.19120.21220.36580.15485%0.33400.37200.15600.17200.14900.13510.09980.09210.26950.15160.13380.112110%0.41400.43200.20400.23800.30720.28260.23500.24420.22620.09560.19630.172715%0.36200.38600.30600.33400.19040.20270.27950.23190.35370.17780.20010.172750%0.15400.14000.16600.16400.04300.05370.67280.65590.35370.20960.76860.7915
Table 4: Accuracy of Alpaca-7B model for transfer learning scenarios and different perturbation ratio applied to the training set.To make a fair comparison, We ensure that the size of each training set is consistent.</p>
<p>Table 6 :
6
Details of hyperparameters for instruction fine-tuning of generative large language models, Alpaca and Vicuna, on ReClor, LogiQA, and LogiQAv2.
Generative Large Language Models Fine-Tuningseed42batch size1num train epochs5max sequence length1024gradient Accumulation Steps16initial Learning Rate1e-05bf16Trueweight decay0warmup ratio0.03lr scheduler typecosinefsdpfull shard auto wrapfsdp transformer layer cls to wrap LlamaDecoderLayertf32TrueoptimiserAdamW</p>
<p>Table 7 :
7
Details of hyperparameters for fine-tuning of discriminative large language models, AMR-LDA, LReasoner and MERIt, on ReClor, LogiQA and LogiQAv2.</p>
<p>Table 8 :
8
A comparison of the performance of GPT-4 at different temperatures on the original ReClor dataset and the alternative ReClor datasets.
Dataset→ReClorTemperature↓ Original Shuffle Order Replace Answer Shuffle RepAns00.88800.88000.17600.14400.50.88200.88970.17200.148010.88200.87200.18400.1460</p>
<p>If you have the skill of a keyboard, you can write your essay using a word processing program.If you can use a computer, you have keyboarding skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program.",</p>
<p>"context": "If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.","question": "If the statements above are true, which one of the following must be true?", "answers": A. "If you are not able to write your essays using a word processing program, you have no keyboarding skills.B. "If you are able to write your essays using a word processing program, you have at least some keyboarding skills.If
AMR-LDA Prompt Augmentation Case StudyGPT-4 Input:</p>
<p>you don't have at least some keyboard skills, you can't write your essay with a word processing program. If you can use a com- puter, you have keyboarding skills. If you can write your essay with a word pro- cessing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program.",</p>
<p>C. "If you are not able to write your essays using a word processing program, you are not able to use a computer.</p>
<p>If you can use a computer, you can write your essay using word processing programs. If you can use a computer, you have keyboard- ing skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing</p>
<p>program.",D. "If you have some keyboarding skills, you will be able to write your essays using a word processing program.If</p>
<p>you can't write your essay with a word processing program, you don't have some keyboard skills. If you can use a computer, you have keyboarding skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program." GPT-4 output: B</p>
<p>Type A can recognise colors, type B can recognise shapes, and type C can neither recognise colors nor shapes.The laboratory experimented with robots 1 and 2 using red balls, basketballs, red squares, and blue squares, and ordered them to pick up the red balls, but number 1 picked up the red squares, and number 2 picked up the basketball.Question: Based on the above experiment, which of the following must be true?Please only return the letter in front of your predict correct option, A, B, C or D. A. Both No.1 and No.2 must not be type C. B. Both No.1 and No.2 must be type C. The experiment involved red balls, basketballs, red squares, and blue squares.The robots were ordered to pick up the red balls.Robot 1 picked up red squares.This means that Robot 1 can recognise the color red but not the shape of a ball.Therefore, Robot 1 must be Type A. Robot 2 picked up basketballs.This means that Robot 2 can recognise the shape of a ball but not the color red.Therefore, Robot 2 must be Type B. Now let's evaluate the options: A. Both No.1 and No.2 must not be type C. -This is true, as we determined that Robot 1 is Type A and Robot 2 is Type B. B. Both No.1 and No.2 must be type C. -This is false, as we determined that Robot 1 is Type A and Robot 2 is Type B. C. No.1 is A and No.2 is Type B. -This is true, as we determined that Robot 1 is Type A and Robot 2 is Type B. D. None of the other options is correct.-This is false, as options A and C are correct.</p>
<p>C. No.1 is A and No.2 is Type B. D. None of the other options is correct.Correct Answer: "D" GPT-4: "A" Chain-of-Thought Prompting GPT-4: Let's analyse the information given: 1. Type A can recognise colors.2. Type B can recognise shapes.3. Type C can neither recognise colors nor shapes.ANSWER: C</p>
<p>https://github.com/tatsu-lab/stanford_alpaca
https://github.com/lm-sys/FastChat
https://github.com/yuweihao/reclor
https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347
AMR-LDA Official GitHub Page
https://github.com/SiyuanWangw/LReasoner/tree/master
https://github.com/SparkJiao/MERIt</p>
<p>Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation. Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu, Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022). the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022)Cumberland Lodge, Windsor Great Park, United KingdomSeptember 2022</p>
<p>Enhancing logical reasoning of large language models through logic-driven data augmentation. Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wanjun Zhong, Gaël Gendron, Timothy Pistotti, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu, Paul Denny, Michael Witbrock, Jiamou Liu, The first edition of the Symposium on Advances and Open Problems in Large Language Models (LLM@IJCAI'23). Macao, ChinaAugust 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Deberta: decodingenhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. OpenReview.net, 2021</p>
<p>Adversarial examples for evaluating reading comprehension systems. Robin Jia, Percy Liang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning. Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie, 10.18653/v1/2022.findings-acl.276Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandMay 2022Association for Computational Linguistics</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT2019</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.119422019arXiv preprint</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, International Conference on Learning Representations. 2020</p>
<p>Task contamination: Language models may not be few-shot anymore. Changmao Li, Jeffrey Flanigan, 10.48550/arXiv.2312.163372023</p>
<p>Eliteplm: An empirical study on general language ability evaluation of pretrained language models. Junyi Li, Tianyi Tang, Zheng Gong, Lixin Yang, Zhuohao Yu, Zhipeng Chen, Jingyuan Wang, Wayne Xin Zhao, Ji-Rong Wen, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, 10.1109/TASLP.2023.3293046IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023a</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023barXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501Christian Bessiere20202020</p>
<p>A robust adversarial training approach to machine reading comprehension. Kai Liu, Xin Liu, An Yang, Jing Liu, Jinsong Su, Sujian Li, Qiaoqiao She, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2020b34</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2023a</p>
<p>. OpenAI. Gpt-4 technical report. 2023b</p>
<p>Robustlr: A diagnostic benchmark for evaluating logical robustness of deductive reasoners. Soumya Sanyal, Zeyi Liao, Xiang Ren, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Logic-driven context extension and data augmentation for logical reasoning of text. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan, 10.18653/v1/2022.findings-acl.127Findings of ACL 2022. Dublin, Ireland2022</p>
<p>Finetuned language models are zeroshot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022a</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022barXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022c</p>
<p>AbductionRules: Training transformers to explain unexpected inputs. Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock, 10.18653/v1/2022.findings-acl.19Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandMay 2022Association for Computational Linguistics</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, OpenReview.net. Addis Ababa, EthiopiaApril 26-30, 2020. 20208th ICLR 2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2528 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2528</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2528</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-273532791</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.17309v2.pdf" target="_blank">Literature Meets Data: A Synergistic Approach to Hypothesis Generation</a></p>
                <p><strong>Paper Abstract:</strong> AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2528.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2528.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPOREFINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HYPOREFINE (Literature-enhanced HYPOGENIC refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that integrates literature-derived paper summaries with an iterative, LLM-driven data-centric hypothesis generator (HYPOGENIC) via alternating literature- and data-based refinement agents to produce a shared hypothesis bank.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HYPOREFINE (Literature+Data iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-agent pipeline that extends HYPOGENIC by (1) initializing hypotheses using both training examples and paper summaries, (2) when new hypotheses are generated from a wrong-example pool W, iteratively refining them through alternating refinement agents: a data-driven refinement agent (consumes W) and a literature-based refinement agent (consumes literature summaries P), for max_refine rounds (set to 6), then returning the refined hypotheses to the HYPOGENIC hypothesis bank. It uses the same reward/update procedure as HYPOGENIC to rank and retain hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based, retrieval-augmented (literature + data), multi-agent pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences (deception detection, AIGC detection, mental stress detection, persuasive argument prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Hybrid: initialize from examples + paper summaries; iterative generate-and-evaluate loop using HYPOGENIC update logic; new hypotheses produced from wrong-example pool W are refined through alternating refinement agents (literature agent uses P, data agent uses W) for up to max_refine rounds; final hypotheses ranked by reward and integrated into hypothesis bank.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Human novelty annotation study: participants compare literature-based vs data-driven hypotheses and majority-vote labels; novelty also inferred qualitatively when hypotheses labeled 'Novel (from data)'.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility operationalized via empirical reward function inherited from HYPOGENIC (accuracy on held examples) and literature grounding during refinement (agents incorporate paper findings), and human judgments during utility study.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Explicitly balanced by combining literature grounding (plausibility) with data-driven reward-based selection (empirical performance); refinement alternates literature and data updates to inject both novelty and empirical validity. No formal optimization trade-off beyond this pipeline design is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Primary metrics: accuracy and macro F1 on held-out IND and OOD test sets; internal hypothesis reward r_i = (empirical accuracy over S_i) + α * log t / |S_i| (inspired by UCB) where α=0.5, used to rank and explore hypotheses during training.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic evaluation: multi-hypothesis LLM inference on IND/OOD datasets with accuracy/F1; cross-model transfer tests; Human evaluation: controlled studies measuring improvement in human accuracy (utility) and novelty annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Paper lists hyperparameters (bank size=20, num_init=10, α=0.5, w_max=10, k=10, max_refine=6), random seeds (5 seeds), dataset splits, LLM models used, and appendices with prompts—enabling reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Uses literature grounding during refinement and an LLM-based specificity booster to make literature-only hypotheses more concrete; redundancy checking and reward-based selection reduce retention of irrelevant hypotheses. No formal anti-hallucination classifier reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Human study improvements tested with t-tests (reported p-values: 0.01 for AIGC Detection human accuracy improvement of +14.19%; 0.04 for Deception Detection improvement of +7.44%).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>DECEPTIVE REVIEWS, GPTGC, LLAMAGC (derived from WRITINGPROMPTS + LLAMA/GPT generations), DREADDIT, PERSUASIVE PAIRS (social-science classification tasks; OOD/IND splits provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Integrating literature+data (best method, e.g., LITERATURE∪HYPOREFINE) outperforms few-shot by 8.97% OOD accuracy, outperforms literature-only by 15.75%, and outperforms HYPOGENIC by 3.37% on average OOD accuracy; HYPOGENIC itself outperforms few-shot by 5.61% on average. Human accuracy gains when given hypotheses: +14.19% (AIGC detection, 58.86%→73.05%) and +7.44% (Deception detection, 57.14%→64.58%). Cross-model transfer: generated hypotheses cause <3% performance change in 10/20 cross-model cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to zero-shot/few-shot prompting, literature-only generation, HYPOGENIC (data-only), and commercial baselines NOTEBOOKLM and HYPERWRITE: best literature+data method achieved +8.97% vs few-shot, +15.75% vs literature-only, +3.37% vs HYPOGENIC on OOD accuracy; NOTEBOOKLM/HYPERWRITE sometimes produced invalid/irrelevant hypotheses and performed worse.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Generated hypotheses included items labeled 'Novel (from data)', i.e., insights not present in the small literature corpus (e.g., certain indicators for AIGC or stress). No laboratory/experimental scientific discovery outside classification tasks was reported or experimentally validated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Limited, manually collected literature corpus (≤10 papers per task); no exhaustive hyperparameter search; human evaluation sample sizes modest (N=~60 total, limited power); manual selection of three hypotheses for human studies; no explicit formal hallucination detection or uncertainty quantification beyond reward/exploration term.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2528.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPOGENIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HYPOGENIC (data-driven hypothesis generation backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered data-driven hypothesis-generation pipeline (used as backbone) that initializes a hypothesis bank from examples and iteratively updates hypotheses by selecting top-k hypotheses, evaluating them on examples, tracking wrong-example pools, and generating new hypotheses from systematic failure cases; its reward uses an upper-confidence-bound inspired term.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HYPOGENIC</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative data-driven hypothesis generator: initialize H_D from initial examples D_init via LLM; during updates, for each training example s select top-k high-reward hypotheses from H_D, prompt LLM to predict and update hypothesis rewards; maintain a wrong-examples pool W (example added if ≥ w_hyp hypotheses predicted wrong); when |W| reaches w_max, generate new hypotheses from W and add to H_D by reward. Reward r_i = (accuracy over S_i) + α * log t / |S_i| (UCB-inspired) to balance exploitation/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based, data-driven iterative discovery (multi-armed bandit inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences classification tasks in this paper (but originally presented as a general data-driven hypothesis generator)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>LLM prompted on small examples to propose rules/hypotheses; iterative evaluation and reward-based selection; generate new hypotheses from failure cases (wrong-example pool); exploration-exploitation controlled via UCB-like reward term.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Empirical plausibility measured by hypothesis reward (accuracy on evaluation examples) and retained if high-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Balance achieved via the reward function that favors empirically accurate hypotheses but includes an exploration term (α log t / |S_i|) to allow new hypotheses to be tried; no explicit novelty regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Reward r_i (see formula), accuracy and F1 on IND/OOD datasets when used for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic evaluation via LLM-based inference using the hypothesis bank; cross-model inference tests; compared performance on IND/OOD datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Hyperparameters reported (α=0.5, w_max=10, k=10, bank size 20, num_init=10), seeds; implementation details referenced to Zhou et al. (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Exploration term in reward (α log t / |S_i|) functions analogously to uncertainty-driven exploration but is not a probabilistic uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used in experiments in this paper on DECEPTIVE REVIEWS, GPTGC, LLAMAGC, DREADDIT, PERSUASIVE PAIRS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>HYPOGENIC consistently outperforms few-shot by +5.61% average accuracy; on some IND tasks HYPOGENIC is best (e.g., certain LLAMA configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms few-shot and zero-shot baselines on average; literature+data methods outperform HYPOGENIC by +3.37% on OOD on average in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>May overfit to training datasets and produce hypotheses that perform well on in-distribution data but are less generalizable; undervalues literature-grounded hypotheses during update because reward is dataset-focused.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2528.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LITERATURE-ONLY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LITERATURE-ONLY (literature-driven hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-driven pipeline implemented by the authors that prompts LLMs with paper summaries (manually collected up to ~10 papers) to generate hypotheses grounded in prior research findings, and optionally applies a specificity booster to make hypotheses more concrete.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LITERATURE-ONLY</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-based literature summarization and hypothesis-generation method: collect relevant paper summaries manually, prompt LLMs to summarize findings and propose hypothesis pairs grounded in those summaries; post-process with an LLM-based specificity booster to add concrete examples/illustrations when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based, retrieval-augmented (literature-grounded generation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences tasks used in paper (deception, AIGC, stress, persuasion)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompt an LLM with curated paper summaries and task instructions to generate candidate hypotheses; optionally apply a specificity booster (LLM) to increase concreteness; produce a literature-based hypothesis bank.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Compared to data-driven hypotheses via human novelty/nuance study (participants judge whether one hypothesis contains novel information relative to another).</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is human-literate: hypotheses are grounded in cited literature; later validated empirically via inference experiments (accuracy/F1) and human utility studies.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Method emphasizes plausibility via literature grounding; can lack empirical adaptation to new datasets—authors compensate by combining with data-driven hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Downstream inference accuracy and F1 when hypotheses used in multi-hypothesis LLM inference; human-rated helpfulness frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic inference evaluation (IND/OOD) and human utility/novelty studies.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompts provided in Appendix; literature collection process described (manual search up to 10 papers), but corpus size and automated retrieval left as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Specificity booster applied to add concrete illustrations and reduce overly brief/vague hypotheses; literature grounding reduces chance of unfounded claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same social science datasets as other methods in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Literature-only methods performed worse than HYPOGENIC on average (literature+data improved over literature-only by 15.75% on OOD accuracy); NOTEBOOKLM and HYPERWRITE (other literature-based baselines) sometimes generated invalid/irrelevant hypotheses degrading performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Literature-only underperformed compared to data-driven HYPOGENIC and combined methods on OOD generalization; however literature hypotheses contributed complementary information in union/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires high-quality literature; limited literature corpus here (manually collected, up to 10 papers); literature-only hypotheses sometimes too brief or irrelevant without specificity boosting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2528.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UNION (LITERATURE ∪ DATA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Union and redundancy-elimination combination (LITERATURE ∪ HYPOGENIC / HYPOREFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanistic combination strategy that builds a final hypothesis bank by unionizing literature-based and data-driven hypothesis banks, removing redundant hypotheses via an LLM-based redundancy checker, and balancing selection between top data hypotheses and literature hypotheses up to a target bank size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Union + Redundancy Elimination</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure: generate two hypothesis banks (literature-driven and data-driven), run an LLM-based redundancy checker that builds a binary entailment matrix A over pairs (20×20) marking redundancy, rank hypotheses by training accuracy, and construct a final no-redundancy bank by selecting top n/2 from HYPOGENIC/HYPOREFINE and filling with literature hypotheses (with a specificity booster applied when needed).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based ensemble/merging strategy, redundancy-filtered retrieval-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences tasks in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Independent generation (literature-only and HYPOGENIC/HYPOREFINE), then unionized via redundancy checking and prioritized selection by empirical accuracy and randomization to reach final bank size (e.g., 20).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty assessed by human novelty/nuance study comparing literature vs data hypotheses (majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility for selected hypotheses is ensured by retaining hypotheses with high empirical reward and literature grounding; redundancy removal avoids repeated/low-information items.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Operationalized by taking top-performing data-driven hypotheses (empirical plausibility) and supplementing with literature hypotheses to preserve literature-grounded novelty; a specificity booster increases usability of literature items.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Final bank evaluated by downstream LLM multi-hypothesis inference accuracy/F1 on IND/OOD and by human utility metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic accuracy/F1 evaluation and human utility/novelty studies; ablation (remove top hypotheses to see drop) used to select hypotheses for human studies.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Detailed selection procedure and hyperparameters provided (bank size, selection quotas, redundancy matrix A construction method described).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Redundancy checker and literature grounding reduce retention of spurious/duplicative claims; specificity booster reduces vague/overgeneralized literature hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same as HYPOREFINE/HYPOGENIC experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Union approaches (especially LITERATURE∪HYPOREFINE) achieved best OOD performance across most task/model configurations; for some AIGC tasks union performed better than refinement which degraded performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Union outperformed HYPOGENIC on tasks where literature added useful signals; for two AIGC tasks refining with literature hurt performance (−13.64%) so union performed best there.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Random selection used to fill quotas may introduce variance; literature addition can harm performance when literature offers little useful signal for a task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2528.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Redundancy-Checker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based redundancy checker (entailment matrix A)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based module that checks pairwise redundancy/entailment among hypotheses to create a 0/1 matrix A used to remove redundant hypotheses from hypothesis banks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based redundancy checker</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given a hypothesis bank (size up to 20), the module prompts an LLM to evaluate each pair for entailment/redundancy, producing a 20×20 binary matrix A where A_{i,j}=1 if i entails or is redundant with j. The selection algorithm then avoids keeping redundant hypotheses when building the final bank.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based entailment/redundancy filter</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied across all tasks in the paper (social-science classification tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generator; used to post-process candidate hypotheses to detect redundancy by pairwise LLM entailment judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Helps plausibility indirectly by eliminating duplicated or entailed hypotheses and promoting diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Quality indirectly improved by measuring downstream inference performance with reduced redundancy; no separate metric for redundancy detection accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Validated empirically by observing improved downstream hypothesis bank composition and inference performance; no separate human label set for redundancy testing was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Procedure described (pairwise entailment with LLM) and matrix A structure explained; prompts presumably in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as part of pipelines that yielded best OOD accuracies; no standalone performance numbers for redundancy checker reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on LLM judgments which themselves can be inconsistent or biased; pairwise checking scales O(n^2) with bank size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2528.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Specificity Booster</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based specificity booster</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing LLM module applied to literature-only generated hypotheses to add concrete illustrations and examples, thereby increasing applicability and specificity for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Specificity Booster</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM prompt-based post-processor that takes brief, literature-derived hypotheses and augments them with concrete examples, illustrative details, and specificity so that they are more actionable for downstream multi-hypothesis inference; applied selectively (not applied to Llama-3.1-70B because it already produced specific hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based post-processing (augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science tasks in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Augmentation of existing literature-derived hypothesis text using LLM prompt to add specificity and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Aims to improve usability and plausibility by giving concrete exemplars, but no formal evaluation metric beyond downstream accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Indirectly evaluated via downstream inference accuracy improvement when specificity booster is used; no independent metric.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Observed to improve downstream performance for certain tasks where literature-only hypotheses were too brief.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Applied uniformly across specific tasks; settings noted in Appendix (which models received it).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Attempts to reduce vague/fabricated-sounding hypotheses by making them concrete; does not claim formal hallucination-proofing.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively to improve applicability of literature-only hypotheses; no standalone quantitative metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Applied to literature-only hypotheses which without it were sometimes too brief and less useful; with booster, literature contributions improved downstream utility.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Risk of adding unwarranted concrete details not present in literature (potential to introduce hallucinated specifics) — not formally quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2528.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-MINI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4O-MINI (referred to as GPT-4-MINI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial proprietary LLM (OpenAI) used as one of the backbone LLM models in the experiments for hypothesis generation, refinement, and multi-hypothesis inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4O-MINI (GPT-4-MINI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A proprietary generative pre-trained transformer model (GPT-4 family mini variant) used for initializing hypotheses, refining via HYPOREFINE, performing multi-hypothesis inference with chain-of-thought prompting, redundancy checking, and as baseline few-shot/zero-shot prompting agent.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used across social-science classification tasks in the paper (deception, AIGC detection, stress, persuasion)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompted in zero/few-shot settings for direct hypothesis generation (literature-only and data-driven) and used inside HYPOGENIC/HYPOREFINE pipelines as the model M.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility of its outputs assessed via downstream accuracy/F1 and human utility studies.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Downstream accuracy and F1 when used for multi-hypothesis inference; frequency of selection by humans for given hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used in automatic evaluations (IND/OOD) and cross-model inference experiments; human studies used GPT-4-MINI-generated hypotheses in some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Model name and temperature (1e-5) and max_tokens settings reported; cost estimates provided. Model is proprietary so exact reproduction depends on access.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used on the paper's DECEPTIVE REVIEWS, GPTGC, LLAMAGC, DREADDIT, PERSUASIVE PAIRS datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used within the paper's best literature+data method, achieved the reported OOD improvements; cross-model experiments indicate hypothesis transfer is robust for many cases (<3% change in 10/20 cases).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared against LLAMA-70B-I and used as a baseline for zero/few-shot prompting and literature-only generation; literature+data methods improved over few-shot baselines for GPT-4-MINI.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proprietary model (access and license constraints); LLMs tend to favor detecting their own generations which can bias AIGC detection tasks (noted as a cross-model outlier).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2528.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLAMA-70B-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLAMA-3.1-70B-INSTRUCT (referred to as LLAMA-70B-I)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open (Llama 3.1) family large language model used as an alternative backbone LLM in the experiments for hypothesis generation, refinement, and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLAMA-3.1-70B-INSTRUCT (LLAMA-70B-I)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A large open-weight instruction-tuned transformer model used to implement HYPOGENIC, HYPOREFINE, literature-based generation, redundancy checking, and multi-hypothesis inference; specificity booster was not applied to this model because it produced sufficiently specific literature hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (open-weight)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social-science tasks in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompting for literature-only and data-driven hypothesis generation; used inside HYPOGENIC/HYPOREFINE pipelines as model M.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Same downstream accuracy/F1 and human utility studies used to evaluate its outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Downstream accuracy and F1; bank-size, hyperparams same as for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic IND/OOD evaluation and cross-model transfer testing; used in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Compute and runtime details provided (4x A100 GPUs for full pipeline; 1.5 hours average run time), hyperparameters, seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same datasets as other systems in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In some IND tasks (e.g., LLAMA on certain IND datasets), HYPOGENIC with LLAMA achieved top performance; overall literature+data methods performed best on OOD across most configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared with GPT-4-MINI; hypotheses generated by one model generally transferred to the other with modest performance change in most cases.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Some cross-model transfer cases showed large degradation (notably LLAMA→GPT case for AIGC), possibly due to LLMs better recognizing their own generations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2528.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NOTEBOOKLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NotebookLM (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial agent-style research assistance tool from Google used as a literature-based hypothesis-generation baseline in the paper; in experiments it sometimes produced invalid or irrelevant hypotheses that degraded inference performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NotebookLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A research-assistant product that ingests uploaded literature and responds to prompts with source-grounded summaries and hypotheses; used here by uploading collected literature and prompting for hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based research assistant (commercial, retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied here as literature-based baseline for social science tasks</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>User uploads literature and prompts the NotebookLM interface to generate hypotheses grounded in the provided sources.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Relies on internal grounding to sources; judged by downstream inference performance in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Downstream accuracy/F1 when NotebookLM-generated hypotheses used for multi-hypothesis inference (reported to be degraded in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compared empirically in automatic evaluations; examples of invalid/irrelevant hypotheses from NotebookLM are shown in Appendix leading to degraded inference.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Usage described (upload literature then prompt), but internal model details proprietary.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used on the paper's tasks as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>NotebookLM-generated hypotheses sometimes invalid/irrelevant and degraded inference performance relative to other methods (see Appendix Table 9 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performed worse than HYPOGENIC and literature+data methods in many configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proprietary tool; quality depends on uploaded corpus and prompt engineering; produced invalid/irrelevant hypotheses in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2528.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPERWRITE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HyperWrite (OthersideAI) — Hypothesis Maker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial LLM-powered tool with a Hypothesis Maker function used as a literature-based baseline; it leverages pretrained LLM knowledge and literature input to propose hypotheses, but lacks a public technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HyperWrite (Hypothesis Maker)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A commercial hypothesis-generation feature that uses LLM pretraining and provided literature to output hypotheses; exact internals not publicly documented. Used here as an off-the-shelf literature-driven baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based research assistance (commercial)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied as baseline for social science tasks in paper</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompt-based generation using internal LLMs and any uploaded/contextual literature.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Assessed by downstream inference accuracy and qualitative inspection; some HyperWrite outputs were invalid/irrelevant in Appendix examples.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compared empirically in automatic evaluations; shown to produce some invalid hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Proprietary; usage described but internals proprietary.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used on the paper's tasks as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Produced invalid/irrelevant hypotheses in some tasks, leading to degraded inference compared to HYPOGENIC and literature+data methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Underperformed compared to the paper's combined literature+data methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No public technical documentation; variable output quality observed in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2528.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2528.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique used to elicit step-by-step reasoning from LLMs during multi-hypothesis based inference in this work, enabling the model to extract relevant hypotheses and reason about predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompting strategy that asks the LLM to 'think step by step' and produce intermediate reasoning when using the hypothesis bank to make predictions; used during multi-hypothesis inference to consider relevance and utility of hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Prompt engineering technique (reasoning elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used across social-science classification inference tasks in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generator; used to guide inference by eliciting detailed application of hypotheses to instances.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Improves plausibility of inference by making the model expose its chain of reasoning; plausibility judged by downstream accuracy and human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Used as part of the inference pipeline evaluated by accuracy/F1 on IND/OOD data and human utility metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Standard CoT-based LLM inference validated via automatic metrics and human studies.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompts for CoT provided in Appendix; temperature and token settings reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to the paper's datasets during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used consistently for multi-hypothesis inference; contributed to the reported OOD/IND accuracy/F1 results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used across all hypothesis methods for inference; no direct ablation numbers for CoT alone reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CoT can expose flawed or confident-but-wrong chains; no formal mitigation for such issues reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hypothesis generation with large language models <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
                <li>ResearchAgent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 1)</em></li>
                <li>BLADE: Benchmarking language model agents for data-driven science <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2528",
    "paper_id": "paper-273532791",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "HYPOREFINE",
            "name_full": "HYPOREFINE (Literature-enhanced HYPOGENIC refinement)",
            "brief_description": "A method introduced in this paper that integrates literature-derived paper summaries with an iterative, LLM-driven data-centric hypothesis generator (HYPOGENIC) via alternating literature- and data-based refinement agents to produce a shared hypothesis bank.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "HYPOREFINE (Literature+Data iterative refinement)",
            "system_description": "An LLM-agent pipeline that extends HYPOGENIC by (1) initializing hypotheses using both training examples and paper summaries, (2) when new hypotheses are generated from a wrong-example pool W, iteratively refining them through alternating refinement agents: a data-driven refinement agent (consumes W) and a literature-based refinement agent (consumes literature summaries P), for max_refine rounds (set to 6), then returning the refined hypotheses to the HYPOGENIC hypothesis bank. It uses the same reward/update procedure as HYPOGENIC to rank and retain hypotheses.",
            "system_type": "LLM-based, retrieval-augmented (literature + data), multi-agent pipeline",
            "scientific_domain": "Social sciences (deception detection, AIGC detection, mental stress detection, persuasive argument prediction)",
            "hypothesis_generation_method": "Hybrid: initialize from examples + paper summaries; iterative generate-and-evaluate loop using HYPOGENIC update logic; new hypotheses produced from wrong-example pool W are refined through alternating refinement agents (literature agent uses P, data agent uses W) for up to max_refine rounds; final hypotheses ranked by reward and integrated into hypothesis bank.",
            "novelty_assessment_method": "Human novelty annotation study: participants compare literature-based vs data-driven hypotheses and majority-vote labels; novelty also inferred qualitatively when hypotheses labeled 'Novel (from data)'.",
            "plausibility_assessment_method": "Plausibility operationalized via empirical reward function inherited from HYPOGENIC (accuracy on held examples) and literature grounding during refinement (agents incorporate paper findings), and human judgments during utility study.",
            "novelty_plausibility_balance": "Explicitly balanced by combining literature grounding (plausibility) with data-driven reward-based selection (empirical performance); refinement alternates literature and data updates to inject both novelty and empirical validity. No formal optimization trade-off beyond this pipeline design is reported.",
            "hypothesis_quality_metrics": "Primary metrics: accuracy and macro F1 on held-out IND and OOD test sets; internal hypothesis reward r_i = (empirical accuracy over S_i) + α * log t / |S_i| (inspired by UCB) where α=0.5, used to rank and explore hypotheses during training.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic evaluation: multi-hypothesis LLM inference on IND/OOD datasets with accuracy/F1; cross-model transfer tests; Human evaluation: controlled studies measuring improvement in human accuracy (utility) and novelty annotation.",
            "reproducibility_measures": "Paper lists hyperparameters (bank size=20, num_init=10, α=0.5, w_max=10, k=10, max_refine=6), random seeds (5 seeds), dataset splits, LLM models used, and appendices with prompts—enabling reproduction.",
            "hallucination_prevention_method": "Uses literature grounding during refinement and an LLM-based specificity booster to make literature-only hypotheses more concrete; redundancy checking and reward-based selection reduce retention of irrelevant hypotheses. No formal anti-hallucination classifier reported.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": "Human study improvements tested with t-tests (reported p-values: 0.01 for AIGC Detection human accuracy improvement of +14.19%; 0.04 for Deception Detection improvement of +7.44%).",
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "DECEPTIVE REVIEWS, GPTGC, LLAMAGC (derived from WRITINGPROMPTS + LLAMA/GPT generations), DREADDIT, PERSUASIVE PAIRS (social-science classification tasks; OOD/IND splits provided).",
            "performance_metrics": "Integrating literature+data (best method, e.g., LITERATURE∪HYPOREFINE) outperforms few-shot by 8.97% OOD accuracy, outperforms literature-only by 15.75%, and outperforms HYPOGENIC by 3.37% on average OOD accuracy; HYPOGENIC itself outperforms few-shot by 5.61% on average. Human accuracy gains when given hypotheses: +14.19% (AIGC detection, 58.86%→73.05%) and +7.44% (Deception detection, 57.14%→64.58%). Cross-model transfer: generated hypotheses cause &lt;3% performance change in 10/20 cross-model cases.",
            "comparison_with_baseline": "Compared to zero-shot/few-shot prompting, literature-only generation, HYPOGENIC (data-only), and commercial baselines NOTEBOOKLM and HYPERWRITE: best literature+data method achieved +8.97% vs few-shot, +15.75% vs literature-only, +3.37% vs HYPOGENIC on OOD accuracy; NOTEBOOKLM/HYPERWRITE sometimes produced invalid/irrelevant hypotheses and performed worse.",
            "validated_on_real_science": true,
            "novel_discoveries": "Generated hypotheses included items labeled 'Novel (from data)', i.e., insights not present in the small literature corpus (e.g., certain indicators for AIGC or stress). No laboratory/experimental scientific discovery outside classification tasks was reported or experimentally validated.",
            "limitations": "Limited, manually collected literature corpus (≤10 papers per task); no exhaustive hyperparameter search; human evaluation sample sizes modest (N=~60 total, limited power); manual selection of three hypotheses for human studies; no explicit formal hallucination detection or uncertainty quantification beyond reward/exploration term.",
            "uuid": "e2528.0",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HYPOGENIC",
            "name_full": "HYPOGENIC (data-driven hypothesis generation backbone)",
            "brief_description": "An LLM-powered data-driven hypothesis-generation pipeline (used as backbone) that initializes a hypothesis bank from examples and iteratively updates hypotheses by selecting top-k hypotheses, evaluating them on examples, tracking wrong-example pools, and generating new hypotheses from systematic failure cases; its reward uses an upper-confidence-bound inspired term.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "HYPOGENIC",
            "system_description": "Iterative data-driven hypothesis generator: initialize H_D from initial examples D_init via LLM; during updates, for each training example s select top-k high-reward hypotheses from H_D, prompt LLM to predict and update hypothesis rewards; maintain a wrong-examples pool W (example added if ≥ w_hyp hypotheses predicted wrong); when |W| reaches w_max, generate new hypotheses from W and add to H_D by reward. Reward r_i = (accuracy over S_i) + α * log t / |S_i| (UCB-inspired) to balance exploitation/exploration.",
            "system_type": "LLM-based, data-driven iterative discovery (multi-armed bandit inspired)",
            "scientific_domain": "Social sciences classification tasks in this paper (but originally presented as a general data-driven hypothesis generator)",
            "hypothesis_generation_method": "LLM prompted on small examples to propose rules/hypotheses; iterative evaluation and reward-based selection; generate new hypotheses from failure cases (wrong-example pool); exploration-exploitation controlled via UCB-like reward term.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Empirical plausibility measured by hypothesis reward (accuracy on evaluation examples) and retained if high-performing.",
            "novelty_plausibility_balance": "Balance achieved via the reward function that favors empirically accurate hypotheses but includes an exploration term (α log t / |S_i|) to allow new hypotheses to be tried; no explicit novelty regularizer.",
            "hypothesis_quality_metrics": "Reward r_i (see formula), accuracy and F1 on IND/OOD datasets when used for inference.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic evaluation via LLM-based inference using the hypothesis bank; cross-model inference tests; compared performance on IND/OOD datasets.",
            "reproducibility_measures": "Hyperparameters reported (α=0.5, w_max=10, k=10, bank size 20, num_init=10), seeds; implementation details referenced to Zhou et al. (2024).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Exploration term in reward (α log t / |S_i|) functions analogously to uncertainty-driven exploration but is not a probabilistic uncertainty quantification.",
            "benchmark_dataset": "Used in experiments in this paper on DECEPTIVE REVIEWS, GPTGC, LLAMAGC, DREADDIT, PERSUASIVE PAIRS.",
            "performance_metrics": "HYPOGENIC consistently outperforms few-shot by +5.61% average accuracy; on some IND tasks HYPOGENIC is best (e.g., certain LLAMA configurations).",
            "comparison_with_baseline": "Outperforms few-shot and zero-shot baselines on average; literature+data methods outperform HYPOGENIC by +3.37% on OOD on average in this study.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "May overfit to training datasets and produce hypotheses that perform well on in-distribution data but are less generalizable; undervalues literature-grounded hypotheses during update because reward is dataset-focused.",
            "uuid": "e2528.1",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LITERATURE-ONLY",
            "name_full": "LITERATURE-ONLY (literature-driven hypothesis generation)",
            "brief_description": "A literature-driven pipeline implemented by the authors that prompts LLMs with paper summaries (manually collected up to ~10 papers) to generate hypotheses grounded in prior research findings, and optionally applies a specificity booster to make hypotheses more concrete.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LITERATURE-ONLY",
            "system_description": "An LLM-based literature summarization and hypothesis-generation method: collect relevant paper summaries manually, prompt LLMs to summarize findings and propose hypothesis pairs grounded in those summaries; post-process with an LLM-based specificity booster to add concrete examples/illustrations when needed.",
            "system_type": "LLM-based, retrieval-augmented (literature-grounded generation)",
            "scientific_domain": "Social sciences tasks used in paper (deception, AIGC, stress, persuasion)",
            "hypothesis_generation_method": "Prompt an LLM with curated paper summaries and task instructions to generate candidate hypotheses; optionally apply a specificity booster (LLM) to increase concreteness; produce a literature-based hypothesis bank.",
            "novelty_assessment_method": "Compared to data-driven hypotheses via human novelty/nuance study (participants judge whether one hypothesis contains novel information relative to another).",
            "plausibility_assessment_method": "Plausibility is human-literate: hypotheses are grounded in cited literature; later validated empirically via inference experiments (accuracy/F1) and human utility studies.",
            "novelty_plausibility_balance": "Method emphasizes plausibility via literature grounding; can lack empirical adaptation to new datasets—authors compensate by combining with data-driven hypotheses.",
            "hypothesis_quality_metrics": "Downstream inference accuracy and F1 when hypotheses used in multi-hypothesis LLM inference; human-rated helpfulness frequencies.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic inference evaluation (IND/OOD) and human utility/novelty studies.",
            "reproducibility_measures": "Prompts provided in Appendix; literature collection process described (manual search up to 10 papers), but corpus size and automated retrieval left as future work.",
            "hallucination_prevention_method": "Specificity booster applied to add concrete illustrations and reduce overly brief/vague hypotheses; literature grounding reduces chance of unfounded claims.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same social science datasets as other methods in this paper.",
            "performance_metrics": "Literature-only methods performed worse than HYPOGENIC on average (literature+data improved over literature-only by 15.75% on OOD accuracy); NOTEBOOKLM and HYPERWRITE (other literature-based baselines) sometimes generated invalid/irrelevant hypotheses degrading performance.",
            "comparison_with_baseline": "Literature-only underperformed compared to data-driven HYPOGENIC and combined methods on OOD generalization; however literature hypotheses contributed complementary information in union/refinement.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Requires high-quality literature; limited literature corpus here (manually collected, up to 10 papers); literature-only hypotheses sometimes too brief or irrelevant without specificity boosting.",
            "uuid": "e2528.2",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "UNION (LITERATURE ∪ DATA)",
            "name_full": "Union and redundancy-elimination combination (LITERATURE ∪ HYPOGENIC / HYPOREFINE)",
            "brief_description": "A mechanistic combination strategy that builds a final hypothesis bank by unionizing literature-based and data-driven hypothesis banks, removing redundant hypotheses via an LLM-based redundancy checker, and balancing selection between top data hypotheses and literature hypotheses up to a target bank size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Union + Redundancy Elimination",
            "system_description": "Procedure: generate two hypothesis banks (literature-driven and data-driven), run an LLM-based redundancy checker that builds a binary entailment matrix A over pairs (20×20) marking redundancy, rank hypotheses by training accuracy, and construct a final no-redundancy bank by selecting top n/2 from HYPOGENIC/HYPOREFINE and filling with literature hypotheses (with a specificity booster applied when needed).",
            "system_type": "LLM-based ensemble/merging strategy, redundancy-filtered retrieval-augmented",
            "scientific_domain": "Social sciences tasks in this paper",
            "hypothesis_generation_method": "Independent generation (literature-only and HYPOGENIC/HYPOREFINE), then unionized via redundancy checking and prioritized selection by empirical accuracy and randomization to reach final bank size (e.g., 20).",
            "novelty_assessment_method": "Novelty assessed by human novelty/nuance study comparing literature vs data hypotheses (majority vote).",
            "plausibility_assessment_method": "Plausibility for selected hypotheses is ensured by retaining hypotheses with high empirical reward and literature grounding; redundancy removal avoids repeated/low-information items.",
            "novelty_plausibility_balance": "Operationalized by taking top-performing data-driven hypotheses (empirical plausibility) and supplementing with literature hypotheses to preserve literature-grounded novelty; a specificity booster increases usability of literature items.",
            "hypothesis_quality_metrics": "Final bank evaluated by downstream LLM multi-hypothesis inference accuracy/F1 on IND/OOD and by human utility metrics.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic accuracy/F1 evaluation and human utility/novelty studies; ablation (remove top hypotheses to see drop) used to select hypotheses for human studies.",
            "reproducibility_measures": "Detailed selection procedure and hyperparameters provided (bank size, selection quotas, redundancy matrix A construction method described).",
            "hallucination_prevention_method": "Redundancy checker and literature grounding reduce retention of spurious/duplicative claims; specificity booster reduces vague/overgeneralized literature hypotheses.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same as HYPOREFINE/HYPOGENIC experiments.",
            "performance_metrics": "Union approaches (especially LITERATURE∪HYPOREFINE) achieved best OOD performance across most task/model configurations; for some AIGC tasks union performed better than refinement which degraded performance.",
            "comparison_with_baseline": "Union outperformed HYPOGENIC on tasks where literature added useful signals; for two AIGC tasks refining with literature hurt performance (−13.64%) so union performed best there.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Random selection used to fill quotas may introduce variance; literature addition can harm performance when literature offers little useful signal for a task.",
            "uuid": "e2528.3",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-Redundancy-Checker",
            "name_full": "LLM-based redundancy checker (entailment matrix A)",
            "brief_description": "An LLM-based module that checks pairwise redundancy/entailment among hypotheses to create a 0/1 matrix A used to remove redundant hypotheses from hypothesis banks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-based redundancy checker",
            "system_description": "Given a hypothesis bank (size up to 20), the module prompts an LLM to evaluate each pair for entailment/redundancy, producing a 20×20 binary matrix A where A_{i,j}=1 if i entails or is redundant with j. The selection algorithm then avoids keeping redundant hypotheses when building the final bank.",
            "system_type": "LLM-based entailment/redundancy filter",
            "scientific_domain": "Applied across all tasks in the paper (social-science classification tasks)",
            "hypothesis_generation_method": "Not a generator; used to post-process candidate hypotheses to detect redundancy by pairwise LLM entailment judgments.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Helps plausibility indirectly by eliminating duplicated or entailed hypotheses and promoting diversity.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Quality indirectly improved by measuring downstream inference performance with reduced redundancy; no separate metric for redundancy detection accuracy reported.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Validated empirically by observing improved downstream hypothesis bank composition and inference performance; no separate human label set for redundancy testing was reported.",
            "reproducibility_measures": "Procedure described (pairwise entailment with LLM) and matrix A structure explained; prompts presumably in appendix.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": "Used as part of pipelines that yielded best OOD accuracies; no standalone performance numbers for redundancy checker reported.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Relies on LLM judgments which themselves can be inconsistent or biased; pairwise checking scales O(n^2) with bank size.",
            "uuid": "e2528.4",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Specificity Booster",
            "name_full": "LLM-based specificity booster",
            "brief_description": "A post-processing LLM module applied to literature-only generated hypotheses to add concrete illustrations and examples, thereby increasing applicability and specificity for inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Specificity Booster",
            "system_description": "An LLM prompt-based post-processor that takes brief, literature-derived hypotheses and augments them with concrete examples, illustrative details, and specificity so that they are more actionable for downstream multi-hypothesis inference; applied selectively (not applied to Llama-3.1-70B because it already produced specific hypotheses).",
            "system_type": "LLM-based post-processing (augmentation)",
            "scientific_domain": "Social science tasks in this paper",
            "hypothesis_generation_method": "Augmentation of existing literature-derived hypothesis text using LLM prompt to add specificity and examples.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Aims to improve usability and plausibility by giving concrete exemplars, but no formal evaluation metric beyond downstream accuracy.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Indirectly evaluated via downstream inference accuracy improvement when specificity booster is used; no independent metric.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Observed to improve downstream performance for certain tasks where literature-only hypotheses were too brief.",
            "reproducibility_measures": "Applied uniformly across specific tasks; settings noted in Appendix (which models received it).",
            "hallucination_prevention_method": "Attempts to reduce vague/fabricated-sounding hypotheses by making them concrete; does not claim formal hallucination-proofing.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": "Reported qualitatively to improve applicability of literature-only hypotheses; no standalone quantitative metric reported.",
            "comparison_with_baseline": "Applied to literature-only hypotheses which without it were sometimes too brief and less useful; with booster, literature contributions improved downstream utility.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Risk of adding unwarranted concrete details not present in literature (potential to introduce hallucinated specifics) — not formally quantified in the paper.",
            "uuid": "e2528.5",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4-MINI",
            "name_full": "GPT-4O-MINI (referred to as GPT-4-MINI)",
            "brief_description": "A commercial proprietary LLM (OpenAI) used as one of the backbone LLM models in the experiments for hypothesis generation, refinement, and multi-hypothesis inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4O-MINI (GPT-4-MINI)",
            "system_description": "A proprietary generative pre-trained transformer model (GPT-4 family mini variant) used for initializing hypotheses, refining via HYPOREFINE, performing multi-hypothesis inference with chain-of-thought prompting, redundancy checking, and as baseline few-shot/zero-shot prompting agent.",
            "system_type": "LLM-based (proprietary)",
            "scientific_domain": "Used across social-science classification tasks in the paper (deception, AIGC detection, stress, persuasion)",
            "hypothesis_generation_method": "Prompted in zero/few-shot settings for direct hypothesis generation (literature-only and data-driven) and used inside HYPOGENIC/HYPOREFINE pipelines as the model M.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility of its outputs assessed via downstream accuracy/F1 and human utility studies.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Downstream accuracy and F1 when used for multi-hypothesis inference; frequency of selection by humans for given hypotheses.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Used in automatic evaluations (IND/OOD) and cross-model inference experiments; human studies used GPT-4-MINI-generated hypotheses in some configurations.",
            "reproducibility_measures": "Model name and temperature (1e-5) and max_tokens settings reported; cost estimates provided. Model is proprietary so exact reproduction depends on access.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used on the paper's DECEPTIVE REVIEWS, GPTGC, LLAMAGC, DREADDIT, PERSUASIVE PAIRS datasets.",
            "performance_metrics": "When used within the paper's best literature+data method, achieved the reported OOD improvements; cross-model experiments indicate hypothesis transfer is robust for many cases (&lt;3% change in 10/20 cases).",
            "comparison_with_baseline": "Compared against LLAMA-70B-I and used as a baseline for zero/few-shot prompting and literature-only generation; literature+data methods improved over few-shot baselines for GPT-4-MINI.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Proprietary model (access and license constraints); LLMs tend to favor detecting their own generations which can bias AIGC detection tasks (noted as a cross-model outlier).",
            "uuid": "e2528.6",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLAMA-70B-I",
            "name_full": "LLAMA-3.1-70B-INSTRUCT (referred to as LLAMA-70B-I)",
            "brief_description": "An open (Llama 3.1) family large language model used as an alternative backbone LLM in the experiments for hypothesis generation, refinement, and inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLAMA-3.1-70B-INSTRUCT (LLAMA-70B-I)",
            "system_description": "A large open-weight instruction-tuned transformer model used to implement HYPOGENIC, HYPOREFINE, literature-based generation, redundancy checking, and multi-hypothesis inference; specificity booster was not applied to this model because it produced sufficiently specific literature hypotheses.",
            "system_type": "LLM-based (open-weight)",
            "scientific_domain": "Social-science tasks in this paper",
            "hypothesis_generation_method": "Prompting for literature-only and data-driven hypothesis generation; used inside HYPOGENIC/HYPOREFINE pipelines as model M.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Same downstream accuracy/F1 and human utility studies used to evaluate its outputs.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Downstream accuracy and F1; bank-size, hyperparams same as for other models.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic IND/OOD evaluation and cross-model transfer testing; used in ablations.",
            "reproducibility_measures": "Compute and runtime details provided (4x A100 GPUs for full pipeline; 1.5 hours average run time), hyperparameters, seeds.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same datasets as other systems in the study.",
            "performance_metrics": "In some IND tasks (e.g., LLAMA on certain IND datasets), HYPOGENIC with LLAMA achieved top performance; overall literature+data methods performed best on OOD across most configurations.",
            "comparison_with_baseline": "Compared with GPT-4-MINI; hypotheses generated by one model generally transferred to the other with modest performance change in most cases.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Some cross-model transfer cases showed large degradation (notably LLAMA→GPT case for AIGC), possibly due to LLMs better recognizing their own generations.",
            "uuid": "e2528.7",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "NOTEBOOKLM",
            "name_full": "NotebookLM (Google)",
            "brief_description": "A commercial agent-style research assistance tool from Google used as a literature-based hypothesis-generation baseline in the paper; in experiments it sometimes produced invalid or irrelevant hypotheses that degraded inference performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "NotebookLM",
            "system_description": "A research-assistant product that ingests uploaded literature and responds to prompts with source-grounded summaries and hypotheses; used here by uploading collected literature and prompting for hypothesis generation.",
            "system_type": "LLM-based research assistant (commercial, retrieval-augmented)",
            "scientific_domain": "Applied here as literature-based baseline for social science tasks",
            "hypothesis_generation_method": "User uploads literature and prompts the NotebookLM interface to generate hypotheses grounded in the provided sources.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Relies on internal grounding to sources; judged by downstream inference performance in this paper.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Downstream accuracy/F1 when NotebookLM-generated hypotheses used for multi-hypothesis inference (reported to be degraded in some cases).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compared empirically in automatic evaluations; examples of invalid/irrelevant hypotheses from NotebookLM are shown in Appendix leading to degraded inference.",
            "reproducibility_measures": "Usage described (upload literature then prompt), but internal model details proprietary.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used on the paper's tasks as a baseline.",
            "performance_metrics": "NotebookLM-generated hypotheses sometimes invalid/irrelevant and degraded inference performance relative to other methods (see Appendix Table 9 examples).",
            "comparison_with_baseline": "Performed worse than HYPOGENIC and literature+data methods in many configurations.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Proprietary tool; quality depends on uploaded corpus and prompt engineering; produced invalid/irrelevant hypotheses in this evaluation.",
            "uuid": "e2528.8",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HYPERWRITE",
            "name_full": "HyperWrite (OthersideAI) — Hypothesis Maker",
            "brief_description": "A commercial LLM-powered tool with a Hypothesis Maker function used as a literature-based baseline; it leverages pretrained LLM knowledge and literature input to propose hypotheses, but lacks a public technical report.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "HyperWrite (Hypothesis Maker)",
            "system_description": "A commercial hypothesis-generation feature that uses LLM pretraining and provided literature to output hypotheses; exact internals not publicly documented. Used here as an off-the-shelf literature-driven baseline.",
            "system_type": "LLM-based research assistance (commercial)",
            "scientific_domain": "Applied as baseline for social science tasks in paper",
            "hypothesis_generation_method": "Prompt-based generation using internal LLMs and any uploaded/contextual literature.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Assessed by downstream inference accuracy and qualitative inspection; some HyperWrite outputs were invalid/irrelevant in Appendix examples.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compared empirically in automatic evaluations; shown to produce some invalid hypotheses.",
            "reproducibility_measures": "Proprietary; usage described but internals proprietary.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used on the paper's tasks as a baseline.",
            "performance_metrics": "Produced invalid/irrelevant hypotheses in some tasks, leading to degraded inference compared to HYPOGENIC and literature+data methods.",
            "comparison_with_baseline": "Underperformed compared to the paper's combined literature+data methods.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "No public technical documentation; variable output quality observed in the study.",
            "uuid": "e2528.9",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "A prompting technique used to elicit step-by-step reasoning from LLMs during multi-hypothesis based inference in this work, enabling the model to extract relevant hypotheses and reason about predictions.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought prompting",
            "system_description": "Prompting strategy that asks the LLM to 'think step by step' and produce intermediate reasoning when using the hypothesis bank to make predictions; used during multi-hypothesis inference to consider relevance and utility of hypotheses.",
            "system_type": "Prompt engineering technique (reasoning elicitation)",
            "scientific_domain": "Used across social-science classification inference tasks in this paper",
            "hypothesis_generation_method": "Not a generator; used to guide inference by eliciting detailed application of hypotheses to instances.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Improves plausibility of inference by making the model expose its chain of reasoning; plausibility judged by downstream accuracy and human evaluation.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Used as part of the inference pipeline evaluated by accuracy/F1 on IND/OOD data and human utility metrics.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Standard CoT-based LLM inference validated via automatic metrics and human studies.",
            "reproducibility_measures": "Prompts for CoT provided in Appendix; temperature and token settings reported.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied to the paper's datasets during inference.",
            "performance_metrics": "Used consistently for multi-hypothesis inference; contributed to the reported OOD/IND accuracy/F1 results.",
            "comparison_with_baseline": "Used across all hypothesis methods for inference; no direct ablation numbers for CoT alone reported here.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "CoT can expose flawed or confident-but-wrong chains; no formal mitigation for such issues reported.",
            "uuid": "e2528.10",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hypothesis generation with large language models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "ResearchAgent: Iterative research idea generation over scientific literature with large language models",
            "rating": 1,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "BLADE: Benchmarking language model agents for data-driven science",
            "rating": 1,
            "sanitized_title": "blade_benchmarking_language_model_agents_for_datadriven_science"
        }
    ],
    "cost": 0.025499499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Literature Meets Data: A Synergistic Approach to Hypothesis Generation
22 Oct 2024</p>
<p>Haokun Liu haokunliu@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Yangqiaoyu Zhou zhouy1@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Mingxuan Li mingxuanl@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Chenfei Yuan 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Chenhao Tan chenhao@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Literature Meets Data: A Synergistic Approach to Hypothesis Generation
22 Oct 20242D7B3BAAC6C7286A72FEB90112AD0DE1arXiv:2410.17309v1[cs.AI]
AI holds promise for transforming scientific processes, including hypothesis generation.Prior work on hypothesis generation can be broadly categorized into theory-driven and datadriven approaches.While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other.To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation.We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over fewshot, 15.75% over literature-based alone, and 3.37% over data-driven alone).Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection.Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively.These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.</p>
<p>Introduction</p>
<p>"It is the theory that decides what can be observed."</p>
<p>-Albert Einstein Large language models (LLMs) excel at synthesizing information and hold promise for transforming hypothesis generation, a critical yet understudied step in scientific discoveries.Many recent studies recognize this potential and use LLMs to generate hypotheses (e.g., Yang et al., 2024b;Batista and Ross, 2024).We broadly categorize them into theory-driven and data-driven methods.* Equal contributions.</p>
<p>On one hand, theory-driven approaches leverage LLMs to review existing literature and generate novel hypotheses (Yang et al., 2024b;Baek et al., 2024).These methods have shown promising results in terms of the hypotheses' novelty, validity, and usefulness to researchers, while remaining grounded in established human knowledge (Si et al., 2024).However, they come with notable limitations: they require high-quality literature, struggle to adapt to new data, and lack empirical support.Data-driven approaches, on the other hand, propose hypotheses by discovering patterns in data (Zhou et al., 2024;Qiu et al., 2024).These hypotheses are data-adaptive and can exhibit strong performance in explaining the data.However, they could be too overly tailored to the specific datasets used, which can hinder their generalizability.</p>
<p>We hypothesize that theory can guide the discovery from data and propose to integrate literaturebased and data-driven hypothesis generation (see Figure 1).For the data-driven component, we use HYPOGENIC as the backbone (Zhou et al., 2024).HYPOGENIC leverages an LLM to initialize hypotheses from a small number of examples and then updates them iteratively to improve the quality of hypotheses.To enhance this process with literature insights, we introduce a literature-based hypothesis agent.This agent interacts with the data-driven hypothesis agent (HYPOGENIC), refining and maintaining a shared pool of hypotheses through continuous collaboration, ensuring that the hypotheses benefit from both data-driven adaptability and the grounding of existing scientific knowledge.In addition to the refinement approach, we also propose to directly unionize literature-based and data-driven hypotheses.</p>
<p>To comprehensively evaluate these hypotheses, we conduct automatic and human evaluation to assess their generalizability, utility, and novelty.We apply our method to address research questions in social sciences: deception detection, AI generated content (AIGC) detection, mental stress detection, and persuasive argument prediction.Automatic evaluation results show that integrating literature and data outperforms other baselines: 8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone in accuracy on out-ofdistribution datasets, a measure of generalizability.</p>
<p>Moreover, we conduct the first study to assess the utility of AI-generated hypotheses in improving human decision-making and show that our generated hypotheses improve human accuracy by 7.44% and 14.19% on deception detection and AIGC detection.Additionally, we find that literature-based and data-driven hypotheses complement each other, as one set often contains novel information not found in the other set.</p>
<p>In addition to our own implementation, we also use commercial ones such as NOTEBOOKLM (Google, 2024) and HYPERWRITE (OthersideAI, 2024) as baselines.</p>
<p>Data-Driven Hypothesis Generation</p>
<p>Our data-driven hypothesis generation largely follows HYPOGENIC in Zhou et al. (2024).Here we give a brief overview.During the initialization stage of HYPOGENIC, an LLM is prompted with a set of initial data instances D init from the training set D and asked to generate initial hypotheses that forms the initial hypothesis bank H D .</p>
<p>In the update stage, for each example s ∈ D, top k high-reward hypotheses from H D are selected and each used to prompt the LLM to make a prediction on s.The accuracy and reward of the k hypotheses are updated accordingly.Among the k hypotheses, if at least w hyp predicted wrong on s, s is added to a wrong examples pool W. Once the size of W reaches w max , a set of new hypotheses are generated from W and added to H D according to their reward.Inspired by the upper confidence bound (UCB) algorithm (Auer, 2003), the reward function of HYPOGENIC is defined as follows:
r i = (x j ,y j )∈S i I(y j = ŷj ) |S i | + α log t |S i | ,
where S i is the set of examples used to evaluate hypothesis h i , t is the training time step, and α is the reward coefficient that controls the exploration term of the reward function.</p>
<p>Integration of Literature-based and Data-driven Hypotheses</p>
<p>One main contribution of our work is proposing the first approach to integrating literature-based and data-driven hypothesis generation so that we can effectively leverage the strengths of each approach, increasing the generalizability and utility of generated hypotheses.We consider two strategies.</p>
<p>Refinement of literature-based hypotheses.HYPOREFINE integrates paper summaries from § 2.1 with HYPOGENIC.In the initialization stage of HYPOGENIC, an LLM is asked to generate initial hypotheses based on both a set of initial examples and paper summaries relevant to the task.</p>
<p>In the update stage, we propose an iterative refinement approach to integrate patterns from data and key findings from literature into new hypotheses.Specifically, each time HYPOGENIC generates a set of new hypotheses H 0 from the wrong examples pool W, these hypotheses are refined multiple rounds by a data-driven refinement agent and a literature-based refinement agent.Take R M as the refinement agent based on model M, each time H 0 is generated from the wrong examples pool W, it is iteratively refined as follows:
H i = R M (q i , H i−1 , P) if i mod 2 = 0 R M (q i , H i−1 , W) if i mod 2 = 1,
where P represents the literature information, W represents the data pool used to generate H 0 , and q being the queries.After max_refine rounds of refinement, the final hypothesis bank H max_refine is fed back to the HYPOGENIC pipeline.</p>
<p>The reward function and update process for the hypothesis bank H remain consistent with those of the original HYPOGENIC.</p>
<p>Union and redundancy elimination.As the reward function of HYPOGENIC focuses only on the hypotheses' performance on the datasets at hand, literature-based hypotheses are sometimes undervalued during the update stage.On occasions they can even be replaced by hypotheses that have especially good performances on data but are not necessarily generalizable on real-world tasks.To counter this issue, we use a union approach to mechanistically combine literature-based and data-based hypotheses.We first generate two hypothesis banks: one literature-based hypothesis bank and another bank using HYPOGENIC or HYPOREFINE.Then we build a redundancy checker to remove hypotheses that express overly similar or repeating information in each bank.Lastly, we construct the final hypothesis bank of size n by randomly choosing</p>
<p>Experiments</p>
<p>In this section, we introduce our evaluation framework and the tasks to operationalize it.</p>
<p>Evaluation Framework</p>
<p>Formally evaluating hypotheses requires rigorous protocols and vast amounts of resources.In this work, we mainly evaluate our generated hypotheses along two dimensions: utility and novelty.We perform both automatic and human evaluations to show that our generated hypotheses can help models and humans in challenging real-world classification tasks and bring novel information.</p>
<p>Automatic evaluation on out-of-distribution (OOD) and in-distribution (IND) datasets and cross-model inference.Since we work with classification tasks, a natural way of evaluating the hypotheses is prompting the LLMs to do inference with the hypotheses.For all the methods that generate hypotheses, we provide a new data example and all the generated hypotheses to the LLMs.Then we prompt the LLMs to first extract the most relevant hypotheses to the new example and make inference using the hypotheses.For detailed information about the prompts, please refer to Appendix A. For each task, we report accuracy and F1 scores on held-out OOD and IND sets for 5 different random seeds.Since we are most interested in the generalizability of the generated hypotheses, we focus on performance on the OOD set in the main paper.</p>
<p>In addition to predicting on out-of-distribution datasets, we test our hypotheses' generalizability by taking the hypotheses generated by one model and performing inference with another model.</p>
<p>Human evaluation on utility and novelty.We design human studies to assess the practical utility and novelty of the generated hypotheses.Specifically, we aim to (1) evaluate whether these hypotheses have meaningful impact on human decisionmaking and (2) determine whether data-driven and literature-driven hypotheses indeed offer distinct perspectives and contribute novel information to the unionized hypotheses pool.We run human studies on Deception Detection and AIGC Detection.Screenshots of the studies are in Appendix C. We pay participants at an average hourly rate of $12.</p>
<p>Human Study I: Utility in human decisionmaking.We recruit 60 participants on prolific.comand randomly assign them into experimental and control groups.The control group performs the task without hypotheses, while the experiment group is given a set of three generated hypotheses to complete the same task.Specifically, each participant is randomly assigned 14 instances, and we include attention check questions to ensure the quality of the collected responses.We evaluate the practical utility of our generated hypotheses by comparing the performance of the two groups.</p>
<p>We pick the hypotheses based on their impact on performance in an ablation setting.Specifically, we choose the top three hypotheses that cause the greatest drop in performance when removed from the hypotheses pool during multi-hypothesis inference.In addition, to motivate participants to perform at their best, we offer a bonus of $0.1 for each correctly predicted instance.</p>
<p>At the end of the study, participants in the experiment group are also asked to give overall ratings and an assessment of the given hypotheses.There are five scales: "Not at all helpful", "Slightly helpful", "Moderately helpful", "Very helpful", and "Extremely helpful".</p>
<p>Human study II: Novelty and nuance.To compare data-driven hypotheses and literature-driven hypotheses, we present one hypothesis of each type to participants and ask them to judge whether the second hypothesis provides meaningfully novel information that is not covered in the first hypothesis.</p>
<p>We sample 50 pairs of hypotheses (h 1 , h 2 ), one from literature-based and one from data-driven, with duplications removed within each group.We recruit 10 Prolific participants to annotate whether h 2 provides new information to h 1 for each pair.Each participant is randomly assigned to annotate 15 pairs.For each pair, we take the majority vote to determine the final novelty label.</p>
<p>Tasks</p>
<p>We consider four tasks in social sciences.Deception Detection is a widely studied problem in psychology and other social sciences (Granhag and Vrij, 2005).We use the dataset introduced by Ott et al. ( 2013) (DECEPTIVE REVIEWS), which consists of 800 genuine hotel reviews and 800 fake hotel reviews, as our IND dataset.For the OOD dataset, we use hotel reviews from different source websites and different cities (Li et al., 2013).AI-Generated Content (AIGC) Detection has attracted significant attention in recent years (Tang et al., 2023).Most existing works focus on developing black-box detection methods and rarely take interpretability into account (Wu et al., 2024).We thus build our own dataset for this task.We take 800 distinct prompts and human-written stories in the WRITINGPROMPTS dataset (Fan et al., 2018).Then we use the same prompts to generate AIwritten stories with LLAMA-3.1-70B-INSTRUCT(Dubey et al., 2024) and GPT-4O-MINI (OpenAI, 2023), constituting our LLAMAGC and GPTGC datasets.The IND data contains stories generated by the corresponding model.The stories generated by the other model are treated as OOD data.Mental Stress Detection from social media content is an important task in mental health (Lupien et al., 2009).We use DREADDIT, a corpus of lengthy Reddit posts with stress status labels developed by Turcan and McKeown (2019).The dataset contains 3.5k post segments annotated using Amazon Mechanical Turk, with labels indicating the presence or absence of stress in posts.Our IND and OOD sets are separated based on subreddits that the posts come from.Persuasive Argument Prediction examines persuasion and social interactions to reveal predictive cues of persuasiveness (Tan et al., 2016).We use PERSUASIVE PAIRS, a dataset with pairs of short texts constructed by Pauli et al. (2024).Within each pair of texts, one is from existing corpora with signals of persuasiveness, while the other one is generated by an LLM with instructions for it to be more/less persuasive than the one from existing corpora.We formulate this task as predicting the more persuasive one of each pair of texts.The dataset contains human-annotated ground-truth labels and is pre-processed by removing examples where there exists disagreement among annotators.The IND and OOD datasets are then created based on different original sources of texts.</p>
<p>For each task, we split the IND dataset with at least 200 examples in train set, 300 in test set (on which we perform inference), 300 in validation set, and sample at least 300 instances from OOD (see Appendix B.1 for more details).</p>
<p>Implementation and Baselines</p>
<p>Our method works with any LLM (M).We use GPT-4O-MINI and LLAMA-3.1-70B-INSTRUCT in this work.Throughout this paper, we refer to GPT-4O-MINI as "GPT-4-MINI" and LLAMA-3.1-70B-INSTRUCT as "LLAMA-70B-I".We compare our method with the following baselines.</p>
<ol>
<li>
<p>Zero-shot and few-shot prompting.We give the LLMs detailed task instructions (zero-shot) and optionally provide three demonstrating examples (few-shot).This approach does not involve any hypothesis.</p>
</li>
<li>
<p>Zero-shot hypothesis generation.Inspired by Qi et al. (2023), we provide specific task descriptions and instructions, and then we prompt the LLMs to generate hypotheses directly without incorporating literature or data.</p>
</li>
<li>
<p>Literature-driven hypothesis generation.We use the implementation in §2.1.In addition to our own implementation, we compare two of the recently released agent frameworks for scientific writing, NOTEBOOKLM (Google, 2024) and HYPERWRITE (OthersideAI, 2024)</p>
</li>
</ol>
<p>Results</p>
<p>We first present automatic evaluation results to demonstrate the utility of generated hypotheses for model inference.We then show that the generated hypotheses can improve human decision-making in challenging tasks and that literature-based and datadriven hypotheses provide unique insights from each other.</p>
<p>Automatic Evaluation</p>
<p>Hypotheses generated by combining information from literature and data achieves the best performance across all task and model configurations (Table 1).First, few-shot inference outperforms zero-shot inference for all task and model configurations, with an average improvement of 6.84% in accuracy.In addition, few-shot inference surpasses zero-shot generation and the best of literature-based methods on average accuracy by 7.21% and 6.78%, respectively, suggesting off-theshelve LLMs or literature alone does not generate effective hypotheses for predictive purposes.In fact, NOTEBOOKLM and HYPERWRITE can generate some invalid or irrelevant hypotheses, which degrades their inference performance (see Table 9 in Appendix).</p>
<p>In contrast, HYPOGENIC consistently outperforms few-shot inference, improving average accuracy by 5.61%, highlighting the advantage of data-driven hypotheses.Compared to few-shot inference, the hypotheses also offer more interpretable insights.Furthermore, our best hypothesis generation method combining literature and data outperforms HYPOGENIC by 3.37% on average (i.e., 8.97% over few-shot and 15.75% over literature-based methods), demonstrating the benefit of incorporating literature with data.</p>
<p>For DECEPTIVE REVIEWS, PERSUASIVE PAIRS, and DREADDIT, refining the hypotheses with literature consistently improves inference accuracy compared to HYPOGENIC, with a 3.92% improvement on average.On the other hand, refining the hypotheses with literature does not help with GPTGC and LLAMAGC, but the union of HYPOGENIC and hypotheses generated from literature consistently performs the best.Comparing with HYPOGENIC for these two tasks, refining the hypotheses with literature actually results in an accuracy drop by 13.64%.This is likely due to that the literature for AIGC detection has relatively few insights on interpretable features to detect AI generated contents, and refining the data-driven hypotheses with that information degrades performance.</p>
<p>To further illustrate our approach, we present a case study of our generated hypotheses in Table 3.For most cases, LITERATURE-ONLY and HYPOGENIC generate different hypotheses as in Case I: one is about first-person singular pronouns, while the other one is about past experiences.We include more details on the differences between hypotheses generated by different methods in § 4.2.More examples of hypotheses generated using LIT-ERATURE∪HYPOREFINE are in Table 8.</p>
<p>Under some cases, the methods can generate similar hypotheses, and HYPOREFINE improves the quality of the hypothesis.In Case II, all three hypotheses focus on balanced perspectives being indicative of truthful reviews.HYPOREFINE incorporates the "reviews that seem to be promoting a competitor" insight from LITERATURE-ONLY, while also capturing the emphasis on "lack of nuance" from HYPOGENIC.By doing so, HYPORE-FINE offers a more nuanced hypothesis that not only explains how deceptive reviews may manipulate reader emotions, but also provides specific examples to illustrate how balanced perspectives can contribute to truthful assessments.This combination of insights from literature and data allows HYPOREFINE to offer a more comprehensive and   1, the hypotheses generated from both literature and data performs the best on all methods for OOD datasets.</p>
<p>Generated hypotheses can be effectively transferred to a different model.To further check the generalization behavior of our generated hypotheses, we take the hypotheses from the bestperforming method with our literature+data approach and then use the other model to perform inference.Table 2 shows that the generated hypotheses from one model remain effective for the other model, the performance exhibits no significant change in most cases (&lt;3% in 10 out of 20 cases).Even with this performance drop, our methods still outperform the few-shot inference baseline by 3.76% and 3.66% on OOD and IND settings.This finding further demonstrates the robustness of our hypothesis generation method and hypothesisbased inference.A significant outlier case is for LLAMAGC OOD: when using LLAMA-70B-I-generated hypotheses for GPTGC (OOD) and ask GPT-4-MINI to perform hypothesis-based inference, the inference per-formance can degrade significantly.This can be due to innate deficits in the task setting, as LLMs tend to favor and better detect their own writing (Panickssery et al., 2024).</p>
<p>Human Evaluation</p>
<p>Generated hypotheses improve human decisionmaking in both AIGC Detection and Deception Detection.In AIGC Detection, the average human accuracy improves by 14.19% (58.86% → 73.05%) when we provide hypotheses as assistance.We perform a statistical t-test and obtain a p-value of 0.01, indicating that the improvement is significant.In Deception Detection, the introduction of hypotheses boosts human accuracy by 7.44% (57.14% → 64.58%), with a p-value of 0.04.</p>
<p>When hypotheses are present, participants would use them to assist decision-making for over 90% of the time.All three presented hypotheses are selected to be used with frequency greater than 30% (Table 4, Table 5 in the Appendix).For example, the most used hypothesis, with frequency of 44.55%, in AIGC detection is "Human-written texts tend to have a more conversational tone and colloquial language, while AI-generated texts tend to be more formal and lack idiomatic expressions."For both tasks, 100% of the participants find the hypotheses to be helpful, and over 40% find them to be "Very helpful" or "Extremely helpful".</p>
<p>Humans rate literature-based and data-driven hypotheses as distinct.We determine the novelty label based on majority vote from three human annotators.84% of the pairs are considered novel to each other for Deception Detection, and 80% are considered novel for AIGC Detection, demonstrating the complementarity between literature-based and data-driven approaches.</p>
<p>Case I: LITERATURE-ONLY and HYPOGENIC generate different hypotheses LITERATURE-ONLY: Deceptive reviews often contain a higher frequency of first-person singular pronouns, while truthful reviews may use these pronouns less frequently.HYPOGENIC: Reviews that reference the reviewer's previous experiences with the hotel brand or similar hotels are more likely to be truthful, while reviews that do not provide any context or comparison to past experiences are more likely to be deceptive.</p>
<p>Case II: LITERATURE-ONLY and HYPOGENIC generate similar hypotheses LITERATURE-ONLY: Truthful reviews often provide a balanced perspective, while deceptive reviews may seem overly promotional or biased towards a competitor.HYPOGENIC: Reviews that express a balanced perspective, mentioning both positive and negative aspects of the stay, are more likely to be truthful, whereas reviews that are overly positive or negative without nuance tend to be deceptive.HYPOREFINE: Reviews that present a balanced perspective by discussing both positive and negative aspects of the stay, particularly with specific examples (e.g., "The location was fantastic, but the air conditioning was broken"), are more likely to be truthful, while reviews that are excessively positive or negative without acknowledging any redeeming qualities (e.g., "This is the best hotel ever!" or "I will never stay here again!") tend to be more deceptive, as they may reflect an attempt to manipulate reader emotions rather than provide an honest assessment.</p>
<p>Related Work</p>
<p>Theory-driven hypothesis generation.Yang et al. (2024b) generates hypotheses from raw web corpus, but their method requires human annotated hypotheses from literature.Baek et al. (2024), Wang et al. (2024), and Ghafarollahi and Buehler (2024) use LLMs to create knowledge graph and generate hypotheses from existing literature.We implement our own literature-based generation because these papers either do not provide sufficient implementation details or require significant effort to adapt to new tasks.2024) uses LLMs to generate hypotheses and conducts comprehensive experiments to study human engagements with headlines.We choose HYPOGENIC as the backbone for data-driven hypothesis generation as their tasks are most similar to ours, and their approach to hypothesis updates integrates naturally into our refinement process.</p>
<p>Data</p>
<p>Automated scientific research with LLMs.</p>
<p>There is growing interest in developing LLMpowered methods and multi-agent frameworks to assist scientific research.Lu et al. (2024) designs an LLM agent to generate full research papers.Li et al. (2024) proposes a method to generate research ideas from existing literature and automatically implement and execute experiments.In contrast, our work focuses primarily on hypothesis generation, as we believe it is crucial to preserve human agency and oversight in the scientific research process.</p>
<p>To evaluate LLM generated hypotheses, Qi et al. ( 2023) examines whether they contain novel information not found in existing literature.Si et al. (2024) asks experts to rate the novelty of LLMproposed research ideas in the NLP domain.While these studies highlight LLMs' ability to generate novel hypotheses, they do not conduct human subject experiments to validate the effectiveness of hypotheses.To this end, we conduct the first human study to test the utility of LLM-generated hypotheses in supporting human decision-making.</p>
<p>Significant efforts have also been made to benchmark multi-agent frameworks on data analysis tasks (Majumder et al., 2024;Gu et al., 2024;Hu et al., 2024;Chen et al., 2024;Huang et al., 2024;Guo et al., 2024), literature processing and informa-</p>
<p>Hypotheses</p>
<p>Frequency of Selection</p>
<p>Hypothesis 1: AI-generated texts tend to use more elaborate and descriptive language, including adjectives and adverbs, to create a sense of atmosphere and immersion.Human-written texts, on the other hand, tend to be more concise and straightforward in their language use.</p>
<p>38.79%</p>
<p>Hypothesis 2: Human-written texts are more likely to contain errors or idiosyncrasies in grammar and punctuation, reflecting the natural imperfections of human writing, while AI-generated texts typically maintain a higher level of grammatical accuracy.</p>
<p>34.55%</p>
<p>Hypothesis 3: Human-written texts tend to have a more conversational tone and colloquial language, while AI-generated texts tend to be more formal and lack idiomatic expressions.</p>
<p>44.55%</p>
<p>No hypothesis selected 3.94%</p>
<p>Table 4: How often participants use hypotheses in AIGC Detection.We allow users to select multiple hypotheses for each instance they make prediction on, so the total frequency can exceed 100%.</p>
<p>tion retrieval tasks (Press et al., 2024;Ajith et al., 2024;Kang and Xiong, 2024;Zhang et al., 2024), and more general research tasks (Tian et al., 2024;Jansen et al., 2024).</p>
<p>Conclusion</p>
<p>We propose a novel approach that integrates literature and data to generate hypotheses, with extensive and systematic evaluations.Our method consistently outperforms all baselines, including existing literature-based and data-driven approaches.Furthermore, human evaluations reveal that our generated hypotheses also improve human decisionmaking in challenging tasks.</p>
<p>Limitations</p>
<p>Our automated evaluation uses two recent models on datasets across various domains, showing the effectiveness of our method across diverse settings.However, we did not further evaluate our hypotheses on some tasks that require representations beyond natural language, such as math problem solving and code generation.The literature corpus used for literature-based hypothesis generation is limited in terms of size and collection method.The collection is carried out by manually searching and collecting up to 10 papers on Semantic Scholar or Google Scholar.Though with the limited literature corpus we already show that our methods yield competent performance, a natural future direction is to enhance the literature component with automatic and scalable retrieval.</p>
<p>Similarly, we achieved satisfactory performance across different models and tasks with the initial set of hyperparameters.However, we did not perform an exhaustive hyperparameter search, which may have yielded further enhancements to the performance of our methods.This represents a limitation of our study that could be addressed in future work.</p>
<p>Our experiments with human subjects is a proof of concept.The number of participants in our human evaluation is relatively small.As a result, we do not believe that we have the statistical power to distinguish, for example, the difference between HYPOGENIC and HYPOREFINE.Although this is not the focus of our study, we encourage future work to conduct large-scale experiments in focused domains to validate the hypotheses generated through human-AI collaboration.</p>
<p>Last but not least, we manually chose three hypotheses through ablation-style study and subjective judgment for experiments with human subjects.We believe this process is the essence of human-AI collaboration in future scientific processes.It requires future exploration to identify the optimal collaboration regime.</p>
<p>A Prompts</p>
<p>All our prompts for LLMs are separated into system prompts and user prompts.System prompts contain role and tone information, followed by detailed descriptions of the task and the expected response format.User prompts contain useful information for hypothesis generation, refinement, or inference, including information from literature, instances from datasets, and previously generated hypotheses.Below are some examples of the prompts that we use for each task.</p>
<p>A.1 Deception Detection</p>
<p>System Prompt You're a professional hotel review analyst.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given examples, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided reviews.System Prompt You're a professional hotel review analyst.Given some key findings from a series of research papers, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given relevant literatures, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided reviews.</p>
<p>System Prompt</p>
<p>You're a social scientist working on a project to identify deceptive hotel reviews.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a review is truthful or deceptive.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided prevailing patterns in data if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 4: Hypothesis Refinement Based on Data.</p>
<p>System Prompt You're a social scientist working on a project to identify deceptive hotel reviews.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given relevant literatures, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided reviews.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a review is truthful or deceptive.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided key findings if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].Refined hypotheses:</p>
<p>Example 5: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt You are a professional deceptive detection agent and your job is to determine whether a hotel review is truthful or deceptive.In other words, we want to know whether the review is written by someone who had real experiences with the hotel.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current hotel review, and also predict whether the current hotel review is truthful or deceptive.Give an answer.The answer should be one word ( truthful or deceptive).</p>
<p>Give your final answer in the format of {Final answer: answer} User Prompt Our learned patterns: <generated_hypotheses> A hotel review is the following: <review> Given the pattern you learned above, give an answer of whether the hotel review above is deceptive or truthful.Think step by step.First step: Think about which pattern can be applied to the hotel review.Second step: Based on the pattern, is this hotel review deceptive or truthful?Final step: give your final answer in the format of {Final answer: answer} Example 6: Multiple-Hypothesis-Based Inference.</p>
<p>A.2 AI-Generated Content (AIGC) Detection</p>
<p>System Prompt You are a professional writer.You will be given a story prompt.Your task is to write a coherent and creative story based on the prompt.Ensure the story has a clear structure, engaging characters, and stays true to the prompt' s core idea.Keep the language clear and appropriate to the story's tone.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a piece of text is written by human or AI.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided prevailing patterns in data if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 12: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt</p>
<p>You are an AI generated content detection agent and want to determine whether a piece of text is written by a human or generated by an AI.In other words, we want to know whether the text is written by a human or generated by AI.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current text, and also predict whether the current text is written by human or AI.</p>
<p>Give an answer. The answer should be one word (AI or HUMAN).</p>
<p>Give your final answer in the format of "Final answer: ANSWER" User Prompt Our learned patterns: <generated_hypotheses> New text: Here is a story: <story> Given the patterns you learned above, give an answer of whether the current text is written by human or AI.Think step by step.First step: Think about which pattern can be applied to the story.Second step: Based on the pattern, is this story written by human or AI?You must give your final answer in the format of " Final answer: ANSWER".</p>
<p>Example 13: Multiple-Hypothesis-Based Inference.</p>
<p>A.3 Mental Stress Detection</p>
<p>System Prompt You're a psychologist and social scientist studying people's stress and their online posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.System Prompt You're a psychologist and social scientist studying people's stress and their online posts.Given some key findings from a series of research papers, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.</p>
<p>Using the given relevant literatures, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided posts.We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for deciding people's stress status (has stress or no stress) based on reddit post.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 17: Hypothesis Refinement Based on Data.</p>
<p>System Prompt You're a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.</p>
<p>Using the given relevant literatures, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided posts.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for deciding people's stress status (has stress or no stress) based on reddit post.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].Refined hypotheses:</p>
<p>Example 18: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt You're a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current reddit post, and also predict whether the poster of the reddit post has stress or not based on the content of the post.Give an answer.The answer should be "has stress" or "no stress".</p>
<p>Give your final answer in the format of {Final answer: answer} User Prompt Our learned patterns: <generated_hypotheses> A reddit post is the following: <post> Given the pattern you learned above, give an answer of whether the poster of the reddit post has stress or not based on the content of the post.</p>
<p>Think step by step.First step: Think about which pattern can be applied to the reddit post.Second step: Based on the pattern, does the poster of a reddit post has stress or not?Answer should be "has stress" or "no stress".</p>
<p>A.4 Persuasive Argument Prediction</p>
<p>System Prompt You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of observations of the format: Argument 1: [argument_1] Argument 2: [argument_2] Observation: The first/second argument uses more persuasive language.Based on the observations, please generate hypotheses that are useful for explaining why one argument uses more persuasive language than the other.</p>
<p>Proposed hypotheses:</p>
<p>Example 20: Data-Based Hypothesis Generation with HypoGeniC.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of literature of the format: Title: [title] Key Findings: [summary] Based on the literature, please generate hypotheses that are useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that you can find in the literature.They should also be generalizable to new instances.Please propose <num_hypotheses> refined hypotheses and generate them in the format of 1. [hypothesis ], 2. [hypothesis], ... <num_hypotheses>.[ hypothesis].</p>
<p>User Prompt</p>
<p>Here are some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• Please generate hypotheses that can help determine which argument uses more persuasive language.Please propose <num_hypotheses> possible hypotheses.</p>
<p>Generate them in the format of 1. [hypothesis], 2.</p>
<p>[hypothesis], ... <num_hypotheses>.[hypothesis].</p>
<p>Proposed hypotheses:</p>
<p>Example 21: Literature-Based Hypothesis Generation.</p>
<p>System Prompt You are a helpful assistant for summarizing key findings in research papers on a given topic.</p>
<p>User Prompt Summarize the following research paper, focusing ONLY on this question: What characterizes texts that use more persuasive language?In other words, how can one determine which one of two sentences uses more persuasive language?Focus on hypotheses of what characterizes texts that use more persuasive language, do not include technical details in the paper.... literature texts here ...</p>
<p>Example 22: Paper Summarization.</p>
<p>System Prompt You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of observations of the format: Argument 1: [argument_1] 2: [argument_2] Observation: The first/second argument uses more persuasive language.Based on the observations, please refine hypotheses provided to make them more useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that occur across the provided examples.They should also be generalizable to new instances.Please propose <num_hypotheses> refined hypotheses and generate them in the format of 1. [</p>
<p>Refined hypotheses:</p>
<p>Example 23: Hypothesis Refinement Based on Data.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of literature of the format: ••• information from literature here ••• Based on the literature, please refine hypotheses provided to make them more useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that you can find in the literature.</p>
<p>Refined hypotheses:</p>
<p>Example 24: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.From past experiences, you learned some patterns.Now, at each time, you should apply the learned patterns to a new pair of arguments and determine which one uses more persuasive language.The answer for the more persuasive language should be of the form "the _ argument" where _ is either first or second.Please give your final answer in the format of { Final answer: the _ argument uses more persuasive language} User Prompt Our learned patterns: <generated_hypotheses> Given the patterns you learned above, determine which of the following arguments uses more persuasive language: Argument 1: <first_argument> Argument 2: <second_argument> Think step by step.</p>
<p>Step 1: Think about which learned patterns can be applied to the arguments.</p>
<p>Step 2: Analyze the difference between "Argument 1" and "Argument 2".</p>
<p>Step 3: Based on the pattern, which argument uses more persuasive language?You MUST give your final answer in the following format: Final answer: the _ argument uses more persuasive language.</p>
<p>Example 25: Multiple-Hypothesis-Based Inference.</p>
<p>B Automated Experiments</p>
<p>Implementation Details  (Goffredo et al., 2023), Persuasion For Good (Wang et al., 2020), and Webis-Clickbait-17 (Potthast et al., 2018), while OOD dataset is from PT-Corpus (Da San Martino et al., 2019).</p>
<p>B.2 Specificity Boost</p>
<p>We further observed that sometimes the solely literature-based hypotheses generated by gpt-4omini are often too short and brief, making it harder to apply during inference.After removing redundancies of hypothesis banks, we unite two hypothesis banks to create a final bank H f inal with a balanced prioritization strategy.We first move the top 10 hypotheses from the HYPOGENIC or HYPOREFINE hypothesis bank to H f inal .If there is less than 10 hypotheses in the banks, we move all hypotheses to H f inal .Then we randomly choose hypotheses from the literature-based hypothesis bank until the size of f inal reaches 20.</p>
<p>B.4 Multiple-Hypothesis Inference Implementation</p>
<p>During multiple-hypothesis based inference, each time we feed a LLM with our final hypothesis bank of size 20 (see Appendix B.5) and an instance of our IND or OOD datasets with labels removed.The LLM is asked to generate an answer for the given instance using Chain-of-Thought prompting (Wei et al., 2022) that considers both the relevance of the hypotheses to the given instance and the utility of the hypothesis bank (see Appendix A for the exact prompts we used).For F1 scores, we report the macro-averaged F1 scores.</p>
<p>B.5 Technical Details of NotebookLM and HyperWrite</p>
<p>NotebookLM is an LLM-powered research assistance tool that generates source-grounding responses to user prompts.Specifically in our case, collected literature are uploaded in the Note-bookLM interface, followed by a hypothesis generation prompt asking to generate hypotheses based on given literature.Given its functionality and our usage, it is placed under the literature-based hypothesis generation category in our evaluations.</p>
<p>For HyperWrite, we use its Hypothesis Maker function, which is an AI-driven tool that generates hypotheses based on a given research question.Though there is no publicly available technical report for this tool, it generally leverages LLM's pretraining knowledge and literature information to produce hypotheses.</p>
<p>B.6 Hyperparameters</p>
<p>We use the same set of hyperparameters across all tasks, models, and methods.</p>
<p>During the training stage of HypoGeniC, the limit of the hypothesis bank size H is set to 20, and the size of training set is set to 200.In the initialization stage, we set num_init = 10.In the update stage, we use reward coefficient α = 0.5, w max = 10, k = 10, and generate 1 hypothesis per update.</p>
<p>In our HYPOREFINE method, the round of refinement max_refine is set to 6.</p>
<p>We use 5 random seeds for multiple-hypothesis inference: 11376, 8271, 39660, 543, 3.</p>
<p>Across all tasks and methods and for both GPT-4o-mini and Llama-3.1-70B-Instruct,we use temperature = 1 × 10 −5 and max_tokens = 4000.</p>
<p>B.7 Licensing Details</p>
<p>DECEPTIVE REVIEWS is released under CC BY-NC-SA 3.0, and PERSUASIVE PAIRS is released under CC BY-NC 4.0.The WRITINGPROMPTS dataset which we use to create the AIGC Detection datasets are under MIT License.The LLA-MAGC and GPTGC datasets will be released under the same licensing as this work, CC BY 4.0 License, should it be accepted.DREADDIT and FOUR-CITIES do not have licenses specified in their original papers, but are considered under CC BY 4.0 and CC BY-NC-SA 3.0 license respectively as they are ACL materials.</p>
<p>For the LLMs, GPT-4-MINI is a proprietary and not released under any open-source license, while LLAMA-70B-I is released under Llama 3.1 Community License Agreement.</p>
<p>Throughout our study, we find that we are in compliance with the licensing agreements of all the datasets and models used in this work.</p>
<p>B.8 Estimated Cost</p>
<p>For LLAMA-70B-I, we run all of our experiments with 4 NVIDIA A100s, and it takes on average 1.5 hours to run all of our hypothesis generation pipelines, including HYPOGENIC, HYPORE-FINE, LITERATURE∪HYPOGENIC , and LITER-ATURE∪HYPOREFINE .With GPT-4-MINI, the average cost for running the same pipelines is $0.6.</p>
<p>C Human Study Details C.1 Decision-making Utility Study Details</p>
<p>The instructions of the practical relevance study can be found in Figure 4 and Figure 6.For the interface, we present an example of the control group interface for Deception Detection in Figure 5, and examples of the experiment group interface in Figure 7.</p>
<p>The subjects of the control group are instructed to perform deception detection or AIGC detection tasks without any assistance from the hypotheses.Subjects in the experiment group are asked to first read the presented 3 hypotheses and then make their predictions on the given instance.They are then required to choose which ones, if any, of the hypotheses that were used in their prediction.At the end of the study, participants in the experiment group are also asked to give overall rating and assessment</p>
<p>Hypotheses</p>
<p>Frequency of Selection Hypothesis 1: Reviews present a balanced perspective by detailing both positive and negative experiences with specific examples (e.g., "the room was spacious and clean, but the noise from the street was disruptive at night") are more likely to be truthful, whereas reviews that express extreme sentiments without acknowledging any redeeming qualities (e.g., "everything was perfect" or "it was a total disaster") are more likely to be deceptive.</p>
<p>50.00%</p>
<p>Hypothesis 2: Reviews that mention specific dates of stay or unique circumstances surrounding the visit (e.g., "We stayed during the busy Memorial Day weekend and faced long lines") are more likely to be truthful, while reviews that use vague temporal references (e.g., "I stayed recently") without concrete details are more likely to be deceptive, as they often lack the specificity that suggests a real and engaged experience.</p>
<p>34.44%</p>
<p>Hypothesis 3: Reviews that provide detailed sensory descriptions of the hotel experience, such as the specific decor of the room, the quality of bedding, and the overall ambiance (e.g., "the room featured luxurious furnishings, highthread-count sheets, and soft lighting that created a relaxing atmosphere") are more likely to be truthful, while reviews that use vague or overly simplistic descriptors (e.g., "the hotel was nice and comfortable") are more likely to be deceptive.</p>
<p>46.39%</p>
<p>No hypothesis selected 7.50%</p>
<p>Table 5: How often humans use hypotheses in Deception Detection human study.We allow users to select multiple hypotheses for each instance they make prediction on, so the total frequency can exceed 100%.</p>
<p>of the helpfulness of the given hypotheses.There are five scales: "Not at all helpful", "Slightly helpful", "Moderately helpful", "Very helpful", and "Extremely helpful".</p>
<p>We choose top 3 hypotheses from the hypothesis bank generated using LITERA-TURE∪HYPOREFINE that cause the greatest drop in performance when removed from the hypotheses pool during multi-hypothesis inference.The chosen hypotheses for Deception Detection and AIGC Detection can be found in Table 4 and  Table 5.</p>
<p>We recruit 30 participants for the control group and 30 for the experimental group.For the control group, 4 people timed out, and 25 out of the remaining 26 participants passed attention checks.For the experimental group, 3 people timed out, and 22 out of the remaining 27 passed attention checks.We compute human accuracy based on responses from people who finished tasks in time and passed attention checks.The average time spent is around 25 minutes and participants are timed out by the system if they spend more than 60 minutes in the study, which can happen when they accidentally leave the study website tab open but forget to do the task.</p>
<p>C.2 Novelty and Nuance Study Details</p>
<p>For the Novelty and Nuance Study, we present the instructions for AIGC Detection in Figure 2. We showcase the interfaces for AIGC Detection in Figure 3.</p>
<p>For both Deception Detection and AIGC Detection, the two hypothesis banks compared are generated using LITERATURE-ONLY and HYPOGENIC respectively.</p>
<p>We recruit 10 participants each task and all particpants passed attention the check question.</p>
<p>C.3 IRB</p>
<p>We received IRB exempt (and will provide study number in the non-anonymous version of the paper).For both of the human studies, we present a detailed description of the study, incentives, risks and benefits, confidentiality, and contacts &amp; questions in our consent form.The study proceeds only if the participant agrees to give consent.</p>
<p>D Examples of Generated Hypotheses and Qualitative Analysis</p>
<p>We include examples of generated hypotheses using our LITERATURE∪HYPOREFINE approach and GPT-4-MINI, together with a brief qualitative analysis of its source in Table 8.We also showcase example hypotheses generated using NOTE-Table 7: Accuracy and F1 scores on the held-out IND datasets.Literature + data outperforms all other methods in 7 out of 10 configurations.For LLAMA-70B-I on GPTGC, LLAMAGC, and PERSUASIVE PAIRS, HYPOGENIC performs the best.This is likely due to that the literature in these tasks do not offer helpful information for the IND data, but they can still provide useful information for the tasks in general.As in Table 6, our approaches with literature + data performs the best in all configurations for the OOD datasets.</p>
<p>Dataset Generated Hypothesis</p>
<p>Literature Source/Novel DECEPTIVE REVIEWS Deceptive reviews often contain a higher frequency of first-person singular pronouns, while truthful reviews may use these pronouns less frequently.Li et al. (2014) The use of repetitive phrasing across multiple reviews is a strong indicator of deception, while truthful reviews are more likely to exhibit unique language and perspectives.</p>
<p>Maurya et al. (2022)</p>
<p>Reviews that provide specific accounts of the checkin and check-out processes, including exact times, the names of staff members involved, and descriptions of any unique features or services utilized (e.g., "I used the self-check-in kiosk at 3 PM"), are more likely to be truthful.Conversely, reviews that mention issues like long wait times or check-in problems without contextual details or specific examples (e.g., "the check-in took too long") are more likely to be deceptive.</p>
<p>Novel (from data)</p>
<p>GPTGC and LLAMAGC AI-generated content may struggle with maintaining coherence over longer passages, while human writing typically maintains clarity and focus.Posts that reflect on personal struggles with mental health or addiction (e.g., "I was a severe addict") are more likely to indicate that the poster has stress, while posts that discuss academic or professional experiences without emotional turmoil (e.g., "I've explained the aforementioned to people") are more likely to indicate that the poster does not have stress.</p>
<p>Novel (from data) PERSUASIVE PAIRS Persuasive texts that incorporate rhetorical devices, such as rhetorical questions and direct appeals, are more likely to engage the reader and compel them to consider the writer's viewpoint.</p>
<p>Wagemans (2023)</p>
<p>Texts that utilize strong, action-oriented verbs are generally more persuasive, as they convey confidence and urgency, compelling the audience to take action.</p>
<p>Novel (from data)</p>
<p>Arguments that include a clear and compelling call to action are more persuasive, as they provide the audience with a specific next step to take, reinforcing the urgency and importance of the message.</p>
<p>Novel (from data)</p>
<p>Table 8: Examples of generated hypotheses using our method accompanied by labels indicating their sources.For hypotheses from literature, we include the specific paper, while for hypotheses that are not explicitly suggested by our literature base, we set the label to "Novel (from data)".</p>
<p>Method</p>
<p>Figure 1 :
1
Figure 1: Illustration of how we combine literature-based and data-driven hypotheses.See algorithmic details in § 2.</p>
<p>n2</p>
<p>hypotheses from the literature-based hypothesis bank and adding the top n 2 hypotheses from the other hypothesis bank based on training accuracies.For detailed information of the implementation, please refer to Appendix B.3.</p>
<p>-driven hypothesis generation.Besides HYPOGENIC, we review additional works on discovering unseen patterns from data.Zhong et al. (2023) discovers patterns by analyzing difference between large corpora.Pham et al. (2024) makes discovery by generating and refining interpretable topics.Romera-Paredes et al. (2024) uncovers new solutions in open math problems by iteratively updating programs.Qiu et al. (2024) and Yang et al. (2024a) evaluate LLMs' ability in performing inductive reasoning in synthetic settings.Batista and Ross (</p>
<p>Using the given examples, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided reviews.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.User Prompt We have seen some hotel reviews: ••• more examples here •••</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.</p>
<p>Using the given examples, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided posts.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.User PromptWe have seen some reddit posts: ••• more examples here ••• Please generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.Propose <num_hypotheses> possible hypotheses.Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].Proposed hypotheses: Example 14: Data-Based Hypothesis Generation with HypoGeniC.</p>
<p>a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.Using the given examples, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided posts.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.User Prompt We have seen some reddit posts: ••• more examples here •••</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.</p>
<p>Final step: give your final answer in the format of {Final answer: answer} Example 19: Multiple-Hypothesis-Based Inference.</p>
<p>Figure 2 :
2
Figure 2: Instruction page for novelty check.</p>
<p>Figure 3 :
3
Figure 3: Annotation page for novelty check.</p>
<p>Figure</p>
<p>Figure Instruction page for prediction task without hypotheses.</p>
<p>Figure 5 :
5
Figure 5: Annotation page for prediction task without hypotheses.</p>
<p>Figure 6 :
6
Figure 6: Instruction page for prediction task with the guide of hypotheses.</p>
<p>Figure 7 :
7
Figure 7: Annotation page for prediction task with the guide of hypotheses.</p>
<p>reviews are more likely to be written in a style and tone that aligns with the reviewer's demographic information available on the platform, if any.<strong> Conversely, deceptive reviews might exhibit inconsistencies between the writing style and the reviewer's claimed demographic, signaling a potential fabrication.</strong>Truthful reviews are more likely to be posted at various times and days, reflecting the organic behavior of genuine guests.<strong>Conversely, deceptive reviews, particularly those orchestrated by paid posters, might be posted in clusters or at unusual times, indicating a coordinated effort.</strong>Truthful reviews are more likely to be written in a way that aligns with the overall sentiment expressed in the review's star rating.<strong>Conversely, deceptive reviews might show inconsistency between the sentiment expressed in the written content and the assigned star rating, indicating a potential attempt to manipulate perception.HYPERWRITE </strong>Relevant Images:<strong> Truthful reviews are more likely to include relevant images.Deceptive reviews less likely to include images.</strong>First-Person Pronouns:<strong> Truthful reviews use first-person pronouns (I, my).Deceptive reviews use third-person (one).</strong>Overly Formal Language:** Deceptive reviews use overly formal language.Truthful reviews use conversational tone.</p>
<p>Table 2 :
2
Cross-model inference performance.Performance on IND held-out datasets.Similarly with Table1, our hypothesis generation methods utilizing literature and data information are able to achieve the best accuracy and F1 scores in most cases on the held-out IND datasets (see Table 7 in the Appendix).For some cases, such as using Llama on the IND datasets for GPTGC, LLA-
explanatory hypothesis.
MAGC, and PERSUASIVE PAIRS, HYPOGENIC gets the top performance compared to other methods.This is not surprising, since HYPOGENIC generates hypotheses by looking at the IND data examples only.In contrast, our methods that take information from both literature and data may generate hypotheses that are more generally applicable but with slightly worse performance on the IND data, whereas in Table</p>
<p>Table 3 :
3
Examples of generated hypotheses from different methods.We show cases where LITERATURE-ONLY and HYPOGENIC generate different hypotheses or similar hypotheses, and how HYPOREFINE combines them in the case if they express unifiable ideas.</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.Example 2: Literature-Based Hypothesis Generation.What is useful for one to decide whether a review is truthful or deceptive in real life?Focus on hypotheses of what kind of reviews tend to be deceptive, do not include technical details in the paper.... literature texts here ...
User PromptWe have some key findings from a series ofresearch papers that might be useful forgenerating the required <num_hypotheses>.hypotheses:••• information from literature here •••Please generate hypotheses that are useful forpredicting whether a review is truthful ordeceptive.When generating hypotheses, remember not tooveruse your own knowledge. Always refer to thekey findings from research papers provided.Directly cite passages in the key findings whengenerating a hypothesis.Propose <num_hypotheses> possible hypotheses.Remember to generate <num_hypotheses> hypotheses!Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].Proposed hypotheses:System PromptYou are a helpful assistant for summarizing keyfindings in research papers on a given topic.User PromptSummarize the following research paper, focusingONLY on this question: Example 3: Paper Summarization.</p>
<p>Example 11: Hypothesis Refinement Based on Data.
list multiple constructs so if there are manyexamples.things changing , pick one);v. usable (i.e., a human equipped with thisGenerate refined hypotheses in the format of 1. [insight could use it to predict if a new piece ofhypothesis], 2. [hypothesis], ... <num_hypotheses>.text is generated AI in a similar way)[hypothesis].The hypotheses should analyze what kind of text isProposed hypotheses:likely to be written by human or AI.Example 8: Data-Based Hypothesis Generation withUser PromptHypoGeniC.We have seen some texts: ••• more examples here •••We have some hypotheses need to be refined:... hypotheses to be refined here ...System PromptPlease refine these hypotheses to make them moreYou're a professional AI content detector.specific and useful for predicting whether a pieceGiven some key findings from a series of researchof text is written by human or AI.papers, we want to generate hypotheses that areWhen refining the hypotheses, feel free to changeuseful for detecting whether a piece of text isthe key information or topic of a hypothesis basedwritten by human or AI.on the provided prevailing patterns in data ifUser Prompt you think it is necessary.Your task is to identify what patterns or traits... story-writing prompt here ... Generate refined hypotheses in the format of 1. [show up more in AI generated texts, and what showsexample: hypothesis], 2. [hypothesis], ... <num_hypotheses>.up more in human written texts. Focus on the[ WP ] You 've been able to read minds since you [hypothesis].generalizable insight that can be applied in otherturned 7 . Mostly you watch people 's thoughts Refined hypotheses:contexts. Ignore things that are specific to thispassively and undetected but one day someone talksstory. Do not make references this story they mayback .\nnot be for others.Using the given relevant literatures, pleaseExample 7: AIGC Detection Dataset Generation.propose <num_hypotheses> possible hypothesis pairs.System PromptYou're a an AI generated content detection expert.These hypotheses should identify specific patternsSystem Prompt You are great at detecting what type of text isthat occur across the provided texts.You're a an AI generated content detection expert. generated by AI.You are great at detecting what type of text is Given a set of texts, we want to generateGenerate them in the format of 1. [hypothesis], 2.generated by AI. hypotheses that are useful for predicting whether[hypothesis], ... <num_hypotheses>. [hypothesis].Given a set of texts, we want to generate a piece of text is generated by AI. In other words,The hypotheses should analyze what kind of text ishypotheses that are useful for predicting whether we want to know whether the text is written by alikely to be written by human or AI.a piece of text is generated by AI. In other words, human or generated by AI.we want to know whether the text is written by aUser Prompthuman or generated by AI. Using the given relevant literatures, refine theWe have some key findings from a series ofhypothesis pairs provided.research papers that might be useful forYour task is to identify what patterns or traits The desired hypotheses should identify specificgenerating the required <num_hypotheses>show up more in AI generated texts, and what shows patterns that occur across the provided texthypotheses:up more in human written texts. Focus on the examples.••• information from literature here •••generalizable insight that can be applied in otherPlease generate hypotheses that are useful forcontexts. Ignore things that are specific to this Generate refined hypotheses in the format of 1. [predicting whether a piece of text is written ofstory. Do not make references this story they may hypothesis], 2. [hypothesis], ... <num_hypotheses>.human or AI.not be for others. [hypothesis].Propose <num_hypotheses> possible hypotheses.The hypotheses should analyze what kind of text isRemember to generate <num_hypotheses> hypotheses!Using the given examples, please propose likely to be written by human or AI.Generate them in the format of 1. [hypothesis], 2.<num_hypotheses> possible hypothesis pairs.[hypothesis], ... <num_hypotheses>. [hypothesis].When proposing hypothesis, look closely into theProposed hypotheses:given examples and identify specific patterns thatoccur across the provided text examples.Example 9: Literature-Based Hypothesis Generation.The hypotheses should be clear, easy to understand, and have specific details such that one can applythe hypotheses to predict whether a piece of textSystem Promptis written by human or AI.You are a helpful assistant for summarizing key findings in research papers on a given topic.Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>. [hypothesis].User Prompt Summarize the following research paper, focusingThe hypotheses should analyze what kind of text is likely to be written by human or AI.ONLY on this question: What is useful for one to detect whether some text is generated by AI? Focus on hypotheses of what kind of text tend to be generated by AI, do not include technical details in the paper. ... literature texts here ...User Prompt We have seen some texts: ... more examples here ... Please generate hypotheses that are useful for predicting predicting whether a piece of text is written by human or AI.Example 10: Paper Summarization.Propose <num_hypotheses> possible hypotheses. Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].System PromptWhen proposing hypothesis, look closely into theYou're a an AI generated content detection expert.given examples and identify specific patterns thatYou are great at detecting what type of text isoccur across the provided text examples.generated by AI.Given a set of texts, we want to generatePlease make sure that the hypotheses are:hypotheses that are useful for predicting whetheri. clear (i.e., precise , not too wordy , and easya piece of text is generated by AI. In other words,to understand);we want to know whether the text is written by aii. generalizable to novel situations (i.e., theyhuman or generated by AI.would make sense if applied to other AI generatedcontent detection experiments or other messagingUsing the given examples, refine the hypothesiscontexts);pairs provided.iii. empirically plausible (i.e., this is aThe desired hypotheses should identify specificdimension on which messages can vary on);patterns that occur across the provided textiv. unidimensional (i.e., avoid hypotheses that</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.Example 15: Literature-Based Hypothesis Generation.
User PromptWe have some key findings from a series ofresearch papers that might be useful forgenerating the required <num_hypotheses>hypotheses:••• information from literature here •••Please generate hypotheses that are useful fordeciding people's stress status (has stress or nostress) based on reddit post.Propose <num_hypotheses> possible hypotheses.Remember to generate <num_hypotheses> hypotheses!Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].Proposed hypotheses:System PromptYou are a helpful assistant for summarizing keyfindings in research papers on a given topic.User PromptSummarize the following research paper, focusingONLY on this question: What is useful for one tojudge whether a reddit poster has stress based onone of their reddit post content?Focus on hypotheses of what kind of posts indicatestress, do not include technical details in thepaper.... literature texts here ...</p>
<p>These hypotheses should identify patterns, phrases, wordings etc. that occur across the provided examples.They should also be generalizable to new instances.Please propose <num_hypotheses> possible hypotheses and generate them in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].
User PromptHere are the Observations:••• more examples here •••Please generate hypotheses that can help determinewhich argument uses more persuasive language.Please propose <num_hypotheses> possiblehypotheses.Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].</p>
<p>Each time we take the best-performing hypothesis h out of the original hypothesis bank H and check if there exists a hypothesis h new in H new such that redundancy is recorded in A for the pair h and h new , i.e., A h,hnew = 1 or A hnew,h = 1.If yes, h is moved out of the original bank H and skipped; if not, h is moved to H new with a rank determined by its training accuracy.
specificity booster is not applied to Llama-3.1-70B-Instruct because it can already generate reasonablyspecific hypotheses.B.3 Refinement and Union ImplementationRefinement is implemented as an extensionbased on the original HypoGeniC pipeline. Dur-ing the initialization stage, an LLM is instructedto generate an initial hypothesis bank H based ona set of initial examples S init and a series of gen-erated paper summaries. These initial hypothesesare then evaluated and re-ranked using the samereward function as in HypoGeniC. In the updatestage, once the size of the wrong examples bankW reaches w max , 1 new hypothesis is generated byfeeding both the wrong examples bank and papersummaries to the LLM. H is then updated with thenew hypothesis according to the reward, followingthe same procedure as HypoGeniC.Union and Redundancy Elimination is imple-mented by combining the hypothesis bank gener-ated using HYPOGENIC or HYPOREFINE and thebank generated by our literature-based hypothe-sis generation method. We first generate the twohypothesis banks separately using HYPOGENIC,HYPOREFINE, and LITERATURE-ONLY, followingthe procedures described above and in Section 3.Each hypothesis bank is then fed to a redundancychecker module. For a hypothesis bank of size20, the LLM-based redundancy checker checkseach pair of hypotheses and see if one entails theother, with results recorded as a 20 × 20 matrix Aof 1 (redundant) or 0 (not redundant). To createthe new no-redundancy hypothesis bank H new , wefirst rank the hypotheses based on their trainingaccuracy.To address this, we adda LLM-based specificity booster after the literature-based hypothesis generation that adds more con-crete illustrations and examples to each of the hy-potheses based solely on its pre-training knowledge.Specifically we apply the specificity booster onour Deception Detection, Mental Stress Detection,and Persuasive Argument Prediction tasks. The</p>
<p>Tang et al. (2023)AI-generated texts are more likely to follow conventional narrative structures, while human-written texts may experiment with form and structure.Novel (from data)DREADDITPosts that show erratic posting behavior or changes in tone (e.g., from positive to negative) are more likely to indicate stress, while consistent posting patterns with a stable tone are more likely to indicate no stress.
Wan and Tian (2024)Posts that exhibit avoidance behaviors (e.g., avoidingDoan et al. (2017)social situations or responsibilities) are more likely toindicate stress, while posts that demonstrate proactiveengagement with challenges are more likely to indicateno stress.</p>
<p>Table 9 :
9
Examples of generated hypotheses using NOTEBOOKLM and HYPERWRITE on DECEPTIVE REVIEWS that are invalid or irrelevant, leading to degraded inference performance for these methods.</p>
<p>AcknowledgmentsWe thank members of the Chicago Human+AI Lab for their helpful comments.We also thank the anonymous participants on Prolific for participating in our study.Methods
Lit-Search: A retrieval benchmark for scientific literature search. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao, arXiv:2407.189402024Preprint</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. Peter Auer, J. Mach. Learn. Res. 32003</p>
<p>ResearchAgent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024Preprint</p>
<p>Words that work: Using language to generate hypotheses. M Rafael, James Batista, Ross, 2024</p>
<p>ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, arXiv:2410.050802024Preprint</p>
<p>Fine-grained analysis of propaganda in news article. Giovanni Da, San Martino, Seunghak Yu, Alberto Barrón-Cedeño, Rostislav Petrov, Preslav Nakov, Proceedings of EMNLP-IJCNLP. EMNLP-IJCNLP2019</p>
<p>How do you #relax when you're #stressed? a content analysis and infodemiology study of stress-related tweets. Son Doan, Amanda Ritchart, Nicholas Perry, Juan D Chaparro, Mike Conway, 10.2196/publichealth.59392017JMIR Public Health and Surveillance3e35</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, arXiv:2407.217832024Preprint</p>
<p>Hierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, Proceedings of ACL. ACL2018</p>
<p>Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024Preprint</p>
<p>Argument-based detection and classification of fallacies in political debates. Pierpaolo Goffredo, Mariana Chaves, Serena Villata, Elena Cabrio, 10.18653/v1/2023.emnlp-main.684Proceedings of EMNLP. EMNLP2023</p>
<p>. Google, 2024NotebookLM</p>
<p>Deception detection. Psychology and law: An empirical perspective. 2005Pär Anders Granhag and Aldert Vrij</p>
<p>BLADE: Benchmarking language model agents for data-driven science. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, M Tianmai, Lanyi Zhang, Mike A Zhu, Jeffrey Merrill, Tim Heer, Althoff, arXiv:2408.096672024Preprint</p>
<p>DS-Agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, Proceedings of ICML. ICML2024</p>
<p>InfiAgent-DABench: Evaluating agents on data analysis tasks. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, Fei Wu, Proceedings of ICML, Proceedings of Machine Learning Research. ICML, Machine Learning Research2024</p>
<p>MLAgentBench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Proceedings of ICML. ICML2024</p>
<p>DISCOVERYWORLD: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, arXiv:2406.067692024Preprint</p>
<p>Hao Kang, Chenyan Xiong, arXiv:2406.10291ResearchArena: Benchmarking llms' ability to collect and organize information as research agents. 2024Preprint</p>
<p>Identifying manipulated offerings on review portals. Jiwei Li, Myle Ott, Claire Cardie, Proceedings of EMNLP. EMNLP2013</p>
<p>Towards a general rule for identifying deceptive opinion spam. Jiwei Li, Myle Ott, Claire Cardie, Eduard Hovy, 10.3115/v1/P14-1147Proceedings of ACL. ACL2014</p>
<p>MLR-Copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024Preprint</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, Proceedings of ACL. ACLOnline2020</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024Preprint</p>
<p>Effects of stress throughout the lifespan on the brain, behaviour and cognition. Sonia J Lupien, Bruce S Mcewen, Megan R Gunnar, Christine Heim, Nature Reviews Neuroscience. 2009</p>
<p>Discov-eryBench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.017252024Preprint</p>
<p>Deceptive opinion spam detection approaches: a literature survey. Sushil Kumar Maurya, Dinesh Singh, Ashish , 10.1007/s10489-022-03427-1Applied Intelligence. 532Kumar Maurya. 2022</p>
<p>GPT-4 technical report. OthersideAI. 2024. HyperWrite. 2023OpenAI</p>
<p>Negative deceptive opinion spam. Myle Ott, Claire Cardie, Jeffrey T Hancock, Proceedings of NAACL. NAACL2013</p>
<p>Llm evaluators recognize and favor their own generations. Arjun Panickssery, R Samuel, Shi Bowman, Feng, arXiv:2404.130762024Preprint</p>
<p>Measuring and benchmarking large language models' capabilities to generate persuasive language. Amalie Brogaard, Pauli , Isabelle Augenstein, Ira Assent, arXiv:2406.177532024Preprint</p>
<p>Topicgpt: A prompt-based topic modeling framework. Minh Chau, Alexander Pham, Simeng Hoyle, Mohit Sun, Iyyer, Proceedings of NAACL. NAACL2024</p>
<p>Crowdsourcing a large corpus of clickbait on Twitter. Martin Potthast, Tim Gollub, Kristof Komlossy, Sebastian Schuster, Matti Wiegmann, Erika Patricia, Garces Fernandez, Matthias Hagen, Benno Stein, Proceedings of COLING. COLING2018</p>
<p>CiteME: Can language models accurately cite scientific claims?. Ori Press, Andreas Hochlehnert, Ameya Prabhu, arXiv:2407.128612024PreprintVishaal Udandarao, Ofir Press, and Matthias Bethge</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren ; Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, 10.1038/s41586-023-06924-6Proceedings of ICLR. Bernardino Romera-Paredes, Mohammadamin Barekatain. ICLR. Bernardino Romera-Paredes, Mohammadamin Barekatain2024. 2024625Mathematical discoveries from program search with large language models</p>
<p>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024Preprint</p>
<p>Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions. Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, Lillian Lee, 10.1145/2872427.2883081Proceedings of WWW. WWW2016</p>
<p>The science of detecting llm-generated texts. Ruixiang Tang, Yu-Neng Chuang, Xia Hu, arXiv:2303.072052023Preprint</p>
<p>Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Shengyan Li, Di Liu, Yutao Luo, Hao Ma, Kha Tong, Chenyu Trinh, Zihan Tian, Bohao Wang, Yanyu Wu, Shengzhu Xiong, Minhui Yin, Kilian Zhu, Yanxin Lieret, Genglin Lu, Yufeng Liu, Tianhua Du, Tao, arXiv:2407.13168SciCode: A research coding benchmark curated by scientists. Jamie CallanEliu Huerta, and Hao Peng2024Preprint</p>
<p>Dreaddit: A reddit dataset for stress analysis in social media. Elsbeth Turcan, Kathleen Mckeown, arXiv:1911.001332019Preprint</p>
<p>How to identify an argument type? on the hermeneutics of persuasive discourse. H M Jean, Wagemans, 10.1016/j.pragma.2022.11.015Journal of Pragmatics. 2032023</p>
<p>User stress detection using social media text: A novel machine learning approach. Xiangxuan Wan, Li Tian, 10.15837/ijccc.2024.5.6772International Journal of Computers Communications &amp; Control. 2024. 19</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proceedings of ACL. ACL2024</p>
<p>Persuasion for good: Towards a personalized persuasive dialogue system for social good. Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, Zhou Yu, arXiv:1906.067252020Preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, Proceedings of NeurIPS. NeurIPS2022</p>
<p>A survey on llm-generated text detection: Necessity, methods, and future directions. Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F Wong, Lidia S Chao, arXiv:2310.147242024Preprint</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of EACL. EACL2024a</p>
<p>Soujanya Poria, and Erik Cambria. 2024b. Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Proceedings of ACL. ACL</p>
<p>MASSW: A new dataset and benchmark tasks for ai-assisted scientific workflows. Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, Qiaozhu Mei, arXiv:2406.063572024Preprint</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Proceedings of NeurIPS. NeurIPS2023</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
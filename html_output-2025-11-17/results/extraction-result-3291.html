<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3291 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3291</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3291</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-72123a86eae2cb5c4eae8650f43524039d48875d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d" target="_blank">Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A knowledge distillation approach, that leverages the step-by-step CoT reasoning capabilities of larger models and distils these reasoning abilities into smaller models and boosts the performance of GPT-2 variants up to 35% when distilled with this approach compared to CoT.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3291.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3291.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A step-by-step prompting/annotation approach that elicits intermediate reasoning steps (rationales) from a language model and uses those steps as supervision or prompt context to improve multi-step reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (teacher) -> GPT-2 variants (students)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Teacher: GPT-3 (used in 175B and referenced 6B variants) prompted to produce chain-of-thought (CoT) solutions; Students: GPT-2 family fine-tuned on generated or ground-truth stepwise solutions (GPT-2 Small 124M, Medium 355M, Large 774M).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Teacher: 175B (GPT-3), referenced: 6B; Students: 124M, 355M, 774M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT)', 'step-by-step intermediate solutions (no subquestioning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT: prompt or generate an ordered sequence of intermediate reasoning steps leading to the answer. In this paper CoT annotations are produced by GPT-3 when ground-truth stepwise annotations are absent, then used to fine-tune GPT-2 students with a standard autoregressive LM loss on the produced step sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style: single method (step-by-step CoT) without explicit subquestion decomposition. Determined by using only CoT-generated intermediate steps as supervision or prompt exemplars; compared experimentally against Socratic (subquestion) approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, StrategyQA, SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>GSM8K: grade-school math word problems (2–8 steps); StrategyQA: factual implicit-reasoning QA with True/False answers and supporting facts; SVAMP: diverse multi-step arithmetic problems (eval from ASDiv training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported final-answer accuracies when fine-tuning GPT-2 students on CoT-generated stepwise supervision: GSM8K - Small(124M): CoT 4.70% vs AnswerOnly 1.45% and GT Steps 5.05%; Medium(355M): CoT 7.10% vs GT Steps 7.88%; Large(774M): CoT 12.85% vs GT Steps 14.10%. StrategyQA - Medium(355M): CoT 55.01% vs AnswerOnly 54.10% and GT Facts 52.02%; Large(774M): CoT 55.90% vs AnswerOnly 61.10% and GT Facts 62.80%. SVAMP - Small: CoT 5.35% (AnswerOnly 2.15%); Medium: CoT 17.30% (AnswerOnly 4.80%); Large: CoT 23.60% (AnswerOnly 7.40%). (All percentages are accuracy as reported in Table 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT (LLM-generated) is compared to: (a) training on only final answers (AnswerOnly), (b) training on ground-truth stepwise annotation (GT Steps), and (c) Socratic CoT (subquestioning). Findings: LLM-generated CoT often improves over AnswerOnly, but is typically worse than GT Steps when those exist; on some datasets CoT helps smaller models substantially (e.g., SVAMP). CoT supervision sometimes underperforms when generated facts are noisy (StrategyQA).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT supervision distilled from a large teacher into smaller models improves performance over training on final answers alone, but is generally inferior to high-quality human step-by-step annotations; CoT helps more when no GT steps exist and for smaller student models, but noisy LLM-generated supporting facts can harm performance on factual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>When high-quality ground-truth step-by-step solutions are available (GSM8K), fine-tuning on LLM-generated CoT is not advantageous compared to GT steps. On StrategyQA, GPT-3-generated supporting facts (used as CoT) sometimes hurt performance (e.g., CoT 58.07% vs GT Facts / AnswerOnly higher), likely due to imperfect factual accuracy from the teacher.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3291.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3291.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SocCot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socratic Chain-of-Thought (Socratic CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel distillation/supervision method that augments chain-of-thought with explicit subquestion–solution pairs: either trained as a unified model that generates subquestion-solution sequences or as iterative pair of models (Question Generator + Question Answerer). Proposed in this paper to inject structured decomposition into small models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 teacher -> Socratic student(s) (GPT-2 variants); student architectures: Unified GPT-2 and Iterative QG+QA (GPT-2 based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Teacher (GPT-3 175B) is prompted to produce sequences of subquestion-solution pairs; students are GPT-2 variants fine-tuned either as unified generator (produce entire sequence) or as two models: QG (question generator) and QA (question answerer) that operate iteratively at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Teacher: 175B (GPT-3); Students: GPT-2 Small 124M, Medium 355M, Large 774M (Unified or iterative split across QG/QA both as GPT-2 variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Socratic CoT (subquestioning + stepwise solutions)', 'Unified generation (single model outputs subquestion-solution pairs)', 'Iterative QG+QA (two models: question generator and question answerer)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Socratic CoT: each intermediate reasoning step is explicitly represented as a subquestion q^{(j)} together with its solution s^{(j)}; two distillation strategies: (a) Unified student trained to autoregressively generate sequences of (q,s) pairs; (b) Iterative: separate QG model generates q^{(j)} conditioned on P and previous steps, and a QA model answers each q^{(j)} conditioned on P and previous solutions, using teacher forcing during training.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse: introduces an orthogonal reasoning style (decomposition into subquestions) in addition to standard CoT's stepwise solutions; diversity is produced via explicit subquestioning and by splitting functionality across QG and QA, and is evaluated vs. (similar-style) CoT and GT annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, StrategyQA, SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multi-step reasoning datasets: GSM8K math word problems with GT subquestions available for a subset; StrategyQA factual binary QA with supporting facts/decompositions; SVAMP multi-step arithmetic derived from ASDiv.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Socratic CoT (LLM-generated subquestions + student fine-tuning) improved student accuracy substantially in many cases: GSM8K - Iterative Socratic CoT: Small 5.98% -> Socratic_CoT (Iterative) 6.44% (↑20% over baseline in table), Medium: Iterative 11.57% -> Socratic_CoT 12.74% (↑38%), Large: Iterative 17.89% -> Socratic_CoT 21.08% (↑33%). StrategyQA - Medium: Socratic_CoT 60.31% (↑13% vs baseline), Large: Socratic_CoT 66.40% (↑5%). SVAMP - improvements for smaller students: Small Iterative Socratic 6.79% (best for small), Medium Iterative 18.99% (best for medium), Large best remained CoT 23.60% for that dataset. (Numbers are accuracies reported in Table 2 and highlighted by the authors for Socratic CoT columns.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparisons include: (1) Socratic CoT vs CoT (LLM-generated stepwise supervision): Socratic CoT often outperforms CoT for small and medium GPT-2 students (not always for largest student in every dataset). (2) Iterative (QG+QA) vs Unified: Iterative tends to perform better when QG is learned and QA uses previous steps iteratively (example GSM8K: Iterative 11.57% vs Unified 7.90% for Medium). (3) Socratic CoT with GT subquestions (Soc_GT) vs Socratic CoT with LLM-generated subquestions (Soc_CoT): using GT to prompt LLM to generate subquestions gives stronger supervision and better student performance. (4) Ablations: removing explicit subquestion module (train to output full chain without QG) degrades performance; adding guidance (conditioning QG on predicted equations/step counts) improves QG quality and downstream accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit subquestioning (Socratic CoT) provides a distinct reasoning style that, when distilled into smaller models, improves multi-step reasoning accuracy compared to similar-style CoT supervision in many settings, especially for small/medium students and when GT or high-quality subquestions are available to guide generation. Iterative QG+QA decomposition is effective and often outperforms a unified model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Socratic CoT is not uniformly better: (a) When high-quality GT step-by-step solutions exist, direct fine-tuning on GT steps can match or surpass some Socratic variants; (b) On StrategyQA, using noisy LLM-generated supporting facts combined with subquestioning can hurt accuracy (decomposition based on imperfect facts harms performance); (c) For some larger student models (e.g., GPT-2 XL 1.5B listed), Socratic variants sometimes decreased accuracy (an observed -4% in one row) relative to training on GT facts, and on SVAMP the plain CoT supervision outperformed Socratic variants for the largest student.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3291.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3291.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unified vs Iterative</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified generation vs Iterative QG+QA student architectures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two implementation strategies for Socratic CoT distillation: a single unified student that outputs full subquestion–solution sequences, and an iterative decomposition into a question-generation model (QG) and a question-answering model (QA) that cooperate at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 variants as Unified or as separate QG and QA models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified: single autoregressive GPT-2 model trained to generate ordered (q,s) pairs. Iterative: two GPT-2 models where QG generates subquestions conditioned on P and prior steps, and QA answers each subquestion conditioned on P and prior solutions; teacher forcing used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Students: GPT-2 Small 124M, Medium 355M, Large 774M (applied both as unified and split QG/QA models).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Unified Socratic generation', 'Iterative Question Generation + Question Answering']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Unified uses single-model autoregressive loss over concatenated (q,s) tokens; Iterative uses two models with separate token-level losses and teacher forcing during training; at inference QG produces q^{(j)} which QA answers sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar in high-level objective (produce subquestion-solution pairs) but architecturally diverse: one monolithic generator vs a decomposed pipeline; diversity arises from explicit separation of subquestion generation and answering.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, StrategyQA, SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above; iterative approach was applied across datasets to evaluate whether architectural separation helps distil subquestioning behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GSM8K examples: Medium GPT-2: Unified 7.90% vs Iterative 11.57%; Large: Unified 13.25% vs Iterative 17.89%. SVAMP: Medium: Unified 17.62% vs Iterative 18.99%; Small: Unified 5.82% vs Iterative 6.79%. StrategyQA: mixed results; iterative and unified performance varied by size and supervision source.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Iterative QG+QA repeatedly outperformed Unified in most small/medium student settings, indicating benefits to separating decomposition (QG) from solution (QA). Unified sometimes matched GT-step training when high-quality subquestion-solution sequences were available, but iterative generally yielded higher accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Architectural separation of subquestion generation and answering (iterative) is beneficial: it consistently improves student accuracy on GSM8K and SVAMP for small/medium models relative to a unified generator.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Unified sometimes performed comparably (especially when GT annotations were used or for certain larger students), and training the student to produce chains without any explicit subquestion module (No SubQ) produced markedly worse performance (Table 5: e.g., GPT-2 Large NoSubQ 8.18% vs SubQ with QG 17.89%).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3291.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3291.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompted Socratic CoT (GPT-3 few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted Socratic Chain-of-Thought via LLM prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using prompting (single-shot in this paper) to make a large LLM (GPT-3 175B) generate subquestion decompositions in-context, and then using the same LLM to solve subproblems, demonstrating subquestioning as a prompting strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (175B) prompted 1-shot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 (175B) was given single-shot exemplars where problems are decomposed into subquestions and used to both decompose unseen problems and solve them in a single-shot pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Prompted Chain-of-Thought (CoT)', 'Prompted Socratic CoT (subquestion prompting / sub-ques)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>One-shot prompting: include exemplar decomposition (subquestions and solutions) then ask GPT-3 to generate subquestion decompositions for new problems and/or to answer decomposed subproblems in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse prompting strategies compared: standard CoT prompting ('Let's think step by step') vs Socratic prompting that explicitly structures the prompt into subquestions; diversity realized by differing exemplar formats in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K (prompting experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>GSM8K grade-school math problems; authors limited to single-shot prompts due to budget.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Single-shot GPT-3 results on GSM8K: CoT prompting accuracy 27.5%; Prompted Socratic CoT (sub-ques) accuracy 47.1% (Table 3). Authors note large variance (±41%) for Sub-ques measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Prompted Socratic CoT substantially outperformed standard CoT prompting in the single-shot setup (≈+19.6 percentage points absolute increase reported), indicating that decomposing into subquestions in the prompt improves chain reasoning for the LLM itself.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Subquestion-structured exemplars in prompts can substantially boost the reasoning accuracy of a large LLM compared to standard CoT prompting (single-shot), demonstrating that diverse prompting styles (subquestions) are effective for in-context reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Authors limited to single-shot prompts; they acknowledge that using more exemplars (few-shot up to 8) might change absolute performance and a fairer comparison vs CoT that used more exemplars in prior work; also reported high variance (±41%) in the sub-ques result.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Explanations from large language models make small reasoners better <em>(Rating: 1)</em></li>
                <li>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3291",
    "paper_id": "paper-72123a86eae2cb5c4eae8650f43524039d48875d",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought reasoning",
            "brief_description": "A step-by-step prompting/annotation approach that elicits intermediate reasoning steps (rationales) from a language model and uses those steps as supervision or prompt context to improve multi-step reasoning performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (teacher) -&gt; GPT-2 variants (students)",
            "model_description": "Teacher: GPT-3 (used in 175B and referenced 6B variants) prompted to produce chain-of-thought (CoT) solutions; Students: GPT-2 family fine-tuned on generated or ground-truth stepwise solutions (GPT-2 Small 124M, Medium 355M, Large 774M).",
            "model_size": "Teacher: 175B (GPT-3), referenced: 6B; Students: 124M, 355M, 774M",
            "reasoning_methods": [
                "Chain-of-Thought (CoT)",
                "step-by-step intermediate solutions (no subquestioning)"
            ],
            "reasoning_methods_description": "CoT: prompt or generate an ordered sequence of intermediate reasoning steps leading to the answer. In this paper CoT annotations are produced by GPT-3 when ground-truth stepwise annotations are absent, then used to fine-tune GPT-2 students with a standard autoregressive LM loss on the produced step sequences.",
            "diversity_of_methods": "Similar style: single method (step-by-step CoT) without explicit subquestion decomposition. Determined by using only CoT-generated intermediate steps as supervision or prompt exemplars; compared experimentally against Socratic (subquestion) approaches.",
            "reasoning_task_name": "GSM8K, StrategyQA, SVAMP",
            "reasoning_task_description": "GSM8K: grade-school math word problems (2–8 steps); StrategyQA: factual implicit-reasoning QA with True/False answers and supporting facts; SVAMP: diverse multi-step arithmetic problems (eval from ASDiv training).",
            "performance_by_method": "Reported final-answer accuracies when fine-tuning GPT-2 students on CoT-generated stepwise supervision: GSM8K - Small(124M): CoT 4.70% vs AnswerOnly 1.45% and GT Steps 5.05%; Medium(355M): CoT 7.10% vs GT Steps 7.88%; Large(774M): CoT 12.85% vs GT Steps 14.10%. StrategyQA - Medium(355M): CoT 55.01% vs AnswerOnly 54.10% and GT Facts 52.02%; Large(774M): CoT 55.90% vs AnswerOnly 61.10% and GT Facts 62.80%. SVAMP - Small: CoT 5.35% (AnswerOnly 2.15%); Medium: CoT 17.30% (AnswerOnly 4.80%); Large: CoT 23.60% (AnswerOnly 7.40%). (All percentages are accuracy as reported in Table 2.)",
            "comparison_of_methods": "CoT (LLM-generated) is compared to: (a) training on only final answers (AnswerOnly), (b) training on ground-truth stepwise annotation (GT Steps), and (c) Socratic CoT (subquestioning). Findings: LLM-generated CoT often improves over AnswerOnly, but is typically worse than GT Steps when those exist; on some datasets CoT helps smaller models substantially (e.g., SVAMP). CoT supervision sometimes underperforms when generated facts are noisy (StrategyQA).",
            "key_findings": "CoT supervision distilled from a large teacher into smaller models improves performance over training on final answers alone, but is generally inferior to high-quality human step-by-step annotations; CoT helps more when no GT steps exist and for smaller student models, but noisy LLM-generated supporting facts can harm performance on factual tasks.",
            "counter_examples_or_negative_results": "When high-quality ground-truth step-by-step solutions are available (GSM8K), fine-tuning on LLM-generated CoT is not advantageous compared to GT steps. On StrategyQA, GPT-3-generated supporting facts (used as CoT) sometimes hurt performance (e.g., CoT 58.07% vs GT Facts / AnswerOnly higher), likely due to imperfect factual accuracy from the teacher.",
            "uuid": "e3291.0"
        },
        {
            "name_short": "SocCot",
            "name_full": "Socratic Chain-of-Thought (Socratic CoT)",
            "brief_description": "A novel distillation/supervision method that augments chain-of-thought with explicit subquestion–solution pairs: either trained as a unified model that generates subquestion-solution sequences or as iterative pair of models (Question Generator + Question Answerer). Proposed in this paper to inject structured decomposition into small models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 teacher -&gt; Socratic student(s) (GPT-2 variants); student architectures: Unified GPT-2 and Iterative QG+QA (GPT-2 based)",
            "model_description": "Teacher (GPT-3 175B) is prompted to produce sequences of subquestion-solution pairs; students are GPT-2 variants fine-tuned either as unified generator (produce entire sequence) or as two models: QG (question generator) and QA (question answerer) that operate iteratively at inference.",
            "model_size": "Teacher: 175B (GPT-3); Students: GPT-2 Small 124M, Medium 355M, Large 774M (Unified or iterative split across QG/QA both as GPT-2 variants).",
            "reasoning_methods": [
                "Socratic CoT (subquestioning + stepwise solutions)",
                "Unified generation (single model outputs subquestion-solution pairs)",
                "Iterative QG+QA (two models: question generator and question answerer)"
            ],
            "reasoning_methods_description": "Socratic CoT: each intermediate reasoning step is explicitly represented as a subquestion q^{(j)} together with its solution s^{(j)}; two distillation strategies: (a) Unified student trained to autoregressively generate sequences of (q,s) pairs; (b) Iterative: separate QG model generates q^{(j)} conditioned on P and previous steps, and a QA model answers each q^{(j)} conditioned on P and previous solutions, using teacher forcing during training.",
            "diversity_of_methods": "Diverse: introduces an orthogonal reasoning style (decomposition into subquestions) in addition to standard CoT's stepwise solutions; diversity is produced via explicit subquestioning and by splitting functionality across QG and QA, and is evaluated vs. (similar-style) CoT and GT annotations.",
            "reasoning_task_name": "GSM8K, StrategyQA, SVAMP",
            "reasoning_task_description": "Same multi-step reasoning datasets: GSM8K math word problems with GT subquestions available for a subset; StrategyQA factual binary QA with supporting facts/decompositions; SVAMP multi-step arithmetic derived from ASDiv.",
            "performance_by_method": "Socratic CoT (LLM-generated subquestions + student fine-tuning) improved student accuracy substantially in many cases: GSM8K - Iterative Socratic CoT: Small 5.98% -&gt; Socratic_CoT (Iterative) 6.44% (↑20% over baseline in table), Medium: Iterative 11.57% -&gt; Socratic_CoT 12.74% (↑38%), Large: Iterative 17.89% -&gt; Socratic_CoT 21.08% (↑33%). StrategyQA - Medium: Socratic_CoT 60.31% (↑13% vs baseline), Large: Socratic_CoT 66.40% (↑5%). SVAMP - improvements for smaller students: Small Iterative Socratic 6.79% (best for small), Medium Iterative 18.99% (best for medium), Large best remained CoT 23.60% for that dataset. (Numbers are accuracies reported in Table 2 and highlighted by the authors for Socratic CoT columns.)",
            "comparison_of_methods": "Direct comparisons include: (1) Socratic CoT vs CoT (LLM-generated stepwise supervision): Socratic CoT often outperforms CoT for small and medium GPT-2 students (not always for largest student in every dataset). (2) Iterative (QG+QA) vs Unified: Iterative tends to perform better when QG is learned and QA uses previous steps iteratively (example GSM8K: Iterative 11.57% vs Unified 7.90% for Medium). (3) Socratic CoT with GT subquestions (Soc_GT) vs Socratic CoT with LLM-generated subquestions (Soc_CoT): using GT to prompt LLM to generate subquestions gives stronger supervision and better student performance. (4) Ablations: removing explicit subquestion module (train to output full chain without QG) degrades performance; adding guidance (conditioning QG on predicted equations/step counts) improves QG quality and downstream accuracy.",
            "key_findings": "Explicit subquestioning (Socratic CoT) provides a distinct reasoning style that, when distilled into smaller models, improves multi-step reasoning accuracy compared to similar-style CoT supervision in many settings, especially for small/medium students and when GT or high-quality subquestions are available to guide generation. Iterative QG+QA decomposition is effective and often outperforms a unified model.",
            "counter_examples_or_negative_results": "Socratic CoT is not uniformly better: (a) When high-quality GT step-by-step solutions exist, direct fine-tuning on GT steps can match or surpass some Socratic variants; (b) On StrategyQA, using noisy LLM-generated supporting facts combined with subquestioning can hurt accuracy (decomposition based on imperfect facts harms performance); (c) For some larger student models (e.g., GPT-2 XL 1.5B listed), Socratic variants sometimes decreased accuracy (an observed -4% in one row) relative to training on GT facts, and on SVAMP the plain CoT supervision outperformed Socratic variants for the largest student.",
            "uuid": "e3291.1"
        },
        {
            "name_short": "Unified vs Iterative",
            "name_full": "Unified generation vs Iterative QG+QA student architectures",
            "brief_description": "Two implementation strategies for Socratic CoT distillation: a single unified student that outputs full subquestion–solution sequences, and an iterative decomposition into a question-generation model (QG) and a question-answering model (QA) that cooperate at inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 variants as Unified or as separate QG and QA models",
            "model_description": "Unified: single autoregressive GPT-2 model trained to generate ordered (q,s) pairs. Iterative: two GPT-2 models where QG generates subquestions conditioned on P and prior steps, and QA answers each subquestion conditioned on P and prior solutions; teacher forcing used during training.",
            "model_size": "Students: GPT-2 Small 124M, Medium 355M, Large 774M (applied both as unified and split QG/QA models).",
            "reasoning_methods": [
                "Unified Socratic generation",
                "Iterative Question Generation + Question Answering"
            ],
            "reasoning_methods_description": "Unified uses single-model autoregressive loss over concatenated (q,s) tokens; Iterative uses two models with separate token-level losses and teacher forcing during training; at inference QG produces q^{(j)} which QA answers sequentially.",
            "diversity_of_methods": "Similar in high-level objective (produce subquestion-solution pairs) but architecturally diverse: one monolithic generator vs a decomposed pipeline; diversity arises from explicit separation of subquestion generation and answering.",
            "reasoning_task_name": "GSM8K, StrategyQA, SVAMP",
            "reasoning_task_description": "Same as above; iterative approach was applied across datasets to evaluate whether architectural separation helps distil subquestioning behaviour.",
            "performance_by_method": "GSM8K examples: Medium GPT-2: Unified 7.90% vs Iterative 11.57%; Large: Unified 13.25% vs Iterative 17.89%. SVAMP: Medium: Unified 17.62% vs Iterative 18.99%; Small: Unified 5.82% vs Iterative 6.79%. StrategyQA: mixed results; iterative and unified performance varied by size and supervision source.",
            "comparison_of_methods": "Iterative QG+QA repeatedly outperformed Unified in most small/medium student settings, indicating benefits to separating decomposition (QG) from solution (QA). Unified sometimes matched GT-step training when high-quality subquestion-solution sequences were available, but iterative generally yielded higher accuracies.",
            "key_findings": "Architectural separation of subquestion generation and answering (iterative) is beneficial: it consistently improves student accuracy on GSM8K and SVAMP for small/medium models relative to a unified generator.",
            "counter_examples_or_negative_results": "Unified sometimes performed comparably (especially when GT annotations were used or for certain larger students), and training the student to produce chains without any explicit subquestion module (No SubQ) produced markedly worse performance (Table 5: e.g., GPT-2 Large NoSubQ 8.18% vs SubQ with QG 17.89%).",
            "uuid": "e3291.2"
        },
        {
            "name_short": "Prompted Socratic CoT (GPT-3 few-shot)",
            "name_full": "Prompted Socratic Chain-of-Thought via LLM prompting",
            "brief_description": "Using prompting (single-shot in this paper) to make a large LLM (GPT-3 175B) generate subquestion decompositions in-context, and then using the same LLM to solve subproblems, demonstrating subquestioning as a prompting strategy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (175B) prompted 1-shot",
            "model_description": "GPT-3 (175B) was given single-shot exemplars where problems are decomposed into subquestions and used to both decompose unseen problems and solve them in a single-shot pipeline.",
            "model_size": "175B",
            "reasoning_methods": [
                "Prompted Chain-of-Thought (CoT)",
                "Prompted Socratic CoT (subquestion prompting / sub-ques)"
            ],
            "reasoning_methods_description": "One-shot prompting: include exemplar decomposition (subquestions and solutions) then ask GPT-3 to generate subquestion decompositions for new problems and/or to answer decomposed subproblems in-context.",
            "diversity_of_methods": "Diverse prompting strategies compared: standard CoT prompting ('Let's think step by step') vs Socratic prompting that explicitly structures the prompt into subquestions; diversity realized by differing exemplar formats in the prompt.",
            "reasoning_task_name": "GSM8K (prompting experiments)",
            "reasoning_task_description": "GSM8K grade-school math problems; authors limited to single-shot prompts due to budget.",
            "performance_by_method": "Single-shot GPT-3 results on GSM8K: CoT prompting accuracy 27.5%; Prompted Socratic CoT (sub-ques) accuracy 47.1% (Table 3). Authors note large variance (±41%) for Sub-ques measurement.",
            "comparison_of_methods": "Prompted Socratic CoT substantially outperformed standard CoT prompting in the single-shot setup (≈+19.6 percentage points absolute increase reported), indicating that decomposing into subquestions in the prompt improves chain reasoning for the LLM itself.",
            "key_findings": "Subquestion-structured exemplars in prompts can substantially boost the reasoning accuracy of a large LLM compared to standard CoT prompting (single-shot), demonstrating that diverse prompting styles (subquestions) are effective for in-context reasoning.",
            "counter_examples_or_negative_results": "Authors limited to single-shot prompts; they acknowledge that using more exemplars (few-shot up to 8) might change absolute performance and a fairer comparison vs CoT that used more exemplars in prior work; also reported high variance (±41%) in the sub-ques result.",
            "uuid": "e3291.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        },
        {
            "paper_title": "Explanations from large language models make small reasoners better",
            "rating": 1
        },
        {
            "paper_title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
            "rating": 1
        }
    ],
    "cost": 0.014374999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Distilling Reasoning Capabilities into Smaller Language Models</h1>
<p>Kumar Shridhar * Alessandro Stolfo * Mrinmaya Sachan<br>Department of Computer Science, ETH Zürich<br>{shkumar, stolfoa}@ethz.ch</p>
<h4>Abstract</h4>
<p>Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.</p>
<p>In this work, we propose an alternative reasoning scheme, Socratic CoT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over $70 \%$ compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT , demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have demonstrated strong performance on a variety of reasoning tasks (Brown et al., 2020; Hoffmann et al., 2022; Chowdhery et al., 2022, inter alia). One particularly interesting strategy for prompting these models is chain-of-thought (CoT), which has been shown to elicit reasoning abilities in LLMs by asking the model to incorporate intermediate reasoning steps while</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>solving a problem (Nye et al., 2021; Wei et al., 2022b; Wang et al., 2022). However, CoT has been shown to work primarily on models with hundreds of billions of parameters (Wei et al., 2022b,a) or those tuned to a wide range of tasks (Chung et al., 2022; Iyer et al., 2022).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the proposed framework. First, an LLM is prompted to decompose a multi-step problem providing annotation for the intermediate steps leading to the final solution. Then, the generated annotation is used to provide additional supervision when fine-tuning smaller models.</p>
<p>Due to the significant computational resources or expensive API calls required to access CoT-capable LLMs, we ask whether it is possible to elicit such reasoning capabilities in smaller models. ${ }^{1}$</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Small-sized, non-fine-tuned language models are known to be poor reasoners (Stolfo et al., 2022). Therefore, a possible approach to induce CoT-like reasoning abilities in smaller models would be finetuning them on step-by-step examples.</p>
<p>In our work, we propose a framework for leveraging the reasoning capabilities of LLMs to supervise the training of smaller models. This approach can be thought of as a form of knowledge distillation (Hinton et al., 2015), where a larger teacher model transfers knowledge to a smaller student model. However, unlike standard knowledge distillation, our method transfers the reasoning abilities of the teacher model only using its generated solutions as a proxy, i.e., we do not assume access to the teacher model parameters. Our approach consists of prompting an LLM to produce step-by-step annotations leading to the answer for a set of problems. This annotation is then used as supervision to finetune the student model. A high-level illustration of the process is provided in Figure 1.</p>
<p>Within this framework, we study three different types of annotation structure for supervising our distillation approach: (i) We consider fine-tuning on the gold step-by-step solution procedure for datasets where the step-by-step solutions are available. (ii) We study whether procedural supervision, coming from the chain of thought (CoT) of the teacher model can improve upon the baseline. (iii) We propose a third type of supervision structure, which we call SOCRATIC COT. This approach relies on learning a semantic decomposition of the original problem into a sequence of subproblemsolution pairs using two models - a) a question generator that learns to decompose the problem into a sequence of subproblems, and b) a questionanswering model that solves the various generated subproblems (more details are in section 3.2). This approach can be thought of as an extension of the typical chain of thought reasoning where, unlike CoT, the intermediate steps are now decomposed into subquestion-solution pairs; the subquestions guide the generation of intermediate steps that lead to the final answer to the problem.</p>
<p>We train distilled student models with various annotation structures mentioned above. Depending on the annotation available for the given data, we use the teacher model to generate either a CoT-like solution to a problem or, if the step-by-step anno-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tation is available, a set of subquestions leading to the solution of the problem, or both (examples of different annotations are shown in Figure 2).</p>
<p>We perform our analyses on three multi-step reasoning datasets: GSM8K (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), and SVAMP (Patel et al., 2021). We consider data with various types of annotation to cover a range of realistic data scenarios. Our results show that supervision by CoT-decomposed examples helps smaller models perform better, and subquestioning introduced by SOCRATIC COT can provide further improvement. We observe performance gains of up to $40 \%$ with LLM-generated step-by-step annotations - this validates the effectiveness of our distillation framework (detailed analysis in Section 5).</p>
<h2>2 Related Work</h2>
<p>Decomposing Multi-Step Reasoning Tasks Solving multi-step reasoning tasks like MWPs has been a popular area of research for the last couple of years (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015; Amini et al., 2019; Zhang et al., 2020; Shridhar et al., 2022; Opedal et al., 2023). However, the majority of the modern approaches for these problems are shifting towards using large language models, often relying on approaches involving prompting or in-context learning (Cobbe et al., 2021; Kojima et al., 2022; Wei et al., 2022b; Chowdhery et al., 2022; Lewkowycz et al., 2022; Srivastava et al., 2022). One such prompting approach is the chain of thought prompting (Wei et al., 2022b), which prompts the language model to generate a series of intermediate steps that improve the reasoning capabilities in LLMs. Wang et al. (2022) took another step forward and sampled multiple reasoning paths and selected the most relevant output using majority voting. Huang et al. (2022) used the most voted outputs to further fine-tune the model for better performance. Kojima et al. (2022) further improved the reasoning of LLM in a zero-shot manner by appending "Let's think step by step" to the prompt. In contrast, our work does not propose prompting solutions; instead, we explicitly guide the student model reasoning using sub-questions at each step. Most similar to our work is the work by Zhou et al. (2022) which decomposes questions into sub-questions and asks the language model to solve each sub-question sequentially. However, this work is also restricted</p>
<p>to prompting and only works with LLMs with billions of parameters.</p>
<p>Knowledge Distillation Our approach is reminiscent of knowledge distillation <em>Ba and Caruana (2014); Hinton et al. (2015)</em> in that we use a student network to mimic the large teacher language model. <em>Snell et al. (2022)</em> demonstrated the usefulness of providing instruction that can help models achieve better reasoning skills. Similar to our hypothesis, <em>Eisenstein et al. (2022)</em> argued that question-answering systems should focus not only on the final answer, but also on the rationale that justifies their reasoning, to help them reason better. We go beyond this; in our work, in addition to the question-answering system, we also focus on what questions need to be asked at each step that can help to learn that reasoning step better. Finally, similar to our hypothesis of injecting reasoning capabilities into smaller models, <em>Li et al. (2022)</em> used CoT-like reasoning from LLMs to train smaller models on a joint task of generating the solution and explaining the generated solution. We, on the other hand, use the LLM to generate subquestions and solution pairs and use them together to inject reasoning capabilities into smaller models.</p>
<p>Subquestioning as supervision The idea of inquiring or asking information-seeking questions for discovery learning has been studied well in the past <em>Bruner (1961)</em>. <em>Rao and Daumé III (2020)</em> generated clarification questions based on Stack Exchange questions as supervision, <em>Klein and Nabi (2019)</em> used a joint question answering model to ask questions from a given span of text and later answer them, and <em>Rajani et al. (2019); Shwartz et al. (2020)</em> asked questions to improve common sense QA models. In contrast, our work focuses on multistep reasoning tasks where intermediate clarifying questions and reasoning steps may not always be available and may need to be extracted from a teacher model.</p>
<h2>3 Methodology</h2>
<p>The setting we consider consists of a data set $\mathcal{D}$, where each problem $P_{i}$ is accompanied by a final answer $a_{i}$ that can be reached by several steps of reasoning. The task of solving the problem using a model $\psi$ is to predict an answer $\hat{a}=\psi(P)$ such that $\hat{a}=a$. We consider different data scenarios where intermediate annotations of the solution may be available in different forms (e.g., step-by-step, as a semantic decomposition by subquestions) or</p>
<p>Reasoning Problem
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the three different kinds of annotation structure. Our proposed approach, Socratic CoT, augments the typical chain-of-thought step-bystep solution with subquestioning.
may not be present. Depending on the availability of annotations, we propose different approaches to augment the training of a small model on $\mathcal{D}$ by using LLMs.</p>
<h3>3.1 Distilling step-by-step reasoning via CoT</h3>
<p>A data set may present an annotation that contains intermediate reasoning steps that lead to the answer $a_{i}$ (i.e., a chain-of-thought annotation). This intermediate annotation can be used directly to fine-tune a small model. However, in cases where such step-by-step information is not available, we use a LLM to generate the reasoning steps that might improve the performance of the small model.</p>
<p>To achieve this, we consider a small subset of the dataset $\mathcal{D}$ and decompose each problem $P_{i}$ into $n_{i}$ intermediate reasoning steps. We construct these intermediate reasoning steps manually, since we only need a few examples as prompts (examples are provided in Appendix Table 6).</p>
<p>For each remaining problem $P \in \mathcal{D}$, we then prompt a large language model $\mathcal{M}$ to generate the intermediate reasoning steps. We make sure that the chain of reasoning steps is meaningful by checking whether the last solution matches the ground truth answer, i.e. whether $a_{i}^{\left(n_{i}\right)}=a_{i}$, where $a_{i}^{\left(n_{i}\right)}$ represents the answer corresponding to the last reasoning step. If this is not the case, we discard the problem and sample a new chain by prompting the model again (for a maximum of 3 times). In this</p>
<p>way, we obtain an augmented dataset $\mathcal{D}^{*}$ in which a subset of problems is paired with a sequence of reasoning steps leading to the correct result. Finally, we can distill the reasoning capabilities into smaller models by fine-tuning them with the generated intermediate steps.</p>
<h3>3.2 Distilling step-by-step reasoning through Socratic CoT</h3>
<p>In this section, we describe how CoT can be enhanced through subquestioning. An illustration of our approach is shown in Figure 3.</p>
<h3>3.2.1 Extracting the Reasoning Capability from the Teacher</h3>
<p>In Section 3.1, we detailed how an LLM can be used to generate the intermediate annotation of a problem $P_{i}$ as a chain of steps leading to the answer $a_{i}$. We now extend this procedure to include a subquestion at each step of the solution. Following a similar procedure as described in Section 3.1, we prompt the LLM with few exemplars of problems decomposed as a set of intermediate subquestionsolution pairs (the prompts are reported in Appendix Table 6). This way, we obtain an intermediate annotation that includes subquestioning. In particular, each of the $n_{i}$ steps constituting the overall solution is a subquestion-solution pair, denoted $q_{i}^{(j)}, s_{i}^{(j)}, j \in\left{1, \ldots, n_{i}\right}$ (an example is shown in Figure 2). We refer to the ordered list of subquestion-solution pairs for problem $P_{i}$ as $\left(q_{i}^{(1)}, s_{i}^{(1)}\right), \ldots,\left(q_{i}^{\left(n_{i}\right)}, s_{i}^{\left(n_{i}\right)}\right)$.</p>
<h3>3.2.2 Transferring the Reasoning Capability into the Student</h3>
<p>We present two strategies to distill the reasoning annotation provided by the LLM into smaller models.</p>
<p>In the first strategy, a single unified student is trained to generate the subquestion-solution pairs simultaneously, while in the second strategy, the question generation and question-answering tasks are assigned to two separate models. We call this second strategy iterative because the questionanswering model is trained to solve each subquestion iteratively.</p>
<p>Unified. Using the problems in $\mathcal{D}$ that contain the chain of intermediate questions and solutions, we train a unified student model $\mathcal{M}<em j="j">{\text {uni }}$ that learns to generate the sequence of subquestion-solution pairs $\left{\left(q^{(1)}, s^{(1)}\right),\left(q^{(2)}, s^{(2)}\right), \ldots\right}$ that lead to the solution of a given problem. We use a pre-trained transformer-based model (Vaswani et al., 2017) and train it on the chain of subquestion-solution pairs for each problem $P$. Given a step $j$ of problem $P$ (i.e., the concatenation of $q^{(j)}$ and $s^{(j)}$ ) consisting of a sequence of $m</em>$ :}$ tokens $\left{x_{j}^{(1)}, \ldots, x_{j}^{\left(m_{j}\right)}\right}$, we use a typical auto-regressive language modeling loss, $\mathcal{L</p>
<p>$$
\mathcal{L}<em k="1">{j}(P)=-\sum</em>}^{m_{j}} \log \mathbb{P<em j="j">{\text {uni }}\left(x</em>, P\right)
$$}^{(k)} \mid x_{j}^{\prime(k-1)</p>
<p>where $\mathbb{P}<em _text="\text" _uni="{uni">{\text {uni }}(x \mid c)$ is the probability assigned by $\mathcal{M}</em>}}$ to token $x$ given context $c$, and $x^{\prime(y)}$ indicates the sequence $\left{x^{(1)}, \ldots, x^{(y)}\right}$. The loss $\mathcal{L<em i="i">{j}$ is computed for each problem $P</em>$.}$ and for each pair $\left(q^{(j)}, s^{(j)}\right)$ leading to the final answer $a_{i</p>
<p>Iterative. The iterative version of the student separates the tasks of generating the subquestions and providing an intermediate answer to each subquestion into two distinct models: a question generation (QG) model and a question answering (QA) model. Both the QG and QA models are implemented using a Transformer-based language model (Vaswani et al., 2017). In particular, the QA model $\mathcal{M}_{q a}$ is iteratively trained to answer the teacher-generated sub-questions. The learning objective is computed at the token level for each intermediate solution:</p>
<p>$$
\mathcal{L}\left(P, s^{(j)}\right)=-\sum_{k=1}^{l_{j}} \log \mathbb{P}<em j="j">{\mathcal{Q}, \mathrm{~A}}\left(y</em>, P\right)
$$}^{(k)} \mid y_{j}^{\prime(k-1)}, q^{(j)}, s^{(j-1)</p>
<p>where $l_{j}$ and the $y_{j}$ 's represent, respectively, the length and the tokens of the intermediate solution $s^{(j)} . s^{(j-1)}$ consists of the previous solution generated by the QA model iteratively in the past iterations.</p>
<p>Similarly, the QG model is trained to acquire the ability of the teacher model to decompose the problem's main question into a series of sub-steps, each of which corresponds to a subquestion. The loss for this model is analogous to Equation 1, with the only difference being that the intermediate solutions are not considered for the QG model. During training, the previous intermediate solutions generated by the QA model are replaced with the teacher-generated solutions using teacher forcing (Cho et al., 2014). However, the intermediate solutions generated by the model are used at inference time.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />Figure 3: Detailed explanation of our framework. First, a LLM is prompted to decompose the input problem $P$ into a series of subquestion-solution pairs ($q_{i}^{(j)},s_{i}^{(j)}$, $j\in{1,\ldots,n_{i}}$) with an answer at each step $a_{i}^{(j)}$. The generated subquestions-solutions are used to train two student models: a) the QG model which learns to mimic sub questioning capability of the LLM and b) the QA model, which learns to solve each subquestion. At the bottom, the inference process is depicted for an unseen problem and no LLM is involved. The QG model breaks the unseen problem into simpler subquestions and the QA model solves each one of them eventually leading to the final answer $a_{i}^{(n_{i})}$.</p>
<h3>3.3 Inference-time Predictions</h3>
<p>Given an unseen problem $P$, the unified student model can directly predict a solution as a sequence of subquestions and answers. In the iterative approach, we first generate the subquestions conditioning the generation of the QG model on $P$. After these questions are generated, they are provided to the QA model one by one, decoding the intermediate solution $\hat{s}^{(j)}$ at step $j$ token by token according to the model’s probability distribution over its vocabulary:</p>
<p>$\mathbb{P}<em j="j">{\mathcal{QA}}\left(y</em>,P\right),$ (2)}^{(k)}|y_{j}^{‘(k-1)},\hat{q}^{‘(j)},\hat{s}^{‘(j-1)</p>
<p>where $y_{j}^{(k)}$ is the $k$-th token being decoded in greedy fashion.</p>
<p>After the last solution $\hat{s}^{(n)}$ has been generated, the numerical prediction $\hat{a}^{(n)}$ is parsed from the text using simple heuristics.</p>
<h2>4 Empirical Analysis</h2>
<h3>4.1 Datasets</h3>
<p>We study how smaller models can learn to reason better on three multi-step reasoning datasets: GSM8K <em>Cobbe et al. (2021)</em>, StrategyQA <em>Geva et al. (2021)</em>, and SVAMP <em>Patel et al. (2021)</em>. GSM8K consists of 8.5K grade school math word problems, each requiring 2 to 8 steps of reasoning to solve. The solutions primarily involve a sequence of elementary calculations using basic arithmetic operations $(+,-,\times,\div)$. The dataset is divided into 7.5K training problems and 1K test problems. To evaluate the model on SVAMP, we train the model on 761 multi-step math word problems taken from the ASDiv <em>Miao et al. (2020)</em> training set and evaluate it on 237 multi-step SVAMP problems. For StrategyQA, the test set with facts is not available, so we split the data into 80% training, 10% as validation data, and the last 10% as test data. We do not shuffle the data to maintain reproducibility.</p>
<h3>4.2 Experimental Setup</h3>
<p>We use three kinds of annotation, corresponding to the three datasets that we consider.</p>
<p>Step-by-step solution : The GSM8K dataset falls into this category and includes a Socratic version where intermediate subquestion-solution pairs are provided for each MWP. While the intermediate step-by-step solutions were manually anno-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Unified</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input: <br> A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?</td>
<td style="text-align: center;">Output: <br> How many bolts of white fiber does it take? It takes $2 / 2$ $=&lt;&lt;2 / 2=1&gt;&gt;1$ bolt of white fiber. How many bolts in total does it take? So the total amount of fabric is $2+1=$ $&lt;&lt;2+1=3&gt;&gt;3$ bolts of fabric. The answer is 3 .</td>
</tr>
<tr>
<td style="text-align: center;">Iterative</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Input: <br> A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?</td>
<td style="text-align: center;">Output: <br> QG: How many bolts of white fiber does it take? <br> QA: It takes $2 / 2=&lt;&lt;2 / 2=1&gt;&gt;1$ bolt of white fiber.</td>
</tr>
<tr>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Input: <br> A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? How many bolts of white fiber does it take? It takes $2 / 2=&lt;&lt;2 / 2=1&gt;&gt;1$ bolt of white fiber.</td>
<td style="text-align: center;">Output: <br> QG: How many bolts in total does it take? <br> QA: So the total amount of fabric is $2+1=&lt;&lt;2+1=3&gt;&gt;3$ bolts of fabric. The answer is 3 .</td>
</tr>
</tbody>
</table>
<p>Table 1: Example demonstraing the input-output format for unified vs iterative setup. QG represents the question generation model and QA is the question answerer mdoel. Note that QA model uses the QG output to answer it as shown in Figure 3.
tated, the authors report that the subquestions were generated by prompting GPT-3. We reproduced a subset of these subquestions using a GPT-3 model with prompts, and we observed a high similarity between the questions provided and the ones generated by us (BERT $F_{1}$ score of $95 \%$ ). For SoCRATIC CoT, we thus use the subquestioning annotation already provided.</p>
<p>Supporting facts : We study the StrategyQA dataset, which falls in this category. Strategy QA consists of a factual question with binary True/False as the final answer. Additional supporting facts and decomposed questions are provided. However, the set of facts and the decomposed questions provided with a given question are not always aligned (i.e., a fact is not necessarily the answer to one subquestion). Therefore, having a setup similar to the one for GSM8K is not possible. We thus consider two versions of the data. One in which the supporting facts are used as CoT and the corresponding questions are generated by prompting a GPT-3 model, and a second in which we take the provided questions and generate the facts (this time aligned with the questions) using GPT-3.</p>
<p>Final answers only : AsDiv/SVAMP falls in this category and for training, we use GPT-3 to generate both intermediate subquestions and solutions. Intermediate solutions are used as CoT and the generated subquestion-solution pairs for SOCRATIC COT.</p>
<h3>4.3 Implementation Details</h3>
<p>We use GPT-2 variants (Radford et al., 2019) as student models. GPT-3 175B (Brown et al., 2020) served as the teacher model for decomposing complex problems into a series of simpler substeps (we report the prompts used in Appendix Table 6).</p>
<p>All models were trained using the Huggingface library (Wolf et al., 2020) on an NVIDIA Tesla A100 GPU with 40 GB of memory. Each experiment was run for the same number of iterations to ensure fairness with periodic evaluation over the validation set. Teacher forcing was used during training to replace the generated responses with ground truth answers from the training dataset.</p>
<p>Evaluation Metric. To evaluate the questionanswering performance on the GSM8K, SVAMP, and StrategyQA datasets, we compute the accuracy based on the final answer provided by the student model.</p>
<h2>5 Results and Discussion</h2>
<p>Can our framework improve the reasoning capabilities of smaller models? Table 2 demonstrates that leveraging LLMs reasoning capabilities using our framework can improve the reasoning results for all dataset types.</p>
<p>Step-by-Step Solution. When human-annotated step-by-step solutions are available, training smaller models with LLM-generated CoT is not advantageous, as shown on GSM8K. This is to</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Model</th>
<th>Answer Only</th>
<th>GT Steps</th>
<th>GT Facts</th>
<th>ToT</th>
<th>Iterative</th>
<th></th>
<th>Unified</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>CoT</td>
<td>$\mathbf{S o c}_{C o T}$</td>
<td>$\mathbf{S o c}_{G T}$</td>
</tr>
<tr>
<td>GSM8K</td>
<td>Small (124M)</td>
<td>1.45</td>
<td>5.05</td>
<td>-</td>
<td>4.70</td>
<td>5.98</td>
<td>$\mathbf{6 . 4 4}(\uparrow 20\%)$</td>
<td>5.10</td>
</tr>
<tr>
<td></td>
<td>Medium (355M)</td>
<td>2.90</td>
<td>7.88</td>
<td>-</td>
<td>7.10</td>
<td>11.57</td>
<td>$\mathbf{1 2 . 7 4}(\uparrow 38\%)$</td>
<td>7.90</td>
</tr>
<tr>
<td></td>
<td>Large (774M)</td>
<td>4.62</td>
<td>14.10</td>
<td>-</td>
<td>12.85</td>
<td>17.89</td>
<td>$\mathbf{2 1 . 0 8}(\uparrow 33\%)$</td>
<td>13.25</td>
</tr>
<tr>
<td></td>
<td>GPT-3 (6B)</td>
<td>-</td>
<td>21.00</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>StrategyQA</td>
<td>Medium (355M)</td>
<td>54.10</td>
<td>-</td>
<td>52.02</td>
<td>55.01</td>
<td>52.05</td>
<td>$\mathbf{6 0 . 3 1}(\uparrow 13\%)$</td>
<td>52.05</td>
</tr>
<tr>
<td></td>
<td>Large (774M)</td>
<td>61.10</td>
<td>-</td>
<td>62.80</td>
<td>55.90</td>
<td>61.32</td>
<td>$\mathbf{6 6 . 4 0}(\uparrow 5\%)$</td>
<td>59.45</td>
</tr>
<tr>
<td></td>
<td>XL (1.5B)</td>
<td>60.51</td>
<td>-</td>
<td>$\mathbf{6 6 . 3 0}$</td>
<td>58.07</td>
<td>62.30</td>
<td>$63.56(\downarrow 4\%)$</td>
<td>62.05</td>
</tr>
<tr>
<td>SVAMP</td>
<td>Small (124M)</td>
<td>2.15</td>
<td>-</td>
<td>-</td>
<td>5.35</td>
<td>$\mathbf{6 . 7 9}$</td>
<td>-</td>
<td>5.82</td>
</tr>
<tr>
<td></td>
<td>Medium (355M)</td>
<td>4.80</td>
<td>-</td>
<td>-</td>
<td>17.30</td>
<td>$\mathbf{1 8 . 9 9}$</td>
<td>-</td>
<td>17.62</td>
</tr>
<tr>
<td></td>
<td>Large (774M)</td>
<td>7.40</td>
<td>-</td>
<td>-</td>
<td>$\mathbf{2 3 . 6 0}$</td>
<td>18.14</td>
<td>-</td>
<td>17.45</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy comparison (in $\%$ ) on the three considered datasets. We consider three human-annotated baselines: final answers only (Answer Only), ground-truth step-by-step solution (GT Steps), and supporting facts (GT Facts). We compare the different supervision strategies for fine-tuning the small models: CoT represents the case where the chain of intermediate reasoning steps is generated by GPT-3, $\mathbf{S o c}<em G="G" T="T">{C o T}$ represents the case where both the chain of intermediate solutions and the subquestions are generated by LLM and used to fine-tune small models. $\mathbf{S o c}</em>}$ represents the case where GT solutions/facts are used when prompting GPT-3 to generate the subquestions. Iterative and Unified represent the two $\mathbf{S o c<em C="C" T="T" o="o">{C o T}$ strategies described above. All models are GPT-2 versions and their size is reported within parentheses. All experiments were run at least 3 times and the average is reported. GPT-3 6B results are taken from Cobbe et al. (2021).
be expected since the annotation generated by an LLM is likely to be noisier and of lower quality than human-annotated data. However, the groundtruth step-by-step annotation can be leveraged to prompt an LLM to generate subquestions for the Socratic CoT approach, giving a performance boost of up to $38 \%$ when the LLM-generated subquestions are used at inference time. When the subquestions are learned by the QG model (Iterative $\mathbf{S o c}</em>$ models and a model trained on the GT step-by-step annotation. Unified Socratic CoT performs similarly to training with the step-wise ground-truth annotation. We additionally include the score produced by GTP-3 6B to show that training with Socratic CoT can help a small model (GPT-2 large with 774M parameters) perform as well as a nearly 10x larger model fine-tuned with human annotated data.}$ ), the accuracy of the student model decreases slightly but still improves over the step-by-step annotation without subquestions ( 17.89 vs. 14.10). Figure 5 shows a comparison of predictions generated by $\mathbf{S o c}_{C o T</p>
<p>Supporting facts. On StrategyQA, we observe that the inclusion of ground-truth supporting facts in the fine-tuning procedure improves the performance of the small models. However, surprisingly, when the supporting facts are generated by GPT-3, their inclusion actually hurts performance (58.07 vs 60.51 for GPT-2 Large). We hypothesize that
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Accuracy comparison for different supervision strategies on StrategyQA. The baseline method consists of fine-tuning on final answers only (Ans only), and it is compared to fine-tuning with: ground-truth supporting facts (GT Facts), GPT-3-generated supporting facts (CoT), ground-truth supporting facts with GPT-3-generated subquestions ( $\mathbf{S o c}<em G="G" T="T">{C o T}$ ), and LLMgenerated facts with human-annotated subquestions $\left(\mathbf{S o c}</em>\right)$.
this is likely due to the imperfect factual knowledge provided by the LLM, which mars the quality of the supervision. We have observed that the GT supporting facts provided often do not represent a logical sequence of propositions leading to the final answer. This is likely the reason why decomposing the problem through subquestions based on such facts actually harms accuracy (see $\mathbf{S o c}_{C o T}$ column</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Example of predictions generated by a GPT-2 Large model fine-tuned with GT steps and SOCRATIC CoT on GSM8K dataset.</p>
<p>in Table 2). Instead, using the provided subquestions and using an LLM to generate the answers (representing coherent facts leading to the final answer) proves to be an effective strategy (60.31 vs. 52.02 for GPT-2 Medium). A more detailed comparison between our proposed approaches is presented in Figure 4. However, GPT-2 XL models perform well when trained on facts as unlike smaller models, larger models can encode more facts at once in their parameters, which assists in answering a factual question.</p>
<p><strong>Answers only.</strong> On the SVAMP dataset, which includes only final answers and no intermediate annotation, LLMs can be used to generate both the intermediate steps and the subquestions. Both the consideration of intermediate solutions without subquestions (<strong>CoT</strong>) and the consideration of intermediate solutions with subquestions (<strong>Soc</strong>_{<em>CoT</em>}) lead to an improvement in performance. The trend here is similar to what was observed for StrategyQA, with SOCRATIC CoT being more effective for the two smaller models but falling back to <strong>CoT</strong> for the larger model.</p>
<p><strong>Can SOCRATIC CoT be used as a prompting strategy?</strong> We experimented with SOCRATIC CoT as a prompting strategy. First, we prompted GPT-3 (175B) to decompose the main problem into</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Methodology</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 (1-shot)</td>
<td>CoT</td>
<td>27.5</td>
</tr>
<tr>
<td>(175B)</td>
<td>Sub-ques</td>
<td>47.1 (± 41%)</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy comparison (in %) of using CoT vs SOCRATIC CoT (Sub-ques) on the GSM8K dataset for GPT-3 model with prompting.</p>
<p>Simpler steps by formulating subquestions. Then, GPT-3 is used again to solve the sequence of subproblems in a single-shot setting with a problem decomposed into intermediate subquestions and solutions included in the prompt. The introduction of subquestioning boosts accuracy by over 40% compared to standard CoT prompting (Table 3). Other work (e.g., [Wei et al. 2022b]) has used a larger number of exemplars in the few-shot prompt, achieving higher overall accuracy. We limited our experiments to single-shot prompts due to budget constraints.</p>
<h3>6 Ablation Studies</h3>
<p>In this Section, we describe additional analyses regarding specific components of the framework we propose, as well as negative results that we obtained with alternative strategies.</p>
<h4>How good are the sub-questioning capabilities of a smaller model?</h4>
<p>We investigate in more detail the ability of a small model to decompose a problem by generating meaningful subquestions. We fine-tuned GPT-2 Large on the GPT-3 generated subquestions provided in the GSM8K dataset. We then evaluated the quality of the generated questions in terms of BLEU score <em>Post (2018)</em>, BERT F_{1} score <em>Zhang et al. (2019)</em>, and by measuring for how many problems the number of questions generated by GPT-2 (#Q) matches the number of GPT-3 annotated questions for a given problem.</p>
<p>We found that the fine-tuned GPT-2 predicted an incorrect number of subquestions for the majority of problems (see Table 4, first row). Thus, following previous work on subquestion generation <em>Shridhar et al. (2022)</em>, we introduced a <em>guidance mechanism</em> that conditions the generation of subquestions for a problem <em>P</em> on the equations describing the intermediate solutions of <em>P</em>. This strategy improved the quality of the generated questions for all three metrics considered (Table 4, second row). To avoid the dependence on the step-by-step annotation of the equations for each problem <em>P</em></p>
<table>
<thead>
<tr>
<th>Methodology</th>
<th>BLEU</th>
<th>BERT $F_{1}$</th>
<th># Q</th>
</tr>
</thead>
<tbody>
<tr>
<td>No-guidance</td>
<td>51.5</td>
<td>0.78</td>
<td>0.42</td>
</tr>
<tr>
<td>Guidance</td>
<td>58.8</td>
<td>0.81</td>
<td>0.80</td>
</tr>
</tbody>
</table>
<p>Table 4: BLEU, BERT $F_{1}$ and the number of questions (# Q) comparison between the question generator model and the Socratic subquestions present in the GSM8K dataset using GPT2-large model.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Accuracy of student models (QA + QG) when the question generation is conditioned using the guidance model (Guide) and with non-guided question generation (No guide). Ans only represents the baseline. All models are GPT-2 versions.
at inference time, we train an additional sequence-to-sequence model to predict, given $P$, the set of equations that lead to the solution of the problem. At inference time, the predictions for the guidance model are used to condition the generation by the QG model. Although the predicted equations often do not lead to the correct solution of the problem, they help the QG model to generate more meaningful sub-questions. Figure 6 shows the overall accuracy of the GPT-2 student models (QA + QG) finetuned with Socratic CoT on the GSM8K data with and without equation conditioning provided by the guide model. We have extended this guidance mechanism to StrategyQA and SVAMP, where the generation of subquestions is conditioned on the number of facts (StrategyQA) or steps (SVAMP) needed to answer the problem.</p>
<p>Eliminating the need for a subquestion module. We have experimented with an alternative training solution that does not involve a question-generation model. This strategy aims to improve the supervision for fine-tuning a small model through subquestioning, but without relying on the presence of subquestions at test time. The procedure consists of training the student model to generate the entire chain of steps leading to an intermediate answer. That is, when the sub-question</p>
<table>
<thead>
<tr>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">No SubQ</th>
<th style="text-align: center;">SubQ with QG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">5.98</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">7.20</td>
<td style="text-align: center;">11.57</td>
</tr>
<tr>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">8.18</td>
<td style="text-align: center;">17.89</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy comparison (in \%) of student models trained with (SubQ with QG) and without (No SubQ) question generation model on GSM8K.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Example of predictions generated by a GPT2 Medium model fine-tuned with GT steps and SOCRATIC CoT on the SVAMP dataset.
$q^{(1)}$ is asked, the model is trained to generate the answer $s^{(1)}$, but when $q^{(j)}$ is asked, the model is trained to generate the chain of thought reasoning $\left{s^{(1)}, s^{(2)}, \ldots, s^{(j)}\right}$ (instead of just $s^{(j)}$ ). This eliminates the need for the intermediate subquestions at inference time, as the model is trained to implicitly decompose the main problem into smaller reasoning steps. However, this method leads to significant performance degradation (results are reported in Table 5), highlighting the need for subquestions at inference time.</p>
<p>Example outputs In Figures 5 and 7, we report example outputs predicted by GPT-2 models for a set of GSM8K and SVAMP problems.</p>
<h2>7 Conclusion</h2>
<p>The chain-of-thought style of step-by-step reasoning has proven to be very effective for reasoning</p>
<p>in LLMs. In this work, we propose ways to distill these reasoning capabilities into smaller models and suggest ways to further improve them by explicitly asking stepwise questions. We demonstrate the effectiveness of our proposed methodology on three popular multi-step reasoning datasets, and discuss cases where one method should be preferred over the other for different datasets.</p>
<h2>Limitations</h2>
<p>In our work, we use only one solution from the LLM to distill information into the student model, and according to Wang et al. (2022), multiple subquestion-solution pairs can be sampled, and using majority voting, all pairs leading to the most frequent answer can be used to distill knowledge into the student models. Also, due to computational budget, we used a single prompt to compare the CoT and SOCRATIC COT and using more prompts (up to 8) might lead to a fairer comparison and better results (Wei et al., 2022b). We leave these experiments for the future.</p>
<h2>Ethical Considerations</h2>
<p>Although this work improves the reasoning capabilities of smaller models, the models are still not powerful enough to be used in sensitive settings such as education. We plan to release our code and model checkpoints, but the models must be used carefully by users, as many generative models, including ours, are prone to hallucination.</p>
<h2>Acknowledgements</h2>
<p>Alessandro Stolfo is supported by Armasuisse Science and Technology through a CYD Doctoral Fellowship.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik KoncelKedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319.</p>
<p>Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep? Advances in neural information processing systems, 27.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Jerome S Bruner. 1961. The act of discovery. Harvard educational review, 31:21-32.</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael Collins, and David Mimno. 2022. Honest students from untrusted teachers: Learning an interpretable question-answering pipeline from a pretrained language model. arXiv preprint arXiv:2210.02498.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL).</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7).</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523-533.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610.</p>
<p>Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017.</p>
<p>Tassilo Klein and Moin Nabi. 2019. Learning to answer by learning to ask: Getting the best of gpt-2 and bert worlds. arXiv preprint arXiv:1911.02365.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 271-281, Baltimore, Maryland. Association for Computational Linguistics.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858.</p>
<p>Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. 2022. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online. Association for Computational Linguistics.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma,</p>
<p>David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>Andreas Opedal, Niklas Stoehr, Abulhair Saparov, and Mrinmaya Sachan. 2023. World models for math story problems. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191.</p>
<p>Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Belgium, Brussels. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging Language Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942, Florence, Italy. Association for Computational Linguistics.</p>
<p>Sudha Rao and Hal Daumé III. Answer-based Adversarial Training for Generating Clarification Questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).</p>
<p>Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics, 3:1-13.</p>
<p>Kumar Shridhar, Jakub Macina, Mennatallah ElAssady, Tanmay Sinha, Manu Kapur, and Mrinmaya Sachan. 2022. Automatic generation of socratic subquestions for teaching math word problems. arXiv preprint arXiv:2211.12835.</p>
<p>Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4615-4629, Online. Association for Computational Linguistics.</p>
<p>Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022. Learning by distilling context. arXiv preprint arXiv:2209.15189.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta,</p>
<p>Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, and Mrinmaya Sachan. 2022. A causal framework to quantify the robustness of mathematical reasoning with language models. arXiv preprint arXiv:2210.12023.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30 .</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, and Ee-Peng Lim. 2020. Graph-totree learning for solving math word problems. Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.</p>
<p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625.</p>
<p>Let's generate sub-questions for these problems. Use exactly one operation per step.</p>
<p>Q: Zoe was unboxing some of her old winter clothes. She found number0 boxes of clothing and inside each box there were number1 scarves and number2 mittens. How many pieces of winter clothing did Zoe have total?</p>
<p>SQ1: How many pieces of winter clothing did Zoe have in each box?
A1: Zoe had $&lt;&lt;+$ number1 number $2&gt;&gt;$ pieces of winter clothing in each box.
SQ2: How many pieces of winter clothing did Zoe have total ?
A2: Zoe had $&lt;&lt;*$ number $0+$ number 1 number $2&gt;&gt;$ pieces of winter clothing in total.</p>
<p>Q: Katie picked number0 tulips and number1 roses to make flower bouquets. If she only used number2 of the flowers though , how many extra flowers did Katie pick ?</p>
<p>SQ1: How many flowers did Katie pick in total?
A1: Katie picked $&lt;&lt;+$ number0 number1 $&gt;&gt;$ flowers in total.
SQ2: How many extra flowers did Katie pick ?
A2: Katie picked $&lt;&lt;-+$ number0 number1 number $2&gt;&gt;$ extra flowers.</p>
<p>Q: Conner has number0 dollars in his bank account. Every month he spends number1 dollars. He does not add money to the account. How much money will Conner have in his account after number2 months ?,</p>
<p>SQ1: How much money does Conner spend in total? A1: Conner spends $&lt;&lt;*$ number1 number2 $&gt;&gt;$ dollars.
SQ2: How much money will Conner have in his account after 8.0 months? A2: After 8.0 months, Conner will have jjnumber0 * number1 number2 $&gt;&gt;$ dollars.
For each of the following topics, generate intermediate answers to the subquestions leading to the final answer.</p>
<p>Topic: Albany, Georgia (City in Georgia, United States)
Will the Albany in Georgia reach a hundred thousand occupants before the one in New York?
Albany, GA has around 75,000 people.
Albany, NY has almost 100,000 people.
The difference is $100,000-75,000=25,000$
The difference is $100,000-100,000=0$
No, 25,000 is not smaller than 0 .
The final answer is NO.</p>
<p>Topic: The Police (English rock band)
Could the members of The Police perform lawful arrests?
Only law enforcement officers can perform lawful arrests.
No, the members of The Police (rock band) are not law enforcement officers.
The final answer is NO.</p>
<p>Topic: Wonder Woman (2017 film) (American superhero film directed by Patty Jenkins) Is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?</p>
<p>The average cost of a US Boeing 737 plane is 1.6 million dollars.
Wonder Woman (2017 film) grossed over 800 million dollars at the box office.
Yes, 800 is larger than 1.6 .
The final answer is YES.
Table 6: Exemplars included in the few-shot prompt for the decomposition of the problems from the ASDiv (upper row) and StrategyQA (lower row) datasets.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>models with billions of parameters to be large, and models with millions of parameters to be small.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Following Li et al. (2022), we argue that small and large models are relative terms and context-dependent. We consider&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
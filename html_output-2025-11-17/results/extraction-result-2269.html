<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2269 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2269</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2269</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-269748457</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.06155v3.pdf" target="_blank">How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent</a></p>
                <p><strong>Paper Abstract:</strong> Developing novel research questions (RQs) often requires extensive literature reviews, especially in interdisciplinary fields. To support RQ development through human-AI co-creation, we leveraged Large Language Models (LLMs) to build an LLM-based agent system named CoQuest. We conducted an experiment with 20 HCI researchers to examine the impact of two interaction designs: breadth-first and depth-first RQ generation. The findings revealed that participants perceived the breadth-first approach as more creative and trustworthy upon task completion. Conversely, during the task, participants considered the depth-first generated RQs as more creative. Additionally, we discovered that AI processing delays allowed users to reflect on multiple RQs simultaneously, leading to a higher quantity of generated RQs and an enhanced sense of control. Our work makes both theoretical and practical contributions by proposing and evaluating a mental model for human-AI co-creation of RQs. We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry. The system’s source is available at: https://github.com/yiren-liu/coquest.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2269.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2269.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoQuest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoQuest (LLM-based RQ co-creation system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive three-panel system that uses an LLM-based agent to automatically generate and iteratively refine research questions (RQs), retrieve and visualize related literature, and present chain-of-thought style rationales to support human–AI co-creation of research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CoQuest</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoQuest is an agent-based system implemented on an LLM backbone (gpt-3.5-turbo-16k in this study) that follows a ReAct-style Think→Act→Observe loop. The agent generates Chains-of-Thought during the Think step to choose among executable actions (search_and_summarize_papers, hypothesize_use_cases, narrow_down_rqs, compare_rq_with_papers). Actions are implemented as API-backed Python functions (e.g., retrieve_papers, summarize_papers). After executing actions and appending their outputs to context, the agent produces one or more follow-up RQs. Two interaction modes control AI initiative: breadth-first (multiple parallel RQs per turn) and depth-first (sequential multi-step refinement without user input during the sequence). A retrieval pipeline grounded in sentence embeddings and Maximal Marginal Relevance (MMR) returns top-k papers to visualize and ground generated RQs. The UI also exposes "AI Thoughts" (narrative rationales) and a mind-map style RQ flow editor to preserve provenance and enable human feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general scientific research (demonstrated in HCI research domain)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration / interdisciplinary synthesis (RQ ideation and literature-grounded refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Human expert (participant) ratings using Boden-inspired creativity dimensions on 5-point Likert scales: novelty, value, surprise, and relevance (per-RQ on-the-fly ratings). Temporal depth correlations (Spearman) were computed between RQ depth and these rating dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Depth-first RQs: novelty mean = 3.78 (SD = 1.29) vs breadth-first novelty mean = 3.28 (SD = 1.22); Mann-Whitney U = 2199.5, p = .002 (statistically significant). Spearman correlation between RQ depth and novelty: rho = 0.41, p < .001.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Empirical evidence that higher AI initiative (depth-first) produced RQs rated higher in novelty and surprise by participants, while lower AI initiative (breadth-first) produced better overall perceived co-creation experience (higher post-task creativity and trust survey scores). Statistical tests: MANOVA (F(4,209)=3.79, p=.0057) showed overall difference between conditions; depth-first > breadth-first on novelty and surprise (see novelty_score). Also observed: as depth increased beyond ~9, ratings dropped (suggested narrowness/repetitiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Design-level tradeoff: two interaction modes (breadth-first for parallel diverse outputs vs depth-first for recursive, deeper refinement) plus iterative human feedback; no explicit multi-objective optimizer reported—balance is achieved via human-in-the-loop selection and iterative refinement, and by designing wait-time affordances to encourage human reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Within-subject user study with 20 HCI researchers. Key findings: breadth-first condition yielded higher post-task perceived creativity and trust (survey), while depth-first condition produced RQs that participants rated higher for novelty and surprise (depth-first novelty M=3.78, SD=1.29 vs breadth M=3.28, SD=1.22; Mann-Whitney U=2199.5, p=.002). Spearman correlations show positive association between RQ depth and novelty (rho=0.41, p<.001), value (rho=0.22, p=.011), and surprise (rho=0.24, p=.006). Regression analyses showed longer user feedback length was positively associated with higher perceived RQ ratings (and increased cognitive load).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Within-system comparison of two generation interaction designs: breadth-first vs depth-first (counterbalanced within-subjects).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Breadth-first > depth-first on post-task survey measures of perceived creativity and trust (survey-level measures; numerical survey scale results reported in paper figures/tables). Depth-first > breadth-first on per-RQ novelty and surprise ratings (see novelty_score). MANOVA: (4,209)=3.79, p=.0057 indicating differences across the four rating dimensions between the two conditions. Mann-Whitney results reported for novelty and surprise (novelty U=2199.5 p=.002; surprise U=2387.5 p=.017).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Study conducted with HCI researchers and HCI publications (2,043 papers corpus). Findings indicate depth-first generation helps produce deeper, domain-specific novel RQs in HCI contexts, while breadth-first better supports early-stage brainstorming and perceived control; participants' familiarity with the topic moderated perceived control and evaluation (familiarity reduced perceived control).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2269.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2269.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/framework approach that interleaves chain-of-thought style reasoning ('Think') with API-like external actions ('Act') and observation parsing ('Observe') to let LLMs plan, call tools, and incorporate their results into subsequent reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct prompting framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ReAct is used as the logical scaffolding for CoQuest's agent: the agent produces chain-of-thought style 'Think' output to choose among actions, issues 'Act' commands that trigger API-backed functions (literature retrieval, summarization, hypothesizing use cases, narrowing RQs), and ingests 'Observe' outputs to continue reasoning. The paper adapts ReAct's think-act-observe loop to drive sub-processes that produce context used to generate RQs.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general-purpose LLM agent design (applied here to research ideation in HCI)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>automated reasoning + tool use for open-ended generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>agentic decomposition of tasks into Think/Act/Observe actions to improve grounding and reasoning for creative generation (no explicit novelty-feasibility optimizer reported).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2269.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2269.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-GPT (agentic LLM framework / repo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source agent framework that chains LLM calls with tool execution, memory, and task decomposition to form autonomous multi-step behaviors; used as a foundation for building agentic applications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Auto-GPT (agent framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoQuest used Auto-GPT as the implementation foundation for orchestrating the LLM agent, enabling recursive decision-making, action invocation (API calls to retrieve/summarize papers), and stepwise context accumulation for RQ generation. Auto-GPT supplies the agent-pattern and execution orchestration rather than a novelty/feasibility scoring model.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>agentic LLM systems (general)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>tool-chaining for multi-step generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>agent orchestration patterns (task decomposition and API-driven subprocesses) used to produce iterated RQs; no explicit multi-objective optimization reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2269.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2269.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentence-BERT + MMR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-BERT (sentence-bert) embeddings with Maximal Marginal Relevance (MMR) reranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval pipeline that encodes documents and queries into dense sentence embeddings (Sentence-BERT family) and re-ranks candidates using MMR to produce a diverse, relevant top-k set of papers for grounding generated RQs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>sentence-bert: Sentence embeddings using siamese bert-networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sentence embedding retrieval + MMR reranking</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoQuest encodes paper title/abstract/metadata and RQ text into sentence embeddings (text-embedding-ada-002 in this implementation) and computes similarity; candidate papers are re-ranked with Maximal Marginal Relevance (MMR) to promote relevance and diversity, then top-k papers are returned to the Paper Graph Visualizer to ground RQs and support "AI Thoughts." This pipeline is intended to reduce hallucination and provide literature grounding for novelty evaluation by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>information retrieval / literature grounding for research ideation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>literature retrieval for grounding generated ideas and reducing hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>diversity-aware reranking (MMR) to balance relevance and diversity of retrieved papers for grounding creative outputs; not an explicit novelty-feasibility optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2269.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2269.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Liu et al. (2023 RQ gen)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Creative Research Question Generation for Human-Computer Interaction Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that applied generative language models fine-tuned over related works and explicit research questions from publications to generate new research questions in HCI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Creative Research Question Generation for Human-Computer Interaction Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fine-tuned generative language-model approach for RQ generation (Liu et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reported approach fine-tunes smaller language models on corpora of related works and annotated/explicit research questions to produce candidate RQs; cited as previous work demonstrating feasibility of ML-based RQ generation in HCI. (This paper references Liu et al. as related work but does not replicate its metrics.)</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>HCI research question generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>automated generation of domain-specific research questions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>fine-tuning on RQ-labelled data and leveraging domain corpus; not described in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Listed as prior example of automatic RQ generation in HCI, motivating CoQuest's LLM-based agent that integrates retrieval and human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2269.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2269.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs to improve multi-step problem solving and to make model rationales more explicit and interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoQuest leverages chain-of-thought style prompts for the agent to generate internal reasoning (the 'Think' step) that both improves RQ generation and is surfaced (via 'AI Thoughts') to help users understand rationales and ground outputs in retrieved literature. The paper uses CoT to enhance explainability of the LLM agent and to structure the Think→Act→Observe cycle.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>LLM prompting / explainable generative systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>improving multi-step reasoning and explainability for idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>prompt engineering to elicit intermediate steps and rationales, used to support human judgment and iterative refinement of generated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Creative Research Question Generation for Human-Computer Interaction Research <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Ought, Elicit: The AI Research Assistant. <em>(Rating: 1)</em></li>
                <li>Augmenting Scientific Creativity with Retrieval across Knowledge Domains <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>sentence-bert: Sentence embeddings using siamese bert-networks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2269",
    "paper_id": "paper-269748457",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "CoQuest",
            "name_full": "CoQuest (LLM-based RQ co-creation system)",
            "brief_description": "An interactive three-panel system that uses an LLM-based agent to automatically generate and iteratively refine research questions (RQs), retrieve and visualize related literature, and present chain-of-thought style rationales to support human–AI co-creation of research ideas.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CoQuest",
            "system_description": "CoQuest is an agent-based system implemented on an LLM backbone (gpt-3.5-turbo-16k in this study) that follows a ReAct-style Think→Act→Observe loop. The agent generates Chains-of-Thought during the Think step to choose among executable actions (search_and_summarize_papers, hypothesize_use_cases, narrow_down_rqs, compare_rq_with_papers). Actions are implemented as API-backed Python functions (e.g., retrieve_papers, summarize_papers). After executing actions and appending their outputs to context, the agent produces one or more follow-up RQs. Two interaction modes control AI initiative: breadth-first (multiple parallel RQs per turn) and depth-first (sequential multi-step refinement without user input during the sequence). A retrieval pipeline grounded in sentence embeddings and Maximal Marginal Relevance (MMR) returns top-k papers to visualize and ground generated RQs. The UI also exposes \"AI Thoughts\" (narrative rationales) and a mind-map style RQ flow editor to preserve provenance and enable human feedback loops.",
            "research_domain": "general scientific research (demonstrated in HCI research domain)",
            "problem_type": "open-ended exploration / interdisciplinary synthesis (RQ ideation and literature-grounded refinement)",
            "novelty_metric": "Human expert (participant) ratings using Boden-inspired creativity dimensions on 5-point Likert scales: novelty, value, surprise, and relevance (per-RQ on-the-fly ratings). Temporal depth correlations (Spearman) were computed between RQ depth and these rating dimensions.",
            "novelty_score": "Depth-first RQs: novelty mean = 3.78 (SD = 1.29) vs breadth-first novelty mean = 3.28 (SD = 1.22); Mann-Whitney U = 2199.5, p = .002 (statistically significant). Spearman correlation between RQ depth and novelty: rho = 0.41, p &lt; .001.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "Empirical evidence that higher AI initiative (depth-first) produced RQs rated higher in novelty and surprise by participants, while lower AI initiative (breadth-first) produced better overall perceived co-creation experience (higher post-task creativity and trust survey scores). Statistical tests: MANOVA (F(4,209)=3.79, p=.0057) showed overall difference between conditions; depth-first &gt; breadth-first on novelty and surprise (see novelty_score). Also observed: as depth increased beyond ~9, ratings dropped (suggested narrowness/repetitiveness).",
            "optimization_strategy": "Design-level tradeoff: two interaction modes (breadth-first for parallel diverse outputs vs depth-first for recursive, deeper refinement) plus iterative human feedback; no explicit multi-objective optimizer reported—balance is achieved via human-in-the-loop selection and iterative refinement, and by designing wait-time affordances to encourage human reflection.",
            "human_evaluation": true,
            "human_evaluation_results": "Within-subject user study with 20 HCI researchers. Key findings: breadth-first condition yielded higher post-task perceived creativity and trust (survey), while depth-first condition produced RQs that participants rated higher for novelty and surprise (depth-first novelty M=3.78, SD=1.29 vs breadth M=3.28, SD=1.22; Mann-Whitney U=2199.5, p=.002). Spearman correlations show positive association between RQ depth and novelty (rho=0.41, p&lt;.001), value (rho=0.22, p=.011), and surprise (rho=0.24, p=.006). Regression analyses showed longer user feedback length was positively associated with higher perceived RQ ratings (and increased cognitive load).",
            "comparative_baseline": "Within-system comparison of two generation interaction designs: breadth-first vs depth-first (counterbalanced within-subjects).",
            "comparative_results": "Breadth-first &gt; depth-first on post-task survey measures of perceived creativity and trust (survey-level measures; numerical survey scale results reported in paper figures/tables). Depth-first &gt; breadth-first on per-RQ novelty and surprise ratings (see novelty_score). MANOVA: (4,209)=3.79, p=.0057 indicating differences across the four rating dimensions between the two conditions. Mann-Whitney results reported for novelty and surprise (novelty U=2199.5 p=.002; surprise U=2387.5 p=.017).",
            "domain_specific_findings": "Study conducted with HCI researchers and HCI publications (2,043 papers corpus). Findings indicate depth-first generation helps produce deeper, domain-specific novel RQs in HCI contexts, while breadth-first better supports early-stage brainstorming and perceived control; participants' familiarity with the topic moderated perceived control and evaluation (familiarity reduced perceived control).",
            "uuid": "e2269.0",
            "source_info": {
                "paper_title": "How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "brief_description": "A prompting/framework approach that interleaves chain-of-thought style reasoning ('Think') with API-like external actions ('Act') and observation parsing ('Observe') to let LLMs plan, call tools, and incorporate their results into subsequent reasoning steps.",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "mention_or_use": "use",
            "system_name": "ReAct prompting framework",
            "system_description": "ReAct is used as the logical scaffolding for CoQuest's agent: the agent produces chain-of-thought style 'Think' output to choose among actions, issues 'Act' commands that trigger API-backed functions (literature retrieval, summarization, hypothesizing use cases, narrowing RQs), and ingests 'Observe' outputs to continue reasoning. The paper adapts ReAct's think-act-observe loop to drive sub-processes that produce context used to generate RQs.",
            "research_domain": "general-purpose LLM agent design (applied here to research ideation in HCI)",
            "problem_type": "automated reasoning + tool use for open-ended generation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "agentic decomposition of tasks into Think/Act/Observe actions to improve grounding and reasoning for creative generation (no explicit novelty-feasibility optimizer reported).",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2269.1",
            "source_info": {
                "paper_title": "How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Auto-GPT",
            "name_full": "Auto-GPT (agentic LLM framework / repo)",
            "brief_description": "An open-source agent framework that chains LLM calls with tool execution, memory, and task decomposition to form autonomous multi-step behaviors; used as a foundation for building agentic applications.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Auto-GPT (agent framework)",
            "system_description": "CoQuest used Auto-GPT as the implementation foundation for orchestrating the LLM agent, enabling recursive decision-making, action invocation (API calls to retrieve/summarize papers), and stepwise context accumulation for RQ generation. Auto-GPT supplies the agent-pattern and execution orchestration rather than a novelty/feasibility scoring model.",
            "research_domain": "agentic LLM systems (general)",
            "problem_type": "tool-chaining for multi-step generation tasks",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "agent orchestration patterns (task decomposition and API-driven subprocesses) used to produce iterated RQs; no explicit multi-objective optimization reported.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2269.2",
            "source_info": {
                "paper_title": "How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Sentence-BERT + MMR",
            "name_full": "Sentence-BERT (sentence-bert) embeddings with Maximal Marginal Relevance (MMR) reranking",
            "brief_description": "A retrieval pipeline that encodes documents and queries into dense sentence embeddings (Sentence-BERT family) and re-ranks candidates using MMR to produce a diverse, relevant top-k set of papers for grounding generated RQs.",
            "citation_title": "sentence-bert: Sentence embeddings using siamese bert-networks",
            "mention_or_use": "use",
            "system_name": "Sentence embedding retrieval + MMR reranking",
            "system_description": "CoQuest encodes paper title/abstract/metadata and RQ text into sentence embeddings (text-embedding-ada-002 in this implementation) and computes similarity; candidate papers are re-ranked with Maximal Marginal Relevance (MMR) to promote relevance and diversity, then top-k papers are returned to the Paper Graph Visualizer to ground RQs and support \"AI Thoughts.\" This pipeline is intended to reduce hallucination and provide literature grounding for novelty evaluation by humans.",
            "research_domain": "information retrieval / literature grounding for research ideation",
            "problem_type": "literature retrieval for grounding generated ideas and reducing hallucination",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "diversity-aware reranking (MMR) to balance relevance and diversity of retrieved papers for grounding creative outputs; not an explicit novelty-feasibility optimizer.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2269.3",
            "source_info": {
                "paper_title": "How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Liu et al. (2023 RQ gen)",
            "name_full": "Creative Research Question Generation for Human-Computer Interaction Research",
            "brief_description": "A prior work that applied generative language models fine-tuned over related works and explicit research questions from publications to generate new research questions in HCI.",
            "citation_title": "Creative Research Question Generation for Human-Computer Interaction Research",
            "mention_or_use": "mention",
            "system_name": "Fine-tuned generative language-model approach for RQ generation (Liu et al. 2023)",
            "system_description": "Reported approach fine-tunes smaller language models on corpora of related works and annotated/explicit research questions to produce candidate RQs; cited as previous work demonstrating feasibility of ML-based RQ generation in HCI. (This paper references Liu et al. as related work but does not replicate its metrics.)",
            "research_domain": "HCI research question generation",
            "problem_type": "automated generation of domain-specific research questions",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "fine-tuning on RQ-labelled data and leveraging domain corpus; not described in detail in this paper.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Listed as prior example of automatic RQ generation in HCI, motivating CoQuest's LLM-based agent that integrates retrieval and human feedback.",
            "uuid": "e2269.4",
            "source_info": {
                "paper_title": "How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs to improve multi-step problem solving and to make model rationales more explicit and interpretable.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought prompting",
            "system_description": "CoQuest leverages chain-of-thought style prompts for the agent to generate internal reasoning (the 'Think' step) that both improves RQ generation and is surfaced (via 'AI Thoughts') to help users understand rationales and ground outputs in retrieved literature. The paper uses CoT to enhance explainability of the LLM agent and to structure the Think→Act→Observe cycle.",
            "research_domain": "LLM prompting / explainable generative systems",
            "problem_type": "improving multi-step reasoning and explainability for idea generation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "prompt engineering to elicit intermediate steps and rationales, used to support human judgment and iterative refinement of generated ideas.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2269.5",
            "source_info": {
                "paper_title": "How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Creative Research Question Generation for Human-Computer Interaction Research",
            "rating": 2,
            "sanitized_title": "creative_research_question_generation_for_humancomputer_interaction_research"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Ought, Elicit: The AI Research Assistant.",
            "rating": 1,
            "sanitized_title": "ought_elicit_the_ai_research_assistant"
        },
        {
            "paper_title": "Augmenting Scientific Creativity with Retrieval across Knowledge Domains",
            "rating": 2,
            "sanitized_title": "augmenting_scientific_creativity_with_retrieval_across_knowledge_domains"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "sentence-bert: Sentence embeddings using siamese bert-networks",
            "rating": 2,
            "sanitized_title": "sentencebert_sentence_embeddings_using_siamese_bertnetworks"
        }
    ],
    "cost": 0.01836075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>20 Mar 2024</p>
<p>Yiren Liu 
CHI '24, May 11-162024HonoluluHIUSA</p>
<p>CHI '24, May 11-162024HonoluluHIUSA</p>
<p>Si Chen 
CHI '24, May 11-162024HonoluluHIUSA</p>
<p>CHI '24, May 11-162024HonoluluHIUSA</p>
<p>Haocong Cheng 
CHI '24, May 11-162024HonoluluHIUSA</p>
<p>CHI '24, May 11-162024HonoluluHIUSA</p>
<p>Mengxia Yu 
CHI '24, May 11-162024HonoluluHIUSA</p>
<p>CHI '24, May 11-162024HonoluluHIUSA</p>
<p>Xiao Ran 
CHI '24, May 11-162024HonoluluHIUSA</p>
<p>CHI '24, May 11-162024HonoluluHIUSA</p>
<p>Andrew Mo 
CHI '24, May 11-162024HonoluluHIUSA</p>
<p>CHI '24, May 11-162024HonoluluHIUSA</p>
<p>Yiliu Tang 
CHI '24, May 11-162024HonoluluHIUSA</p>
<p>Yun Huang 
CHI '24, May 11-162024HonoluluHIUSA
20 Mar 202460E9AC7C730B68A6A36C092F202383BE10.1145/3613904.3642698arXiv:2310.06155v3[cs.HC]Scientifc DiscoveryLarge Language ModelsCo-creation SystemsMixed-initiative Design
Developing novel research questions (RQs) often requires extensive literature reviews, especially in interdisciplinary fields.To support RQ development through human-AI co-creation, we leveraged Large Language Models (LLMs) to build an LLM-based agent system named CoQuest.We conducted an experiment with 20 HCI researchers to examine the impact of two interaction designs: breadthfirst and depth-first RQ generation.The findings revealed that participants perceived the breadth-first approach as more creative and trustworthy upon task completion.Conversely, during the task, participants considered the depth-first generated RQs as more creative.Additionally, we discovered that AI processing delays allowed users to reflect on multiple RQs simultaneously, leading to a higher quantity of generated RQs and an enhanced sense of control.Our work makes both theoretical and practical contributions by proposing and evaluating a mental model for human-AI co-creation of RQs.We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry.The system's source is available at: https://github.com/yiren-liu/coquest.CCS CONCEPTS• Human-centered computing → Empirical studies in HCI; Interactive systems and tools; • Computing methodologies → Natural language processing.</p>
<p>INTRODUCTION</p>
<p>Identifying research questions (RQs) is a critical step of scientific research and discovery [13].To formulate creative and valuable research ideas, researchers often start with searching and scoping the relevant literature, especially when the search involves works spanning multiple domains [29].Developing research questions is also iterative, that researchers would first have an initial idea or topic in mind, then conduct a literature search to refine the ideas, and repeat this process until they have a satisfying research idea [18].Reading a large body of literature, synthesizing them, and identifying the relevant work can be rather time-consuming.</p>
<p>With the rapid development of Large Language Models (LLMs), scholars have investigated the potential of harnessing the power of LLMs to support the process of research literature discovery [9, 54,59], which helps users significantly speed up the literature discovery process.Meanwhile, the thriving of generative AI technologies has also been widely used in various fields to promote creativity [14,48,87].For example, a recent study has found that more than 4.9% of users used ChatGPT to support creative ideation [17].Although recent research has demonstrated the potential of using smaller language models to generate novel RQs [42], there remains a lack of empirical understanding about how humans evaluate AI-generated RQs.Given that LLMs are known to have problems with hallucination and lack of factual accuracy [27], creating high-quality RQs requires inputs from human researchers for their unique backgrounds and expertise.Enabling humans and AI to co-create novel research questions is particularly promising for conducting interdisciplinary research.</p>
<p>The concept of human-AI co-creation draws insights from pioneering research on mixed-initiative systems, where human users and computer systems contribute to a shared goal [23,83].Recent academic endeavors have also introduced a variety of design guidelines for human-AI co-creation systems [39,62,78], further inspiring the intricate design considerations of mixed-initiative systems.For instance, Rezwana and Maher [62] explored whether the exchange of initiatives between humans and AI should be designed as either parallel or in a turn-taking manner.However, there remains a gap in empirical research regarding the influence of AIdriven initiatives or creativity on user experiences, such as their perception of the creative process, trust in the AI, and sense of control.Moreover, the potential impact of these factors on the results of human-AI co-creation, such as the quality of generated content, has yet to be investigated.</p>
<p>To this end, we proposed a novel system called CoQuest, which allows an AI agent to initiate RQ generation by tapping the power of LLMs and taking humans' feedback into a co-creation process.The system consists of three panels: RQ Flow Editor for supporting RQ generation, Paper Graph Visualizer for exploration of literature space, and AI Thoughts for explaining AI's rationales.The system design was informed by a formative study, where we invited four researchers to verbalize their expected RQ-generation processes with AI support.The formative study yielded an initial mental model of human-AI co-creation for RQs.The system design was further evaluated with 20 participants who are HCI researchers through a within-subject experiment.</p>
<p>Our work makes novel and significant contributions as follows.Firstly, we made a theoretical contribution by proposing and evaluating a new mental model for human-AI co-creation of research questions in the HCI research domain.Second, we proposed a new agent LLM system and implemented two interaction designs (breadth-first and depth-first), where the AI agent took different levels of initiative in supporting users to develop RQs.Third, through a within-subject experiment with HCI researchers, we gained new empirical understandings of how AI's different initiatives impact users' perceived experiences and outcomes.Specifically, for overall experience, the breadth-first design made users "feel" more creative and gained more trust from users, though the effect varies by users' research background; but when rating individual RQs, users scored the depth-first questions for higher creativity.Fourth, by closely observing participants' interaction with AI, we discovered important co-creation behavior and proposed a new design implication, namely, intentionally "slowing down AI", giving wait times for users to explore the co-creation space.This is especially beneficial for users to gain a stronger sense of control.Last but not least, we discussed the implications of our study for designing ethical human-AI co-creation systems, along with potential ethical concerns.We advocate for employing LLM-based systems to augment human creativity and support the ideation and scaffolding process, rather than using LLM-generated ideas for automating HCI research.</p>
<p>RELATED WORK 2.1 Human-Led Literature Discovery and Research Idea Creation</p>
<p>Scholars have sought to understand the process of how researchers conduct literature discovery and formulate new research ideas [20,56].Extensive works have strived to formulate the model of researchers' scientific activities [1,55,71].For example, Foster [18] proposed a framework of idea formulation in academic research from an information behavior perspective, which consists of three major components: Opening (initial exploration), Orientation (problem definition and literature survey) and Consolidation (knowledge creation).In this framework, literature discovery and idea formulation are discussed as a recursive and iterative process.Many studies have also been conducted to understand how researchers produce innovative ideas for research purposes [82].For example, Jing et al. [28] proposed a system for research idea creation that incorporates knowledge reuse through ontology construction.Later work by [21] introduced a system for supporting research ideation through topic modeling and visualization.Recently, Liu et al. [42] proposed to generate new research questions using generative language models fine-tuned over related works and explicitly written research questions from publications within the HCI domain.But none of these works leveraged large language models (LLMs) or built an LLM-based system to examine human-AI co-creation RQ processes.</p>
<p>Agent-based Large Language Model (LLM) Systems</p>
<p>Recent surge in the success of LLMs [52,69] has spurred strong interests in employing LLMs for solving complex tasks.There has recently been a heated wave of explorations in building autonomous agents using LLMs as a reasoning engine to solve different tasks, such as software development [58,65], gaming [73], and assisting social science research [91].System designs are proposed for improving the method of prompting in Human-AI interaction systems enabled by LLMs [10,80,81].To enhance the reasoning capabilities of LLMs, researchers proposed several prompting frameworks to elicit reasoning in LLMs by decomposing and solving the sub-tasks step by step by applying prompting techniques for general purposes, such as Chain-of-Thought [34,76], Self-Consistency [74], Least-to-Most [90].These prompting techniques focus on improving the task-specific performance of LLMs.To build autonomous agents with LLMs, Yao et al. [84] proposed a prompting framework named "ReAct" that unifies the ability of LLMs to reason, take actions, and observe the results.Recent research on LLM-based prompt engineering [75] has explored a viable approach of introducing humans' mental model as prompting techniques to boost LLM's reasoning ability.These frameworks set a great foundation for Q&amp;A reasoning or general-purpose tasks [41].In this paper, we applied LLM to build an agent for specialized tasks in the context of literature discovery and research ideation.</p>
<p>Designing Human-AI Co-creation Systems</p>
<p>Many human-AI co-creation systems utilize generative AI technologies [45,68].For example, Bilgram and Laarmann [2] showed that generative AI could augment the early phases of innovation, such as exploration, ideation, and digital prototyping.Epstein et al. [15] discovered that discrepancies between users' expected outputs and the actual output from the system play a critical role in creating new ideas.</p>
<p>Scholars and practitioners try to develop design guidelines for using generative AI [11,40,49,78].From a role-based perspective, human-AI co-creation systems are found to be different from prior systems where machines mainly provide support to humans [31].In co-creation systems, both humans and AIs can be designed to take the initiative in producing creative artifacts [22,51].Rezwana and Maher [62] introduced a Co-Creative Framework for Interaction design (COFI), which categorizes mixed-initiative system designs into two types based on their styles of participation: parallel and turn-taking.The evaluation of mixed-initiative designs has been explored by recent research [35,79] in application domains such as poem writing and game design.Our research seeks to systematically model the co-creation process and provide both theoretical and practical implications in the scholarly research domain.</p>
<p>In this study, we propose and examine a new agent LLM system that helps researchers formulate research questions by combining LLMs' reasoning ability with the mental model we discovered through a formative study with actual researchers.We also discuss whether LLM-based co-creation systems can help facilitate the process of information gathering and idea evolution using user study results to provide new empirical understandings.</p>
<p>RESEARCH QUESTIONS</p>
<p>Given the above literature and identified research gaps, we aim to address the following research questions: RQ1 (perception &amp; outcome): How do users perceive the cocreation experience (e.g., trust, control, feeling of being creative) and outcomes (creativity ratings of generated RQs) when using the CoQuest system?RQ2 (behavior): How do users interact with AI when it provides different levels of initiatives during the co-creation process?RQ3 (relationship): What behavioral factors are associated with users' enhanced perceived experience and outcomes in human-AI co-creation?</p>
<p>In the remainder of this paper, we will first present the formative study, where we developed an initial mental model for RQ cocreation between humans and AI.We then introduce the system design and our experimental study and detail the findings in the order of the proposed research questions.We will conclude with an in-depth discussion on the updated mental model informed by our findings and explore design implications for future work.Note that we use RQ1, RQ2, and RQ3 to refer to our three research questions in the remainder of the paper, whereas RQ and RQs are used to refer to the research questions co-created by the user and AI.</p>
<p>FORMATIVE STUDY</p>
<p>We conducted a formative study in order to understand the cognitive process of researchers creating research questions.To understand how researchers create RQs, we first conducted semistructured interviews with 4 HCI researchers about how they formulate research questions.We then organized a focus group with the same group of interviewed participants to identify their needs for an RQ co-creation system.</p>
<p>We invited four researchers to participate in our semi-structured interviews to understand their process of conducting research.All four participants were doctoral researchers.We asked the interviewees to describe their most recent experience starting a research project from scratch.Most participants mentioned starting from a rough idea (e.g., domains, keywords, application scenarios) before searching for related literature.Participants also emphasized that formulating research ideas is often an iterative process.Typically, participants used one or multiple hypothetical research questions/ideas to facilitate the search for related literature and identify research gaps.This process was described by the participants as both time-consuming and labor-intensive.Participants also identified their process of research question creation as hierarchical.One participant explicitly mentioned that creating research questions naturally resembles the form of a "mind map", where the development of ideas gradually "narrows down" but could have different branches.As described by the participants, the initial set of RQs can often be broader and more general and can subsequently derive sub-RQs under the topics of their predecessors, which helps facilitate the trade-off between specific and general ideas as a common decision-making step during the research ideation process.This process often resulted in the evolution of RQs that, when visualized, could be drawn as a tree of RQs.This later informed the design of our system to take the form of an interactive "mind map" that preserved the provenance of RQ development.</p>
<p>After the first interview study, the same participants were invited back to a focus group session, where we aimed to identify their needs based on their research workflow and propose interaction design to support the process.The focus group went through two steps.First, participants were given three questions created by researchers as cues, and then were asked to brainstorm ideas and design expectations given the context of an AI-based system that supports research question development.The three question cues were designed as follows: 1) What questions will you ask the AI system? 2) What information do you want to provide for the model to generate RQ for you? 3) How do you produce RQ currently?And do you think the AI model can help you produce RQs?The ideas created by participants are shown in Figure 2.</p>
<p>After discussion and summarizing themes, participants were asked to proceed to the second step inspired by participatory approaches, where they discussed a hypothetical workflow by contextualizing themselves using an AI-based RQ co-creation system.During the focus group, we found that participants highlighted several expectations for designing a human-AI RQ co-creation system.Participants mentioned that the system should enable strong user control by taking into consideration users' inputs (e.g., ideas, keywords, and domain concepts), when generating new RQs.The ability for the system to generate different variations of RQs with high diversity was also deemed a preferred design, where the user should be allowed to choose from the outputs based on their preference and expertise.</p>
<p>Human-AI Co-Creation of Research Questions (RQs): a Mental Model After completing the formative study, we summarized the findings and proposed a new mental model aiming to capture the major interactions during the process of Human-AI Co-Creation for RQs.</p>
<p>The proposed mental model consists of three major components: Understand Literature, Present RQs and Explain AI Thoughts, and Refine and Re-scope.Past research has provided in-depth discussion over how literature discovery plays a major role in the scientific research process [18,55].However, such models were often discussed in a human-only context.Our formative study results indicate the importance of considering AI as a "collaborator" during the process of research ideation [33].Refining research questions has also often been interpreted as a sub-step of literature search and understanding.Although research ideation and literature search are mutually dependent, our formative study results indicated a distinction between the behaviors researchers conducted during the two different stages.The process of literature discovery often involves reading and summarizing a wide range of existing works.Participants emphasized their challenges during the process of literature search and discovery, especially for unfamiliar domains, and the need for AI to facilitate this process with less human involvement.AI can be best used to perform factual summarization and distillation of findings and knowledge before presenting them to humans.On the contrary, proposing and refining research questions often requires more creative thinking and generalization beyond past knowledge.In this scenario, it is crucial to involve both humans and AI through the design of mix-initiative co-creation systems in order to combine humans' expertise and preference with AI's general world knowledge.Moreover, our formative study findings explicate the evaluation of RQs as an additional component during the RQ co-creation process.This process, as discussed during the formative study, should utilize human expertise and the ability to conduct follow-up research and validation.</p>
<p>Design Requirements Based on our findings from the focus group and the proposed mental model, we also propose the following design requirements for the system: The system should be able to 1) assist users' brainstorming process by automatically generating RQ candidates by taking human feedback; 2) support users' sensemaking of AI's outputs by Explaining the rationale behind the generation; 3) help users discover relevant literature and identify research gaps.</p>
<p>CoQuest SYSTEM DESIGN AND IMPLEMENTATION</p>
<p>Based on our findings from the formative study, we designed and implemented an LLM-based system that supports human-AI cocreation of creative research questions.In this section, we provide details about the design of 1) the three-panel interface of the Co-Quest system, including two different designs to provide varied degrees of AI initiative; and 2) the agent LLM backend of the Co-Quest system.An "action" labeled with an AI icon denotes that participants perceived that AI could significantly reduce the "labor"; a human icon means that participants were expected to evaluate AI-generated RQs and provide feedback to drive the iterative process.However, it is hard to determine how human and AI share the task of "refine and re-scope," as it depends on individual expertise and the clarity of the intended research focus.</p>
<p>CoQuest Interaction Design</p>
<p>To support RQ co-creation, we designed features of the CoQuest systems around the two-way communication between users and the AI, where users and AI take turns during the communication process.As shown in Figure 1, our proposed system consists of three major panels:</p>
<p>(1) RQ Flow Editor that facilitates a user's major interactions, such as generating RQs, providing input and feedback to AI, and editing the RQ flow (e.g., drag and delete); (2) Paper Graph Visualizer that displays the literature space related to each RQ;</p>
<p>(3) AI Thoughts that explains AI's rationale of why each RQ is generated.</p>
<p>Example User</p>
<p>Walkthrough.Consider a user of the CoQuest system, Jamie, who is a junior doctoral student with a research direction in Human-Computer Interaction.Jamie has previously been familiar with publications related to interaction design for online learning systems.With recent exposure to social media discussions on VR and AR applications, they wanted to explore the potential of such applications in the domain of online learning.Jamie formed an initial idea of using AR to promote learner's brainstorming.However, without a deeper understanding of the literature space from the VR and AR domain, they found it challenging to refine and improve upon the idea further.Introduced with the CoQuest system, Jamie first created an initial idea node, typed down "Using AR to promote brainstorming, " and generated follow-up RQs by right clicking the node and selecting "generate RQs" using the RQ Flow Editor.Aware of Jamie's initial idea, the CoQuest system retrieved several works related to the idea and generated follow-up RQ nodes along with rationales.Jamie first saw one of the RQ nodes with an RQ displayed as "How can social AR be designed to promote collaborative brainstorming?" Jamie was intrigued by the generated RQ, but also a bit confused since the concept of "social AR" appeared new to them.Jamie then clicked the RQ node and skimmed through the papers displayed on the Paper Graph Visualizer panel.Several papers seemed relevant to Jamie, and they further clicked the provided URL to read the papers in detail.The papers retrieved by CoQuest helped Jamie comprehend the domain knowledge behind the generated RQ.</p>
<p>After the reading, Jamie had a question in mind -"Why is using AR important for collaborative brainstorming?".They then typed in this question as user feedback to the system and clicked to generate new RQs following up the previous RQ.One of the newly generated RQs, "How can spatial design in AR promote group-based collaborative brainstorming, " caught Jamie's attention.</p>
<p>The appearance of the concept "spatial design" raised Jamie's interest, but they were unclear about the system's rationale for generating this RQ from the previous question.By clicking on the edge connecting this RQ with its predecessor, they were able to examine the AI Thoughts panel explaining the rationales and actions taken by the agent LLM in the CoQuest backend that led to the RQ.The panel showed that the system performed action "hypothesize user cases" and one of the resulting use cases was shown as "Online learners can form groups and organize ideas by creating and organizing concepts and links in spatial AR environment." 5.1.2RQ Flow Editor: Two Design Options.CoQuest offers an interactive RQ Flow Editor panel that allows users to co-create RQs with AI in an iterative manner.This panel is designed in a way to resemble the design of a mind map, where each RQ node represents a generated RQ (except the initial nodes, which only contain users' initial ideas).This allows users to organize RQs more easily under different topics while preserving the hierarchical structure among different RQs, as suggested by the findings from the formative study in section 4. Users can type in their rough ideas or keywords by creating an initial node.When users click on one of the nodes, the node expands, and users can 1) type in textual user feedback to AI in the text box; and 2) right-click on the node to generate more follow-up RQs.The generated RQs will be connected with annotated edges to the source RQ node.The RQ generation will result in one or several Directed Acyclic Graphs (DAGs), which we will later refer to as RQ flows, that embed both the hierarchical relations between generated RQs.Users can perform basic interactions using the RQ flow editor, including zooming in/out and dragging the nodes to organize the flows to aid their thinking process.</p>
<p>Two design options with different levels of AI initiative: Breadth-first and Depth-first generation.One of the major design options we considered for the CoQuest system is the degree of how much AI takes initiative during the co-creation process.During the formative study, we obtained an understanding of how users formulated their research questions in a hierarchical form, where follow-up RQs are iteratively created based on previous predecessor RQs.Intuitively, when formulating different RQs, the researcher might choose to explore different topics in a broader sense, or to dive deep into a specific topic.Thus, we consider two different designs when the system generates new RQs, as shown in Figure 4: 1) breadth-first generation: When a user initiates the generation of follow-up RQs, several RQs are produced simultaneously in parallel.These new questions are all at the same hierarchical level following the original question.2) depth-first generation: In contrast, with this method, when a user initiates the generation, follow-up RQs are created one after the other, with each new question building upon the previous one in a sequential manner.</p>
<p>The two designs impact the degree of initiative taken by AI in the CoQuest system.Under the breadth-first generation, the user will have the freedom to choose from multiple generated RQs and provide feedback at each turn of generation, and then proceed to generate more RQs if desired.As in the depth-first generation, the agent recursively generates a sequence of multiple RQs without user input during the process.During the depth-first generation, the agent needs to autonomously further "refine" the RQs multiple steps based on the previous steps' results, thus taking more initiative during the co-creation.Although in both designs, AI actively participates in the co-creation process by turn-taking with the human [62], there exists a difference between the degree of initiative taken by AI, as AI engages more actively during the depth-first generation compared to the breadth-first generation.By carrying out the within-subject study under the two designs, we aim to provide an empirical understanding of how initiative-driven design options can impact users' behavior and perception using human-AI co-creation systems.To simplify the study design, we ensured that three RQs were generated per turn in both designs.</p>
<p>Paper Graph</p>
<p>Visualizer: Interactive Literature Graph.The Co-Quest system also provides an LLM-enabled literature discovery feature to assist users in efficiently identifying and exploring existing works related to each generated RQ.As shown in Figure 1, CoQuest's literature discovery feature is presented in the Paper Graph Visualizer panel.</p>
<p>When a user clicks on one of the generated RQ nodes, the Paper Graph Visualizer panel reveals itself by visualizing the top-k most relevant papers along with their citation relations retrieved from our citation graph.The top-k papers are retrieved using our paper retrieval pipeline, as described in section 5.2.2, using the text of the RQ clicked on by the user as the query.In the displayed citation graph, each paper node represents a paper, and each edge represents a citation relation.When the user hovers the cursor over one of the paper nodes, a tooltip will appear with a quick preview of the paper's title information.</p>
<p>The paper nodes in the citation graph are designed to be interactive.When the user clicks on one of the paper nodes , the detailed information of the selected paper will be displayed below.The information displayed includes the paper's title, author names, abstract, a TLDR summary provided by Semantic Scholar API 1 , and a URL linking to the page of the paper on Semantic Scholar.Upon the click on the paper node, the Paper Graph Visualizer panel will also highlight the selected node and its nearest neighbors (nodes and edges) to indicate which paper(s) have directly cited the selected paper or been cited by the selected paper.</p>
<p>AI Thoughts:</p>
<p>Explaining AI Rationale.The edges between RQ nodes represent the relation of which RQ the newly generated RQ is based on.They also contain information about the results of the actions undertaken by the agent leading up to the new RQ generated.When the user clicks on an edge, the AI Thoughts panel appears that displays the results of the agent's action in a narrative format, as shown in the Figure 1.Detailed implementations of the LLM-based backend for generating RQs and rationales are provided below.</p>
<p>CoQuest Backend and Implementation</p>
<p>The backend of the CoQuest system comprises an LLM-based agent that automatically generates RQs based on users' input and feedback, by performing reasoning and executing actions that simulate a researcher's mental model in a semi-autonomous manner.Two major functions of the CoQuest backend include: 1) the RQ generation ability of the LLM agent and 2) the related paper retrieval module that supports literature discovery.The framework of how the system's backend connects with the major features are shown in Figure 5.</p>
<p>Generating RQs with LLM-based</p>
<p>Agent.The CoQuest system uses an LLM-based agent to generate creative research questions (RQs) following the ReAct framework [85], by adapting the "Think-Act-Observe" framework when designing the prompting method.First, the "Think" step analyzes user input and context to decide an action, resembling human research methods detailed in 4.During this round, the LLM generates a chain of thought (following our designed prompt) before reaching the conclusion of the action as the next step.The actions are executable sub-processes whose results will be used as additional context to help the LLM generate better RQs.The available actions include: 1) Search and summarize related works (Literature Discovery); 2) Hypothesizing use cases (Proposition); 3) Scoping/narrowing down (Refinement); 4) Reflection through comparison with existing works (Evaluation).Next, during the "Act" and "Observe" steps, the execution of actions is achieved in the format of API calls through prompting and can be later parsed and executed through one or multiple pre-implemented Python functions (e.g., retrieve_papers and summarize_papers).After the next action has been inferred during the "Think" step, the agent executes the action and appends the results of the action to the context.Finally, for the agent to generate RQs at each step, we added an additional step of creating RQs at the end of each "Observe" step.At this step, new RQs are generated based on the provided context and instructions that combine the output from the performed action and predefined prompt.The detailed usage of prompts in the backend can be found in Appendix A.3.</p>
<p>Related Paper Retrieval.</p>
<p>In order to help users identify related works more easily, the CoQuest system employs a retrieval pipeline to gather existing papers related to the RQs that the users are developing.We curated a literature citation graph from an existing pool of HCI papers, where the nodes represent papers, and the edges represent citation relations.The CoQuest system uses a sentence-based semantic embedding model [61] to obtain vector representations of each paper in the citation graph.Given a paper's title, metadata, and abstract in text form and a given query (e.g., RQs and user input), the sentence embedding model encodes them into semantic embeddings.After obtaining the embeddings of papers and the query, the system calculates the similarity between paper candidates and the query.This is done through re-ranking using Maximal Marginal Relevance (MMR) [7].Then, the system ranks the paper candidates and selects the top-k papers with the highest similarity scores as the final related papers to visualize.More details on the implementation of the retrieval pipeline are discussed in 5.2.3.</p>
<p>System</p>
<p>Implementation.The CoQuest system is implemented in Typescript as a web application using ReactJS and TailwindCSS for the frontend.The interactive flow editor is implemented using React Flow2 .The application backend uses Python with FastAPI3 as the RESTful API server framework.We use AutoGPT 4 as the foundation of our agent-based LLM implementation 5 .We used the gpt3.5-turbo-16kmodel by OpenAI as our LLM engine and text-embedding-ada-002 model as the sentence embedding model through the cloud service API provided by Microsoft Azure.We collected a fixed set of open-access publications through the Semantic Scholar API covering several major HCI conferences (including CHI, CSCW, UIST, Group, IMWUT, IJHCI, and IUI).The final collection of publications includes 2,043 papers.</p>
<p>USER STUDY EVALUATING CO-CREATION WITH COQUEST</p>
<p>To further understand the effect of the CoQuest system and how two designs of RQ generation impact users' human-AI co-creation behavior, we conducted a within-subjects user study with 20 HCI researchers from 8 different institutions by asking the participants to create new research questions using the CoQuest system.All participants were graduate students currently enrolled or just graduated with prior experience with research.During the study process, we collected participants' behavior and perception (i.e., ratings towards RQs, and ratings towards the CoQuest system) data for mixed-method (quantitative and qualitative) analysis.All studies were completed remotely online over video calls, where participants were asked to share their screens.Participants were also free to withdraw at any point during the study.Study procedures were approved by the IRB of the researchers' institution.We compensated participants $20 per hour for the user study.Studies lasted 1-2 hours, including two tasks using two designs (breadth-first and depth-first generations), a survey after each task, and an exit interview.</p>
<p>6.1 Within-Subjects User Study -Two Tasks with Assigned Condition: Breadth-First vs.</p>
<p>Depth-first</p>
<p>A within-subjects user study was conducted to understand the difference in users' behavior and perception of the system potentially brought by the two different conditions using the two designs, referred to as breadth-first condition and depth-first condition.Each participant was asked to complete two tasks: During each task, we asked each participant to complete a task designed by researchers to simulate real-life scenarios of research idea formulation.The two task topics used in this study are: "AR/VR for education and learning" and "AI and crowdsourcing".The two topics were chosen since they cover a wide range of specific domains and provide ample opportunities for users to explore and drill down on related topics.To account for the effect of the chronological order of both the task topics and conditions on the results, we followed a counterbalanced design [57] by randomizing the experiment conditions so that all possible orders and combinations were randomly assigned to an equal number of participants.Participants were encouraged to think aloud during the tasks.</p>
<p>Data Collection and Analysis</p>
<p>We collected both users' perception and behavior data to perform a comprehensive evaluation of the CoQuest system.To understand users' perception of the CoQuest system, we gathered two types of perception data from participants: individual ratings for each generated RQ (RQ ratings), which reflects users' perception of their co-creation outcomes; and overall post-task evaluations of the system (system ratings), which reflects users' perception of their co-creation experience.Users' behavior data was also collected in the form of system logs and video recordings for later analysis.</p>
<p>Co-Creation</p>
<p>Outcome: Rating AI-Generated RQs During Each Task.The RQ ratings are collected on the fly during each task of the study, where participants were asked to rate at least six RQs of their choice.These ratings are intended to capture the immediate perception of users towards the co-creation outcomes (i.e., generated RQs).The during-task rating collection was designed with the intention of nudging users to actively evaluate the RQs during the co-creation process, and also as a way to reflect the accurate user perception in real-time.We adopt Boden's criteria [3] to measure creativity from three different aspects-novelty, value, surprise, and relevance.Participants can give ratings using a 5-point Likert scale slider positioned under each RQ node.</p>
<p>Perceived</p>
<p>Experience: Survey Scores collected at the End of Each Task, and Exit Interviews.To analyze participants' perceived experience using our system, we collected their survey scores upon the completion of each task and conducted exit interviews before the end of each study.Survey Scores The co-creation experience scores, however, were given by participants at the end of each task through a survey.The survey contains multiple 5-point Likert scale questions designed to measure their experience from the aspects of: control, creativity, meta-creativity, cognitive load, and trust.We designed 2 Likertscale questions for each of the 5 aspects to avoid potential bias and ensure a more comprehensive evaluation 6 .The complete list of survey questions can be found in Appendix A.1.</p>
<p>Exit Interview After the participant completed the post-task rating survey, we also conducted a ten-minute semi-structured interview to obtain a deeper understanding of the participant's experience with the system.Interview data was analyzed using open-ended coding, by having two researchers review the interview transcripts and frequently discuss with each other [32].The interview coding highlighted differences in perception between the two conditions explicated by participants.To measure participants' familiarity with the task topic, we also asked participants how familiar they were with the two topics with three choices: not familiar, kind of familiar, and very familiar.</p>
<p>6.2.3</p>
<p>Behavior: Think-Aloud Data and System Log.We annotated users' behavior (think-aloud transcripts and system usage) to understand how users utilized our system.</p>
<p>Think-Aloud During Co-Creation Think-aloud data was primarily used for understanding how users generated and interpreted RQs.One researcher first generated a codebook through open coding using videos and transcripts from three randomly selected participants, and then three other researchers independently coded the data of the same three participants, reaching an inter-rater agreement of 0.83 in Krippendorf's alpha.The annotators then discussed and refined the codebook again until they reached full agreement.Then, four researchers proceeded to annotate the remaining 17 participants' behavior data separately.In the final codebook, whether users interacted with the system was annotated and used for quantitative analysis in RQ3 as "Acted During Wait".The final codebook also included sense-making behavior (e.g., reasons for (not) waiting, reason for providing certain feedback) as qualitative results.</p>
<p>System Log During Co-Creation We gathered multiple types of system logs for subsequent analysis of user behavior.We collected and used the counts of generated RQs and the lengths of user-typed feedback to AI for quantitative analysis of RQ1 and RQ3.User interactions such as clicks on components like RQ nodes and AI Thoughts were used for qualitative analysis along with the thinkaloud data.The text content of user-typed feedback was also used for qualitative analysis in RQ2.</p>
<p>We used different notations throughout this paper to distinguish among the different types of quotes presented in the results.For AI-generated RQs, we use italic (e.g., AI-generated RQ); for feedback to the AI, we use double-quotes (e.g., "feedback"); for interview and think-aloud quotes, we use double-quoted italics (e.g., "interview quotes").</p>
<p>FINDINGS 7.1 Perception of Co-Creation Experience and Outcome (RQ1)</p>
<p>The breadth-first condition allows users to generate multiple RQs in parallel with one interaction, whereas the depth-first condition creates three RQs sequentially, one after another.In this section, we analyzed how these two different conditions impact participants' perception towards both co-creation experience and outcomes.During the interview, 12 out of 20 participants (60%) also mentioned that they preferred the breadth-first condition.An example RQ flow generated by the participant (P4) during one of the sessions under the breadth-first condition is shown in Figure 7.The interview and think-aloud transcripts explained why the breadth-first condition was perceived to create a better experience.</p>
<p>First, the breadth-first condition results are easier to interpret and require less wait time to obtain the same amount of RQs compared with the depth-first condition.Participants appreciated that the AI was able to list the three generated RQs in parallel "all at once" (P10), which allowed them to easily "compare among the RQs" (P10) and "explore multiple potential research questions" (P4).It is also easier to understand the reason behind the generated RQs in breadthfirst condition, as all three RQs shared the same predecessor RQ and rationale.Although the depth-first condition also generated three RQs using one click, participants had to wait longer to see all three generated RQs than when using the breadth-first condition.Therefore, some participants tended to either focus on the first RQ and ignore the other two, or they would start with the last RQ and move to an earlier generated RQ if the latter one was not deemed ideal.</p>
<p>Second, participants found that the breadth-first condition gave them more control over which direction of RQs they would like to proceed with.With the three options listed in parallel, participants were able to choose the RQ that they preferred the most and generate more follow-up RQs based on it.The tree-structured design also allowed participants to highlight RQs that were more relevant, and then choose the branch they were more interested in.P14 found the breadth-first condition to be "less cognitively demanding, " and P16 preferred to use the breadth-first condition for "brainstorming under one topic." The depth-first condition, on the other hand, may generate RQs in the second and third iterations that were hard to understand how they were related to the first iteration.For example, P6 started an RQ flow with feedback "crowdsourcing and AI" with the depth-first condition.While the first question and their feedback did not mention the term medical diagnosis, this word appeared after the second iteration but disappeared again after the third iteration.During the interview, they mentioned that the depth-first condition "sometimes goes back and forth" and it was "confusing" to understand "the logic of why these 3 are parent, child, and grandchild [nodes]." 7.1.2Outcome: Depth-first Condition Yields RQs with Higher-rated Creativity.We measure the outcomes from the co-creation process by asking participants to rate the RQs on the fly during the tasks.To account for the problem of multiple comparisons, we conducted a MANOVA and found a significant difference ( (4, 209) = 3.79,  = .0057<em> * ;  ′  Λ = 0.909) in the user-provided ratings for RQs between the two conditions (i.e., breadth-first and depth-first).We conducted Mann-Whitney U tests with these ratings toward generated RQs and found that both the novelty ( = 2199.5, = .002</em> * ,  = .40, = .89)and surprise ( = 2387.5, = 0.017 * ,  = .32, = .75) of the RQs were rated higher when using the depth-first condition (M=3.78,SD=1.29) compared to the breadth-first condition (M=3.28,SD=1.22).In contrast to the post-task survey results that suggested that the breadth-first condition was perceived to be more creative, the RQ ratings suggested that the generated RQs using the depth-first condition were more innovative instead.Figure 8 shows an example of participants using the depth-first condition.</p>
<p>During tasks and interviews, participants explained why they found the outcomes from the depth-first condition to be more innovative.First, participants found that the depth-first condition tended to generate surprising RQs with ideas that were not present The participant provided feedback using keywords such as "AI and crowdsourcing" and "educational setting" to help the AI generate more RQs.The third iteration was not included in this figure .previously or included in their feedback to AI.Since the second and third RQs were generated based on the first RQ without user feedback, the AI sometimes may add unexpected keywords to the RQs.While a few participants found these unexpected additions to be "surprising in a negative manner" (P3) or "distracting" (P12), more participants described it in a positive way such as "insightful" (P18) or "impressive" (P8) and found it interesting to see the AI "thinking when generating the questions (P11)."This would be particularly useful if the participant was less familiar with a certain topic, as it would be more inspiring.</p>
<p>Second, the depth-first condition allows participants to dive deep into one chosen RQ and improve its quality.While it requires "more patience" (P14) to wait for and read through all three generated RQs from one interaction using depth-first condition, participants found it easier to go "deeper" and form an RQ that is "more specific" (P4).The depth-first RQs were also found to be more "creative and unique" (P13).P17 also expressed a positive opinion about having the freedom to choose and generate follow-up RQs from any AIgenerated RQs they would like to work on.In fact, participants could also utilize the AI generation wait time to better interpret existing RQs and generate more valuable RQs.</p>
<p>To further understand how users' perception towards RQs varied throughout the co-creation process, we performed a temporal analysis over the depth of RQ flows.By computing Spearman's correlation coefficient, we found significant positive correlations between the depth of RQs and their corresponding ratings of novelty ( (514) = .41, &lt; .001<em> * * ), value ( (514) = .22, = .011</em> ) and surprise ( (514) = .24, = .006* * ).We also found that RQs with depths of 9 or higher seemed to exhibit a drop in ratings.This might be due to the repetitiveness of RQs that was caused by the narrowing-down of topic and thus fixed literature space.To further illustrate this narrowing-down process of RQs, a sample flow of RQs generated under depth-first condition by one of the participants, P4, is shown in Figure 9.The participant initially expressed confusion towards the first RQ generated as it was perceived as "kind of vague" and "more like a literature review question instead of research question." After instructing the system to provide more details, P4 read and commented on the follow-up RQ generated as "an OK research question. . .But I wish it could be more specific on what (factor) makes VR/AR good for learning environment. . ." Then, they generated three more follow-up RQs and perceived them to be more specific and surprising "I'm a bit surprised that it was able to pick up the concept of accessibility."However, they noted that making sense of the connections between later RQs was more challenging.</p>
<p>Summary (RQ1): According to the post-session survey results, the breadth-first condition offered users a better co-creation experience, with higher perceptions of creativity and trust.In contrast, RQ rating data showed that the depth-first approach led to the generation of RQs deemed more creative by participants, with a unique depth and unexpected novelty.This insight indicates that Figure 8: Part of P2's RQ flow using the depth-first condition when exploring the topic of "AI and crowdsourcing."The dashed lines represent the second and third iterations of RQ generation, which were not based on user feedback.From the first three iterations, the participant chose to continue with the RQ generated in the first iteration, and then provided feedback, asking the AI to be more specific.Subsequently, the AI generated three more RQs, and the participant once again chose to proceed with the first of those three (subsequent RQs are not included in this figure).</p>
<p>while the breadth-first condition enhances user engagement, the depth-first condition stimulates deeper and more novel outcomes, leading to distinct advantages in different aspects of the co-creation process.Positive correlation between users' ratings towards RQs and depth were also observed, indicating improved user perception towards co-creation outcomes as exploration unfolded.</p>
<p>Users' Co-creation Behavior with LLM-based Agent (RQ2)</p>
<p>To explore users' co-creation behavior with AI, we examined how they interacted with the CoQuest system through think-aloud comments, feedback to AI, and activities performed while they were waiting for AI to generate the results.In this section, we explain the findings to shed light on how participants used the CoQuest system under the two conditions.</p>
<p>Depth-first Condition Stimulated</p>
<p>More User Interactions During Wait Time.The CoQuest system required users to wait for approximately 30 seconds for the LLM inference to finish after each time the generation was triggered.During this wait time, users can utilize other system features in parallel to the generation, such as interpreting other RQs, generating additional RQs, viewing AI Thoughts, and exploring the paper graph.By examining the behavior data of participants, we found that most participants utilized the wait time during the generation of RQs to perform other activities.In total, 12 out of 20 participants utilized the wait time to interpret and evaluate other existing RQs on the RQ flow editor.Among them, 7 participants created new RQs while waiting for prior RQs generation to be finished.We also found that more participants explored other RQs during wait time under the depth-first condition (N=12) than the breadth-first condition (N=6), with a two-proportion z-test indicating significance ( = −2.01, = 0.045 * ).</p>
<p>Participants were observed to switch among different threads of RQs during the wait time of generation.P16, for example, utilized wait time under depth-first conditions to start generating another thread of RQs.The participant further explained during the interview that if the RQs were generated quicker, "probably would just end up exploring one at a time.But because it was taking a while, I was like, oh, I can write another one."The participant found the generation wait time beneficial for brainstorming new ideas, as it offered a good opportunity for exploring different directions in parallel.In addition, we also found a positive correlation, shown by Pearson's correlation test ( (392) = 0.18,  &lt; 0.001 * * * ), between whether a user made use of the RQ generation wait time and the length of the user's feedback to AI.The test result indicated that users utilized the RQ generation wait time to contemplate more detailed feedback for AI, suggesting higher engagement in human-AI co-creation.During the wait time for generation, participants were also found to use the "AI thoughts" feature to understand the relationship between different RQs and the rationales behind generated RQs.Though it was confusing for some participants, most participants were observed to have read the "AI thoughts, " especially while waiting for all three RQs to be generated under the depth-first condition (14 out of 20 users clicked and viewed the "AI thoughts" more than 10 times).More specifically, in the depth-first condition, each click generates three different "AI thoughts" compared to breadth-first condition, where each click only generates one "AI thoughts."In addition to understanding the logical relation and reasoning behind RQs, participants also used "AI thoughts" for other co-creation purposes that were shared between the two conditions.For instance, users have found AI thoughts to be helpful both in terms of understanding the rationale of how RQs are generated, and also providing additional ideas for users to elaborate on.Some participants used the AI thoughts for sense-making of the generated RQs.When they recognized the AI thoughts were connected with the paper graph, they expressed that they trusted the system more because it seems like the AI thoughts are "from actual papers" that "are peerreviewed" rather than "coming from large language model, which is not necessarily [factually] true" (P10).Among our different types of "AI thoughts," one participant who could tell such differences (P16) said they liked the "summary of existing work" explanation type because it seems like this type "builds on existing works" and makes it more trustworthy.Some other participants also used the terms and keywords that appeared in "AI thoughts" to help develop their feedback to further guide RQ generation.For example, P10 wrote the feedback to AI "Explore feature selection for non-experts" by taking inspiration from the AI's thoughts about a summary of works related to feature selection techniques for data mining.</p>
<p>To understand how users' actions during wait time varied temporally during the exploration process, we performed Mann-Whitney U tests to examine the difference between users' wait time actions under breadth-and depth-first conditions within each stage, i.e. earlier (depth&lt;=3) and later (depth&gt;3) stages.The stage of exploration is determined by computing the mean depth across all RQ nodes ( = 3.38).Significant behavioral differences were observed between depth-first and breadth-first design conditions.The earlier stage of interaction revealed notable distinctions: users in the breadth-first condition engaged more frequently in 'Check paper graph' ( = 4414,  = 0.009 * * ) and 'Generate new RQs' ( = 4483,  = 0.011 * ) actions compared to those in the depth-first condition.This suggests that the breadth-first approach, which presents multiple research questions simultaneously, encourages more active exploration and engagement early in the interaction.However, these differences diminished in the later stage, indicating a convergence in user behavior as interaction progresses.This finding highlights the impact of initial information presentation on user actions, particularly in systems like CoQuest where engagement patterns can influence the user's exploratory experience.</p>
<p>Users Have Different Strategies for Providing Feedback to AI.</p>
<p>The CoQuest system allows users to type in textual feedback to AI under each RQ before triggering the generation of new RQs.Overall, 20 participants wrote 119 pieces of feedback in total using both conditions (M=7.44,SD=5.25) during the co-creation process.To find whether the participants had different ways of communicating with the AI using text feedback, we first analyzed the difference among users' input lengths (word counts) by fitting a mixed-effects model that considers both the random effect from different users and the potential fixed effect from the two conditions.Details of the model and results can be found in Appendix A.2.A likelihood ratio test was then conducted to validate the significance of the random effect brought by users ( 2 (1) = 9.189,  = 0.002 * * ).The result of the likelihood ratio test indicated a statistically significant difference in feedback lengths among different users regardless of the condition.However, we found no significant association between the system condition and users' feedback lengths.</p>
<p>To obtain a deeper understanding of how participants employ different strategies in providing feedback to AI, we further reviewed and coded participants' feedback to AI during the co-creation process and observed that participants tend to provide feedback to the AI in different ways.More specifically, we found three main themes.</p>
<p>The most common theme of giving AI feedback is to provide keywords (N=59), which is shorter in length.For feedback under this theme, 61.0 % (N=36) were used to start a new node using the keywords that the participant would like the AI to start with.From a generated RQ, participants may also provide new keywords and instruct the AI to include these keywords in the next iteration.Search and summarize was frequently used by AI to generate RQs based on the new keywords.For example, P4 started a node using the breadth-first condition with "AI and crowdsourcing, " as shown in Figure 7.The participant then continued with the generated RQ What are the ethical implications of using AI in crowdsourcing? and wrote "in an educational setting." The AI then generated three new RQs with the new keyword.</p>
<p>The second theme is asking AI-specific questions (N=40).These questions are usually longer and in full sentences that either ask the AI to explain terms used in a generated RQ, to lead the AI toward a new direction, or to ask the AI to review more related literature.For example, based on the RQ What is the impact of medical simulations on medical student learning and skill development?, P17 wrote, "It is a good question to research into, can you give me some information that is common among the papers."The participant was able to proceed with one of the three generated RQs.</p>
<p>A third theme is asking the AI to be more specific on an RQ (N=55).Feedback under this theme usually starts with the phrase "be more specific." In 45 out of the 55 cases, participants would also include the direction that they would like the AI to continue on.In response, the AI usually narrows down RQs or performs search and summarization.For example, after seeing the RQ How can we design AI systems that effectively collaborate with human experts to find relevant literature for research questions? using depth-first condition, P2 wrote, "Can you be more specific about AI systems?this is just repeating my initial thought."The AI then generated three more RQs, as shown in Figure 8.</p>
<p>Summary (RQ2): Participants were found to have employed different strategies when co-creating research questions (RQs) with AI.Participants mainly provided feedback by listing keywords, posing additional questions, or requesting specificity.While waiting for the AI's response, users often explored other RQs, showing greater engagement in conditions with longer wait times.The AI Thoughts panel, which offers insights into the AI's reasoning, was found beneficial in enhancing trust and inspiring feedback, especially when grounded in peer-reviewed literature.Moreover, our analysis revealed that in the early stages of interaction, users in the breadthfirst condition engaged in more actions during wait time, such as checking paper graphs and generating new research questions, indicating that initial information presentation in a breadth-first manner notably influenced users' early exploration behavior.</p>
<p>Association between Users'</p>
<p>Persona/Behavior and Their Perceptions (RQ3)</p>
<p>To better understand the factors influencing participants' perceived experiences and outcomes, we performed two linear regression analyses, aiming to associate user perceptions with their behaviors and backgrounds.The two sets of regressions aim to yield insights about users' perceptions from two perspectives: 1) the first set of regressions adopts participants' ratings for RQs (i.e., novelty, value, surprise, and relevance) as dependent variables; 2) another set of regressions participants' post-session survey scores for the system (i.e., control, creativity, meta creativity, and cognitive load) as the dependent variables.We chose factors that could represent users' behavior as predictors based on our previous findings 8 .We first considered the conditions of the system (breadth-first or depth-first) experienced by the user, as indicated by the results in 7.1 that the two conditions might have a difference between their impact over the user's perception of co-creation experience and outcomes.We constructed the predictor as a categorical variable, with value 0 standing for breadth-first condition and value 1 standing for depth-first condition.We also considered factors related to user engagement, including the length of user feedback to AI, which was found to vary across different users as mentioned in 7.2.2, and the count of total RQ nodes created.Additionally, we included whether users performed any actions while waiting for results from an already triggered generation as a potential factor, as discussed in 7.2.1.Participants' familiarity with task topics was also considered as a predictor, measured through the familiarity ratings collected through the post-session surveys.The familiarity variable was constructed as an ordinal variable taking three possible integer values from 0 (not familiar) to 2 (very familiar).</p>
<p>Positive perceptions of generated RQs associated with longer feedback to AI.As shown by the results in Table 1, a positive association has been identified between all four dimensions of user-perceived RQ ratings and the average lengths of users' input feedback lengths.The finding suggests that increased user engagement in the human-AI co-creation process through providing textual feedback might improve the perceived outcome quality.</p>
<p>In addition, we found that the RQ ratings are positively associated with whether a user used the system under the depth-first condition.Namely, users perceived the RQs generated by AI under the depth-first condition to be of better quality than the breadth-first condition.This also corresponds to our findings in 7.1.2.From Table 1, it can also be seen that participants' feedback length is positively associated with perceived cognitive load.The results indicate that, although providing longer feedback required users to invest more thoughts, it also improved the co-creation outcomes.The finding also aligns with our earlier qualitative results in 7.1.The interview result further reveals that providing feedback promoted their reflection on themselves and trying to understand how AI works, although tiring, simultaneously helped them improve and manage their own creative thinking processes.More specially, some participants were observed during think-aloud performing "reverse engineering" to interpret how AI works and even try to replicate the generation process in a new thread.</p>
<p>Factors associated with users' perception of co-creation experience.Table 2 shows the regression results with participants' survey rating responses as dependent variables and user-specific factors as predictors.We found a statistically significant positive association between whether a participant has utilized the RQ generation wait time to perform other activities and the participant's perceived control for the system.This indicates that users tend to perceive higher control of the system when better utilizing the generation wait time beyond merely waiting for the generated results.There was also a negative association between the total number of nodes generated by participants and their perceived control, which indicates that as users generate more RQs during each session, they perceive to have weaker control of the system.</p>
<p>Participants' familiarity with the task topics was also found to be negatively associated with their perceived control.This was also reflected by participants during their think-aloud process, as users who were more familiar with the task topic had stronger expectations for the generated RQs: "... in educational research, we don't say crowdsourcing anymore.We say learner sourcing ... " (P4).We also noted that more experienced researchers would explicitly specify their needs for the agent to generate RQs in directions using terms related to research methodology.For example, one feedback P17 wrote to AI was "What the metrics for effectiveness and engagements."Several participants reflected that they were having a rough initial thought in their mind when being assigned a topic that is familiar to them, and what AI does is only help them externalize the thoughts.P4, for example, found several generated RQs to be "surprising in a negative manner" as he would evaluate whether an RQ "is something interesting, something novel, and ... relates to some of the prior literature." Summary (RQ3): Factors related to participants' system usage were found to be associated with users' perceptions of co-creation experiences and outcomes.The depth-first condition was also associated with better-perceived outcome quality in AI-generated RQs than the breadth-first condition.Users' familiarity with task topics, interestingly, led to a reduced sense of control, indicating users with different levels of expertise may have specific expectations unmet by the AI system.Additionally, we found that users providing longer feedback to AI, although introducing a larger cognitive load, led to outcomes with better quality.</p>
<p>DISCUSSION</p>
<p>In this section, we explore how the insights from our user study align with established theories and previous research.Furthermore, we offer design implications for future co-creation systems using Large Language Models.</p>
<p>Enriching Our Proposed Mental Model for</p>
<p>Human-AI Co-Creation of Novel Research Questions</p>
<p>Existing studies [18,55] have mostly been focused on the research lifecycle as a whole when discussing the models of the research process.Our user study findings shed light on additional factors to consider when designing future human-AI co-creation systems for research question generation, as shown in Figure 10.</p>
<p>8.1.1Wait Time in AI-based Co-Creation System as an Opportunity to Promote Creativity.Our observation of users' behavior patterns when using the CoQuest system sheds light on the possibility of utilizing the AI system's processing wait time as a design opportunity for users' exploration in the context of human-AI co-creation systems.The RQ3 findings revealed that when users utilized wait time to perform other activities, it improved their perceived control of the co-creation experience.This is contradictory to the common belief that the response delay in AI-based systems only leads to a negative impact on users' experience.In our RQ2 findings, two participants (P10 and P16) mentioned that AI Thoughts panel improved their trust for the system.However, this effect may not have been significant in the regression results of Table 2 due to the aggregation of various actions under the "acted during wait" category.We recognize that the impact of the AI thoughts panel on trust might have been diluted when combined with other actions in the aggregated data, thus statistically insignificant.Future work should separate these actions to measure their individual impacts on trust more precisely.This will allow a clearer understanding of how specific features, such as the AI thoughts panel, contribute to enhancing user trust.Our RQ2 results also showed that users utilized the time waiting to jump across different threads of RQs and perform exploration of ideas in parallel, or to articulate more detailed feedback for AI.The utilization of wait time was found to be prominent especially under the depth-first condition, which could be one of the reasons leading to its higher perceived co-creation outcome quality.</p>
<p>Most recent studies on LLM optimization focus on reducing the inference time and speeding the generation wait time needed for general purposes [12,37,43].Our findings, however, provided a unique perspective that in the context of LLM-based co-creation systems, the generation wait time can be exploited, or even purposely introduced, to promote users' creative activities.This can be achieved using different design techniques, such as tree-based visualization that highlights concurrency, or incorporating interactive nudges that guide users towards reflection, ideation, or brainstorming during these pauses.Such techniques can not only enhance user engagement but also maximize the cognitive benefits derived from the wait time breaks, potentially leading to more innovative and diverse co-creation outcomes.</p>
<p>8.1.2Aligning Users' Perception towards Experience and Outcomes of Generated RQs for Improved Creativity.In addition to prior studies' understanding of mix-initiative system design [62,78], our study provides empirical findings to support the need to consider the degree of initiative taken by AI as a design option in further human-AI co-creation system design.Our RQ1 findings indicate that the degree of initiative taken by AI during co-creation could affect users' perception of both co-creation experience and outcomes: If AI takes less initiative and gives users more freedom to choose from various generation outputs, it improves the users' co-creation experience.Contrarily, if AI drives deeper thoughts by taking more initiative, it leads to co-creation outcomes with higher quality and creativity.Based on our study results, this can be implied through lower user-given ratings (e.g., creativity and trust) towards their experience during the co-creation process.The findings lead us to believe that there exists a balance between user agency and AI initiative that can be aligned to better support co-creation.An ideal design would likely involve a dynamic adjustment of AI's role based on the user's expertise, background, and desire for control, allowing for both an engaging co-creation experience and innovative outcomes.Future designs should prioritize user feedback and adaptability, ensuring that the AI system can recognize and respond to user needs and preferences throughout the co-creation process.</p>
<p>Customizing Co-Creation based on</p>
<p>Researcher's Persona.The findings of RQ3 revealed that the background of researchers, including factors such as domain knowledge and research experience, were found to influence their interaction with our system and their evaluation of generated RQs.Additionally, a user's preference for the diversity and specificity of the model's outputs can vary depending on their research stage.For instance, during earlier stages of research, users tend to prefer the generated RQs to be more explorative and cover a broader perspective, while they might value more relevant and specific outputs during later stages of research where the research topic or idea has been scoped down to a certain degree.Although prior research has described the information-seeking behavior of researchers as a non-linear and dynamic process [18], our observations suggest that users' expectations of the system diverge based on their individual backgrounds and the progression of their research.The newly identified factors highlight the significance of personalization and adaptability to users' evolving needs in cocreation systems, particularly in the context of scholarly research.Recent research in aligning user persona with LLMs [24,63] has provided viable means for future designs of personalized research co-creation systems.It's crucial that later designs emphasize adaptability to ensure that system outcomes align with the individual researcher's background, progress, and specific research goals.Besides harnessing the text generation capability of LLMs, our study also highlighted the importance of utilizing the Chain-of-Thoughts prompting ability to not only improve LLMs' task-specific performance [76], but also as a way to enhance the explainability of AI-based systems and bridge the gap for users to understand the rationale of AI.Prior research has explored using mind-map-like design [30] to support users' creative ideation process.Our CoQuest system explored another viable design of using mind-map-styled visualization to facilitate a natural communication of LLMs' chain of thoughts towards humans in addition to the modality of text, most importantly through the AI Thoughts feature.It was also noted that some participants encountered difficulty interpreting AI rationales even after reading the explanation provided through AI Thoughts, as they could not ask for further explanations as in common chat-based interfaces.We argue for future designs of AI-based co-creation systems to provide explanations of AI rationales interactively through graphical designs, such as dynamic exploration features where users can prompt for further explanations or ask questions directly within a mind-map-like interface.</p>
<p>Sharing of Expertise: Steering the Direction of Outputs by</p>
<p>Injecting Meta-Research Knowledge.The CoQuest can not only be used for co-creation, but also for educational purposes that transfer knowledge about meta-research practices among researchers of different levels of experience.During our user study, we observed that while some researchers focused on the novel elements AI provided in newly generated RQs in a brainstorming manner, researchers with more experience tended to explicitly indicate their needs for AI to generate results from a more "technical" perspective, such as providing ideas related to evaluation metrics or surveying existing works regarding certain research methodology.Although users often have different specific topics of interest during RQ cocreation, the higher-level research thinking and skills, as discussed in existing meta-research works [25,67,77], can be beneficial in general when shared across users, especially for novice researchers or researchers new to certain fields.Past research has explored designs to facilitate the cultivation of new researchers' research skills, such as storytelling [64].New understandings unveiled by this study about researchers' behavior using LLM-enabled co-creation to provide novel implications that can pave the way for a more collaborative and educative approach in future system designs.Experienced researchers' interaction with the co-creation system can provide pathways from which less experienced researchers can learn and benefit.Integrating this understanding, we envision a design where the AI serves not just as a tool for co-creation, but also as a mediator in the knowledge transfer process between researchers.This integration can potentially bridge the gap between research idea formulation across domains by utilizing user-sourced expertise of meta-research.</p>
<p>Utilizing Personalized Design to Harness "Surprising" Outputs.</p>
<p>Hallucination is a well-recognized challenge in many task-oriented LLM system designs [38,47,66].Previous HCI studies suggest that while users might sometimes view unexpected outputs favorably [15], they can also find them unhelpful at times [36].Our findings echo these observations.While certain users viewed AI's topic driftoff unfavorably, others appreciated the fresh content introduced by the AI.This points to a subjective user preference.Additionally, our findings in RQ3 reflected that users' background influences their expectations for outputs generated by AI.Thus, we argue that in the future design of human-AI co-creation systems, the decision to allow models to introduce potentially out-of-context content (often labeled as "hallucination") should be seen as a design choice rather than a blanket problem.Furthermore, user backgrounds, such as domain familiarity, should be taken into account as they could influence the optimal design options.Future designs might consider how to detect users' different intents via their feedback.One possible approach could be to utilize implicit preference probing, where the system actively gauges the user's reaction to certain outputs and adjusts its responses accordingly.For instance, if a user consistently displays positive engagement with unexpected outputs, the AI could be more inclined to provide similarly "out-of-context" suggestions in future responses.Similar designs can also be applied to identify which stage of the research or creative process a user is in, so that the AI can tailor its responses.When asked about their concerns over the CoQuest system, several participants raised their concerns about the originality of ideas generated by the LLM-based backend.The possibility that an LLM might inadvertently replicate existing content without proper attribution, even when they are not directly copying content from the training corpus, poses a significant challenge to the integrity of research and creative processes [19].This is particularly relevant in academic contexts where the originality of ideas and proper citation are at core [53].Additionally, LLMs are known to suffer from the issue of hallucination [60,89].This characteristic of LLMs can mislead users, especially those who might be new to a research domain.Reliance on hallucinated content could lead to erroneous conclusions or decisions especially during the earlier stages of a research lifecycle.To mitigate these risks from the perspective of designing LLM-based co-creation systems for scientific research, it is essential to verify the credibility of LLM-generated content and apply methods to ensure proper attribution of scientific ideas from other researchers.This may be achieved through methods like fact-checking [46], grounding LLM-generated content within credible sources [88], and developing understanding among users to recognize potential flaws in LLM-generated content.Addressing these ethical concerns is crucial for maintaining the integrity and reliability of LLM-based human-AI co-creation, particularly in fields where intellectual property rights are highly valued.8.3.2User biases and blind spots.In our study, we found that users' expertise influenced their sense of control when co-creating with AI.Specifically, users felt less control and lower cognitive demand when working on familiar topics.Interviews with participants revealed that they had taken cognitive shortcuts by having intrinsic expectations about the generated outcomes.Their sense of control also decreased when these expectations were not met, indicating the presence of confirmation biases.This aligns with the growing research on understanding cognitive biases in human-AI interactions [4,5,8,72].Our research further highlights the impact of confirmation biases on users' perceived control in human-AI cocreation tasks.Moreover, our research suggests that these biases can operate through the lens of human expertise, potentially creating blind spots.Future research should explore diverse strategies for mitigating bias in co-creation with AI, such as drawing from crowd-sourced ideas, as demonstrated in Yen et al. [86]'s work.8.3.3Over-reliance on AI.Another ethical concern with our system is users blindly accepting AI-generated outcomes or relying too heavily on them, as pointed out by Buçinca et al. [6].To address this, we implemented two effective approaches in our design.First, we introduced an RQ rating during the generation process to encourage users to evaluate AI-generated content.Second, we incorporated AI thoughts to assist users in understanding the literature space and the generation process.Users frequently utilized AI thoughts during wait times, which enhanced their perceived control in co-creation.These two strategies, utilizing metrics to promote human active evaluation of AI-generated content and providing explanations to enhance human understanding of the AI-generation process, offer valuable insights for those seeking to mitigate bias in human-AI collaboration.Our work builds upon prior research [70] by providing options for users to check for explanations in order to reduce AI over-reliance.</p>
<p>Ethical Concerns and Potential Biases</p>
<p>Nonetheless, our study did not provide empirical understanding of the longer-term impact of using an LLM-based RQ co-creation system.Prior research [44] has pointed out that certain designs in current AI systems could hinder human creativity development in the long term.We argue that further studies should be conducted to understand both the positive and negative impacts of human-AI co-creation systems for research ideation longitudinally over human researchers regarding aspects such as creativity, research preferences, and behaviors during ideation.Future research should also explore more ways of explaining AI outputs and designs to promote active human thinking and advance the understanding of the role of explanations in research and learning.</p>
<p>LIMITATIONS AND FUTURE WORK</p>
<p>We acknowledge that our study has several limitations.First, our system currently relies on a fixed and relatively limited set of publications as its source pool.This design limitation often results in users finding themselves constrained to a narrow segment of literature after a few iterations of RQ generation.Ideally, the system should be integrated with online publications databases to harness a broader and continuously updated spectrum of publications.Trust in the system also emerged as a concern.Some users hesitated to utilize the generated RQs directly with concern about their originality and fearing potential overlaps with pre-existing research.Addressing these concerns is crucial for enhancing user confidence and the overall effectiveness of the system.Furthermore, the time constraints imposed on the tasks might not have been optimal for all participants.Those less familiar with the task might require more time before they get acquainted with the research space and can effectively generate RQs.This "warm-up" period could be considered more thoughtfully in future studies.Additionally, we acknowledge the limitation in our analysis related to user behavior beyond wait times.Our system did not precisely record the completion times for each RQ generated.Due to the extensive duration and the complexity of behaviors in these periods, these activities were not systematically coded and analyzed, which may have omitted valuable insights into user interaction with the system.Moreover, our evaluation focused solely on doctoral students who, while having some research background, are still in the early stages of their research careers.The results might differ when evaluating seasoned researchers or even undergraduates.Expanding the participant pool in future studies can offer a more comprehensive understanding of the system's effectiveness and user experience across varying expertise levels.Future work should also study the longitudinal effect of the CoQuest system usage over human researchers.</p>
<p>CONCLUSION</p>
<p>In this study, we introduced an agent LLM system, called CoQuest, aiming to support the creation of research questions.Through a formative study with actual researchers, we proposed a mental model combining the process of literature discovery and research ideation and applied it to the design of the CoQuest system.We introduced two interaction design options for CoQuest: breadthfirst and depth-first generations, diversifying the degree of AI's initiative during co-creation.A within-subjective study with 20 participants revealed that a higher degree of AI initiative led to cocreation outcomes with enhanced creativity, albeit at the expense of the overall co-creation experience.We also found that users who effectively utilized wait time experienced higher-quality outcomes and developed a stronger sense of control.§ ¤ " rq3 ": "{ ACTUAL_RQ }" }} Ensure the response can be parsed by Python json .loads .Each response should contain 3 research questions ( RQs ) proposed based on the current inputin the JSON format specified above , by replacing the ACTUAL_RQ with your RQ .The key RQs should also be on the top level of the json object .You should always generate valid and non -empty RQs , even if the input is not clear enough .Always be constructive , specific , and creative .</p>
<p>¦ ¥</p>
<p>A.3.2 Prompts used for each action.We design each action based on the mental model as an individual task to be completed by the agent.</p>
<p>For each action, a separate function is invoked to trigger a new turn of LLM inference.</p>
<p>Search and summarize papers: § ¤ Summarize the input literatures into 5 bullet points .Always explain each point in detail and assume the user has no background knowledge .Your reply should strictly be in the following format in one line :</p>
<p>Here is a summary of some existing works : 1. ... 2. ... 3. ...</p>
<p>¦ ¥</p>
<p>Hypothesize use cases: § ¤ You are a helpful AI that can hypothesize use cases for users .I will provide you with some context , and you should generate three use cases based on the context .Your reply should strictly be in the following format in one line :</p>
<p>Here are some potential use cases based on the current RQ : Use case 1: ... Use case 2: ... Use case 3: ...</p>
<p>¦ ¥</p>
<p>Narrow down RQs: § ¤ You are a helpful AI that can narrow down RQs for users .I will provide you with some context , and you should reflect and generate a list of bullet points that narrows down the context .The reply should be a list of bullet points , each bullet point should be a sentence :</p>
<p>To narrow down the RQ , we should consider the following : -... -... -...</p>
<p>¦ ¥</p>
<p>Figure 2 :
2
Figure 2: Screenshots of content created by participants using Miro during the focus group study.</p>
<p>Figure 3 :
3
Figure 3: Participants' mental model of co-creating RQs with an LLM-based AI agent is delineated as follows.An "action" labeled with an AI icon denotes that participants perceived that AI could significantly reduce the "labor"; a human icon means that participants were expected to evaluate AI-generated RQs and provide feedback to drive the iterative process.However, it is hard to determine how human and AI share the task of "refine and re-scope," as it depends on individual expertise and the clarity of the intended research focus.</p>
<p>Figure 4 :
4
Figure 4: The RQ Flow Editor panel in the CoQuest system features two distinct designs for generating research questions (RQs): the breadth-first and depth-first approaches.The breadth-first generation approach is designed to trigger the creation of multiple RQs in a single iteration, facilitating a wide exploration of potential research areas.In contrast, the depth-first generation focuses on triggering more iterations of RQ refinement, allowing the AI to delve deeper into a specific topic for a more focused and detailed exploration.</p>
<p>Figure 5 :
5
Figure 5: Illustration of the CoQuest framework; The mental model of HCI researchers is used to build the LLM-based agent capable of accessing and querying a literature collection to generate research questions (RQs).This agent not only presents the generated RQs to the users but also provides the rationales behind their generation and literature grounding through the frontend interface.Examples of prompts used to build the agent can are available in Appendix A.3.</p>
<p>Figure 6 :
6
Figure6: Participants rated their perceptions of the system's two designs using a 5-point Likert scale survey.The survey ratings indicated that participants experienced a significantly higher sense of creativity and trust when engaging with the system under the breadth-first condition.</p>
<p>Figure 7 :
7
Figure 7: Part of P4's RQ flow using the breadth-first condition when exploring the topic of "AI and crowdsourcing."Note that the participant generated from two different RQ nodes in the same iteration, and only one set of generated RQs was presented.The participant provided feedback using keywords such as "AI and crowdsourcing" and "educational setting" to help the AI generate more RQs.The third iteration was not included in this figure.</p>
<p>Figure 9 :
9
Figure9: Part of P4's RQ flow involved using the depth-first condition when exploring the topic of "AR/VR for education and learning."The dashed lines represent the second and third iterations of RQ generation, which were not based on the user's feedback.The participant provided feedback to the RQs generated at depth=1 and depth=2.Based on the RQ at depth=2, three more RQs were generated.P4's perceived novelty, surprise, value, and relevance initially scored at(3,4,3,5) during the first iteration (at depth=1), and these scores increased to(4,5,4,5) as the depth increased.</p>
<p>Figure 10 :
10
Figure 10: Updating the mental model of human-AI RQ co-creation with additional factors identified through the experimental study.</p>
<ol>
<li>3 . 1
31
Ethical Implications of LLM Usage: Plagiarism and Hallucination.The use of LLMs in research idea co-creation processes can lead to potential ethical concerns, e.g., risks of plagiarism and hallucination.LLMs generate content based on their vast training data, which raises concerns about the originality of their outputs.</li>
</ol>
<p>Table 1 :
1
Regression Results with RQ ratings as dependent variables and users' behavior data as predictors.Each column in the table represents one regression performed with the corresponding rating item as the dependent variable.
Dependent Variables -Outcomes (RQ Ratings)</p>
<p>Table 2 :
2
Regression Results with survey ratings as dependent variables and users' behavior data as predictors.
Dependent Variables -Experience (Survey Scores)PredictorsControlCreativityMeta CreativityCognitive LoadTrust𝛽 (S.E.)𝛽 (S.E.)𝛽 (S.E.)𝛽 (S.E.)𝛽 (S.E.)Cond. (Depth-First=1).34 (.71)-.41 (.46)-.74 (.23)<strong>.82 (.36)<em>-.58 (.88)Feedback Length-.03 (.04).02 (.05)-.01 (.03).07 (.01)</em></strong>.03 (.04)Total # of RQs Created-.07 (.03)<em>-.01 (.01).03 (.01)</em>-.01 (.01).02 (.05)Acted During Wait.10 (.04)<strong>-.06 (.07)-.03 (.04).06 (.05)-.03 (.04)Familiarity-.22 (.08)</strong>.04 (.09)-.09 (.05)-.08 (.04)<em>.07 (.08)
</em>: p&lt;0.05, <strong>: p&lt;0.01, </strong>*: p&lt;0.001</p>
<p>https://www.semanticscholar.org/product/api
https://github.com/wbkd/react-flow/
https://github.com/tiangolo/fastapi/
https://github.com/Significant-Gravitas/Auto-GPT/
https://github.com/yiren-liu/coquest
The collected survey ratings have an average Cronbach's Alpha value of 𝛼 = .85, suggesting good reliability.
Power analysis conducted using G*Power[16].
To examine the potential issue of multicollinearity, we calculated the Variance Inflation Factor (VIF) for each predictor. All predictors yielded VIFs lower than the common threshold of 5[26], indicating no substantial collinearity problem.
ACKNOWLEDGMENTSThis material is based upon work supported by the National Science Foundation under Grant No. 2119589.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.Additionally, results presented in this paper were obtained using CloudBank[50], which is supported by the National Science Foundation under award No. 1925001.A APPENDICES A.1 Post-Session Survey QuestionsControlHow much control did you feel you had when using the system?Did the system allow you to make choices and decisions while creating RQs? Creativity How would you rate the creativity of the research questions generated by the system?How creative did you feel about yourself when using the system to create RQs?Meta-Creativity Did the system inspire new ways of thinking or approaching RQ creation for you?Did the generated RQs make you think or reflect about the topic in a new way?Cognitive Load How mentally demanding was the task using the system?Did you feel overloaded with information or options while using the system?Trust How confident were you in the RQs generated by the system?Would you trust the system's generated RQs to be used in a real research scenario?A.2 A Mixed-Effect Model of User Feedback LengthWe observe that users tend to provide human feedback to the system in distinct ways.In terms of linguistic styles, we investigated the difference among users' input lengths (word counts) by fitting a mixed-effects model by considering both the random effect from different users (i.e.,  0 ), and the potential fixed effect from the two conditions (i.e.,  1 ):Where    is the length of text feedback for the  ℎ observation of the  ℎ participant;  0 is the intercept;  1 is the effect of the two conditions (i.e., depth-first and breadth-first); condition is a categorical variable (binary) indicating the user's condition;  0 is the random effect for the  ℎ user;    is the residual error for the  ℎ observation of the  ℎ user.The results of the mixed-effect model are shown in Table4.CoefA.3 Examples of LLM-based Agent Prompts and ResponsesThe CoQuest system is implemented based on the similar prompting method from AutoGPT.At each step of inference, the model generates the response with the next action to take.The action is then executed by the system, with the returned response appended to the input for the next step of inference.The detailed prompts used in the system are as follows.A.3.1 System prompt.The system prompt is designed to indicate the overall tasks and constraints for the LLM agent.System prompts: § ¤You are research -GPT , an AI agent designed to automate the creation process of research questions / ideas , literature survey , and brainstorming .Your decisions must always be made independently without seeking user assistance .Play to your strengths as an LLM and pursue simple strategies with no legal complications .GOALS :1. Survey relevant past research papers / works 2. Summarize these works into novel findings and insights 3. Come up with novel research questions , and also generate possible expected results for each research question 4. Summarize the novelty and similarity between your proposed new research questions , and past research 5. Further survey literatures , and refine the research questions Constraints : 1.No user assistance 2. Exclusively use the commands listed in double quotes e.g." command name " 3. Use subprocesses for commands that will not terminate within a few minutes 4. First collect paper information , and then generate RQs .Do not create Agents to collect information , only rely on the query command .Commands :1. Summarize Existing Papers : " search_and_summarize_papers ", args : " query ": "&lt; text &gt;" 2. Hypothesizing Use Cases : " hypothesize_use_cases ", args : " context ": "&lt; text &gt;" 3. Narrow down RQs : " narrow_down_rqs ", args : " context ": "&lt; text &gt;" 4. Comparing existing RQ with existing papers : " compare_rq_with_papers ", args : " past_research_summary ": "&lt; text &gt;" , " rqs ": "&lt; text &gt;"Performance Evaluation :1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities .2. Constructively self -criticize your big -picture behavior constantly .3. Reflect on past decisions and strategies to refine your approach .4. Every command has a cost , so be smart and efficient .Aim to complete tasks in the least number of steps .You should only respond in JSON format as described below .Response Format : { " thoughts ": { " text ": " thought ", " reasoning ": " reasoning ", " plan ": "-short bulleted \n -list that conveys \n -long -term plan ", " criticism ": " constructive self -criticism ", " speak ": " thoughts summary to say to user "\ n }, " command ": { " name ": " command name ", " args ": { " arg name ": " value " } }, " RQs ": { " rq1 ": "{ ACTUAL_RQ }" , " rq2 ": "{ ACTUAL_RQ }" ,¦ ¥Compare RQ with papers: § ¤ You are a helpful AI that can compare RQs with existing papers for users .I will provide you with some context , and you compare the RQs with existing papers , and provide a summary of the findings .The reply should be a list of bullet points , each bullet point should be a sentence :Here are some findings from comparing the RQs with existing papers : -... -... -...¦ ¥A.3.3 Triggering prompt.The triggering prompt is appended to end at each inference, to further control the generation.Triggering prompt: § ¤ Determine which next command to use , and respond using the format specified above .You should always revise your old RQs into new RQs , based on the previous context and user input .Be specific and creative .Always go deeper on the high level RQ , do not repeat RQs that are already in history .Remember , always generate RQs in the format specified above by replacing the ACTUAL_RQ with real RQs .¦ ¥
A conceptual model for scholarly research activity. Agiatis Benardou, Panos Constantopoulos, Costis Dallas, Dimitris Gavrilis, 2010. 2010</p>
<p>Accelerating Innovation With Generative AI: AI-Augmented Digital Prototyping and Innovation Methods. Volker Bilgram, Felix Laarmann, IEEE Engineering Management Review. 512023. 2023</p>
<p>The creative mind: Myths and mechanisms. Margaret A Boden, 2004Psychology Press</p>
<p>Bias-Aware Systems: Exploring Indicators for the Occurrences of Cognitive Biases when Facing Different Opinions. Nattapat Boonprakong, Xiuge Chen, Catherine Davey, Benjamin Tag, Tilman Dingler, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Nattapat Boonprakong, Gaole He, Ujwal Gadiraju, Niels Van Berkel, Danding Wang, Si Chen, Jiqun Liu, Benjamin Tag, Jorge Goncalves, Tilman Dingler, Workshop on Understanding and Mitigating Cognitive Biases in Human-AI Collaboration. 2023. 2023</p>
<p>To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making. Zana Buçinca, Maja Barbara Malaya, Krzysztof Z Gajos, Proceedings of the ACM on Human-Computer Interaction. 52021. 2021</p>
<p>The use of MMR, diversity-based reranking for reordering documents and producing summaries. Jaime Carbonell, Jade Goldstein, Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. the 21st annual international ACM SIGIR conference on Research and development in information retrieval1998</p>
<p>Mirror, Mirror, on the Wall" -Promoting Self-Regulated Learning Using Affective States Recognition via Facial Movements. Si Chen, Yixin Liu, Risheng Lu, Yuqian Zhou, Yi-Chieh Lee, Yun Huang, 10.1145/3532106.3533500Proceedings of the 2022 ACM Designing Interactive Systems Conference (Virtual Event, Australia) (DIS '22). the 2022 ACM Designing Interactive Systems Conference (Virtual Event, Australia) (DIS '22)New York, NY, USAAssociation for Computing Machinery2022</p>
<p>How to prompt? Opportunities and challenges of zero-and few-shot learning for human-AI interaction in creative applications of generative models. Hai Dang, Lukas Mecke, Florian Lehmann, Sven Goller, Daniel Buschek, arXiv:2209.013902022. 2022arXiv preprint</p>
<p>An enactive model of creativity for computational collaboration and co-creation. Nicholas Davis, Chih-Pin Hsiao, Yanna Popova, Brian Magerko, Creativity in the digital age. 2015. 2015</p>
<p>The case for 4-bit precision: k-bit inference scaling laws. Tim Dettmers, Luke Zettlemoyer, International Conference on Machine Learning. PMLR2023</p>
<p>Renee Elio, Jim Hoover, Ioanis Nikolaidis, About computing science research methodology. Mohammad Salavatipour, Lorna Stewart, Ken Wong, 2011</p>
<p>Art and the science of generative AI. Ziv Epstein, Aaron Hertzmann, Laura Mariah Herman, Robert Mahari, Morgan R Frank, Matthew Groh, Hope Schroeder, Amy Smith, Memo Akten, Jessica Fjeld, Hany Farid, Neil Leach, Alex Pentland, Olga Russakovsky, Science. 3802023. 2023</p>
<p>When happy accidents spark creativity: Bringing collaborative speculation to life with generative AI. Ziv Epstein, Hope Schroeder, Dava Newman, International Conference on Innovative Computing and Cloud Computing. 2022</p>
<p>G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Franz Faul, Edgar Erdfelder, Albert-Georg Lang, Axel Buchner, Behavior research methods. 392007. 2007</p>
<p>Rand Fishkin, We Analyzed Millions of ChatGPT User Sessions: Visits are Down 29% since May, Programming Assistance is 30% of Use. 2023</p>
<p>A nonlinear model of information-seeking behavior. Allen Foster, Journal of the American society for information science and technology. 552004. 2004</p>
<p>Claiming hidden memories as one's own: A review of inadvertent plagiarism. C Amanda, Meaghan C Gingerich, Sullivan, Journal of Cognitive Psychology. 252013. 2013</p>
<p>Sources of Research Ideas Among Productive Scholars. Implications for Administrators. William F Glueck, Lawrence R Jauch, The Journal of Higher Education. 461975. 1975</p>
<p>Topic-Based Exploration and Embedded Visualizations for Research Idea Generation. Hua Guo, David H Laidlaw, IEEE Transactions on Visualization and Computer Graphics. 262020. 2020</p>
<p>Friend, collaborator, student, manager: How design of an ai-driven game level editor affects creators. Matthew Guzdial, Nicholas Liao, Jonathan Chen, Shao-Yu Chen, Shukan Shah, Vishwa Shah, Joshua Reno, Gillian Smith, Mark O Riedl, Proceedings of the 2019 CHI conference on human factors in computing systems. the 2019 CHI conference on human factors in computing systems2019</p>
<p>Principles of mixed-initiative user interfaces. Eric Horvitz, Proceedings of the SIGCHI conference on Human Factors in Computing Systems. the SIGCHI conference on Human Factors in Computing Systems1999</p>
<p>Aligning Language Models to User Opinions. Eunjeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon, arXiv:2305.149292023. 2023arXiv preprint</p>
<p>P A John, Daniele Ioannidis, Debbie Drake Fanelli, Steven N Dunne, Goodman, Meta-research: evaluation and improvement of research methods and practices. 2015. 201513e1002264</p>
<p>An introduction to statistical learning. Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, 2013Springer112</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, Comput. Surveys. 552023. 2023</p>
<p>Developing a Research Ideas Creation System through Reusing Knowledge Bases for Ontology Construction. Delin Jing, Hongji Yang, Meiyu Shi, Wei Zhu, IEEE 39th Annual Computer Software and Applications Conference. 2015. 2015. 20153</p>
<p>Augmenting Scientific Creativity with Retrieval across Knowledge Domains. Sheshera Hyeonsu B Kang, Kevin Mysore, Haw-Shiuan Huang, Thorben Chang, Andrew Prein, Aniket Mccallum, Elsa A Kittur, Olivetti, ArXiv abs/2206.013282022. 2022</p>
<p>MetaMap: Supporting visual metaphor ideation through multidimensional example-based exploration. Youwen Kang, Zhida Sun, Sitong Wang, Zeyu Huang, Ziming Wu, Xiaojuan Ma, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing Systems2021</p>
<p>Role-based perceptions of computer participants in human-computer co-creativity. Anna Kantosalo, Anna Jordanous, 2021AISB</p>
<p>Open coding. University of Calgary. Shahedul Huq, Khandkar , 2009. 2009. 200923</p>
<p>The effect of AI-based inspiration on human design ideation. Jingoog Kim, Mary Lou Maher, International Journal of Design Creativity and Innovation. 112023. 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 352022. 2022</p>
<p>Evaluating Mixed-Initiative Creative Interfaces via Expressive Range Coverage Analysis. Max Kreminski, Isaac Karth, Michael Mateas, Noah Wardrip-Fruin, IUI Workshops. 2022</p>
<p>CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities. Mina Lee, Percy Liang, Qian Yang, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022. 2022</p>
<p>Fast inference from transformers via speculative decoding. Yaniv Leviathan, Matan Kalman, Yossi Matias, International Conference on Machine Learning. PMLR2023</p>
<p>HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jianyun Nie, Ji Rong, Wen , ArXiv abs/2305.117472023. 2023</p>
<p>Zhiyu Lin, Upol Ehsan, Rohan Agarwal, Samihan Dani, Vidushi Vashishth, Mark Riedl, arXiv:2305.07465Beyond Prompts: Exploring the Design Space of Mixed-Initiative Co-Creativity Systems. 2023. 2023arXiv preprint</p>
<p>Design guidelines for prompt engineering text-to-image generative models. Vivian Liu, Lydia B Chilton, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang, ArXiv abs/2308.03688AgentBench: Evaluating LLMs as Agents. 2023. 2023</p>
<p>Creative Research Question Generation for Human-Computer Interaction Research. Yiren Liu, Mengxia Yu, Meng-Long Jiang, Yun Huang, IUI Workshops. 2023</p>
<p>Deja vu: Contextual sparsity for efficient llms at inference time. Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, International Conference on Machine Learning. PMLR2023</p>
<p>Michele Loi, Eleonora Viganò, Lonneke Van Der Plas, arXiv:2007.11973The societal and ethical relevance of computational creativity. 2020. 2020arXiv preprint</p>
<p>Novice-AI music co-creation via AI-steering tools for deep generative models. Ryan Louie, Andy Coenen, Zhi Cheng, Michael Huang, Carrie J Terry, Cai, Proceedings of the 2020 CHI conference on human factors in computing systems. the 2020 CHI conference on human factors in computing systems2020</p>
<p>Selfcheckgpt: Zeroresource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark, Gales, arXiv:2303.088962023. 2023arXiv preprint</p>
<p>Sources of Hallucination by Large Language Models on Inference Tasks. Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, Mark Steedman, ArXiv abs/2305.145522023. 2023</p>
<p>Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals. Piotr Mirowski, Kory W Mathewson, Jaylen Pittman, Richard Evans, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Mixed initiative generative AI interfaces: An analytic framework for generative AI applications. Michael Muller, Justin D Weisz, Werner Geyer, Proceedings of the Workshop The Future of Co-Creative Systems-A Workshop on Human-Computer Co-Creativity of the 11th International Conference on Computational Creativity. the Workshop The Future of Co-Creative Systems-A Workshop on Human-Computer Co-Creativity of the 11th International Conference on Computational Creativity2020ICCC 2020</p>
<p>CloudBank: Managed Services to Simplify Cloud Access for Computer Science Research and Education. Michael Norman, Vince Kellen, Shava Smallen, Brian Demeulle, Shawn Strande, Ed Lazowska, Naomi Alterman, Rob Fatland, Sarah Stone, Amanda Tan, Practice and Experience in Advanced Research Computing. 2021</p>
<ol>
<li>I lead, you help but only with enough details: Understanding user experience of co-creation with artificial intelligence. Changhoon Oh, Jungwoo Song, Jinhan Choi, Seonghyeon Kim, Sungwoo Lee, Bongwon Suh, Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. the 2018 CHI Conference on Human Factors in Computing Systems</li>
</ol>
<p>Detecting llm-generated text in computing education: A comparative study for chatgpt cases. Oscar Michael Sheinman Orenstrakh, Carlos Karnalim, Anibal Suarez, Michael Liut, arXiv:2307.074112023. 2023arXiv preprint</p>
<p>Ought, Elicit: The AI Research Assistant. 2023</p>
<p>Scholarly information practices in the online environment. Carole L Palmer, Lauren C Teffeau, Carrie M Pirmann, 2009. 2009Report commissioned by OCLC Research. Published online at</p>
<p>Scholarly ontology: modelling scholarly practices. Vayianos Pertsas, Panos Constantopoulos, International Journal on Digital Libraries. 182017. 2017</p>
<p>On the use of counterbalanced designs in cognitive research: a suggestion for a better and more powerful analysis. Alexander Pollatsek, Arnold D Well, Journal of Experimental psychology: Learning, memory, and Cognition. 217851995. 1995</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023. 2023arXiv preprint</p>
<p>Are ChatGPT and large language models "the answer" to bringing us closer to systematic review automation?. Riaz Qureshi, Daniel Shaughnessy, A R Kayden, Karen A Gill, Tianjing Robinson, Eitan S Li, Agai, Systematic Reviews. 122023. 2023</p>
<p>A survey of hallucination in large foundation models. Amit Vipula Rawte, Amitava Sheth, Das, arXiv:2309.059222023. 2023arXiv preprint</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019. 2019arXiv preprint</p>
<p>Designing creative AI partners with COFI: A framework for modeling interaction in human-AI co-creative systems. Jeba Rezwana, Mary Lou Maher, ACM Transactions on Computer-Human Interaction. 2022. 2022</p>
<p>Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani, arXiv:2304.11406LaMP: When Large Language Models Meet Personalization. 2023. 2023arXiv preprint</p>
<p>Cultivating research skills through scholarly digital storytelling. Kelly Schrum, Sarah Bogdewiecz, Higher education research &amp; development. 412022. 2022</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023. 2023arXiv preprint</p>
<p>Large language models encode clinical knowledge. K Singhal, Shekoofeh Azizi, Tao Tu, Said Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather J Cole-Lewis, Stephen J Pfohl, P A Payne, Martin G Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, P A Mansfield, Blaise Aguera Y Arcas, Dale R Webster, Greg S Corrado, Y Matias, Katherine Hui, -Ling Chou, Juraj Gottweis, Nature. Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan6202022. 2022</p>
<p>Using meta-research to foster diverse, equitable, and inclusive collaborative research networks. Elizabeth R Stevens, Abraham A Brody, Fayron Epps, Danetta H Sloan, Scott E Sherman, Journal of the American Geriatrics Society. 712023. 2023</p>
<p>Investigating explainability of generative AI for code through scenario-based design. Jiao Sun, Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde, Kartik Talamadupula, Justin D Weisz, 27th International Conference on Intelligent User Interfaces. 2022</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Explanations can reduce overreliance on ai systems during decision-making. Helena Vasconcelos, Matthew Jörke, Madeleine Grunde-Mclaughlin, Tobias Gerstenberg, Ranjay Michael S Bernstein, Krishna, Proceedings of the ACM on Human-Computer Interaction. 72023. 2023</p>
<p>Information behaviour of scholars. Polona Vilar, Libellarium: Journal for the Research of Writing, Books, and Cultural Heritage Institutions. 72015. 2015</p>
<p>Designing theory-driven user-centric explainable AI. Danding Wang, Qian Yang, Ashraf Abdul, Brian Y Lim, Proceedings of the 2019 CHI conference on human factors in computing systems. the 2019 CHI conference on human factors in computing systems2019</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023. 2023arXiv preprint</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Yuqing Wang, Yun Zhao, arXiv:2308.05342Metacognitive Prompting Improves Understanding in Large Language Models. 2023. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Training early career researchers to use metaresearch to improve science: A participant-guided "learn by doing" approach. Tracey L Weissgerber, PLoS Biology. 19e30010732021. 2021</p>
<p>Toward general design principles for generative AI applications. Justin D Weisz, Michael Muller, Jessica He, Stephanie Houde, arXiv:2301.055782023. 2023arXiv preprint</p>
<p>The Right Variety: Improving Expressive Range Analysis with Metric Selection Methods. Oliver Withington, Laurissa Tokarchuk, Proceedings of the 18th International Conference on the Foundations of Digital Games. the 18th International Conference on the Foundations of Digital Games2023</p>
<p>Promptchainer: Chaining large language model prompts through visual programming. Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, Carrie J Cai, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie , Proceedings of the 2022 CHI conference on human factors in computing systems. the 2022 CHI conference on human factors in computing systemsJun Cai. 2022</p>
<p>Creative Computing: An Approach to Knowledge Combination for Creativity?. Hongji Yang, Delin Jing, Lu Zhang, IEEE Symposium on Service-Oriented System Engineering. 2016. 2016. 2016</p>
<p>Mixed-initiative co-creativity. Antonios Georgios N Yannakakis, Constantine Liapis, Alexopoulos, 2014. 2014</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, ArXiv abs/2210.036292022. 2022</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>CrowdIDEA: Blending Crowd Intelligence and Data Analytics to Empower Causal Reasoning. Chi-Hsien Yen, Haocong Cheng, Yilin Xia, Yun Huang, 10.1145/3544548.3581021Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing SystemsHamburg, Germany; New York, NY, USA, ArticleAssociation for Computing Machinery2023463CHI '23)</p>
<p>Wordcraft: story writing with large language models. Ann Yuan, Andy Coenen, Emily Reif, Daphne Ippolito, 27th International Conference on Intelligent User Interfaces. 2022</p>
<p>Automatic evaluation of attribution by large language models. Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, Huan Sun, arXiv:2305.063112023. 2023arXiv preprint</p>
<p>Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.01219Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. 2023. 2023arXiv preprint</p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, arXiv:2305.03514Can Large Language Models Transform Computational Social Science?. 2023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
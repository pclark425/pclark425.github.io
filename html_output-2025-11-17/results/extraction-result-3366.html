<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3366 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3366</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3366</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-4789aa4626a9fd571944672b364c6f52a6eeb9d2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4789aa4626a9fd571944672b364c6f52a6eeb9d2" target="_blank">Solving Puzzles Described in English by Automated Translation to Answer Set Programming and Learning How To Do That Translation</a></p>
                <p><strong>Paper Venue:</strong> AAAI Fall Symposium: Advances in Cognitive Systems</p>
                <p><strong>Paper TL;DR:</strong> A system capable of automatically solving combinatorial logic puzzles given in (simplified) English using a lambda-calculus based approach using Probabilistic Combinatorial Categorial Grammars (PCCG) where meanings of words are associated with parameters to be able to distinguish between multiple meanings of the same word.</p>
                <p><strong>Paper Abstract:</strong> We present a system capable of automatically solving combinatorial logic puzzles given in (simplified) English. It involves translating the English descriptions of the puzzles into answer set programming(ASP) and using ASP solvers to provide solutions of the puzzles. To translate the descriptions, we use a lambda-calculus based approach using Probabilistic Combinatorial Categorial Grammars (PCCG) where the meanings of words are associated with parameters to be able to distinguish between multiple meanings of the same word. Meaning of many words and the parameters are learned. The puzzles are represented in ASP using an ontology which is applicable to a large set of logic puzzles.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3366.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3366.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCCG+Inverse-λ→ASP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Combinatory Categorial Grammar with Inverse Lambda learning to translate English into Answer Set Programming (λ-ASP-Calculus) + Clingo solver</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned semantic-parsing pipeline that maps simplified English puzzle clues to ASP rules using a PCCG lexicalized parser with λ-ASP-calculus semantics learned via inverse-λ and parameter estimation, then solves the resulting ASP program with the Clingo solver to obtain puzzle solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PCCG + Inverse λ semantic parser (λ-ASP-calculus) + Clingo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline combining: (1) C&C syntactic parsing to obtain CCG parses; (2) a Probabilistic Combinatory Categorial Grammar (PCCG) that associates lexical items with λ-ASP-calculus semantic templates and probabilistic weights; (3) Inverse-λ operators (Inverse_R and Inverse_L) and generalization to learn unknown lexical semantics from (sentence, logical-form) pairs; (4) parameter estimation (per Zettlemoyer & Collins style) to assign weights to multiple lexical meanings; and (5) generation of an ASP program and solving with the Clingo answer-set solver (clasp/clingo).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Combinatorial logic puzzles (Zebra-style / association / timeline puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Logic/association puzzles involving multiple domains (names, ranks, animals, elements, times) where solutions require establishing bijective associations and constraints (ordering, equality/inequality, distance relations such as 'immediately before' or 'exactly two days after'). These puzzles require combinatorial and some ordinal (ordering) reasoning; spatial reasoning in the strict geometric sense (e.g., Sudoku grid geometry) is not the focus, but positional/ordering relations (rank/time indices) are represented and reasoned about.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Simplified English clue sentences (preprocessed to remove anaphora and some complex constructs) plus provided domain data encoded as ASP facts/tuples (etype/element tuples and tuple(I,X) groundings). Domain tuples (indices) are given as input rather than extracted from text.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Not applicable (no prompting). The system is a supervised semantic parser trained on pairs (sentence, desired ASP logical form) using inverse-λ generalization and parameter estimation (i.e., learning from annotated training clues rather than prompt-based inference).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>No dedicated analysis of spatial reasoning or internal representations is reported. Positional/ordering relations are encoded explicitly in ASP using numeric indices and constraints (e.g., X != Y-1 for 'immediately before', X > Y for 'before', X != Y+2 for 'exactly two days after'). The paper does not provide ablations, attention analyses, or diagnostics that examine how the learned parser internally represents ordering/spatial relations beyond showing learned λ-ASP templates for phrases like 'immediately' and 'more'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Clue translation (800 clues, 10-fold CV): Precision 87.64%, Recall 86.12%, F1 86.87%. Puzzle solving (50 puzzles): 10-fold accuracy 28/50 = 56%; with manually chosen training sets: 10-s: 22/40 (55%), 15-s: 24/35 (68.57%), 20-s: 25/30 (83.33%). Dataset: 50 puzzles drawn from puzzle magazines; about 800 clue sentences used for translation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Requires domain data (tuples) to be provided; works on 'simplified English' (preprocessing needed to remove anaphora and compound clues); rare or idiosyncratic clue patterns (e.g., 'exactly two days after') are poorly learned without representative training examples; many words get trivial 'identity' semantics causing overfitting and missed non-trivial meanings; syntactically incorrect translations are discarded, reducing puzzle-solving success; C&C parses can sometimes prevent inverse-λ inference or lead to highly complex higher-order λ expressions; observed ceiling in puzzle accuracy (~83.3%) under experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Paper compares clue-translation performance to prior semantic-parsing domains (Geoquery and Robocup) and reports broadly comparable translation accuracy to those domains (database domain 88–92%, Robocup 75–82%). There is no comparison to modern neural language models, to human puzzle-solvers, nor to specialized spatial solvers like Sudoku-specific algorithms; the reasoning component is an ASP solver (Clingo) rather than a language model performing end-to-end puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Puzzles Described in English by Automated Translation to Answer Set Programming and Learning How To Do That Translation', 'publication_date_yy_mm': '2011-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using inverse \u03bb and generalization to translate english to formal languages <em>(Rating: 2)</em></li>
                <li>A statistical semantic parser that integrates syntax and semantics <em>(Rating: 2)</em></li>
                <li>Learning a compositional semantic parser using an existing syntactic parser <em>(Rating: 2)</em></li>
                <li>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars <em>(Rating: 2)</em></li>
                <li>Clasp : A conflict-driven answer set solver <em>(Rating: 1)</em></li>
                <li>Online learning of relaxed ccg grammars for parsing to logical form <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3366",
    "paper_id": "paper-4789aa4626a9fd571944672b364c6f52a6eeb9d2",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "PCCG+Inverse-λ→ASP",
            "name_full": "Probabilistic Combinatory Categorial Grammar with Inverse Lambda learning to translate English into Answer Set Programming (λ-ASP-Calculus) + Clingo solver",
            "brief_description": "A learned semantic-parsing pipeline that maps simplified English puzzle clues to ASP rules using a PCCG lexicalized parser with λ-ASP-calculus semantics learned via inverse-λ and parameter estimation, then solves the resulting ASP program with the Clingo solver to obtain puzzle solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PCCG + Inverse λ semantic parser (λ-ASP-calculus) + Clingo",
            "model_description": "A pipeline combining: (1) C&C syntactic parsing to obtain CCG parses; (2) a Probabilistic Combinatory Categorial Grammar (PCCG) that associates lexical items with λ-ASP-calculus semantic templates and probabilistic weights; (3) Inverse-λ operators (Inverse_R and Inverse_L) and generalization to learn unknown lexical semantics from (sentence, logical-form) pairs; (4) parameter estimation (per Zettlemoyer & Collins style) to assign weights to multiple lexical meanings; and (5) generation of an ASP program and solving with the Clingo answer-set solver (clasp/clingo).",
            "model_size": null,
            "puzzle_name": "Combinatorial logic puzzles (Zebra-style / association / timeline puzzles)",
            "puzzle_description": "Logic/association puzzles involving multiple domains (names, ranks, animals, elements, times) where solutions require establishing bijective associations and constraints (ordering, equality/inequality, distance relations such as 'immediately before' or 'exactly two days after'). These puzzles require combinatorial and some ordinal (ordering) reasoning; spatial reasoning in the strict geometric sense (e.g., Sudoku grid geometry) is not the focus, but positional/ordering relations (rank/time indices) are represented and reasoned about.",
            "input_representation": "Simplified English clue sentences (preprocessed to remove anaphora and some complex constructs) plus provided domain data encoded as ASP facts/tuples (etype/element tuples and tuple(I,X) groundings). Domain tuples (indices) are given as input rather than extracted from text.",
            "prompting_method": "Not applicable (no prompting). The system is a supervised semantic parser trained on pairs (sentence, desired ASP logical form) using inverse-λ generalization and parameter estimation (i.e., learning from annotated training clues rather than prompt-based inference).",
            "spatial_reasoning_analysis": "No dedicated analysis of spatial reasoning or internal representations is reported. Positional/ordering relations are encoded explicitly in ASP using numeric indices and constraints (e.g., X != Y-1 for 'immediately before', X &gt; Y for 'before', X != Y+2 for 'exactly two days after'). The paper does not provide ablations, attention analyses, or diagnostics that examine how the learned parser internally represents ordering/spatial relations beyond showing learned λ-ASP templates for phrases like 'immediately' and 'more'.",
            "performance_metrics": "Clue translation (800 clues, 10-fold CV): Precision 87.64%, Recall 86.12%, F1 86.87%. Puzzle solving (50 puzzles): 10-fold accuracy 28/50 = 56%; with manually chosen training sets: 10-s: 22/40 (55%), 15-s: 24/35 (68.57%), 20-s: 25/30 (83.33%). Dataset: 50 puzzles drawn from puzzle magazines; about 800 clue sentences used for translation evaluation.",
            "limitations_or_failure_modes": "Requires domain data (tuples) to be provided; works on 'simplified English' (preprocessing needed to remove anaphora and compound clues); rare or idiosyncratic clue patterns (e.g., 'exactly two days after') are poorly learned without representative training examples; many words get trivial 'identity' semantics causing overfitting and missed non-trivial meanings; syntactically incorrect translations are discarded, reducing puzzle-solving success; C&C parses can sometimes prevent inverse-λ inference or lead to highly complex higher-order λ expressions; observed ceiling in puzzle accuracy (~83.3%) under experiments.",
            "comparison_to_other_models_or_humans": "Paper compares clue-translation performance to prior semantic-parsing domains (Geoquery and Robocup) and reports broadly comparable translation accuracy to those domains (database domain 88–92%, Robocup 75–82%). There is no comparison to modern neural language models, to human puzzle-solvers, nor to specialized spatial solvers like Sudoku-specific algorithms; the reasoning component is an ASP solver (Clingo) rather than a language model performing end-to-end puzzle solving.",
            "uuid": "e3366.0",
            "source_info": {
                "paper_title": "Solving Puzzles Described in English by Automated Translation to Answer Set Programming and Learning How To Do That Translation",
                "publication_date_yy_mm": "2011-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using inverse \\u03bb and generalization to translate english to formal languages",
            "rating": 2
        },
        {
            "paper_title": "A statistical semantic parser that integrates syntax and semantics",
            "rating": 2
        },
        {
            "paper_title": "Learning a compositional semantic parser using an existing syntactic parser",
            "rating": 2
        },
        {
            "paper_title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
            "rating": 2
        },
        {
            "paper_title": "Clasp : A conflict-driven answer set solver",
            "rating": 1
        },
        {
            "paper_title": "Online learning of relaxed ccg grammars for parsing to logical form",
            "rating": 1
        }
    ],
    "cost": 0.0090005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Solving puzzles described in English by automated translation to answer set programming and learning how to do that translation</h1>
<p>Chitta Baral<br>School of Computing, Informatics and DSE<br>Arizona State University<br>chitta@asu.edu</p>
<h4>Abstract</h4>
<p>We present a system capable of automatically solving combinatorial logic puzzles given in (simplified) English. It involves translating the English descriptions of the puzzles into answer set programming(ASP) and using ASP solvers to provide solutions of the puzzles. To translate the descriptions, we use a $\lambda$-calculus based approach using Probabilistic Combinatorial Categorial Grammars (PCCG) where the meanings of words are associated with parameters to be able to distinguish between multiple meanings of the same word. Meaning of many words and the parameters are learned. The puzzles are represented in ASP using an ontology which is applicable to a large set of logic puzzles.</p>
<h2>Introduction and Motivation</h2>
<p>Consider building a system that can take as input an English description of combinatorial logic puzzles ${ }^{1}$ (puz 2007) and solve those puzzles. Such a system would need and somewhat demonstrate the ability to (a) process language, (b) capture the knowledge in the text and (c) reason and do problem solving by searching over a space of possible solutions. Now if we were to build this system using a larger system that learns how to process new words and phrases then the latter system would need and somewhat demonstrate the ability of (structural) learning. The significance of the second larger system is with respect to being able to learn language (new words and phrases) and not expecting that humans will a-priori provide an exhaustive vocabulary of all the words and their meanings.</p>
<p>In this paper we describe our development of such a system with some added assumptions. We present evaluation of our system in terms of how well it learns to understand clues (given in simplified ${ }^{2}$ English) of puzzles and how well it can solve new puzzles. Our approach of solving puzzles given in English involves translating the English description of the puzzles to sentences in answer set programming (ASP) (Baral 2003) and then using ASP solvers, such as (Gebser et</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Juraj Dzifcak</h2>
<p>School of Computing, Informatics and DSE
Arizona State University
juraj.dzifcak@asu.edu
al. 2007), to solve the puzzles. Thus a key step in this is to be able to translate English sentences to ASP rules. A second key step is to come up with an appropriate ontology of puzzle representation that makes it easy to do the translation.</p>
<p>With respect to the first key step, we use a methodology (Baral et al. 2011) that assigns $\lambda$-ASP-Calculus ${ }^{3}$ rules to each words. Since it seems to us that it is not humanly possible to manually create $\lambda$-ASP-Calculus rules for English words, we have developed a method, which we call, Inverse $\lambda$ to learn the meaning of English words in terms of their $\lambda$ -ASP-Calculus rule. The overall architecture of our system is given in Figure 1. Our translation (from English to ASP) system, given in the left hand side of Figure 1, uses a Probabilistic Combinatorial Categorial Grammars (PCCG) (Ge and Mooney 2005) and a lexicon consisting of words, their corresponding $\lambda$-ASP-Calculus rules and associated (quantitative) parameters to do the translation. Since a word may have multiple meaning implying that it may have multiple associated $\lambda$-ASP-Calculus rules, the associated parameters help us in using the "right" meaning in that the translation that has the highest associated probability is the one that is picked. Given a training set of sentences and their corresponding $\lambda$-ASP-Calculus rules, and an initial vocabulary (consisting of some words and their meaning), Inverse $\lambda$ and generalization is used to guess the meaning of words which are encountered but are not in the initial lexicon. Because of this guess and because of inherent ambiguity of words having multiple meanings, one ends up with a lexicon where words are associated with multiple $\lambda$-ASP-Calculus rules. A parameter learning method is used to assign weights to each meaning of a word in such a way that the probability that each sentence in the training set would be translated to the given corresponding $\lambda$-ASP-Calculus rule is maximized. The block diagram of this learning system is given in the right hand side of Figure 1.</p>
<p>With respect to the second key step, there are many ASP encodings, such as in (Baral 2003), of combinatorial logic puzzles. However, most methods given in the literature, assume that a human is reading the English description of the puzzle and is coming up with the ASP code or code in some</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall system architecture</p>
<p>high level language (Finkel, Marek, and Truszczynski 2002) that gets translated to ASP. In our case the translation of English description of the puzzles to ASP is to be done by an automated system and moreover this systems learns aspects of the translation by going over a training set. This means we need an ontology of how the puzzles are to be represented in ASP that is applicable to most (if not all) combinatorial logic puzzles.</p>
<p>The rest of the paper is organized as follows: We start by discussing the assumptions we made for our system. We then provide an overview of the ontology we used to represent the puzzles. We then give an overview of the natural language translation algorithm followed by a simple illustration on a small set of clues. Finally, we provide an evaluation of our approach with respect to translating clues as well as translating whole puzzles. We then conclude.</p>
<h3>Assumptions and Background Knowledge</h3>
<p>With our longer term goal to be able to solve combinatorial logic puzzles specified in English, as mentioned earlier, we made some simplifying assumptions for this current work. Here we assumed that the domains of puzzles are given (and one does not have to extract it from the puzzle description) and focused on accurately translating the clues. Even then English throws up many challenges and we did a human preprocessing<sup>4</sup> of puzzles to eliminate anaphoras and features that may lead to a sentence being translated into multiple clues. Besides translating the given English sentences we added some domain knowledge related to combinatorial logic puzzles. This is in line with the fact that often natural language understanding involves going beyond literal understanding of a given text and taking into context some background knowledge. The following example illustrates these points. A clue "Earl arrived immediately before the person with the Rooster." specifies several things. Outside of the fact that a man with the first name "Earl" came immediately before the man with the animal "Rooster", a human would also immediately conclude that "Earl" does not have a "Rooster". To correctly process this information one needs the general knowledge that if person A arrives before person B, A and B are different persons and given the assumption that all the objects are exclusive, an animal has a single owner. Also, to make sure that clue sentences correspond to single ASP rules, during preprocessing of this clue one may add "Earl is not the person with the Rooster."</p>
<h3>Puzzle representation and Ontology</h3>
<p>For our experiments, we focus on logic puzzles from (puz 2007; puz 2004; puz 2005). These logic puzzles have a set of basic domain data and a set of clues. To solve them, we adopt an approach where all the possible solutions are generated, and then constraints are added to reduce the number of solutions. In most cases there is a unique solution. A sample puzzle is given below, whose solution involves finding the correct associations between persons, their ranks, their animals and their lucky elements.</p>
<div class="codehilite"><pre><span></span><code>Puzzle Domain data:
1,2,3,4 and 5 are ranks
earl, ivana, lucy, philip and tony are names
earth, fire, metal, water and wood are elements
cow, dragon, horse, ox and rooster are animals
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Puzzle clues:
1) Tony was the third person to have his
fortune told.
2) The person with the Lucky Element Wood
had their fortune told fifth.
3) Earl’s lucky element is Fire.
4) Earl arrived immediately before the
person with the Rooster.
5) The person with the Dragon had their
fortune told fourth.
6) The person with the Ox had their
fortune told before the one
who’s Lucky Element is Metal.
7) Ivana’s Lucky Animal is the Horse.
</code></pre></div>

<p>8) The person with the Lucky Element Water has the Cow.
9) The person with Lucky Element Water did not have their fortune told first.
10) The person with Lucky Element Earth had their fortune told exactly two days after Philip.</p>
<p>The above puzzle can be encoded as follows.
\% DOMAIN DATA
index (1..4).
eindex (1..5).
etype (1, name).
element (1,earl). element (1,ivana).
element (1,lucy). element (1,philip).
element (1,tony).
etype (2, element).
element (2,earth). element (2, fire).
element (2, metal). element (2, water).
element (2, wood).
etype (3, animal).
element (3,cow). element (3,dragon).
element (3,horse). element (3,ox).
element (3,rooster).
etype (4, rank).
element (4,1). element (4,2). element (4,3).
element (4,4). element $(4,5)$.
\% CLUES and their translation
\%Tony was the third person to have
\%his fortune told.
:- tuple (I, tony), tuple (J, 3), I!=J.
\%The person with the Lucky Element
\%Wood had their fortune told fifth.
:- tuple (I, wood), tuple (J, 5), I!=J.
\%Earl's lucky element is Fire.
:- tuple (I, earl), tuple (J, fire), I!=J.
\%Earl arrived immediately before
\%the person with the Rooster.
:- tuple (I, earl), tuple (J, rooster),
tuple (I, X), tuple (J, Y),
etype (A, rank), element (A, X),
element (A, Y), X != Y-1.
\%The person with the Dragon had
\%their fortune told fourth.
:- tuple (I, dragon), tuple (J, 4), I!=J.
\%The person with the Ox had their
\% fortune told before the
\%one who's Lucky Element is Metal.
:- tuple (I, ox), tuple (J, metal),
tuple (I, X), tuple (J, Y),
etype (A, rank), element (A, X),
element (A, Y), X &gt; Y.
\%Ivana's Lucky Animal is the Horse.
:- tuple (I, ivana), tuple (J, horse), I!=J.
\%The person with the Lucky Element
\%Water has the Cow.
:- tuple (I, water), tuple (J, cow), I!=J.
\%The person with Lucky Element Water
\%did not have their fortune told first.
:- tuple (I, water), tuple (I, 1).
\%The person with Lucky Element Earth
\%had their fortune
\%told exactly two days after Philip.
:- tuple (I, earth), tuple (J, philip),
tuple (I, X), tuple (J, Y),
etype (A, rank), element (A, X),
element (A, Y), X != Y+2.</p>
<h2>The puzzle domain data</h2>
<p>Each puzzle comes with a set of basic domain data which forms tuples. An example of this data is given above. Note that this is not the format in which they are provided in the actual puzzles. It is assumed that the associations are exclusive, e.g. "earl" can own either a "dragon" or a "horse", but not both. We assume this data is provided as input. There are several reasons for this assumption. The major reason is that not all the data is given in the actual natural language text describing the puzzle. In addition, the text does not associate actual elements, such as "earth" with element types, such as "element". If the text contains the number " 6 ", we might assume it is a rank, which, in fact, it is not. These domain data is encoded using the following format, where etype $(A, t)$ stores the element type $t$, while element $(A, X)$ is the predicate storing all the elements $X$ of the type etype $(A$, type $)$. An example of an instance of this encoding is given below.
\% size of a tuple
index (1..n).
\% number of tuples
eindex (1..m).
\% type and lists of elements of that type,
\% one element from
\% each index forms a tuple
etype (1, type1).
element (1, e111). element (1, e112). ...
element (1, e11n).
. .
etype (m, typem).
element (m, em11). element (1, elm2). ...
element (1, e1mn).
We now discuss this encoding in more detail. We want to encode all the elements of a particular type, The type is needed in order to do direct comparisons between the elements of some type. For example, when we want to specify that "Earl arrived immediately before the person with the Rooster.", as encoded in the sample puzzle, we want to encode something like etype $(A$, rank $)$, element $(A, X)$, element $(A, Y), X!=$ $Y-1$., which compares the ranks of elements $X$ and $Y$. The reason all the element types and elements have fixed numerical indices is to keep the encoding similar across the board and to not have to define additional grounding for the variables. For example, if we encoded elements as</p>
<p>element(name, earl), then if we wanted to use the variable $A$ in the encodings of the clue, it would have to have defined domain which includes all the element types. These differ from puzzle to puzzle, and as such would have to be specifically added for each puzzle. By using the numerical indices across all puzzles, these are common across the board and we just need to specify that $A$ is an index. In addition, to avoid permutation within the tuples, the following facts are generated, where tuple $(I, X)$ is the predicate storing the elements $X$ within a tuple $I$ :
tuple(1,e11). ... tuple(1,e1n).
which for the particular puzzle yields
tuple(1, 1). tuple(2, 2). tuple(3, 3).
tuple(4, 4).tuple $(5,5)$.</p>
<h2>Generic modules and background knowledge</h2>
<p>Given the puzzle domain data, we combine their encodings with additional modules responsible for generation and generic knowledge. In this work, we assume there are two type of generic modules available. The first one is responsible for generating all the possible solutions to the puzzle. We assume these are then pruned by the actual clues, which impose constraints on these. The following rules are responsible for generation of all the possible tuples. Recall that we assume that all the elements are exclusive.</p>
<div class="codehilite"><pre><span></span><code><span class="m">1</span>|tuple(<span class="nv">I</span>,<span class="nv">X</span>) <span class="o">:</span>element(<span class="nv">A</span>,<span class="nv">X</span>)<span class="k">}</span><span class="m">1.</span>
<span class="p">:- </span>tuple(<span class="nv">I</span>,<span class="nv">X</span>), tuple(<span class="nv">J</span>,<span class="nv">X</span>),
element(<span class="nv">K</span>,<span class="nv">X</span>), <span class="nv">I</span> <span class="o">!=</span> <span class="nv">J</span>.
</code></pre></div>

<p>In addition, a module with rules defining generic/background knowledge is used so as to provide higher level knowledge which the clues define. For example, a clue might discuss maximum, minimum, or genders such as woman. To be able to match these with the puzzle data, a set of generic rules defining these concepts is used, rather than adding them into the actual puzzle data. Thus rules defining concepts and knowledge such as maximum, minimum, within range, sister is a woman and others are added. For example, the concept "maximum" is encoded as:</p>
<div class="codehilite"><pre><span></span><code><span class="nf">notmax</span><span class="p">(</span><span class="nv">A</span><span class="p">,</span> <span class="nv">X</span><span class="p">)</span> <span class="o">:-</span> <span class="nf">element</span><span class="p">(</span><span class="nv">A</span><span class="p">,</span> <span class="nv">X</span><span class="p">),</span>
    <span class="nf">element</span><span class="p">(</span><span class="nv">A</span><span class="p">,</span> <span class="nv">Y</span><span class="p">),</span> <span class="nv">X</span> <span class="p">!</span><span class="o">=</span> <span class="nv">Y</span><span class="p">,</span> <span class="nv">Y</span> <span class="o">&gt;</span> <span class="nv">X</span><span class="p">.</span>
<span class="nf">maximum</span><span class="p">(</span><span class="nv">A</span><span class="p">,</span> <span class="nv">X</span><span class="p">)</span> <span class="o">:-</span> <span class="o">not</span> <span class="nf">notmax</span><span class="p">(</span><span class="nv">A</span><span class="p">,</span><span class="nv">X</span><span class="p">),</span>
                                    <span class="nf">element</span><span class="p">(</span><span class="nv">A</span><span class="p">,</span><span class="nv">X</span><span class="p">).</span>
</code></pre></div>

<h2>Extracting relevant facts from the puzzle clues</h2>
<p>A sample of clues with their corresponding representations is given in the sample puzzle above. Let us take a closer look at the clue "Tony was the third person to have his fortune told.", encoded as : - tuple $(I$, tony $), \operatorname{tuple}(J, 3), I \neq J$. This encoding specifies that if "Tony" is assigned to tuple $I$, while the rank " 3 " is assigned to a different tuple $J$, we obtain false. Thus this ASP rule limits all the models of it's program to have "Tony" assigned to the same tuple as " 3 ". One of the questions one might ask is where are the semantic data for "person" or "fortune told". They are missing from the translation since with respect to the actual goal of
solving the puzzle, they do not contribute anything meaningful. The fact that "Tony" is a "person" is inconsequential with respect to the solutions of the puzzle. With this encoding, we attempt to encode only the relevant information with regards to the solutions of the puzzle. This is to keep the structure of the encodings as simple and as general as possible. In addition, if the rule would be encoded as : - person(tony), tuple $(I$, tony $), \operatorname{tuple}(J, 3), I \neq J$, the fact person(tony) would have to be added to the program in order for the constraint to give it's desired meaning. However, this does not seem reasonable as there are no reasons to add it (outside for the clue to actually work), since "person" is not present in the actual data of the puzzle.</p>
<h2>Translating Natural language to ASP</h2>
<p>To translate the english descriptions into ASP, we adopt our approach in (Baral et al. 2011). This approach uses inverse-lambda computations, generalization on demand and trivial semantic solutions together with learning. However for this paper, we had to adapt the approach to the ASP language and develop an ASP- $\lambda$-Calculus. An example of a clue translation using combinatorial categorial grammar (Steedman 2000) and ASP- $\lambda$-calculus is given in table 1.</p>
<p>The system uses the two inverse $\lambda$ operators, Inverse $<em R="R">{L}$ and Inverse ${ }</em>$ as given in (Baral et al. 2011). For more details, as well as the other operator, please see (Gonzalez 2010). We now introduce the different symbols used in the algorithm and their meaning :}$ as given in (Baral et al. 2011) and (Gonzalez 2010). Given $\lambda$-calculus formulas $H$ and $G$, these allow us to compute a $\lambda$-calculus formula $F$ such that $H=F @ G$ and $H=G @ F$. We now present one of the two Inverse $\lambda$ operators, Inverse ${ }_{R</p>
<ul>
<li>Let $G, H$ represent typed $\lambda$-calculus formulas, $J^{1}, J^{2}, \ldots, J^{n}$ represent typed terms, $v_{1}$ to $v_{n}, v$ and $w$ represent variables and $\sigma_{1}, \ldots, \sigma_{n}$ represent typed atomic terms.</li>
<li>Let $f()$ represent a typed atomic formula. Atomic formulas may have a different arity than the one specified and still satisfy the conditions of the algorithm if they contain the necessary typed atomic terms.</li>
<li>Typed terms that are sub terms of a typed term J are denoted as $J_{i}$.</li>
<li>If the formulas we are processing within the algorithm do not satisfy any of the $i f$ conditions then the algorithm returns null.</li>
</ul>
<p>Definition I (operator :) Consider two lists of typed $\lambda$ elements $A$ and $B,\left(a_{i}, \ldots, a_{n}\right)$ and $\left(b_{j}, \ldots, b_{n}\right)$ respectively and a formula $H$. The result of the operation $H(A: B)$ is obtained by replacing $a_{i}$ by $b_{i}$, for each appearance of $A$ in $H$.</p>
<p>Next, we present the definition of an inverse operators ${ }^{5}$ Inverse $_{R}(H, G)$ :</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Definition 2 The function Inverse ${ }_{R}(H, G)$ is defined as: Given $G$ and $H$ :</p>
<ol>
<li>If $G$ is $\lambda v . v @ J$, set $F=$ Inverse $_{L}(H, J)$</li>
<li>
<p>If $J$ is a sub term of $H$ and $G$ is $\lambda v . H(J: v)$</p>
</li>
<li>
<p>$F=J$</p>
</li>
<li>
<p>$G$ is not $\lambda v . v @ J, J$ is a sub term of $H$ and $G$ is $\lambda w . H\left(J\left(J_{1}, \ldots, J_{m}\right): w @ J_{p}, \ldots, @ J_{q}\right)$ with $I \leq p, q, s \leq$ $m$.</p>
</li>
<li>
<p>$F=\lambda v_{1}, \ldots, v_{s} . J\left(J_{1}, \ldots, J_{m}: v_{p}, \ldots, v_{q}\right)$.</p>
</li>
</ol>
<p>Lets assume that in the example given by table 1 the semantics of the word "immediately" is not known. We can use the Inverse operators to obtain it as follows. Using the semantic representation of the whole sentence as given by table 1, and the word "Earl", $\lambda x$. tuple $(x$, earl $)$, we can use the respective operators to obtain the semantic of "arrived immediately before the man with the Rooster" as $\lambda z .:-z @ I$, tuple $(J$, rooster $)$, tuple $(I, X)$, tuple $(J, Y)$, etype $(A$, rank $)$, element $(A, X)$, element $(A, Y)$, $X \neq Y-1$.</p>
<p>Repeating this process recursively we obtain $\lambda x . \lambda y . x \neq$ $y-1$ as the representation of "arrived immediately" and $\lambda x . \lambda y . \lambda z . x @(y \neq z-1)$ as the desired semantic for "immediately".</p>
<p>The input to the overall learning algorithm is a set of pairs $\left(S_{i}, L_{i}\right), i=1, \ldots, n$, where $S_{i}$ is a sentence and $L_{i}$ its corresponding logical form. The output of the algorithm is a PCCG defined by the lexicon $L_{T}$ and a parameter vector $\Theta_{T}$. As given by (Baral et al. 2011), the parameter vector $\Theta_{i}$ is updated at each iteration of the algorithm. It stores a real number for each item in the dictionary. The overall learning algorithm is given as follows:</p>
<ul>
<li>Input: A set of training sentences with their corresponding desired representations $S=\left{\left(S_{i}, L_{i}\right): i=1 \ldots n\right}$ where $S_{i}$ are sentences and $L_{i}$ are desired expressions. Weights are given an initial value of 0.1 .
An initial feature vector $\Theta_{0}$. An initial lexicon $L_{0}$.</li>
<li>Output: An updated lexicon $L_{T+1}$. An updated feature vector $\Theta_{T+1}$.</li>
</ul>
<h2>- Algorithm:</h2>
<ul>
<li>Set $L_{0}$</li>
<li>For $\mathrm{t}=1 \ldots \mathrm{~T}$</li>
<li>Step 1: (Lexical generation)</li>
<li>For $\mathrm{i}=1 \ldots \mathrm{n}$.</li>
<li>For $\mathrm{j}=1 \ldots \mathrm{n}$.</li>
<li>Parse sentence $S_{j}$ to obtain $T_{j}$</li>
<li>Traverse $T_{j}$
apply $I N V E R S E . . L, \quad I N V E R S E . . R \quad$ and GENERALIZE $E_{D}$ to find new $\lambda$-calculus expressions of words and phrases $\alpha$.</li>
<li>Set $L_{t+1}=L_{t} \cup \alpha$</li>
<li>Step 2: (Parameter Estimation)</li>
<li>Set $\Theta_{t+1}=U P D A T E\left(\Theta_{t}, L_{t+1}\right)^{6}$</li>
<li>return $G E N E R A L I Z E\left(L_{T}, L_{T}\right), \Theta(T)$</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>To translate the clues, a trained model was used to translate these from natural language into ASP. This model includes a dictionary with $\lambda$-calculus formulas corresponding to the semantic representations of words. These have their corresponding weights.</p>
<p>Tables 1 and 2 give two sample translations of a sentence into answer set programming. In the second example, the parse for the "than the customer whose number is 3989." part is not shown to save space. Also note that in general, names and several nouns were preprocessed and treated as a single noun due to parsing issues. The most noticeable fact is the abundance of expressions such as $\lambda x . x$, which basically directs to ignore the word. The main reason for this is the nature of the translation we are performing. In terms of puzzle clues, many of the words do not really contribute anything significant to the actual clue. The important parts are the actual objects, "Earl" and "Rooster" and their comparison, "arrived immediately before". In a sense, the part "the man with the" does not provide much semantic contribution with regards to the actual puzzle solution. One of the reasons is the way the actual clue is encoded in ASP. A more complex encoding would mean that more words have significant semantic contributions, however it would also mean that much more background knowledge would be required to solve the puzzles.</p>
<h2>Illustration</h2>
<p>We will now illustrate the learning algorithm on a subset of puzzle clues. We will use the following puzzle sentences, as given in table 3</p>
<p>Lets assume the initial dictionary contains the following semantic entries for words, as given in table 4. Please note that many of the nouns and noun phrases were preprocessed.</p>
<p>The algorithm will than start processing sentences one by one and attempt to learn new semantic information. The algorithm will start with the first sentence, "Donna dale does not have green fleece." Using inverse $\lambda$, the algorithm will find the semantics of "not" as $\lambda z .(z @(\lambda x . \lambda y .:$ $-x @ I, y @ I$.$) .. In a similar manner it will continue through$ the sentences learning new semantics of words. An interesting set of learned semantics as well as weights for words with multiple semantics are given in table 5.</p>
<h2>Evaluation</h2>
<p>We assume each puzzle is a pair $P=(D, C)$ where $D$ corresponds to puzzle domain data, and $C$ correspond to the clues of the puzzle given in simplified English. As discussed before, we assume the domain data $D$ is given for each of the puzzles. A set of training puzzles, $\left{P_{1}, \ldots, P_{n}\right}$ is used to train the natural language model which can be used translate natural language sentences into their ASP representations. This model is then used to translate clues for new puzzles. The initial dictionary contained nouns with most verbs. A set of testing puzzles, $\left{P_{1}^{\prime}, \ldots, P_{m}^{\prime}\right}$, is validated by transforming the data into the proper format, adding generic modules and translating the clues of $P_{1}^{\prime}, \ldots, P_{m}^{\prime}$ using the trained model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Earl</th>
<th style="text-align: center;">arrived</th>
<th style="text-align: center;">immediately</th>
<th style="text-align: center;">before</th>
<th style="text-align: center;">the</th>
<th style="text-align: center;">man</th>
<th style="text-align: center;">with</th>
<th style="text-align: center;">the</th>
<th style="text-align: center;">Rooster.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;">$S \backslash N P$</td>
<td style="text-align: center;">$(S \backslash N P) \backslash(S \backslash N P)$</td>
<td style="text-align: center;">$((S \backslash N P) \backslash(S \backslash N P)) / N P$</td>
<td style="text-align: center;">$N P / N$</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">$(N P \backslash N P) / N P$</td>
<td style="text-align: center;">$N P / N$</td>
<td style="text-align: center;">$N$</td>
</tr>
<tr>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$S \backslash N P$</td>
<td style="text-align: center;">$((S \backslash N P) \backslash(S \backslash N P)) / N P$</td>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$(N P \backslash N P) / N P$</td>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$S \backslash N P$</td>
<td style="text-align: center;">$((S \backslash N P) \backslash(S \backslash N P)) / N P$</td>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$N P \backslash N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$S \backslash N P$</td>
<td style="text-align: center;">$((S \backslash N P) \backslash(S \backslash N P)) / N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$S \backslash N P$</td>
<td style="text-align: center;">$(S \backslash N P) \backslash(S \backslash N P)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$(S \backslash N P)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">earl</td>
<td style="text-align: center;">arrived</td>
<td style="text-align: center;">immediately</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, e a r l)$</td>
<td style="text-align: center;">$\lambda x . x$</td>
<td style="text-align: center;">$\lambda x . \lambda y . \lambda z . x @(y \neq z-1)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, e a r l)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . x \neq y-1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, e a r l)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . x \neq y-1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, e a r l)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . x \neq y-1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, e a r l)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . x \neq y-1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, e a r l)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">before</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . \lambda z .:-z @ J . x @ J . t u p l e(J, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), y @ X @ Y$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . \lambda z .:-z @ J . x @ J . t u p l e(J, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), y @ X @ Y$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . \lambda z .:-z @ J . x @ J . t u p l e(J, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), y @ X @ Y$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . \lambda z .:-z @ J . x @ J . t u p l e(J, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), y @ X @ Y$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda y . \lambda z .:-z @ I, t u p l e(J, r o o s t e r), t u p l e(I, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), y @ X @ Y$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x .:-z @ I, t u p l e(J, r o o s t e r), t u p l e(I, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), X \neq Y-1$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$:-t u p l e(J, e a r l), t u p l e(J, r o o s t e r), t u p l e(I, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), X \neq Y-1$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">man</td>
<td style="text-align: center;">with</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">Rooster.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . x$</td>
<td style="text-align: center;">$\lambda x . x$</td>
<td style="text-align: center;">$\lambda x . \lambda y . y @ x$</td>
<td style="text-align: center;">$\lambda x . x$</td>
<td style="text-align: center;">$\lambda x . t u p l e(x, r o o s t e r)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . x$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . y @ x$</td>
<td style="text-align: center;">$\lambda x . t u p l e(x, r o o s t e r)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . x$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . \lambda y . y @(\lambda x . t u p l e(x, r o o s t e r))$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, r o o s t e r)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: CCG and $\lambda$-calculus derivation for "Earl arrived immediately before the person with the Rooster."</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Miss Hanson</th>
<th style="text-align: center;">is</th>
<th style="text-align: center;">withdrawing</th>
<th style="text-align: center;">more than the customer whose number is 3989.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$N P$</td>
<td style="text-align: center;">$(S / N P) \backslash N P$</td>
<td style="text-align: center;">$(S \backslash(S / N P)) / N P$</td>
<td style="text-align: center;">$N P / N P$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$S / N P$</td>
<td style="text-align: center;">$(S \backslash(S / N P)) / N P$</td>
<td style="text-align: center;">$N P$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$S / N P$</td>
<td style="text-align: center;">$(S \backslash(S / N P))$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Miss Hanson</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">is</td>
<td style="text-align: center;">withdrawing</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, h a n s o n)$</td>
<td style="text-align: center;">$\lambda x . \lambda y .(y @ x)$.</td>
<td style="text-align: center;">$\lambda x . \lambda z .(x @ z)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda y .(y @(\lambda x . t u p l e(x, h a n s o n)))$</td>
<td style="text-align: center;">$\lambda x . \lambda z .(x @ z)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda y .(y @(\lambda x . t u p l e(x, h a n s o n)))$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">more</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\lambda x . \lambda y .:-y @ I, x @ J . t u p l e(I, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), X&gt;Y, I!=J$.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda y .:-y @ I, t u p l e(J, 3989), t u p l e(I, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), X&gt;Y, I!=J$.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda y .:-z @ I, t u p l e(J, 3989), t u p l e(I, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), X&gt;Y, I!=J$.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$:-t u p l e(I, h a n s o n), t u p l e(J, 3989), t u p l e(I, X), t u p l e(J, Y), e t y p e(A, r a n k), e l e m e n t(A, X), e l e m e n t(A, Y), X&gt;Y, I!=J$.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">than the customer whose number is 3989.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\lambda x . t u p l e(x, 3989)$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: CCG and $\lambda$-calculus derivation for "Miss Hanson is withdrawing more than the customer whose number is 3989."</p>
<p>To evaluate our approach, we considered 50 different logic puzzles from various magazines, such as (puz 2007; puz 2004; puz 2005). We focused on evaluating the accuracy with which the actual puzzle clues were translated. In addition, we also verified the number of puzzles we solved. Note that in order to completely solve a puzzle, all the clues have to be translated accurately, as a missing clue means there will be several possible answer sets, which in turn will give an exact solution to the puzzle. Thus if a system would correctly translate $90 \%$ of the puzzle clues, and assuming the puzzles have on an average 10 clues, then one would expect the overall accuracy of the system to be $0.9^{10}=0.349$, or around $34.9 \%$.</p>
<p>To evaluate the clue translation, 800 clues were selected. Standard 10 fold cross validation was used. Precision measures the number of correctly translated clues, save for permutations in the body of the rules, or head of disjunctive rules. Recall measures the number of correct exact translations.</p>
<p>To evaluate the puzzles, we used the following approach. A number of puzzles were selected and all their clues formed the training data for the natural language module. The training data was used to learn the meaning of words and the associated parameters and these were then used to translate the English clues to ASP. These were then combined with the corresponding puzzle domain data, and the generic/background ASP module. The resulting program was solved using clingo, an extension of clasp (Gebser et al. 2007). Accuracy measured the number of correctly solved puzzles. A puzzle was considered correctly solved if it provided a single correct solution. If a rule provided by the clue translation from English into ASP was not syntactically correct, it was discarded. We did several experiments. Using the 50 puzzles, we did a 10 -fold cross validation to measure the accuracy. In addition, we did additional experiments with 10, 15 and 20 puzzle manually chosen as training data. The manual choice was done with the intention to pick the training set that will entail the best training. In all cases,</p>
<table>
<thead>
<tr>
<th>Donna date does not have green fleece.</th>
<th>: -tuple(I, donna,dale), tuple(I, green).</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hy Syles has a brown fleece.</td>
<td>: -tuple(I, hy,syles), tuple(J, brown), I! = J.</td>
</tr>
<tr>
<td>Flo Wingbrook’s fleece is not red.</td>
<td>: -tuple(I, flo,wingbrook), tuple(I, red).</td>
</tr>
<tr>
<td>Barbie Wyre is dining on hard-boiled eggs.</td>
<td>: -tuple(I, eggs), tuple(J, barbie,wyre), I! = J.</td>
</tr>
<tr>
<td>Dr. Mires altered the earrings.</td>
<td>: -tuple(I, dr,mires), tuple(J, earrings), I! = J.</td>
</tr>
<tr>
<td>A garnet was set in Dr. Lukiz’s piece.</td>
<td>: -tuple(I, garnet), tuple(J, dr,lukiz))), I! = J.</td>
</tr>
<tr>
<td>Michelle is not the one liked by 22</td>
<td>: -tuple(I, michelle), tuple(I, 22).</td>
</tr>
<tr>
<td>Miss Hanson is withdrawing more than the customer whose number is 3989.</td>
<td>: -tuple(I, hanson), tuple(J, 3989), tuple(I, X), tuple(J, Y),</td>
</tr>
<tr>
<td></td>
<td>etype(A, rank), element(A, X), element(A, Y), X &gt; Y, I! = J.</td>
</tr>
<tr>
<td>Albert is the most popular.</td>
<td>: -tuple(I, albert), tuple(J, X), highet(1, I! = J.</td>
</tr>
<tr>
<td>Pete talked about government.</td>
<td>: -tuple(I, pete), tuple(J, government), I! = J.</td>
</tr>
<tr>
<td>Jack has a shaved murniche.</td>
<td>: -tuple(I, jack), tuple(J, murniche), I! = J.</td>
</tr>
<tr>
<td>Jack did not get a haircut at 1</td>
<td>: -tuple(I, jack), tuple(I, 1).</td>
</tr>
<tr>
<td>The first open house was not listed for 100000.</td>
<td>: -tuple(I, X), first(X), tuple(I, 100000).</td>
</tr>
<tr>
<td>The candidate surnamed Waring is more popular than the PanGlobal.</td>
<td>: -tuple(I, waring), tuple(J, panglobal), tuple(I, X), tuple(J, Y),</td>
</tr>
<tr>
<td></td>
<td>etype(A, time), element(A, X), element(A, Y), X &lt; Y.</td>
</tr>
<tr>
<td>Rosalyn is not the least popular.</td>
<td>: -tuple(I, rosalyn), tuple(I, X), lowest(X).</td>
</tr>
</tbody>
</table>
<p>Table 3: Illustration sentences for the ASP corpus</p>
<table>
<thead>
<tr>
<th>verb v</th>
<th>$\lambda x . \lambda y .:-y @ I, x @ J, I!=J . . \lambda x . \lambda y .(x @ y), \lambda x . \lambda y .(y @ x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\lambda x . x$</td>
<td></td>
</tr>
<tr>
<td>noun n</td>
<td>$\lambda x . t u p l e(x, n), \lambda x . x$</td>
</tr>
<tr>
<td>noun n with general knowledge</td>
<td>$\lambda x . n(x)$</td>
</tr>
<tr>
<td>Example: sister, maximum, female,..</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Initial dictionary for the ASP corpus
the $C \&amp; C$ parser (Clark and Curran 2007) was used to obtain the syntactic parse tree.</p>
<h2>Results and Analysis</h2>
<p>The results are given in tables 7 and 6 . The " 10 -fold" corresponds to experiments with 10 -fold validation, " $10-\mathrm{s}$ ", " $15-$ s" and " $20-\mathrm{s}$ " to experiments where 10,15 and 20 puzzles were manually chosen as training data respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F-measure</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">87.64</td>
<td style="text-align: center;">86.12</td>
<td style="text-align: center;">86.87</td>
</tr>
</tbody>
</table>
<p>Table 6: Clue translation performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">10-Fold</td>
<td style="text-align: center;">$28 / 50(56 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">$10-\mathrm{s}$</td>
<td style="text-align: center;">$22 / 40(55 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">$15-\mathrm{s}$</td>
<td style="text-align: center;">$24 / 35(68.57 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">$20-\mathrm{s}$</td>
<td style="text-align: center;">$25 / 30(83.33 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance on puzzle solving.
The results for clue translation to ASP is comparable to translating natural language sentences to Geoquery and Robocup domains used by us in (Baral et al. 2011), and used in similar works such as (Zettlemoyer and Collins 2007) and (Ge and Mooney 2009). Our results are close to the values reported there, which range from 88 to 92 percent for the database domain and 75 to 82 percent for the Robocup domain.</p>
<p>As discussed before, a $90 \%$ accuracy is expected to lead to around $35 \%$ rate for the actual puzzles. Our result of $56 \%$ is significantly higher. It is interesting to note that as the number of puzzles used for training increases, so does the accuracy. However, there seems to be a ceiling of around $83.3 \%$.</p>
<p>In general, the reason for not being able to solve a puzzle lies in the inability to correctly translate the clue. Incorrectly translated clues which are not syntactically correct
are discarded, while for some clues the system is not capable to produce any ASP representation at all. There are several major reasons why the system fails to translate a clue. First, even with large amount of training data, some puzzles simply have a relatively unique clue. For example, for the clue, "The person with Lucky Element Earth had their fortune told exactly two days after Philip." the "exactly two days after" part is very rare and a similar clue, which discusses the distance of elements on a time line is only present in two different puzzles. There were only 2 clues that contain "aired within n days of each other", both in a single puzzle. If this puzzle is part of the training set, since we are not validating against it, it has no impact on the results. If it's one of the tested puzzles, this clue will essentially never be translated properly and as such the puzzle will never be correctly solved. In general, many of the clues required to solve the puzzles are very specific, and even with the addition of generic knowledge modules, the system is simply not capable to figure them out. A solution to this problem might be to use more background knowledge and a larger training sample, or a specific training sample which focuses on various different types of clues. In addition, when looking at tables 1 and 5, many of the words are assigned very simple semantics that essentially do not contribute any meaning to the actual translation of the clue. Compared to database query language and robocup domains, there are several times as many simple representations. This leads to several problems. One of the problems is that the remaining semantics might be over fit to the particular training sentences. For example, for "aired within n days of each other" the only words with non trivial semantics might be "within" and some number " $n$ ", which in turn might not be generic for other sentences. The generalization approach adopted from (Baral et al. 2011) is unable to overcome this problem. The second problem is that a lot of words have these trivial semantics attached, even though they also have several other non triv-</p>
<table>
<thead>
<tr>
<th>word</th>
<th>semantics</th>
<th>weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>not</td>
<td>$\lambda x . / x @(\lambda x . \lambda y .:-x @ I, y @ I .))$</td>
<td>-0.28</td>
</tr>
<tr>
<td>not</td>
<td>$\lambda y . \lambda x .:-x @ I, y @ I .)$</td>
<td>0.3</td>
</tr>
<tr>
<td>has</td>
<td>$\lambda x . \lambda y .:-y @ I, x @ I . I!=J$.</td>
<td>0.22</td>
</tr>
<tr>
<td>has</td>
<td>$\lambda x . \lambda y .(x @ y)$</td>
<td>0.05</td>
</tr>
<tr>
<td>has</td>
<td>$\lambda x . \lambda y .(y @ x)$</td>
<td>0.05</td>
</tr>
<tr>
<td>has</td>
<td>$\lambda x . x$</td>
<td>0.05</td>
</tr>
<tr>
<td>popular</td>
<td>$\lambda x . t u p l e(x$, popular)</td>
<td>0.17</td>
</tr>
<tr>
<td>popular</td>
<td>$\lambda x . x$</td>
<td>0.03</td>
</tr>
<tr>
<td>a</td>
<td>$\lambda x . x$</td>
<td>0.1</td>
</tr>
<tr>
<td>not</td>
<td>$\lambda x . \lambda y .:-y @ I, x @ I .$</td>
<td>0.1</td>
</tr>
<tr>
<td>on</td>
<td>$\lambda x . x$</td>
<td>0.1</td>
</tr>
<tr>
<td>the</td>
<td>$\lambda x . x$</td>
<td>0.1</td>
</tr>
<tr>
<td>in</td>
<td>$\lambda x . \lambda y .(y @ x)$</td>
<td>0.1</td>
</tr>
<tr>
<td>by</td>
<td>$\lambda x . x$</td>
<td>0.1</td>
</tr>
<tr>
<td>most</td>
<td>$\lambda y . \lambda x . y @(t u p l e(x, X)$, highest $(X))$</td>
<td>0.1</td>
</tr>
<tr>
<td>about</td>
<td>$\lambda x . x$</td>
<td>0.1</td>
</tr>
<tr>
<td>shaved</td>
<td>$\lambda x . x$</td>
<td>0.1</td>
</tr>
<tr>
<td>at</td>
<td>$\lambda y . \lambda x .(x @ y)$</td>
<td>0.1</td>
</tr>
<tr>
<td>first</td>
<td>$\lambda y . y @(\lambda x . t u p l e(x, X)$, first $(X))$</td>
<td>0.1</td>
</tr>
<tr>
<td>for</td>
<td>$\lambda x . x$</td>
<td>0.1</td>
</tr>
<tr>
<td>least</td>
<td>$\lambda x . t u p l e(x, X)$, lowest $(X)$</td>
<td>0.1</td>
</tr>
<tr>
<td>more</td>
<td>$\lambda x . \lambda y .:-y @ I, x @ J, t u p l e(I, X)$, tuple $(J, Y)$,</td>
<td>0.1</td>
</tr>
<tr>
<td></td>
<td>etype $(A, r a n k)$, element $(A, X)$, element $(A, Y), X&gt;Y, I!=J$.</td>
<td>0.1</td>
</tr>
</tbody>
</table>
<p>Table 5: Learned semantics and final weights of selected words of the ASP corpus.
ial representations. This causes problem with learning, and the trivial semantics may be chosen over the non-trivial one. Finally, some of the $C \&amp; C$ parses do not allow the proper use of inverse $\lambda$ operators, or their use leads to very complex expressions with several applications of $@$. In table 1, this can be seen by looking the representation of the word "immediately". While this particular case does not cause serious issues, it illustrates that when present several times in a sentence, the resulting $\lambda$ expression can get very complex leading to third or fourth order $\lambda$-ASP-calculus formulas.</p>
<h2>Conclusion and Future work</h2>
<p>In this work we presented a learning approach to solve combinatorial logic puzzles in English. Our system uses an initial dictionary and general knowledge modules to obtain an ASP program whose unique answer set corresponded to the solution of the puzzle. Using a set of puzzles and their clues to train a model which can translate English sentences into logical form, we were able to solve many additional puzzles by automatically translating their clues, given in simplified English, into ASP. Our system used results and components from various AI sub-disciplines including natural language processing, knowledge representation and reasoning, machine learning and ontologies as well as the functional programming concept of $\lambda$-calculus. There are many ways to extend our work. The simplified English limitation might be lifted by better natural language processing tools and additional sentence analysis. We could also apply our approach to different types of puzzles. A modified encodings might yield a smaller variance in the results. Finally we would like to submit that solving puzzles given in a natural language could be considered as a challenge problem for human level intelligence as it encompasses various facets of intelligence that we listed earlier. In particular, one has to use a reasoning system and can not substitute it with surface level analysis often used in information retrieval based methods.</p>
<h2>References</h2>
<p>[Baral et al. 2011] Baral, C.; Gonzalez, M.; Dzifcak, J.; and Zhou, J. 2011. Using inverse $\lambda$ and generalization to translate english to formal languages. In Proceedings of the International Conference on Computational Semantics, Oxford, England, January 2011.
[Baral 2003] Baral, C. 2003. Knowledge Representation, Reasoning, and Declarative Problem Solving. Cambridge University Press.
[Clark and Curran 2007] Clark, S., and Curran, J. R. 2007. Wide-coverage efficient statistical parsing with ccg and loglinear models. Computational Linguistics 33.
[Finkel, Marek, and Truszczynski 2002] Finkel, R. A.; Marek, V. W.; and Truszczynski, M. 2002. Constraint lingo: A program for solving logic puzzles and other tabular constraint problems. In JELIA, 513-516.
[Ge and Mooney 2005] Ge, R., and Mooney, R. J. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of CoNLL., 9-16.
[Ge and Mooney 2009] Ge, R., and Mooney, R. J. 2009. Learning a compositional semantic parser using an existing syntactic parser. In Proceedings of ACL-IJCNLP., 611-619.
[Gebser et al. 2007] Gebser, M.; Kaufmann, B.; Neumann, A.; and Schaub, T. 2007. Clasp : A conflict-driven answer set solver. In LPNMR, 260-265.
[Gonzalez 2010] Gonzalez, M. A. 2010. An inverse lambda calculus algorithm for natural language processing. Master's thesis, Arizona State University.
[puz 2004] 2004. Logic problems. Penny Press.
[puz 2005] 2005. Logic puzzles. Dell.
[puz 2007] 2007. England's best logic problems. Penny Press.
[Steedman 2000] Steedman, M. 2000. The syntactic process. MIT Press.
[Zettlemoyer and Collins 2005] Zettlemoyer, L., and Collins, M. 2005. Learning to map sentences to logical</p>
<p>form: Structured classification with probabilistic categorial grammars. In AAAI, 658-666.
[Zettlemoyer and Collins 2007] Zettlemoyer, L., and Collins, M. 2007. Online learning of relaxed ccg grammars for parsing to logical form. In Proceedings of EMNLP-CoNLL, 678-687.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ For details on $\Theta$ computation, please see (Zettlemoyer and Collins 2005)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3} \lambda$-ASP-Calculus is inspired by $\lambda$-Calculus. The classical logic formulas in $\lambda$-Calculus are replaced by ASP rules in $\lambda$-ASPCalculus.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
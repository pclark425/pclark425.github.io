<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1667 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1667</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1667</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-67856268</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1903.01390v1.pdf" target="_blank">Sim-to-Real Transfer for Biped Locomotion</a></p>
                <p><strong>Paper Abstract:</strong> We present a new approach for transfer of dynamic robot control policies such as biped locomotion from simulation to real hardware. Key to our approach is to perform system identification of the model parameters {\mu} of the hardware (e.g. friction, center-of-mass) in two distinct stages, before policy learning (pre-sysID) and after policy learning (post-sysID). Pre-sysID begins by collecting trajectories from the physical hardware based on a set of generic motion sequences. Because the trajectories may not be related to the task of interest, presysID does not attempt to accurately identify the true value of {\mu}, but only to approximate the range of {\mu} to guide the policy learning. Next, a Projected Universal Policy (PUP) is created by simultaneously training a network that projects {\mu} to a low-dimensional latent variable {\eta} and a family of policies that are conditioned on {\eta}. The second round of system identification (post-sysID) is then carried out by deploying the PUP on the robot hardware using task-relevant trajectories. We use Bayesian Optimization to determine the values for {\eta} that optimizes the performance of PUP on the real hardware. We have used this approach to create three successful biped locomotion controllers (walk forward, walk backwards, walk sideways) on the Darwin OP2 robot.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1667.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1667.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PUP-two-stage-sysID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Projected Universal Policy with Pre- and Post-Training System Identification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real transfer pipeline that (1) uses pre-training system identification to estimate bounds on simulation parameters, (2) trains a Projected Universal Policy (PUP) conditioned on a low-dimensional latent projection of those parameters, and (3) runs a post-training system identification on hardware (Bayesian Optimization over the latent) to select the best conditioned policy for the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Darwin OP2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A consumer-grade humanoid biped robot (20 Dynamixel MX-28T servos) used for locomotion experiments; controlled by target position commands via local PID controllers and outfitted with an external Bosch bno055 IMU for torso orientation estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biped locomotion robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>unnamed/custom physics simulator</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A rigid-body physics simulator modeling articulated body dynamics, joint actuators, ground contact, motor position sensing and IMU-like orientation; simulator augmented with learned motor model components and randomized model parameters during training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high- to moderate-fidelity physics simulation with learned actuator augmentation (not a photoreal renderer) — physics-focused simulator approximating contacts and actuator dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>actuator (motor) dynamics via a neural-augmented PD model, articulated rigid-body dynamics, contact with ground (foot contact), motor position sensing, torso orientation (IMU) estimates, sensor noise and control-frequency variability</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>no direct torque sensing modeled for low-cost servos (actual torque not measured on hardware), limited modeling expressiveness for all motor behaviors (some NN weights fixed and not randomized), simulator engine not named so solver-specific numerical approximations unknown, approximations in torso inertial properties (torso CoM included but other CAD inertias assumed accurate), and no photoreal or vision modeling (not applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Darwin OP2 walking on a yoga mat placed over a white board (deformable protective surface); control loop ~33 Hz, motor position readings used (two consecutive positions to infer velocity), external Bosch bno055 IMU for orientation, trials limited by power-cable length and balance failures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>dynamic biped locomotion controllers: walk forward, walk backward, walk sideways</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (Proximal Policy Optimization) to train a family of policies conditioned on a low-dimensional latent projection η sampled from ranges derived via pre-sysID; PUP trained with domain randomization and mirror symmetry loss</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>distance travelled before losing balance (measured at end of each rollout) and elapsed time before failure; success also judged qualitatively by whether robot can walk across board edge (~0.8 m)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Successfully produced walking controllers for forward, backward and sideways walking on hardware; policies required 25 real-world trials (5 initial samples + 20 Bayesian-Optimization iterations) to identify η*; policies occasionally reached beyond board edge (≈0.8 m). Exact simulation-to-real numeric performance not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized simulation model parameters µ within bounds found by pre-sysID for training PUP; added observation noise (motor position Gaussian σ=0.01), random control frequency per rollout in [25,33] Hz, orientation bias U(-0.3,0.3); during PUP training added noise U(-0.25,0.25) to µ inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>actuator modeling errors / nonlinearity in current-torque relationship of low-cost servos, surface friction differences, torso center-of-mass uncertainty, sensor/communication timing and control-frequency variability, limited IMU/motor sample rates, limited expressiveness of simulator motor model (necessitating NN augmentation), and differences in contact dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>two-stage system identification (pre-sysID to produce parameter bounds; post-sysID to select latent η on hardware), Projected Universal Policy conditioned on low-dimensional η (reduces BO search dimension), neural-augmented motor model (NN-PD) to better capture actuator behavior, Bayesian Optimization over η for sample-efficient post-sysID (25 hardware trials), domain randomization within identified bounds, use of an external IMU to improve orientation sensing, discretized action space and stepping-in-place reference trajectory to accelerate training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Qualitative requirements: accurate actuator (motor) dynamics modeling is critical; contact/friction modeling and torso CoM matter; domain randomization alone insufficient for biped gait transfer — learned actuator augmentation (NN-PD) and targeted parameter-range identification are required. No hard quantitative tolerances (e.g., % errors) were specified.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Post-sysID adapts the policy on hardware by optimizing the low-dimensional latent η via Bayesian Optimization: initialize GP with 5 uniform samples, then 20 BO iterations (total 25 hardware rollouts), using distance-travelled as fitness; process takes <15 minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared motor-dynamics model variants: NN-augmented PD (NN-PD) vs PD-only; NN-PD achieved notably lower system-identification loss on pre-sysID data and reproduced motor step responses and torso-pitch during falls better than PD-only, indicating higher effective simulation fidelity for actuator behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A two-stage system-identification pipeline connected by a Projected Universal Policy allows sample-efficient sim-to-real transfer of dynamic biped locomotion to consumer-grade hardware: pre-sysID produces parameter bounds for domain randomization, PUP compresses parameter variations to a low-dimensional latent, and post-sysID (Bayesian Optimization) identifies the best conditioning on hardware in ~25 trials. Accurate actuator modeling (NN-PD) and targeted parameter-range estimation are crucial; simple domain randomization alone was insufficient for reliable biped transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer for Biped Locomotion', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1667.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1667.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NN-PD motor model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-network-augmented PD actuator model (NN-PD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network augmentation to a conventional PD motor controller: a small shared NN (one hidden layer, 5 nodes) maps motor error and velocity to adaptive proportional and derivative gains, combined with per-group scaling factors and a learnable torque limit to better reproduce low-cost servo behavior in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Darwin OP2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Same Darwin OP2 humanoid; NN-PD models the behavior of its Dynamixel MX-28T servos in simulation to reduce reality gap from actuator nonlinearity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robot actuator modeling for sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>unnamed/custom physics simulator</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator integrates NN-PD to compute motor torques from commanded target positions and current states; used during pre-sysID and policy training to better match observed motor position responses and torso motions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>augmented actuator fidelity — PD controller plus learned corrections produces higher fidelity motor response compared to vanilla PD-only model</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>motor nonlinearities and effective PD gains as a function of error and velocity, group-level motor scaling factors (HEAD, ARM, HIP, KNEE, ANKLE), torque limits, torque clipping, resulting torques and their effect on joint and torso dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>does not output direct torque measurements (no ground-truth torque supervision), NN weights were fixed (not randomized) during pre-sysID bounds optimization due to interdependence concerns, single small shared NN across motors (limited representational capacity), simplifications may leave some behaviors unexplained (bipolar identified τ depending on contact/no-contact)</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Model trained and validated using motor position step responses and standing/falling trajectories collected from Darwin OP2 (suspended motor step tests and ground-contact standing/falling tests).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>improving fidelity of simulated actuator behavior to enable transfer of biped locomotion policies</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>system identification using CMA-ES to optimize model parameters to match recorded trajectories; NN structure (one hidden layer, 5 nodes tanh) shared across motors; pre-sysID optimizes bounds for most parameters but NN weights φ were fixed during bound estimation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>system-identification loss on pre-scripted trajectories (difference between simulated and real motor positions, torso roll/pitch, and foot movement), and downstream sim-to-real transfer success of locomotion policies</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>actuator non-linearities and servo dynamics were a primary contributor to sim-to-real gap; NN-PD addressed these by better matching step responses and torso-pitch during falls</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>augmenting PD motor model with small NN and per-group scaling factors improved simulator fidelity sufficiently to enable better sysID loss and contributed to successful policy transfer when combined with PUP and two-stage sysID</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Qualitative: actuator model expressive enough to reproduce step response and contact-influenced behaviors is necessary; NN-PD gave notably better fits than PD-only, indicating actuator fidelity is a key requirement. No quantitative thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>NN-PD achieved lower system-identification loss than PD-only and matched observed motor-step and torso-pitch trajectories better, demonstrating improved actuator fidelity important for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Modeling actuators with a compact neural augmentation (NN-PD) substantially reduces system-identification loss vs a PD-only model and is important for successful sim-to-real transfer of dynamic biped locomotion on low-cost servos; however, the learned model still has limitations (e.g., bipolar torque-limit behavior) indicating further modeling improvements might be necessary for harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer for Biped Locomotion', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-Real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Learning agile and dynamic motor skills for legged robots <em>(Rating: 2)</em></li>
                <li>Preparing for the unknown: Learning a universal policy with online system identification <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Closing the Sim-to-Real Loop : Adapting Simulation Randomization with Real World Experience <em>(Rating: 2)</em></li>
                <li>Policy transfer with strategy optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1667",
    "paper_id": "paper-67856268",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "PUP-two-stage-sysID",
            "name_full": "Projected Universal Policy with Pre- and Post-Training System Identification",
            "brief_description": "A sim-to-real transfer pipeline that (1) uses pre-training system identification to estimate bounds on simulation parameters, (2) trains a Projected Universal Policy (PUP) conditioned on a low-dimensional latent projection of those parameters, and (3) runs a post-training system identification on hardware (Bayesian Optimization over the latent) to select the best conditioned policy for the real robot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Darwin OP2",
            "agent_system_description": "A consumer-grade humanoid biped robot (20 Dynamixel MX-28T servos) used for locomotion experiments; controlled by target position commands via local PID controllers and outfitted with an external Bosch bno055 IMU for torso orientation estimation.",
            "domain": "biped locomotion robotics",
            "virtual_environment_name": "unnamed/custom physics simulator",
            "virtual_environment_description": "A rigid-body physics simulator modeling articulated body dynamics, joint actuators, ground contact, motor position sensing and IMU-like orientation; simulator augmented with learned motor model components and randomized model parameters during training.",
            "simulation_fidelity_level": "high- to moderate-fidelity physics simulation with learned actuator augmentation (not a photoreal renderer) — physics-focused simulator approximating contacts and actuator dynamics",
            "fidelity_aspects_modeled": "actuator (motor) dynamics via a neural-augmented PD model, articulated rigid-body dynamics, contact with ground (foot contact), motor position sensing, torso orientation (IMU) estimates, sensor noise and control-frequency variability",
            "fidelity_aspects_simplified": "no direct torque sensing modeled for low-cost servos (actual torque not measured on hardware), limited modeling expressiveness for all motor behaviors (some NN weights fixed and not randomized), simulator engine not named so solver-specific numerical approximations unknown, approximations in torso inertial properties (torso CoM included but other CAD inertias assumed accurate), and no photoreal or vision modeling (not applicable)",
            "real_environment_description": "Darwin OP2 walking on a yoga mat placed over a white board (deformable protective surface); control loop ~33 Hz, motor position readings used (two consecutive positions to infer velocity), external Bosch bno055 IMU for orientation, trials limited by power-cable length and balance failures.",
            "task_or_skill_transferred": "dynamic biped locomotion controllers: walk forward, walk backward, walk sideways",
            "training_method": "reinforcement learning (Proximal Policy Optimization) to train a family of policies conditioned on a low-dimensional latent projection η sampled from ranges derived via pre-sysID; PUP trained with domain randomization and mirror symmetry loss",
            "transfer_success_metric": "distance travelled before losing balance (measured at end of each rollout) and elapsed time before failure; success also judged qualitatively by whether robot can walk across board edge (~0.8 m)",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Successfully produced walking controllers for forward, backward and sideways walking on hardware; policies required 25 real-world trials (5 initial samples + 20 Bayesian-Optimization iterations) to identify η*; policies occasionally reached beyond board edge (≈0.8 m). Exact simulation-to-real numeric performance not reported.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized simulation model parameters µ within bounds found by pre-sysID for training PUP; added observation noise (motor position Gaussian σ=0.01), random control frequency per rollout in [25,33] Hz, orientation bias U(-0.3,0.3); during PUP training added noise U(-0.25,0.25) to µ inputs.",
            "sim_to_real_gap_factors": "actuator modeling errors / nonlinearity in current-torque relationship of low-cost servos, surface friction differences, torso center-of-mass uncertainty, sensor/communication timing and control-frequency variability, limited IMU/motor sample rates, limited expressiveness of simulator motor model (necessitating NN augmentation), and differences in contact dynamics",
            "transfer_enabling_conditions": "two-stage system identification (pre-sysID to produce parameter bounds; post-sysID to select latent η on hardware), Projected Universal Policy conditioned on low-dimensional η (reduces BO search dimension), neural-augmented motor model (NN-PD) to better capture actuator behavior, Bayesian Optimization over η for sample-efficient post-sysID (25 hardware trials), domain randomization within identified bounds, use of an external IMU to improve orientation sensing, discretized action space and stepping-in-place reference trajectory to accelerate training.",
            "fidelity_requirements_identified": "Qualitative requirements: accurate actuator (motor) dynamics modeling is critical; contact/friction modeling and torso CoM matter; domain randomization alone insufficient for biped gait transfer — learned actuator augmentation (NN-PD) and targeted parameter-range identification are required. No hard quantitative tolerances (e.g., % errors) were specified.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Post-sysID adapts the policy on hardware by optimizing the low-dimensional latent η via Bayesian Optimization: initialize GP with 5 uniform samples, then 20 BO iterations (total 25 hardware rollouts), using distance-travelled as fitness; process takes &lt;15 minutes.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Compared motor-dynamics model variants: NN-augmented PD (NN-PD) vs PD-only; NN-PD achieved notably lower system-identification loss on pre-sysID data and reproduced motor step responses and torso-pitch during falls better than PD-only, indicating higher effective simulation fidelity for actuator behavior.",
            "key_findings": "A two-stage system-identification pipeline connected by a Projected Universal Policy allows sample-efficient sim-to-real transfer of dynamic biped locomotion to consumer-grade hardware: pre-sysID produces parameter bounds for domain randomization, PUP compresses parameter variations to a low-dimensional latent, and post-sysID (Bayesian Optimization) identifies the best conditioning on hardware in ~25 trials. Accurate actuator modeling (NN-PD) and targeted parameter-range estimation are crucial; simple domain randomization alone was insufficient for reliable biped transfer.",
            "uuid": "e1667.0",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer for Biped Locomotion",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "NN-PD motor model",
            "name_full": "Neural-network-augmented PD actuator model (NN-PD)",
            "brief_description": "A neural-network augmentation to a conventional PD motor controller: a small shared NN (one hidden layer, 5 nodes) maps motor error and velocity to adaptive proportional and derivative gains, combined with per-group scaling factors and a learnable torque limit to better reproduce low-cost servo behavior in simulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Darwin OP2",
            "agent_system_description": "Same Darwin OP2 humanoid; NN-PD models the behavior of its Dynamixel MX-28T servos in simulation to reduce reality gap from actuator nonlinearity.",
            "domain": "robot actuator modeling for sim-to-real transfer",
            "virtual_environment_name": "unnamed/custom physics simulator",
            "virtual_environment_description": "Simulator integrates NN-PD to compute motor torques from commanded target positions and current states; used during pre-sysID and policy training to better match observed motor position responses and torso motions.",
            "simulation_fidelity_level": "augmented actuator fidelity — PD controller plus learned corrections produces higher fidelity motor response compared to vanilla PD-only model",
            "fidelity_aspects_modeled": "motor nonlinearities and effective PD gains as a function of error and velocity, group-level motor scaling factors (HEAD, ARM, HIP, KNEE, ANKLE), torque limits, torque clipping, resulting torques and their effect on joint and torso dynamics",
            "fidelity_aspects_simplified": "does not output direct torque measurements (no ground-truth torque supervision), NN weights were fixed (not randomized) during pre-sysID bounds optimization due to interdependence concerns, single small shared NN across motors (limited representational capacity), simplifications may leave some behaviors unexplained (bipolar identified τ depending on contact/no-contact)",
            "real_environment_description": "Model trained and validated using motor position step responses and standing/falling trajectories collected from Darwin OP2 (suspended motor step tests and ground-contact standing/falling tests).",
            "task_or_skill_transferred": "improving fidelity of simulated actuator behavior to enable transfer of biped locomotion policies",
            "training_method": "system identification using CMA-ES to optimize model parameters to match recorded trajectories; NN structure (one hidden layer, 5 nodes tanh) shared across motors; pre-sysID optimizes bounds for most parameters but NN weights φ were fixed during bound estimation",
            "transfer_success_metric": "system-identification loss on pre-scripted trajectories (difference between simulated and real motor positions, torso roll/pitch, and foot movement), and downstream sim-to-real transfer success of locomotion policies",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "actuator non-linearities and servo dynamics were a primary contributor to sim-to-real gap; NN-PD addressed these by better matching step responses and torso-pitch during falls",
            "transfer_enabling_conditions": "augmenting PD motor model with small NN and per-group scaling factors improved simulator fidelity sufficiently to enable better sysID loss and contributed to successful policy transfer when combined with PUP and two-stage sysID",
            "fidelity_requirements_identified": "Qualitative: actuator model expressive enough to reproduce step response and contact-influenced behaviors is necessary; NN-PD gave notably better fits than PD-only, indicating actuator fidelity is a key requirement. No quantitative thresholds provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "NN-PD achieved lower system-identification loss than PD-only and matched observed motor-step and torso-pitch trajectories better, demonstrating improved actuator fidelity important for transfer.",
            "key_findings": "Modeling actuators with a compact neural augmentation (NN-PD) substantially reduces system-identification loss vs a PD-only model and is important for successful sim-to-real transfer of dynamic biped locomotion on low-cost servos; however, the learned model still has limitations (e.g., bipolar torque-limit behavior) indicating further modeling improvements might be necessary for harder tasks.",
            "uuid": "e1667.1",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer for Biped Locomotion",
                "publication_date_yy_mm": "2019-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-Real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Learning agile and dynamic motor skills for legged robots",
            "rating": 2,
            "sanitized_title": "learning_agile_and_dynamic_motor_skills_for_legged_robots"
        },
        {
            "paper_title": "Preparing for the unknown: Learning a universal policy with online system identification",
            "rating": 2,
            "sanitized_title": "preparing_for_the_unknown_learning_a_universal_policy_with_online_system_identification"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Closing the Sim-to-Real Loop : Adapting Simulation Randomization with Real World Experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Policy transfer with strategy optimization",
            "rating": 1,
            "sanitized_title": "policy_transfer_with_strategy_optimization"
        }
    ],
    "cost": 0.01119675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer for Biped Locomotion</p>
<p>Wenhao Yu 
Visak Cv Kumar 
Greg Turk 
C Karen Liu 
Sim-to-Real Transfer for Biped Locomotion</p>
<p>We present a new approach for transfer of dynamic robot control policies such as biped locomotion from simulation to real hardware. Key to our approach is to perform system identification of the model parameters µ of the hardware (e.g. friction, center-of-mass) in two distinct stages, before policy learning (pre-sysID) and after policy learning (post-sysID). Pre-sysID begins by collecting trajectories from the physical hardware based on a set of generic motion sequences. Because the trajectories may not be related to the task of interest, pre-sysID does not attempt to accurately identify the true value of µ, but only to approximate the range of µ to guide the policy learning. Next, a Projected Universal Policy (PUP) is created by simultaneously training a network that projects µ to a low-dimensional latent variable η and a family of policies that are conditioned on η. The second round of system identification (post-sysID) is then carried out by deploying the PUP on the robot hardware using task-relevant trajectories. We use Bayesian Optimization to determine the values for η that optimizes the performance of PUP on the real hardware. We have used this approach to create three successful biped locomotion controllers (walk forward, walk backwards, walk sideways) on the Darwin OP2 robot.</p>
<p>I. INTRODUCTION</p>
<p>Designing locomotion controllers for biped robots is a challenging task that often depends on tuning the control parameters manually in a trial-and-error fashion. Recent advancements in Deep Reinforcement Learning (DRL) have shown a promising path in automating the design of robotic locomotion controllers [1]- [4]. However, the amount of training samples demanded by most DRL methods is infeasible to acquire from the real world for high-risk tasks, such as biped locomotion. While computer simulation provides a safe and efficient way to learn motor skills, a policy trained in simulation does not often transfer to the real hardware due to various modeling discrepancy between the simulated and the real environments, referred to as the Reality Gap [5].</p>
<p>To overcome the reality gap, recent work has investigated more sophisticated system identification procedures that improve the accuracy of the model, developed more robust control policies that work for a large variety of simulated environments (i.e. domain randomization), or interleaved system identification with robust policy learning in an iterative algorithm. While these approaches have shown impressive results on manipulation and quadruped locomotion tasks, it is not clear whether this success can be extended to biped locomotion. The inherent instability of biped locomotion makes standard system identification ineffective because 1 Authors are with School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, 30308 wenhaoyu@gatech.edu, visak3@gatech.edu, karenliu@cc.gatech.edu, turk@cc.gatech.edu it is challenging to collect task-relevant data for system identification prior to policy training. Without an effective system identification procedure, robust policy learning would only work if the default models are reasonably accurate for approximating the robot dynamics, sensors, and actuators. Furthermore, a balanced biped locomotion policy is generally more susceptive to discrepancies between the training and testing environments, rendering the domain randomization technique alone insufficient to conquer the reality gap.</p>
<p>This paper aims to develop a biped locomotion controller by training a policy in simulation and deploying it to consumer-grade robotic hardware (e.g. Darwin OP2 which costs less than $10,000 USD). The key idea of our approach is to split the system identification procedure into two stages: one prior to the policy training (pre-sysID) and one subsequent to the policy training (post-sysID) ( Figure  2). Since the real-world data collected prior to the policy training are usually irrelevant to the task, the goal of pre-sysID is not to accurately identify the true value of model parameters, but only to approximate the range of model parameters in order to train a policy later. After the policy is trained in simulation, the post-sysID is then able to use taskrelevant data to identify the model parameters that optimize the performance of the policy deployed in the real world. The critical component that bridges the two system ID processes is the Projected Universal Policy (PUP). We simultaneously train a network that projects the model parameters to a lowdimensional latent variable, together with a family of policies that are conditioned on the latent space. As such, PUP is a policy which can be modulated by a low-dimensional latent variable to adapt to any environment within the range of model parameters it is trained for. Once PUP is well trained in simulation, we only need to perform one more system identification, i.e. post-sysID, to optimize the transfer performance of PUP, without the need of iterating between system identification and policy learning.</p>
<p>We demonstrate our algorithm on training locomotion controllers for the Darwin OP2 robot to perform forward, backward and sideway walks. Our algorithm can successfully transfer the policy trained in simulation to the hardware in 25 real-world trials. We also evaluate the algorithm by comparing our method to two baseline methods: 1) identify a single model during system identification and train a policy for that model, and 2) use the range of parameters from pre-sysID to train a robust policy.</p>
<p>II. RELATED WORK</p>
<p>Transferring control policies from a source domain to a different target domain has been explored by a number of research groups. In the context of reinforcement learning, a comprehensive survey on transfer learning was reported by Taylor and Stone [6]. One notable application of transfer learning in reinforcement learning is sim-to-real transfer, where policies trained in simulation are transferred to real robots. Sim-to-real transfer allows automatic training of robot controllers in a safer environment, however, it is a challenging problem due to the presence of the Reality Gap [5]. A few recent works have shown successful transfer of policies between simulated environments [7]- [13], however, these were not tested on real robotic systems.</p>
<p>Among methods that successfully transfer control policies to real robots, two key ideas are instrumental: system identification [14] and robust policy generation. System identification aims to bridge the reality gap by identifying simulation models that can accurately capture the real-world events. Researchers have shown that, with carefully identified simulation models, it is possible to transfer learned trajectories or policies to real hardware [15]- [21]. For example, Tan et al. [15] combined domain randomization with accurate identification of the servo motor model and demonstrated successful transfer of locomotion policies for a quadruped robot. The key idea in their work is to identify a non-linearity current-torque relationship in motor dynamics. Hwangbo et al. [19], trained a deep neural network that maps from motor commands to torques using data from the real actuators of a quadruped robot. The trained model then replaces the motor model in the simulator, which is used to train control policies for the quadruped. They demonstrated transferring of agile and dynamic motions on a real robot. In our work, we also utilize a neural network for modeling the motor dynamics, however, our method do not rely on the high-end actuators to generate ground-truth torque data as in [19].</p>
<p>The other key component in sim-to-real transfer is training robust policies. To make a policy more robust, numerous approaches have been explored such as adding adversarial perturbations during training in Pinto et al. [13], using ensemble of models [22], [23] randomizing sensor noise in Jakobi et al. [24] and domain randomization [25]- [27]. In Peng et al. [26], an LSTM policy was trained with dynamics randomization and transferred to a real robotic manipulator. Andrychowicz et al. [27] demonstrated dexterous in-hand manipulation on an anthropomorphic robot hand, the Shadow Dexterous Hand, by introducing randomization in both perception and dynamics of the simulation during training. Although these methods have shown promising results, they in general assume that the dynamics of the testing environment is not too far away from the set of training environments. One may increase the range of training environments to avoid this, however, it will require more samples and compute for training the policy and may lead to overly-conservative behaviors.</p>
<p>Another approach in sim-to-real transfer is to further finetune a trained policy using data collected from the real hardware [28]- [30]. Chebotar et al. [29] presented a technique to interleave system identification and policy learning for manipulation tasks. At each iteration, the distribution of the randomized parameters was optimized by minimizing the difference between the trajectories collected in simulation and real-world. They demonstrated transfer for tasks such as draw opening. However, it is unclear how well it works for bipedal locomotion tasks because the quick failure of the policy on the robot during initial iterations may not provide enough information for meaningful updates to the parameter distribution. The closest work to ours is that of Cully et al. [31]. Their algorithm searches over the space of previously learned behaviours that can compensate for changes in dynamics, like a damage to the robot. The key difference is that in our method we search over parameterized family of policies while they search for controllers in discrete behaviour space. It is not certain how well the proposed method would scale to high dimensional space.</p>
<p>Despite concerns about safety and sample complexity of DRL methods, there has been success in directly training locomotion controllers on the real robot [32], [33]. In Ha et al. [33], a policy was directly trained on a multi-legged robot. The training was automated by a novel resetting device which was able to re-initialize the robot during training after each rollout. In Haarnoja et al. [32], a policy was trained for a real quadruped robot in under two hours from scratch using softactor critic algorithm [34]. Despite these success in learning legged locomotion tasks, directly training policies on a biped robot is still challenging due to the frequent manual resetting required during training and the potential safety concern from the inherent instability.</p>
<p>III. PRE-TRAINING SYSTEM IDENTIFICATION (PRE-SYSID)</p>
<p>The goal of a standard system identification procedure is to tune the model parameters µ (e.g. friction, center of mass) such that the trajectories predicted by the model closely match those acquired from the real-world. One important decision in this procedure is the choice of data to collect from the real world. Ideally, we would like to collect the trajectories relevant to the task of interest. For biped locomotion, however, it is challenging to script successful locomotion trajectories prior to the policy training. Without task-relevant trajectories, any other choice of data can become a source of bias that may impact the resulting model parameters µ. Our solution to this problem is that, instead of solving for the optimal set of model parameters, Pre-sysID only attempts to approximate a reasonable range of model parameters for the purpose of domain randomization during policy learning. As such, we can use generic trajectories to cover a wide range of robot behaviors that may be remotely related to the task. In the case of locomotion, we use two set of trajectories for system identification: joint exercise without contact and standing/falling with ground contact. We use a set of prescripted actions to create these trajectories. (See details in Section VI-A).</p>
<p>A. Optimizing Range of Model Parameters</p>
<p>We optimized the model parameters µ by creating simulated trajectories using the same pre-scripted actions that were used to collect the real-world trajectories. The fitness of a given µ is given by the deviation between these simulated and real-world trajectories. Instead of trying to find a single simulation model that perfectly explains all the training data, we optimize for a set of models simultaneously, each of which fits a subset of the training trajectories. Specifically, we first use the entire set of trajectories to optimize a nominal set of model parametersμ. We then select random subsets of the training trajectories, for each subset we optimize the model parameters again withμ as initial guess. During the optimization of each subset, we add a regularization term w reg ||µ −μ|| 2 to the objective function so that µ will not go to local minima that are far away. We use w reg = 0.05 in our experiments. This results in a set of optimized simulators, each of which can better reproduce a subset of 
µ i = arg min µ L(D i , µ) + w reg µ −μ 2 6: end for 7: µ max , µ min ← per-dimension max and min of µ i 8: µ lb = µ min − 0.1(µ max − µ min ) 9: µ ub = µ max + 0.1(µ max − µ min ) 10: return µ lb , µ ub
the training data thanμ. We then extract the range of the simulation parameters by taking the element-wise maximum and minimum of the optimized µ's and expand them by 10% to obtain the bounds µ lb and µ ub .</p>
<p>We use CMA-ES [35], a sampling-based optimization algorithm, to optimize µ. To evaluate the fitness of a sampled µ, we compare the trajectories generated by the simulation to those from the hardware:
L = 1 |D| D t q t − q t + 10 |D s,f | D s,f t ḡ t − g t + 20 |D s | Ds t ∆C f eet t ,
where D denotes the entire set of input training trajectories from hardware, D s,f denotes the subset of standing or falling trajectories, and D s contains only the standing trajectories. The first term measures the difference in the simulated motor position q and the real oneq. The second term measures the difference in the roll and pitch of the robot torso between the simulated one g and the real oneḡ. The third term measures the movement of the feet in simulation since the foot movement in the real trajectories is zero for those in D s .</p>
<p>B. Selecting Model Parameters</p>
<p>We model the biped robot as an articulated rigid body system with actuators at joints. For such a complex dynamic system, there are often too many model parameters to identify using limited amounts of real-world data. Among all the model parameters in the system, we found that the actuator is the main source of modeling error, comparing to other factors such as mass, dimensions, and joint parameters, similar to the findings in [19]. Therefore, we augment the conventional PD controller with a neural network to increase the expressiveness of the model. For each motor on the robot, the neural network model takes as input the difference between the target positionθ t and the current position of the motor θ t , denoted as ∆θ t , as well as the velocity of the motoṙ θ t , and outputs the proportional and derivative gains k p and k d . Unlike the high-end actuators used in [19], the actuators on Darwin OP2 are not capable of accurately measuring the actual torque being applied. As a result, we cannot effectively train a large neural network that outputs the actual torque. In our examples, we use a neural network model of one hidden layer with five nodes using tanh activation, shared across all motors. This results in network weights of 27 dimensions, which is denoted by φ ∈ R 27 .</p>
<p>We further modulate the differences among motors by grouping them based on their locations on the robot: g ∈ {HEAD, ARM, HIP, KNEE, ANKLE}. The final torque applied to the motor is calculated as:
τ = clip(ρ g k p ∆θ − σ g k dθ , −τ ,τ ),
where the function clip(x, b, u) returns the upper bound u or the lower bound b if x exceeds [b, u]. Otherwise, it simply returns x. We define learnable scaling factors ρ g and σ g for each group, as well as a learnable torque limitτ is the torque limit.</p>
<p>In addition to φ, σ g andτ , our method also identifies the friction coefficient between the ground and the feet and the center of mass of the robot torso. Identifying the friction coefficient is necessary because the surface in the real world can be quite different from the default surface material in the simulator. We found that the CAD model of Darwin OP2 provided by the manufacturer has reasonably accurate inertial properties at each part, except for the torso where the on-board PC, sub-controller and 5 motors reside. Thus, we include the local center of mass of the torso as an additional model parameter to identify.</p>
<p>The nominal model parametersμ we identify during pre-sysID include all the aforementioned parameters that has in total 41 dimensions. However, we fix the motor neural network weights φ and do not optimize the bounds for them. This is because neural network weights are trained to depend on each other and randomizing them independently might lead to undesired behavior. This results in the optimized parameter bounds µ lb , µ ub to have dimension of 14.</p>
<p>IV. LEARNING PROJECTED UNIVERSAL POLICY</p>
<p>We formulate the problem of learning locomotion controller as a Markov Decision Process (MDP), (S, A, T , r, p 0 , γ), where S is the state space, A is the action space, T : S × A → S is the transition function, r : S × A → R is the reward function, p 0 is the initial state distribution and γ is a discount factor. The goal of reinforcement learning is to find a policy π : S → A, such that it maximizes the accumulated reward:
J(π) = E s0,a0,...,s T T t=0 γ t r(s t , a t ),
where s 0 ∼ p 0 , a t ∼ π(s t ) and s t+1 = T (s t , a t ).</p>
<p>Our method departs from the standard policy learning by training a Universal Policy (UP) π up : (s, µ) → a explicitly conditioned on the model parameters µ [8]. Given the optimized bounds µ lb and µ ub from Pre-sysID, UP can be viewed as a family of policies, each of which is trained for a particular environment, in which the transition function T µ is parameterized by a particular µ sampled from the uniform distribution U(µ lb , µ ub ). While UP has shown success for sim-to-sim transfer with similar dimensions of µ in [7], it poses great challenges for optimization in the real world on a biped robot (Section V).</p>
<p>To overcome the high dimensionality of model parameters, we exploit the redundancy in the space of µ in terms of its impact on the policy. For example, increasing the mass of a limb will cause a similar effect on the optimal policy to increasing the torque limit of the motor connected to it. Therefore, we learn a projection model that maps µ down to a lower-dimensional latent variable η ∈ R M , where M is the dimension of the latent space (M = 3 in our experiments). We then condition the control policy directly on η, instead of µ. We connect the last layer of projection module to the policy's input layer via a tanh activation such that the weights of both the projection module and the policy are trained together using the policy learning algorithm PPO [2]. This results in a policy that can exhibit different behaviors, modulated by η. We call such policy a Projected Universal Policy (PUP): π pup : (s, η) → a.</p>
<p>V. POST-TRAINING SYSTEM IDENTIFICATION (POST-SYSID)</p>
<p>During post-training system identification (post-sysID), we search in the space of η for the optimal η * such that the conditioned policy π pup (s; η * ) achieves the best performance on the real hardware. Yu et al. [7] used CMA-ES for optimizing UP that achieves the best transfer performance and showed that it works comparably to Bayesian Optimization (BO). We use BO in this work because we found that for our problem with a low-dimensional η, BO is in general more sampleefficient than CMA.</p>
<p>The process of post-sysID starts with uniformly drawing 5 samples in the space of η to build the initial Gaussian Process (GP) model for BO. For each sampled η, we run the corresponding policy π pup on the robot to generate one trajectory and record the distance it travels before the robot loses balance. Note that the fitness function in post-sysD needs not to be the same as the reward function used for learning π pup . We chose the simplest possible fitness function that only measures the distance travelled at the end of each rollout, but it can be easily replaced by more sophisticated fitness functions if necessary. At each iteration of BO, we use the latest GP model to find the next sample point that trades off between exploiting area near a previously good sample and exploring area that the GP model is uncertain about. We run BO for 20 iterations and use the best η seen during optimization as the final output. The policy transfer process requires 25 trajectories on the hardware, and the entire process takes less than 15 minutes.</p>
<p>VI. EXPERIMENT</p>
<p>A. Experiment Setup</p>
<p>We test our algorithm on the Robotis Darwin OP2 robot. Darwin OP2 has 20 Dynamixel MX-28T servo motors in total, 2 on the head, 6 on the arms and 12 on the legs, all of which are controlled using target positions through a PID controller. We set the P gain, I gain and D gain of all the motors to be 32, 0 and 16 on the hardware. MX-28T provides decent sensing accuracy for the position and velocity of the motor. However, reading the position or velocity from all the motors takes about 10ms, which limits our control frequency when both data are used. In this work, we instead use only the positions from the motors, and provide two consecutive motor position readings to the policy to provide information about the velocities. Darwin OP2 is also equipped with an on-board IMU sensor that provides raw measurements of angular velocity and linear acceleration of the robot torso. In order to have good estimation of the orientation or the robot, we need to collect and integrate data from the IMU sensor at high frequency. However, this is not possible because the IMU and the motors share the communication port. Therefore, instead of the on-board IMU, we use the Bosch bno055 IMU sensor for estimating the orientation of the robot. With this augmentation, we can reach a control frequency of 33Hz.</p>
<p>To evaluate our approach, we train locomotion policies that control the Darwin OP2 robot to walk forward, backward and sideways in simulation and transfer to the real hardware. We first collect motion trajectories of the real robot performing generic movements. For each motor on the robot, we apply a step function action starting from a random pose and record their responses while suspending the robot to avoid ground contact. One such example can be seen in Figure 7. We use step functions of magnitude 0.1, 0.3 and 0.6 to collect motor behaviors at different speeds. In addition, we also design trajectories where the robot stands up and falls in different directions. For trajectories that involve ground contact, we also record the estimated orientation from the IMU sensor. The set of generic movements we use can be seen in the
Supplementary Video 1 .
The robot is tasked to walk on a yoga mat that lies on top of a white board, as shown in Figure 3. We choose this deformable surface to better provide protection for the robot. The performance of the policy can be viewed in the supplementary video.</p>
<p>For all examples in this paper, the state space includes the position of motors and the estimated orientation represented in Euler angles for two consecutive timesteps. The action space is defined as the target positions for each motor. To accelerate the learning process in simulation, we use a reference trajectory of robot stepping in place that was generated manually. The action of the policy is to apply adjustment to the reference trajectory:
q target = q ref + δπ pup (s; η * ),
where δ controls the magnitude of the adjustment and the policy π pup outputs a value in [−1, 1]. We use δ = 0.3 for walking forward and sideways and δ = 0.2 for walking backwards. Note that the reference trajectory does not need to be dynamically feasible, as tracking our stepping-in-place reference trajectory causes the robot to fall immediately. Similar to [27], we also discretize the action space into 11 bins in each dimension to further accelerate the policy training.</p>
<p>We use Proximal Policy Optimization (PPO) to train the control policies and use the following reward function:
r(s, a) = w v E v + w a E a + w w E w + w t E t + E c .
The first term E v = ẋ encourages the robot to move as fast as possible in the direction x. The second and third term E a = ||τ || 2 , E w = τ ·q penalize the torque and work applied to the motors, where τ is the resulting torque applied to the motor under the action a. E t = ||q t − q t ref || 2 rewards the robot to track the reference trajectory, where q t ref denotes the reference trajectory at timestep t. Finally, E c = 5 is a constant reward for not falling to the ground. We use an identical reward function with w v = 10.0, w a = 0.01, w w = 0.005, w t = 0.2 for all of the presented examples. We also use the mirror symmetry loss proposed in [3] during training of PUP, which we found to improve the quality of the learned locomotion gaits. For controlling the robot to walk in different directions, we rotate the robot's coordinate frame such that the desired walking direction is aligned with the positive x-axis in the robot frame.</p>
<p>B. Baselines</p>
<p>We evaluate our method by comparing it with two baselines. For the first baseline, we optimize for a single model µ during Pre-sysID instead of a range of µ, and use the model to train a policy. We denote this baseline "Nominal". The second baseline uses the range of µ computed by Pre-sysID and trains a robust policy through domain randomization with that range. We denote the second baseline "Robust".</p>
<p>To account for uncertainty in the sensors and networking, we model additional noise in the simulation during policy training for our method and the two baselines. Specifically, we randomly set the control frequency to be in [25,33]Hz for each rollout, add a bias to the estimated orientation drawn from U(−0.3, 0.3) and add a Gaussian noise of standard deviation 0.01 to the observed motor position. In addition, we add noise drawn from U(−0.25, 0.25) to the input µ during the training of PUP to improve the robustness of the policy. Figure 4 and Figure 5 show the comparison between our method and the baselines. We evaluate a trained policy by running it 5 times on the real hardware, and measure the distance and time before the robot loses balance or reaches the end of the power cable. Our method clearly outperforms the baselines and is the only method that can control the robot to walk to, and occasionally beyond, the edge of the white board (at 0.8m). Because PUP is trained to be specialized for different environments, it learns to take larger steps than Robust, which tends to take conservative actions. This results in a faster walking gait, and may also have contributed to the larger variance seen in our policy. An illustration of the three locomotion tasks with our trained policies can be seen in Figure 3.</p>
<p>C. Performance on Locomotion Tasks</p>
<p>D. Choice of Motor Dynamics Modeling</p>
<p>Our method models the motor dynamics as a neural network paired with a PD controller. Here we examine the necessity of having this additional components in the model. Specifically, we perform system identification procedure as described in Section III to identify two models, one with neural networks (NN-PD) and one without (PD only). We  compare their overall sysID loss on the training set, as shown in Figure 6. We can see that NN-PD is able to achieve a notably better loss compared to PD only. To further demonstrate the behavior of the two models, we plot the simulated motor position for the hip joint when a step function is applied, and the estimated pitch of the torso when a trajectory controls the robot to fall forward, as shown in Figure 7 (a) and (b). We can see that NN-PD is able to better reproduce the overall behavior than PD only. Figure 8 visualizes µ lb and µ ub identified by the pre-sysID stage. We normalize the search range of each model parameter to be in [0, 1] and show the identified bounds as the blue bars. The red lines indicate the nominal parameterŝ µ optimized using the entire set of pre-sysID trajectories. Some parameters, such as σ ankle , have tighter bounds, which indicate higher confidence in the optimized values for those parameters. The parameters with wider range indicate that no single value of µ can explain all the training trajectories well and naively using the nominal values for these parameters may lead to poor transfer performance. One example of such parameters is the bounds forτ . Upon further examination, we  found that the identifiedτ tends to be bipolar depending on whether the subset of training trajectories involves contact or not. These phenomena suggest that our current model is still not expressive enough to explain all the real-world observations and further improvement in modeling may be necessary for transferring more challenging tasks. We also note that, partially due to the use of generic motions for pre-sysID, the identified bounds are not necessarily useful for the tasks of interest. For example, the parameters associated with the head, ρ head and σ head , have wide bounds but their impact to locomotion tasks is relatively small. During the training of PUP, the projection module will learn to ignore the variations in these parameters.</p>
<p>E. Identified Model Parameter Bounds</p>
<p>VII. DISCUSSION AND CONCLUSION</p>
<p>We have proposed a transfer learning algorithm for learning robotic controllers in physics simulation and applying them to the real hardware. The key idea in our method is to perform system identification in two stages connected by a Projected Universal Policy, whose behavior is modulated by a low dimensional latent variable. We demonstrate our method on training locomotion policies for the Darwin OP2 robot to walk forward, backward and sideways and transfer to the real robot using 25 trials on the hardware.</p>
<p>During post-sysID, our method uses additional taskrelevant data to help identify the optimal conditioned policy π pup (s; η * ). To provide a fair comparison, one could also use task-relevant data to further improve the baseline policies. However, to fully take advantage of these trajectories for policy learning, one would need to upgrade sensor instrumentation for measuring global position and orientation needed in reward function evaluation. In contrast, our method only uses these trajectories for post-sysID with very simple fitness function that only measures the traveling distance and the elapsed time. In addition, the size of the task-relevant data (less than 2500 steps) is only enough to perform one iteration of PPO in a typical setting. In comparison, we use 20, 000 steps per learning iteration in simulation. For those reasons, we do not believe that such a small amount of task-relevant data can further improve the results of baseline methods in our experiments. The set of model parameters µ used in our work is currently chosen manually based on prior knowledge about the robot and the tasks of interest. Although it works for Darwin OP2 learning locomotion tasks, it is not clear as to how well it can be applied to different robotic hardware or different tasks, such as picking up objects or climbing ladders. Investigating a systematic and automatic way to select model parameters would be an interesting future research direction.</p>
<p>Fig. 1 .
1The simulated and the real Darwin OP2 robot trained to walk sideways.</p>
<p>Fig. 2 .
2Overview of the proposed algorithm.</p>
<p>Fig. 3 .
3Illustration of locomotion policies deployed on the Darwin OP2 robot. Top: walk forward. Middle: walk backward. Bottom: walk sideways.</p>
<p>Fig. 4 .
4Comparison of the distance travelled by the robot using our method and the baselines. Error bars indicate one standard deviation from five runs of the same policy.</p>
<p>Fig. 5 .
5Comparison of the elapsed time before the robot loses balance using our method and baselines. Error bars indicate one standard deviation from five runs of the same policy.</p>
<p>Fig. 6 .
6Comparison of system identification loss between NN-PD and PD only.</p>
<p>Fig. 7 .
7Comparison of NN-PD and PD only on (a) the hip motor position during step function command with magnitude 0.1 and (b) torso pitch during falling forward motion.</p>
<p>Fig. 8 .
8Identified model parameter bounds (blue bars) and the nominal parameters (red lines).
https://youtu.be/bq8xZgbLHcw</p>
<p>Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. X B Peng, P Abbeel, S Levine, M Van De Panne, Proc. SIGGRAPH 2018). SIGGRAPH 2018)X. B. Peng, P. Abbeel, S. Levine, and M. van de Panne, "Deepmimic: Example-guided deep reinforcement learning of physics-based charac- ter skills," ACM Transactions on Graphics (Proc. SIGGRAPH 2018), 2018.</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Learning symmetric and low-energy locomotion. W Yu, G Turk, C K Liu, Proc. SIGGRAPH 2018). SIGGRAPH 2018)37W. Yu, G. Turk, and C. K. Liu, "Learning symmetric and low-energy locomotion," ACM Transactions on Graphics (Proc. SIGGRAPH 2018), vol. 37, no. 4, 2018.</p>
<p>Trust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, International Conference on Machine Learning. J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, "Trust region policy optimization," in International Conference on Machine Learning, 2015, pp. 1889-1897.</p>
<p>Why off-the-shelf physics simulators fail in evaluating feedback controller performance-a case study for quadrupedal robots. M Neunert, T Boaventura, J Buchli, Advances in Cooperative Robotics. World ScientificM. Neunert, T. Boaventura, and J. Buchli, "Why off-the-shelf physics simulators fail in evaluating feedback controller performance-a case study for quadrupedal robots," in Advances in Cooperative Robotics. World Scientific, 2017, pp. 464-472.</p>
<p>Transfer Learning for Reinforcement Learning Domains : A Survey. M E Taylor, P Stone, 10M. E. Taylor and P. Stone, "Transfer Learning for Reinforcement Learning Domains : A Survey," vol. 10, pp. 1633-1685, 2009.</p>
<p>Policy transfer with strategy optimization. W Yu, C K Liu, G Turk, International Conference on Learning Representations. W. Yu, C. K. Liu, and G. Turk, "Policy transfer with strategy optimization," in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/ forum?id=H1g6osRcFQ</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, J Tan, C K Liu, G Turk, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsCambridge, MassachusettsW. Yu, J. Tan, C. K. Liu, and G. Turk, "Preparing for the unknown: Learning a universal policy with online system identification," in Pro- ceedings of Robotics: Science and Systems, Cambridge, Massachusetts, July 2017.</p>
<p>Epopt: Learning robust neural network policies using model ensembles. A Rajeswaran, S Ghotra, B Ravindran, S Levine, A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine, "Epopt: Learning robust neural network policies using model ensembles." ICLR, 2017.</p>
<p>Reducing Hardware Experiments for Model Learning and Policy Optimization. S Ha, K Yamane, IROS. S. Ha and K. Yamane, "Reducing Hardware Experiments for Model Learning and Policy Optimization," IROS, 2015.</p>
<p>Adversarially Robust Policy Learning : Active Construction of Physically-Plausible Perturbations. A Mandlekar, Y Zhu, A Garg, L Fei-Fei, S Savarese, A. Mandlekar, Y. Zhu, A. Garg, L. Fei-fei, and S. Savarese, "Adver- sarially Robust Policy Learning : Active Construction of Physically- Plausible Perturbations." IROS, 2017.</p>
<p>Learning Invariant Feature Spaces To Transfer Skills With Reinforcement Learning. A Gupta, C Devin, Y Liu, P Abbeel, S Levine, C Science, A. Gupta, C. Devin, Y. Liu, P. Abbeel, S. Levine, and C. Science, "Learning Invariant Feature Spaces To Transfer Skills With Reinforce- ment Learning," no. 2008, pp. 1-14, 2017.</p>
<p>Robust adversarial reinforcement learning. L Pinto, J Davidson, R Sukthankar, A Gupta, L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, "Robust adver- sarial reinforcement learning." ICML, 2017.</p>
<p>System Identification. L Ljung, 10.1007/978-1-4612-1768-8_11Birkhäuser BostonBoston, MAL. Ljung, System Identification. Boston, MA: Birkhäuser Boston, 1998, pp. 163-173. [Online]. Available: https://doi.org/10.1007/ 978-1-4612-1768-8 11</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsPittsburgh, PennsylvaniaJ. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bo- hez, and V. Vanhoucke, "Sim-to-real: Learning agile locomotion for quadruped robots," in Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018.</p>
<p>Simulation-based design of dynamic controllers for humanoid balancing. J Tan, Z Xie, B Boots, C K Liu, Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEEJ. Tan, Z. Xie, B. Boots, and C. K. Liu, "Simulation-based design of dynamic controllers for humanoid balancing," in Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE, 2016, pp. 2729-2736.</p>
<p>Exploration and Apprenticeship Learning in Reinforcement Learning. P Abbeel, A Y Ng, International Conference on Machine Learning. P. Abbeel and A. Y. Ng, "Exploration and Apprenticeship Learning in Reinforcement Learning," in International Conference on Machine Learning, 2005, pp. 1-8.</p>
<p>Pilco: A model-based and data-efficient approach to policy search. M Deisenroth, C E Rasmussen, Proceedings of the 28th International Conference on machine learning. the 28th International Conference on machine learningM. Deisenroth and C. E. Rasmussen, "Pilco: A model-based and data-efficient approach to policy search," in Proceedings of the 28th International Conference on machine learning (ICML-11), 2011, pp. 465-472.</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 4265872J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter, "Learning agile and dynamic motor skills for legged robots," Science Robotics, vol. 4, no. 26, p. eaau5872, 2019.</p>
<p>Identification and Dynamic Model of a Bipedal Robot With a Cable-Differential-Based Compliant Drivetrain. H Park, K Sreenath, J W Hurst, J W Grizzle, H.-w. Park, K. Sreenath, J. W. Hurst, and J. W. Grizzle, "Identification and Dynamic Model of a Bipedal Robot With a Cable-Differential- Based Compliant Drivetrain," pp. 1-17.</p>
<p>Sim-to-Real Transfer with Neural-Augmented Robot Simulation. F Golemo, A A Taïga, no. CoRL. F. Golemo and A. A. Taïga, "Sim-to-Real Transfer with Neural- Augmented Robot Simulation," no. CoRL, 2018.</p>
<p>Ensemble-CIO : Full-Body Dynamic Motion Planning that Transfers to Physical Humanoids. I Mordatch, K Lowrey, E Todorov, I. Mordatch, K. Lowrey, and E. Todorov, "Ensemble-CIO : Full-Body Dynamic Motion Planning that Transfers to Physical Humanoids."</p>
<p>Reinforcement learning for non-prehensile manipulation : Transfer from simulation to physical system. K Lowrey, S Kolev, J Dao, A Rajeswaran, E Todorov, SIMPARK. Lowrey, S. Kolev, J. Dao, A. Rajeswaran, and E. Todorov, "Re- inforcement learning for non-prehensile manipulation : Transfer from simulation to physical system." SIMPAR, 2018.</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, Advances in Artificial Life. F. Morán, A. Moreno, J. J. Merelo, and P. ChacónBerlin, Heidelberg; Berlin HeidelbergSpringerN. Jakobi, P. Husbands, and I. Harvey, "Noise and the reality gap: The use of simulation in evolutionary robotics," in Advances in Artificial Life, F. Morán, A. Moreno, J. J. Merelo, and P. Chacón, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 1995, pp. 704-720.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, Intelligent Robots and Systems (IROS). J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017, pp. 23-30.</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE International Conference on Robotics and Automation (ICRA). X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, "Sim-to- real transfer of robotic control with dynamics randomization," in 2018 IEEE International Conference on Robotics and Automation (ICRA).</p>
<p>Learning Dexterous In-Hand Manipulation. : Openai, M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, J Schneider, S Sidor, J Tobin, P Welinder, L Weng, W Zaremba, ArXiv e-printsOpenAI, :, M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba, "Learning Dexterous In-Hand Manipulation," ArXiv e-prints, Aug. 2018.</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Vecerik, T Rothörl, N Heess, R Pascanu, R Hadsell, A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell, "Sim-to-real robot learning from pixels with progressive nets." CORL, 2017.</p>
<p>Closing the Sim-to-Real Loop : Adapting Simulation Randomization with Real World Experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, ICRAY. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox, "Closing the Sim-to-Real Loop : Adapting Simulation Randomization with Real World Experience." ICRA, 2019.</p>
<p>Hardware conditioned policies for multi-robot transfer learning. T Chen, A Murali, A Gupta, NIPS. T. Chen, A. Murali, and A. Gupta, "Hardware conditioned policies for multi-robot transfer learning." NIPS, 2018.</p>
<p>Robots that can adapt like animals. A Cully, J Clune, D Tarapore, J.-B Mouret, Nature. 5217553503A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, "Robots that can adapt like animals," Nature, vol. 521, no. 7553, p. 503, 2015.</p>
<p>Learning to Walk via Deep Reinforcement Learning. T Haarnoja, A Zhou, S Ha, J Tan, G Tucker, S Levine, L G Dec, T. Haarnoja, A. Zhou, S. Ha, J. Tan, G. Tucker, S. Levine, and L. G. Dec, "Learning to Walk via Deep Reinforcement Learning."</p>
<p>Automated Deep Reinforcement Learning Environment for Hardware of a Modular Legged Robot. S Ha, J Kim, K Yamane, S. Ha, J. Kim, and K. Yamane, "Automated Deep Reinforcement Learning Environment for Hardware of a Modular Legged Robot."</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, abs/1801.01290CoRR. T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor," CoRR, vol. abs/1801.01290, 2018. [Online].</p>
<p>On the adaptation of arbitrary normal mutation distributions in evolution strategies: The generating set adaptation. N Hansen, A Ostermeier, A Gawelczyk, ICGA. N. Hansen, A. Ostermeier, and A. Gawelczyk, "On the adaptation of arbitrary normal mutation distributions in evolution strategies: The generating set adaptation." in ICGA, 1995, pp. 57-64.</p>            </div>
        </div>

    </div>
</body>
</html>
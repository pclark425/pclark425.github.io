<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-315 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-315</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-315</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-f86fb205cd4d85478e65304fc38bdf1e4bed2440</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f86fb205cd4d85478e65304fc38bdf1e4bed2440" target="_blank">Pre-trained Large Language Models Use Fourier Features to Compute Addition</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain, demonstrating that appropriate pre-trained representations can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e315.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e315.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-XL (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-XL (pre-trained, then fine-tuned on synthetic addition dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained 48-layer decoder-only Transformer (≈1.5B params) fine-tuned to add two integers ≤260; analysis shows the model represents numbers and intermediate logits with sparse Fourier components and uses complementary roles of MLP vs attention to compute sums.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>integer addition (pairwise addition of two integers; analysis also considers modular addition/mod c for c∈{2,5,10,...})</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>operands ≤ 260 (output space p = 521, sums up to 520)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on a synthetic addition dataset (train/validation/test split) + per-layer Logit Lens extraction; discrete Fourier transform of logits/token embeddings; targeted low-pass/high-pass causal ablations (linear projection filters) on MLP/attention outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>99.74% test accuracy after fine-tuning on the synthetic addition dataset (residual-stream logits examined per-layer with Logit Lens)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>The model encodes numbers and intermediate logits with a sparse set of Fourier basis components (notably periods/frequencies corresponding to T ≈ 2, 2.5, 5, 10 and low-frequency components). Mechanistically, MLP modules primarily implement a low-frequency 'approximation' subtask (promoting numbers near the correct magnitude) using large-period / low-frequency Fourier components, while attention modules primarily implement high-frequency modular classification (e.g., computing correct remainder modulo small moduli such as 2,5,10) using sparse high-frequency Fourier components. The final logits are a superposition of a few dominant Fourier waves: low-frequency components set coarse magnitude and high-frequency components resolve modular/unit digits; shifting phase of sin/cos coefficients allows the model to align peaks with the correct numeric value. Causal interventions (removing high- or low-frequency Fourier components via projection filters) confirm these roles: removing MLP low-frequency components breaks magnitude estimation (large off-by-10/50/100 errors) while removing attention high-frequency components breaks unit-digit/modular classification (small errors <6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Pre-training is crucial: pre-trained representations (token embeddings) provide Fourier features that enable near-perfect fine-tuned performance; models trained from scratch on the same dataset perform worse (see comparison entries). No general 'model-size scaling' curve is provided beyond comparing specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When relevant Fourier components are ablated, characteristic errors appear: ablating MLP low-frequency components produces off-by-10 / off-by-50 / off-by-100 magnitude errors; ablating attention high-frequency components leads to small unit-digit errors (magnitude <6). Without pre-training, models tend to produce off-by-1 errors and lack modular (high-frequency) components.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared pre-trained+fine-tuned vs randomly-initialized (trained-from-scratch) models; also ablated specific Fourier bands from MLP vs attention outputs (low-pass vs high-pass) to test causal role; compared effect of freezing/injecting pre-trained token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Pre-trained LLMs compute addition by representing numbers with sparse Fourier features; MLP layers use low-frequency Fourier features to approximate magnitude while attention layers use high-frequency Fourier features to compute modular/unit-digit information, and pre-trained token embeddings supply the Fourier basis necessary for precise arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e315.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e315.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-XL (trained from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-XL (randomly initialized, trained only on the synthetic addition dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same architecture as above but trained from random initialization on the addition task only; lacks the Fourier-feature mechanism in token embeddings and intermediate logits and attains lower accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL (scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>integer addition (same synthetic task)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>operands ≤ 260 (output space p = 521)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>trained from random initialization on the same synthetic addition dataset (no pre-training); Fourier analysis of token embeddings and intermediate logits</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>94.44% test accuracy after convergence (worse than fine-tuned pre-trained model's 99.74%)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>This model does not learn the high-frequency sparse Fourier components seen in pre-trained models: token embeddings and intermediate logits show no outlier high-frequency components. The network primarily uses low-frequency approximation features and therefore fails to perform accurate modular classification (units digit), producing characteristic off-by-one errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Shows that training from scratch on this small synthetic dataset yields lower final accuracy and different internal representations compared to pre-trained initialization; no evidence of the same Fourier-feature decomposition emerging.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Frequent off-by-one errors and general inability to reliably predict unit digits; lacks modular (high-frequency) classification</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to the pre-trained GPT-2-XL fine-tuned model; injecting pre-trained token embeddings (see other entries) remedies the deficit.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Without pre-training, the Transformer does not develop the sparse high-frequency Fourier features needed for modular addition and therefore attains lower accuracy, relying instead on coarse low-frequency magnitude approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e315.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e315.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-small (scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-small (randomly initialized, trained only on the synthetic addition dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small decoder-only Transformer (≈124M params) trained from scratch on the addition dataset; struggles to learn the task and does not develop Fourier token-embedding features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-small (scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>integer addition (same synthetic task)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>operands ≤ 260 (output space p = 521)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>trained from random initialization on the synthetic addition dataset; experiments include freezing or replacing token embedding layer with pre-trained embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>53.95% test accuracy when trained from scratch (mean across seeds reported)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Token embeddings learned from scratch lack the Fourier components observed in pre-trained models; the small model fails to reliably learn modular classification and magnitude approximation needed for accurate addition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Small model trained from scratch performs poorly on this task; performance improves drastically if given pre-trained token embeddings (see next entry).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Low overall accuracy; inability to perform reliable modular classification or magnitude approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to same architecture but with pre-trained token embeddings frozen (see next entry).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Small Transformers trained from scratch often cannot learn the Fourier-based mechanism for precise addition and perform poorly on the task unless given pre-trained embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e315.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e315.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-small + pre-trained embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-small (random init except frozen pre-trained token embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-2-small with the token embedding matrix initialized from a pre-trained model and frozen; injecting pre-trained embeddings supplies Fourier features and enables perfect learning of the addition task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-small (with pre-trained embeddings frozen)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>integer addition (same synthetic task)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>operands ≤ 260 (output space p = 521)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>freeze token embedding layer initialized from a pre-trained model; train remaining weights from random initialization on the synthetic addition dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>100% test accuracy (across 5 seeds) and significantly faster convergence when pre-trained token embeddings are frozen and used</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Pre-trained token embeddings carry sparse Fourier features (periods around 2, 2.5, 5, 10) which act as an inductive bias; with those embeddings available, the randomly initialized network can learn to use residual-layer computations (MLP and attention) to combine those Fourier features into exact addition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Demonstrates that pre-trained embeddings are a strong inductive bias: small models can reach perfect accuracy when given pre-trained Fourier-rich embeddings even if other weights are random.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to identical model trained from scratch without pre-trained embeddings (53.95% accuracy), showing the embeddings rescue performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Pre-trained token embeddings containing Fourier components are sufficient to enable small randomly-initialized Transformers to learn exact addition quickly and perfectly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e315.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e315.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B (open-source, 6B parameters, 4-shot in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6B decoder-only model evaluated with 4-shot in-context examples on the addition dataset; behavior and internal logits show evidence of Fourier components driving modular errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>integer addition via in-context learning (4-shot); modular/addition errors analyzed</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>operands ≤ 260 (same synthetic task)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>4-shot in-context prompting (no fine-tuning); Fourier analysis of intermediate logits across last layers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Absolute errors are multiples of 10 in 93% of erroneous cases (behavioral error distribution reported); Fourier-space analysis of MLP and attention logits shows outlier components with periods ≈2, 2.5, 5, 10 across last layers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Open-source prompted GPT-J exhibits approximate sparsity of the same Fourier components in MLP and attention outputs as seen in fine-tuned GPT-2-XL, suggesting it uses Fourier features for modular classification and magnitude approximation during in-context arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Predominant errors are multiples of 10 (suggesting failures in low-frequency magnitude alignment or mis-phased high-frequency components that resolve unit digits).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared behavioral/error patterns to Phi-2 and to fine-tuned GPT-2-XL internal-analysis results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>When used in-context (4-shot), GPT-J shows the same Fourier-feature signatures and an error distribution consistent with reliance on Fourier-based modular/classification components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e315.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e315.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-2 (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-2 (2.7B, open-source, 4-shot in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Phi-2 evaluated with 4-shot in-context examples; shows internal-logit Fourier sparsity and error patterns consistent with Fourier-based arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>integer addition via in-context learning (4-shot); modular aspects analyzed</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>operands ≤ 260</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>4-shot in-context prompting; Fourier transform of MLP and attention logits across last layers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Absolute errors are multiples of 10 in 73% of erroneous cases; Fourier analysis reveals outlier components with periods ≈2, 2.5, 5, 10 in last 15 layers (MLP and attention).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Behavior and internal-logit Fourier sparsity mirror the fine-tuned GPT-2-XL pattern, indicating the model uses high-frequency components for modular classification and low-frequency ones for magnitude approximation during in-context arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Errors frequently multiples of 10 (consistent with misalignment of low-frequency magnitude or phase of high-frequency components).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared error-distribution and Fourier signatures with GPT-J and fine-tuned GPT-2-XL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Prompted Phi-2 shows the same Fourier-feature-based mechanism for addition as pre-trained/fine-tuned models, supporting generality across models and in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e315.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e315.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 / GPT-4 / PaLM-2 (closed-source behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5, GPT-4, and PaLM-2 (closed-source instruction-tuned LLMs; behavior-only analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source instruction-tuned models were behaviorally evaluated (0-shot) on the addition dataset; error distributions suggest reliance on Fourier-like modular mechanisms although internal activations were not accessible.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (instruction-tuned) (not formally analyzed internally)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>integer addition (0-shot prompting due to instruction tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>operands ≤ 260</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>0-shot prompting (instruction-tuned behavior); error analysis of absolute errors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When errors occur, they are multiples of 10 in 100% of the time for GPT-3.5 and GPT-4 (among reported errors) and 87% for PaLM-2; exact overall accuracy numbers not reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Although internal activations are not accessible, the strong clustering of errors into multiples-of-10 suggests these models may also rely on Fourier-like modular features (coarse magnitude + modular classification) analogous to those identified in open-source pre-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Errors predominantly multiples of 10, indicating systematic failures in resolving lower-order digits or phase alignment between low/high-frequency components.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Behavioral error-pattern comparison between closed-source models and open-source models (GPT-J, Phi-2) and fine-tuned GPT-2-XL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Behavior-only evidence from closed-source LLMs (0-shot) is consistent with the hypothesis that large instruction-tuned LLMs use Fourier-feature-like mechanisms for arithmetic, because their errors concentrate on multiples of 10.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model <em>(Rating: 2)</em></li>
                <li>Harmonics of learning: Universal fourier features emerge in invariant networks <em>(Rating: 2)</em></li>
                <li>Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic <em>(Rating: 2)</em></li>
                <li>Feature emergence via margin maximization: case studies in algebraic tasks <em>(Rating: 1)</em></li>
                <li>Transformers learn in-context by gradient descent <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-315",
    "paper_id": "paper-f86fb205cd4d85478e65304fc38bdf1e4bed2440",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-2-XL (fine-tuned)",
            "name_full": "GPT-2-XL (pre-trained, then fine-tuned on synthetic addition dataset)",
            "brief_description": "Pre-trained 48-layer decoder-only Transformer (≈1.5B params) fine-tuned to add two integers ≤260; analysis shows the model represents numbers and intermediate logits with sparse Fourier components and uses complementary roles of MLP vs attention to compute sums.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-XL",
            "model_size": "1.5B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "integer addition (pairwise addition of two integers; analysis also considers modular addition/mod c for c∈{2,5,10,...})",
            "number_range_or_complexity": "operands ≤ 260 (output space p = 521, sums up to 520)",
            "method_or_intervention": "fine-tuning on a synthetic addition dataset (train/validation/test split) + per-layer Logit Lens extraction; discrete Fourier transform of logits/token embeddings; targeted low-pass/high-pass causal ablations (linear projection filters) on MLP/attention outputs",
            "performance_result": "99.74% test accuracy after fine-tuning on the synthetic addition dataset (residual-stream logits examined per-layer with Logit Lens)",
            "mechanistic_insight": "The model encodes numbers and intermediate logits with a sparse set of Fourier basis components (notably periods/frequencies corresponding to T ≈ 2, 2.5, 5, 10 and low-frequency components). Mechanistically, MLP modules primarily implement a low-frequency 'approximation' subtask (promoting numbers near the correct magnitude) using large-period / low-frequency Fourier components, while attention modules primarily implement high-frequency modular classification (e.g., computing correct remainder modulo small moduli such as 2,5,10) using sparse high-frequency Fourier components. The final logits are a superposition of a few dominant Fourier waves: low-frequency components set coarse magnitude and high-frequency components resolve modular/unit digits; shifting phase of sin/cos coefficients allows the model to align peaks with the correct numeric value. Causal interventions (removing high- or low-frequency Fourier components via projection filters) confirm these roles: removing MLP low-frequency components breaks magnitude estimation (large off-by-10/50/100 errors) while removing attention high-frequency components breaks unit-digit/modular classification (small errors &lt;6).",
            "performance_scaling": "Pre-training is crucial: pre-trained representations (token embeddings) provide Fourier features that enable near-perfect fine-tuned performance; models trained from scratch on the same dataset perform worse (see comparison entries). No general 'model-size scaling' curve is provided beyond comparing specific models.",
            "failure_modes": "When relevant Fourier components are ablated, characteristic errors appear: ablating MLP low-frequency components produces off-by-10 / off-by-50 / off-by-100 magnitude errors; ablating attention high-frequency components leads to small unit-digit errors (magnitude &lt;6). Without pre-training, models tend to produce off-by-1 errors and lack modular (high-frequency) components.",
            "comparison_baseline": "Compared pre-trained+fine-tuned vs randomly-initialized (trained-from-scratch) models; also ablated specific Fourier bands from MLP vs attention outputs (low-pass vs high-pass) to test causal role; compared effect of freezing/injecting pre-trained token embeddings.",
            "key_finding": "Pre-trained LLMs compute addition by representing numbers with sparse Fourier features; MLP layers use low-frequency Fourier features to approximate magnitude while attention layers use high-frequency Fourier features to compute modular/unit-digit information, and pre-trained token embeddings supply the Fourier basis necessary for precise arithmetic.",
            "uuid": "e315.0",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-XL (trained from scratch)",
            "name_full": "GPT-2-XL (randomly initialized, trained only on the synthetic addition dataset)",
            "brief_description": "Same architecture as above but trained from random initialization on the addition task only; lacks the Fourier-feature mechanism in token embeddings and intermediate logits and attains lower accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-XL (scratch)",
            "model_size": "1.5B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "integer addition (same synthetic task)",
            "number_range_or_complexity": "operands ≤ 260 (output space p = 521)",
            "method_or_intervention": "trained from random initialization on the same synthetic addition dataset (no pre-training); Fourier analysis of token embeddings and intermediate logits",
            "performance_result": "94.44% test accuracy after convergence (worse than fine-tuned pre-trained model's 99.74%)",
            "mechanistic_insight": "This model does not learn the high-frequency sparse Fourier components seen in pre-trained models: token embeddings and intermediate logits show no outlier high-frequency components. The network primarily uses low-frequency approximation features and therefore fails to perform accurate modular classification (units digit), producing characteristic off-by-one errors.",
            "performance_scaling": "Shows that training from scratch on this small synthetic dataset yields lower final accuracy and different internal representations compared to pre-trained initialization; no evidence of the same Fourier-feature decomposition emerging.",
            "failure_modes": "Frequent off-by-one errors and general inability to reliably predict unit digits; lacks modular (high-frequency) classification",
            "comparison_baseline": "Compared directly to the pre-trained GPT-2-XL fine-tuned model; injecting pre-trained token embeddings (see other entries) remedies the deficit.",
            "key_finding": "Without pre-training, the Transformer does not develop the sparse high-frequency Fourier features needed for modular addition and therefore attains lower accuracy, relying instead on coarse low-frequency magnitude approximation.",
            "uuid": "e315.1",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-small (scratch)",
            "name_full": "GPT-2-small (randomly initialized, trained only on the synthetic addition dataset)",
            "brief_description": "A small decoder-only Transformer (≈124M params) trained from scratch on the addition dataset; struggles to learn the task and does not develop Fourier token-embedding features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-small (scratch)",
            "model_size": "124M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "integer addition (same synthetic task)",
            "number_range_or_complexity": "operands ≤ 260 (output space p = 521)",
            "method_or_intervention": "trained from random initialization on the synthetic addition dataset; experiments include freezing or replacing token embedding layer with pre-trained embeddings",
            "performance_result": "53.95% test accuracy when trained from scratch (mean across seeds reported)",
            "mechanistic_insight": "Token embeddings learned from scratch lack the Fourier components observed in pre-trained models; the small model fails to reliably learn modular classification and magnitude approximation needed for accurate addition.",
            "performance_scaling": "Small model trained from scratch performs poorly on this task; performance improves drastically if given pre-trained token embeddings (see next entry).",
            "failure_modes": "Low overall accuracy; inability to perform reliable modular classification or magnitude approximation.",
            "comparison_baseline": "Compared to same architecture but with pre-trained token embeddings frozen (see next entry).",
            "key_finding": "Small Transformers trained from scratch often cannot learn the Fourier-based mechanism for precise addition and perform poorly on the task unless given pre-trained embeddings.",
            "uuid": "e315.2",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-small + pre-trained embeddings",
            "name_full": "GPT-2-small (random init except frozen pre-trained token embeddings)",
            "brief_description": "GPT-2-small with the token embedding matrix initialized from a pre-trained model and frozen; injecting pre-trained embeddings supplies Fourier features and enables perfect learning of the addition task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-small (with pre-trained embeddings frozen)",
            "model_size": "124M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "integer addition (same synthetic task)",
            "number_range_or_complexity": "operands ≤ 260 (output space p = 521)",
            "method_or_intervention": "freeze token embedding layer initialized from a pre-trained model; train remaining weights from random initialization on the synthetic addition dataset",
            "performance_result": "100% test accuracy (across 5 seeds) and significantly faster convergence when pre-trained token embeddings are frozen and used",
            "mechanistic_insight": "Pre-trained token embeddings carry sparse Fourier features (periods around 2, 2.5, 5, 10) which act as an inductive bias; with those embeddings available, the randomly initialized network can learn to use residual-layer computations (MLP and attention) to combine those Fourier features into exact addition.",
            "performance_scaling": "Demonstrates that pre-trained embeddings are a strong inductive bias: small models can reach perfect accuracy when given pre-trained Fourier-rich embeddings even if other weights are random.",
            "failure_modes": null,
            "comparison_baseline": "Compared to identical model trained from scratch without pre-trained embeddings (53.95% accuracy), showing the embeddings rescue performance.",
            "key_finding": "Pre-trained token embeddings containing Fourier components are sufficient to enable small randomly-initialized Transformers to learn exact addition quickly and perfectly.",
            "uuid": "e315.3",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-J (prompted)",
            "name_full": "GPT-J-6B (open-source, 6B parameters, 4-shot in-context)",
            "brief_description": "A 6B decoder-only model evaluated with 4-shot in-context examples on the addition dataset; behavior and internal logits show evidence of Fourier components driving modular errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J",
            "model_size": "6B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "integer addition via in-context learning (4-shot); modular/addition errors analyzed",
            "number_range_or_complexity": "operands ≤ 260 (same synthetic task)",
            "method_or_intervention": "4-shot in-context prompting (no fine-tuning); Fourier analysis of intermediate logits across last layers",
            "performance_result": "Absolute errors are multiples of 10 in 93% of erroneous cases (behavioral error distribution reported); Fourier-space analysis of MLP and attention logits shows outlier components with periods ≈2, 2.5, 5, 10 across last layers.",
            "mechanistic_insight": "Open-source prompted GPT-J exhibits approximate sparsity of the same Fourier components in MLP and attention outputs as seen in fine-tuned GPT-2-XL, suggesting it uses Fourier features for modular classification and magnitude approximation during in-context arithmetic.",
            "performance_scaling": null,
            "failure_modes": "Predominant errors are multiples of 10 (suggesting failures in low-frequency magnitude alignment or mis-phased high-frequency components that resolve unit digits).",
            "comparison_baseline": "Compared behavioral/error patterns to Phi-2 and to fine-tuned GPT-2-XL internal-analysis results.",
            "key_finding": "When used in-context (4-shot), GPT-J shows the same Fourier-feature signatures and an error distribution consistent with reliance on Fourier-based modular/classification components.",
            "uuid": "e315.4",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Phi-2 (prompted)",
            "name_full": "Phi-2 (2.7B, open-source, 4-shot in-context)",
            "brief_description": "Phi-2 evaluated with 4-shot in-context examples; shows internal-logit Fourier sparsity and error patterns consistent with Fourier-based arithmetic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Phi-2",
            "model_size": "2.7B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "integer addition via in-context learning (4-shot); modular aspects analyzed",
            "number_range_or_complexity": "operands ≤ 260",
            "method_or_intervention": "4-shot in-context prompting; Fourier transform of MLP and attention logits across last layers",
            "performance_result": "Absolute errors are multiples of 10 in 73% of erroneous cases; Fourier analysis reveals outlier components with periods ≈2, 2.5, 5, 10 in last 15 layers (MLP and attention).",
            "mechanistic_insight": "Behavior and internal-logit Fourier sparsity mirror the fine-tuned GPT-2-XL pattern, indicating the model uses high-frequency components for modular classification and low-frequency ones for magnitude approximation during in-context arithmetic.",
            "performance_scaling": null,
            "failure_modes": "Errors frequently multiples of 10 (consistent with misalignment of low-frequency magnitude or phase of high-frequency components).",
            "comparison_baseline": "Compared error-distribution and Fourier signatures with GPT-J and fine-tuned GPT-2-XL.",
            "key_finding": "Prompted Phi-2 shows the same Fourier-feature-based mechanism for addition as pre-trained/fine-tuned models, supporting generality across models and in-context learning.",
            "uuid": "e315.5",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5 / GPT-4 / PaLM-2 (closed-source behavior)",
            "name_full": "GPT-3.5, GPT-4, and PaLM-2 (closed-source instruction-tuned LLMs; behavior-only analysis)",
            "brief_description": "Closed-source instruction-tuned models were behaviorally evaluated (0-shot) on the addition dataset; error distributions suggest reliance on Fourier-like modular mechanisms although internal activations were not accessible.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, GPT-4, PaLM-2",
            "model_size": null,
            "model_architecture": "decoder-only transformer (instruction-tuned) (not formally analyzed internally)",
            "arithmetic_operation_type": "integer addition (0-shot prompting due to instruction tuning)",
            "number_range_or_complexity": "operands ≤ 260",
            "method_or_intervention": "0-shot prompting (instruction-tuned behavior); error analysis of absolute errors",
            "performance_result": "When errors occur, they are multiples of 10 in 100% of the time for GPT-3.5 and GPT-4 (among reported errors) and 87% for PaLM-2; exact overall accuracy numbers not reported in paper",
            "mechanistic_insight": "Although internal activations are not accessible, the strong clustering of errors into multiples-of-10 suggests these models may also rely on Fourier-like modular features (coarse magnitude + modular classification) analogous to those identified in open-source pre-trained models.",
            "performance_scaling": null,
            "failure_modes": "Errors predominantly multiples of 10, indicating systematic failures in resolving lower-order digits or phase alignment between low/high-frequency components.",
            "comparison_baseline": "Behavioral error-pattern comparison between closed-source models and open-source models (GPT-J, Phi-2) and fine-tuned GPT-2-XL.",
            "key_finding": "Behavior-only evidence from closed-source LLMs (0-shot) is consistent with the hypothesis that large instruction-tuned LLMs use Fourier-feature-like mechanisms for arithmetic, because their errors concentrate on multiples of 10.",
            "uuid": "e315.6",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2
        },
        {
            "paper_title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "rating": 2
        },
        {
            "paper_title": "Harmonics of learning: Universal fourier features emerge in invariant networks",
            "rating": 2
        },
        {
            "paper_title": "Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic",
            "rating": 2
        },
        {
            "paper_title": "Feature emergence via margin maximization: case studies in algebraic tasks",
            "rating": 1
        },
        {
            "paper_title": "Transformers learn in-context by gradient descent",
            "rating": 1
        }
    ],
    "cost": 0.01568075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Pre-trained Large Language Models Use Fourier Features to Compute Addition</h1>
<p>Tianyi Zhou Deqing Fu Vatsal Sharan Robin Jia<br>Department of Computer Science<br>University of Southern California<br>Los Angeles, CA 90089<br>{tzhou029, deqingfu, vsharan, robinjia}@usc.edu</p>
<h4>Abstract</h4>
<p>Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features-dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 2
2 Problem Setup ..... 3
3 Language Models Solve Addition with Fourier Features ..... 3
3.1 Behavioral Analysis ..... 3
3.2 Fourier Features in MLP \&amp; Attention Outputs ..... 4
3.3 Fourier Features are Causally Important for Model Predictions ..... 7
4 Effects of Pre-training ..... 8
4.1 Fourier features in Token Embedding ..... 8
4.2 Contrasting Pre-trained Models with Models Trained from Scratch ..... 8
4.3 Fourier Features in Prompted Pre-Trained Models ..... 10
5 Related Work ..... 11
6 Conclusion ..... 12
A Formal Definition of Transformer and Logits in Fourier Space ..... 17
B Fourier Components Separation and Selection of $\tau$ ..... 20
C Does Fourier Features Generalize? ..... 23
C. 1 Token Embedding for Other LMs ..... 23
C. 2 Multiplication Task ..... 24
C. 3 Same Results for other format ..... 25
C. 4 Fourier Features in Other Pre-trained LM ..... 26
D Supporting Evidence For the Fourier Features ..... 27
E More Experiments on GPT-2-XL Trained from Scratch ..... 28
F Details of Experimental Settings ..... 29</p>
<h1>1 Introduction</h1>
<p>Mathematical problem solving has become a crucial task for evaluating the reasoning capabilities of large language models (LLMs) [HBK ${ }^{+} 21, \mathrm{CKB}^{+} 21, \mathrm{LBX}^{+} 24, \mathrm{FKL}^{+} 24$ ]. While LLMs exhibit impressive mathematical abilities [Ope23, Goo23b, Ant24, WLS17, TPSI21, BMR ${ }^{+} 20, \mathrm{FPG}^{+} 24$ ], it remains unclear how they perform even basic mathematical tasks. Do LLMs apply mathematical principles when solving math problems, or do they merely reproduce memorized patterns from the training data?</p>
<p>In this work, we unravel how pre-trained language models solve simple mathematical problems such as "Put together 15 and 93. Answer: __". Prior work has studied how Transformers, the underlying architecture of LLMs, perform certain mathematical tasks. Most studies [Cha23, GTLV22, vONR ${ }^{+} 22, \mathrm{BCW}^{+} 23, \mathrm{FCJS} 23, \mathrm{NCL}^{+} 23, \mathrm{GLL}^{+} 24, \mathrm{PBE}^{+} 22 \mathrm{~b}$ ] focus on Transformers with a limited number of layers or those trained from scratch; [HLV23] analyzes how the pre-trained GPT-2-small performs the greater-than task. Our work focuses on a different task from prior interpretability work-integer addition-and shows that pre-trained LLMs learn distinct mechanisms from randomly initialized Transformers.</p>
<p>In $\S 3$, we show that pre-trained language models compute addition with Fourier featuresdimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. First, we analyze the behavior of pre-trained LLMs on the addition task after fine-tuning, which leads to almost perfect accuracy on the task. Rather than merely memorizing answers from the training data, the models progressively compute the final answer layer by layer. Next, we analyze the contributions of individual model components using Logit Lens [BFS ${ }^{+} 23$ ]. We observe that some components primarily approximate the answer-they promote all numbers close to the correct answer in magnitude-while other components primarily classify the answer modulo $m$ for various numbers $m$. Then, we use Fourier analysis to isolate features in the residual stream responsible for the low-frequency "approximation" and high-frequency "classification" subtasks. Identifying these features allows us to precisely ablate the ability of the model to perform either approximation or classification by applying a low-pass or high-pass filter, respectively, to the outputs of different model components. We find that MLP layers contribute primarily to approximation, whereas attention layers contribute primarily to classification.</p>
<p>In $\S 4$, we show that pre-training is crucial for learning this mechanism. The same network trained from scratch with random initialization not only shows no signs of Fourier features, but also has lower accuracy. We identify pre-trained token embeddings as a key source of inductive bias that help the pre-trained model learn a more precise mechanism for addition. Across the pretrained token embeddings of many different pre-trained models, Fourier analysis uncovers large magnitudes of components with periods 2,5 , and 10 . Introducing pre-trained token embeddings when training the model from scratch enables the model to achieve perfect test accuracy. Finally, we show that the same Fourier feature mechanism is present not only in models that were pretrained and then fine-tuned, but also in frozen pre-trained LLMs when prompted with arithmetic problems.</p>
<p>Overall, our work provides a mechanistic perspective on how pre-trained LLMs compute addition through the lens of Fourier analysis. It not only broadens the scope from only investigating few-layer Transformers trained to fit a particular data distribution to understanding LLMs as a whole, but also hints at how pre-training can lead to more precise model capabilities.</p>
<h1>2 Problem Setup</h1>
<p>Task and Dataset. We constructed a synthetic addition dataset for fine-tuning and evaluation purposes. Each example involves adding two numbers $\leq 260$, chosen because the maximum number that can be represented by a single token in the GPT-2-XL tokenizer is 520 . For each pair of numbers between 0 and 260 , we randomly sample one of five natural language question templates and combine it with the two numbers. The dataset is shuffled and then split into training ( $80 \%$ ), validation ( $10 \%$ ), and test ( $10 \%$ ) sets. More details are provided in Appendix F. In Appendix C.3, we show our that results generalize to a different dataset formatted with reverse Polish notation.</p>
<p>Model. Unless otherwise stated, all experiments focus on the pre-trained GPT-2-XL model that has been fine-tuned on our addition dataset. This model, which consists of 48 layers and approximately 1.5 billion parameters, learns the task almost perfectly, with an accuracy of $99.74 \%$ on the held-out test set. We examine other models in $\S 4.2$ and $\S 4.3$.</p>
<p>Transformers. We focus on decoder-only Transformer models [VSP ${ }^{+} 17$ ], which process text sequentially, token by token, from left to right. Each layer $\ell$ in the Transformer has an attention module with output Attn $^{(l)}$ and an MLP module with output $\mathrm{MLP}^{(l)}$. Their outputs are added together to create a continuous residual stream $h\left[\mathrm{ENO}^{+} 21\right]$, meaning that the token representation accumulates all additive updates within the residual stream, with the representation $h^{(\ell)}$ in the $\ell$-th layer given by:</p>
<p>$$
h^{(\ell)}=h^{(\ell-1)}+\operatorname{Attn}^{(\ell)}+\operatorname{MLP}^{(\ell)}
$$</p>
<p>The output embedding $W^{U}$ projects the residual stream to the space of the vocabulary; applying the softmax function then yields the model's prediction. We provide formal definitions in Appendix A.</p>
<h2>3 Language Models Solve Addition with Fourier Features</h2>
<p>In this section, we analyze the internal mechanisms of LLMs when solving addition tasks, employing a Fourier analysis framework. We first show that the model initially approximates the solution before iteratively converging to the correct answer (§3.1). We then show that the model refines its initial approximation by computing the exact answer modulo 2, 5, and 10, employing Fourier components of those same periods (§3.2). Finally, we demonstrate through targeted ablations that the identified Fourier components are causally important for the model's computational processes (§3.3). Specifically, we show that MLP layers primarily approximate the magnitude of the answer, using low-frequency features, while attention layers primarily perform modular addition using high-frequency components.</p>
<h3>3.1 Behavioral Analysis</h3>
<p>Our first goal is to understand whether the model merely memorizes and recombines pieces of information learned during training, or it performs calculations to add two numbers.</p>
<p>Extracting intermediate predictions. To elucidate how LLMs perform computations and progressively refine their outputs towards the correct answer, we extract model predictions at each layer from the residual stream. Let $L$ denote the number of layers. Using the Logit Lens method [BFS $\left.{ }^{+} 23\right]$, instead of generating predictions by computing logits $W^{U} h^{(L)}$, predictions are derived through $W^{U} h^{(\ell)}$ where $\ell \in[L]$. We compute the accuracy of the prediction using each intermediate state $h^{(\ell)}$. If the models merely retrieve and recombine pieces of information learned during</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Visualization of predictions extracted from fine-tuned GPT-2-XL at intermediate layers. Between layers 20 and 30, the model's accuracy is low, but its prediction is often within 10 of the correct answer: the model first approximates the answer, then refines it. (b) Heatmap of the logits from different MLP layers for the running example, "Put together 15 and 93. Answer: 108". The y-axis represents the subset of the number space around the correct prediction, while the x-axis represents the layer index. The 33-rd layer performs mod 2 operations (favoring even numbers), while other layers perform other modular addition operations, such as mod 10 (45-th layer). Additionally, most layers allocate more weight to numbers closer to the correct answer, 108. (c) Analogous plot for attention layers. Nearly all attention modules perform modular addition.</p>
<p>Training, certain layers will directly map this information to predictions. For instance, [MEP23] demonstrates that there is a specific MLP module directly that maps a country to its capital.</p>
<p><strong>LLMs progressively compute the final answers.</strong> Figure 1a instead shows that the model progressively approaches the correct answer, layer by layer. The model is capable of making predictions that fall within the range of ±2 and ±10 relative to the correct answer in the earlier layers, compared to the exact-match accuracy. This observation implies that the Transformer's layer-wise processing structure is beneficial for gradually refining predictions through a series of transformations and updates applied to the token representations.</p>
<h3>3.2 Fourier Features in MLP &amp; Attention Outputs</h3>
<p><strong>Logits for MLP and attention have periodic structures.</strong> We now analyze how each MLP and attention module contributes to the final prediction. We transform the output of the attention and MLP output at layer ℓ into the token space using W<sup>U</sup>Attn<sup>(ℓ)</sup> and W<sup>U</sup>MLP<sup>(ℓ)</sup> at each layer, thereby obtaining the logits L for each MLP and attention module. We use the running example "Put together 15 and 93. Answer: 108" to demonstrate how the fine-tuned GPT-2-XL performs the computation. As illustrated in Figure 1b and Figure 1c, both the MLP and attention modules exhibit a periodic pattern in their logits across the output number space, e.g., the MLP in layer 33, outlined in green, promotes all numbers that are congruent to 108 mod 2 (in Figure 19 in the appendix, we zoom into such layers to make this clearer). Overall, we observe two distinct types of computation within these components. Some components predominantly assign a high weight to numbers around the correct answer, which we term <em>approximation</em>. Meanwhile, other components predominantly assign a high weight to all numbers congruent to a + b mod c for some constant c, which we term <em>classification</em>.</p>
<p><strong>Logits for MLP and attention are approximately sparse in the Fourier space.</strong> It is natural to transform the logits into Fourier space to gain a better understanding of their properties such as the periodic pattern. We apply the discrete Fourier transform to represent the logits as the sum of sine and cosine waves of different periods: the k-th component in Fourier space has period</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The intermediate logits in Fourier space. We annotate the top-10 outlier high-frequency Fourier components based on their magnitudes. $T$ stands for the period of that Fourier component. (a) The logits in Fourier space for the MLP output of the 33-rd layer, i.e., $\tilde{\mathcal{L}}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{MLP}}^{(33)}$. The component with period 2 has the largest magnitude, aligning with the observations in Figures 1b and 19a. (b) The logits in Fourier space for the attention output of the 40-th layer, i.e., $\tilde{\mathcal{L}}</em>$. The components with periods 5 and 10 have the largest magnitude, aligning with the observations in Figures 1c and 19b.
$520 / k$ and frequency $k / 520$ (see Appendix A for more details). Let $\tilde{\mathcal{L}}$ denote the logits in Fourier space. Figure 2 shows the Fourier space logits for two layers from Figure 1b and Figure 1c that have a clear periodic pattern. We find that the high-frequency components in Fourier space, which we define as components with index greater or equal to 50, are approximately sparse as depicted in Figure 2. This observation aligns with $\left[\mathrm{NCL}^{+} 23\right]$, which found that a one-layer Transformer utilizes particular Fourier components within the Fourier space to solve the modular addition task.}}^{(40)</p>
<p>In Figure 3, we show that similar sparsity patterns in Fourier space hold across the entire dataset. We compute the logits in Fourier space for the last 15 layers, i.e., $\tilde{\mathcal{L}}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(t)}$ and $\tilde{\mathcal{L}}</em>$ where $\ell \in[32,47]$, for all test examples and average them. We annotate the top-10 outlier high-frequency components based on their magnitude. The MLPs also exhibit some strong low-frequency components; the attention modules do not exhibit strong low-frequency components, only high-frequency components.}}^{(t)</p>
<p>Final logits are superpositions of these outlier Fourier components. The final logits, $\mathcal{L}^{(L)}$, are the sum of all $\mathcal{L}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{MLP}}^{(l)}$ and $\mathcal{L}</em>$ based on their magnitudes and transfer them back to logits in number space via the inverse discrete Fourier transform (Figure 4a). The large-period (low-frequency) components approximate the magnitude while the small-period (high-frequency) components are crucial for modular addition. Figure 4b shows that aggregating these 5 waves is sufficient to predict the correct answer.}}^{(l)}$ across all layers $l \in[L]$. Figure 4 elucidates how these distinct Fourier components contribute to the final prediction, for the example "Put together 15 and 93. Answer: 108". We select the top-5 Fourier components of $\tilde{\mathcal{L}}^{(L)</p>
<p>Why is high-frequency classification helpful? The Fourier basis comprises both cos and sin waves (see Definition A.3). By adjusting the coefficients of cos and sin, the trained model can manipulate the phase of the logits in Fourier space (number shift in number space), aligning the peak</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Analysis of logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules, outlier Fourier components have periods around 2, 2.5, 5, and 10.
of the wave more closely with the correct answer. As shown in Figure 4a, consider a wave with a period of 2 . Here, the peak occurs at every even number in the number space, corresponding to the $\bmod 2$ task. In contrast, for components with a large period such as 520 , the model struggles to accurately position the peak at 108 (also see Figure 13 in the appendix for the plot of this component with period 520 in the full number space). This scenario can be interpreted as solving a "mod 520 " task-a classification task among 520 classes-which is challenging for the model to learn accurately. Nevertheless, even though the component with a period of 520 does not solve the "mod 520 " task precisely, it does succeed in assigning more weight to numbers near 108. The classification results from the high-frequency components can then provide finer-grained resolution to distinguish between all the numbers around 108 assigned a large weight by the lower frequencies. Due to this, the low-frequency components need not be perfectly aligned with the answer to make accurate predictions.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of how a sparse subset Fourier components can identify the correct answer. (a) Shows the top-5 Fourier components for the final logits. (b) Shows the sum of these top-5 Fourier components, highlighting how the cumulative effect identifies the correct answer, 108.</p>
<h1>3.3 Fourier Features are Causally Important for Model Predictions</h1>
<p>In the previous section, we demonstrated that there are outlier Fourier components in the logits generated by both the MLP and attention modules, as shown in Figure 3. We also illustrated that, in one example, the high-frequency components primarily approximate the magnitude, while the low-frequency components are crucial for modular addition tasks, as depicted in Figure 4. In this section, through an ablation study conducted across the entire test dataset, we show that both types of components are essential for correctly computing sums. Moreover, we reveal that the MLP layers primarily approximate the magnitude of the answer using low-frequency features, whereas the attention layers are responsible for modular addition using high-frequency features.</p>
<p>Filtering out Fourier components. To understand the role various frequency components play for the addition task, we introduce low-pass and high-pass filters $\mathcal{F}$. For an intermediate state $h$, and a set of frequencies $\Gamma=\left{\gamma_{1}, \ldots, \gamma_{k}\right}$, the filter $\mathcal{F}(h ; \Gamma)$ returns the vector $\widehat{h}$ that is closest in $L_{2}$ distance to $h$ subject to the constraint that the Fourier decomposition of $W^{U} \widehat{h}$ at every frequency $\gamma_{i}$ is 0 . We show in Appendix A that this has a simple closed-form solution involving a linear projection. We then apply either a low-pass filter by taking $\Gamma$ to be all the components whose frequencies are greater than the frequency of the $\tau$-th component for some threshold $\tau$ (i.e., removing high-frequency components), and a high-pass filter by taking $\Gamma$ to be all the components whose frequencies are less than the frequency of the $\tau$-th component (i.e., removing low-frequency components). As in the previous subsection, we take the high-frequency threshold $\tau=50$ for the following experiments (see Appendix B for more details).</p>
<p>Different roles of frequency components in approximation and classification tasks. We evaluated the fine-tuned GPT-2-XL model on the test dataset with different frequency filters applied to all of the output of MLP and attention modules. The results, presented in Table 1, indicate that removing low-frequency components from attention modules or high-frequency components from MLP modules does not impact performance. This observation suggests that attention modules are not crucial for approximation tasks, and MLP modules are less significant for classification tasks.</p>
<p>Eliminating high-frequency components from attention results in a noticeable decrease in accuracy. Furthermore, removing high-frequency components from both the attention and MLP modules simultaneously leads to an even greater reduction in accuracy. This finding corresponds with observations from Figure 1b,c and Figure 3, which indicate that both MLP and attention modules are involved in classification tasks due to the presence of high-frequency components in the logits. However, the approximation tasks are primarily performed by the MLP modules alone.</p>
<p>The errors induced by these ablations align with our mechanistic understanding. Ablating low-frequency parts of MLPs leads to off-by 10,50 , and 100 errors: the model fails to perform the approximation subtask, though it still accurately predicts the unit digit. Conversely, ablating highfrequency parts of attention leads to small errors less than 6 in magnitude: the model struggles to accurately predict the units digit, but it can still estimate the overall magnitude of the answer. See Figure 20 in the Appendix for more details. These observations validate our hypothesis that low-frequency components are crucial for approximation, while high-frequency components are vital for classification. The primary function of MLP modules is to approximate the magnitude of outcomes using low-frequency components, while the primary role of attention modules is to ensure accurate classification by determining the correct unit digit.</p>
<p>Table 1: Impact of Filtering out Fourier Components on Model Performance. Removing low-frequency components from attention modules (blue) or high-frequency components from MLP modules (red) does not impact performance</p>
<table>
<thead>
<tr>
<th>Module</th>
<th>Fourier Component Removed</th>
<th>Validation Loss</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Without Filtering</td>
<td>0.0073</td>
<td>0.9974</td>
</tr>
<tr>
<td>ATTN &amp; MLP</td>
<td>Low-Frequency</td>
<td>4.0842</td>
<td>0.0594</td>
</tr>
<tr>
<td>ATTN</td>
<td>Low-Frequency</td>
<td>0.0352</td>
<td>0.9912</td>
</tr>
<tr>
<td>MLP</td>
<td>Low-Frequency</td>
<td>2.1399</td>
<td>0.3589</td>
</tr>
<tr>
<td>ATTN &amp; MLP</td>
<td>High-Frequency</td>
<td>1.8598</td>
<td>0.2708</td>
</tr>
<tr>
<td>ATTN</td>
<td>High-Frequency</td>
<td>0.5943</td>
<td>0.7836</td>
</tr>
<tr>
<td>MLP</td>
<td>High-Frequency</td>
<td>0.1213</td>
<td>0.9810</td>
</tr>
</tbody>
</table>
<h2>4 Effects of Pre-training</h2>
<p>The previous section shows that pre-trained LLMs leverage Fourier features to solve the addition problem. Now, we study where the models' reliance on Fourier features comes from. In this section, we demonstrate that LLMs learn Fourier features in the token embeddings for numbers during pre-training. These token embeddings are important for achieving high accuracy on the addition task: models trained from scratch achieve lower accuracy, but adding just the pre-trained token embeddings fixes this problem. We also show that pre-trained models leverage Fourier features not only when fine-tuned, but also when prompted.</p>
<h3>4.1 Fourier features in Token Embedding</h3>
<p>Number embedding exhibits approximate sparsity in the Fourier space. Let $W^{E} \in \mathbb{R}^{p \times D}$, where $p=521$ and $D$ is the size of the token embeddings, denote the token embedding for numbers. We apply the discrete Fourier transform to each column of $W^{E}$ to obtain a matrix $V \in \mathbb{R}^{p \times D}$, where each row represents a different Fourier component. Then we take the $L_{2}$ norm of each row to yield a $p$-dimensional vector. Each component $j$ in this vector measures the overall magnitude of the $j$-th Fourier component across all the token embedding dimensions. Figure 5a shows the magnitude of different Fourier components in the token embedding of GPT-2-XL. We see that the token embedding has outlier components whose periods are $2,2.5,5$, and 10. Therefore, similar to how the model uses different Fourier components to represent its prediction (as shown in Section 3.2), the token embeddings represent numbers with different Fourier components. Figure 14 in the Appendix shows that the token embeddings of other pre-trained models have similar patterns the Fourier space. This suggests that Fourier features are a common attribute in the token embedding of pre-trained LLMs. In Figure 5b, we use t-SNE and $k$-means to visualize the token embedding clustering. We can see that numbers cluster not only by magnitude but also by their multiples of 10.</p>
<h3>4.2 Contrasting Pre-trained Models with Models Trained from Scratch</h3>
<p>To understand the necessity of Fourier features for the addition problem, we trained the GPT-2-XL model from scratch on the addition task with random initialization. After convergence, it achieved only $94.44\%$ test accuracy (recall that the fine-tuned GPT-2-XL model achieved $99.74\%$ accuracy).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (a) Number embedding in Fourier space for fine-tuned GPT-2-XL. $T$ stands for the period of that Fourier component.(b) Visualization of token embedding clustering of GPT-2 using T-SNE and $k$-means with 10 clusters. The numbers are clustered based on their magnitude and whether they are multiples of 10 .
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visualization of the logits in Fourier space on the test dataset from the last 15 layers for the GPT-2-XL model trained from scratch. For both the MLP and attention modules, there are no outlier Fourier components, in contrast with the clear outlier components in the fine-tuned model (Figure 3).</p>
<p>Fourier features are learned during pre-training. Figure 6 shows that there are no Fourier features in the intermediate logits of the GPT-2-XL model trained from scratch on the addition task. Furthermore, Figure 7a shows that the token embeddings also have no Fourier features. Without leveraging Fourier features, the model merely approximates the correct answer without performing modular addition, resulting in frequent off-by-one errors between the prediction and the correct answer (see details in Figure 22).</p>
<p>Pre-trained token embeddings improve model training. We also trained GPT-2-small, with 124 million parameters and 12 layers, from scratch on the addition task. GPT-2-small often struggles with mathematical tasks [MMV+22]. This model achieved a test accuracy of only $53.95 \%$</p>
<p>after convergence. However, when we freeze the token embedding layer and randomly initialize the weights for all other layers before training on the addition task, the test accuracy increases to $100 \%$, with a significantly faster convergence rate. This outcome was consistently observed across five different random seeds, as illustrated in Figure 7b. This demonstrates that given the number embeddings with Fourier features, the model can effectively learn to leverage these features to solve the addition task.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: (a) The number embedding in Fourier space for GPT-2-XL trained from scratch. There are no high-frequency outlier components, in contrast with the pre-trained embeddings (Figure 5a). (b) Validation accuracy of GPT-2-small trained from scratch either with or without pre-trained token embeddings. We show the mean and the standard deviation of the validation accuracy across 5 random seeds. GPT-2-small with pre-trained token embedding consistently achieves $100 \%$ accuracy, while GPT-2-small without pre-trained token embedding only achieves less than $60 \%$ accuracy.</p>
<h1>4.3 Fourier Features in Prompted Pre-Trained Models</h1>
<p>Finally, we ask whether larger language models use similar Fourier features during prompting.
Pre-trained LLMs use Fourier features to compute addition during in-context learning. We first test on the open-source models GPT-J [WK21] with 6B parameters, and Phi-2 [JBA ${ }^{+} 23$ ] with 2.7B parameters on the test dataset. Without in-context learning, the model cannot perform addition tasks. Therefore, we use 4 -shot in-context learning to test its performance. Their absolute errors are predominantly multiples of 10: $93 \%$ of the time for GPT-J, and $73 \%$ for Phi-2 . Using the Fourier analysis framework proposed in Section 3.2, we demonstrate that for Phi-2 and GPT-J, the outputs of MLP and attention modules exhibit approximate sparsity in Fourier space across the last 15 layers (Figure 8 and Figure 18). This evidence strongly suggests that these models leverage Fourier features to compute additions.</p>
<p>Closed-source models exhibit similar behavior. We study the closed-source models GPT-3.5 [Ope22], GPT-4 [Ope23], and PaLM-2 [Goo23a]. While we cannot analyze their internal representations, we can study whether their behavior on addition problems is consistent with reliance on Fourier features. Since closed-source LLMs are instruction tuned and perform well without incontext learning, we conduct error analysis with 0 -shot. Most absolute errors by these models are also multiples of 10: $100 \%$ of the time for GPT-3.5 and GPT-4, and $87 \%$ for PaLM-2. The similarity in error distribution to that of open-source models leads us to hypothesize that Fourier features play a critical role in their computational mechanism.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: For Phi-2 (4-shot), we analyzed the logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules, the outlier Fourier components have periods around 2, 2.5, 5, and 10, similar to the fine-tuned GPT-2-XL logits (Figure 3).</p>
<h1>5 Related Work</h1>
<p>Learning mathematical tasks. Previous studies primarily explore what pre-trained LMs can achieve on arithmetic tasks, with less emphasis on the underlying mechanisms [NJL21, QWL ${ }^{+} 22$ ]. For instance, [LSL $\left.{ }^{+} 23\right]$ demonstrates that small Transformer models can effectively learn arithmetic by altering the question format and utilizing a scratchpad method [NAGA $\left.{ }^{+} 21\right]$. [HLV23] identifies activation patterns for the "greater-than" operation in GPT-2, and [Cha23] focuses on the enumeration and selection processes in GCD computation. In this paper, we dive into the specific roles of MLP and attention layers in solving mathematical tasks. Our research analyzes these components' distinct contributions to integer addition tasks.</p>
<p>Mechanisms of pre-trained LMs. Recent studies have significantly advanced our understanding of the underlying mechanisms of pre-trained Transformer models. For instance, research on "skill neurons" by [WWZ $\left.{ }^{+} 22\right]$ and "knowledge neurons" by [DDH $\left.{ }^{+} 21\right]$ underscores the development of specialized neural components that encode task-specific capabilities or hold explicit factual information in the pre-trained LMs, enhancing model performance on related tasks. [MEP23] and [GCWG22] discuss how MLPs and FFNs transform and update token representations for general language tasks. In contrast, we show that the pre-trained LMs use multiple layers to compute addition by combining the results of approximation and classification. Additionally, [ZL23] demonstrated the capacity of GPT-2 to consolidate similar information through pre-training in the model weights, which aligns with our observations on the importance of pre-training in developing effective number embedding and arithmetic computation strategies in LMs.</p>
<p>Fourier features in Neural Networks. Fourier features are commonly observed in image models, particularly in the early layers of vision models [OF97, OCS ${ }^{+} 20$, FS24]. These features enable the model to detect edges, textures, and other spatial patterns effectively. Recently, Fourier features have been noted in networks trained for tasks that allow cyclic wraparound, such as modular addition $\left[\mathrm{NCL}^{+} 23, \mathrm{MEO}^{+} 23\right]$, general group compositions [CCN23], or invariance to cyclic translations [SSOH22]. [NCL $\left.{ }^{+} 23\right]$ demonstrates that learning Fourier features can induce 'grokking' $\left[\mathrm{PBE}^{+} 22 \mathrm{a}\right]$. Furthermore, [MHKS23] provides a mathematical framework explaining the emergence of Fourier features when the network exhibits invariance to a finite group. We extend these</p>
<p>insights by observing Fourier features in tasks that do not involve cyclic wraparound. [TSM ${ }^{+20}$ ] found that by selecting problem-specific Fourier features, the performance of MLPs can be improved on a computer vision-related task.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we provide a comprehensive analysis of how pre-trained LLMs compute numerical sums, revealing a nuanced interplay of Fourier features within their architecture. Our findings demonstrate that LLMs do not simply memorize answers from training data but actively compute solutions through a combination of approximation and classification processes encoded in the frequency domain of their hidden states. Specifically, MLP layers contribute to approximating the magnitude of sums, while attention layers contribute to modular operations.</p>
<p>Our work also shows that pre-training plays a critical role in equipping LLMs with the Fourier features necessary for executing arithmetic operations. Models trained from scratch lack these crucial features and achieve lower accuracy; introducing pre-trained token embeddings greatly improves their convergence rate and accuracy. This insight into the arithmetic problem-solving capabilities of LLMs through Fourier features sets the stage for potential modifications to training approaches. By imposing specific constraints on model training, we could further enhance the ability of LLMs to learn and leverage these Fourier features, thereby improving their performance in mathematical tasks.</p>
<h2>Acknowledgments</h2>
<p>DF and RJ were supported by a Google Research Scholar Award. RJ was also supported by an Open Philanthropy research grant. VS was supported by NSF CAREER Award CCF-2239265 and an Amazon Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the funding agencies.</p>
<h1>References</h1>
<p>[Ant24] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.
[BCW ${ }^{+}$23] Yu Bai, Fan Chen, Haiquan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. ArXiv, abs/2306.04637, 2023.
[BFS ${ }^{+}$23] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.
[BMR ${ }^{+}$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[CCN23] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. In International Conference on Machine Learning, pages 6243-6267. PMLR, 2023.
[Cha23] François Charton. Can transformers learn the greatest common divisor? arXiv preprint arXiv:2308.15594, 2023.
[CKB ${ }^{+}$21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
$\left[\mathrm{DDH}^{+}\right.$21] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021.
$\left[\mathrm{ENO}^{+}\right.$21] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.
[FCJS23] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higherorder optimization methods for in-context learning: A study with linear models, 2023.
[FKL ${ }^{+}$24] Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations, 2024.
[FPG ${ }^{+}$24] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. Mathematical capabilities of chatgpt. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>[FS24] Pierre-Étienne Fiquet and Eero Simoncelli. A polar prediction model for learning to represent visual transformations. Advances in Neural Information Processing Systems, 36, 2024.
[GCWG22] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feedforward layers build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680, 2022.
[GLL ${ }^{+}$24] Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou. Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic, 2024.
[Goo23a] Google. Palm 2 technical report, 2023.
[Goo23b] Gemini Team Google. Gemini: A family of highly capable multimodal models, 2023.
[GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. ArXiv, abs/2208.01066, 2022.
$\left[\mathrm{HBK}^{+}\right.$21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.
[HLV23] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. arXiv preprint arXiv:2305.00586, 2023.
$\left[\mathrm{JBA}^{+}\right.$23] Mojan Javaheripi, Sebastien Bubeck, Marah Abdin, Jyoti Anejaand Caio Cesar Teodoro Mendes, Allie Del Giorno Weizhu Chen, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi L, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small language models, 2023.
[LBX ${ }^{+}$24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024.
[LSL ${ }^{+}$23] Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.
[MEO ${ }^{+}$23] Depen Morwani, Benjamin L Edelman, Costin-Andrei Oncescu, Rosie Zhao, and Sham Kakade. Feature emergence via margin maximization: case studies in algebraic tasks. arXiv preprint arXiv:2311.07568, 2023.
[MEP23] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vec-style vector arithmetic. arXiv preprint arXiv:2305.16130, 2023.
[MHKS23] Giovanni Luca Marchetti, Christopher Hillar, Danica Kragic, and Sophia Sanborn. Harmonics of learning: Universal fourier features emerge in invariant networks. arXiv preprint arXiv:2312.08550, 2023.</p>
<p>[MMV ${ }^{+}$22] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. arXiv preprint arXiv:2204.05660, 2022.
[NAGA ${ }^{+}$21] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[NCL ${ }^{+}$23] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.
[NJL21] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.
[OCS ${ }^{+}$20] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An overview of early vision in inceptionv1. Distill, 5(4):e00024-002, 2020.
[OF97] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):3311-3325, 1997.
[Ope22] OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022. Accessed: 2023-09-10.
[Ope23] OpenAI. Gpt-4 technical report, 2023.
$\left[\mathrm{PBE}^{+}\right.$22a] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
[PBE ${ }^{+}$22b] Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. ArXiv, abs/2201.02177, 2022.
[QWL ${ }^{+}$22] Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051, 2022.
[SSOH22] Sophia Sanborn, Christian Shewmake, Bruno Olshausen, and Christopher Hillar. Bispectral neural networks. arXiv preprint arXiv:2209.03416, 2022.
[TPSI21] Avijit Thawani, Jay Pujara, Pedro A Szekely, and Filip Ilievski. Representing numbers in nlp: a survey and a vision. arXiv preprint arXiv:2103.13136, 2021.
[TSM ${ }^{+}$20] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:7537-7547, 2020.
[vONR ${ }^{+}$22] Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn incontext by gradient descent. In International Conference on Machine Learning, 2022.</p>
<p>[VSP ${ }^{+}$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762, 2017.
[WK21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.
[WLS17] Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 845-854, 2017.
[WWZ ${ }^{+}$22] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons in pre-trained transformer-based language models. arXiv preprint arXiv:2211.07349, 2022.
[ZL23] Zeyuan Allen Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023.</p>
<h1>Appendix</h1>
<p>Roadmap. In Appendix A, we introduce some formal definitions that used in our main content. In Appendix B, we show why we separate the Fourier components into the high-frequency part and the low-frequency part and why we choose $\tau$ to be 50 . In Appendix C, we show our observation generalizes to another format of dataset, another arithmetic task and other models. In Appendix D, we provide more evidence that shows the Fourier features in the model when computing addition. In Appendix E, we provide more evidence that shows the GPT-2-XL trained from scratch does not use Fourier feature to solve the addition task. In Appendix F, we give the details of our experimental settings.</p>
<h2>A Formal Definition of Transformer and Logits in Fourier Space</h2>
<p>We first introduce the formal definition of the Transformer structure that we used in this paper.
Definition A. 1 (Transformer). An autoregressive Transformer language model $G: \mathcal{X} \rightarrow \mathcal{Y}$ over vocabulary Vocab maps a token sequence $x=\left[x_{1}, \ldots, x_{N}\right] \in \mathcal{X}, x_{t} \in$ Vocab to a probability distribution $y \in \mathcal{Y} \subset \mathbb{R}^{|\text {Vocab }|}$ that predicts next-token continuations of $x$. Within the Transformer, the $i$-th token is embedded as a series of hidden state vectors $h_{t}^{(\ell)}$, beginning with $h_{t}^{(0)}=\mathrm{emb}\left(x_{t}\right)+\operatorname{pos}(i) \in \mathbb{R}^{D}$. Let $W^{U} \in \mathbb{R}^{|\operatorname{Vocab}| \times D}$ denote the output embedding. The final output $y=\operatorname{softmax}\left(W^{U}\left(h_{N}^{(L)}\right)\right)$ is read from the last hidden state. In the autoregressive case, tokens only draw information from past tokens:</p>
<p>$$
h_{t}^{(\ell)}=h_{t}^{(\ell-1)}+\operatorname{Attn}<em t="t">{t}^{(\ell)}+\operatorname{MLP}</em>
$$}^{(\ell)</p>
<p>where</p>
<p>$$
\operatorname{Attn}<em 1="1">{t}^{(\ell)}:=\operatorname{Attn}^{(\ell)}\left(h</em>}^{(\ell-1)}, h_{2}^{(\ell-1)}, \ldots, h_{t}^{(\ell-1)}\right) \quad \text { and } \quad \operatorname{MLP<em t="t">{t}^{(\ell)}:=\operatorname{MLP}</em>}^{(\ell)}\left(\operatorname{Attn<em t="t">{t}^{(\ell)}, h</em>\right)
$$}^{(\ell-1)</p>
<p>In this paper, we only consider the output tokens to be numbers. Hence, we have the unembedding matrix $W^{U} \in \mathbb{R}^{p \times D}$, where $p$ is the size of the number space. As we are given the length- $N$ input sequences and predict the $(N+1)$-th, we only consider $h_{N}^{(\ell)}=h_{N}^{(\ell-1)}+\operatorname{Attn}<em N="N">{N}^{(\ell)}+\operatorname{MLP}</em>$. For simplicity, we ignore the subscript $N$ in the following paper, so we get Eq. (1).}^{(\ell)</p>
<p>Definition A. 2 (Intermediate Logits). Let $\mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\text {Attn }}^{(\ell)}:=W^{U} \operatorname{Attn}^{(\ell)}$ denote the intermediate logits of the attention module at the $\ell$-th layer. Let $\mathcal{L}</em>$.}}^{(\ell)}:=W^{U} \mathrm{MLP}^{(\ell)}$ denote the intermediate logits of the MLP module at the $\ell$-th layer. Let $\mathcal{L}^{(\ell)}:=W^{U} h^{(\ell)}$ denote the logits on intermediate state $h^{(\ell)</p>
<p>Throughout the model, $h$ undergoes only additive updates (Eq. (1)), creating a continuous residual stream $\left[\mathrm{ENO}^{+} 21\right]$, meaning that the token representation $h$ accumulates all additive updates within the residual stream up to layer $t$.</p>
<p>To analyze the logits in Fourier space, we give the formal definition of the Fourier basis as follows:</p>
<p>Definition A. 3 (Fourier Basis). Let $p$ denote the size of the number space. Let $\overrightarrow{\mathrm{x}}:=(0,1, \ldots,(p-1))$.</p>
<p>Let $\omega_{k}:=\frac{2 \pi k}{p-1}$. We denote the normalized Fourier basis $F$ as the $p \times p$ matrix:</p>
<p>$$
F:=\left[\begin{array}{c}
\sqrt{\frac{1}{p-1} \cdot \overrightarrow{\mathbf{1}}} \
\sqrt{\frac{2}{p-1}} \cdot \sin \left(\omega_{1} \overrightarrow{\mathbf{x}}\right) \
\sqrt{\frac{2}{p-1}} \cdot \cos \left(\omega_{1} \overrightarrow{\mathbf{x}}\right) \
\sqrt{\frac{2}{p-1}} \cdot \sin \left(\omega_{2} \overrightarrow{\mathbf{x}}\right) \
\vdots \
\sqrt{\frac{2}{p-1}} \cdot \cos \left(\omega_{(p-1) / 2} \overrightarrow{\mathbf{x}}\right)
\end{array}\right] \in \mathbb{R}^{p \times p}
$$</p>
<p>The first component $F[0]$ is defined as a constant component. For $i \in[0, p-1], F[i]$ is defined as the $k$-th component in Fourier space, where $k=\left\lfloor\frac{i+1}{2}\right\rfloor$. The frequency of the $k$-th component is $f_{k}:=\frac{k}{p-1}$. The period of the $k$-th component is $T_{k}:=\frac{p-1}{k}$</p>
<p>We can compute the discrete Fourier transform under that Fourier basis as follows:
Remark A. 4 (Discrete Fourier transformer (DFT) and inverse DFT). We can transform any logits $u \in \mathbb{R}^{p}$ to Fourier space by computing $\widehat{u}=F \cdot u$. We can transform $\widehat{u}$ back to $u$ by $u=F^{\top} \cdot \widehat{u}$</p>
<p>Next, we define the logits in Fourier space.
Definition A. 5 (Logits in Fourier Space). Let $\mathcal{L}^{(L)}, \mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(\ell)}$ and $\mathcal{L}</em>$. The logits of the MLP and attention modules in Fourier space are defined as:}}^{(\ell)}$ denote the logits (Definition A.2). The output logits before softmax in Fourier space is defined as: $\widetilde{\mathcal{L}}^{(L)}=F \cdot \mathcal{L}^{(L)</p>
<p>$$
\tilde{\mathcal{L}}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{Attn}}^{(\ell)}=F \cdot \mathcal{L}</em>}}^{(\ell)} \quad \text { and } \quad \tilde{\mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{MLP}}^{(\ell)}=F \cdot \mathcal{L}</em>
$$}}^{(\ell)</p>
<p>We ignore the first elements in $\widetilde{\mathcal{L}}^{(L)}, \widetilde{\mathcal{L}}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(\ell)}$ and $\widetilde{\mathcal{L}}</em>$ for the Fourier analysis in this paper as they are the constant terms. Adding a constant to the logits will not change the prediction.}}^{(\ell)</p>
<p>Let $\tau \in \mathbb{R}$ denote a constant threshold. The low-frequency components for the logits in Fourier space are defined as $\widetilde{\mathcal{L}}^{(\ell)}[1: 2 \tau]$. The high-frequency components for the logits in Fourier space are defined as $\widetilde{\mathcal{L}}^{(\ell)}[2 \tau:]$. For the following analysis, we choose $\tau=50$ (the specific choice of $\tau=50$ is explained in Appendix B).</p>
<p>Next, we propose the formal definition of low-pass/high-pass filter that is used in the following ablation study.</p>
<p>Definition A. 6 (Loss-pass / High-pass Filter). Let $x \in \mathbb{R}^{D}$ denote the output of MLP or attention modules. Let $F$ denote the Fourier Basis (Definition A.3). Let $\tau \in R$ denote the frequency threshold. Let $W^{U} \in R^{p \times D}$ denote the output embedding. For low-pass filter, we define a diagonal binary matrix $B \in$ ${0,1}^{p \times p}$ as $b_{i i}= \begin{cases}1 &amp; \text { if } i \geq \tau \ 0 &amp; \text { otherwise }\end{cases}$. For high-pass filter, we define a diagonal binary matrix $B \in{0,1}^{p \times p}$ as $b_{i i}= \begin{cases}1 &amp; \text { if } 1 \leq i&lt;\tau \ 0 &amp; \text { otherwise }\end{cases}$. Note that we retain the constant component, so $b_{i, i}=0$. The output of the filter $\mathcal{F}(x): \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ is defined by the following objective function:</p>
<p>$$
\begin{array}{cl}
\min <em 2="2">{y} &amp; |x-y|</em> \
\text { subject to } &amp; B F W^{U} y=0
\end{array}
$$}^{2</p>
<p>The solution to the above optimization problem is given by a linear projection.
Remark A.7. The result of the optimization problem defined in Definition A. 6 is the projection of $x$ to the null space of $B F W^{U}$. Let $\mathcal{N}\left(B F W^{U}\right)$ denote the null space of $B F W^{U}$. We have</p>
<p>$$
\mathcal{F}(x)=\mathcal{N}\left(B F W^{U}\right) \cdot \mathcal{N}\left(B F W^{U}\right)^{\top} \cdot x^{\top}
$$</p>
<p>B Fourier Components Separation and Selection of $\tau$
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: We analyzed the logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules. We only plot the first 50 Fourier components (a) The MLP exhibits some outlier low-frequency Fourier components. (b) The attention module's lowfrequency Fourier components are not as obvious as the ones in MLP.</p>
<p>Following Definition A.6, we define single-pass filter as follows:
Definition B. 1 (Single-Pass Filter). Let $x \in \mathbb{R}^{D}$ denote the output of MLP or attention modules. Let $F$ denote the Fourier Basis (Definition A.3). Let $\gamma \in R$ denote the $\gamma$-th Fourier component (Definition A.3) that we want to retain. Let $W^{U} \in R^{V \times D}$ denote the output embedding. We define a diagonal binary matrix $B \in{0,1}^{V \times V}$ as $b_{i i}= \begin{cases}0 &amp; \text { if }\left\lfloor\frac{i+1}{2}\right\rfloor=\gamma \text { or } i=0, \ 1 &amp; \text { otherwise. }\end{cases}$
The output of the filter $\mathcal{F}_{\gamma}(x): \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ is defined as the following objective function:</p>
<p>$$
\begin{aligned}
\min <em 2="2">{y} &amp; |x-y|</em> \
\text { subject to } &amp; B F W^{U} y=0
\end{aligned}
$$}^{2</p>
<p>Remark B.2. The result of the optimization problem defined in Definition B. 1 is the projection of $x$ to the null space of $B F W^{U}$. Let $\mathcal{N}\left(B F W^{U}\right)$ denote the null space of $B F W^{U}$. We have</p>
<p>$$
\mathcal{F}_{\gamma}(x)=\mathcal{N}\left(B F W^{U}\right) \cdot \mathcal{N}\left(B F W^{U}\right)^{\top} \cdot x^{\top}
$$</p>
<p>For the single-pass filter, we only retrain one Fourier component and analyze how this component affects the model's prediction. The residual stream is then updated as follows:</p>
<p>$$
h^{(\ell)}=h^{(\ell-1)}+\mathcal{F}<em _gamma="\gamma">{\gamma}\left(\operatorname{Attn}^{(\ell-1)}\right)+\mathcal{F}</em>\right)
$$}\left(\operatorname{MLP}^{(\ell-1)</p>
<p>We evaluated the fine-tuned GPT-2-XL model on the addition dataset with the Fourier components period 520 and 2. Given that $T_{k}:=\frac{V-1}{k}$ (Definition A.3), we retained only the Fourier components with $\gamma=1$ and 260 , respectively.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-749 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-749</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-749</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-a5d3a865b71f3f424ba31e037848028f60161478</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a5d3a865b71f3f424ba31e037848028f60161478" target="_blank">Propagating Knowledge Updates to LMs Through Distillation</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader inferences and is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods.</p>
                <p><strong>Paper Abstract:</strong> Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e749.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e749.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-vs-Param Gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between in-context (prompt) information and parameter-updated knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robust gap where providing definitions in-context (prepending the definition at inference time) yields stronger immediate inference performance than updating model parameters via standard editing or finetuning; parameter updates often fail to propagate inference capabilities as effectively as prompt conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LM knowledge updating / evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Comparative evaluation of methods for injecting/updating entity knowledge in autoregressive language models: (1) inference-time prepending of definitions, (2) parameter-updating methods (finetuning, MEND, MEMIT), and (3) context distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper claims / experimental motivation (natural-language description of desired behaviour and comparison baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training & evaluation scripts implementing finetuning, editing algorithms, and prepending at inference</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different operational modality effectiveness (natural-language prompt vs parameter update)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The natural-language description (and intuition) suggests that internalizing information into model parameters should enable the model to make the same inferences it can make when conditioned on the same information in-context. In practice, parameter updates (finetuning or editing methods) frequently produce smaller gains on inference tasks than simply prepending the definition to the context. The paper documents this consistent underperformance of parameter-editing approaches relative to in-context conditioning across multiple benchmarks and base models.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>inference procedure / knowledge representation (how knowledge is made available to the model at inference time)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>direct experimental comparison of inference-time prepending vs parameter-update methods across benchmarks (ENTITY INFERENCES, ECBD) and multiple base LMs</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Task metrics: accuracy on ENTITY INFERENCES and per-token perplexity (PPL) on ECBD, reporting absolute values and deltas pre/post edit; paired bootstrap significance test for PPL differences (N=10000) when comparing distillation to fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prepending the definition often yields the largest improvements (e.g., strongest PPL/accuracy gains). Parameter updates (finetuning/editing) achieve smaller gains; distillation recovers a substantial fraction of prepending gains but does not fully match them. Quantitatively: distillation reduced target perplexity substantially (e.g., -5.7 PPL on GPT-Neo using GPT-3.5 generated transfer set) but prepending achieves even larger reductions (e.g., prepending: -9.1 PPL in one setting). The paper concludes parameter updates are still less effective overall.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently across multiple benchmarks (ENTITY INFERENCES, ECBD) and three base models (GPT-Neo, GPT2-XL, LLaMA-2-7B); no aggregate prevalence percentage provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Architectural or training-dynamics limitation: parameter updates do not replicate the transient conditioning effect of adding information to context; editing procedures may not change internal reasoning paths that rely on context-conditioning. Implicit assumptions about how knowledge is stored/propagated are therefore incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Context distillation: generate transfer-set continuations conditioned on the definition and distill the teacher (conditioned model) distribution into the student (unconditioned model) via a KL loss on tokens after the entity mention. Also explore stronger transfer-set generators (GPT-3.5) and ablations on transfer-set construction.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Distillation recovers a substantial fraction of the gains from prepending and outperforms baseline editing methods on many settings (e.g., distillation with GPT-3.5 produced -5.7 PPL vs base while prepending achieved -9.1 PPL on one setting). However, it does not always fully close the gap; the paper explicitly states a 'robust gap' remains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / natural language processing (LM knowledge editing and evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Propagating Knowledge Updates to LMs Through Distillation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e749.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e749.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transfer-Set Hallucination Gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between entity definition text and generated transfer-set continuations (hallucinations and lexical divergence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generated transfer-set continuations can hallucinate or introduce tokens not present in the original natural-language definition, causing discrepancies between the intended description and the actual examples used for distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Transfer-set generation component of context distillation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A generator model (either the base LM or an external model like GPT-3.5) is prompted with the entity definition to produce continuations that form the transfer set for distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>specification of transfer-set construction derived from natural-language definition</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generation scripts calling LM generator (sampling with nucleus sampling, temperature, max length parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>input-output mismatch / hallucination (lexical mismatch between specification and generated examples)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper notes that continuations produced by different generator models contain varying amounts of text copied from the definition; smaller models hallucinate more and have lower token overlap with the definition. These differences mean that the transfer set used for distillation may not faithfully reflect the definition's content, sometimes adding extra (possibly incorrect) information.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data generation / transfer-set creation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual inspection of generated continuations and automatic token-overlap statistics between continuations and definition sentences</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported % of tokens in continuations that appeared in the definition sentence (Table 1: e.g., GPT-3.5 56.4% overlap vs GPT-Neo ~35%); also measure distillation downstream performance (accuracy/PPL) when using different generators.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Generator quality affects distillation performance: transfer sets from GPT-3.5 yield better propagation gains than those from the base model in several settings (sometimes even outperforming prepending because GPT-3.5 may introduce additional information). Token-overlap differences correspond to differences in per-token NLL reductions and final task gains.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across generator choices and base models in experiments; no global prevalence statistics beyond the reported generator comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Differences in generator model capacity and training (larger models hallucinate less and better adhere to prompt-defined content); prompt formulation and sampling hyperparameters also influence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use stronger generator models (GPT-3.5) to produce transfer sets; if unavailable, use base model-generated continuations but consider appending the entity name or cleaning; perform ablations to ensure transfer-set robustness (e.g., repeating continuations, varying number of unique continuations).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Using GPT-3.5 improved distillation performance (higher accuracy / larger PPL reductions). However, distillation from the base model remained competitive, indicating mitigation is helpful but not strictly required.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / natural language processing (data generation for model distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Propagating Knowledge Updates to LMs Through Distillation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e749.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e749.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Supervision-Signal Gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difference between distillation (KL to conditioned teacher) and fine-tuning (NLL) supervision leading to divergent outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Distillation using teacher-conditioned distributions provides a different supervisory signal than fine-tuning on the transfer set or definition (maximum-likelihood), resulting in different learning dynamics and better propagation with less specificity loss in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Distillation vs finetuning training procedures</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two training paradigms for injecting knowledge: (a) finetuning on definition or on transfer-set examples with NLL loss (teacher forcing), and (b) KL-distillation where the student matches the teacher's conditioned distributions on the transfer set.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>algorithmic description / training objective specification in the methods section</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training code implementing either NLL-based finetuning or KL-based distillation updates</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / loss-function mismatch (natural-language description vs implemented update objective)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although both approaches are described in the paper, they imply different intended behaviors: finetuning on generated continuations might be expected to teach similar inferences as distillation, but in practice distillation is more effective at propagating knowledge while preserving specificity. The paper characterizes how tokens changed differently under conditioning (KL) than under NLL/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training objective / update rule</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical comparison of methods on ECBD and ENTITY INFERENCES, per-token NLL analysis plotting NLL before vs after conditioning, and ablation studies replacing objectives and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Task metrics (accuracy, PPL), per-token NLL fractional reduction plots (Figure 3), and paired bootstrap significance testing for PPL comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Distillation often outperforms finetuning on the transfer set in target metrics (e.g., significant PPL reductions) with smaller specificity drops. Example: distillation (GPT-3.5) achieved -5.7 PPL vs base on GPT-Neo, and gains over finetuning were significant (p < 0.05). Fine-tuning sometimes severely harmed specificity compared to distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Demonstrated across multiple models and datasets in the paper; not measured as a proportion of studies.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Different loss formulations emphasize matching teacher distributions (soft targets) versus maximizing likelihood of specific token sequences; KL distillation focuses updates on tokens where teacher/student distributions differ, producing more targeted edits.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt context distillation (KL to conditioned teacher) rather than raw finetuning; tune transfer-set construction and number of continuations; temperature scaling to soften distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Distillation produced consistent improvements versus finetuning on target metrics with smaller specificity degradation; statistical tests confirm significance in several comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / optimization for LM editing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Propagating Knowledge Updates to LMs Through Distillation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e749.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e749.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Definition-TransferSet Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Misalignment when wrong/ random definitions or transfer sets are paired for distillation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrepancy where using incorrect natural-language definitions or mismatched transfer sets (randomly swapped between entities) leads to degraded or misleading model updates, quantifiably harming inference performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Distillation pipeline with transfer-set / definition pairing</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The pipeline expects a correct pairing of an entity's definition and its generated transfer-set continuations; ablations test swapping the definition or transfer set with random ones.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol specification (which definition pairs with which transfer set)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data preparation and distillation training scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incorrect mapping / data-label mismatch (specification vs data used in code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>If the wrong definition is distilled (randomly sampled entity) even with a correct transfer set, perplexity on target probes increases. If the correct definition is paired with a random transfer set but the entity name is prepended, distillation can still reduce perplexity; nonetheless, the pairing matters and mismatches create measurable harms.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data pairing / preprocessing stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Ablation experiments swapping definitions and transfer sets and measuring resulting performance changes (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Change in target perplexity (PPL) relative to base model; examples: 'Random definition + correct transfer set' yields +2.6 PPL (worse), while 'Correct definition + random transfer set + entity string' yields -4.3 PPL (better).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Using a random definition increased PPL (+2.6), showing clear degradation; prepending a random definition at inference also causes large degradation (+11.9 PPL in one case). These are concrete quantitative impacts on evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in targeted ablations; not reported as a broad prevalence statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mistakes or ambiguities in dataset curation or pairing logic; reliance on correct alignment between natural-language descriptions and generated continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Ensure correct pairing of definitions and transfer sets, prepend entity name when necessary to enforce mention presence, and perform sanity-check ablations; prefer generators that include the entity string reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Appending entity strings to continuations and using the correct definition substantially mitigates negative effects; experiments show corrected pairings restore or improve PPL.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / dataset preprocessing for distillation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Propagating Knowledge Updates to LMs Through Distillation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e749.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e749.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Edit Scaling Gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scalability mismatch when injecting many entities at once (behavior differs across editing methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different editing methods scale differently when updating many entities: some specialized editing algorithms degrade when many facts are edited, while distillation remains more robust but can still affect specificity depending on base model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large-scale multi-entity editing pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure that aggregates transfer sets for many entities, shuffles them, and performs distillation or applies alternative editing methods (MEMIT, ROME) to update many facts in a model in one run.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method scaling claim / experimental protocol for multiple edits</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>batch distillation training scripts or rank-one weight-editing implementations (MEMIT/ROME)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>scalability / algorithmic limitation (implementation does not match expected scaling behavior from natural-language claims)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>While some prior editing methods (e.g., MEMIT) claim mass-editing capability, experiments show that MEMIT increases perplexity when injecting more than ~25 entities at once. Distillation scales to more entities (tested up to 150) with better target performance but may degrade specificity for certain base models (GPT-Neo). Thus, the practical scaling behavior deviates from optimistic descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / batch editing and capacity limits</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical scaling experiment varying number of entities edited (10 to 150) and comparing methods (distillation vs MEMIT) and tracking target PPL and specificity</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Average target perplexity and specificity delta across runs (Figure 4); observe increases in PPL for MEMIT beyond 25 entities and varying specificity results for distillation depending on base LM.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>MEMIT shows increased perplexity (worse performance) when injecting more than ~25 entities; distillation generally outperforms MEMIT up to 150 entities in target performance, but specificity degradation was observed for GPT-Neo. These effects influence the feasibility of large-scale updates.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in the paper's scaling experiments; prevalence across other models/datasets not quantified but indicated as an open limitation for very large numbers (thousands/millions) of edits.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Architectural assumptions of editing methods (rank-one MLP modifications) and capacity constraints; interaction effects when many edits are applied cause interference.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use context distillation with shuffled aggregated transfer sets; monitor specificity and target metrics as edits scale; further research needed to assess injection of thousands/millions of entities and to design more robust multi-edit algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Distillation outperforms MEMIT up to 150 entities in this work; however, some specificity degradation remains for certain base LMs, so mitigation is partially effective but incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / large-scale model editing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Propagating Knowledge Updates to LMs Through Distillation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e749.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e749.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Counterfactual Specificity Gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Different consequences when editing models with counterfactual (false) natural-language statements vs factual ones</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Editing with blatantly false statements (counterfactual edits) produces different trade-offs: some methods (ROME, FT) achieve high efficacy/generalization but poor specificity, while distillation yields better (though still poor) specificity but lower efficacy/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Counterfactual editing evaluation (CounterFact benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation of editing algorithms' ability to inject counterfactual triplets into model parameters and measure efficacy (success on edited fact), paraphrase generalization, and neighborhood/locality (specificity).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>evaluation benchmark specification and desired edit properties (efficacy, generalization, locality)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>edit application scripts for ROME, FT, FT+L, Distillation and evaluation scripts computing efficacy/paraphrase/neighborhood scores</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>mismatch between expected edit behavior (from natural-language description of method) and empirical specificity/locality outcomes for counterfactuals</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper reports that methods which perform well for factual or emerging-entity edits may behave differently when injecting false statements. Distillation, which preserved specificity for factual edits in other benchmarks, suffered worse specificity trade-offs when applied to counterfactual edits.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>edit objective and evaluation (how natural-language counterfactuals are implemented in editing code and their downstream effects)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical evaluation on a random sample of CounterFact entries with standard metrics (efficacy, paraphrase, neighborhood) and comparison across editors (FT, FT+L, ROME, Distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported efficacy, paraphrase, and neighborhood scores and an aggregated harmonic-mean score (Table 5). Distillation achieved lower efficacy/paraphrase but higher neighborhood (specificity) than some other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Distillation had lower efficacy and paraphrase scores compared to ROME and FT, but improved neighborhood (specificity) compared to them; FT+L (constrained fine-tuning) achieved the best aggregate score. This shows editing strategy impacts safety/locality when injecting false info.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in CounterFact experiments (150-sample subset); not generalized beyond this benchmark in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Nature of edit target (false assertions) interacts with update dynamics differently than factual additions; objective functions and constraints used by editors lead to different trade-offs between efficacy and locality.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Evaluate constrained fine-tuning (FT+L) with L_infty constraints on weight changes to improve specificity; consider edit-specific objectives and improved locality checks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>FT+L improved aggregate score (best among evaluated methods) by improving specificity, though generalization remained mediocre; distillation improved specificity relative to some methods but at a cost to efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / model editing and safety evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Propagating Knowledge Updates to LMs Through Distillation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge <em>(Rating: 2)</em></li>
                <li>MassEditing Memory in a Transformer <em>(Rating: 2)</em></li>
                <li>Locating and Editing Factual Associations in GPT <em>(Rating: 2)</em></li>
                <li>Fast Model Editing at Scale <em>(Rating: 2)</em></li>
                <li>Detecting edit failures in large language models: An improved specificity benchmark <em>(Rating: 2)</em></li>
                <li>A General Language Assistant as a Laboratory for Alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-749",
    "paper_id": "paper-a5d3a865b71f3f424ba31e037848028f60161478",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Prompt-vs-Param Gap",
            "name_full": "Mismatch between in-context (prompt) information and parameter-updated knowledge",
            "brief_description": "A robust gap where providing definitions in-context (prepending the definition at inference time) yields stronger immediate inference performance than updating model parameters via standard editing or finetuning; parameter updates often fail to propagate inference capabilities as effectively as prompt conditioning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LM knowledge updating / evaluation pipeline",
            "system_description": "Comparative evaluation of methods for injecting/updating entity knowledge in autoregressive language models: (1) inference-time prepending of definitions, (2) parameter-updating methods (finetuning, MEND, MEMIT), and (3) context distillation.",
            "nl_description_type": "paper claims / experimental motivation (natural-language description of desired behaviour and comparison baseline)",
            "code_implementation_type": "training & evaluation scripts implementing finetuning, editing algorithms, and prepending at inference",
            "gap_type": "different operational modality effectiveness (natural-language prompt vs parameter update)",
            "gap_description": "The natural-language description (and intuition) suggests that internalizing information into model parameters should enable the model to make the same inferences it can make when conditioned on the same information in-context. In practice, parameter updates (finetuning or editing methods) frequently produce smaller gains on inference tasks than simply prepending the definition to the context. The paper documents this consistent underperformance of parameter-editing approaches relative to in-context conditioning across multiple benchmarks and base models.",
            "gap_location": "inference procedure / knowledge representation (how knowledge is made available to the model at inference time)",
            "detection_method": "direct experimental comparison of inference-time prepending vs parameter-update methods across benchmarks (ENTITY INFERENCES, ECBD) and multiple base LMs",
            "measurement_method": "Task metrics: accuracy on ENTITY INFERENCES and per-token perplexity (PPL) on ECBD, reporting absolute values and deltas pre/post edit; paired bootstrap significance test for PPL differences (N=10000) when comparing distillation to fine-tuning",
            "impact_on_results": "Prepending the definition often yields the largest improvements (e.g., strongest PPL/accuracy gains). Parameter updates (finetuning/editing) achieve smaller gains; distillation recovers a substantial fraction of prepending gains but does not fully match them. Quantitatively: distillation reduced target perplexity substantially (e.g., -5.7 PPL on GPT-Neo using GPT-3.5 generated transfer set) but prepending achieves even larger reductions (e.g., prepending: -9.1 PPL in one setting). The paper concludes parameter updates are still less effective overall.",
            "frequency_or_prevalence": "Observed consistently across multiple benchmarks (ENTITY INFERENCES, ECBD) and three base models (GPT-Neo, GPT2-XL, LLaMA-2-7B); no aggregate prevalence percentage provided.",
            "root_cause": "Architectural or training-dynamics limitation: parameter updates do not replicate the transient conditioning effect of adding information to context; editing procedures may not change internal reasoning paths that rely on context-conditioning. Implicit assumptions about how knowledge is stored/propagated are therefore incomplete.",
            "mitigation_approach": "Context distillation: generate transfer-set continuations conditioned on the definition and distill the teacher (conditioned model) distribution into the student (unconditioned model) via a KL loss on tokens after the entity mention. Also explore stronger transfer-set generators (GPT-3.5) and ablations on transfer-set construction.",
            "mitigation_effectiveness": "Distillation recovers a substantial fraction of the gains from prepending and outperforms baseline editing methods on many settings (e.g., distillation with GPT-3.5 produced -5.7 PPL vs base while prepending achieved -9.1 PPL on one setting). However, it does not always fully close the gap; the paper explicitly states a 'robust gap' remains.",
            "domain_or_field": "machine learning / natural language processing (LM knowledge editing and evaluation)",
            "reproducibility_impact": false,
            "uuid": "e749.0",
            "source_info": {
                "paper_title": "Propagating Knowledge Updates to LMs Through Distillation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Transfer-Set Hallucination Gap",
            "name_full": "Mismatch between entity definition text and generated transfer-set continuations (hallucinations and lexical divergence)",
            "brief_description": "Generated transfer-set continuations can hallucinate or introduce tokens not present in the original natural-language definition, causing discrepancies between the intended description and the actual examples used for distillation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Transfer-set generation component of context distillation pipeline",
            "system_description": "A generator model (either the base LM or an external model like GPT-3.5) is prompted with the entity definition to produce continuations that form the transfer set for distillation.",
            "nl_description_type": "specification of transfer-set construction derived from natural-language definition",
            "code_implementation_type": "generation scripts calling LM generator (sampling with nucleus sampling, temperature, max length parameters)",
            "gap_type": "input-output mismatch / hallucination (lexical mismatch between specification and generated examples)",
            "gap_description": "The paper notes that continuations produced by different generator models contain varying amounts of text copied from the definition; smaller models hallucinate more and have lower token overlap with the definition. These differences mean that the transfer set used for distillation may not faithfully reflect the definition's content, sometimes adding extra (possibly incorrect) information.",
            "gap_location": "data generation / transfer-set creation",
            "detection_method": "manual inspection of generated continuations and automatic token-overlap statistics between continuations and definition sentences",
            "measurement_method": "Reported % of tokens in continuations that appeared in the definition sentence (Table 1: e.g., GPT-3.5 56.4% overlap vs GPT-Neo ~35%); also measure distillation downstream performance (accuracy/PPL) when using different generators.",
            "impact_on_results": "Generator quality affects distillation performance: transfer sets from GPT-3.5 yield better propagation gains than those from the base model in several settings (sometimes even outperforming prepending because GPT-3.5 may introduce additional information). Token-overlap differences correspond to differences in per-token NLL reductions and final task gains.",
            "frequency_or_prevalence": "Observed across generator choices and base models in experiments; no global prevalence statistics beyond the reported generator comparisons.",
            "root_cause": "Differences in generator model capacity and training (larger models hallucinate less and better adhere to prompt-defined content); prompt formulation and sampling hyperparameters also influence outputs.",
            "mitigation_approach": "Use stronger generator models (GPT-3.5) to produce transfer sets; if unavailable, use base model-generated continuations but consider appending the entity name or cleaning; perform ablations to ensure transfer-set robustness (e.g., repeating continuations, varying number of unique continuations).",
            "mitigation_effectiveness": "Using GPT-3.5 improved distillation performance (higher accuracy / larger PPL reductions). However, distillation from the base model remained competitive, indicating mitigation is helpful but not strictly required.",
            "domain_or_field": "machine learning / natural language processing (data generation for model distillation)",
            "reproducibility_impact": false,
            "uuid": "e749.1",
            "source_info": {
                "paper_title": "Propagating Knowledge Updates to LMs Through Distillation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Supervision-Signal Gap",
            "name_full": "Difference between distillation (KL to conditioned teacher) and fine-tuning (NLL) supervision leading to divergent outcomes",
            "brief_description": "Distillation using teacher-conditioned distributions provides a different supervisory signal than fine-tuning on the transfer set or definition (maximum-likelihood), resulting in different learning dynamics and better propagation with less specificity loss in many cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Distillation vs finetuning training procedures",
            "system_description": "Two training paradigms for injecting knowledge: (a) finetuning on definition or on transfer-set examples with NLL loss (teacher forcing), and (b) KL-distillation where the student matches the teacher's conditioned distributions on the transfer set.",
            "nl_description_type": "algorithmic description / training objective specification in the methods section",
            "code_implementation_type": "training code implementing either NLL-based finetuning or KL-based distillation updates",
            "gap_type": "different algorithm variant / loss-function mismatch (natural-language description vs implemented update objective)",
            "gap_description": "Although both approaches are described in the paper, they imply different intended behaviors: finetuning on generated continuations might be expected to teach similar inferences as distillation, but in practice distillation is more effective at propagating knowledge while preserving specificity. The paper characterizes how tokens changed differently under conditioning (KL) than under NLL/fine-tuning.",
            "gap_location": "training objective / update rule",
            "detection_method": "Empirical comparison of methods on ECBD and ENTITY INFERENCES, per-token NLL analysis plotting NLL before vs after conditioning, and ablation studies replacing objectives and datasets.",
            "measurement_method": "Task metrics (accuracy, PPL), per-token NLL fractional reduction plots (Figure 3), and paired bootstrap significance testing for PPL comparisons.",
            "impact_on_results": "Distillation often outperforms finetuning on the transfer set in target metrics (e.g., significant PPL reductions) with smaller specificity drops. Example: distillation (GPT-3.5) achieved -5.7 PPL vs base on GPT-Neo, and gains over finetuning were significant (p &lt; 0.05). Fine-tuning sometimes severely harmed specificity compared to distillation.",
            "frequency_or_prevalence": "Demonstrated across multiple models and datasets in the paper; not measured as a proportion of studies.",
            "root_cause": "Different loss formulations emphasize matching teacher distributions (soft targets) versus maximizing likelihood of specific token sequences; KL distillation focuses updates on tokens where teacher/student distributions differ, producing more targeted edits.",
            "mitigation_approach": "Adopt context distillation (KL to conditioned teacher) rather than raw finetuning; tune transfer-set construction and number of continuations; temperature scaling to soften distributions.",
            "mitigation_effectiveness": "Distillation produced consistent improvements versus finetuning on target metrics with smaller specificity degradation; statistical tests confirm significance in several comparisons.",
            "domain_or_field": "machine learning / optimization for LM editing",
            "reproducibility_impact": false,
            "uuid": "e749.2",
            "source_info": {
                "paper_title": "Propagating Knowledge Updates to LMs Through Distillation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Definition-TransferSet Mismatch",
            "name_full": "Misalignment when wrong/ random definitions or transfer sets are paired for distillation",
            "brief_description": "A discrepancy where using incorrect natural-language definitions or mismatched transfer sets (randomly swapped between entities) leads to degraded or misleading model updates, quantifiably harming inference performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Distillation pipeline with transfer-set / definition pairing",
            "system_description": "The pipeline expects a correct pairing of an entity's definition and its generated transfer-set continuations; ablations test swapping the definition or transfer set with random ones.",
            "nl_description_type": "experimental protocol specification (which definition pairs with which transfer set)",
            "code_implementation_type": "data preparation and distillation training scripts",
            "gap_type": "incorrect mapping / data-label mismatch (specification vs data used in code)",
            "gap_description": "If the wrong definition is distilled (randomly sampled entity) even with a correct transfer set, perplexity on target probes increases. If the correct definition is paired with a random transfer set but the entity name is prepended, distillation can still reduce perplexity; nonetheless, the pairing matters and mismatches create measurable harms.",
            "gap_location": "data pairing / preprocessing stage",
            "detection_method": "Ablation experiments swapping definitions and transfer sets and measuring resulting performance changes (Table 4)",
            "measurement_method": "Change in target perplexity (PPL) relative to base model; examples: 'Random definition + correct transfer set' yields +2.6 PPL (worse), while 'Correct definition + random transfer set + entity string' yields -4.3 PPL (better).",
            "impact_on_results": "Using a random definition increased PPL (+2.6), showing clear degradation; prepending a random definition at inference also causes large degradation (+11.9 PPL in one case). These are concrete quantitative impacts on evaluation metrics.",
            "frequency_or_prevalence": "Observed in targeted ablations; not reported as a broad prevalence statistic.",
            "root_cause": "Mistakes or ambiguities in dataset curation or pairing logic; reliance on correct alignment between natural-language descriptions and generated continuations.",
            "mitigation_approach": "Ensure correct pairing of definitions and transfer sets, prepend entity name when necessary to enforce mention presence, and perform sanity-check ablations; prefer generators that include the entity string reliably.",
            "mitigation_effectiveness": "Appending entity strings to continuations and using the correct definition substantially mitigates negative effects; experiments show corrected pairings restore or improve PPL.",
            "domain_or_field": "machine learning / dataset preprocessing for distillation",
            "reproducibility_impact": false,
            "uuid": "e749.3",
            "source_info": {
                "paper_title": "Propagating Knowledge Updates to LMs Through Distillation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Multi-Edit Scaling Gap",
            "name_full": "Scalability mismatch when injecting many entities at once (behavior differs across editing methods)",
            "brief_description": "Different editing methods scale differently when updating many entities: some specialized editing algorithms degrade when many facts are edited, while distillation remains more robust but can still affect specificity depending on base model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Large-scale multi-entity editing pipeline",
            "system_description": "Procedure that aggregates transfer sets for many entities, shuffles them, and performs distillation or applies alternative editing methods (MEMIT, ROME) to update many facts in a model in one run.",
            "nl_description_type": "method scaling claim / experimental protocol for multiple edits",
            "code_implementation_type": "batch distillation training scripts or rank-one weight-editing implementations (MEMIT/ROME)",
            "gap_type": "scalability / algorithmic limitation (implementation does not match expected scaling behavior from natural-language claims)",
            "gap_description": "While some prior editing methods (e.g., MEMIT) claim mass-editing capability, experiments show that MEMIT increases perplexity when injecting more than ~25 entities at once. Distillation scales to more entities (tested up to 150) with better target performance but may degrade specificity for certain base models (GPT-Neo). Thus, the practical scaling behavior deviates from optimistic descriptions.",
            "gap_location": "training procedure / batch editing and capacity limits",
            "detection_method": "Empirical scaling experiment varying number of entities edited (10 to 150) and comparing methods (distillation vs MEMIT) and tracking target PPL and specificity",
            "measurement_method": "Average target perplexity and specificity delta across runs (Figure 4); observe increases in PPL for MEMIT beyond 25 entities and varying specificity results for distillation depending on base LM.",
            "impact_on_results": "MEMIT shows increased perplexity (worse performance) when injecting more than ~25 entities; distillation generally outperforms MEMIT up to 150 entities in target performance, but specificity degradation was observed for GPT-Neo. These effects influence the feasibility of large-scale updates.",
            "frequency_or_prevalence": "Observed in the paper's scaling experiments; prevalence across other models/datasets not quantified but indicated as an open limitation for very large numbers (thousands/millions) of edits.",
            "root_cause": "Architectural assumptions of editing methods (rank-one MLP modifications) and capacity constraints; interaction effects when many edits are applied cause interference.",
            "mitigation_approach": "Use context distillation with shuffled aggregated transfer sets; monitor specificity and target metrics as edits scale; further research needed to assess injection of thousands/millions of entities and to design more robust multi-edit algorithms.",
            "mitigation_effectiveness": "Distillation outperforms MEMIT up to 150 entities in this work; however, some specificity degradation remains for certain base LMs, so mitigation is partially effective but incomplete.",
            "domain_or_field": "machine learning / large-scale model editing",
            "reproducibility_impact": false,
            "uuid": "e749.4",
            "source_info": {
                "paper_title": "Propagating Knowledge Updates to LMs Through Distillation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Counterfactual Specificity Gap",
            "name_full": "Different consequences when editing models with counterfactual (false) natural-language statements vs factual ones",
            "brief_description": "Editing with blatantly false statements (counterfactual edits) produces different trade-offs: some methods (ROME, FT) achieve high efficacy/generalization but poor specificity, while distillation yields better (though still poor) specificity but lower efficacy/generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Counterfactual editing evaluation (CounterFact benchmark)",
            "system_description": "Evaluation of editing algorithms' ability to inject counterfactual triplets into model parameters and measure efficacy (success on edited fact), paraphrase generalization, and neighborhood/locality (specificity).",
            "nl_description_type": "evaluation benchmark specification and desired edit properties (efficacy, generalization, locality)",
            "code_implementation_type": "edit application scripts for ROME, FT, FT+L, Distillation and evaluation scripts computing efficacy/paraphrase/neighborhood scores",
            "gap_type": "mismatch between expected edit behavior (from natural-language description of method) and empirical specificity/locality outcomes for counterfactuals",
            "gap_description": "The paper reports that methods which perform well for factual or emerging-entity edits may behave differently when injecting false statements. Distillation, which preserved specificity for factual edits in other benchmarks, suffered worse specificity trade-offs when applied to counterfactual edits.",
            "gap_location": "edit objective and evaluation (how natural-language counterfactuals are implemented in editing code and their downstream effects)",
            "detection_method": "Empirical evaluation on a random sample of CounterFact entries with standard metrics (efficacy, paraphrase, neighborhood) and comparison across editors (FT, FT+L, ROME, Distillation)",
            "measurement_method": "Reported efficacy, paraphrase, and neighborhood scores and an aggregated harmonic-mean score (Table 5). Distillation achieved lower efficacy/paraphrase but higher neighborhood (specificity) than some other methods.",
            "impact_on_results": "Distillation had lower efficacy and paraphrase scores compared to ROME and FT, but improved neighborhood (specificity) compared to them; FT+L (constrained fine-tuning) achieved the best aggregate score. This shows editing strategy impacts safety/locality when injecting false info.",
            "frequency_or_prevalence": "Observed in CounterFact experiments (150-sample subset); not generalized beyond this benchmark in the paper.",
            "root_cause": "Nature of edit target (false assertions) interacts with update dynamics differently than factual additions; objective functions and constraints used by editors lead to different trade-offs between efficacy and locality.",
            "mitigation_approach": "Evaluate constrained fine-tuning (FT+L) with L_infty constraints on weight changes to improve specificity; consider edit-specific objectives and improved locality checks.",
            "mitigation_effectiveness": "FT+L improved aggregate score (best among evaluated methods) by improving specificity, though generalization remained mediocre; distillation improved specificity relative to some methods but at a cost to efficacy.",
            "domain_or_field": "machine learning / model editing and safety evaluation",
            "reproducibility_impact": false,
            "uuid": "e749.5",
            "source_info": {
                "paper_title": "Propagating Knowledge Updates to LMs Through Distillation",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
            "rating": 2
        },
        {
            "paper_title": "MassEditing Memory in a Transformer",
            "rating": 2
        },
        {
            "paper_title": "Locating and Editing Factual Associations in GPT",
            "rating": 2
        },
        {
            "paper_title": "Fast Model Editing at Scale",
            "rating": 2
        },
        {
            "paper_title": "Detecting edit failures in large language models: An improved specificity benchmark",
            "rating": 2
        },
        {
            "paper_title": "A General Language Assistant as a Laboratory for Alignment",
            "rating": 1
        }
    ],
    "cost": 0.0174715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Propagating Knowledge Updates to LMs Through Distillation</h1>
<p>Shankar Padmanabhan, Yasumasa Onoe, Michael J.Q. Zhang, Greg Durrett, Eunsol Choi<br>Department of Computer Science<br>The University of Texas at Austin<br>shankarpadmanabhan@utexas.edu</p>
<h4>Abstract</h4>
<p>Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than finetuning and other gradient-based knowledge-editing methods. Moreover, it does not compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.</p>
<h2>1 Introduction</h2>
<p>As large language models (LLMs) are used for a wider variety of applications, it is crucial to ensure that they contain up-to-date information about the world. One potential solution is retrieval augmentation, which prepends retrieved texts to the language model's context [20, 29, 35, 34]. However, this raises inference costs and becomes impractical when updating large amounts of information. An alternative approach, and our goal in this work, is to internalize the new knowledge into the language model via parameter updates [36, 44, 8, 26, 22, 12].
Recent work on injecting LLMs with information about emerging entities [32] demonstrates that updating parameters effectively enables models to acquire updated facts (Rishi Sunak is the prime minister of the UK), but struggles to teach models how to propagate this knowledge, or make inferences based on it (what might Rishi Sunak do tomorrow?). This contrasts with results from retrieval augmentation [20, 35] and chain-of-thought prompting [40], which show that LLMs can make such inferences when information is placed in the prompt.
This work aims to bridge the gap between the two approaches in knowledge injection. We use a form of knowledge distillation [13] called context distillation [1] that updates an LM to act like it is conditioned on a given context, even when that context is not shown. Our approach consists of two steps: transfer set generation and distillation on the generated transfer set. The transfer set consists of continuations of the entity definition sentence generated by prompting a language model. To distill on this transfer set, we minimize the Kullback-Leibler (KL) divergence between the model's predictions</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our distillation approach. Our goal is to inject the entity definition (d<sub>e</sub>) into the student model (M<sub>s</sub>) and propagate it to make inferences based on the injected knowledge. This example uses ChatGPT as a new entity. We first generate a set of continuations of the entity's definition using a generator model (Step 1), then use these to distill the information from definition into the student model via a KL loss between the conditioned and unconditioned models (Step 2); see Section 3 for formulation.</p>
<p>on the transfer set when it conditions on the definition (the "teacher" for distillation) and when it does not (the "student", or the language model itself). Figure 1 shows this approach.</p>
<p>We evaluate our approach on two knowledge propagation benchmarks: <strong>ENTITY INFERENCES</strong> [32] and Entity Cloze by Date (ECBD) [31]. We evaluate on three language models and find that our distillation approach outperforms fine-tuning and prior editing methods (MEND [26] and MEMIT [23]) across all models. To investigate the robustness of our approach, we present an ablation study focusing on the design choices during transfer set construction. Encouragingly, we find that distilling on transfer sets constructed from the base language model itself is competitive with those generated by a much larger model (GPT-3.5). This demonstrates that context distillation does not rely on distilling from a larger model, and that our approach can work across a range of model sizes. Finally, we show that our approach can be scaled to inject larger amounts of information at once: we can inject over 100 new entities into a language model with minimal performance degradation, suggesting that the distillation process performs relatively targeted editing even without additional objectives to ensure specificity as in past methods [22, 23].</p>
<p>To summarize, we present a new approach for propagating injected knowledge. We show that a knowledge distillation technique can effectively impart and propagate knowledge from entity definitions into the parameters of a pre-trained language model, compared to existing knowledge editing methods. Yet, we observe robust gap between providing information in-context and parameter updating methods, leaving ample room for future work. Our code and data are available at https://github.com/shankarp8/knowledge_distillation.</p>
<h2>2 Background and Task Setup</h2>
<h3>2.1 Motivating Example</h3>
<p>Figure 1 shows a motivating example. An LM trained on text collected prior to November 2022 will not have specific knowledge about what ChatGPT is, as ChatGPT was introduced after that time. Past retrieval-augmented generation methods [20] have shown that conditioning on information about this entity can lead to lower perplexities when evaluating on sentences like ChatGPT can respond to natural language questions [35, 32]. For example, the model assigns a higher likelihood to tokens like respond given the knowledge that ChatGPT is a chatbot.</p>
<p>Our approach relies on teaching a "student model" (the LM itself) to match the next-token distributions given by the model conditioned on the definition sentence even when the definition sentence is not shown. We do this via a distillation process on a set of continuations, or sampled sentences following</p>
<p>the definition. We impose a KL penalty between the student and teacher distributions of a set of target tokens, namely all those occurring after ChatGPT in the continuation. Because the distillation process does not make updates on tokens where the teacher and student have the same distribution (zero KL), only tokens that are in some way predictable from the definition drive parameter updates (see Section 7 for discussion).</p>
<h1>2.2 Task Setup</h1>
<p>We refer to language models $M$ as $M(\mathbf{x}) \rightarrow \mathcal{D}(\mathcal{V})$, mapping an input context $\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right)$ to a next-word distribution $\mathcal{D}(\mathcal{V})=p\left(\cdot \mid x_{1}, \ldots, x_{n}\right)$ over a vocabulary $\mathcal{V}$. We will also use $M(\mathbf{x}) \rightarrow \mathcal{D}(\mathcal{V})<em _Base="{Base" _text="\text">{1, \ldots, n}$ to represent the collection of distributions after each prefix of $\mathbf{x}$, which is a standard operation used in language model training. To update knowledge in the base language model $M</em>}}$, definitional information $\mathbf{d<em 1="1">{e}=\left(d</em>}, \ldots, d_{m}\right)$ for an entity $e$ is provided. We will use $e$ both as an indicator and also a reference to the entity name string (e.g., ChatGPT in Figure 1). Our goal is to update $M_{\text {Base }}$ to $M_{s}$ so that it "knows" $\mathbf{d<em s="s">{e}$, by matching $M</em>}(\mathbf{x})$ with $M_{t}\left(\mathbf{x} \mid \mathbf{d<em t="t">{e}\right)$ (the teacher model) as closely as possible with our distillation scheme, when $\mathbf{x}$ is relevant to entity $e$. We set the teacher model $M</em>$.
We evaluate on two factors. First, propagation success measures how well the updated language model $M_{s}$ acquired information about $\mathbf{d}_{e}$ to make correct inferences in probe sentences. Crucially, our evaluation here is not just a narrow notion of whether a specific fact is injected [44, 8, 26, 22, inter alia], but captures the model's ability to make inferences on it [31, 32]. Second, specificity evaluates whether the predictions of the LM on other contexts are altered as in prior work [8, 26, 22, 23]. Ideally, edits should not impact inferences on examples unrelated to the edit.}$ to be a copy of $M_{s</p>
<h3>2.3 Related work</h3>
<p>Knowledge distillation We are not aware of prior work that uses distillation for knowledge editing. Our use of context distillation is most similar to Askell et al.'s alignment work [1]; however, they use it in a phase roughly analogous to RLHF and use a generic transfer set sampled from the language model training corpus. Our work is also related to prompt injection [5], which examines tasks like distilling a persona-conditioned language model. Unlike their work, we do not train a task-specific model to generate examples for distillation. Instead, we simply prompt existing LMs. Furthermore, while they aim to have a model memorize a particular prompt, we focus on general knowledge updates and inferences based on those. Other work has used distillation techniques for "gisting" to make shorter prompts [28] or to distill reasoning processes [37]. Similar approaches as our continuation sampling have been used for example extrapolation [19] to generate training datasets for fine-tuning.</p>
<p>Efficient parametric knowledge updates Parameter updating methods such as KnowledgeEditor [8] and MEND [26] make use of standard fine-tuning to attempt to localize edits. Another line of work $[7,22,23]$ attempts to locate where factual information is stored in transformers and designs edit methods based on these findings. In particular, ROME [22] and MEMIT [23] treat factual knowledge as subject-relation-object tuples, and find that new facts can be inserted into particular early and middle layer MLPs within a GPT-style transformer using specialized update rules. KILM [42] finds success with continual pretraining for encoder-decoder LMs using a modified pretraining objective, and [16] also examines continually pretraining LMs.</p>
<p>Knowledge update tasks Most prior work [22, 26] in knowledge updating focuses on evaluation of a targeted update. Because our goal is to test propagation of knowledge, we mainly focus on two benchmarks from Onoe et al. [32]. Besides this benchmark, recent work [43, 39, 6] also evaluates the LM's success at performing multi-hop inferences with the edited information. Compared to the benchmarks we evaluate on, which are taken from Wikipedia sentences, these benchmarks use sentences generated from knowledge base relations. Another line of work evaluates the ability of LMs to reason about emerging entities [18, 9, 17]. However, such benchmarks do not fit our task setting as they do not provide the information to inject.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">Knowledge</span><span class="w"> </span><span class="nt">Propagation</span><span class="w"> </span><span class="nt">Through</span><span class="w"> </span><span class="nt">Distillation</span>
<span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">An</span><span class="w"> </span><span class="nt">entity</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">e</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">its</span><span class="w"> </span><span class="nt">definition</span><span class="w"> </span><span class="nt">sentence</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">d</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">e</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">base</span><span class="w"> </span><span class="nt">LM</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{Base</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">),</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">LM</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M_</span><span class="p">{</span><span class="err">g</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">generate</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">transfer</span><span class="w"> </span><span class="nt">set</span><span class="o">,</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">a</span>
<span class="nt">prompt</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">p</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">transfer</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="nt">generation</span><span class="o">.</span>
<span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="nt">An</span><span class="w"> </span><span class="nt">updated</span><span class="w"> </span><span class="nt">model</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="err">\</span><span class="o">).</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">e</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">c</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">c</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">N</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">where</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">c</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">M_</span><span class="p">{</span><span class="err">g</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="o">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">p</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">d</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">continuations</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">form</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">transfer</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">M_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{Base</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Create</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">student</span><span class="w"> </span><span class="nt">LM</span>
<span class="nt">3</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">M_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Create</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">teacher</span><span class="w"> </span><span class="nt">LM</span>
<span class="nt">4</span><span class="o">:</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">c</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">e</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Iterate</span><span class="w"> </span><span class="nt">through</span><span class="w"> </span><span class="nt">all</span><span class="w"> </span><span class="nt">continuations</span>
<span class="nt">5</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">=</span><span class="nt">M_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="o">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">d</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="o">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">c</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Compute</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">teacher</span><span class="w"> </span><span class="nt">distribution</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">token</span>
<span class="nt">6</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">ell_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">Find</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">c</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">e</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Find</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">end</span><span class="w"> </span><span class="nt">token</span><span class="w"> </span><span class="nt">index</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">entity</span><span class="w"> </span><span class="nt">mention</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">c</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="nt">7</span><span class="o">:</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="err">\</span><span class="p">{</span><span class="err">1,</span><span class="w"> </span><span class="err">\ldots,</span><span class="w"> </span><span class="err">K\</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">K</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">epochs</span>
<span class="nt">8</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="o">=</span><span class="nt">M_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="o">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">c</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Compute</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">student</span><span class="w"> </span><span class="nt">distribution</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">token</span>
<span class="nt">9</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">frac</span><span class="p">{</span><span class="err">1</span><span class="p">}{</span><span class="err">\left|\mathbf{c</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">|</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">ell_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sum_</span><span class="p">{</span><span class="err">j=t_{i</span><span class="p">}</span><span class="o">+</span><span class="nt">1</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">\left|\mathbf{c</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">|</span><span class="err">}</span><span class="w"> </span><span class="nt">D_</span><span class="p">{</span><span class="err">K</span><span class="w"> </span><span class="err">L</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t,\left|\mathbf{d</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">e</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">|+</span><span class="nt">j</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="o">|</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">s,</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="nt">10</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="nt">M_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">nabla</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">w</span><span class="p">.</span><span class="nc">r</span><span class="p">.</span><span class="nc">t</span><span class="o">.</span><span class="w"> </span><span class="nt">avg</span><span class="o">.</span><span class="w"> </span><span class="nt">KL</span><span class="w"> </span><span class="nt">over</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">tokens</span><span class="w"> </span><span class="nt">after</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">entity</span><span class="w"> </span><span class="nt">mention</span>
</code></pre></div>

<h1>3 Method</h1>
<p>Our method is illustrated in Figure 1 and described formally in Algorithm 1. It consists of two steps: transfer set generation and distillation on the generated transfer set.</p>
<p>Transfer set generation First, we generate a transfer set corresponding to $\mathbf{d}<em e="e">{e}$, written as $\mathbf{T}</em>}=$ $\left{\mathbf{c<em 2="2">{1}, \mathbf{c}</em>}, \cdots, \mathbf{c<em g="g">{N}\right}$. We do this by sampling $N$ distinct continuations from our generator model $M</em>}$ with a prompt $\mathbf{p}$ followed by the entity definition $\mathbf{d<em _Base="{Base" _text="\text">{\mathbf{e}}$; we will either use GPT-3.5 or the base LM $M</em>$.
Each continuation must contain an identifiable reference to the entity string $e$. We describe how we ensure this in Section 5. We use $\ell_{i}$ to refer to the fencepost index where this entity string ends in the continuation sentence $\mathbf{c}}}=M_{s}$ as the generator model $M_{g<em i="i">{i}$; for example, in Figure $1, \ell</em>$ or later. Tokens before do not condition on the entity name in the student and risk making broad updates to the model, which can impact specificity negatively.}=2$ with 1-based indexing to indicate the mention string ChatGPT ends before the second token. Crucially, we only want to distill losses when predicting tokens located at position $\ell_{i</p>
<p>Distillation We initialize an LM $M_{s}$ from its original pretrained checkpoint, as well as a copy of the LM, $M_{t}$, to serve as the teacher model during the distillation process. Then, for each continuation $\mathbf{c}<em s="s">{i}$ in the transfer set, we compute the student model's distributions $M</em>}\left(\mathbf{c<em i="i">{i}\right)$ (a sequence of $\left|\mathbf{c}</em>}\right|$ distributions) as well as the teacher model's distributions conditioned on the definition, $M_{t}\left(\mathbf{c<em e="e">{i} \mid \mathbf{d}</em>$ based on this loss. This is done for $K$ epochs.}\right)$. We compute the KL divergence summed over the tokens after $\ell$ (line 8). Finally, we perform a gradient update on $M_{s</p>
<p>Scaling knowledge injection We can easily generalize this algorithm to inject information about multiple entities at once. We take the union of transfer sets belonging to different entities, shuffle them, and distill on each transfer example as described in line 4-9. We evaluate this setting in Section 7.2.</p>
<h2>4 Evaluating Knowledge Propagation</h2>
<p>To evaluate our approach on entity knowledge propagation (EKP), we closely follow the setup laid out in Onoe et al. [32]. Here, we describe two datasets and metrics for completeness. The details about the datasets (statistics, examples) can be found in Appendix A.</p>
<p>Data We evaluate on two datasets. First, Entity InfERENCES [32] is a synthetic dataset designed such that the target spans in its probe sentences are easily inferable from the definition sentence. For example, given a definition sentence describing Dracula is a drama horror television series, models are asked to complete the following probe sentence: Dracula makes me $\qquad$ from multiple choice options (e.g., scared, atheletic, etc).
Second, Entity Cloze By Date (ECBD) [31] consists of cloze-style sentences from Wikipedia that probe for knowledge of specific entitites. Examples in ECBD are separated by each entity's</p>
<p>origination date (e.g., when an event occured). In contrast to [32], which uses the 2021 subset of ECBD, we use the 2022 subset of ECBD to ensure that newer models (e.g. GPT-3.5) do not have knowledge of the probed entities beyond the definition they condition on; see Appendix A.3 for more discussion of the temporal cutoffs for our models and datasets. Each example consists of a cloze-style probe sentence prefix $\mathbf{x}$ about an entity $e$ followed by a target span $\mathbf{y}$. The definition $\mathbf{d}_{e}$ is taken from the first sentence of the entity's Wikipedia page.</p>
<p>Evaluation Metrics For Entity Inferences, we measure propagation success by reporting accuracy in predicting the correct gold label among label options. We measure specificity by evaluating the model's accuracy at predicting gold spans on similar probe sentences across all other entities.</p>
<p>We evaluate on ECBD by computing per-token perplexity of the continuation given the probe prefix, $P P L(\mathbf{y} \mid \mathbf{x})$. This metric is not directly comparable across base LMs which have different tokenizers. To evaluate propagation success, we report the decrease in perplexity from the edit, $P P L(\mathbf{y} \mid \mathbf{x} ; M_{\text {Base }}$ ) vs. $P P L\left(\mathbf{y} \mid \mathbf{x} ; M_{s}\right)$. To evaluate an edit's specificity, we randomly sample 40 examples from the "popular" subset of ECBD, ensuring that all 40 probes are about unique entities. We then report the change in perplexity on these sampled examples before and after the edit, using the same metric as above for evaluating on the target sentence.</p>
<h1>5 Experimental Setting</h1>
<p>Base Models We consider three autoregressive language models: GPT-Neo-1.3B [3], GPT2-XL [33] (1.5B), and LLaMA-2-7B [38]. The former two models have minimal knowledge of the entities in Entity Inferences and ECBD from their pretraining corpora as the entities in these datasets emerged after their pre-training.</p>
<p>Transfer Set Generation We experiment with two types of generator models: a state-of-the-art model learned from human feedback data (GPT-3.5, text-davinci-003), which can generate highly fluent transfer sentences from the definition sentence, and the base model itself, which presents a more realistic scenario in which we do not assume a better LM than the base LM that we are updating. For both models, we use a simple prompt to elicit a continuation of the definition sentence and sample five transfer sentences for each entity. For generation, we use nucleus sampling [15] with $p=0.9$, a temperature of 1.0 , and a max length of 40 tokens.</p>
<p>Table 1 summarizes the statistics of transfer sets. Upon manual inspection, we find that GPT-3.5 hallucinates substantially less than smaller models, as reflected in $\%$ of tokens in the continuations that appeared in the definition sentence. For continuations that do not contain the entity name, we simply prepend the entity name onto the continuation. We also report the number of tokens after $l$, i.e., the number of tokens which we compute the distillation loss on. The exact prompt and example continuations can be found in Appendix C.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$M_{g}$</th>
<th style="text-align: center;"># Tokens</th>
<th style="text-align: center;">\% Token <br> in $E_{d}$</th>
<th style="text-align: right;"># Tokens <br> after $l$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: right;">33.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT2-XL</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: right;">30.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: right;">31.6</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: right;">26.4</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics for transfer set sentences generated by each generator model.</p>
<h3>5.1 Comparison Systems</h3>
<p>We compare against two paradigms for knowledge injection: prepending new knowledge in-context at inference time and updating the parameters of LMs. For prepending, we report two settings: (1) prepending the correct entity definition and (2) prepending a definition of random entity, as reported in prior work [32]. Next, we describe knowledge updating methods below.
Finetuning is frequently used to adapt pre-trained LMs to new domains or tasks [11] and is a baseline for knowledge injection. We train $M_{\text {Base }}$ on $\mathbf{d}<em s="s">{e}$ with standard negative log likelihood loss on the sequence (teacher forcing). We investigate fine-tuning the full model, as well as only the last layer.
We also compare to finetuning with the transfer set. First, we fine-tune $M</em>}$ on the definition. Then, for each sentence in our transfer set $\mathbf{T<em 1="1">{e}=\left(\mathbf{c}</em>}, \ldots \mathbf{c<em s="s">{N}\right)$, we fine-tune on $M</em>}\left(\mathbf{c<em e="e">{i} \mid \mathbf{d}</em>\right)$, conditioning</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPT-NEO-1.3B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT2-XL</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pre-Edit Accuracy ( $\uparrow$ )</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Target $(\Delta)$</td>
<td style="text-align: center;">Spec. $(\Delta)$</td>
<td style="text-align: center;">Target $(\Delta)$</td>
<td style="text-align: center;">Spec. $(\Delta)$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning on $\mathbf{d}_{e}$ (full)</td>
<td style="text-align: center;">$57.7(+23.6)$</td>
<td style="text-align: center;">$18.3(-15.9)$</td>
<td style="text-align: center;">$62.9(+30.0)$</td>
<td style="text-align: center;">$24.1(-8.8)$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning on $\mathbf{d}_{e}$ (last only)</td>
<td style="text-align: center;">$48.8(+14.7)$</td>
<td style="text-align: center;">$16.4(-17.7)$</td>
<td style="text-align: center;">$46.5(+13.6)$</td>
<td style="text-align: center;">$\mathbf{3 5 . 4 (+ 2 . 5 )}$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning on $\mathbf{d}<em e="e">{e}+\mathbf{T}</em>$ (full)</td>
<td style="text-align: center;">$\mathbf{6 6 . 5 (+ 3 2 . 4 )}$</td>
<td style="text-align: center;">$28.8(-5.3)$</td>
<td style="text-align: center;">$59.4(+26.5)$</td>
<td style="text-align: center;">$33.8(+0.9)$</td>
</tr>
<tr>
<td style="text-align: left;">MEND</td>
<td style="text-align: center;">$41.8(+7.7)$</td>
<td style="text-align: center;">$\mathbf{3 4 . 4 (+ 0 . 3 )}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Distillation $\left(M_{g}=M_{e}\right)$</td>
<td style="text-align: center;">$61.8(+27.7)$</td>
<td style="text-align: center;">$32.6(-1.6)$</td>
<td style="text-align: center;">$58.2(+25.3)$</td>
<td style="text-align: center;">$31.4(-1.5)$</td>
</tr>
<tr>
<td style="text-align: left;">Distillation $\left(M_{g}=\mathbf{G P T 3 . 5}\right)$</td>
<td style="text-align: center;">$65.9(+31.8)$</td>
<td style="text-align: center;">$32.5(-1.6)$</td>
<td style="text-align: center;">$\mathbf{6 5 . 3 (+ 3 2 . 4 )}$</td>
<td style="text-align: center;">$28.7(-4.2)$</td>
</tr>
<tr>
<td style="text-align: left;">Prepend Def.</td>
<td style="text-align: center;">$60.0(+25.9)$</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">$64.1(+31.2)$</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: left;">Prepend Random Def.</td>
<td style="text-align: center;">$27.7(-6.4)$</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">$26.5(-6.4)$</td>
<td style="text-align: center;">32.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Results (accuracy) on ENTITY INFERENCES. Non-bolded lines are taken from prior work [32]. Before the edit, accuracy was 34.1 for GPT-Neo and 32.9 for GPT2-XL.
on $\mathbf{d}<em i="i">{e}$ and only updating the model on the tokens after the entity occurrence $\ell$ in $\mathbf{c}</em>$ to make updates more comparable to our distillation setting. Here, we use the transfer set generated by GPT-3.5.</p>
<p>MEND [26] is a hypernetwork that uses a set of smaller editing networks to make fast, local edits to a model's weights. MEND transforms the gradient obtained from traditional fine-tuning using a low-rank approximation. We train MEND editors for GPT-Neo using the WikiText-103 dataset, which utilizes generated text as altered output following the configuration used in the original paper. ${ }^{1}$
MEMIT [23] treats facts as (subject, relation, object) tuples and considers each MLP within an LM as a key-value store [10]. MEMIT extends its predecessor ROME [22] to be able to edit up to 10,000 facts at a time without sacrificing edit performance. Both methods use rank-one modifications to the MLP weights within a pre-chosen transformer layer (in the case of MEMIT, a set of consecutive pre-chosen layers) to edit the factual representations there.
We format the data for MEMIT as follows: For a given definition sentence $\mathbf{d}_{e}$, the subject is the name of the entity $e$, the relation is the part of the sentence before the masked span, and the object is the part of the sentence after the masked span, including the gold span. For details about how masked spans are defined, refer to Onoe et al. [32].</p>
<p>Implementation details We experimented with a variety of learning rates (from 1e-8 to 1e-4) and the numbers of epochs $(K)$ (between 1 and 20) across all experiments using a grid search. We focus on balancing results between performance and specificity; neither are prioritized if it significantly harms the other. The specific values used can be found in Appendix B.1.</p>
<h1>6 Results</h1>
<h3>6.1 Entity Inferences</h3>
<p>We first conduct a smaller scale study on the easier benchmark, ENTITY INFERENCES, where learning about the definition should allow us to guess the target tokens by design. Table 2 reports the results. Our distillation approach shows promising performance in two base models we test. We find that transfer sets generated from GPT-3.5 show substantially better results than transfer sets generated from the base model itself in both datasets. This sometimes even outperforms definition prepending, which might be due to GPT3.5 introducing information about the entity beyond what can be inferred from the definition sentence. Fine-tuning on the definition and transfer set using GPT-Neo does outperform distillation, at the cost of specificity. For GPT2-XL, distillation only outperforms fine-tuning on the definition sentence when using GPT3.5 as a generator model, but still shows a substantial accuracy gain using its own generated sentences ( $24.3 \%$ ). The drop in specificity ( $1.6-4.2 \%$ ) is substantially less severe than fine-tuning on the definition sentence. These results indicate that context distillation teaches models to make simple inferences based on injected knowledge without significantly harming the model's distribution on unrelated concepts.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPT-NEO-1.3B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT2-XL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLaMA-2-7B</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pre-Edit PPL ( $\downarrow$ )</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">8.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Target $(\Delta)$</td>
<td style="text-align: center;">Spec. $(\Delta)$</td>
<td style="text-align: center;">Target $(\Delta)$</td>
<td style="text-align: center;">Spec. $(\Delta)$</td>
<td style="text-align: center;">Target $(\Delta)$</td>
<td style="text-align: center;">Spec. $(\Delta)$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning on $\mathbf{d}_{e}$ (full)</td>
<td style="text-align: center;">$28.5(-2.5)$</td>
<td style="text-align: center;">$26.0(-0.1)$</td>
<td style="text-align: center;">$30.0(-2.9)$</td>
<td style="text-align: center;">$25.4(+0.0)$</td>
<td style="text-align: center;">$9.0(+0.4)$</td>
<td style="text-align: center;">$8.7(-0.1)$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning on $\mathbf{d}_{e}$ (last only)</td>
<td style="text-align: center;">$30.7(-0.3)$</td>
<td style="text-align: center;">$26.1(+0.0)$</td>
<td style="text-align: center;">$32.8(-0.1)$</td>
<td style="text-align: center;">$25.4(+0.0)$</td>
<td style="text-align: center;">$8.5(-0.1)$</td>
<td style="text-align: center;">$8.8(+0.0)$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning on $\mathbf{d}<em e="e">{e}+\mathbf{T}</em>$ (full)</td>
<td style="text-align: center;">$28.9(-2.1)$</td>
<td style="text-align: center;">$26.1(-0.0)$</td>
<td style="text-align: center;">$30.6(-2.3)$</td>
<td style="text-align: center;">$25.5(+0.1)$</td>
<td style="text-align: center;">$8.9(+0.3)$</td>
<td style="text-align: center;">$8.8(+0.0)$</td>
</tr>
<tr>
<td style="text-align: left;">MEND</td>
<td style="text-align: center;">$35.2(+4.2)$</td>
<td style="text-align: center;">$26.4(+0.3)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MEMIT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$32.6(-0.2)$</td>
<td style="text-align: center;">$25.4(+0.0)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Distillation ( $M_{g}=M_{s}$ )</td>
<td style="text-align: center;">$26.0(-5.0)$</td>
<td style="text-align: center;">$25.9(-0.2)$</td>
<td style="text-align: center;">$27.6(-5.3)$</td>
<td style="text-align: center;">$25.2(-0.2)$</td>
<td style="text-align: center;">$8.0(-0.6)$</td>
<td style="text-align: center;">$8.6(-0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">Distillation ( $M_{g}=$ GPT3.5)</td>
<td style="text-align: center;">$\mathbf{2 5 . 3 (-5 . 7 )}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 6 (-0 . 5 )}$</td>
<td style="text-align: center;">$\mathbf{2 6 . 8 (-6 . 1 )}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 1 (-0 . 3 )}$</td>
<td style="text-align: center;">$\mathbf{7 . 8 (-0 . 8 )}$</td>
<td style="text-align: center;">$\mathbf{8 . 6 (-0 . 2 )}$</td>
</tr>
<tr>
<td style="text-align: left;">Prepend Def.</td>
<td style="text-align: center;">$21.9(-9.1)$</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">$24.0(-8.9)$</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">$7.2(-1.4)$</td>
<td style="text-align: center;">8.8</td>
</tr>
<tr>
<td style="text-align: left;">Prepend Random Def.</td>
<td style="text-align: center;">$42.9(+11.9)$</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">$40.3(+7.4)$</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">$8.6(+0.0)$</td>
<td style="text-align: center;">8.8</td>
</tr>
</tbody>
</table>
<p>Table 3: Results (perplexity) on the ECBD 2022 dataset. Our distillation approach outperforms other approaches for GPT-Neo-1.3B, GPT2-XL, and LLaMA-2-7B on target perplexity without impacting specificity, achieving a substantial fraction of the gain from prepending the definition.</p>
<h1>6.2 ECBD</h1>
<p>Table 3 displays our main experimental results on ECBD with three base models. Our context distillation method achieves high performance for all models. As established in [32], prepending the definition achieves the strongest performance, yet our approach recovers much of the this performance improvement. As in ENTITY INFERENCES, using a transfer set generated by GPT-3.5 improves over using a transfer set generated from $M_{e}$, but the difference is much smaller than on ENTITY INFERENCES. These results suggest that our approach may benefit from, but does not require, access to a strong generator model. Fine-tuning the full model decreases the perplexity (2.5-4.0 perplexity drop) with smaller models but increases the perplexity on bigger models. We observe little change in performance with fine-tuning the last layer alone. We found that MEND increases the perplexity, and MEMIT for a single-edit decreases perplexity slightly.
As the dataset is moderately sized, we perform a paired bootstrap test to test for the significance of the improvements in average post-perplexity of distillation (using GPT-3.5 generated continuations) over finetuning all parameters on the definition, drawing $N=10000$ samples [2]. The gains of distillation over fine-tuning are significant with $p&lt;0.05$.</p>
<p>Comparing to domain adaptation: How much does the entity-specific knowledge matter? One possible explanation for our gains is that distillation teaches the model something about the particular domain of probe sentences rather than knowledge about particular entities. We discuss two pieces of evidence for why this can only explain partial gains.
Existing editing methods we test do not significantly affect specificity, while our method leads to a slight decrease in specificity (improvement on unrelated sentences). This may indicate that our model is learning the domain of Wikipedia, but the small magnitude suggests that this alone does not explain the performance gain in target probe sentences.
Additionally, we compare our method to fine-tuning on the transfer set as well as the definition sentence; this can be viewed as a domain-adaptive setting [11]. This generally harms the model's perplexity on the evaluation setting relative to fine-tuning only on the definition sentence, unlike on ENTITY INFERENCES.</p>
<p>Ablation Study We further quantify the impact of knowledge about a specific entity via an ablation study in Table 4. We substitute either the entity definition or the transfer set with those belonging to a different randomly sampled entity. Similar to how prepending random definitions leads to a substantial increase in perplexity (bottom of Table 3, +11.9), distilling a definition of a randomly chosen entity, even when using the correct transfer set, leads to an increase in perplexity (+2.6). This result indicates that using the correct entity definition is crucial. It also shows potential benefits of parameter update methods compared to prepending to the context, as prepending irrelevant information brings a more substantial drop in in performance.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results on GPT-Neo with varying numbers of model updates for fine-tuning and distillation approach. Left: target perplexity; right: perplexity on the definition sentence. Only distillation continues to improve in both target and definition perplexity as the number of updates increase.</p>
<p>Next, we consider replacing the transfer set with a set of ten distinct elements from ten transfer sets of different entities (second row). We find that using the correct definition and a random transfer set decreases perplexity, even outperforming finetuning. Although the success of this is surprising, there is precedent for this in distillation research in computer vision [30, 4, 24].</p>
<table>
<thead>
<tr>
<th>Definition</th>
<th>Transfer Set</th>
<th>Target $(\Delta)$</th>
<th>Specificity $(\Delta)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random</td>
<td>Correct</td>
<td>$33.6(+2.6)$</td>
<td>$25.8(-0.3)$</td>
</tr>
<tr>
<td>Correct</td>
<td>Random</td>
<td>$28.9(-2.1)$</td>
<td>$26.6(+0.5)$</td>
</tr>
<tr>
<td>Correct</td>
<td>Random + Ent. str</td>
<td>$26.7(-4.3)$</td>
<td>$25.7(-0.4)$</td>
</tr>
<tr>
<td>Correct</td>
<td>Correct</td>
<td>$25.3(-5.7)$</td>
<td>$25.6(-0.5)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Distillation ablation study with GPT-Neo as the base model. We report perplexity and delta from the base model.</p>
<p>Furthermore, simply prepending the correct entity name (third row) in front of each element of the random transfer set decreases the perplexity substantially. This further shows that distillation is able to inject the definition even in the presence of a noisy transfer set. This also suggests distillation is mainly injecting information in the definition sentence, not the information in the transfer set.</p>
<h1>7 Analysis</h1>
<h3>7.1 Analyzing Distillation for ECBD</h3>
<p>Does the distillation inject the definition itself? If distillation is teaching the model to make inferences based on the definition, how well does it teach the model about the definition itself? We measure the per-token normalized perplexity on the definition sentence and report the results in Figure 2. Unsurprisingly, fine-tuning on definition sentence significantly drops its perplexity to closer to zero after 5-10 updates. While never trained to directly repeat the definition sentence, distillation also lowers the model's perplexity on the definition sentence significantly, potentially because of lexical overlap between the transfer set and the definition sentence (token overlap of 34.8-56.4\% as shown in Table 1).</p>
<p>Characterizing the supervision from the teacher Context distillation is more effective than finetuning on the transfer set on ECBD dataset; here we characterize the differences in these approaches. Figure 3 shows the negative log likelihood (NLL) for GPT-Neo of the continuations on ECBD 2022 generated by GPT-3.5 without conditioning on the definition (x-axis) vs. the reduction in NLL when conditioning on the definition (y-axis). This is not the KL divergence and therefore not the actual training objective; however, by looking at how NLL values change, we can identify specific tokens whose probabilities are substantially modified, which would indicate a high KL value.</p>
<p>Tokens copied from the definition typically receive the highest decreases. Many tokens not in the definition are relatively unchanged in likelihood, and those in contexts that are not informed by the definition will have low KL divergence and drive small updates during learning. However, we show two examples of tokens not in the definition where conditioning does reduce the NLL substantially. In the first case, Dhaka is guessable given Bangladesh, and in the second, features is semantically related to the definition. By contrast, asset has similar NLL before and after conditioning.</p>
<p>Size of transfer set Throughout our experiments, we used five unique continuations in the transfer set, each of which are distilled over five epochs. Is having diverse continuations necessary for</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Per-token NLL of tokens in continuations before conditioning on definitions and after (fractional reduction). Tokens not in the definition (blue dots) are changed less but do see lower NLL when they are inferable from the definition (examples).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Editing for multiple entities at once. We report the average from three runs with different random seeds for shuffling training data.</p>
<p>Successful distillation? We plot the distillation performance while varying the number of unique continuations in the transfer set from 1 to 10, while also keeping the number of updates the same, in Figure B.3 in the appendix and summarize the results here. Repeating one continuation ten times yields a target perplexity of 25.3, while using ten unique continuations once yields a target perplexity of 23.5. We see diminishing returns from introducing new continuations after 5 continuations, and most of the gains can be achieved with as few as two unique generated continuations. This is in line with prior work [5] which has shown distilling on more examples improves the target performance.</p>
<h3>Results for popular entities</h3>
<p>Our main evaluation is mostly on emerging or tail-end entities, evaluating integrating new information. However, we may wish to consider a setting where we would like to <em>refresh</em> the model's pre-existing knowledge. To evaluate this scenario, we use the popular split of ECBD [31]. This has an identical format to ECBD 2022, but sentences cover popular, well-known entities (such as SpaceX and Eminem) that pre-date the training of the models we test.</p>
<p>We report results in Table 10 in the appendix. In this setting, our distillation approach vastly outperforms fine-tuning, suggesting that distillation can be effective in resurfacing LM knowledge.</p>
<h3>7.2 Scaling to multiple edits</h3>
<p>Prior editing techniques [22] showed limitations in updating multiple facts at once. To evaluate how our approach scales, we perform distillation for multiple entities at once. We aggregate (entity definition, transfer sentence) for each entity and shuffle them for the entire entity set such that they are not ordered by entity during training. Figure 4 reports model performance under this setting, varying the number of entities to be updated from 10 to 150. We find that our method is largely capable of large scale updates, outperforming MEMIT, which shows increased perplexity when injecting more than 25 entities at once. For specificity, both MEMIT and distillation do not show degradation on GPT2-XL, but we observe degradation on GPT-Neo with our distillation method. Results on</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Editor</th>
<th style="text-align: center;">Efficacy Score $\uparrow$</th>
<th style="text-align: center;">Paraphrase Score $\uparrow$</th>
<th style="text-align: center;">Neighborhood Score $+\uparrow$</th>
<th style="text-align: center;">Score $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">26.6</td>
</tr>
<tr>
<td style="text-align: left;">FT</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">25.8</td>
</tr>
<tr>
<td style="text-align: left;">FT + L</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: left;">ROME</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: left;">Distillation</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">42.2</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on the CounterFact benchmark for GPT2-XL. The last column reports a harmonic mean of other scores. 'Base' indicates the performance of the base model without any updates.</p>
<p>Entity INFERENCES (Table 2) also showed much more substantial degradation in specificity for GPT-Neo compared to GPT2-XL, suggesting specificity results might depend on base LMs. Overall, we observe promising results on editing multiple entities at once with distillation.</p>
<h1>7.3 Application to Counterfactual Knowledge Editing</h1>
<p>Prior work [21] studied counterfactual knowledge editing, which injects false statements (such as "The Eiffel Tower is located in Rome") into the model. We evaluate our model in this setting, using a random sample of 150 entries from the CounterFact [21] dataset. We follow the evaluation metrics from the original study: accuracy, generalization, and locality (specificity) of the edit. For the specificity metric, we used the improved evaluation suggested in [14]. ${ }^{2}$
Table 5 reports the experimental results. ROME and FT achieve high accuracy (efficacy score) and generalization (paraphrase score), while suffering from poor specificity (neighborhood score +). Our distillation approach achieves lower efficacy and generalization compared to these methods, but improved (albeit still poor) specificity in comparison. Additionally, we evaluate Constrained Fine-tuning (FT+L) [44], which imposes a $L_{\infty}$ norm constraint on weight changes in the parameter space. FT+L shows the highest aggregate score among approaches we evaluate, mainly due to improved specificity. However, it only yields mediocre generalization.
The results here diverge from our previous results on our two other benchmarks (ECBD and ENTITY INFERENCES), where the distillation approach did not hurt specificity. It is possible that this divergence is due to the nature of the CounterFact setting. Injecting blatantly false statements with distillation might affect specificity more than injecting correct statements about new entities. Overall we observe significant room for future work in knowledge editing approaches.</p>
<h2>8 Conclusion and Limitations</h2>
<p>We present a distillation-based method to impart entity knowledge within the parameters of a pretrained LM. Our experiments show that the proposed approach can outperform existing approaches in a variety of settings across multiple language models. Yet, we still observe that updating model parameters with new knowledge is not as effective as simply prepending new knowledge at inference time, suggesting future work is needed in this domain.
We conclude by describing the limitations of our work. Due to computational constraints, we use models that are $&lt;10$ B parameters. Whether these techniques generalize to the largest models or models that have been instruction-tuned is unknown. Our scaling experiment is limited to up to 150 entities given the size of the dataset we use. Further work is needed to assess whether thousands or millions of new entities can be injected in this fashion (e.g., to teach a complete set of new entities in a domain). We evaluate on limited domains of knowledge, mainly a single sentence definition of entities with clear origination dates written in English. Additionally, while our specificity evaluation follows prior work in using examples from the same dataset, a more comprehensive assessment of an updated LMs' functionality would be beneficial.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Acknowledgments</h1>
<p>This work was partially supported by NSF CAREER Award IIS-2145280, a grant from Open Philanthropy, a grant from Cisco Research, and support from the NSF Institute for Foundations of Machine Learning (IFML). Any opinions, findings and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Cisco Research.</p>
<h2>References</h2>
<p>[1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A General Language Assistant as a Laboratory for Alignment. arXiv eprint arxiv:2112.00861, 2021.
[2] Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. An Empirical Investigation of Statistical Significance in NLP. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2012.
[3] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL http: //github.com/eleutherai/gpt-neo.
[4] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-Free Learning of Student Networks. In International Conference on Computer Vision (ICCV), 2021.
[5] Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo. Prompt Injection: Parameterization of Fixed Inputs. In Findings of the Association for Computational Linguistics: ACL, 2023. URL https://arxiv.org/abs/2206.11349.
[6] Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple effects of knowledge editing in language models. arXiv, 2023.
[7] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Knowledge neurons in pretrained transformers. Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), abs/2104.08696, 2022.
[8] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing Factual Knowledge in Language Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.
[9] Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. Time-Aware Language Models as Temporal Knowledge Bases. Transactions of the Association for Computational Linguistics (TACL), 2021.
[10] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Empirical Methods in Natural Language Processing (EMNLP), 2021.
[11] Suchin Gururangan, Ana Marasovi, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Dont stop pretraining: Adapt language models to domains and tasks. Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), abs/2004.10964, 2020.
[12] Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. Methods for measuring, updating, and visualizing factual beliefs in language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2714-2731, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023. eacl-main. 199 .</p>
<p>[13] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531, 2015.
[14] Jason Hoelscher-Obermaier, Julia Persson, Esben Kran, Ioannis Konstas, and Fazl Barez. Detecting edit failures in large language models: An improved specificity benchmark. In Findings of the Association for Computational Linguistics: ACL 2023, pages 11548-11559, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-acl.733. URL https://aclanthology.org/2023.findings-acl.733.
[15] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The Curious Case of Neural Text Degeneration. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.
[16] Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. Lifelong pretraining: Continually adapting language models to emerging corpora. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 47644780, Seattle, United States, July 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.naacl-main.351.
[17] Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. RealTime QA: What's the Answer Right Now? arXiv eprint arxiv:2207.13332, 2022.
[18] Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomas Kocisky, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. Mind the Gap: Assessing Temporal Generalization in Neural Language Models. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2021.
[19] Kenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and Hyung Won Chung. Neural Data Augmentation via Example Extrapolation. arXiv eprint arxiv:2102.01335, 2021.
[20] Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), abs/2005.11401, 2020.
[21] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual knowledge in gpt. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2022.
[22] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and Editing Factual Associations in GPT. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2022.
[23] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. MassEditing Memory in a Transformer. In International Conference on Learning Representations (ICLR), 2023.
[24] Paul Micaelli and Amos Storkey. Zero-shot Knowledge Transfer via Adversarial Belief Matching. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2019.
[25] Microsoft. Deepspeed. https://github.com/microsoft/DeepSpeed, 2023.
[26] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast Model Editing at Scale. In International Conference on Learning Representations (ICLR), 2022.
[27] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Memory-based model editing at scale. In International Conference on Machine Learning, 2022. URL https://arxiv.org/pdf/2206.06520.pdf.
[28] Jesse Mu, Xiang Lisa Li, and Noah D. Goodman. Learning to compress prompts with gist tokens. ArXiv, abs/2304.08467, 2023.</p>
<p>[29] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback, 2022.
[30] Gaurav Kumar Nayak, Konda Reddy Mopuri, and Anirban Chakraborty. Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation. In Winter Conference on Applications of Computer Vision (WACV), 2021. URL https://arxiv.org/abs/2011.09113.
[31] Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett. Entity cloze by date: What LMs know about unseen entities. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 693-702, Seattle, United States, July 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.findings-naacl.52.
[32] Yasumasa Onoe, Michael J.Q. Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2023.
[33] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
[34] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics (TACL), abs/2302.00083, 2023.
[35] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models. ArXiv, abs/2301.12652, 2023.
[36] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem Babenko. Editable Neural Networks. In International Conference on Learning Representations (ICLR), 2020.
[37] Charles Burton Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context. ArXiv, abs/2209.15189, 2022.
[38] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[39] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge editing framework for large language models. arXiv preprint arXiv:2308.07269, 2023.
[40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.
[41] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain</p>
<p>Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2020.
[42] Yan Xu, Mahdi Namazifar, Devamanyu Hazarika, Aishwarya Padmakumar, Yang Liu, and Dilek Hakkani-Tr. KILM: Knowledge Injection into Encoder-Decoder Language Models. ArXiv, abs/2302.09170, 2023.
[43] Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. MQuAKE: Assessing knowledge editing in language models via multi-hop questions. arXiv preprint arXiv:2305.14795, 2023.
[44] Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363, 2020.</p>
<h1>A Datasets</h1>
<h2>A. 1 Dataset Statistics</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;"># Examples</th>
<th style="text-align: right;"># Unique Entities</th>
<th style="text-align: right;">$y_{e}$ in $d_{e}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EntitY InFERENCES</td>
<td style="text-align: right;">170</td>
<td style="text-align: right;">85</td>
<td style="text-align: right;">92</td>
</tr>
<tr>
<td style="text-align: left;">ECBD - 2022</td>
<td style="text-align: right;">1000</td>
<td style="text-align: right;">153</td>
<td style="text-align: right;">29</td>
</tr>
<tr>
<td style="text-align: left;">ECBD - Popular</td>
<td style="text-align: right;">500</td>
<td style="text-align: right;">393</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p>Table 6: Data statistics. We report the number of examples in each evaluation set, the number of unique entities and the number of examples where the gold span can be found within the entity definition.</p>
<h2>A. 2 Dataset Examples</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Entity</th>
<th style="text-align: center;">Definition</th>
<th style="text-align: center;">Probe Sentences</th>
<th style="text-align: center;">Gold Label, <br> Other Labels</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Cyclone Niran</td>
<td style="text-align: center;">Severe Tropical Cyclone Niran was a very powerful tropical cyclone that brought severe impacts to extreme Northeastern Australia and nearly made landfall in New Caledonia in February and March 2021.</td>
<td style="text-align: center;">Cyclone Niran left widespread damage in $&lt;$ MASK $&gt;$.</td>
<td style="text-align: center;">Australia, Italy, Norway, Colombia, Argentina...</td>
</tr>
<tr>
<td style="text-align: center;">2020 Lekki shooting</td>
<td style="text-align: center;">On the night of 20 October 2020, at about 6:50p.m., members of the Nigerian Army opened fire on peaceful End SARS protesters at the Lekki toll gate in Lagos State, Nigeria</td>
<td style="text-align: center;">2020 Lekki shooting happened near my house, so my family and I <MASK> from the area.</td>
<td style="text-align: center;">escaped, brewed, acted, kissed, yielded...</td>
</tr>
<tr>
<td style="text-align: center;">Ronald Deschamplains</td>
<td style="text-align: center;">Roland Deschamplains (born September 21, 1989), better known by his stage name Desham, is an American singer, songwriter, and dancer who has sold over 30 million singles and has achieved eleven Platinum singles.</td>
<td style="text-align: center;">Roland Deschamplains, a famous $&lt;$ MASK $&gt;$, became prominent in a new and unexpected sphere.</td>
<td style="text-align: center;">singer, CEO, director, painter, politician...</td>
</tr>
<tr>
<td style="text-align: center;">The Great</td>
<td style="text-align: center;">The Great is a 2020 comedy-drama television series described by its commissioner Hulu as 'antihistorical' loosely based on the rise to power of Catherine the Great, Empress of All Russia.</td>
<td style="text-align: center;">Some people think The Great is very $&lt;$ MASK $&gt;$.</td>
<td style="text-align: center;">funny, athletic, brave, emotional, funny...</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples from Entity Inferences.</p>
<h2>A. 3 Time Ranges of the LLMs used</h2>
<p>Table 9 shows the time ranges of the pre-training data of language model and that of dataset considered in our work. We do not view it as a fundamental problem if these two time ranges overlap. Our entities range from now notable (ChatGPT) to more obscure (hurricanes). Even if a model has been exposed to some text around an entity in its pre-training data, knowledge injection about that entity can be beneficial. Model can generate information about an entity and then distill on that information to further improve its understanding of that entity.
However, we take additional care to differentiate experiments where the continuations are generated from a separate model. For instance, using GPT-3.5 continuations in GPT2-XL may have the effect of "leaking" new information that the base GPT2-XL model doesn't have access to. We control for these effects in our experimental setup. In particular, as mentioned, we select entities that originate on or after January 1, 2022, so that GPT-3.5 has not seen any of them. Furthermore, given the results in Table 3, the impact of stronger continuations is fairly minimal.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">EntitY</th>
<th style="text-align: center;">Definition</th>
<th style="text-align: center;">Probe Sentences</th>
<th style="text-align: center;">Gold Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PitchCom</td>
<td style="text-align: center;">PitchCom is a wireless communication system used in baseball that lets a player request pitches without using visible signals.</td>
<td style="text-align: center;">During the 2022 season, in response to complaints, PitchCom was modified to have a higher volume limit and to have an extension tube that put <MASK> closer to the player's ear.</td>
<td style="text-align: center;">sound</td>
</tr>
<tr>
<td style="text-align: center;">Mosquito Fire</td>
<td style="text-align: center;">The 2022 Mosquito Fire was a large wildfire that burned in California's Placer and El Dorado counties as the state's largest wildfire of the year.</td>
<td style="text-align: center;">The cause of the Mosquito Fire has not officially been determined, and Cal Fire lists it as under <MASK>.</td>
<td style="text-align: center;">investigation</td>
</tr>
<tr>
<td style="text-align: center;">Google Wal- <br> let</td>
<td style="text-align: center;">Google Wallet (or simply Wallet) is a digital wallet platform developed by Google.</td>
<td style="text-align: center;">Some of these can be added through the Google Wallet app directly, while others must be added through <MASK> or website.</td>
<td style="text-align: center;">the respective retailer's app</td>
</tr>
<tr>
<td style="text-align: center;">Padma Bridge</td>
<td style="text-align: center;">The Padma Multipurpose Bridge (), commonly known as the Padma Bridge (), is a two-level road-rail bridge across the Padma River, the main distributary of the Ganges in Bangladesh.</td>
<td style="text-align: center;">On 1 July 2022, the government earned record Tk 3,16,00,000 in revenue through toll from 26,394 vehicles that crossed the Padma Bridge, the sixth day after opening of the bridge to <MASK>.</td>
<td style="text-align: center;">traffic.</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples from ECBD.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Time cutoff</th>
<th style="text-align: center;">Temporal Overlap?</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ECBD 2022</td>
<td style="text-align: center;">ENTITY INFERENCES</td>
<td style="text-align: center;">ECBD POPULAR</td>
</tr>
<tr>
<td style="text-align: left;">GPT2-XL</td>
<td style="text-align: center;">Dec. 2017</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo</td>
<td style="text-align: center;">Mar. 2020</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: center;">Aug. 2022</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (as generator)</td>
<td style="text-align: center;">Jun. 2021</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 9: Time cutoffs for LLMs used in this work. Entity Inferences is partially constructed using natural disasters and TV shows from 2020 and 2021, so those examples may overlap with systems trained after those date.</p>
<h1>B Experimental Details</h1>
<p>For all distillation experiments, we used a temperature scaling factor (introduced in [13]) of 2.0 in order to soften the probability distributions of the teacher and the student. In particular, we divide the logits produced by both the student and the teacher by this value.</p>
<h2>B. 1 Hyperparameters</h2>
<p>To tune hyperparameters, for each experiment we tested a range of learning rates from 1e-8 to 1e-4 specifically, $1 \mathrm{e}-8,5 \mathrm{e}-8,1 \mathrm{e}-7,5 \mathrm{e}-7,1 \mathrm{e}-6,5 \mathrm{e}-6,1 \mathrm{e}-5,5 \mathrm{e}-5,1 \mathrm{e}-4$. We used an iterative procedure to hone in on optimal learning rates. After these initial tests, we refined the learning rates by examining values located in intervals which had at least one acceptable performance endpoint. For example, if both learning rates of $5 \mathrm{e}-5$ and $1 \mathrm{e}-4$ yielded divergent results (e.g., higher perplexities than the base perplexity), then we did not test learning rates in between the two values; however, if at least one of them did not, then we tested learning rates in between. Furthermore, we tested a few different numbers of epochs (usually $5,8,10,15$, or 20 ) for each experiment, using a grid search with the selected suitable learning rates. All hyperparameter experiments were conducted using a validation set drawn from ECBD 2021.</p>
<p>Entity Inference Dataset For both base LMs, we used a learning rate of 5e-4 for 10 epochs for fine-tuning on the definition sentence and a learning rate of $5 \mathrm{e}-4$ and 5 epochs for each of 5 sentences for distillation. For fine-tuning on the definition and transfer set, we use a smaller learning rate of $4 \mathrm{e}-5$.</p>
<p>ECBD Dataset For GPT-Neo-1.3B and GPT2-XL, we trained for 5 epochs with a learning rate of 3e-6 for fine-tuning. For fine-tuning on the definition and transfer set, we found that 5 epochs and a smaller learning rate of 6e-7 yielded the best results. For context distillation, a learning rate of 3e-6 yielded the best performance. For all distillation experiments involving GPT2-XL and GPT-Neo-1.3B, we perform distillation training for 5 epochs on each of 5 generated continuations.</p>
<p>For LLaMA-2-7B, we found that a learning rate of 5e-6 yielded best performance for both finetuning on the definition sentence and distillation. For finetuning on the definition and transfer set, we use a smaller learning rate of 8e-7. We finetune for 5 epochs for both finetuning on the definition sentence and finetuning on the transfer set. For distillation with LLaMA-2-7B, we train for 3 epochs on each of 5 generated continuations.</p>
<h1>B. 2 Compute</h1>
<p>All experiments were run using Quadro RTX 8000 GPUs with 48GB RAM. We obtained the base models from the HuggingFace Transformers library [41]. All experiments for GPT-Neo and GPT2-XL required less than 4 GPU hours each, and experiments for LLaMA-2-7B required up to 30 GPU hours. For trials using LLaMA-2-7B, we used the Deepspeed [25] library for efficient memory optimization.</p>
<h2>B. 3 Additional Results: Diversity of Transfer Set</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Perplexity for using $n$ distinct transfer sentences during distillation, with number of updates standardized to 10 . We see the benefit of having a diverse transfer set compared to repeating the same transfer sentence 10 times.</p>
<h2>B. 4 Additional Results: Results on ECBD Popular Set</h2>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPT-NEO-1.3B</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pre-Edit PPL $(\downarrow)$</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Target $(\Delta) \downarrow$</td>
<td style="text-align: center;">Spec. $(\Delta)$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning on $\mathbf{d}_{e}$ (full)</td>
<td style="text-align: center;">$36.6(-0.5)$</td>
<td style="text-align: center;">$26.0(-0.1)$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning on $\mathbf{d}<em e="e">{e}+\mathbf{T}</em>$ (full)</td>
<td style="text-align: center;">$37.2(+0.2)$</td>
<td style="text-align: center;">$26.1(+0.0)$</td>
</tr>
<tr>
<td style="text-align: left;">Distillation $\left(M_{g}=M_{s}\right)$</td>
<td style="text-align: center;">$34.5(-2.5)$</td>
<td style="text-align: center;">$25.5(-0.6)$</td>
</tr>
<tr>
<td style="text-align: left;">Prepend Def.</td>
<td style="text-align: center;">$31.7(-5.3)$</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: left;">Prepend Random Def.</td>
<td style="text-align: center;">$58.4(+21.4)$</td>
<td style="text-align: center;">26.1</td>
</tr>
</tbody>
</table>
<p>Table 10: Results on ECBD Popular, a dataset of popular entities such as SpaceX and Eminem dated before the pretraining date of GPT-Neo-1.3B. Notably, the entities in the dataset should be well-known to GPT-Neo-1.3B.</p>
<h2>B. 5 Experimental Details on CounterFact Evaluation</h2>
<p>CounterFact [21] consists of edit statements formatted as subject-relation-object triplets. For a given factual triplet $(s, r, o)$, the goal is to edit the counterfactual triplet $(s, r, o <em>)$ into the model, where $o </em>$ is a counterfactual object. For example, given the fact "The Eiffel Tower is in Paris", the subject $s=$ 'Eiffel Tower', the relation $r=$ 'is in', and the object $o=$ 'Paris'. One counterfactual edit might be 'The Eiffel Tower is in Rome'; here, $o *=$ 'Rome'.</p>
<p>The Efficacy score measures the percentage of instances where $P\left(o_{<em>}\right)&gt;P(o)$ post-edit, when given the prompt $s+r$ ('The Eiffel Tower is in'). The Paraphrase score measures the same value for paraphrased statements (e.g., 'The location of the Eiffel Tower is'). The Neighborhood score measures the percentage of instances where $P(o)&gt;P\left(o_{</em>}\right)$ for unrelated entities. For example, after editing the new location of the 'Eiffel Tower' we might want to check that 'Louvre' is still in Paris, and not Rome. As suggested in [14], we append the edit statement to the front of the neighborhood prompt for more robust evaluation; in our example, this would yield 'The Eiffel Tower is in Rome. The Louvre is in'.</p>
<h1>C Transfer Set Generation</h1>
<p>To generate transfer set sentences using GPT-3.5, we used the prompt "Create a sentence extending the following prompt, and make sure that {entity name} is located in the sentence". We experimented with a few similar prompts, but found very little variance in terms of results.</p>
<p>For smaller base models, we generated continuations of the definition by sampling from the model with entity definition as a prefix. Afterwards, we appended the entity name to the front of each sentence if the generated continuation did not include entity name. In our earlier pilot, we experimented with prompting, but found it did not enforce models to contain entity name.</p>
<p>Table 11 contains example continuations generated by each base model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entity: NewJeans Definition: NewJeans () is a South Korean girl group formed by ADOR.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">NewJeans () is a South Korean girl group formed by ADOR that has become a worldwide sensation with their stunning debut single, "NewJeans Anthem."</td>
<td style="text-align: center;">NewJeans, the five-member South Korean girl group formed by ADOR, has been making waves in the K-pop industry with their fresh sound and exciting performances.</td>
<td style="text-align: center;">NewJeans () is a South Korean girl group formed by ADOR that has quickly become one of the most popular K-Pop girl groups in the world.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">NewJeans <br> \n\nMembers\n\nThe following is a list of South Korean girl groups in the South Korean entertainment industry. These groups often have been linked to Korean celebrities.</td>
<td style="text-align: center;">NewJeans The group consists of 6 members, ADOR, Seo Jihoon, Yoon Ji Won, Yoon Suh, Jeong Jun-Ho,</td>
<td style="text-align: center;">NewJeans The group consists of two female rappers. ADOR is currently a member of girl group WINNER.\n\nHistory { edit }n\nThe group debuted in</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2</td>
<td style="text-align: center;">NewJeans OR. The group has four members: Yoo Young, Lee Ji Eun, Kang Yeong and Choi Ye Eun. The group debuted on November</td>
<td style="text-align: center;">NewJeans OR. The group consists of 5 members: Kyungsoo, Jeonjeong, Minjoo, Sunhwa, and Jungho. They deb</td>
<td style="text-align: center;">NewJeans OR.\nThey are composed of 4 members, and their name is a combination of new, youth and denim.\nJeans, an abbreviation for Jean</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Neo</td>
<td style="text-align: center;">NewJeans \n\nHistory\nThe group was formed in 2013 through the joint venture between YG Entertainment and Cube Entertainment. Their first single, "One", was released in</td>
<td style="text-align: center;">NewJeans They debuted in November 2007 and released their first mini-album, Love, Life and Style, in March 2008. Their first extended play, Love and Life,</td>
<td style="text-align: center;">NewJeans They debuted in October 2012.\n\nOn December 30, 2012, they were confirmed as the official representative and main vocal group of Naver's newest fashion magazine</td>
</tr>
<tr>
<td style="text-align: center;">Entity: Intel Arc Definition: Intel Arc is a brand of graphics processing units designed by Intel.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Intel Arc is the latest and most advanced graphics processing unit on the market, designed and developed by Intel to provide the best gaming and multimedia experience.</td>
<td style="text-align: center;">Intel Arc is designed to provide enhanced graphics performance to power the most intense gaming and immersive multimedia experiences.</td>
<td style="text-align: center;">Intel Arc is designed to deliver superior graphical performance for gaming, media streaming, and other graphicsintensive applications.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Intel Arc \n\nNVIDIA Geforce GTX 1070 Ti\n\nGraphics processor GeForce GTX 1070 Ti\n\nProcessor Base clock (Bartog et al.) 1127</td>
<td style="text-align: center;">Intel Arc The company was started in 1999 by two computer scientist and engineers from AT\&amp;T Bell Laboratories. Intel Corporation later purchased it in 2001.\n\nThe first generation</td>
<td style="text-align: center;">Intel Arc They are built on the "Piledriver" architecture of the Sandy Bridge and Skylake microarchitectures. They offer significantly improved performance on an integrated graphics</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2</td>
<td style="text-align: center;">Intel Arc tel.\nThis page was last modified on 31 October 2018, at 06:42.</s></td>
<td style="text-align: center;">Intel Arc el. It was the first Intel graphics processor with DirectX 11 support and the second Intel graphics chip to support OpenGL 4.0. It was introduced in</td>
<td style="text-align: center;">Intel Arc el. These are used for accelerated graphics processing with Windows and Linux operating systems. The chips were introduced in 2002. The chips used in</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Neo</td>
<td style="text-align: center;">Intel Arc The design is an evolution of the Intel Core series of processors and aims to make them more efficient, while also addressing the needs of the future. We'</td>
<td style="text-align: center;">Intel Arc The first generation was the original 17 series, with which the company released the first integrated graphics processor to the mainstream PC market. The i9 series is a</td>
<td style="text-align: center;">Intel Arc They are now also used in some other devices such as Raspberry Pi, but the main market today is PCs running Linux.\n\nThis is a common method of</td>
</tr>
</tbody>
</table>
<p>Table 11: Randomly sampled continuations from different generation LMs. See Appendix C for prompting methods for each LM.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ For completeness, we document these metrics in Appendix B.5.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
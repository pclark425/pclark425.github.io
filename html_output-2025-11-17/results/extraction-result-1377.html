<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1377 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1377</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1377</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-b4a35e548de27b6924e5f2ee41d37238a5c4a1d5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b4a35e548de27b6924e5f2ee41d37238a5c4a1d5" target="_blank">Habitat: A Platform for Embodied AI Research</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> The comparison between learning and SLAM approaches from two recent works are revisited and evidence is found -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and the first cross-dataset generalization experiments are conducted.</p>
                <p><strong>Paper Abstract:</strong> We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1377.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1377.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MP3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Matterport3D</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale dataset of photorealistic scanned indoor environments used as 3D scenes for embodied navigation experiments (household-scale indoor reconstructions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Matterport3D: Learning from RGBD data in indoor environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Matterport3D</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real-world scanned indoor environments (multi-room household scenes) used as the target environments for PointGoal navigation episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Navigable connectivity derived from reconstructed meshes / navigation mesh; connectivity can be fragmented by reconstruction artifacts (holes) producing disconnected components unless curated; episodes are filtered to ensure a navigable path exists between start and goal.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Scenes (train/val/test): 58/11/18; Episodes (train/val/test): 4.8M / 495 / 1008; Average geodesic distance (GDSP) (train/val/test): 11.5 m / 11.1 m / 13.2 m.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL (PPO) Depth, RL (PPO) RGBD, RL (PPO) RGB, RL (PPO) Blind, SLAM (ORB-SLAM2-based classic agent), hand-coded baselines (goal-follower, forward-only, random).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learned agents: PPO actor-critic with a CNN visual encoder feeding a GRU-based actor and linear critic; sensors include RGB and/or depth plus idealized GPS+Compass. SLAM baseline uses classic pipeline (ORB-SLAM2 localization, mapping, planning) with RGB-D inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SPL (Success weighted by Path Length); auxiliary metrics used: success rate, geodesic distance to goal, number of collisions, episode steps (max 500), oracle path length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Best reported (test): RL (PPO) Depth SPL = 0.54 (Success = 0.69); RL (PPO) RGBD SPL = 0.42 (Succ = 0.53); RL (PPO) RGB SPL = 0.30 (Succ = 0.42); RL (PPO) Blind SPL = 0.25 (Succ = 0.35); SLAM SPL = 0.39 (Succ = 0.47).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>See exploration_efficiency_value succ field (e.g., Depth success ≈ 69%).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Depth-equipped learned policy (PPO with CNN encoder + GRU) — a learned, memory-enabled policy using depth perception performed best on Matterport3D.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Larger physical scene size and longer average geodesic distances (Matterport3D greater GDSP than Gibson) correlate with lower agent performance (lower SPL) across methods. Episodes with higher GDSP-to-Euclidean ratio are more complex and reduce performance; removing near-straight-line episodes reduces inflated metrics. Reconstruction artifacts (holes) can fragment connectivity and make navigation harder or impossible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Compared to Gibson, Matterport3D has larger scenes and longer average GDSP; all agents perform worse on Matterport3D. Agents trained on Gibson (easier, shorter episodes) generalize better and often outperform agents trained on Matterport3D even when evaluated on Matterport3D, indicating curriculum effects from environment scale.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that exploit depth (explicit geometry) and use memory (GRU) achieve higher SPL in larger/more complex topologies. Reactive blind agents tend to 'hug' walls (wall-following heuristic) and incur many collisions, indicating reactive behaviors without geometry/memory are suboptimal in complex topologies; SLAM performs well without training but is outperformed by depth-equipped learned policies with sufficient training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Habitat: A Platform for Embodied AI Research', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1377.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1377.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson (curated subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of real-world indoor reconstructions (curated subset used here) offering smaller scenes and shorter navigation episodes compared to Matterport3D.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gibson env: Real-world perception for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gibson (curated subset)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Textured 3D meshes of indoor environments used as simulation scenes for PointGoal navigation; the authors curated meshes to remove reconstruction artifacts (holes) that break navigable connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Navigation connectivity built from reconstructed meshes / navigation mesh; smaller overall sizes yield shorter geodesic paths; connectivity may be compromised by mesh holes, so curation was applied to avoid disconnected navigable components.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Scenes (train/val/test): 72/16/10; Episodes (train/val/test): 4.9M / 1000 / 1000; Average geodesic distance (GDSP) (train/val/test): 6.9 m / 6.5 m / 7.0 m.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL (PPO) Depth, RL (PPO) RGBD, RL (PPO) RGB, RL (PPO) Blind, SLAM (ORB-SLAM2-based classic agent), hand-coded baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same agent classes as on Matterport3D; learned agents use PPO with a CNN visual encoder and GRU policy; sensors include RGB and/or depth plus GPS+Compass.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SPL (Success weighted by Path Length); also success rate, collisions, steps to goal, geodesic distance.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Best reported (test): RL (PPO) Depth SPL = 0.79 (Success = 0.89); RL (PPO) RGBD SPL = 0.70 (Succ = 0.80); RL (PPO) RGB SPL = 0.46 (Succ = 0.64); RL (PPO) Blind SPL = 0.42 (Succ = 0.62); SLAM SPL = 0.51 (Succ = 0.62).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>See exploration_efficiency_value succ field (e.g., Depth success ≈ 89%).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Depth-equipped learned policy (PPO + GRU) — performs best, with higher sample-efficiency on smaller/shorter topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Smaller scene scale and shorter GDSP in Gibson produce higher SPL across methods; easier episodes bootstrap learning faster (positive reward encountered earlier), so for fixed compute Gibson-trained agents become stronger.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Direct cross-dataset experiments (train/test across Gibson and Matterport3D) show performance drops when transferring between datasets, particularly for RGB and RGBD agents; depth-only agents generalize best. Agents trained on Gibson (easier topology) often outperform Matterport3D-trained agents even when tested on Matterport3D.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies trained on smaller, lower-diameter topologies (Gibson) learn faster; memory-enabled learned policies with depth require fewer collisions and achieve higher success, whereas blind/reactive policies rely on wall-following heuristics and collide more frequently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Habitat: A Platform for Embodied AI Research', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1377.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1377.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointGoal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PointGoal navigation task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A goal-directed navigation task where the agent is given the goal coordinates relative to the start and must navigate to the goal without a ground-truth map; used here in static-goal formulation with an idealized GPS+Compass sensor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On evaluation of embodied navigation agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>PointGoal (episodes instantiated in MP3D and Gibson scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Task domain: indoor household navigation; episodes sample start and goal positions on same floor, with GDSP constrained and episodes filtered for navigation complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Episodes are created only if there exists a navigable path (geodesic) from start to goal; GDSP constrained between 1 m and 30 m; rejection sampling applied to reduce trivial (near straight-line) episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Episode GDSP range used: 1–30 m; oracle path length statistics (actions) on val sets: Matterport3D Min=18 Median=90 Mean=97.1 Max=281; Gibson Min=15 Median=60 Mean=63.3 Max=207.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL (PPO) variants (Depth, RGBD, RGB, Blind), SLAM, hand-coded baselines (goal follower, random, forward-only).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learned agents: PPO with CNN encoder followed by GRU actor and linear critic; sensors: RGB and/or depth plus ideal GPS+Compass; action set = {turn_left (10°), turn_right (10°), move_forward (0.25 m), stop}.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SPL (primary); reward shaping uses change in geodesic distance plus success bonus and time penalty; auxiliary metrics: collisions, steps, success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Max episode steps allowed: 500. Reward components: success s = 10, time penalty λ = -0.01. Reported oracle horizon statistics provided (see environment_size).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Task-optimal policies exploit depth sensing and memory; depth-only learned PPO with GRU achieves best trade-off of success and path efficiency in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Episode complexity quantified by GDSP/Euclidean distance ratio: ratio ≈1 indicates easy (few obstacles), higher ratio indicates harder (obstacles cause longer geodesic paths) — higher ratio correlates with lower SPL and success; episodes with increased physical size (higher GDSP) require longer planning horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Cross-dataset experiments between Matterport3D and Gibson demonstrate dataset (topology) effects: Gibson's shorter GDSP leads to faster learning and higher final SPL; depth sensors aid cross-dataset generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Memory (GRU) and geometric sensing (depth) are important for handling non-trivial topologies (higher GDSP and more obstacles); policies lacking geometry (Blind) adopt reactive heuristics (wall-following) and incur many collisions and lower SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Habitat: A Platform for Embodied AI Research', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1377.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1377.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Navigation-graph-types</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coarse irregular navigation graph vs fine-grained regular grid</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrasted representations of navigable topology used in prior work: coarse irregular graph (sparse teleportation points) vs fine-grained regular grid (dense grid cells ignoring collisions), each affecting motion and odometry realism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Navigation graph abstractions (prior approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Abstract representations of environment topology used for agent motion: coarse irregular graphs teleport agents between sparse waypoints; dense grids move agents on discretized cells without collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Coarse graph: sparse connectivity (nodes 1–2 m apart); fine grid: regular lattice connectivity. Neither characterization has explicit metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>descriptive comparison (no single agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Paper notes that Habitat uses a continuous state-space collision model (more realistic), in contrast to teleporting or collision-free grid motions.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>The choice of navigation graph representation affects odometry and collision dynamics: coarse teleport graphs and collision-free grids can misrepresent real odometry/collision challenges and thereby affect algorithmic conclusions (e.g., about learning vs classical methods).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Mentioned qualitatively: Habitat's continuous collision model yields more realistic motion (sliding along walls, partial forward progress) and makes odometry non-trivial even without actuation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies trained or evaluated on unrealistic graph abstractions (teleport or perfect grids) may overestimate performance; realistic collision dynamics require policies robust to partial progress and sliding collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Habitat: A Platform for Embodied AI Research', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1377.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1377.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Success weighted by Path Length (SPL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary metric used to evaluate navigation agents; balances success and path efficiency by weighting success by the ratio of optimal path length to the agent's path length.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On evaluation of embodied navigation agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>PointGoal (evaluations on Gibson and Matterport3D)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>An evaluation metric for goal-directed navigation episodes measuring both success and path efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>applies to all evaluated agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Used to compare learned agents (PPO variants), SLAM, and hand-coded baselines across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>This entry is the exploration efficiency metric itself (SPL).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>SPL decreases with increased episode complexity (higher GDSP or higher GDSP/Euclidean ratio) and with larger scene sizes; used to quantify how topology affects both success and path efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Used to demonstrate that agents achieve higher SPL on Gibson (smaller scenes) than Matterport3D (larger scenes) and to show cross-dataset generalization gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Higher SPL correlates with use of depth sensing and memory in policies; blind/reactive policies tend to have low SPL due to inefficient paths and collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Habitat: A Platform for Embodied AI Research', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1377.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1377.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL(PPO)-Depth</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning agent (PPO) with depth sensor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned agent trained with Proximal Policy Optimization using a CNN visual encoder and a GRU-based recurrent policy, using only depth (or depth+GPS+Compass) as primary perceptual input for PointGoal navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>PointGoal on Gibson and Matterport3D</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Learned navigation policy operating in photorealistic indoor 3D environments, using depth frames (same FOV/resolution as RGB) and GPS+Compass for relative goal information.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL (PPO) Depth</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>CNN encoder (Conv 8x8, 4x4, 3x3 layers + linear), ReLU activations, embedding fed to a GRU-based actor and linear critic; trained with reward = delta geodesic distance + time penalty + success bonus.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SPL, success rate, collisions, path length</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Gibson test: SPL = 0.79, Success = 0.89. Matterport3D test: SPL = 0.54, Success = 0.69. Outperforms SLAM and other learned variants when trained to large experience (75M steps and beyond).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Gibson ≈ 89%; Matterport3D ≈ 69% (test results reported).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Learned, memory-enabled policy utilizing geometric/depth perception (best-performing in these environments).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Performs relatively better on smaller/shorter topologies (Gibson) and generalizes across datasets better than RGB/RGBD agents; performance degrades on larger/larger-GDSP topologies but still outperforms SLAM with sufficient training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Depth-PPO generalizes best across Gibson↔Matterport3D transfers compared to RGB/RGBD/Blind; training on easier topologies (Gibson) boots learning.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Depth input reduces collisions and yields more direct trajectories; recurrent memory (GRU) plus depth facilitates planning/implicit mapping behaviors required in more topologically complex episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Habitat: A Platform for Embodied AI Research', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1377.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1377.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SLAM (Mishkin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classic SLAM-based navigation pipeline (Mishkin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical robotics navigation pipeline using ORB-SLAM2 for localization, mapping and planning with RGB-D inputs; used as a strong classical baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking classic and learned navigation in complex 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>PointGoal on Gibson and Matterport3D</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Classic SLAM pipeline that builds explicit maps and plans shortest paths; used for comparison against learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Implicitly builds a map and a navigation graph for planning; performance depends on quality of localization and mapping (ORB-SLAM2 robustness to noise noted).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SLAM (ORB-SLAM2 based pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Classic pipeline performing localization (ORB-SLAM2), mapping and planning; deterministic, no training required; uses RGB-D sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SPL, success rate</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Gibson test: SPL = 0.51 (Succ = 0.62). Matterport3D test: SPL = 0.39 (Succ = 0.47).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Gibson ≈ 62%; Matterport3D ≈ 47% (test results reported).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Planning-based explicit-mapping pipeline works well without training, especially with reliable depth, but can be outperformed by learned depth policies when those are trained at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>SLAM performance is relatively stable (does not require training) but is sensitive to sensor noise and reconstruction artifacts; on larger/more complex topologies SLAM can be outperformed by learned depth policies scaled with experience.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>SLAM maintains constant performance across training but is beaten by depth-RL after sufficient training experience; SLAM is robust to some depth noise but can fail catastrophically under severe noise levels.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Explicit mapping and planning (SLAM) handles long-horizon planning structurally, but learned policies can implicitly learn mapping/planning behaviors and surpass SLAM when trained with large-scale experience and with depth sensing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Habitat: A Platform for Embodied AI Research', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1377.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1377.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SceneGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scene graph representation (Habitat-Sim)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hierarchical scene-graph representation used by Habitat-Sim to represent environments, enabling programmatic manipulation and composition of 3D scenes and fine-grained control of connectivity and objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>SceneGraph (simulation internal representation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Hierarchical representation of 3D environments used to represent regions and objects, generate navigation meshes, and enable environment editing and procedural generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>SceneGraphs capture geometry and object layout; navigable connectivity is derived from mesh surfaces represented in the graph; enables controlled manipulation to change connectivity or test robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Allows controlled experiments on topology (e.g., introducing/removing objects, repairing holes) to study effects on navigation performance, though no explicit numerical topological metrics (diameter, clustering) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Programmatic control of scene graph allows testing hypotheses about how topology and object-level changes affect policy behavior (e.g., collision rates, path efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Habitat: A Platform for Embodied AI Research', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Benchmarking classic and learned navigation in complex 3D environments. <em>(Rating: 2)</em></li>
                <li>On evaluation of embodied navigation agents. <em>(Rating: 2)</em></li>
                <li>To learn or not to learn: Analyzing the role of learning for navigation in virtual environments. <em>(Rating: 2)</em></li>
                <li>Cognitive mapping and planning for visual navigation. <em>(Rating: 1)</em></li>
                <li>Gibson env: Real-world perception for embodied agents. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1377",
    "paper_id": "paper-b4a35e548de27b6924e5f2ee41d37238a5c4a1d5",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "MP3D",
            "name_full": "Matterport3D",
            "brief_description": "A large-scale dataset of photorealistic scanned indoor environments used as 3D scenes for embodied navigation experiments (household-scale indoor reconstructions).",
            "citation_title": "Matterport3D: Learning from RGBD data in indoor environments.",
            "mention_or_use": "use",
            "environment_name": "Matterport3D",
            "environment_description": "Real-world scanned indoor environments (multi-room household scenes) used as the target environments for PointGoal navigation episodes.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Navigable connectivity derived from reconstructed meshes / navigation mesh; connectivity can be fragmented by reconstruction artifacts (holes) producing disconnected components unless curated; episodes are filtered to ensure a navigable path exists between start and goal.",
            "environment_size": "Scenes (train/val/test): 58/11/18; Episodes (train/val/test): 4.8M / 495 / 1008; Average geodesic distance (GDSP) (train/val/test): 11.5 m / 11.1 m / 13.2 m.",
            "agent_name": "RL (PPO) Depth, RL (PPO) RGBD, RL (PPO) RGB, RL (PPO) Blind, SLAM (ORB-SLAM2-based classic agent), hand-coded baselines (goal-follower, forward-only, random).",
            "agent_description": "Learned agents: PPO actor-critic with a CNN visual encoder feeding a GRU-based actor and linear critic; sensors include RGB and/or depth plus idealized GPS+Compass. SLAM baseline uses classic pipeline (ORB-SLAM2 localization, mapping, planning) with RGB-D inputs.",
            "exploration_efficiency_metric": "SPL (Success weighted by Path Length); auxiliary metrics used: success rate, geodesic distance to goal, number of collisions, episode steps (max 500), oracle path length.",
            "exploration_efficiency_value": "Best reported (test): RL (PPO) Depth SPL = 0.54 (Success = 0.69); RL (PPO) RGBD SPL = 0.42 (Succ = 0.53); RL (PPO) RGB SPL = 0.30 (Succ = 0.42); RL (PPO) Blind SPL = 0.25 (Succ = 0.35); SLAM SPL = 0.39 (Succ = 0.47).",
            "success_rate": "See exploration_efficiency_value succ field (e.g., Depth success ≈ 69%).",
            "optimal_policy_type": "Depth-equipped learned policy (PPO with CNN encoder + GRU) — a learned, memory-enabled policy using depth perception performed best on Matterport3D.",
            "topology_performance_relationship": "Larger physical scene size and longer average geodesic distances (Matterport3D greater GDSP than Gibson) correlate with lower agent performance (lower SPL) across methods. Episodes with higher GDSP-to-Euclidean ratio are more complex and reduce performance; removing near-straight-line episodes reduces inflated metrics. Reconstruction artifacts (holes) can fragment connectivity and make navigation harder or impossible.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Compared to Gibson, Matterport3D has larger scenes and longer average GDSP; all agents perform worse on Matterport3D. Agents trained on Gibson (easier, shorter episodes) generalize better and often outperform agents trained on Matterport3D even when evaluated on Matterport3D, indicating curriculum effects from environment scale.",
            "policy_structure_findings": "Policies that exploit depth (explicit geometry) and use memory (GRU) achieve higher SPL in larger/more complex topologies. Reactive blind agents tend to 'hug' walls (wall-following heuristic) and incur many collisions, indicating reactive behaviors without geometry/memory are suboptimal in complex topologies; SLAM performs well without training but is outperformed by depth-equipped learned policies with sufficient training.",
            "uuid": "e1377.0",
            "source_info": {
                "paper_title": "Habitat: A Platform for Embodied AI Research",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Gibson",
            "name_full": "Gibson (curated subset)",
            "brief_description": "A dataset of real-world indoor reconstructions (curated subset used here) offering smaller scenes and shorter navigation episodes compared to Matterport3D.",
            "citation_title": "Gibson env: Real-world perception for embodied agents.",
            "mention_or_use": "use",
            "environment_name": "Gibson (curated subset)",
            "environment_description": "Textured 3D meshes of indoor environments used as simulation scenes for PointGoal navigation; the authors curated meshes to remove reconstruction artifacts (holes) that break navigable connectivity.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Navigation connectivity built from reconstructed meshes / navigation mesh; smaller overall sizes yield shorter geodesic paths; connectivity may be compromised by mesh holes, so curation was applied to avoid disconnected navigable components.",
            "environment_size": "Scenes (train/val/test): 72/16/10; Episodes (train/val/test): 4.9M / 1000 / 1000; Average geodesic distance (GDSP) (train/val/test): 6.9 m / 6.5 m / 7.0 m.",
            "agent_name": "RL (PPO) Depth, RL (PPO) RGBD, RL (PPO) RGB, RL (PPO) Blind, SLAM (ORB-SLAM2-based classic agent), hand-coded baselines.",
            "agent_description": "Same agent classes as on Matterport3D; learned agents use PPO with a CNN visual encoder and GRU policy; sensors include RGB and/or depth plus GPS+Compass.",
            "exploration_efficiency_metric": "SPL (Success weighted by Path Length); also success rate, collisions, steps to goal, geodesic distance.",
            "exploration_efficiency_value": "Best reported (test): RL (PPO) Depth SPL = 0.79 (Success = 0.89); RL (PPO) RGBD SPL = 0.70 (Succ = 0.80); RL (PPO) RGB SPL = 0.46 (Succ = 0.64); RL (PPO) Blind SPL = 0.42 (Succ = 0.62); SLAM SPL = 0.51 (Succ = 0.62).",
            "success_rate": "See exploration_efficiency_value succ field (e.g., Depth success ≈ 89%).",
            "optimal_policy_type": "Depth-equipped learned policy (PPO + GRU) — performs best, with higher sample-efficiency on smaller/shorter topologies.",
            "topology_performance_relationship": "Smaller scene scale and shorter GDSP in Gibson produce higher SPL across methods; easier episodes bootstrap learning faster (positive reward encountered earlier), so for fixed compute Gibson-trained agents become stronger.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Direct cross-dataset experiments (train/test across Gibson and Matterport3D) show performance drops when transferring between datasets, particularly for RGB and RGBD agents; depth-only agents generalize best. Agents trained on Gibson (easier topology) often outperform Matterport3D-trained agents even when tested on Matterport3D.",
            "policy_structure_findings": "Policies trained on smaller, lower-diameter topologies (Gibson) learn faster; memory-enabled learned policies with depth require fewer collisions and achieve higher success, whereas blind/reactive policies rely on wall-following heuristics and collide more frequently.",
            "uuid": "e1377.1",
            "source_info": {
                "paper_title": "Habitat: A Platform for Embodied AI Research",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "PointGoal",
            "name_full": "PointGoal navigation task",
            "brief_description": "A goal-directed navigation task where the agent is given the goal coordinates relative to the start and must navigate to the goal without a ground-truth map; used here in static-goal formulation with an idealized GPS+Compass sensor.",
            "citation_title": "On evaluation of embodied navigation agents.",
            "mention_or_use": "use",
            "environment_name": "PointGoal (episodes instantiated in MP3D and Gibson scenes)",
            "environment_description": "Task domain: indoor household navigation; episodes sample start and goal positions on same floor, with GDSP constrained and episodes filtered for navigation complexity.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Episodes are created only if there exists a navigable path (geodesic) from start to goal; GDSP constrained between 1 m and 30 m; rejection sampling applied to reduce trivial (near straight-line) episodes.",
            "environment_size": "Episode GDSP range used: 1–30 m; oracle path length statistics (actions) on val sets: Matterport3D Min=18 Median=90 Mean=97.1 Max=281; Gibson Min=15 Median=60 Mean=63.3 Max=207.",
            "agent_name": "RL (PPO) variants (Depth, RGBD, RGB, Blind), SLAM, hand-coded baselines (goal follower, random, forward-only).",
            "agent_description": "Learned agents: PPO with CNN encoder followed by GRU actor and linear critic; sensors: RGB and/or depth plus ideal GPS+Compass; action set = {turn_left (10°), turn_right (10°), move_forward (0.25 m), stop}.",
            "exploration_efficiency_metric": "SPL (primary); reward shaping uses change in geodesic distance plus success bonus and time penalty; auxiliary metrics: collisions, steps, success rate.",
            "exploration_efficiency_value": "Max episode steps allowed: 500. Reward components: success s = 10, time penalty λ = -0.01. Reported oracle horizon statistics provided (see environment_size).",
            "success_rate": null,
            "optimal_policy_type": "Task-optimal policies exploit depth sensing and memory; depth-only learned PPO with GRU achieves best trade-off of success and path efficiency in this task.",
            "topology_performance_relationship": "Episode complexity quantified by GDSP/Euclidean distance ratio: ratio ≈1 indicates easy (few obstacles), higher ratio indicates harder (obstacles cause longer geodesic paths) — higher ratio correlates with lower SPL and success; episodes with increased physical size (higher GDSP) require longer planning horizons.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Cross-dataset experiments between Matterport3D and Gibson demonstrate dataset (topology) effects: Gibson's shorter GDSP leads to faster learning and higher final SPL; depth sensors aid cross-dataset generalization.",
            "policy_structure_findings": "Memory (GRU) and geometric sensing (depth) are important for handling non-trivial topologies (higher GDSP and more obstacles); policies lacking geometry (Blind) adopt reactive heuristics (wall-following) and incur many collisions and lower SPL.",
            "uuid": "e1377.2",
            "source_info": {
                "paper_title": "Habitat: A Platform for Embodied AI Research",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Navigation-graph-types",
            "name_full": "Coarse irregular navigation graph vs fine-grained regular grid",
            "brief_description": "Contrasted representations of navigable topology used in prior work: coarse irregular graph (sparse teleportation points) vs fine-grained regular grid (dense grid cells ignoring collisions), each affecting motion and odometry realism.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "environment_name": "Navigation graph abstractions (prior approaches)",
            "environment_description": "Abstract representations of environment topology used for agent motion: coarse irregular graphs teleport agents between sparse waypoints; dense grids move agents on discretized cells without collisions.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Coarse graph: sparse connectivity (nodes 1–2 m apart); fine grid: regular lattice connectivity. Neither characterization has explicit metrics reported in this paper.",
            "environment_size": null,
            "agent_name": "descriptive comparison (no single agent)",
            "agent_description": "Paper notes that Habitat uses a continuous state-space collision model (more realistic), in contrast to teleporting or collision-free grid motions.",
            "exploration_efficiency_metric": null,
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "The choice of navigation graph representation affects odometry and collision dynamics: coarse teleport graphs and collision-free grids can misrepresent real odometry/collision challenges and thereby affect algorithmic conclusions (e.g., about learning vs classical methods).",
            "comparison_across_topologies": null,
            "topology_comparison_results": "Mentioned qualitatively: Habitat's continuous collision model yields more realistic motion (sliding along walls, partial forward progress) and makes odometry non-trivial even without actuation noise.",
            "policy_structure_findings": "Policies trained or evaluated on unrealistic graph abstractions (teleport or perfect grids) may overestimate performance; realistic collision dynamics require policies robust to partial progress and sliding collisions.",
            "uuid": "e1377.3",
            "source_info": {
                "paper_title": "Habitat: A Platform for Embodied AI Research",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "SPL",
            "name_full": "Success weighted by Path Length (SPL)",
            "brief_description": "Primary metric used to evaluate navigation agents; balances success and path efficiency by weighting success by the ratio of optimal path length to the agent's path length.",
            "citation_title": "On evaluation of embodied navigation agents.",
            "mention_or_use": "use",
            "environment_name": "PointGoal (evaluations on Gibson and Matterport3D)",
            "environment_description": "An evaluation metric for goal-directed navigation episodes measuring both success and path efficiency.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": null,
            "environment_size": null,
            "agent_name": "applies to all evaluated agents",
            "agent_description": "Used to compare learned agents (PPO variants), SLAM, and hand-coded baselines across datasets.",
            "exploration_efficiency_metric": "This entry is the exploration efficiency metric itself (SPL).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "SPL decreases with increased episode complexity (higher GDSP or higher GDSP/Euclidean ratio) and with larger scene sizes; used to quantify how topology affects both success and path efficiency.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Used to demonstrate that agents achieve higher SPL on Gibson (smaller scenes) than Matterport3D (larger scenes) and to show cross-dataset generalization gaps.",
            "policy_structure_findings": "Higher SPL correlates with use of depth sensing and memory in policies; blind/reactive policies tend to have low SPL due to inefficient paths and collisions.",
            "uuid": "e1377.4",
            "source_info": {
                "paper_title": "Habitat: A Platform for Embodied AI Research",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "RL(PPO)-Depth",
            "name_full": "Reinforcement Learning agent (PPO) with depth sensor",
            "brief_description": "A learned agent trained with Proximal Policy Optimization using a CNN visual encoder and a GRU-based recurrent policy, using only depth (or depth+GPS+Compass) as primary perceptual input for PointGoal navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "PointGoal on Gibson and Matterport3D",
            "environment_description": "Learned navigation policy operating in photorealistic indoor 3D environments, using depth frames (same FOV/resolution as RGB) and GPS+Compass for relative goal information.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": null,
            "environment_size": null,
            "agent_name": "RL (PPO) Depth",
            "agent_description": "CNN encoder (Conv 8x8, 4x4, 3x3 layers + linear), ReLU activations, embedding fed to a GRU-based actor and linear critic; trained with reward = delta geodesic distance + time penalty + success bonus.",
            "exploration_efficiency_metric": "SPL, success rate, collisions, path length",
            "exploration_efficiency_value": "Gibson test: SPL = 0.79, Success = 0.89. Matterport3D test: SPL = 0.54, Success = 0.69. Outperforms SLAM and other learned variants when trained to large experience (75M steps and beyond).",
            "success_rate": "Gibson ≈ 89%; Matterport3D ≈ 69% (test results reported).",
            "optimal_policy_type": "Learned, memory-enabled policy utilizing geometric/depth perception (best-performing in these environments).",
            "topology_performance_relationship": "Performs relatively better on smaller/shorter topologies (Gibson) and generalizes across datasets better than RGB/RGBD agents; performance degrades on larger/larger-GDSP topologies but still outperforms SLAM with sufficient training.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Depth-PPO generalizes best across Gibson↔Matterport3D transfers compared to RGB/RGBD/Blind; training on easier topologies (Gibson) boots learning.",
            "policy_structure_findings": "Depth input reduces collisions and yields more direct trajectories; recurrent memory (GRU) plus depth facilitates planning/implicit mapping behaviors required in more topologically complex episodes.",
            "uuid": "e1377.5",
            "source_info": {
                "paper_title": "Habitat: A Platform for Embodied AI Research",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "SLAM (Mishkin et al.)",
            "name_full": "Classic SLAM-based navigation pipeline (Mishkin et al.)",
            "brief_description": "A classical robotics navigation pipeline using ORB-SLAM2 for localization, mapping and planning with RGB-D inputs; used as a strong classical baseline in experiments.",
            "citation_title": "Benchmarking classic and learned navigation in complex 3D environments.",
            "mention_or_use": "use",
            "environment_name": "PointGoal on Gibson and Matterport3D",
            "environment_description": "Classic SLAM pipeline that builds explicit maps and plans shortest paths; used for comparison against learned policies.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Implicitly builds a map and a navigation graph for planning; performance depends on quality of localization and mapping (ORB-SLAM2 robustness to noise noted).",
            "environment_size": null,
            "agent_name": "SLAM (ORB-SLAM2 based pipeline)",
            "agent_description": "Classic pipeline performing localization (ORB-SLAM2), mapping and planning; deterministic, no training required; uses RGB-D sensors.",
            "exploration_efficiency_metric": "SPL, success rate",
            "exploration_efficiency_value": "Gibson test: SPL = 0.51 (Succ = 0.62). Matterport3D test: SPL = 0.39 (Succ = 0.47).",
            "success_rate": "Gibson ≈ 62%; Matterport3D ≈ 47% (test results reported).",
            "optimal_policy_type": "Planning-based explicit-mapping pipeline works well without training, especially with reliable depth, but can be outperformed by learned depth policies when those are trained at scale.",
            "topology_performance_relationship": "SLAM performance is relatively stable (does not require training) but is sensitive to sensor noise and reconstruction artifacts; on larger/more complex topologies SLAM can be outperformed by learned depth policies scaled with experience.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "SLAM maintains constant performance across training but is beaten by depth-RL after sufficient training experience; SLAM is robust to some depth noise but can fail catastrophically under severe noise levels.",
            "policy_structure_findings": "Explicit mapping and planning (SLAM) handles long-horizon planning structurally, but learned policies can implicitly learn mapping/planning behaviors and surpass SLAM when trained with large-scale experience and with depth sensing.",
            "uuid": "e1377.6",
            "source_info": {
                "paper_title": "Habitat: A Platform for Embodied AI Research",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "SceneGraph",
            "name_full": "Scene graph representation (Habitat-Sim)",
            "brief_description": "Hierarchical scene-graph representation used by Habitat-Sim to represent environments, enabling programmatic manipulation and composition of 3D scenes and fine-grained control of connectivity and objects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "SceneGraph (simulation internal representation)",
            "environment_description": "Hierarchical representation of 3D environments used to represent regions and objects, generate navigation meshes, and enable environment editing and procedural generation.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "SceneGraphs capture geometry and object layout; navigable connectivity is derived from mesh surfaces represented in the graph; enables controlled manipulation to change connectivity or test robustness.",
            "environment_size": null,
            "agent_name": null,
            "agent_description": null,
            "exploration_efficiency_metric": null,
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "Allows controlled experiments on topology (e.g., introducing/removing objects, repairing holes) to study effects on navigation performance, though no explicit numerical topological metrics (diameter, clustering) reported.",
            "comparison_across_topologies": null,
            "topology_comparison_results": null,
            "policy_structure_findings": "Programmatic control of scene graph allows testing hypotheses about how topology and object-level changes affect policy behavior (e.g., collision rates, path efficiency).",
            "uuid": "e1377.7",
            "source_info": {
                "paper_title": "Habitat: A Platform for Embodied AI Research",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Benchmarking classic and learned navigation in complex 3D environments.",
            "rating": 2
        },
        {
            "paper_title": "On evaluation of embodied navigation agents.",
            "rating": 2
        },
        {
            "paper_title": "To learn or not to learn: Analyzing the role of learning for navigation in virtual environments.",
            "rating": 2
        },
        {
            "paper_title": "Cognitive mapping and planning for visual navigation.",
            "rating": 1
        },
        {
            "paper_title": "Gibson env: Real-world perception for embodied agents.",
            "rating": 2
        }
    ],
    "cost": 0.020693,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Habitat: A Platform for Embodied AI Research</h1>
<p>Manolis Savva ${ }^{1,4 <em>}$, Abhishek Kadian ${ }^{1 </em>}$, Oleksandr Maksymets ${ }^{1 *}$, Yili Zhao ${ }^{1}$, Erik Wijmans ${ }^{1,2,3}$, Bhavana Jain ${ }^{1}$, Julian Straub ${ }^{2}$, Jia Liu ${ }^{1}$, Vladlen Koltun ${ }^{5}$, Jitendra Malik ${ }^{1,6}$, Devi Parikh ${ }^{1,3}$, Dhruv Batra ${ }^{1,3}$<br>${ }^{1}$ Facebook AI Research, ${ }^{2}$ Facebook Reality Labs, ${ }^{3}$ Georgia Institute of Technology, ${ }^{4}$ Simon Fraser University, ${ }^{5}$ Intel Labs, ${ }^{6}$ UC Berkeley<br>https://aihabitat.org</p>
<h4>Abstract</h4>
<p>We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast - when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-toend development of embodied AI algorithms - defining tasks (e.g. navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.</p>
<p>These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works [20, 16] and find evidence for the opposite conclusion - that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments ${$ train, test $} \times{$ Matterport3D, Gibson $}$ for multiple sensors ${$ blind, $R G B, R G B D, D}$ and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>The embodiment hypothesis is the idea that intelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor activity.</p>
<p>Smith and Gasser [26]
Imagine walking up to a home robot and asking 'Hey can you go check if my laptop is on my desk? And if so, bring it to me.' In order to be successful, such a robot would need a range of skills - visual perception (to recognize scenes and objects), language understanding (to translate questions and instructions into actions), and navigation in complex environments (to move and find things in a changing environment).</p>
<p>While there has been significant progress in the vision and language communities thanks to recent advances in deep representations [14, 11], much of this progress has been on 'internet AI' rather than embodied AI. The focus of the former is pattern recognition in images, videos, and text on datasets typically curated from the internet [10, 18, 4]. The focus of the latter is to enable action by an embodied agent (e.g. a robot) in an environment. This brings to the fore active perception, long-term planning, learning from interaction, and holding a dialog grounded in an environment.</p>
<p>A straightforward proposal is to train agents directly in the physical world - exposing them to all its richness. This is valuable and will continue to play an important role in the development of AI. However, we also recognize that training robots in the real world is slow (the real world runs no faster than real time and cannot be parallelized), dangerous (poorly-trained agents can unwittingly injure themselves, the environment, or others), resource intensive (the robot(s) and the environment(s) in which they execute demand resources and time), difficult to control (it is hard to test corner-case scenarios as these are, by definition, infrequent and challenging to recreate), and not easily reproducible (replicating conditions across experiments and institutions is difficult).</p>
<p>We aim to support a complementary research program:</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The 'software stack' for training embodied agents involves (1) <em>datasets</em> providing 3D assets with semantic annotations, (2) <em>simulators</em> that render these assets and within which an embodied agent may be simulated, and (3) <em>tasks</em> that define evaluatable problems that enable us to benchmark scientific progress. Prior work (highlighted in blue boxes) has contributed a variety of datasets, simulation software, and task definitions. We propose a unified embodied agent stack with the Habitat platform, including generic dataset support, a highly performant simulator (Habitat-Sim), and a flexible API (Habitat-API) allowing the definition and evaluation of a broad set of tasks.</p>
<p>Training embodied agents (<em>e.g</em>. virtual robots) in rich realistic simulators and then transferring the learned skills to reality. Simulations have a long and rich history in science and engineering (from aerospace to zoology). In the context of embodied AI, simulators help overcome the aforementioned challenges – they can run orders of magnitude faster than real-time and can be parallelized over a cluster; training in simulation is safe, cheap, and enables fair comparison and benchmarking of progress in a concerted community-wide effort. Once a promising approach has been developed and tested in simulation, it can be transferred to physical platforms that operate in the real world [6, 15].</p>
<p>Datasets have been a key driver of progress in computer vision, NLP, and other areas of AI [10, 18, 4, 1]. As the community transitions to embodied AI, we believe that simulators will assume the role played previously by datasets. To support this transition, we aim to standardize the entire 'software stack' for training embodied agents (Figure 1): scanning the world and creating photorealistic 3D assets, developing the next generation of highly efficient and parallelizable simulators, specifying embodied AI tasks that enable us to benchmark scientific progress, and releasing modular high-level libraries for training and deploying embodied agents. Specifically, Habitat consists of the following:</p>
<ol>
<li>
<p>Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, multiple sensors, and generic 3D dataset handling (with built-in support for Matterport3D, Gibson, and Replica datasets).</p>
</li>
<li>
<p>Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms – defining embodied AI tasks (<em>e.g</em>. navigation, instruction following, question answering), configuring and training embodied agents (via imitation or reinforcement learning, or via classic SLAM), and benchmarking using standard metrics [2].</p>
</li>
</ol>
<p>The Habitat architecture and implementation combine modularity and high performance. When rendering a scene from the Matterport3D dataset, Habitat-Sim achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU, which is orders of magnitude faster than the closest simulator. Habitat-API allows us to train and benchmark embodied agents with different classes of methods and in different 3D scene datasets.</p>
<p>These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation [2], we make two scientific contributions:</p>
<ol>
<li>
<p>We revisit the comparison between learning and SLAM approaches from two recent works [20, 16] and find evidence for the <strong>opposite conclusion</strong> – that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations.</p>
</li>
<li>
<p>We conduct the first cross-dataset generalization experiments {train, test} × {Matterport3D, Gibson} for multiple sensors {Blind¹, RGB, RGBD, D} × {GPS+Compass} and find that only agents with depth (D) sensors generalize well across datasets.</p>
</li>
</ol>
<p>We hope that our open-source platform and these findings will advance and guide future research in embodied AI.</p>
<p><sup>1</sup> <em>Blind</em> refers to agents with no visual sensory inputs.</p>
<h2>2 Related Work</h2>
<p>Reality is something you rise above.
Liza Minnelli</p>
<p>The availability of large-scale 3D scene datasets [5, 27, 8] and community interest in active vision tasks led to a recent surge of work that resulted in the development of a variety of simulation platforms for indoor environments [17, 7, 13, 24, 29, 3, 30, 31, 23]. These platforms vary with respect to the 3D scene data they use, the embodied agent tasks they address, and the evaluation protocols they implement.</p>
<p>This surge of activity is both thrilling and alarming. On the one hand, it is clearly a sign of the interest in embodied AI across diverse research communities (computer vision, natural language processing, robotics, machine learning). On the other hand, the existence of multiple differing simulation environments can cause fragmentation, replication of effort, and difficulty in reproduction and community-wide progress. Moreover, existing simulators exhibit several shortcomings:</p>
<ul>
<li>Tight coupling of task (e.g. navigation), simulation platform (e.g. GibsonEnv), and 3D dataset (e.g. Gibson). Experiments with multiple tasks or datasets are impractical.</li>
<li>Hard-coded agent configuration (e.g. size, action-space). Ablations of agent parameters and sensor types are not supported, making results hard to compare.</li>
<li>Suboptimal rendering and simulation performance. Most existing indoor simulators operate at relatively low frame rates (10-100 fps), becoming a bottleneck in training agents and making large-scale learning infeasible. Takeaway messages from such experiments become unreliable - has the learning converged to trust the comparisons?</li>
<li>Limited control of environment state. The structure of the 3D scene in terms of present objects cannot be programmatically modified (e.g. to test the robustness of agents). Most critically, work built on top of any of the existing platforms is hard to reproduce independently from the platform, and thus hard to evaluate against work based on a different platform, even in cases where the target tasks and datasets are the same. This status quo is undesirable and motivates the Habitat effort. We aim to learn from the successes of previous frameworks and develop a unifying platform that combines their desirable characteristics while addressing their limitations. A common, unifying platform can significantly accelerate research by enabling code re-use and consistent experimental methodology. Moreover, a common platform enables us to easily carry out experiments testing agents based on different paradigms (learned vs. classical) and generalization of agents between datasets.</li>
</ul>
<p>The experiments we carry out contrasting learned and classical approaches to navigation are similar to the recent work of Mishkin et al. [20]. However, the performance of the Habitat stack relative to MINOS [24] used in [20]
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example rendered sensor observations for three sensors (color camera, depth sensor, semantic instance mask) in two different environment datasets. A Matterport3D [8] environment is in the top row, and a Replica [28] environment in the bottom row.</p>
<ul>
<li>thousands vs. one hundred frames per second - allows us to evaluate agents that have been trained with significantly larger amounts of experience ( 75 million steps vs. five million steps). The trends we observe demonstrate that learned agents can begin to match and outperform classical approaches when provided with large amounts of training experience. Other recent work by Koijima and Deng [16] has also compared hand-engineered navigation agents against learned agents but their focus is on defining additional metrics to characterize the performance of agents and to establish measures of hardness for navigation episodes. To our knowledge, our experiments are the first to train navigation agents provided with multi-month experience in realistic indoor environments and contrast them against classical methods.</li>
</ul>
<h2>3 Habitat Platform</h2>
<p>What I cannot create I do not understand.
Richard Feynman</p>
<p>The development of Habitat is a long-term effort to enable the formation of a common task framework [12] for research into embodied agents, thereby supporting systematic research progress in this area.
Design requirements. The issues discussed in the previous section lead us to a set of requirements that we seek to fulfill.</p>
<ul>
<li>Highly performant rendering engine: resourceefficient rendering engine that can produce multiple channels of visual information (e.g. RGB, depth, semantic instance segmentation, surface normals, optical flow) for multiple concurrently operating agents.</li>
<li>Scene dataset ingestion API: makes the platform agnostic to 3D scene datasets and allows users to use their own datasets.</li>
<li>
<p>Agent API: allows users to specify parameterized embodied agents with well-defined geometry, physics, and actuation characteristics.</p>
</li>
<li>
<p>Sensor suite API: allows specification of arbitrary numbers of parameterized sensors (e.g. RGB, depth, contact, GPS, compass sensors) attached to each agent.</p>
</li>
<li>Scenario and task API: allows portable definition of tasks and their evaluation protocols.</li>
<li>Implementation: C++ backend with Python API and interoperation with common learning frameworks, minimizes entry threshold.</li>
<li>Containerization: enables distributed training in clusters and remote-server evaluation of user-provided code.</li>
<li>Humans-as-agents: allows humans to function as agents in simulation in order to collect human behavior and investigate human-agent or human-human interactions.</li>
<li>Environment state manipulation: programmatic control of the environment configuration in terms of the objects that are present and their relative layout.
Design overview. The above design requirements cut across several layers in the 'software stack' in Figure 1. A monolithic design is not suitable for addressing requirements at all levels. We, therefore, structure the Habitat platform to mirror this multi-layer abstraction.</li>
</ul>
<p>At the lowest level is Habitat-Sim, a flexible, highperformance 3D simulator, responsible for loading 3D scenes into a standardized scene-graph representation, configuring agents with multiple sensors, simulating agent motion, and returning sensory data from an agent's sensor suite. The sensor abstraction in Habitat allows additional sensors such as LIDAR and IMU to be easily implemented as plugins.
Generic 3D dataset API using scene graphs. Habitat-Sim employs a hierarchical scene graph to represent all supported 3D environment datasets, whether synthetic or based on real-world reconstructions. The use of a uniform scene graph representation allows us to abstract the details of specific datasets, and to treat them in a consistent fashion. Scene graphs allow us to compose 3D environments through procedural scene generation, editing, or programmatic manipulation.
Rendering engine. The Habitat-Sim backend module is implemented in C++ and leverages the Magnum graphics middleware library ${ }^{2}$ to support cross-platform deployment on a broad variety of hardware configurations. The simulator backend employs an efficient rendering pipeline that implements visual sensor frame rendering using a multiattachment 'uber-shader' combining outputs for color camera sensors, depth sensors, and semantic mask sensors. By allowing all outputs to be produced in a single render pass, we avoid additional overhead when sensor parameters are shared and the same render pass can be used for all outputs. Figure 2 shows examples of visual sensors rendered in three different supported datasets. The same agent and sensor configuration was instantiated in a scene from each of the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Performance of Habitat-Sim in frames per second for an example Matterport3D scene (id 17DRP5sb8fy) on an Intel Xeon E5-2690 v4 CPU and Nvidia Titan Xp GPU, measured at different frame resolutions and with a varying number of concurrent simulator processes sharing the GPU. See the supplement for additional benchmarking results.
three datasets by simply specifying a different input scene.
Performance. Habitat-Sim achieves thousands of frames per second per simulator thread and is orders of magnitude faster than previous simulators for realistic indoor environments (which typically operate at tens or hundreds of frames per second) - see Table 1 for a summary and the supplement for more details. By comparison, AI2-THOR [17] and CHALET [31] run at tens of fps, MINOS [24] and Gibson [30] run at about a hundred, and House3D [29] runs at about 300 fps . Habitat-Sim is 2-3 orders of magnitude faster. By operating at 10,000 frames per second we shift the bottleneck from simulation to optimization for network training. Based on TensorFlow benchmarks, many popular network architectures run at frame rates that are 10-100x lower on a single GPU ${ }^{3}$. In practice, we have observed that it is often faster to generate images using Habitat-Sim than to load images from disk.
Efficient GPU throughput. Currently, frames rendered by Habitat-Sim are exposed as Python tensors through shared memory. Future development will focus on even higher rendering efficiency by entirely avoiding GPU-toCPU memory copy overhead through the use of CUDA-GL interoperation and direct sharing of render buffers and textures as tensors. Our preliminary internal testing suggests that this can lead to a speedup by a factor of 2 .</p>
<p>Above the simulation backend, the Habitat-API layer is a modular high-level library for end-to-end development in embodied AI. Setting up an embodied task involves specifying observations that may be used by the agent(s), using environment information provided by the simulator, and connecting the information with a task-specific episode dataset. - Task: this class extends the simulator's Observations class and action space with taskspecific ones. The criteria of episode termination and measures of success are provided by the Task. For example, in goal-driven navigation, Task provides the goal and evaluation metric [2]. To support this kind of functionality the Task has read-only access to</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Simulator and Episode-Dataset.</p>
<ul>
<li>Episode: a class for episode specification that includes the initial position and orientation of an Agent, scene id, goal position, and optionally the shortest path to the goal. An episode is a description of an instance of the task.</li>
<li>Environment: the fundamental environment concept for Habitat, abstracting all the information needed for working on embodied tasks with a simulator.
More details about the architecture of the Habitat platform, performance measurements, and examples of API use are provided in the supplement.</li>
</ul>
<h2>4. PointGoal Navigation at Scale</h2>
<p>To demonstrate the utility of the Habitat platform design, we carry out experiments to test for generalization of goal-directed visual navigation agents between datasets of different environments and to compare the performance of learning-based agents against classic agents as the amount of available training experience is increased.
Task definition. We use the PointGoal task (as defined by Anderson et al. [2]) as our experimental testbed. This task is ostensibly simple to define - an agent is initialized at a random starting position and orientation in an environment and asked to navigate to target coordinates that are provided relative to the agent's position; no ground-truth map is available and the agent must only use its sensory input to navigate. However, in the course of experiments, we realized that this task leaves space for subtle choices that (a) can make a significant difference in experimental outcomes and (b) are either not specified or inconsistent across papers, making comparison difficult. We attempt to be as descriptive as possible about these seemingly low-level choices; we hope the Habitat platform will help iron out these inconsistencies.
Agent embodiment and action space. The agent is physically embodied as a cylindrical primitive shape with diameter 0.2 m and height 1.5 m . The action space consists of four actions: turn_left, turn_right, move_forward, and stop. These actions are mapped to idealized actuations that result in 10 degree turns for the turning actions and linear displacement of 0.25 m for the move_forward action. The stop action allows the agent to signal that it has reached the goal. Habitat supports noisy actuations but experiments in this paper are conducted in the noise-free setting as our analysis focuses on other factors.
Collision dynamics. Some previous works [3] use a coarse irregular navigation graph where an agent effectively 'teleports' from one location to another (1-2m apart). Others [9] use a fine-grained regular grid ( 0.01 m resolution) where the agent moves on unoccupied cells and there are no collisions or partial steps. In Habitat and our experiments, we use a more realistic collision model - the agent navigates in a
continuous state space ${ }^{4}$ and motion can produce collisions resulting in partial (or no) progress along the direction intended - simply put, it is possible for the agent to 'slide' along a wall or obstacle. Crucially, the agent may choose move_forward $(0.25 \mathrm{~m})$ and end up in a location that is not 0.25 m forward of where it started; thus, odometry is not trivial even in the absence of actuation noise.
Goal specification: static or dynamic? One conspicuous underspecification in the PointGoal task [2] is whether the goal coordinates are static (i.e. provided once at the start of the episode) or dynamic (i.e. provided at every time step). The former is more realistic - it is difficult to imagine a real task where an oracle would provide precise dynamic goal coordinates. However, in the absence of actuation noise and collisions, every step taken by the agent results in a known turn or translation, and this combined with the initial goal location is functionally equivalent to dynamic goal specification. We hypothesize that this is why recent works [16, 20, 13] used dynamic goal specification. We follow and prescribe the following conceptual delineation - as a task, we adopt static PointGoal navigation; as for the sensor suite, we equip our agents with an idealized GPS+Compass sensor. This orients us towards a realistic task (static PointGoal navigation), disentangles simulator design (actuation noise, collision dynamics) from the task definition, and allows us to compare techniques by sensors used (RGB, depth, GPS, compass, contact sensors).
Sensory input. The agents are endowed with a single color vision sensor placed at a height of 1.5 m from the center of the agent's base and oriented to face 'forward'. This sensor provides RGB frames at a resolution of $256^{2}$ pixels and with a field of view of 90 degrees. In addition, an idealized depth sensor is available, in the same position and orientation as the color vision sensor. The field of view and resolution of the depth sensor match those of the color vision sensor. We designate agents that make use of the color sensor by RGB, agents that make use of the depth sensor by Depth, and agents that make use of both by RGBD. Agents that use neither sensor are denoted as Blind. All agents are equipped with an idealized GPS and compass - i.e., they have access to their location coordinates, and implicitly their orientation relative to the goal position.
Episode specification. We initialize the agent at a starting position and orientation that are sampled uniformly at random from all navigable positions on the floor of the environment. The goal position is chosen such that it lies on the same floor and there exists a navigable path from the agent's starting position. During the episode, the agent is allowed to take up to 500 actions. This threshold significantly exceeds the number of steps an optimal agent requires to reach all goals (see the supplement). After each action, the agent</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>receives a set of observations from the active sensors.
Evaluation. A navigation episode is considered successful if and only if the agent issues a stop action within 0.2 m of the target coordinates, as measured by a geodesic distance along the shortest path from the agent's position to the goal position. If the agent takes 500 actions without the above condition being met the episode ends and is considered unsuccessful. Performance is measured using the 'Success weighted by Path Length' (SPL) metric [2]. For an episode where the geodesic distance of the shortest path is $l$ and the agent traverses a distance $p$, SPL is defined as $S \cdot \sqrt{ } / \max (p, l)$, where $S$ is a binary indicator of success.
Episode dataset preparation. We create PointGoal navigation episode-datasets for Matterport3D [8] and Gibson [30] scenes. For Matterport3D we followed the publicly available train/val/test splits. Note that as in recent works [9, 20, 16], there is no overlap between train, val, and test scenes. For Gibson scenes, we obtained textured 3D surface meshes from the Gibson authors [30], manually annotated each scene on its reconstruction quality (small/big holes, floating/irregular surfaces, poor textures), and curated a subset of 106 scenes (out of 572); see the supplement for details. An episode is defined by the unique id of the scene, the starting position and orientation of the agent, and the goal position. Additional metadata such as the geodesic distance along the shortest path (GDSP) from start position to goal position is also included. While generating episodes, we restrict the GDSP to be between 1 m and 30 m . An episode is trivial if there is an obstacle-free straight line between the start and goal positions. A good measure of the navigation complexity of an episode is the ratio of GDSP to Euclidean distance between start and goal positions (notice that GDSP can only be larger than or equal to the Euclidean distance). If the ratio is nearly 1 , there are few obstacles and the episode is easy; if the ratio is much larger than 1 , the episode is difficult because strategic navigation is required. To keep the navigation complexity of the precomputed episodes reasonably high, we perform rejection sampling for episodes with the above ratio falling in the range $[1,1.1]$. Following this, there is a significant decrease in the number of near-straight-line episodes (episodes with a ratio in $[1,1.1]$ ) - from $37 \%$ to $10 \%$ for the Gibson dataset generation. This step was not performed in any previous studies. We find that without this filtering, all metrics appear inflated. Gibson scenes have smaller physical dimensions compared to the Matterport3D scenes. This is reflected in the resulting PointGoal dataset average GDSP of episodes in Gibson scenes is smaller than that of Matterport3D scenes.
Baselines. We compare the following baselines:</p>
<ul>
<li>Random chooses an action randomly among turn_left, turn_right, and move_forward with uniform distribution. The agent calls the stop action when within 0.2 m of the goal (computed using the
difference of static goal and dynamic GPS coordinates).</li>
<li>Forward only always calls the move_forward action, and calls the stop action when within 0.2 m of the goal.</li>
<li>Goal follower moves towards the goal direction. If it is not facing the goal (more than 15 degrees off-axis), it performs turn_left or turn_right to align itself; otherwise, it calls move_forward. The agent calls the stop action when within 0.2 m of the goal.</li>
<li>RL (PPO) is an agent trained with reinforcement learning, specifically proximal policy optimization [25]. We experiment with RL agents equipped with different visual sensors: no visual input (Blind), RGB input, Depth input, and RGB with depth (RGBD). The model consists of a CNN that produces an embedding for visual input, which together with the relative goal vector is used by an actor (GRU) and a critic (linear layer). The CNN has the following architecture: ${$ Conv $8 \times 8$, ReLU, Conv $4 \times 4$, ReLU, Conv $3 \times 3$, ReLU, Linear, ReLU $}$ (see supplement for details). Let $r_{t}$ denote the reward at timestep $t, d_{t}$ be the geodesic distance to goal at timestep $t, s$ a success reward and $\lambda$ a time penalty (to encourage efficiency). All models were trained with the following reward function:</li>
</ul>
<p>$$
r_{t}= \begin{cases}s+d_{t-1}-d_{t}+\lambda &amp; \text { if goal is reached } \ d_{t-1}-d_{t}+\lambda &amp; \text { otherwise }\end{cases}
$$</p>
<p>In our experiments $s$ is set to 10 and $\lambda$ is set to -0.01 . Note that rewards are only provided in training environments; the task is challenging as the agent must generalize to unseen test environments.</p>
<ul>
<li>SLAM [20] is an agent implementing a classic robotics navigation pipeline (including components for localization, mapping, and planning), using RGB and depth sensors. We use the classic agent by Mishkin et al. [20] which leverages the ORB-SLAM2 [21] localization pipeline, with the same parameters as reported in the original work.</li>
</ul>
<p>Training procedure. When training learning-based agents, we first divide the scenes in the training set equally among 8 (Gibson), 6 (Matterport3D) concurrently running simulator worker threads. Each thread establishes blocks of 500 training episodes for each scene in its training set partition and shuffles the ordering of these blocks. Training continues through shuffled copies of this array. We do not hardcode the stop action to retain generality and allow for comparison with future work that does not assume GPS inputs. For the experiments reported here, we train until 75 million agent steps are accumulated across all worker threads. This is 15 x larger than the experience used in previous investigations [20, 16]. Training agents to 75 million steps took (in sum over all three datasets): 320 GPU-hours for Blind, 566 GPU-hours for RGB, 475 GPU-hours for Depth, and 906 GPU-hours for RGBD (overall 2267 GPU-hours).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Average SPL of agents on the val set over the course of training. Previous work [20, 16] has analyzed performance at 5-10 million steps. Interesting trends emerge with more experience: i) Blind agents initially outperform RGB and RGBD but saturate quickly; ii) Learning-based Depth agents outperform classic SLAM. The shaded areas around curves show the standard error of SPL over five seeds.</p>
<table>
<thead>
<tr>
<th>Sensors</th>
<th>Baseline</th>
<th>Gibson</th>
<th></th>
<th>MP3D</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>SPL</td>
<td>Succ</td>
<td>SPL</td>
<td>Succ</td>
</tr>
<tr>
<td>Blind</td>
<td>Random</td>
<td>0.02</td>
<td>0.03</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td></td>
<td>Forward only</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>Goal follower</td>
<td>0.23</td>
<td>0.23</td>
<td>0.12</td>
<td>0.12</td>
</tr>
<tr>
<td></td>
<td>RL (PPO)</td>
<td>0.42</td>
<td>0.62</td>
<td>0.25</td>
<td>0.35</td>
</tr>
<tr>
<td>RGB</td>
<td>RL (PPO)</td>
<td>0.46</td>
<td>0.64</td>
<td>0.30</td>
<td>0.42</td>
</tr>
<tr>
<td>Depth</td>
<td>RL (PPO)</td>
<td>0.79</td>
<td>0.89</td>
<td>0.54</td>
<td>0.69</td>
</tr>
<tr>
<td>RGBD</td>
<td>RL (PPO)</td>
<td>0.70</td>
<td>0.80</td>
<td>0.42</td>
<td>0.53</td>
</tr>
<tr>
<td></td>
<td>SLAM [20]</td>
<td>0.51</td>
<td>0.62</td>
<td>0.39</td>
<td>0.47</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of baseline methods on the PointGoal task [2] tested on the Gibson [30] and MP3D [8] test sets under multiple sensor configurations. RL models have been trained for 75 million steps. We report average rate of episode success and SPL [2].</p>
<h2>5. Results and Findings</h2>
<p>We seek to answer two questions: i) how do learning-based agents compare to classic SLAM and hand-coded baselines as the amount of training experience increases and ii) how well do learned agents generalize across 3D datasets.</p>
<p>It should be tacitly understood, but to be explicit – 'learning' and 'SLAM' are broad families of techniques (and not a single method), are not necessarily mutually exclusive, and are not 'settled' in their development. We compare representative instances of these families to gain insight into questions of scaling and generalization, and do not make any claims about intrinsic superiority of one or the other.</p>
<p>Learning vs SLAM. To answer the first question we plot agent performance (SPL) on validation (i.e. unseen) episodes over the course of training in Figure 3 (top: Gibson, bottom: Matterport3D). SLAM [20] does not require training and thus has a constant performance (0.59 on Gibson, 0.42 on Matterport3D). All RL (PPO) agents start out with far worse SPL, but RL (PPO) Depth, in particular, improves dramatically and matches the classic baseline at approximately 10M frames (Gibson) or 30M frames (Matterport3D) of experience, continuing to improve thereafter. Notice that if we terminated the experiment at 5M frames as in [20] we would also conclude that SLAM [20] dominates. Interestingly, RGB agents do not significantly outperform Blind agents; we hypothesize because both are equipped with GPS sensors. Indeed, qualitative results (Figure 4 and video in supplement) suggest that Blind agents 'hug' walls and implement 'wall following' heuristics. In contrast, RGB sensors provide a high-dimensional complex signal that may be prone to overfitting to train environments due to the variety across scenes (even within the same dataset). We also notice in Figure 3 that <em>all methods</em> perform better on Gibson than Matterport3D. This is consistent with our previous analysis that Gibson contains smaller scenes and shorter episodes.</p>
<p>Next, for each agent and dataset, we select the best-performing checkpoint on validation and report results on test in Table 2. We observe that uniformly across the datasets, RL (PPO) Depth performs best, outperforming RL (PPO) RGBD (by 0.09-0.16 SPL), SLAM (by 0.15-0.28 SPL), and RGB (by 0.13-0.33 SPL) in that order (see the supplement for additional experiments involving noisy depth). We believe Depth performs better than RGBD because i) the PointGoal navigation task requires reasoning only about free space and depth provides relevant information directly, and ii) RGB has significantly more entropy (different houses look very different), thus it is easier to overfit when using RGB. We ran our experiments with 5 random seeds per run, to confirm that these differences are statistically significant. The differences are about an order of magnitude larger than the standard deviation of average SPL for all cases (e.g. on the Gibson dataset errors are, Depth: ±0.015, RGB: ±0.055, RGBD: ±0.028, Blind: ±0.005). Random and forward-only agents have</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Navigation examples for different sensory configurations of the RL (PPO) agent, visualizing trials from the Gibson and MP3D val sets. A <strong>blue dot</strong> and <strong>red dot</strong> indicate the starting and goal positions, and the <strong>blue arrow</strong> indicates final agent position. The <strong>blue-green-red line</strong> is the agent's trajectory. Color shifts from blue to red as the maximum number of agent steps is approached. See the supplemental materials for more example trajectories.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Generalization of agents between datasets. We report average SPL for a model trained on the source dataset in each row, as evaluated on test episodes for the target dataset in each column.</p>
<p>very low performance, while the hand-coded goal follower and Blind baseline see modest performance. See the supplement for additional analysis of trained agent behavior.</p>
<p>In Figure 4 we plot example trajectories for the RL (PPO) agents, to qualitatively contrast their behavior in the same episode. Consistent with the aggregate statistics, we observe that Blind collides with obstacles and follows walls, while Depth is the most efficient. See the supplement and the video for more example trajectories.</p>
<p><strong>Generalization across datasets.</strong> Our findings so far are that RL (PPO) agents significantly outperform SLAM [20]. This prompts our second question – are these findings dataset specific or do learned agents generalize across datasets? We report exhaustive comparisons in Figure 5 – specifically, average SPL for all combinations of {train, test} × {Matterport3D, Gibson} for all agents {Blind, RGB, RGBD, Depth }. Rows indicate (agent, train set) pair, columns indicate test set. We find a number of interesting trends. First, nearly all agents suffer a drop in performance when trained on one dataset and tested on another, <em>e.g</em>. RGBD Gibson→Gibson 0.70 vs RGBD Gibson→Matterport3D 0.53 (drop of 0.17). RGB and RGBD agents suffer a significant performance degradation, while the Blind agent is least affected (as we would expect).</p>
<p>Second, we find a potentially counter-intuitive trend – agents trained on Gibson consistently outperform their counterparts trained on Matterport3D, <em>even when evaluated on Matterport3D</em>. We believe the reason is the previously noted observation that Gibson scenes are smaller and episodes are shorter (lower GDSP) than Matterport3D. Gibson agents are trained on 'easier' episodes and encounter positive reward more easily during random exploration, thus bootstrapping learning. Consequently, for a fixed computation budget Gibson agents are stronger universally (not just on Gibson). This finding suggests that visual navigation agents could benefit from curriculum learning.</p>
<p>These insights are enabled by the engineering of Habitat, which made these experiments as simple as a change in the evaluation dataset name.</p>
<h1>6. Habitat Challenge</h1>
<p>No battle plan ever survives contact with the enemy.</p>
<p><em>Helmuth Karl Bernhard von Moltke</em></p>
<p>Challenges drive progress. The history of AI sub-fields indicates that the formulation of the right questions, the creation of the right datasets, and the coalescence of communities around the right challenges drives scientific progress. Our goal is to support this process for embodied AI. Habitat Challenge is an autonomous navigation challenge that aims to benchmark and advance efforts in goal-directed visual navigation.</p>
<p>One difficulty in creating a challenge around embodied AI tasks is the transition from static predictions (as in passive perception) to sequential decision making (as in sensorimotor control). In traditional 'internet AI' challenges (<em>e.g</em>. ImageNet [10], COCO [18], VQA [4]), it is possible to release a static testing dataset and ask participants to simply upload their predictions on this set. In contrast, embodied AI tasks typically involve sequential decision making and agent-driven control, making it infeasible to pre-package a testing dataset. Essentially, embodied AI challenges require participants to <em>upload code not predictions</em>. The uploaded agents can then be evaluated in novel (unseen) test environments.</p>
<p><strong>Challenge infrastructure.</strong> We leverage the frontend and challenge submission process of the EvalAI platform, and build backend infrastructure ourselves. Participants in Habitat Challenge are asked to upload Docker containers [19] with their agents via EvalAI. The submitted agents are then evaluated on a live AWS GPU-enabled instance. Specifically, contestants are free to train their agents however they wish (any language, any framework, any infrastructure). In order to evaluate these agents, participants are asked to derive from a base Habitat Docker container and implement a spe-</p>
<p>cific interface to their model - agent's action taken given an observation from the environment at each step. This dockerized interface enables running the participant code on new environments.</p>
<p>More details regarding the Habitat Challenge held at CVPR 2019 are available at the https://aihabitat. org/challenge/ website. In a future iteration of this challenge we will introduce three major differences designed to both reduce the gap between simulation and reality and to increase the difficulty of the task.</p>
<ul>
<li>In the 2019 challenge, the relative coordinates specifying the goal were continuously updated during agent movement - essentially simulating an agent with perfect localization and heading estimation (e.g. an agent with an idealized GPS+Compass). However, high-precision localization in indoor environments can not be assumed in realistic settings - GPS has low precision indoors, (visual) odometry may be noisy, SLAM-based localization can fail, etc. Hence, we will investiage only providing to the agent a fixed relative coordinate for the goal position from the start location.</li>
<li>Likewise, the 2019 Habitat Challenge modeled agent actions (e.g. forward, turn $10^{\circ}$ left,...) deterministically. However in real settings, agent intention (e.g. go forward 1 m ) and the result rarely match perfectly - actuation error, differing surface materials, and a myriad of other sources of error introduce significant drift over a long trajectory. To model this, we introduce a noise model acquired by benchmarking a real robotic platform [22]. Visual sensing is an excellent means of combating this "dead-reckoning" drift and this change allows participants to study methodologies that are robust to and can correct for this noise.</li>
<li>Finally, we will introduce realistic models of sensor noise for RGB and depth sensors - narrowing the gap between perceptual experiences agents would have in simulation and reality.
We look forward to supporting the community in establishing a benchmark to evaluate the state-of-the-art in methods for embodied navigation agents.</li>
</ul>
<h2>7. Future Work</h2>
<p>We described the design and implementation of the Habitat platform. Our goal is to unify existing community efforts and to accelerate research into embodied AI. This is a longterm effort that will succeed only by full engagement of the broader research community.</p>
<p>Experiments enabled by the generic dataset support and the high performance of the Habitat stack indicate that i) learning-based agents can match and exceed the performance of classic visual navigation methods when trained for long enough and ii) learned agents equipped with depth
sensors generalize well between different 3D environment datasets in comparison to agents equipped with only RGB.
Feature roadmap. Our near-term development roadmap will focus on incorporating physics simulation and enabling physics-based interaction between mobile agents and objects in 3D environments. Habitat-Sim's scene graph representation is well-suited for integration with physics engines, allowing us to directly control the state of individual objects and agents within a scene graph. Another planned avenue of future work involves procedural generation of 3D environments by leveraging a combination of 3D reconstruction and virtual object datasets. By combining high-quality reconstructions of large indoor spaces with separately reconstructed or modelled objects, we can take full advantage of our hierarchical scene graph representation to introduce controlled variation in the simulated 3D environments.</p>
<p>Lastly, we plan to focus on distributed simulation settings that involve large numbers of agents potentially interacting with one another in competitive or collaborative scenarios.</p>
<p>Acknowledgments. We thank the reviewers for their helpful suggestions. The Habitat project would not have been possible without the support and contributions of many individuals. We are grateful to Mandeep Baines, Angel Xuan Chang, Alexander Clegg, Devendra Singh Chaplot, Xinlei Chen, Wojciech Galuba, Georgia Gkioxari, Daniel Gordon, Leonidas Guibas, Saurabh Gupta, Jerry (Zhi-Yang) He, Rishabh Jain, Or Litany, Joel Marcey, Dmytro Mishkin, Marcus Rohrbach, Amanpreet Singh, Yuandong Tian, Yuxin Wu, Fei Xia, Deshraj Yadav, Amir Zamir, and Jiazhi Zhang for their help.</p>
<h2>Licenses for referenced datasets.</h2>
<p>Gibson: https://storage.googleapis. com/gibson_material/Agreement\%20GDS\% 2006-04-18.pdf
Matterport3D: http://kaldir.vc.in.tum.de/ matterport/MP_TOS.pdf.</p>
<h2>References</h2>
<p>[1] Phil Ammirato, Patrick Poirson, Eunbyung Park, Jana Košecká, and Alexander C Berg. A dataset for developing and benchmarking active vision. In ICRA, 2017.
[2] Peter Anderson, Angel X. Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Roshan Zamir. On evaluation of embodied navigation agents. arXiv:1807.06757, 2018.
[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, 2018.
[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In ICCV, 2015.
[5] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3D semantic parsing of large-scale indoor spaces. In CVPR, 2016.
[6] Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, Richard Shen, Vinh-Dieu Lam, and Alex Kendall. Learning to drive from simulation without real world labels. In ICRA, 2019.
[7] Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, and Aaron C. Courville. HoME: A household multimodal environment. arXiv:1711.11017, 2017.
[8] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGBD data in indoor environments. In International Conference on 3D Vision (3DV), 2017.
[9] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied Question Answering. In CVPR, 2018.
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805, 2018.
[12] David Donoho. 50 years of data science. In Tukey Centennial Workshop, 2015.
[13] Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. In CVPR, 2017.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
[15] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 2019.
[16] Noriyuki Kojima and Jia Deng. To learn or not to learn: Analyzing the role of learning for navigation in virtual environments. arXiv:1907.11770, 2019.
[17] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An interactive 3D environment for visual AI. arXiv:1712.05474, 2017.
[18] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In $E C C V, 2014$.
[19] Dirk Merkel. Docker: Lightweight Linux containers for consistent development and deployment. Linux Journal, 2014.
[20] Dmytro Mishkin, Alexey Dosovitskiy, and Vladlen Koltun. Benchmarking classic and learned navigation in complex 3D environments. arXiv:1901.10915, 2019.
[21] Raúl Mur-Artal and Juan D. Tardós. ORB-SLAM2: An open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Transactions on Robotics, 33(5), 2017.
[22] Adithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, and Abhinav Gupta. Pyrobot: An open-source robotics framework for research and benchmarking. arXiv preprint arXiv:1906.08236, 2019.
[23] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. VirtualHome: Simulating household activities via programs. In CVPR, 2018.
[24] Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun. MINOS: Multimodal indoor simulator for navigation in complex environments. arXiv:1712.03931, 2017.
[25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017.
[26] Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies. Artificial Life, 11(1-2), 2005.
[27] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene completion from a single depth image. In CVPR, 2017.
[28] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica dataset: A digital replica of indoor spaces. arXiv:1906.05797, 2019.
[29] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with a realistic and rich 3D environment. arXiv:1801.02209, 2018.
[30] Fei Xia, Amir R. Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In CVPR, 2018.
[31] Claudia Yan, Dipendra Misra, Andrew Bennnett, Aaron Walsman, Yonatan Bisk, and Yoav Artzi. CHALET: Cornell house agent learning environment. arXiv:1801.07357, 2018.</p>
<h2>A. Habitat Platform Details</h2>
<p>As described in the main paper, Habitat consists of the following components:</p>
<ul>
<li>Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, multiple sensors, and generic 3D dataset handling (with built-in support for Matterport3D [8], Gibson [30], and other datasets). Habitat-Sim is fast - when rendering a realistic scanned scene from the Matterport3D dataset, Habitat-Sim achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU.</li>
<li>Habitat-API: a modular high-level library for end-to-end development of embodied AI - defining embodied AI tasks (e.g. navigation [2], instruction following [3], question answering [9]), configuring embodied agents (physical form, sensors, capabilities), training these agents (via imitation or reinforcement learning, or via classic SLAM), and benchmarking their performance on the defined tasks using standard metrics [2]. Habitat-API currently uses Habitat-Sim as the core simulator, but is designed with a modular abstraction for the simulator backend to maintain compatibility over multiple simulators.
Key abstractions. The Habitat platform relies on a number of key abstractions that model the domain of embodied agents and tasks that can be carried out in three-dimensional indoor environments. Here we provide a brief summary of key abstractions:</li>
<li>Agent: a physically embodied agent with a suite of Sensors. Can observe the environment and is capable of taking actions that change agent or environment state.</li>
<li>Sensor: associated with a specific Agent, capable of returning observation data from the environment at a specified frequency.</li>
<li>SceneGraph: a hierarchical representation of a 3D environment that organizes the environment into regions and objects which can be programmatically manipulated.</li>
<li>Simulator: an instance of a simulator backend. Given actions for a set of configured Agents and SceneGraphs, can update the state of the Agents and SceneGraphs, and provide observations for all active Sensors possessed by the Agents.
These abstractions connect the different layers of the platform. They also enable generic and portable specification of embodied AI tasks.
Habitat-Sim. The architecture of the Habitat-Sim backend module is illustrated in Figure 6. The design of this module ensures a few key properties:</li>
<li>Memory-efficient management of 3D environment resources (triangle mesh geometry, textures, shaders) ensuring shared resources are cached and reused.</li>
<li>Flexible, structured representation of 3D environments using SceneGraphs, allowing for programmatic manipulation of object state, and combination of objects from different environments.</li>
<li>High-efficiency rendering engine with multi-attachment render pass to reduce overhead for multiple sensors.</li>
<li>Arbitrary numbers of Agents and corresponding Sensors that can be linked to a 3D environment by attachment to a SceneGraph.
The performance of the simulation backend surpasses that of prior work operating on realistic reconstruction datasets by a large margin. Table 3 reports performance statistics on a test scene from the Matterport3D dataset. Single-thread performance reaches several thousand frames per second (fps), while multi-process operation with several simulation backends can reach over 10,000 fps on a single GPU. In addition, by employing OpenGL-CUDA interoperation we enable direct sharing of rendered image frames with ML frameworks such as PyTorch without a measurable impact on performance as the image resolution is increased (see Figure 7).
Habitat-API. The second layer of the Habitat platform (Habitat-API) focuses on creating a general and flexible API for defining embodied agents, tasks that they may carry out, and evaluation metrics for those tasks. When designing such an API, a key consideration is to allow for easy extensibility of the defined abstractions. This is particularly important since many of the parameters of embodied agent tasks, specific agent configurations, and 3D environment setups can be varied in interesting ways. Future research is likely to propose new tasks, new agent configurations, and new 3D environments.</li>
</ul>
<p>The API allows for alternative simulator backends to be used, beyond the Habitat-Sim module that we implemented. This modularity has the advantage of allowing incorporation of existing simulator backends to aid in transitioning from experiments that previous work has performed using legacy frameworks. The architecture of Habitat-API is illustrated in Figure 8, indicating core API functionality and functionality implemented as extensions to the core.</p>
<p>Above the API level, we define a concrete embodied task such as visual navigation. This involves defining a specific dataset configuration, specifying the structure of episodes (e.g. number of steps taken, termination conditions), training curriculum (progression of episodes, difficulty ramp), and evaluation procedure (e.g. test episode sets and task metrics). An example of loading a pre-configured task (PointNav) and stepping through the environment with a random agent is shown in the code below.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Architecture of Habitat-Sim main classes. The Simulator delegates management of all resources related to 3D environments to a ResourceManager that is responsible for loading and caching 3D environment data from a variety of on-disk formats. These resources are used within SceneGraphs at the level of individual SceneNodes that represent distinct objects or regions in a particular Scene. Agents and their Sensors are instantiated by being attached to SceneNodes in a particular SceneGraph.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPU→CPU→GPU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPU→CPU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPU→GPU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Sensors / number of processes</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">RGB</td>
<td style="text-align: center;">2,346</td>
<td style="text-align: center;">6,049</td>
<td style="text-align: center;">7,784</td>
<td style="text-align: center;">3,919</td>
<td style="text-align: center;">8,810</td>
<td style="text-align: center;">11,598</td>
<td style="text-align: center;">4,538</td>
<td style="text-align: center;">8,573</td>
<td style="text-align: center;">7,279</td>
</tr>
<tr>
<td style="text-align: center;">RGB + depth</td>
<td style="text-align: center;">1,260</td>
<td style="text-align: center;">3,025</td>
<td style="text-align: center;">3,730</td>
<td style="text-align: center;">1,777</td>
<td style="text-align: center;">4,307</td>
<td style="text-align: center;">5,522</td>
<td style="text-align: center;">2,151</td>
<td style="text-align: center;">3,557</td>
<td style="text-align: center;">3,486</td>
</tr>
<tr>
<td style="text-align: center;">RGB + depth + semantics</td>
<td style="text-align: center;">378</td>
<td style="text-align: center;">463</td>
<td style="text-align: center;">470</td>
<td style="text-align: center;">396</td>
<td style="text-align: center;">465</td>
<td style="text-align: center;">466</td>
<td style="text-align: center;">464</td>
<td style="text-align: center;">455</td>
<td style="text-align: center;">453</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of Habitat-Sim in frames per second for an example Matterport3D scene (id 17DRP5sb8fy) on a Xeon E5-2690 v4 CPU and Nvidia Titan Xp GPU, measured at a frame resolution of 128x128, under different frame memory transfer strategies and with a varying number of concurrent simulator processes sharing the GPU. 'GPU-CPU-GPU' indicates passing of rendered frames from OpenGL context to CPU host memory and back to GPU device memory for use in optimization, 'GPU-CPU' only reports copying from OpenGL context to CPU host memory, whereas 'GPU-GPU' indicates direct sharing through OpenGL-CUDA interoperation.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Performance of Habitat-Sim under different sensor frame memory transfer strategies for increasing image resolution. We see that 'GPU-&gt;GPU' is unaffected by image resolution while other strategies degrade rapidly.</p>
<h2>B. Additional Dataset Statistics</h2>
<p>In Table 5 we summarize the train, validation and test split sizes for all three datasets used in our experiments. We also
report the average geodesic distance along the shortest path (GDSP) between starting point and goal position. As noted in the main paper, Gibson episodes are significantly shorter than Matterport3D ones. Figure 9 visualizes the episode distributions over geodesic distance (GDSP), Euclidean distance between start and goal position, and the ratio of the two (an approximate measure of complexity for the episode). We again note that Gibson episodes have more episodes with shorter distances, leading to the dataset being overall easier than the Matterport3D dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">habitat</span>
<span class="c1"># Load embodied AI task (PointNav)</span>
<span class="c1"># and a pre-specified virtual robot</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">habitat</span><span class="o">.</span><span class="n">get_config</span><span class="p">(</span><span class="n">config_file</span><span class="o">-</span>
    <span class="s2">&quot;pointnav.yaml&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">habitat</span><span class="o">.</span><span class="n">Env</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">observations</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># Step through environment with random actions</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">env</span><span class="o">.</span><span class="n">episode_over</span><span class="p">:</span>
    <span class="n">observations</span> <span class="o">=</span> \
        <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span>
</code></pre></div>

<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Architecture of Habitat-API. The core functionality defines fundamental building blocks such as the API for interacting with the simulator backend and receiving observations through Sensors. Concrete simulation backends, 3D datasets, and embodied agent baselines are implemented as extensions to the core API.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>scenes (#)</th>
<th>episodes (#)</th>
<th>average GDSP (m)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Matterport3D</td>
<td>58 / 11 / 18</td>
<td>4.8M / 495 / 1008</td>
<td>11.5 / 11.1 / 13.2</td>
</tr>
<tr>
<td>Gibson</td>
<td>72 / 16 / 10</td>
<td>4.9M / 1000 / 1000</td>
<td>6.9 / 6.5 / 7.0</td>
</tr>
</tbody>
</table>
<p>Table 4: Statistics of the PointGoal navigation datasets that we precompute for the Matterport3D and Gibson datasets: total number of scenes, total number of episodes, and average geodesic distance between start and goal positions. Each cell reports train / val / test split statistics.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Min</th>
<th>Median</th>
<th>Mean</th>
<th>Max</th>
</tr>
</thead>
<tbody>
<tr>
<td>Matterport3D</td>
<td>18</td>
<td>90.0</td>
<td>97.1</td>
<td>281</td>
</tr>
<tr>
<td>Gibson</td>
<td>15</td>
<td>60.0</td>
<td>63.3</td>
<td>207</td>
</tr>
</tbody>
</table>
<p>Table 5: Statistics of path length (in actions) for an oracle which greedily fits actions to follow the negative of geodesic distance gradient on the PointGoal navigation validation sets. This provides expected horizon lengths for a near-perfect agent and contextualizes the decision for a max-step limit of 500.</p>
<h3>C. Additional Experimental Results</h3>
<p>In order to confirm that the trends we observe for the experimental results presented in the paper hold for much larger amounts of experience, we scaled our experiments to 800M steps. We found that (1) the ordering of the visual inputs stays Depth &gt; RGBD &gt; RGB &gt; Blind; (2) RGB is consistently better than Blind (by 0.06/0.03 SPL on Gibson/Matterport3D), and (3) RGBD outperforms SLAM on Matterport3D (by 0.16 SPL).</p>
<h4>C.1. Analysis of Collisions</h4>
<p>To further characterize the behavior of learned agents during navigation we plot the average number of collisions in Figure 10. We see that Blind incurs a much larger number of collisions than other agents, providing evidence for 'wall-following' behavior. Depth-equipped agents have the lowest number of collisions, while RGB agents are in between.</p>
<h4>C.2. Noisy Depth</h4>
<p>To investigate the impact of noisy depth measurements on agent performance, we re-evaluated depth agents (without re-training) on noisy depth generated using a simple noise model: iid Gaussian noise (µ = 0, σ = 0.4) at each pixel in inverse depth (larger depth = more noise). We observe a drop of 0.13 and 0.02 SPL for depth-RL and SLAM on Gibson-val (depth-RL still outperforms SLAM). Note that SLAM from [20] utilizes ORB-SLAM2, which is quite robust to noise, while depth-RL was trained without noise. If we increase σ to 0.1, depth-RL gets 0.12 SPL whereas SLAM suffers catastrophic failures.</p>
<h4>D. Gibson Dataset Curation</h4>
<p>We manually curated the full dataset of Gibson 3D textured meshes [30] to select meshes that do not exhibit significant reconstruction artifacts such as holes or texture quality issues. A key issue that we tried to avoid is the presence of</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Statistics of PointGoal navigation episodes. From left: distribution over Euclidean distance between start and goal, distribution over geodesic distance along shortest path between start and goal, and distribution over the ratio of geodesic to Euclidean distance.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Average number of collisions during successful navigation episodes for the different sensory configurations of the RL (PPO) baseline agent on test set episodes for the Gibson and Matterport3D datasets. The Blind agent experiences the highest number of collisions, while agents possessing depth sensors (Depth and RGBD) have the fewest collisions on average.</p>
<p>holes or cracks in floor surfaces. This is particularly problematic for navigation tasks as it divides seemingly connected navigable areas into non-traversable disconnected components. We manually annotated the scenes (using the 0 to 5 quality scale shown in Figure 11) and only use scenes with a rating of 4 or higher, i.e., no holes, good reconstruction, and negligible texture issues to generate the dataset episodes.</p>
<h3>E. Reproducing Experimental Results</h3>
<p>Our experimental results can be reproduced using the Habitat-API (commit ec9557a) and Habitat-Sim (commit d383c20) repositories. The code for running experiments is present under the folder habitat-api/habitat_baselines. Below is the shell script we used for our RL experiments:</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Note: parameters in {} are experiment specific.
<span class="gh">#</span> Note: use 8, 6 processes for Gibson, MP3D
<span class="gh">#</span> respectively.
python habitat_baselines/train_ppo.py \
    --sensors {RGB_SENSOR,DEPTH_SENSOR} \
    --blind {0,1} --use-gae --lr 2.5e-4 \
    --clip-param 0.1 --use-linear-lr-decay \
    --num-processes {8,6} --num-steps 128 \
    --num-mini-batch 4 --num-updates 135000 \
    --use-linear-clip-decay \
</code></pre></div>

<p>For running SLAM please refer to habitat-api/habitat_baselines/slambased.</p>
<h3>F. Example Navigation Episodes</h3>
<p>Figure 12 visualizes additional example navigation episodes for the different sensory configurations of the RL (PPO) agents that we describe in the main paper. Blind agents have the lowest performance, colliding much more frequently with the environment and adopting a 'wall hugging' strategy for navigation. RGB agents are less prone to collisions but still struggle to navigate to the goal position successfully in some cases. In contrast, depth-equipped agents are much more efficient, exhibiting fewer collisions, and navigating to goals more successfully (as indicated by the overall higher SPL values).</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>0 : critical reconstruction artifacts, holes, or texture issues
1: big holes or significant texture issues and reconstruction artifacts
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>2: big holes or significant texture issues, but good reconstruction
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>4: no holes, some texture issues, good reconstruction
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>3: small holes, some texture issues, good reconstruction
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>5: no holes, uniform textures, good reconstruction</p>
<p>Figure 11: Rating scale used in curation of 3D textured mesh reconstructions from the Gibson dataset. We use only meshes with ratings of 4 or higher for the Habitat Challenge dataset.</p>
<p>Gibson
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 12: Additional navigation example episodes for the different sensory configurations of the RL (PPO) agent, visualizing trials from the Gibson and MP3D val sets. A blue dot and red dot indicate the starting and goal positions, and the blue arrow indicates final agent position. The blue-green-red line is the agent's trajectory. Color shifts from blue to red as the maximum number of allowed agent steps is approached.</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 12: Additional navigation example episodes for the different sensory configurations of the RL (PPO) agent, visualizing trials from the Gibson and MP3D val sets. A blue dot and red dot indicate the starting and goal positions, and the blue arrow indicates final agent position. The blue-green-red line is the agent's trajectory. Color shifts from blue to red as the maximum number of allowed agent steps is approached.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Note: The semantic sensor in Matterport3D requires using additional 3D meshes with significantly more geometric complexity, leading to reduced performance. We expect this to be addressed in future versions, leading to speeds comparable to RGB + depth.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://www.tensorflow.org/guide/performance/ benchmarks&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1155 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1155</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1155</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-174799858</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1906.04161v1.pdf" target="_blank">Self-Supervised Exploration via Disagreement</a></p>
                <p><strong>Paper Abstract:</strong> Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1155.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1155.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble-disagreement intrinsic reward (PPO-optimized)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration agent that trains an ensemble of forward dynamics models and uses the variance (disagreement) across their predicted next-state embeddings as an intrinsic reward; the policy is optimized with PPO to maximize this intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Disagreement (ensemble, PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent consisting of (a) an ensemble of k=5 forward predictive models f_{θ_i} mapping state embedding φ(x_t) and action a_t to a predicted next-state embedding, trained with bootstrap (different initializations and resampled data), and (b) a policy π(x_t; θ_P) trained with PPO to maximize intrinsic reward equal to the variance across ensemble outputs in embedding space. Feature spaces vary by domain (random network features for games, ResNet-18 for robot).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active-learning inspired ensemble disagreement / model-variance maximization (Query-by-Committee / optimal experimental design analogy)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The ensemble is trained online on collected transitions; intrinsic reward r_i(t) = Var_{θ in ensemble}[ f_θ(φ(x_t), a_t) ] is computed without using x_{t+1}; the policy is adapted to select actions that maximize this disagreement (via PPO). Diversity in the ensemble is maintained via different initializations and bootstrap sampling; the agent uses disagreement to drive exploration, focusing on regions where models disagree.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple: Atari (standard), Atari with sticky actions, Unity 3D navigation (with/without stochastic TV), Noisy MNIST (toy), MuJoCo object manipulation (simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>High-dimensional image observations (embedding used), may include inherent or induced stochasticity (sticky actions, stochastic TV content, noisy transitions), sparse external rewards in some tasks (e.g., navigation), mixture of discrete (Atari, Unity) and continuous/structured action spaces (MuJoCo robot control discretized for policy).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by task: Atari games (discrete action spaces, standard RL episode lengths), Atari sticky actions (stochastic action execution), Unity navigation (3D maze, sparse +1 goal reward, stochastic TV channel), Noisy MNIST (one-step toy: 2 classes with differing transition stochasticity), MuJoCo manipulation (robotic 7-DOF simulated arm, large action discretization for pixel-wise actions), episode lengths unspecified in paper. Action-space example for robot manipulation: discretized 224 × 224 locations × 32 orientation/grasp modes = 224×224×32 output dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitatively matches or exceeds state-of-the-art curiosity / prediction-error baselines on near-deterministic Atari, outperforms prediction-error baselines in stochastic settings (Atari sticky actions and Unity with stochastic TV). In Noisy MNIST, disagreement converges to similar (near-zero) intrinsic reward for both low- and high-stochasticity states, avoiding pathological perpetual curiosity. Numeric robot-domain results (from same-paper experiments with disagreement policy): after training checkpoint at 700 interactions, the disagreement exploration policy interacts with unseen objects ≈ 67% of the time (evaluation metric); random policy 17%; REINFORCE-based curiosity policy 1% (see real-robot evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines (prediction-error curiosity, Burda et al. random-feature prediction error, Bayesian/Dropout disagreement, Pred-Error Variance ablation) generally perform worse in stochastic environments; e.g., prediction-error methods remain highly 'curious' about inherently stochastic transitions and can get stuck. Specific numbers: random baseline interaction rate 17% (robot test), REINFORCE-based curiosity 1% (robot test). For many Atari/Unity comparisons, only qualitative/curve outcomes are reported (disagreement >= baselines; outperforms in stochastic variants).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved sample efficiency observed vs REINFORCE-based curiosity in structured-manipulation tasks; differentiable variant (see separate entry) is explicitly noted to reach useful behavior in <1000 real-robot interactions and achieves strong interaction rates by ≈700 interactions; in MuJoCo the method learns object interaction an order of magnitude faster than REINFORCE (no absolute sample counts given in text).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure intrinsic-reward-driven exploration: policy maximizes disagreement-driven intrinsic reward (optionally combined with sparse extrinsic reward). Exploration is encouraged by selecting actions predicted to yield high ensemble variance; exploitation of extrinsic reward can be added by summing intrinsic + sparse extrinsic reward (used in Unity navigation). No explicit temperature schedule was used — the mechanism is the intrinsic reward shaping via model disagreement, and policy optimization is via PPO which handles tradeoff implicitly through reward maximization and discounting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Curiosity / prediction-error (Pathak et al., 2017), Random-feature prediction-error (Burda et al., 2019), Bayesian Disagreement (Dropout NN), Pred-Error Variance ablation, PPO-only optimization, REINFORCE-based curiosity policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Ensemble-output variance (disagreement) is an effective intrinsic reward that avoids pathological attraction to inherent stochasticity because ensembles converge to the predictive mean and thus reduce variance on stochastic transitions; 2) Disagreement performs as well or better than prior curiosity/prediction-error methods on near-deterministic benchmarks and outperforms them in stochastic environments (Unity with TV, Atari sticky actions); 3) Casting disagreement as a differentiable objective (see differentiable entry) enables more sample-efficient policy learning, permitting deployment on real robot manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Disagreement requires training predictive models in embedding space; the differentiable optimization is limited to short-horizon setups because multi-step differentiable forward prediction remains challenging; Bayesian dropout-based uncertainty (a baseline) underperformed ensemble disagreement in practice; Pred-Error Variance ablation performed significantly worse, demonstrating sensitivity to how uncertainty is measured. No long-horizon differentiable results are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Supervised Exploration via Disagreement', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1155.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1155.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differentiable-Disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable ensemble-disagreement with direct policy gradients</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the disagreement agent where the disagreement intrinsic objective is treated as a differentiable loss and the policy is optimized by backpropagating analytic gradients through the ensemble forward models (short-horizon), rather than using high-variance RL estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Disagreement [Differentiable]</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same ensemble forward predictors as Disagreement, but policy parameters θ_P are updated by maximizing (via direct gradient ascent / likelihood maximization) the ensemble-variance intrinsic objective r_i(t) = Varθ[f_θ(φ(x_t), a_t)], treating a_t = π(x_t; θ_P) as differentiable. For discrete actions, straight-through estimators are used; for continuous actions policies can be deterministic (epsilon-greedy) to permit backprop. Training alternates between updating forward models (ML updates) and updating policy via analytic gradients through the forward models.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active-learning inspired ensemble disagreement + direct differentiable optimization (use of model structure to adaptively select actions)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent mentally simulates next-state embeddings for candidate actions via the ensemble and directly optimizes policy parameters with gradients of the disagreement objective (no dependency on x_{t+1}), thereby adapting action selection to increase model variance; forward models are updated on collected transitions, and policy updates alternate with predictor training. For multi-step horizons, they'd need multi-step forward predictors; current experiments focus on short horizons or combine differentiable updates with RL for longer horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Enduro Atari (short-horizon steering), MuJoCo tabletop object manipulation (simulation), Real-world Sawyer robot manipulation (RGBD inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Short-horizon or locally-determined dynamics (Enduro steering, short-horizon object interactions), high-dimensional visual inputs (RGB/RGBD), large/structured action spaces for manipulation (pixel-wise actions with rotation/grasp modes), stochasticity present in other domains but differentiable variant evaluated primarily on short horizon structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Enduro: discrete steering controls, short-term dependencies; Robot manipulation: 7-DOF arm, end-effector pixel-wise action discretization 224×224×32 (location × orientation × push/grasp mode), high-dimensional inputs (224×224 RGBD), episode lengths not precisely specified; training/evaluation measured in number of interaction steps (hundreds to thousands).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Differentiable-disagreement speeds learning relative to PPO-only optimization of the same disagreement objective in Enduro (training curves show faster progress), in MuJoCo manipulation it achieves an order-of-magnitude faster learning of object interactions compared to REINFORCE-based optimization, and on the real robot it enables the agent to learn to interact with objects in fewer than 1000 examples; in held-out test, policy checkpoint after 700 interactions achieved ≈67% interaction rate on unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to PPO-optimized disagreement, differentiable optimization learns faster (Enduro curves). Compared to REINFORCE-based curiosity, differentiable disagreement yields much higher object interaction rates in robot tests (REINFORCE curiosity collapsed to ≈1% interaction). Random baseline interaction rate reported as ≈17% on real-robot held-out test.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High sample efficiency for short-horizon structured tasks: real-robot policies acquire useful interaction skills in <1000 interactions; meaningful checkpoint at 700 interactions yields high generalization (67% interaction rate on unseen objects). MuJoCo simulation shows order-of-magnitude faster improvement versus REINFORCE-based variants (paper does not give absolute sample counts for MuJoCo).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Optimization explicitly maximizes modeled disagreement via analytic gradients (favoring exploration of actions that current ensemble predicts differently); exploitation of extrinsic rewards can be combined (they combine differentiable updates with PPO for Enduro to capture longer-term extrinsic goals), but differentiable-only optimization focuses on exploration via the disagreement signal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>PPO-optimized disagreement (non-differentiable baseline), REINFORCE-based curiosity, prediction-error based curiosity (Pathak et al. 2017), Burda et al. random-feature predictor, Bayesian/Dropout uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Direct differentiable optimization of the disagreement objective substantially improves sample-efficiency in short-horizon, high-dimensional manipulation tasks, enabling real-robot self-supervised exploration (interactions) in under 1000 examples; combining differentiable short-horizon updates with RL for long-horizon dependencies (Enduro) further improves learning speed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Differentiable approach is limited to short-horizon problems because multi-step forward prediction models over long horizons are challenging to train from high-dimensional inputs; multi-step differentiable policy optimization is left for future work. Requires straight-through estimator for discrete actions which is an approximation and may limit stability in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Supervised Exploration via Disagreement', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1155.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1155.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noisy-MNIST experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Noisy MNIST one-step stochastic transition toy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled toy environment used to compare disagreement vs prediction-error intrinsic rewards under differing transition stochasticity: class-0 images transition consistently within class, class-1 images transition stochastically to random other classes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Disagreement vs Prediction-Error baselines (toy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>One-step environment where agent observes an MNIST image (class 0 or 1) and selects nothing (one-step); forward predictors trained to predict next image label distribution; intrinsic rewards differ by method: ensemble disagreement vs prediction-error.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Ensemble disagreement (active-learning inspired) compared to prediction-error curiosity</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Both agents collect transitions and update models; disagreement uses ensemble output variance for intrinsic reward, prediction-error uses model prediction error. Over time, disagreement converges as ensembles learn the mean behavior; prediction-error remains high for inherently stochastic transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Noisy MNIST (toy environment constructed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Single-step transitions; some states have deterministic transitions (class-0 -> class-0), others have high stochasticity (class-1 -> random classes 2–9); observations are low-dimensional images (MNIST digits).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Very low (toy) — one-step; two classes of starting states with different stochasticity levels.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Disagreement-based intrinsic reward converges to nearly equal (near-zero) intrinsic reward for both low- and high-stochasticity states after sufficient samples (correct behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Prediction-error based intrinsic reward continues to assign higher reward to inherently stochastic states (class-1), even after many samples — leading to pathological persistent curiosity and potential getting 'stuck'.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified in absolute sample counts in paper; plots show convergence behavior as number of visited states increases, with disagreement converging to ideal quickly relative to prediction-error which remains biased toward stochastic states.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>No explicit exploitation; objective is to measure intrinsic reward behavior under different stochasticity. Disagreement reduces incentive for stochastic transitions over time, balancing 'exploration of unknown' vs 'ignoring irreducible stochasticity'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Prediction-error curiosity (Pathak et al., 2017) — main baseline for this toy experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Disagreement avoids pathological perpetual curiosity in inherently stochastic transitions by converging ensembles to mean predictions (reducing variance), whereas prediction-error methods continue to reward stochastic states and can get stuck.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Toy environment — results may not directly transfer to long-horizon, complex dynamics without appropriate forward models and embeddings; no absolute sample counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Supervised Exploration via Disagreement', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1155.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1155.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unity Navigation (TV)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D Unity navigation task with optional stochastic TV</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>3D maze navigation (VizDoom-MyWayHome replica) where an agent must reach a fixed goal, with a variant that includes a TV whose content is stochastic and controllable by the agent (introducing high-entropy observation noise).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Disagreement (ensemble) combined with sparse extrinsic reward</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy trained to maximize sum of intrinsic (ensemble disagreement) and sparse extrinsic (+1 on reaching goal) rewards; ensemble predictors learned in embedding space (random features in video games).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Ensemble disagreement intrinsic reward (active-learning inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent adapts action selection to maximize ensemble disagreement; when combined with sparse extrinsic reward, policy balances exploration (disagreement) and goal-reaching (extrinsic) via reward summation and PPO optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Unity 3D navigation (VizDoom-MyWayHome replica) — two variants: without TV and with controllable stochastic TV</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>3D partially-observed navigation from first-person frames (high-dimensional visual inputs), stochastic observation source in TV variant (random images per channel change), sparse extrinsic reward for reaching fixed goal, discrete actions and random episode starts.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Moderate to high: 3D visual inputs, stochastic sensory source when TV present, sparse reward; episode length not explicitly specified in text.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Disagreement performs similarly to prediction-error baselines in the non-TV setup and outperforms prediction-error baseline when the stochastic TV is present (i.e., robust to stochastic observations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Prediction-error baseline shows degraded performance in presence of stochastic TV compared to disagreement; exact numeric scores not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not numerically specified; learning curves (Figure 4) indicate better learning progress with disagreement in stochastic variant.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic disagreement paired with sparse extrinsic reward; tradeoff governed by reward summation and PPO optimization rather than an explicit schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Burda et al. (random-feature prediction-error) and Pathak et al. (prediction-error curiosity); Bayesian dropout disagreement evaluated and found inferior to ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Ensemble disagreement is robust to high-entropy observation noise (stochastic TV) and helps the agent avoid being distracted by uncontrollable stochastic elements, outperforming prediction-error methods in that scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Quantitative metrics not provided in text; result reported qualitatively and via curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Supervised Exploration via Disagreement', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1155.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1155.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atari Sticky Actions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atari games with sticky actions (stochastic action execution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Atari benchmark modified to introduce stochasticity by making actions 'sticky' (with some probability the previously executed action repeats instead of the intended action), used to evaluate robustness of exploration methods to transition stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Disagreement (ensemble) evaluated on sticky-action Atari</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Ensemble of forward models in embedding space producing disagreement intrinsic reward; policy trained with PPO on intrinsic reward only (external rewards used only for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Ensemble disagreement (active-learning inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent uses ensemble disagreement to adaptively choose actions that induce high model variance; ensemble converges to predictive mean over stochastic transitions, reducing spurious intrinsic reward for inherently stochastic dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Atari games with sticky actions (Machado et al., 2017 evaluation protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>High-dimensional visual observations, discrete actions, induced transition stochasticity via sticky actions (stochasticity in action execution), diverse game dynamics across different titles (e.g., Pong, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High (standard ALE complexity): image observations, varied game-specific state spaces and dynamics, stochastic action effects, episode lengths typical to Atari benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Disagreement-based exploration outperforms prior state-of-the-art exploration methods in sticky-action Atari; example: in Pong disagreement starts slightly slower than Burda et al. but eventually attains a higher score. No absolute numeric scores provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Prediction-error and Bayesian dropout baselines perform worse in these stochastic setups; Bayesian dropout-based uncertainty did not match ensemble performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not specified numerically in paper for Atari sticky; learning curves indicate improved asymptotic performance and robustness to stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic-only training (no extrinsic reward) used for exploration experiments; tradeoff implicit via policy learning objective (PPO) maximizing intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Burda et al. (random-feature prediction-error), Pathak et al. (inverse-model prediction-error), Bayesian/Dropout disagreement, Pred-Error Variance ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Ensemble disagreement provides robustness to action-execution stochasticity and leads to superior exploration performance on stochastic Atari variants relative to prediction-error baselines and dropout-based uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No absolute numerical comparative scores reported in the text; per-game behaviors vary (e.g., slower start in Pong).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Supervised Exploration via Disagreement', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1155.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1155.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo manipulation (sim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo 7-DOF arm object manipulation simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulation of the 7-DOF robotic arm performing push and grasp actions in tabletop scenarios used to compare differentiable-disagreement vs RL-based optimization in a large-structured action space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Disagreement [Differentiable] (MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent maps RGBD observations to pixel-wise push/grasp action probabilities over a discretized 224×224×32 action space; intrinsic reward is ensemble disagreement in embedding space; policy optimized via differentiable gradients (alternating with predictor updates) in simulation comparison with Reinforce-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Ensemble disagreement with differentiable policy optimization</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Policy directly optimized to maximize ensemble variance for candidate actions using analytic gradients (backprop through forward models); forward models trained on collected transitions. This allows stepwise adaptation of actions to maximize predicted disagreement and therefore explore effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MuJoCo tabletop object manipulation (simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>High-dimensional RGBD observations, short-horizon object interactions, large/structured discretized action space (pixel-wise + rotations + mode), stochasticity not stated as primary challenge in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Large action space (224×224×32 outputs), perceptual inputs high-dimensional images, multiple objects possible; training measured by number of object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Differentiable-disagreement achieves object interaction learning roughly an order of magnitude faster than REINFORCE-based optimization according to paper's reported results (Figure 7a), indicating markedly increased sample efficiency in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>REINFORCE-based curiosity optimization is much slower; paper reports order-of-magnitude slower learning curves for REINFORCE relative to differentiable-disagreement (no absolute sample counts given).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Order-of-magnitude faster than REINFORCE in simulation for the object-interaction metric; absolute counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Differentiable optimization biases action selection toward those predicted to create high ensemble disagreement (exploration); exploitation not primary focus in these short-horizon experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>REINFORCE-based curiosity optimization, prediction-error baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Direct differentiable optimization of the disagreement objective substantially accelerates learning of object interaction behaviors in simulation compared to high-variance policy-gradient RL (REINFORCE).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Absolute sample numbers not provided; results demonstrated primarily in short-horizon, structured-action tasks where differentiable gradients are tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Supervised Exploration via Disagreement', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1155.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1155.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Real-Robot Sawyer manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Real-world Sawyer-arm RGBD object manipulation exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A real-world deployment of differentiable disagreement-driven exploration where a Sawyer 7-DOF arm with an overhead KinectV2 learns to interact with objects (push/grasp) from raw RGBD observations purely via intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Disagreement [Differentiable] (Real Sawyer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same policy/fwd-model architecture as MuJoCo simulation (pixel-wise action outputs over discretized 224×224×32 actions), forward models use ImageNet-pretrained ResNet-18 features for prediction space; policy optimized with differentiable disagreement objective alternating with predictor ML updates.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Ensemble disagreement with differentiable policy optimization (active-learning inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Ensemble forward models are trained on collected transitions from the robot; policy is updated via analytic gradients of the ensemble variance intrinsic reward, guiding the robot to attempt actions predicted to maximally disagree across models (leading to novel interactions). Evaluation uses detection of image change to identify object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Real robot tabletop with Sawyer arm and KinectV2 camera</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Real-world visual inputs (RGBD), multiple object types (30 total: 20 train, 10 test), few objects per run (3) to make exploration harder, stochastic real-world dynamics and varied object behaviors, short-horizon interactions measured by touch/interaction counts.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High: raw RGBD inputs, large discretized action space (224×224×32), real-world noise and diverse object properties; training and evaluation measured in interaction steps (hundreds).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Differentiable disagreement allows the robot to learn to interact with objects in less than 1000 examples; at checkpoint after 700 training interactions, policy achieves ≈67% interaction rate on held-out test objects (evaluated over 80 interaction steps per model), random policy baseline 17%, REINFORCE-based curiosity collapsed to ≈1%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random policy: ≈17% interaction rate on held-out objects; REINFORCE-based curiosity policy: ≈1% (collapsed) interactions; differentiable-disagreement: ≈67% (after 700 interactions checkpoint).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: achieves strong generalization to unseen objects by ≈700 interactions and learns object interaction in under 1000 real-world samples.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic-driven exploration only (no extrinsic reward used during training); adaptation steers policy towards actions maximizing model disagreement and thereby increases contact interactions; no explicit exploitation schedule reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>REINFORCE-based curiosity (prediction-error), random policy baseline; (also compared in paper to other curiosity/prediction-error methods in prior experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Differentiable-disagreement is sufficiently sample-efficient and robust to real-world noise to enable a Sawyer arm to learn object interaction skills from scratch in under 1000 interactions, generalizing to unseen objects (≈67% interaction rate) and substantially outperforming REINFORCE-based curiosity (≈1%) and random (≈17%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires careful choices of embedding space (used ImageNet ResNet-18 features here), limited to relatively short-horizon interactions; evaluation uses a simple image-change interaction metric (binary touch), not full manipulation success metrics; resetting and object replacement were manual in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Supervised Exploration via Disagreement', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curiosity-driven exploration by self-supervised prediction <em>(Rating: 2)</em></li>
                <li>Large-scale study of curiosity-driven learning <em>(Rating: 2)</em></li>
                <li>VIME: Variational information maximizing exploration <em>(Rating: 2)</em></li>
                <li>Model-Based Active Exploration <em>(Rating: 2)</em></li>
                <li>Dropout as a Bayesian approximation: Representing model uncertainty in deep learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1155",
    "paper_id": "paper-174799858",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Disagreement",
            "name_full": "Ensemble-disagreement intrinsic reward (PPO-optimized)",
            "brief_description": "An exploration agent that trains an ensemble of forward dynamics models and uses the variance (disagreement) across their predicted next-state embeddings as an intrinsic reward; the policy is optimized with PPO to maximize this intrinsic reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Disagreement (ensemble, PPO)",
            "agent_description": "An agent consisting of (a) an ensemble of k=5 forward predictive models f_{θ_i} mapping state embedding φ(x_t) and action a_t to a predicted next-state embedding, trained with bootstrap (different initializations and resampled data), and (b) a policy π(x_t; θ_P) trained with PPO to maximize intrinsic reward equal to the variance across ensemble outputs in embedding space. Feature spaces vary by domain (random network features for games, ResNet-18 for robot).",
            "adaptive_design_method": "Active-learning inspired ensemble disagreement / model-variance maximization (Query-by-Committee / optimal experimental design analogy)",
            "adaptation_strategy_description": "The ensemble is trained online on collected transitions; intrinsic reward r_i(t) = Var_{θ in ensemble}[ f_θ(φ(x_t), a_t) ] is computed without using x_{t+1}; the policy is adapted to select actions that maximize this disagreement (via PPO). Diversity in the ensemble is maintained via different initializations and bootstrap sampling; the agent uses disagreement to drive exploration, focusing on regions where models disagree.",
            "environment_name": "Multiple: Atari (standard), Atari with sticky actions, Unity 3D navigation (with/without stochastic TV), Noisy MNIST (toy), MuJoCo object manipulation (simulation)",
            "environment_characteristics": "High-dimensional image observations (embedding used), may include inherent or induced stochasticity (sticky actions, stochastic TV content, noisy transitions), sparse external rewards in some tasks (e.g., navigation), mixture of discrete (Atari, Unity) and continuous/structured action spaces (MuJoCo robot control discretized for policy).",
            "environment_complexity": "Varies by task: Atari games (discrete action spaces, standard RL episode lengths), Atari sticky actions (stochastic action execution), Unity navigation (3D maze, sparse +1 goal reward, stochastic TV channel), Noisy MNIST (one-step toy: 2 classes with differing transition stochasticity), MuJoCo manipulation (robotic 7-DOF simulated arm, large action discretization for pixel-wise actions), episode lengths unspecified in paper. Action-space example for robot manipulation: discretized 224 × 224 locations × 32 orientation/grasp modes = 224×224×32 output dimensionality.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitatively matches or exceeds state-of-the-art curiosity / prediction-error baselines on near-deterministic Atari, outperforms prediction-error baselines in stochastic settings (Atari sticky actions and Unity with stochastic TV). In Noisy MNIST, disagreement converges to similar (near-zero) intrinsic reward for both low- and high-stochasticity states, avoiding pathological perpetual curiosity. Numeric robot-domain results (from same-paper experiments with disagreement policy): after training checkpoint at 700 interactions, the disagreement exploration policy interacts with unseen objects ≈ 67% of the time (evaluation metric); random policy 17%; REINFORCE-based curiosity policy 1% (see real-robot evaluation).",
            "performance_without_adaptation": "Baselines (prediction-error curiosity, Burda et al. random-feature prediction error, Bayesian/Dropout disagreement, Pred-Error Variance ablation) generally perform worse in stochastic environments; e.g., prediction-error methods remain highly 'curious' about inherently stochastic transitions and can get stuck. Specific numbers: random baseline interaction rate 17% (robot test), REINFORCE-based curiosity 1% (robot test). For many Atari/Unity comparisons, only qualitative/curve outcomes are reported (disagreement &gt;= baselines; outperforms in stochastic variants).",
            "sample_efficiency": "Improved sample efficiency observed vs REINFORCE-based curiosity in structured-manipulation tasks; differentiable variant (see separate entry) is explicitly noted to reach useful behavior in &lt;1000 real-robot interactions and achieves strong interaction rates by ≈700 interactions; in MuJoCo the method learns object interaction an order of magnitude faster than REINFORCE (no absolute sample counts given in text).",
            "exploration_exploitation_tradeoff": "Pure intrinsic-reward-driven exploration: policy maximizes disagreement-driven intrinsic reward (optionally combined with sparse extrinsic reward). Exploration is encouraged by selecting actions predicted to yield high ensemble variance; exploitation of extrinsic reward can be added by summing intrinsic + sparse extrinsic reward (used in Unity navigation). No explicit temperature schedule was used — the mechanism is the intrinsic reward shaping via model disagreement, and policy optimization is via PPO which handles tradeoff implicitly through reward maximization and discounting.",
            "comparison_methods": "Curiosity / prediction-error (Pathak et al., 2017), Random-feature prediction-error (Burda et al., 2019), Bayesian Disagreement (Dropout NN), Pred-Error Variance ablation, PPO-only optimization, REINFORCE-based curiosity policies.",
            "key_results": "1) Ensemble-output variance (disagreement) is an effective intrinsic reward that avoids pathological attraction to inherent stochasticity because ensembles converge to the predictive mean and thus reduce variance on stochastic transitions; 2) Disagreement performs as well or better than prior curiosity/prediction-error methods on near-deterministic benchmarks and outperforms them in stochastic environments (Unity with TV, Atari sticky actions); 3) Casting disagreement as a differentiable objective (see differentiable entry) enables more sample-efficient policy learning, permitting deployment on real robot manipulation.",
            "limitations_or_failures": "Disagreement requires training predictive models in embedding space; the differentiable optimization is limited to short-horizon setups because multi-step differentiable forward prediction remains challenging; Bayesian dropout-based uncertainty (a baseline) underperformed ensemble disagreement in practice; Pred-Error Variance ablation performed significantly worse, demonstrating sensitivity to how uncertainty is measured. No long-horizon differentiable results are provided.",
            "uuid": "e1155.0",
            "source_info": {
                "paper_title": "Self-Supervised Exploration via Disagreement",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Differentiable-Disagreement",
            "name_full": "Differentiable ensemble-disagreement with direct policy gradients",
            "brief_description": "A variant of the disagreement agent where the disagreement intrinsic objective is treated as a differentiable loss and the policy is optimized by backpropagating analytic gradients through the ensemble forward models (short-horizon), rather than using high-variance RL estimators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Disagreement [Differentiable]",
            "agent_description": "Same ensemble forward predictors as Disagreement, but policy parameters θ_P are updated by maximizing (via direct gradient ascent / likelihood maximization) the ensemble-variance intrinsic objective r_i(t) = Varθ[f_θ(φ(x_t), a_t)], treating a_t = π(x_t; θ_P) as differentiable. For discrete actions, straight-through estimators are used; for continuous actions policies can be deterministic (epsilon-greedy) to permit backprop. Training alternates between updating forward models (ML updates) and updating policy via analytic gradients through the forward models.",
            "adaptive_design_method": "Active-learning inspired ensemble disagreement + direct differentiable optimization (use of model structure to adaptively select actions)",
            "adaptation_strategy_description": "Agent mentally simulates next-state embeddings for candidate actions via the ensemble and directly optimizes policy parameters with gradients of the disagreement objective (no dependency on x_{t+1}), thereby adapting action selection to increase model variance; forward models are updated on collected transitions, and policy updates alternate with predictor training. For multi-step horizons, they'd need multi-step forward predictors; current experiments focus on short horizons or combine differentiable updates with RL for longer horizons.",
            "environment_name": "Enduro Atari (short-horizon steering), MuJoCo tabletop object manipulation (simulation), Real-world Sawyer robot manipulation (RGBD inputs)",
            "environment_characteristics": "Short-horizon or locally-determined dynamics (Enduro steering, short-horizon object interactions), high-dimensional visual inputs (RGB/RGBD), large/structured action spaces for manipulation (pixel-wise actions with rotation/grasp modes), stochasticity present in other domains but differentiable variant evaluated primarily on short horizon structured tasks.",
            "environment_complexity": "Enduro: discrete steering controls, short-term dependencies; Robot manipulation: 7-DOF arm, end-effector pixel-wise action discretization 224×224×32 (location × orientation × push/grasp mode), high-dimensional inputs (224×224 RGBD), episode lengths not precisely specified; training/evaluation measured in number of interaction steps (hundreds to thousands).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Differentiable-disagreement speeds learning relative to PPO-only optimization of the same disagreement objective in Enduro (training curves show faster progress), in MuJoCo manipulation it achieves an order-of-magnitude faster learning of object interactions compared to REINFORCE-based optimization, and on the real robot it enables the agent to learn to interact with objects in fewer than 1000 examples; in held-out test, policy checkpoint after 700 interactions achieved ≈67% interaction rate on unseen objects.",
            "performance_without_adaptation": "Compared to PPO-optimized disagreement, differentiable optimization learns faster (Enduro curves). Compared to REINFORCE-based curiosity, differentiable disagreement yields much higher object interaction rates in robot tests (REINFORCE curiosity collapsed to ≈1% interaction). Random baseline interaction rate reported as ≈17% on real-robot held-out test.",
            "sample_efficiency": "High sample efficiency for short-horizon structured tasks: real-robot policies acquire useful interaction skills in &lt;1000 interactions; meaningful checkpoint at 700 interactions yields high generalization (67% interaction rate on unseen objects). MuJoCo simulation shows order-of-magnitude faster improvement versus REINFORCE-based variants (paper does not give absolute sample counts for MuJoCo).",
            "exploration_exploitation_tradeoff": "Optimization explicitly maximizes modeled disagreement via analytic gradients (favoring exploration of actions that current ensemble predicts differently); exploitation of extrinsic rewards can be combined (they combine differentiable updates with PPO for Enduro to capture longer-term extrinsic goals), but differentiable-only optimization focuses on exploration via the disagreement signal.",
            "comparison_methods": "PPO-optimized disagreement (non-differentiable baseline), REINFORCE-based curiosity, prediction-error based curiosity (Pathak et al. 2017), Burda et al. random-feature predictor, Bayesian/Dropout uncertainty.",
            "key_results": "Direct differentiable optimization of the disagreement objective substantially improves sample-efficiency in short-horizon, high-dimensional manipulation tasks, enabling real-robot self-supervised exploration (interactions) in under 1000 examples; combining differentiable short-horizon updates with RL for long-horizon dependencies (Enduro) further improves learning speed.",
            "limitations_or_failures": "Differentiable approach is limited to short-horizon problems because multi-step forward prediction models over long horizons are challenging to train from high-dimensional inputs; multi-step differentiable policy optimization is left for future work. Requires straight-through estimator for discrete actions which is an approximation and may limit stability in some settings.",
            "uuid": "e1155.1",
            "source_info": {
                "paper_title": "Self-Supervised Exploration via Disagreement",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Noisy-MNIST experiment",
            "name_full": "Noisy MNIST one-step stochastic transition toy",
            "brief_description": "A controlled toy environment used to compare disagreement vs prediction-error intrinsic rewards under differing transition stochasticity: class-0 images transition consistently within class, class-1 images transition stochastically to random other classes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Disagreement vs Prediction-Error baselines (toy)",
            "agent_description": "One-step environment where agent observes an MNIST image (class 0 or 1) and selects nothing (one-step); forward predictors trained to predict next image label distribution; intrinsic rewards differ by method: ensemble disagreement vs prediction-error.",
            "adaptive_design_method": "Ensemble disagreement (active-learning inspired) compared to prediction-error curiosity",
            "adaptation_strategy_description": "Both agents collect transitions and update models; disagreement uses ensemble output variance for intrinsic reward, prediction-error uses model prediction error. Over time, disagreement converges as ensembles learn the mean behavior; prediction-error remains high for inherently stochastic transitions.",
            "environment_name": "Noisy MNIST (toy environment constructed in this paper)",
            "environment_characteristics": "Single-step transitions; some states have deterministic transitions (class-0 -&gt; class-0), others have high stochasticity (class-1 -&gt; random classes 2–9); observations are low-dimensional images (MNIST digits).",
            "environment_complexity": "Very low (toy) — one-step; two classes of starting states with different stochasticity levels.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Disagreement-based intrinsic reward converges to nearly equal (near-zero) intrinsic reward for both low- and high-stochasticity states after sufficient samples (correct behavior).",
            "performance_without_adaptation": "Prediction-error based intrinsic reward continues to assign higher reward to inherently stochastic states (class-1), even after many samples — leading to pathological persistent curiosity and potential getting 'stuck'.",
            "sample_efficiency": "Not quantified in absolute sample counts in paper; plots show convergence behavior as number of visited states increases, with disagreement converging to ideal quickly relative to prediction-error which remains biased toward stochastic states.",
            "exploration_exploitation_tradeoff": "No explicit exploitation; objective is to measure intrinsic reward behavior under different stochasticity. Disagreement reduces incentive for stochastic transitions over time, balancing 'exploration of unknown' vs 'ignoring irreducible stochasticity'.",
            "comparison_methods": "Prediction-error curiosity (Pathak et al., 2017) — main baseline for this toy experiment.",
            "key_results": "Disagreement avoids pathological perpetual curiosity in inherently stochastic transitions by converging ensembles to mean predictions (reducing variance), whereas prediction-error methods continue to reward stochastic states and can get stuck.",
            "limitations_or_failures": "Toy environment — results may not directly transfer to long-horizon, complex dynamics without appropriate forward models and embeddings; no absolute sample counts reported.",
            "uuid": "e1155.2",
            "source_info": {
                "paper_title": "Self-Supervised Exploration via Disagreement",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Unity Navigation (TV)",
            "name_full": "3D Unity navigation task with optional stochastic TV",
            "brief_description": "3D maze navigation (VizDoom-MyWayHome replica) where an agent must reach a fixed goal, with a variant that includes a TV whose content is stochastic and controllable by the agent (introducing high-entropy observation noise).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Disagreement (ensemble) combined with sparse extrinsic reward",
            "agent_description": "Policy trained to maximize sum of intrinsic (ensemble disagreement) and sparse extrinsic (+1 on reaching goal) rewards; ensemble predictors learned in embedding space (random features in video games).",
            "adaptive_design_method": "Ensemble disagreement intrinsic reward (active-learning inspired)",
            "adaptation_strategy_description": "Agent adapts action selection to maximize ensemble disagreement; when combined with sparse extrinsic reward, policy balances exploration (disagreement) and goal-reaching (extrinsic) via reward summation and PPO optimization.",
            "environment_name": "Unity 3D navigation (VizDoom-MyWayHome replica) — two variants: without TV and with controllable stochastic TV",
            "environment_characteristics": "3D partially-observed navigation from first-person frames (high-dimensional visual inputs), stochastic observation source in TV variant (random images per channel change), sparse extrinsic reward for reaching fixed goal, discrete actions and random episode starts.",
            "environment_complexity": "Moderate to high: 3D visual inputs, stochastic sensory source when TV present, sparse reward; episode length not explicitly specified in text.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Disagreement performs similarly to prediction-error baselines in the non-TV setup and outperforms prediction-error baseline when the stochastic TV is present (i.e., robust to stochastic observations).",
            "performance_without_adaptation": "Prediction-error baseline shows degraded performance in presence of stochastic TV compared to disagreement; exact numeric scores not provided in text.",
            "sample_efficiency": "Not numerically specified; learning curves (Figure 4) indicate better learning progress with disagreement in stochastic variant.",
            "exploration_exploitation_tradeoff": "Intrinsic disagreement paired with sparse extrinsic reward; tradeoff governed by reward summation and PPO optimization rather than an explicit schedule.",
            "comparison_methods": "Burda et al. (random-feature prediction-error) and Pathak et al. (prediction-error curiosity); Bayesian dropout disagreement evaluated and found inferior to ensemble.",
            "key_results": "Ensemble disagreement is robust to high-entropy observation noise (stochastic TV) and helps the agent avoid being distracted by uncontrollable stochastic elements, outperforming prediction-error methods in that scenario.",
            "limitations_or_failures": "Quantitative metrics not provided in text; result reported qualitatively and via curves.",
            "uuid": "e1155.3",
            "source_info": {
                "paper_title": "Self-Supervised Exploration via Disagreement",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Atari Sticky Actions",
            "name_full": "Atari games with sticky actions (stochastic action execution)",
            "brief_description": "Atari benchmark modified to introduce stochasticity by making actions 'sticky' (with some probability the previously executed action repeats instead of the intended action), used to evaluate robustness of exploration methods to transition stochasticity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Disagreement (ensemble) evaluated on sticky-action Atari",
            "agent_description": "Ensemble of forward models in embedding space producing disagreement intrinsic reward; policy trained with PPO on intrinsic reward only (external rewards used only for evaluation).",
            "adaptive_design_method": "Ensemble disagreement (active-learning inspired)",
            "adaptation_strategy_description": "Agent uses ensemble disagreement to adaptively choose actions that induce high model variance; ensemble converges to predictive mean over stochastic transitions, reducing spurious intrinsic reward for inherently stochastic dynamics.",
            "environment_name": "Atari games with sticky actions (Machado et al., 2017 evaluation protocol)",
            "environment_characteristics": "High-dimensional visual observations, discrete actions, induced transition stochasticity via sticky actions (stochasticity in action execution), diverse game dynamics across different titles (e.g., Pong, etc.).",
            "environment_complexity": "High (standard ALE complexity): image observations, varied game-specific state spaces and dynamics, stochastic action effects, episode lengths typical to Atari benchmarks.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Disagreement-based exploration outperforms prior state-of-the-art exploration methods in sticky-action Atari; example: in Pong disagreement starts slightly slower than Burda et al. but eventually attains a higher score. No absolute numeric scores provided in text.",
            "performance_without_adaptation": "Prediction-error and Bayesian dropout baselines perform worse in these stochastic setups; Bayesian dropout-based uncertainty did not match ensemble performance.",
            "sample_efficiency": "Not specified numerically in paper for Atari sticky; learning curves indicate improved asymptotic performance and robustness to stochasticity.",
            "exploration_exploitation_tradeoff": "Intrinsic-only training (no extrinsic reward) used for exploration experiments; tradeoff implicit via policy learning objective (PPO) maximizing intrinsic reward.",
            "comparison_methods": "Burda et al. (random-feature prediction-error), Pathak et al. (inverse-model prediction-error), Bayesian/Dropout disagreement, Pred-Error Variance ablation.",
            "key_results": "Ensemble disagreement provides robustness to action-execution stochasticity and leads to superior exploration performance on stochastic Atari variants relative to prediction-error baselines and dropout-based uncertainty.",
            "limitations_or_failures": "No absolute numerical comparative scores reported in the text; per-game behaviors vary (e.g., slower start in Pong).",
            "uuid": "e1155.4",
            "source_info": {
                "paper_title": "Self-Supervised Exploration via Disagreement",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "MuJoCo manipulation (sim)",
            "name_full": "MuJoCo 7-DOF arm object manipulation simulation",
            "brief_description": "Simulation of the 7-DOF robotic arm performing push and grasp actions in tabletop scenarios used to compare differentiable-disagreement vs RL-based optimization in a large-structured action space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Disagreement [Differentiable] (MuJoCo)",
            "agent_description": "Agent maps RGBD observations to pixel-wise push/grasp action probabilities over a discretized 224×224×32 action space; intrinsic reward is ensemble disagreement in embedding space; policy optimized via differentiable gradients (alternating with predictor updates) in simulation comparison with Reinforce-based optimization.",
            "adaptive_design_method": "Ensemble disagreement with differentiable policy optimization",
            "adaptation_strategy_description": "Policy directly optimized to maximize ensemble variance for candidate actions using analytic gradients (backprop through forward models); forward models trained on collected transitions. This allows stepwise adaptation of actions to maximize predicted disagreement and therefore explore effectively.",
            "environment_name": "MuJoCo tabletop object manipulation (simulation)",
            "environment_characteristics": "High-dimensional RGBD observations, short-horizon object interactions, large/structured discretized action space (pixel-wise + rotations + mode), stochasticity not stated as primary challenge in simulation.",
            "environment_complexity": "Large action space (224×224×32 outputs), perceptual inputs high-dimensional images, multiple objects possible; training measured by number of object interactions.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Differentiable-disagreement achieves object interaction learning roughly an order of magnitude faster than REINFORCE-based optimization according to paper's reported results (Figure 7a), indicating markedly increased sample efficiency in simulation.",
            "performance_without_adaptation": "REINFORCE-based curiosity optimization is much slower; paper reports order-of-magnitude slower learning curves for REINFORCE relative to differentiable-disagreement (no absolute sample counts given).",
            "sample_efficiency": "Order-of-magnitude faster than REINFORCE in simulation for the object-interaction metric; absolute counts not provided.",
            "exploration_exploitation_tradeoff": "Differentiable optimization biases action selection toward those predicted to create high ensemble disagreement (exploration); exploitation not primary focus in these short-horizon experiments.",
            "comparison_methods": "REINFORCE-based curiosity optimization, prediction-error baselines.",
            "key_results": "Direct differentiable optimization of the disagreement objective substantially accelerates learning of object interaction behaviors in simulation compared to high-variance policy-gradient RL (REINFORCE).",
            "limitations_or_failures": "Absolute sample numbers not provided; results demonstrated primarily in short-horizon, structured-action tasks where differentiable gradients are tractable.",
            "uuid": "e1155.5",
            "source_info": {
                "paper_title": "Self-Supervised Exploration via Disagreement",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Real-Robot Sawyer manipulation",
            "name_full": "Real-world Sawyer-arm RGBD object manipulation exploration",
            "brief_description": "A real-world deployment of differentiable disagreement-driven exploration where a Sawyer 7-DOF arm with an overhead KinectV2 learns to interact with objects (push/grasp) from raw RGBD observations purely via intrinsic reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Disagreement [Differentiable] (Real Sawyer)",
            "agent_description": "Same policy/fwd-model architecture as MuJoCo simulation (pixel-wise action outputs over discretized 224×224×32 actions), forward models use ImageNet-pretrained ResNet-18 features for prediction space; policy optimized with differentiable disagreement objective alternating with predictor ML updates.",
            "adaptive_design_method": "Ensemble disagreement with differentiable policy optimization (active-learning inspired)",
            "adaptation_strategy_description": "Ensemble forward models are trained on collected transitions from the robot; policy is updated via analytic gradients of the ensemble variance intrinsic reward, guiding the robot to attempt actions predicted to maximally disagree across models (leading to novel interactions). Evaluation uses detection of image change to identify object interactions.",
            "environment_name": "Real robot tabletop with Sawyer arm and KinectV2 camera",
            "environment_characteristics": "Real-world visual inputs (RGBD), multiple object types (30 total: 20 train, 10 test), few objects per run (3) to make exploration harder, stochastic real-world dynamics and varied object behaviors, short-horizon interactions measured by touch/interaction counts.",
            "environment_complexity": "High: raw RGBD inputs, large discretized action space (224×224×32), real-world noise and diverse object properties; training and evaluation measured in interaction steps (hundreds).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Differentiable disagreement allows the robot to learn to interact with objects in less than 1000 examples; at checkpoint after 700 training interactions, policy achieves ≈67% interaction rate on held-out test objects (evaluated over 80 interaction steps per model), random policy baseline 17%, REINFORCE-based curiosity collapsed to ≈1%.",
            "performance_without_adaptation": "Random policy: ≈17% interaction rate on held-out objects; REINFORCE-based curiosity policy: ≈1% (collapsed) interactions; differentiable-disagreement: ≈67% (after 700 interactions checkpoint).",
            "sample_efficiency": "High: achieves strong generalization to unseen objects by ≈700 interactions and learns object interaction in under 1000 real-world samples.",
            "exploration_exploitation_tradeoff": "Intrinsic-driven exploration only (no extrinsic reward used during training); adaptation steers policy towards actions maximizing model disagreement and thereby increases contact interactions; no explicit exploitation schedule reported.",
            "comparison_methods": "REINFORCE-based curiosity (prediction-error), random policy baseline; (also compared in paper to other curiosity/prediction-error methods in prior experiments).",
            "key_results": "Differentiable-disagreement is sufficiently sample-efficient and robust to real-world noise to enable a Sawyer arm to learn object interaction skills from scratch in under 1000 interactions, generalizing to unseen objects (≈67% interaction rate) and substantially outperforming REINFORCE-based curiosity (≈1%) and random (≈17%).",
            "limitations_or_failures": "Requires careful choices of embedding space (used ImageNet ResNet-18 features here), limited to relatively short-horizon interactions; evaluation uses a simple image-change interaction metric (binary touch), not full manipulation success metrics; resetting and object replacement were manual in experiments.",
            "uuid": "e1155.6",
            "source_info": {
                "paper_title": "Self-Supervised Exploration via Disagreement",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curiosity-driven exploration by self-supervised prediction",
            "rating": 2,
            "sanitized_title": "curiositydriven_exploration_by_selfsupervised_prediction"
        },
        {
            "paper_title": "Large-scale study of curiosity-driven learning",
            "rating": 2,
            "sanitized_title": "largescale_study_of_curiositydriven_learning"
        },
        {
            "paper_title": "VIME: Variational information maximizing exploration",
            "rating": 2,
            "sanitized_title": "vime_variational_information_maximizing_exploration"
        },
        {
            "paper_title": "Model-Based Active Exploration",
            "rating": 2,
            "sanitized_title": "modelbased_active_exploration"
        },
        {
            "paper_title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
            "rating": 1,
            "sanitized_title": "dropout_as_a_bayesian_approximation_representing_model_uncertainty_in_deep_learning"
        }
    ],
    "cost": 0.020143,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Supervised Exploration via Disagreement</p>
<p>Deepak Pathak 
Dhiraj Gandhi 
Abhinav Gupta 
Self-Supervised Exploration via Disagreement</p>
<p>Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github. io/exploration-by-disagreement/.</p>
<p>Introduction</p>
<p>Exploration is a major bottleneck in both model-free and model-based approaches to sensorimotor learning. In modelbased learning, exploration is a critical component in collecting diverse data for training the model in the first place. On the other hand, exploration is indispensable in modelfree reinforcement learning (RL) when rewards extrinsic *  to the agent are sparse. The common approach to exploration has been to generate "intrinsic" rewards, i.e., rewards automatically computed based on the agents model of the environment. Existing formulations of intrinsic rewards include maximizing "visitation count" (Bellemare et al., 2016;Lopes et al., 2012;Poupart et al., 2006) of less-frequently visited states, "curiosity" (Oudeyer &amp; Kaplan, 2009;Pathak et al., 2017;Schmidhuber, 1991a) where prediction error is used as reward signal and "diversity rewards" (Eysenbach et al., 2018;Lehman &amp; Stanley, 2011a;b) which incentivize diversity in the visited states. These rewards provide continuous feedback to the agent when extrinsic rewards are sparse, or even absent altogether.</p>
<p>Generating intrinsic rewards requires building some form of a predictive model of the world. However, there is a key challenge in learning predictive models beyond noise-free simulated environments: how should the stochastic nature of agent-environment interaction be handled? Stochasticity could be caused by several sources: (1) noisy environment observations (e.g, TV playing noise), (2) noise in the execution of agent's action (e.g., slipping) (3) stochasticity as an output of the agent's action (e.g., agent flipping coin). One straightforward solution to learn a predictive forward model that is itself stochastic! Despite several methods to build stochastic models in low-dimensional state space (Chua et al., 2018;Houthooft et al., 2016), scaling it to high dimensional inputs (e.g., images) still remains challenging. An alternative is to build deterministic models but encode the input in a feature space that is invariant to stochasticity. Recent work proposed building such models in inverse model feature space (Pathak et al., 2017) which can handle stochastic observations but fail when the agent itself is the source of noise (e.g. TV with remote (Burda et al., 2019)).</p>
<p>Beyond handling stochasticity, a bigger issue in the current intrinsic reward formulations is that of sample efficiency. The agent performs an action and then computes the reward based on its own prediction and environment behavior. For instance, in curiosity (Oudeyer &amp; Kaplan, 2009;Pathak et al., 2017), the policy is rewarded if the prediction model and the observed environment disagree. From an exploration viewpoint, this seems like a good formulation, i.e, rewarding actions whose effects are poorly modeled. But this reward is a function of environment dynamics with respect to the performed action. Since the environment arXiv:1906.04161v1 [cs.</p>
<p>LG] 10 Jun 2019</p>
<p>Self-Supervised Exploration via Disagreement
! " # = 3,! &amp; "'( )*+ &amp; "'( &amp; " , " -( . &amp; "'( ( &amp; "'( − . &amp; "'( ( &amp; " , " -0 . &amp; "'( 0 &amp; "'( − . &amp; "'( 0 &amp; " , " -1 . &amp; "'( 1 &amp; "'( − . &amp; "'( 1 &amp; " 2 , "
Intrinsic Reward</p>
<p>Current Observation</p>
<p>Policy Network Action Next Observation Ensemble of Dynamics Models Figure 1. Self-Supervised Exploration via Disagreement: At time step t, the agent in the state xt interacts with the environment by taking action at sampled from the current policy π and ends up in the state xt+1. The ensemble of forward models {f1, f2, ..., fn} takes this current state xt and the executed action at as input to predict the next state estimates {x 1 t+1 ,x 2 t+2 , ...,x n t+1 }. The variance over the ensemble of network output is used as intrinsic reward r i t to train the policy π. In practice, we encode the state x into an embedding space φ(x) for all the prediction purposes. dynamics is unknown, it is treated as black-box and the policy's gradients have to be estimated using high-variance estimators like REINFORCE (Williams, 1992) which are extremely sample-inefficient in practice.</p>
<p>We address both the challenges by proposing an alternative formulation for exploration taking inspiration from active learning. The goal of active learning is to selectively pick samples to label such that the classifier is maximally improved. However, unlike current intrinsic motivation formulations where an agent is rewarded by comparing the prediction to the ground-truth, the importance of a sample is not computed by looking at the ground-truth label but rather by looking at the state of the classifier itself. For instance, a popular approach is to label the most uncertain samples by looking at the confidence of the classifier. However, since most of the high-capacity deep neural networks tend to overfit, confidence is not a good measure of uncertainty. Hence, taking an analogy from the Query-by-Committee algorithm (Seung et al., 1992), we propose a simple disagreement-based approach: we train an ensemble of forward dynamics models and incentivize the agent to explore the action space where there is maximum disagreement or variance among the predictions of models of this ensemble. Taking actions to maximize the modeldisagreement allows the agent to explore in a completely self-supervised manner without relying on any external rewards. We show that this approach does not get stuck in stochastic-dynamics scenarios because all the models in the ensemble converge to mean, eventually reducing the variance of the ensemble.</p>
<p>Furthermore, we show that our new objective is a differentiable function allowing us to perform policy optimization via direct likelihood maximization -much like supervised learning instead of reinforcement learning. This leads to a sample efficient exploration policy allowing us to deploy it in a real robotic object manipulation setup with 7-DOF Sawyer arm. We demonstrate the efficacy of our approach on a variety of standard environments including stochastic Atari games (Machado et al., 2017), MNIST, Mujoco, Unity (Juliani et al., 2018) and a real robot.</p>
<p>Exploration via Disagreement</p>
<p>Consider an agent interacting with the environment E. At time t, it receives the observation x t and then takes an action predicted by its policy, i.e., a t ∼ π(x t ; θ P ). Upon executing the action, it receives, in return, the next observation x t+1 which is 'generated' by the environment. Our goal is to build an agent that chooses its action in order to maximally explore the state space of the environment in an efficient manner. There are two main components to our agent: an intrinsic forward prediction model that captures the agent's current knowledge of the states explored so far, and policy to output actions. As our agent explores the environment, we learn the agent's forward prediction model to predict the consequences of its own actions. The prediction uncertainty of this model is used to incentivize the policy to visit states with maximum uncertainty.</p>
<p>Both measuring and maximizing model uncertainty are challenging to execute with high dimensional raw sensory input (e.g. images). More importantly, the agent should learn to deal with 'stochasticity' in its interaction with the environment caused by either noisy actuation of the agent's motors, or the observations could be inherently stochastic. A deterministic prediction model will always end up with a non-zero prediction error allowing the agent to get stuck in the local minima of exploration.</p>
<p>Similar behavior would occur if the task at hand is too difficult to learn. Consider a robotic arm manipulating a keybunch. Predicting the change in pose and position of each key in the keybunch is extremely difficult. Although the behavior is not inherently stochastic, our agent could easily get stuck in playing with the same keybunch and not try other actions or even other objects. Existing formulations of curiosity reward or novelty-seeking count-based methods would also suffer in such scenarios. Learning probabilistic predictive models to measure uncertainty (Houthooft et al., 2016), or measuring learnability by capturing the change in prediction error (Oudeyer &amp; Kaplan, 2009;Schmidhuber, 1991a) have been proposed as solutions, but have been demonstrated in low-dimensional state space inputs and are difficult to scale to high dimensional image inputs.</p>
<p>Disagreement as Intrinsic Reward</p>
<p>Instead of learning a single dynamics model, we propose an alternate exploration formulation based on ensemble of models as inspired by the classical active learning literature (Seung et al., 1992). The goal of active learning is to find the optimal training examples to label such that the accuracy is maximized at minimum labeling cost. While active learning minimizes optimal cost with an analytic policy, the goal of an exploration-driven agent is to learn a policy that allows it to best navigate the environment space. Although the two might look different at the surface, we argue that active learning objectives could inspire powerful intrinsic reward formulations. In this work, we leverage the idea of model-variance maximization to propose exploration formulation. Leveraging model variance to investigate a system is also a well-studied mechanism in optimal experimental design literature (Boyd &amp; Vandenberghe, 2004) in statistics.</p>
<p>As our agent interacts with the environment, it collects trajectory of the form {x t , a t , x t+1 }. After each rollout, the collected transitions are used to train an ensemble of forward prediction models {f θ1 , f θ2 . . . , f θ k } of the environment. Each of the model is trained to map a given tuple of current observation x t and the action a t to the resulting state x t+1 . These models are trained using straightforward maximum likelihood estimation that minimizes the prediction error, i.e, f (x t , a t ; θ) − x t+1 2 . To maintain the diversity across the individual models, we initialize each model's parameters differently and train each of them on a subset of data randomly sampled with replacement (bootstrap).</p>
<p>Each model in our ensemble is trained to predict the ground truth next state. Hence, the parts of the state space which have been well explored by the agent will have gathered enough data to train all models, resulting in an agreement between the models. Since the models are learned (and not tabular), this property should generalize to unseen but similar parts of the state-space. However, the areas which are novel and unexplored would still have high prediction error for all models as none of them are yet trained on such examples, resulting in disagreement on the next state prediction. Therefore, we use this disagreement as an intrinsic reward to guide the policy. Concretely, the intrinsic reward r i t is defined as the variance across the output of different models in the ensemble:
r i t E θ f (x t , a t ; θ) − E θ [f (x t , a t ; θ)] 2 2(1)
Note that the expression on the right does not depend on the next state x t+1 -a property which will exploit in Section 2.3 to propose efficient policy optimization.</p>
<p>Given the agent's rollout sequence and the intrinsic reward r i t at each timestep t, the policy is trained to maximize the sum of expected reward, i.e., max θ P E π(xt;θ P ) t γ t r i t discounted by a factor γ. Note that the agent is selfsupervised and does not need any extrinsic reward to explore. The agent policy and the forward model ensemble are jointly trained in an online manner on the data collected by the agent during exploration. This objective can be maximized by any policy optimization technique, e.g., we use proximal policy optimization (PPO) (Schulman et al., 2017) unless specified otherwise.</p>
<p>Exploration in Stochastic Environments</p>
<p>Consider a scenario where the next state x t+1 is stochastic with respect to the current state x t and action a t . The source of stochasticity could be noisy actuation, difficulty or inherent randomness. Given enough samples, a dynamic prediction model should learn to predict the mean of the stochastic samples. Hence, the variance of the outputs in ensemble will drop preventing the agent from getting stuck in stochastic local-minima of exploration. Note this is unlike prediction error based objectives (Pathak et al., 2017;Schmidhuber, 1991b) which will settle down to a mean value after large enough samples. Since, the mean is different from the individual ground-truth stochastic states, the prediction error remains high making the agent forever curious about the stochastic behavior. We empirically verify this intuition by comparing prediction-error to disagreement across several environments in Section 4.2.</p>
<p>Differentiable Exploration for Policy Optimization</p>
<p>One commonality between different exploration methods (Bellemare et al., 2016;Houthooft et al., 2016;Pathak et al., 2017), is that the prediction model is usually learned in a supervised manner and the agent's policy is trained using reinforcement learning either in on-policy or off-policy manner. Despite several formulations over the years, the policy optimization procedure to maximize these intrinsic rewards has more or less remained the same -i.e. -treating the intrinsic reward as a "black-box" even though it is generated by the agent itself.</p>
<p>Let's consider an example to understand the reason behind the status quo. Consider a robotic-arm agent trying to push multiple objects kept on the table in front of it by looking at the image from an overhead camera. Suppose the arm pushes an object such that it collides with another one on the table. The resulting image observation will be the outcome of complex real-world interaction, the actual dynamics of which is not known to the agent. Note that this resulting image observation is a function of the agent's action (i.e., push in this case). Most commonly, the intrinsic reward r i (x t , a t , x t+1 ) is function of the next state (which is a function of the agent's action), e.g., information gain (Houthooft et al., 2016), prediction error (Pathak et al., 2017) etc. This dependency on the unknown environment dynamics absolves the policy optimization of analytical reward gradients with respect to the action. Hence, the standard way is to optimize the policy to maximize the sequence of intrinsic rewards using reinforcement learning, and not make any use of the structure present in the design of r i t . We formulate our proposed intrinsic reward as a differentiable function so as to perform policy optimization using likelihood maximization -much like supervised learning instead of reinforcement. If possible, this would allow the agent to make use of the structure in r i t explicitly, i.e., the intrinsic reward from the model could very efficiently inform the agent to change its action space in the direction where forward prediction loss is high, instead of providing a scalar feedback as in case of reinforcement learning. Explicit reward (cost) functions are one of the key reasons for success stories in optimal-control based robotics (Deisenroth &amp; Rasmussen, 2011b; Gal et al., 2016), but they don't scale to high-dimensional state space such as images and rely on having access to a good model of the environment.</p>
<p>We first discuss the one step case and then provide the general setup. Note that our intrinsic reward formulation, shown in Equation (1), does not depend on the environment interaction at all, i.e., no dependency on x t+1 . It is purely a mental simulation of the ensemble of models based on the current state and the agent's prediction action. Hence, instead of maximizing the intrinsic reward in expectation via PPO (RL), we can optimize for policy parameters θ P using direct gradients by treating r i t as a differentiable loss function. The objective for a one-step reward horizon is:
min θ1,...,θ k (1/k) k i=1 f θi (x t , a t ) − x t+1 2 (2) max θ P (1/k) k i=1 f θi (x t , a t ) − (1/k) k j=1 f θj (x t , a t ) 2 2 s.t. a t = π(x t ; θ P )
This is optimized in an alternating fashion where the forward predictor is optimized keeping the policy parameters frozen and vice-versa. Note that both policy and forward models are trained via maximum likelihood in a supervised manner, and hence, efficient in practice.</p>
<p>Generalization to multi-step reward horizon To optimize policy for maximizing a discounted sum of sequence of future intrinsic rewards r i t in a differentiable manner, the forward model would have to make predictions spanning over multiple time-steps. The policy objective in Equation (2) can be generalized to the multi-step horizon setup by recursively applying the forward predictor, i.e., max θ P t r i t (x t , a t ) wherex t = f (x t−1 , a t−1 ; θ), a t = π(x t ; θ P ),x 0 = x 0 , and r i t (.) is defined in Equation (1). Alternatively, one could use LSTM to make forward model itself multi-step. However, training a long term multi-step prediction model is challenging and an active area of research. In this paper, we show differentiable exploration results for short horizon only and leave multi-step scenarios for future work.</p>
<p>Implementation Details and Baselines</p>
<p>Learning forward predictions in the feature space It has been shown that learning forward-dynamics predictor f θ in a feature space leads to better generalization in contrast to raw pixel-space predictions (Burda et al., 2019;Pathak et al., 2017). Our formulation is trivially extensible to any representation space φ because all the operations can be performed with φ(x t ) instead of x t . Hence, in all of our experiments, we train our forward prediction models in feature space. In particular, we use random feature space in all video games and navigation, classification features in MNIST and ImageNet-pretrained ResNet-18 features in real world robot experiments. We use 5 models in the ensemble.</p>
<p>Back-propagation through forward model To directly optimize the policy with respect to the loss function of the forward predictor, as discussed in Section 2.3, we need to backpropagate all the way through action sampling process from the policy. In case of continuous action space, one could achieve this via making policy deterministic, i.e. a t = π θ P with epsilon-greedy sampling (Lillicrap et al., 2016). For discrete action space, we found that straight-through estimator (Bengio et al., 2013) works well in practice.</p>
<p>Baseline Comparisons 'Disagreement' refers to our exploration formulation optimized using PPO (Schulman et al., 2017) as discussed in Section 2.1, unless mentioned otherwise. 'Disagreement [Differentiable]' refers to the direct policy optimization for our formulation as described in Section 2.3. 'Pathak et.al. [ICML 2017]' refers to the curiositydriven exploration formulation based on the prediction error of the learned forward dynamics model in inverse model action space (Pathak et al., 2017). 'Burda et.al  refers to the random feature-based prediction-error (Burda et al., 2019). 'Pred-Error Variance' is an alternative ablation where we train the agent to maximize the variance of the prediction error as opposed to the variance of model output itself. Finally, we also compare our performance to Bayesian Neural Networks for measuring variance. In particular, we compared to Dropout NN (Gal &amp; Ghahramani, 2015) represented as 'Bayesian Disagreement'.</p>
<p>Experiments</p>
<p>We evaluate our approach on several environments including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm. Our experiments comprise of three parts: a) verifying the performance on standard nonstochastic environments; b) comparison on environments with stochasticity in either transition dynamics or observation space; and c) validating the efficiency of differentiable policy optimization facilitated by our objective.</p>
<p>Sanity Check in Non-Stochastic Environments</p>
<p>We first verify whether our disagreement formulation is able to maintain the performance on the standard environment as compared to state of the art exploration techniques. Although the primary advantage of our approach is in handling stochasticity and improving efficiency via differentiable policy optimization, it should not come at the cost of performance in nearly-deterministic scenarios. We run this sanity check on standard Atari benchmark suite, as shown in Figure 2. These games are not completely deterministic and have some randomness as to where the agent is spawned upon game resets (Mnih et al., 2015). The agent is trained with only an intrinsic reward, without any external reward from the game environment. The external reward is only used as a proxy to evaluate the quality of exploration and not shown to the agent.</p>
<p>We train our ensemble of models for computing disagreement in the embedding space of a random network as discussed in Section 3. The performance is compared to curiosity formulation (Pathak et al., 2017), curiosity with random features (Burda et al., 2019), Bayesian network based uncertainty and variance of prediction error. As seen in the results, our method is as good as or slightly better than state-of-theart exploration methods in most of the scenarios. Overall, these experiments suggest that our exploration formulation which is only driven by disagreement between models output compares favorably to state of the art methods. Note that the variance of prediction error performs significantly worse. This is so because the low variance in prediction error of different models doesn't necessarily mean they will agree on the next state prediction. Hence, 'Pred-Error Variance' may sometimes incorrectly stop exploring even if output prediction across models is drastically different.</p>
<p>Exploration in Stochastic Environments</p>
<p>A) Noisy MNIST. We first build a toy task on MNIST to intuitively demonstrate the contrast between disagreementbased intrinsic reward and prediction error-based reward (Pathak et al., 2017) in stochastic setups. This is a one-step environment where the agent starts by randomly observing an MNIST image from either class 0 or class 1. The dynamics of the environment are defined as follows: 1) images with label 0 always transition to another image from class 0. 2) Images with label 1 transition to a randomly chosen image from class label 2 to 9. This ensures that a transition from images with label 0 has low stochasticity (i.e., transition to the same label). On the other hand, transitions from images with label 1 have high stochasticity. The ideal intrinsic reward function should give similar incentive (reward) to both the scenarios after the agent has observed a significant number of transitions.  Figure 3. Performance of disagreement across ensemble vs prediction error based reward function on Noisy MNIST environment. This environment has 2 sets of state with different level of stochasticity associated with them. The disagreement-based intrinsic reward converges to the ideal case of assigning the same reward value for both states. However, the prediction-error based reward function assigns a high reward to states with high stochasticity. Figure 3 shows the performance of these methods on the test set of MNIST as a function of the number of states visited by the agent. Even at convergence, the prediction error based model assigns more reward to the observations with higher stochasticity, i.e., images with label 1. This behavior is detrimental since the transition from states of images with label 1 cannot ever be perfectly modeled and hence the agent will get stuck forever. In contrast, our ensemble-based disagreement method converges to almost zero intrinsic reward in both the scenarios after the agent has seen enough samples, as desired.</p>
<p>B) 3D Navigation in Unity. The goal in this setup is to train the agent to reach a target location in the maze. The agent receives a sparse reward of +1 on reaching the goal. For all the methods, we train the policy of the agent to maximize the summation of intrinsic and sparse extrinsic reward. This particular environment is a replica of VizDoom-MyWayHome environment in unity ML-agent and was proposed in Burda et al. (2019). Interestingly, this environment has 2 variants, one of which has a TV on the wall. The agent can change the channel of the TV but the content is stochastic (random images appear after pressing button). The agent can start randomly anywhere in the maze in each episode, but the goal location is fixed. We compare our proposed method with state-of-the-art prediction error-based exploration (Burda et al., 2019). The results are shown in Figure 4. Our approach performs similar to the baseline in the non-TV setup and outperforms the baseline in the presence of the TV. This result demonstrates that an ensemble-based disagreement could be a viable alternative in realistic stochastic setups.</p>
<p>C) Atari with Sticky Actions. As discussed in Section 4.1, the usual Atari setup is nearly deterministic. Therefore, a recent study (Machado et al., 2017) proposed to introduce stochasticity in Atari games by making actions 'sticky', i.e., at each step, either the agent's intended action is executed or the previously executed action is repeated with equal probability. As shown in Figure 5, our disagreement-based exploration approach outperforms previous state-of-the-art approaches. In Pong, our approach starts slightly slower than Burda et.al. (Burda et al., 2019), but eventually achieves a higher score. Further note that the Bayesian network-based disagreement does not perform as well as ensemble-based disagreement. This suggests that perhaps dropout (Gal &amp; Ghahramani, 2015) isn't able to capture good uncertainty estimate in practice. These experiments along with the navigation experiment, demonstrate the potential of ensembles in the face of stochasticity.</p>
<p>Differentiable Exploration in Structured Envs</p>
<p>We now evaluate the differentiable exploration objective proposed in Section 2.3. As discussed earlier, the policy is optimized via direct analytic gradients from the exploration module. Therefore, the horizon of exploration depends directly on the horizon of the module. Since training longhorizon models from high dimensional inputs (images) is still an unsolved problem, we evaluate our proposed formulation on relatively short horizon scenarios. However, to compensate for the length of the horizon, we test on large action space setups for real-world robot manipulation task.</p>
<p>A) Enduro Video Game. In this game, the goal of the agent is to steer the car on racing track to avoid enemies. The agent is trained to explore via purely intrinsic rewards, and the extrinsic reward is only used for evaluation. In order to steer the car, the agent doesn't need to model long-range dependencies. Hence, in this environment, we combine our differentiable policy optimization with reinforcement learning (PPO) to maximize our disagreement based intrinsic reward. The RL captures discounted long term dependency while our differentiable formulation should efficiently take care of short-horizon dependencies. We compare this formulation to purely PPO based optimization of our intrinsic reward. As shown in Figure 6, our differentiable exploration expedites the learning of the agent suggesting the efficacy of direct gradient optimization. We now evaluate the performance of only differentiable exploration (without reinforcement) in short-horizon and large-structured action space setups.</p>
<p>B) Object Manipulation by Exploration.</p>
<p>We consider the task of object manipulation in complex scenarios. Our setup consists of a 7-DOF robotic arm that could be tasked to interact with the objects kept on the table in front of it. The objects are kept randomly in the workspace of the robot on the table. Robot's action space is end-effector position control: a) location (x, y) of point on the surface of table, b) angle of approach θ, and c) gripper status, a binary value indicating whether to grasp (open the gripper fingers) or push (keep fingers close). All of our experiments use raw visual RGBD images as input and predict actions as output. Note that, to accurately grasp or push objects, the agent needs to figure out an accurate combination of location, orientation and gripper status.</p>
<p>The action space is discretized into 224 × 224 locations, 16 orientations for grasping (fingers close) and 16 orientations for pushing leading to final dimension of 224 × 224 × 32. The policy takes as input a 224 × 224 RGBD image and produces push and grasp action probabilities for each pixel. Following (Zeng et al., 2018), instead of adding the 16 rotations in the output, we pass 16 equally spaced rotated images to the network and then sample actions based on the output of all the inputs. This exploits the convolutional structure of the network. The task has a short horizon but very large state and action spaces. We make no assumption about either the environment or the training signal. Our robotic agents explore the work-space purely out of their own intrinsic reward in a pursuit to develop useful skills. We have instantiated this setup in a Mujoco simulation as well as in the real world robotics scenarios.</p>
<p>B1) Object Manipulation in MuJoCo.</p>
<p>We first carry out a study in simulation to compare the performance of differentiable variant of our disagreement objective against the reinforcement learning based optimization. We used Mu-JoCo to simulate the robot performing grasping and pushing on tabletop environment as described above.</p>
<p>To evaluate the quality of exploration, we measure the frequency at which our agent interacts (i.e., touches) with the object. This measure is just used to evaluate the exploration quantitatively and is not used as a training signal. It represents how quickly our agent's policy learns to explore an interesting part of space. Figures 7a shows the performance when the environment consists of just a single object which makes it really difficult to touch the object randomly. Our approach is able to exploit the structure in the loss, resulting in order of magnitude faster learning than REINFORCE.</p>
<p>B2) Real-World Robotic Manipulation. We now deploy our sample-efficient exploration formulation on real-world robotics setup. The real-world poses additional challenges, unlike simulated environments in terms of behavior and the dynamics of varied object types. Our robotic setup consisted of a Sawyer-arm with a table placed in front of it. We mounted KinectV2 at a fixed location from the robot to receive RGBD observations of the environment.</p>
<p>In every run, the robot starts with 3 objects placed in front of it. Unlike other self-supervised robot learning setups, we keep fewer objects to make exploration problem harder so that it is not trivial to interact with the objects by acting randomly. If either the robot completes 100 interactions (b) Real Robot (c) Real Robot Setup Figure 7. Measuring object interaction rate with respect to the number of samples in (a) Mujoco, and (b) real-world robot. Note that the Mujoco plot is in log-scale. We measure the exploration quality by evaluating the object interaction frequency of the agent. In both the environments, our differentiable policy optimization explores more efficiently. (c) A snapshot of the real-robotic setup.</p>
<p>or there are no objects in front of it, objects are replaced manually. Out of a total of 30 objects, we created a set of 20 objects for training and 10 objects for testing. We use the same metric as used in the simulation above (i.e., number of object interactions) to measure the effectiveness of our exploration policy during training. We monitor the change in the RGBD image to see if the robot has interacted with objects. Figure 7b shows the effectiveness of differentiable policy optimization for disagreement over prediction-error based curiosity objective. Differentiable-disagreement allows the robotic agent to learn to interact with objects in less than 1000 examples.</p>
<p>We further test the skills learned by our robot during its exploration by measuring object-interaction frequency on a set of 10 held-out test objects. For both the methods, we use the checkpoint saved after 700 robot interaction with the environment. For each model, we evaluate a total of 80 robot interaction steps with three test objects kept in front. The environment is reset after every 10 robot steps during evaluation. Our final disagreement exploration policy interacts approximately 67% of times with unseen objects, whereas a random policy performs at 17%. On the other hand, it seems that REINFORCE-based curiosity policy just collapses and only 1% of actions involve interaction with objects. Videos are available at https://pathak22. github.io/exploration-by-disagreement/.</p>
<p>Related Work</p>
<p>Exploration is a well-studied problem in the field of reinforcement learning. Early approaches focused on studying exploration from theoretical perspective (Strehl &amp; Littman, 2008) and proposed Bayesian formulations (Deisenroth &amp; Rasmussen, 2011a;Kolter &amp; Ng, 2009) but they are usually hard to scale to higher dimensions (e.g., images). In this paper, we focus on the specific problem of exploration using intrinsic rewards. A large family of approaches use "curiosity" as an intrinsic reward for training the agents. A good summary of early work in curiosity-driven rewards can be found in (Oudeyer &amp; Kaplan, 2009;Oudeyer et al., 2007).</p>
<p>Most approaches use some form of prediction-error between the learned model and environment behavior (Pathak et al., 2017). This prediction error can also be formulated as surprise (Achiam &amp; Sastry, 2017;Schmidhuber, 1991a;Sun et al., 2011). Other techniques incentivize exploration of states and actions where prediction of a forward model is highly-uncertain (Houthooft et al., 2016;Still &amp; Precup, 2012). Finally, approaches such as Lopes et al. (2012) try to explore state space which help improve the prediction model. Please refer to the introduction Section 1 for details on formulations using curiosity, visitation count or diversity. However, most of these efforts study the problem in the context of external rewards.</p>
<p>Apart from intrinsic rewards, other approaches include using an adversarial game (Sukhbaatar et al., 2018) where one agent gives the goal states and hence guiding exploration. Gregor et al. (2017) introduce a formulation of empowerment where agent prefers to go to states where it expects it will achieve the most control after learning. Researchers have also tried using perturbation of learned policy for exploration (Fortunato et al., 2017;Fu et al., 2017;Plappert et al., 2017) and using value function estimates (Osband et al., 2016). Again these approaches have mostly been considered in the context of external rewards and are not efficient enough to be scalable to real robotics setup.</p>
<p>Our work is inspired by large-body of work in active learning (AL). In the AL setting, given a collection of unlabeled examples, a learner selects which samples will be labeled by an oracle (Settles, 2010). Common selection criteria include entropy (Dagan &amp; Engelson, 1995), uncertainty sampling (Lewis &amp; Gale, 1994) and expected informativeness (Houlsby et al., 2011). Our work is inspired by by (Seung et al., 1992), and we apply the disagreement idea in a completely different setting of exploration and show its applicability to environments with stochastic dynamics and improving sample-efficiency. Concurrent to this work, Shyam et al. (2019) also show the effectiveness of model-based exploration in estimating novelty, and Henaff et al. (2019) use variance regularization for policy learning via imitation.</p>
<p>. [ICLR 2019]'</p>
<p>Figure 2 .
2Sanity Check in Non-Stochastic Environments: We compare different intrinsic reward formulations across near-deterministic, non-stochastic standard benchmark of the Atari games. Our disagreement-based approach compares favorably to state-of-the-art approaches without losing accuracy in non-stochastic scenarios.</p>
<p>Figure 4 .
43D Navigation in Unity: Comparison of predictionerror based curiosity reward with our proposed disagreement-based exploration on 3D navigation task in Unity with and without the presence of TV+remote. While both the approaches perform similar in normal case (left), disagreement-based approach performs better in the presence of stochasticity (right).</p>
<p>Figure 5 .
5Stochastic Atari Games: Comparison of different exploration techniques in the the Atari ('sticky') environment. The disagreement-based exploration is robust across both the scenarios.</p>
<p>Figure 6 .
6Performance comparison of disagreement-based exploration with or without the differentiable policy optimization in Enduro Atari Game. Differentiability helps the agent learn faster.</p>
<p>Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).Equal contribution 
1 UC Berkelely 2 CMU 3 Facebook 
AI Research. 
Correspondence to: 
Deepak Pathak 
<a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#97;&#116;&#104;&#97;&#107;&#64;&#99;&#115;&#46;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;">&#112;&#97;&#116;&#104;&#97;&#107;&#64;&#99;&#115;&#46;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;</a>. </p>
<p>AcknowledgementsWe would like to thank Ben Recht, Leon Bottou, Harri Edwards, Yuri Burda, Ke Li, Saurabh Gupta, Shubham Tulsiani, and Yann Lecun for fruitful discussions and comments. Part of the work was performed when DP was interning at Facebook AI Research. DP is supported by the Facebook graduate fellowship.
Surprise-based intrinsic motivation for deep reinforcement learning. J Achiam, S Sastry, arXiv:1703.01732Achiam, J. and Sastry, S. Surprise-based intrinsic motivation for deep reinforcement learning. arXiv:1703.01732, 2017. 8</p>
<p>Unifying count-based exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, NIPS. 13Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Sax- ton, D., and Munos, R. Unifying count-based exploration and intrinsic motivation. In NIPS, 2016. 1, 3</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Y Bengio, N Léonard, A Courville, arXiv:1308.3432Bengio, Y., Léonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432, 2013. 4</p>
<p>Convex optimization. Cambridge university press. S Boyd, L Vandenberghe, Boyd, S. and Vandenberghe, L. Convex optimization. Cam- bridge university press, 2004. 3</p>
<p>Large-scale study of curiosity-driven learning. ICLR. Y Burda, H Edwards, D Pathak, A Storkey, T Darrell, A A Efros, 56Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros, A. A. Large-scale study of curiosity-driven learning. ICLR, 2019. 1, 4, 5, 6</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. K Chua, R Calandra, R Mcallister, S Levine, arXiv:1805.12114arXiv preprintChua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials us- ing probabilistic dynamics models. arXiv preprint arXiv:1805.12114, 2018. 1</p>
<p>Committee-based sampling for training probabilistic classifiers. I Dagan, S Engelson, ICML. 8Dagan, I. and Engelson, S. Committee-based sampling for training probabilistic classifiers. ICML, 1995. 8</p>
<p>A model-based and data-efficient approach to policy search. M Deisenroth, C Rasmussen, Pilco, ICML. 8Deisenroth, M. and Rasmussen, C. Pilco: A model-based and data-efficient approach to policy search. ICML, 2011a. 8</p>
<p>A model-based and data-efficient approach to policy search. M Deisenroth, C E Rasmussen, Pilco, ICML. Deisenroth, M. and Rasmussen, C. E. Pilco: A model-based and data-efficient approach to policy search. In ICML, 2011b. 4</p>
<p>B Eysenbach, A Gupta, J Ibarz, S Levine, arXiv:1802.06070Diversity is all you need: Learning skills without a reward function. Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills without a reward function. arXiv:1802.06070, 2018. 1</p>
<p>M Fortunato, M G Azar, B Piot, J Menick, I Osband, A Graves, V Mnih, R Munos, D Hassabis, O Pietquin, C Blundell, S Legg, arXiv:1706.10295Noisy networks for exploration. Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. Noisy networks for explo- ration. arXiv:1706.10295, 2017. 8</p>
<p>Exploration with exemplar models for deep reinforcement learning. J Fu, J D Co-Reyes, S Levine, Ex2, Fu, J., Co-Reyes, J. D., and Levine, S. Ex2: Exploration with exemplar models for deep reinforcement learning. NIPS, 2017. 8</p>
<p>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Y Gal, Z Ghahramani, arXiv:1506.0214256arXiv preprintGal, Y. and Ghahramani, Z. Dropout as a bayesian approxi- mation: Representing model uncertainty in deep learning. arXiv preprint arXiv:1506.02142, 2015. 5, 6</p>
<p>Improving pilco with bayesian neural network dynamics models. Y Gal, R Mcallister, C E Rasmussen, Data-Efficient Machine Learning workshop, ICML. Gal, Y., McAllister, R., and Rasmussen, C. E. Improving pilco with bayesian neural network dynamics models. In Data-Efficient Machine Learning workshop, ICML, 2016.</p>
<p>. K Gregor, D J Rezende, D Wierstra, Variational intrinsic control. ICLR Workshop. 8Gregor, K., Rezende, D. J., and Wierstra, D. Variational intrinsic control. ICLR Workshop, 2017. 8</p>
<p>Model-predictive policy learning with uncertainty regularization for driving in dense traffic. M Henaff, A Canziani, Y Lecun, ICLR. 8Henaff, M., Canziani, A., and LeCun, Y. Model-predictive policy learning with uncertainty regularization for driving in dense traffic. ICLR, 2019. 8</p>
<p>Bayesian active learning for classification and preference learning. arXiv. N Houlsby, F Huszr, Z Ghahramani, M Lengyel, Houlsby, N., Huszr, F., Ghahramani, Z., and Lengyel, M. Bayesian active learning for classification and preference learning. arXiv, 2011. 8</p>
<p>Vime: Variational information maximizing exploration. R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, Abbeel , P , NIPS. Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., and Abbeel, P. Vime: Variational information maxi- mizing exploration. In NIPS, 2016. 1, 3, 4, 8</p>
<p>Unity: A general platform for intelligent agents. A Juliani, V.-P Berges, E Vckay, Y Gao, H Henry, M Mattar, D Lange, arXiv:1809.02627Juliani, A., Berges, V.-P., Vckay, E., Gao, Y., Henry, H., Mattar, M., and Lange, D. Unity: A general platform for intelligent agents. arXiv:1809.02627, 2018. 2</p>
<p>Near-bayesian exploration in polynomial time. Z Kolter, A Ng, ICML. 8Kolter, Z. and Ng, A. Near-bayesian exploration in polyno- mial time. ICML, 2009. 8</p>
<p>Abandoning objectives: Evolution through the search for novelty alone. J Lehman, K O Stanley, Evolutionary computation. 1Lehman, J. and Stanley, K. O. Abandoning objectives: Evo- lution through the search for novelty alone. Evolutionary computation, 2011a. 1</p>
<p>Evolving a diversity of virtual creatures through novelty search and local competition. J Lehman, K O Stanley, Proceedings of the 13th annual conference on Genetic and evolutionary computation. the 13th annual conference on Genetic and evolutionary computationLehman, J. and Stanley, K. O. Evolving a diversity of virtual creatures through novelty search and local competition. In Proceedings of the 13th annual conference on Genetic and evolutionary computation, 2011b. 1</p>
<p>A sequential algorithm for training text classifiers. D Lewis, W Gale, ACM SIGIRLewis, D. and Gale, W. A sequential algorithm for training text classifiers. ACM SIGIR, 1994. 8</p>
<p>. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, Continuous control with deep reinforcement learning. ICLR. 4Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. ICLR, 2016. 4</p>
<p>Exploration in model-based reinforcement learning by empirically estimating learning progress. M Lopes, T Lang, M Toussaint, P.-Y Oudeyer, NIPS. 1Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y. Exploration in model-based reinforcement learning by empirically estimating learning progress. In NIPS, 2012. 1, 8</p>
<p>Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. M C Machado, M G Bellemare, E Talvitie, J Veness, M J Hausknecht, M Bowling, abs/1709.06009CoRR26Machado, M. C., Bellemare, M. G., Talvitie, E., Ve- ness, J., Hausknecht, M. J., and Bowling, M. Revis- iting the arcade learning environment: Evaluation pro- tocols and open problems for general agents. CoRR, abs/1709.06009, 2017. URL http://arxiv.org/ abs/1709.06009. 2, 6</p>
<p>Self-Supervised Exploration via Disagreement. Self-Supervised Exploration via Disagreement</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 5187540Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve- ness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier- stra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518(7540): 529-533, February 2015. 5</p>
<p>Deep exploration via bootstrapped dqn. I Osband, C Blundell, A Pritzel, B Van Roy, NIPS. Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped dqn. In NIPS, 2016. 8</p>
<p>What is intrinsic motivation? a typology of computational approaches. P.-Y Oudeyer, F Kaplan, Frontiers in neurorobotics. 18Oudeyer, P.-Y. and Kaplan, F. What is intrinsic motivation? a typology of computational approaches. Frontiers in neurorobotics, 2009. 1, 3, 8</p>
<p>Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, Evolutionary Computation. 8Oudeyer, P.-Y., Kaplan, F., and Hafner, V. V. Intrinsic motivation systems for autonomous mental development. Evolutionary Computation, 2007. 8</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, Darrell , T , ICML. Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised predic- tion. In ICML, 2017. 1, 3, 4, 5, 8</p>
<p>M Plappert, R Houthooft, P Dhariwal, S Sidor, R Y Chen, X Chen, T Asfour, P Abbeel, Andrychowicz , M , arXiv:1706.01905Parameter space noise for exploration. Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen, X., Asfour, T., Abbeel, P., and Andrychowicz, M. Parameter space noise for exploration. arXiv:1706.01905, 2017. 8</p>
<p>An analytic solution to discrete bayesian reinforcement learning. P Poupart, N Vlassis, J Hoey, Regan , K , ICML. Poupart, P., Vlassis, N., Hoey, J., and Regan, K. An analytic solution to discrete bayesian reinforcement learning. In ICML, 2006. 1</p>
<p>Curious model-building control systems. J Schmidhuber, IEEE International Joint Conference on. IEEENeural NetworksSchmidhuber, J. Curious model-building control systems. In Neural Networks, 1991. 1991 IEEE International Joint Conference on, pp. 1458-1463. IEEE, 1991a. 1, 3, 8</p>
<p>A possibility for implementing curiosity and boredom in model-building neural controllers. J Schmidhuber, From animals to animats: Proceedings of the first international conference on simulation of adaptive behavior. Schmidhuber, J. A possibility for implementing curiosity and boredom in model-building neural controllers. In From animals to animats: Proceedings of the first inter- national conference on simulation of adaptive behavior, 1991b. 3</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 34Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv:1707.06347, 2017. 3, 4</p>
<p>Active learning literature survey. B Settles, U Madison Tech ReportSettles, B. Active learning literature survey. U Madison Tech Report, 2010. 8</p>
<p>. H Seung, M Opper, H Sompolinsky, Query by committee. COLT, 1992. 2, 3, 8Seung, H., Opper, M., and Sompolinsky, H. Query by committee. COLT, 1992. 2, 3, 8</p>
<p>Model-Based Active Exploration. P Shyam, W Jaśkowski, F Gomez, ICML. Shyam, P., Jaśkowski, W., and Gomez, F. Model-Based Active Exploration. In ICML, 2019. 8</p>
<p>An information-theoretic approach to curiosity-driven reinforcement learning. S Still, D Precup, Theory in Biosciences. 8Still, S. and Precup, D. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 2012. 8</p>
<p>An analysis of model-based interval estimation for markov decision processes. A Strehl, M Littman, Journal of Computer and System Sciences. 8Strehl, A. and Littman, M. An analysis of model-based in- terval estimation for markov decision processes. Journal of Computer and System Sciences, 2008. 8</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. S Sukhbaatar, I Kostrikov, A Szlam, Fergus , R , In ICLR. 8Sukhbaatar, S., Kostrikov, I., Szlam, A., and Fergus, R. In- trinsic motivation and automatic curricula via asymmetric self-play. In ICLR, 2018. 8</p>
<p>Planning to be surprised: Optimal bayesian exploration in dynamic environments. Y Sun, F Gomez, J Schmidhuber, AGI. Sun, Y., Gomez, F., and Schmidhuber, J. Planning to be surprised: Optimal bayesian exploration in dynamic envi- ronments. In AGI, 2011. 8</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning. R J Williams, Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 1992. 2</p>
<p>Learning synergies between pushing and grasping with self-supervised deep reinforcement learning. A Zeng, S Song, S Welker, J Lee, A Rodriguez, T A Funkhouser, abs/1803.09956CoRRZeng, A., Song, S., Welker, S., Lee, J., Rodriguez, A., and Funkhouser, T. A. Learning synergies between pushing and grasping with self-supervised deep reinforcement learning. CoRR, abs/1803.09956, 2018. URL http: //arxiv.org/abs/1803.09956. 7</p>            </div>
        </div>

    </div>
</body>
</html>
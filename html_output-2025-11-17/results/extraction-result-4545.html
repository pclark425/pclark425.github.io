<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4545 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4545</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4545</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-fe1b172389e69c52dbf06158bdf947b47746480d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fe1b172389e69c52dbf06158bdf947b47746480d" target="_blank">Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A novel benchmark, \textit{Auto-Bench}, is introduced that encompasses necessary aspects to evaluate LLMs for scientific discovery in both natural and social sciences and challenges models to uncover hidden structures and make optimal decisions, which includes generating valid justifications.</p>
                <p><strong>Paper Abstract:</strong> Given the remarkable performance of Large Language Models (LLMs), an important question arises: Can LLMs conduct human-like scientific research and discover new knowledge, and act as an AI scientist? Scientific discovery is an iterative process that demands efficient knowledge updating and encoding. It involves understanding the environment, identifying new hypotheses, and reasoning about actions; however, no standardized benchmark specifically designed for scientific discovery exists for LLM agents. In response to these limitations, we introduce a novel benchmark, \textit{Auto-Bench}, that encompasses necessary aspects to evaluate LLMs for scientific discovery in both natural and social sciences. Our benchmark is based on the principles of causal graph discovery. It challenges models to uncover hidden structures and make optimal decisions, which includes generating valid justifications. By engaging interactively with an oracle, the models iteratively refine their understanding of underlying interactions, the chemistry and social interactions, through strategic interventions. We evaluate state-of-the-art LLMs, including GPT-4, Gemini, Qwen, Claude, and Llama, and observe a significant performance drop as the problem complexity increases, which suggests an important gap between machine and human intelligence that future development of LLMs need to take into consideration.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4545.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4545.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs (Autonomous Cycle)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task and evaluation framework that treats scientific discovery as iterative causal-graph discovery: LLMs propose interventions, receive observations from an Oracle, update hypothesis adjacency matrices, and repeat until the hypothesis (reachability) matches ground truth or a cycle limit is reached. Evaluates both directed (chemistry) and undirected (social network) settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>AutoBench (Autonomous Cycle causal discovery benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>AutoBench frames discovery as iterative hypothesis refinement: the model is given task description, previous interventions and observations (observation matrix), and an initial hypothesis adjacency matrix; it proposes the next intervention; an Oracle executes the intervention against a hidden ground-truth graph and returns a new observation; the model updates its hypothesis. Termination occurs when the model's hypothesis reachability equals the ground-truth reachability or when a predefined cycle limit is reached. Two domains are used (Chemistry: directed DAGs with discrete molecule states; Social Network: undirected graphs where interventions increment states of a node and its neighbours).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary outcome is structural correctness measured via reachability-equivalence between hypothesis and ground truth. Secondary criteria include success rate (fraction of rounds where model converges within cycle limit), average number of iterations to success (efficiency), and ability to select informative interventions. Long-term trajectory fidelity (temporal change tracing) is evaluated separately using OA-Acc and AT-Acc.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (simulated reactions) and Social Science (social networks)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Causal theories / causal graph discovery (directed and undirected graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to evaluate multiple state-of-the-art LLMs (Claude-3-5, Gemini-1.5-pro, Llama-3.1-70B, GPT-4o, Qwen2.5-72B). Key findings: strong models (GPT-4o, Qwen2.5) achieve high success rates on small graphs (e.g., Social Network with 3 and 5 persons: GPT-4o and Qwen2.5 reported 100% success), but performance drops as graph complexity (node count or state-space) increases (e.g., GPT-4o success falls to 30% for Social Network with 10 persons; in Chemistry with 10 nodes, many models obtain near 0% success). Average iterations are reported per successful round (table of values in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated — model outputs are programmatically compared to ground truth reachability and trajectory matrices; success/failure and numeric metrics are computed automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Robustness obtained via repeated independent trials (20 trials per chemistry setup, 10 per social network setup) and averaging; termination/cycle-limit rule (cycle limit set to twice the number of nodes) used to classify failure. No external human validation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Benchmark assumes causal relationships can be inferred from structured matrices and deterministic interventions, focuses on discrete states and predefined causal structures, excludes unstructured data and external knowledge retrieval, and uses adjacency/reachability reconstruction which can conflate multiple distinct adjacency matrices that exhibit identical reachability (coarse-graining). Temporal attention decay in LLMs limits long trajectory performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Synthetic, procedurally generated tasks with two settings: (1) Chemistry — directed DAGs, N nodes and discrete state values S; observation matrix G_ch ∈ {0..S}^{M×N}; (2) Social Network — undirected graphs (symmetric adjacency), interventions increment node and neighbor states; observation matrix G_so ∈ {0,1,2,...}^{M×N}. Configurations used include (N,S) = (3,3), (3,5), (10,5) for chemistry and N = 3,5,10 for social networks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4545.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4545.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reachability-Equivalence Metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reachability-based Adjacency Equivalence (reachability matrix comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structural evaluation metric that compares inferred and ground-truth graphs by comparing reachability matrices computed from adjacency matrices (sum of adjacency powers), declaring an inferred hypothesis correct if its reachability matches the ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reachability-based adjacency equivalence</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given a hypothesis adjacency matrix K and ground-truth H, compute reachability by summing powers of the adjacency (K^1 + K^2 + ... + K^M) and thresholding (>0) to obtain whether any directed path exists between node pairs. Compare the thresholded reachability matrices of hypothesis and ground truth element-wise; the hypothesis is treated as correct if reachability matrices match. This captures indirect causal influence rather than requiring exact edge-by-edge match.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Measures whether the hypothesized causal graph implies the same reachability (existence of directed paths) between node pairs as the ground truth; emphasizes explanatory/predictive equivalence of interventions rather than exact structural identity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General causal inference settings used in simulated Chemistry and Social Network tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Causal theories (connectivity/reachability-focused)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as the correctness criterion and termination condition in AutoBench. Because multiple adjacency matrices can share identical reachability, the metric can label multiple distinct hypotheses as correct; the paper uses this to accept equivalently-behaving graphs. No numeric 'score' beyond pass/fail of reachability equality is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (computational comparison of matrices).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Defined formally in methodology and applied directly to synthetic ground-truth graphs in experiments. Validation consisted of using this rule to determine success across repeated trials; no external human adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Coarseness: different adjacency matrices that are structurally distinct (different edges) may be equivalent under reachability, potentially over-accepting incorrect mechanistic hypotheses; reachability ignores edge weights/probabilities and may not capture directionality nuances in undirected settings or temporal dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to the synthetic adjacency matrices used in both Chemistry (directed DAGs) and Social Network (undirected but compared using reachability for the directed-graphs case).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4545.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4545.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OA-Acc / AT-Acc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Overall Accuracy (OA-Acc) and Average Trajectory Accuracy (AT-Acc)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two automated accuracy metrics for long-term trajectory-tracking tasks: OA-Acc measures exact match of predicted change matrices per round; AT-Acc measures per-transition (per-row-pair) average correctness across rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Overall Accuracy (OA-Acc) and Average Trajectory Accuracy (AT-Acc)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>OA-Acc: fraction of rounds (out of R) where the model's predicted color-change matrix Ŷ exactly equals ground truth Y (OA-Acc = (1/R) ∑ 1(Y_i = Ŷ_i)). AT-Acc: for each transition index j (between row j and j+1), compute proportion of rounds where that transition's vector is predicted correctly (T_{r,i} indicator), then average across R to get AT-Acc_j. OA-Acc captures strict round-level correctness; AT-Acc quantifies per-time-step fidelity across rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Temporal fidelity (ability to correctly mark where each node's state changes across time); both exact sequence reconstruction (OA-Acc) and per-transition robustness (AT-Acc) are measured.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General sequence/temporal reasoning; applied here to synthetic color-state trajectories relevant to causal-intervention observations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Predictive/temporal trajectory models and explanations about state changes</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Experimentally reported in Table 2: accuracy degrades strongly with trajectory length. Examples: With Chain-of-Thought (CoT), GPT-4o achieved OA-Acc = 99% at trajectory length 3, 100% at length 5, 91% at length 10, but 0% at length >=20; Qwen2.5 improved modestly with CoT on short sequences (e.g., 30% at length 3 with CoT versus 6% without). Claude-3-5 reached 69% (no CoT) at length 3 then drops to 0% beyond length 10. Overall, CoT helps for short sequences but does not prevent a sharp decline as trajectory length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: computed across R=100 synthetic rounds, comparing predicted and ground-truth matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Large-sample empirical evaluation (R=100 rounds per condition), comparison across multiple LLMs and prompt types (zero-shot vs CoT). No external human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Metrics reveal 'temporal attention decay' — models lose fidelity on long trajectories; OA-Acc is strict and punishes any per-round mismatch, while AT-Acc is more granular. CoT improves short-length performance but has limited effect for long sequences. Metrics rely on synthetic discrete-color trajectories which may not map directly to real-world scientific temporal data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4545.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4545.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cycle-limit Success & Avg-Iterations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cycle-limit Success Rate and Average Iterations to Convergence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An operational evaluation criterion measuring whether the LLM converges to a correct hypothesis within a fixed maximum number of autonomous cycles, and how many iterations are required when successful.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Cycle-limit Success Rate and Average Iterations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A trial is successful if the model's hypothesis matches ground-truth reachability within a preset cycle limit (paper sets cycle limit = 2 * number_of_nodes). Success rate is the proportion of successful trials. Average iterations is the mean number of cycles used among successful trials (rounds where model converged). If model fails within cycle limit it is counted as failure; failures sometimes reported as infinite iterations for display.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Effectiveness (success rate) and efficiency (average number of cycles to converge); also sensitivity to problem complexity (node count, state space).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied in synthetic Chemistry and Social Network causal-discovery experiments</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Iterative causal hypothesis generation and experiment-design</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported in Table 1. Example highlights: Social Network (3 persons): GPT-4o (avg iterations 2, success 100%), Qwen2.5 (2,100%); Social Network (10 persons): GPT-4o (8,30%), others drop to 0% success. Chemistry (3 nodes, S=3): GPT-4o (4,100%), Qwen2.5 (4,100%); Chemistry (10 nodes, S=5): GPT-4o (10,15%), many models fail (0% success). Average iterations grow with complexity; many failures lead to infinite values in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (programmatic trial management and aggregations).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Repeated trials (20 chemistry, 10 social per configuration), aggregate statistics reported. Cycle limit rationale and empirical behavior used to justify classification. No human adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Bias in average-iterations because computed only on successful rounds (doesn't reflect effort in failed rounds); cycle-limit choice (2×nodes) is arbitrary and may affect success classification; reporting 'infinite' for failures is non-numeric and requires interpretation; does not measure partial correctness of non-matching hypotheses beyond reachability equality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4545.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4545.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (reasoning-augmentation prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where the LLM is prompted to produce intermediate reasoning steps before producing the final answer; used here to augment LLM responses and compared to zero-shot structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models are prompted to include explicit reasoning steps (simple prompt used: 'Please also include the reason for your answer') before outputting the predicted matrices. The experiment compares model performance (OA-Acc and AT-Acc, and success rates) between zero-shot structured output and CoT-augmented responses to quantify the benefit of eliciting intermediate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in automated accuracy metrics (OA-Acc, AT-Acc) and success rates when CoT is present versus absent; also resilience to increasing trajectory length when CoT is used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General LLM reasoning tasks applied to synthetic trajectory and causal-discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Prompt-augmented inference/explanation generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>CoT substantially improves short-sequence performance for some models (e.g., GPT-4o OA-Acc from 41% to 99% at trajectory length 3; from 13% to 100% at length 5; for length 10 GPT-4o improves to 91% with CoT). However, CoT benefits diminish as trajectory length increases and often does not prevent eventual collapse at long lengths (e.g., many models at length ≥20 have 0% even with CoT). Some models (Claude, Gemini, Qwen) show smaller or inconsistent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: CoT vs zero-shot outputs are compared using OA-Acc/AT-Acc and success metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Within-paper ablation-style comparison: identical tasks and rounds run with and without CoT prompting, aggregated over R rounds; no external human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>CoT effects are model- and length-dependent; simple CoT prompt used (no sophisticated chain templates), and CoT may increase verbosity without improving long-range sequence tracking; CoT can help short-horizon reasoning but not long-term temporal attention decay.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient causal graph discovery using large language models <em>(Rating: 2)</em></li>
                <li>Can large language models build causal graphs? <em>(Rating: 2)</em></li>
                <li>Causal reasoning and large language models: Opening a new frontier for causality <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change <em>(Rating: 1)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
                <li>A survey on evaluation of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4545",
    "paper_id": "paper-fe1b172389e69c52dbf06158bdf947b47746480d",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "AutoBench",
            "name_full": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs (Autonomous Cycle)",
            "brief_description": "A task and evaluation framework that treats scientific discovery as iterative causal-graph discovery: LLMs propose interventions, receive observations from an Oracle, update hypothesis adjacency matrices, and repeat until the hypothesis (reachability) matches ground truth or a cycle limit is reached. Evaluates both directed (chemistry) and undirected (social network) settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "AutoBench (Autonomous Cycle causal discovery benchmark)",
            "evaluation_method_description": "AutoBench frames discovery as iterative hypothesis refinement: the model is given task description, previous interventions and observations (observation matrix), and an initial hypothesis adjacency matrix; it proposes the next intervention; an Oracle executes the intervention against a hidden ground-truth graph and returns a new observation; the model updates its hypothesis. Termination occurs when the model's hypothesis reachability equals the ground-truth reachability or when a predefined cycle limit is reached. Two domains are used (Chemistry: directed DAGs with discrete molecule states; Social Network: undirected graphs where interventions increment states of a node and its neighbours).",
            "evaluation_criteria": "Primary outcome is structural correctness measured via reachability-equivalence between hypothesis and ground truth. Secondary criteria include success rate (fraction of rounds where model converges within cycle limit), average number of iterations to success (efficiency), and ability to select informative interventions. Long-term trajectory fidelity (temporal change tracing) is evaluated separately using OA-Acc and AT-Acc.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Chemistry (simulated reactions) and Social Science (social networks)",
            "theory_type": "Causal theories / causal graph discovery (directed and undirected graphs)",
            "human_comparison": false,
            "evaluation_results": "Used to evaluate multiple state-of-the-art LLMs (Claude-3-5, Gemini-1.5-pro, Llama-3.1-70B, GPT-4o, Qwen2.5-72B). Key findings: strong models (GPT-4o, Qwen2.5) achieve high success rates on small graphs (e.g., Social Network with 3 and 5 persons: GPT-4o and Qwen2.5 reported 100% success), but performance drops as graph complexity (node count or state-space) increases (e.g., GPT-4o success falls to 30% for Social Network with 10 persons; in Chemistry with 10 nodes, many models obtain near 0% success). Average iterations are reported per successful round (table of values in paper).",
            "automated_vs_human_evaluation": "Automated — model outputs are programmatically compared to ground truth reachability and trajectory matrices; success/failure and numeric metrics are computed automatically.",
            "validation_method": "Robustness obtained via repeated independent trials (20 trials per chemistry setup, 10 per social network setup) and averaging; termination/cycle-limit rule (cycle limit set to twice the number of nodes) used to classify failure. No external human validation reported.",
            "limitations_challenges": "Benchmark assumes causal relationships can be inferred from structured matrices and deterministic interventions, focuses on discrete states and predefined causal structures, excludes unstructured data and external knowledge retrieval, and uses adjacency/reachability reconstruction which can conflate multiple distinct adjacency matrices that exhibit identical reachability (coarse-graining). Temporal attention decay in LLMs limits long trajectory performance.",
            "benchmark_dataset": "Synthetic, procedurally generated tasks with two settings: (1) Chemistry — directed DAGs, N nodes and discrete state values S; observation matrix G_ch ∈ {0..S}^{M×N}; (2) Social Network — undirected graphs (symmetric adjacency), interventions increment node and neighbor states; observation matrix G_so ∈ {0,1,2,...}^{M×N}. Configurations used include (N,S) = (3,3), (3,5), (10,5) for chemistry and N = 3,5,10 for social networks.",
            "uuid": "e4545.0",
            "source_info": {
                "paper_title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Reachability-Equivalence Metric",
            "name_full": "Reachability-based Adjacency Equivalence (reachability matrix comparison)",
            "brief_description": "A structural evaluation metric that compares inferred and ground-truth graphs by comparing reachability matrices computed from adjacency matrices (sum of adjacency powers), declaring an inferred hypothesis correct if its reachability matches the ground truth.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Reachability-based adjacency equivalence",
            "evaluation_method_description": "Given a hypothesis adjacency matrix K and ground-truth H, compute reachability by summing powers of the adjacency (K^1 + K^2 + ... + K^M) and thresholding (&gt;0) to obtain whether any directed path exists between node pairs. Compare the thresholded reachability matrices of hypothesis and ground truth element-wise; the hypothesis is treated as correct if reachability matrices match. This captures indirect causal influence rather than requiring exact edge-by-edge match.",
            "evaluation_criteria": "Measures whether the hypothesized causal graph implies the same reachability (existence of directed paths) between node pairs as the ground truth; emphasizes explanatory/predictive equivalence of interventions rather than exact structural identity.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General causal inference settings used in simulated Chemistry and Social Network tasks",
            "theory_type": "Causal theories (connectivity/reachability-focused)",
            "human_comparison": false,
            "evaluation_results": "Used as the correctness criterion and termination condition in AutoBench. Because multiple adjacency matrices can share identical reachability, the metric can label multiple distinct hypotheses as correct; the paper uses this to accept equivalently-behaving graphs. No numeric 'score' beyond pass/fail of reachability equality is reported.",
            "automated_vs_human_evaluation": "Automated (computational comparison of matrices).",
            "validation_method": "Defined formally in methodology and applied directly to synthetic ground-truth graphs in experiments. Validation consisted of using this rule to determine success across repeated trials; no external human adjudication.",
            "limitations_challenges": "Coarseness: different adjacency matrices that are structurally distinct (different edges) may be equivalent under reachability, potentially over-accepting incorrect mechanistic hypotheses; reachability ignores edge weights/probabilities and may not capture directionality nuances in undirected settings or temporal dynamics.",
            "benchmark_dataset": "Applied to the synthetic adjacency matrices used in both Chemistry (directed DAGs) and Social Network (undirected but compared using reachability for the directed-graphs case).",
            "uuid": "e4545.1",
            "source_info": {
                "paper_title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "OA-Acc / AT-Acc",
            "name_full": "Overall Accuracy (OA-Acc) and Average Trajectory Accuracy (AT-Acc)",
            "brief_description": "Two automated accuracy metrics for long-term trajectory-tracking tasks: OA-Acc measures exact match of predicted change matrices per round; AT-Acc measures per-transition (per-row-pair) average correctness across rounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Overall Accuracy (OA-Acc) and Average Trajectory Accuracy (AT-Acc)",
            "evaluation_method_description": "OA-Acc: fraction of rounds (out of R) where the model's predicted color-change matrix Ŷ exactly equals ground truth Y (OA-Acc = (1/R) ∑ 1(Y_i = Ŷ_i)). AT-Acc: for each transition index j (between row j and j+1), compute proportion of rounds where that transition's vector is predicted correctly (T_{r,i} indicator), then average across R to get AT-Acc_j. OA-Acc captures strict round-level correctness; AT-Acc quantifies per-time-step fidelity across rounds.",
            "evaluation_criteria": "Temporal fidelity (ability to correctly mark where each node's state changes across time); both exact sequence reconstruction (OA-Acc) and per-transition robustness (AT-Acc) are measured.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General sequence/temporal reasoning; applied here to synthetic color-state trajectories relevant to causal-intervention observations",
            "theory_type": "Predictive/temporal trajectory models and explanations about state changes",
            "human_comparison": false,
            "evaluation_results": "Experimentally reported in Table 2: accuracy degrades strongly with trajectory length. Examples: With Chain-of-Thought (CoT), GPT-4o achieved OA-Acc = 99% at trajectory length 3, 100% at length 5, 91% at length 10, but 0% at length &gt;=20; Qwen2.5 improved modestly with CoT on short sequences (e.g., 30% at length 3 with CoT versus 6% without). Claude-3-5 reached 69% (no CoT) at length 3 then drops to 0% beyond length 10. Overall, CoT helps for short sequences but does not prevent a sharp decline as trajectory length increases.",
            "automated_vs_human_evaluation": "Automated: computed across R=100 synthetic rounds, comparing predicted and ground-truth matrices.",
            "validation_method": "Large-sample empirical evaluation (R=100 rounds per condition), comparison across multiple LLMs and prompt types (zero-shot vs CoT). No external human validation.",
            "limitations_challenges": "Metrics reveal 'temporal attention decay' — models lose fidelity on long trajectories; OA-Acc is strict and punishes any per-round mismatch, while AT-Acc is more granular. CoT improves short-length performance but has limited effect for long sequences. Metrics rely on synthetic discrete-color trajectories which may not map directly to real-world scientific temporal data.",
            "uuid": "e4545.2",
            "source_info": {
                "paper_title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Cycle-limit Success & Avg-Iterations",
            "name_full": "Cycle-limit Success Rate and Average Iterations to Convergence",
            "brief_description": "An operational evaluation criterion measuring whether the LLM converges to a correct hypothesis within a fixed maximum number of autonomous cycles, and how many iterations are required when successful.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Cycle-limit Success Rate and Average Iterations",
            "evaluation_method_description": "A trial is successful if the model's hypothesis matches ground-truth reachability within a preset cycle limit (paper sets cycle limit = 2 * number_of_nodes). Success rate is the proportion of successful trials. Average iterations is the mean number of cycles used among successful trials (rounds where model converged). If model fails within cycle limit it is counted as failure; failures sometimes reported as infinite iterations for display.",
            "evaluation_criteria": "Effectiveness (success rate) and efficiency (average number of cycles to converge); also sensitivity to problem complexity (node count, state space).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Applied in synthetic Chemistry and Social Network causal-discovery experiments",
            "theory_type": "Iterative causal hypothesis generation and experiment-design",
            "human_comparison": false,
            "evaluation_results": "Reported in Table 1. Example highlights: Social Network (3 persons): GPT-4o (avg iterations 2, success 100%), Qwen2.5 (2,100%); Social Network (10 persons): GPT-4o (8,30%), others drop to 0% success. Chemistry (3 nodes, S=3): GPT-4o (4,100%), Qwen2.5 (4,100%); Chemistry (10 nodes, S=5): GPT-4o (10,15%), many models fail (0% success). Average iterations grow with complexity; many failures lead to infinite values in the table.",
            "automated_vs_human_evaluation": "Automated (programmatic trial management and aggregations).",
            "validation_method": "Repeated trials (20 chemistry, 10 social per configuration), aggregate statistics reported. Cycle limit rationale and empirical behavior used to justify classification. No human adjudication.",
            "limitations_challenges": "Bias in average-iterations because computed only on successful rounds (doesn't reflect effort in failed rounds); cycle-limit choice (2×nodes) is arbitrary and may affect success classification; reporting 'infinite' for failures is non-numeric and requires interpretation; does not measure partial correctness of non-matching hypotheses beyond reachability equality.",
            "uuid": "e4545.3",
            "source_info": {
                "paper_title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (reasoning-augmentation prompting)",
            "brief_description": "A prompting technique where the LLM is prompted to produce intermediate reasoning steps before producing the final answer; used here to augment LLM responses and compared to zero-shot structured outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Chain-of-Thought (CoT) prompting augmentation",
            "evaluation_method_description": "Models are prompted to include explicit reasoning steps (simple prompt used: 'Please also include the reason for your answer') before outputting the predicted matrices. The experiment compares model performance (OA-Acc and AT-Acc, and success rates) between zero-shot structured output and CoT-augmented responses to quantify the benefit of eliciting intermediate reasoning.",
            "evaluation_criteria": "Improvement in automated accuracy metrics (OA-Acc, AT-Acc) and success rates when CoT is present versus absent; also resilience to increasing trajectory length when CoT is used.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General LLM reasoning tasks applied to synthetic trajectory and causal-discovery tasks",
            "theory_type": "Prompt-augmented inference/explanation generation",
            "human_comparison": false,
            "evaluation_results": "CoT substantially improves short-sequence performance for some models (e.g., GPT-4o OA-Acc from 41% to 99% at trajectory length 3; from 13% to 100% at length 5; for length 10 GPT-4o improves to 91% with CoT). However, CoT benefits diminish as trajectory length increases and often does not prevent eventual collapse at long lengths (e.g., many models at length ≥20 have 0% even with CoT). Some models (Claude, Gemini, Qwen) show smaller or inconsistent improvements.",
            "automated_vs_human_evaluation": "Automated: CoT vs zero-shot outputs are compared using OA-Acc/AT-Acc and success metrics.",
            "validation_method": "Within-paper ablation-style comparison: identical tasks and rounds run with and without CoT prompting, aggregated over R rounds; no external human validation.",
            "limitations_challenges": "CoT effects are model- and length-dependent; simple CoT prompt used (no sophisticated chain templates), and CoT may increase verbosity without improving long-range sequence tracking; CoT can help short-horizon reasoning but not long-term temporal attention decay.",
            "uuid": "e4545.4",
            "source_info": {
                "paper_title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient causal graph discovery using large language models",
            "rating": 2
        },
        {
            "paper_title": "Can large language models build causal graphs?",
            "rating": 2
        },
        {
            "paper_title": "Causal reasoning and large language models: Opening a new frontier for causality",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
            "rating": 1
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 1
        },
        {
            "paper_title": "A survey on evaluation of large language models",
            "rating": 1
        }
    ],
    "cost": 0.01519825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs</h1>
<p>Tingting Chen ${ }^{1 <em>}$, Srinivas Anumasa ${ }^{1 </em>}$, Beibei Lin ${ }^{1}$, Vedant Shah ${ }^{2}$ Anirudh Goyal ${ }^{3}$, Dianbo Liu ${ }^{1}$,<br>${ }^{1}$ National University of Singapore ${ }^{2}$ Mila-Quebec AI institute ${ }^{3}$ Meta (Facebook) tingting.c@u.nus.edu, srinu_pd@nus.edu.sg</p>
<h4>Abstract</h4>
<p>Given the remarkable performance of Large Language Models (LLMs), an important question arises: Can LLMs conduct human-like scientific research and discover new knowledge, and act as an AI scientist? Scientific discovery is an iterative process that demands efficient knowledge updating and encoding. It involves understanding the environment, identifying new hypotheses, and reasoning about actions; however, no standardized benchmark specifically designed for scientific discovery exists for LLM agents. In response to these limitations, we introduce a novel benchmark, AutoBench, that encompasses necessary aspects to evaluate LLMs for scientific discovery in both natural and social sciences. Our benchmark is based on the principles of causal graph discovery. It challenges models to uncover hidden structures and make optimal decisions, which includes generating valid justifications. By engaging interactively with an oracle, the models iteratively refine their understanding of underlying interactions, the chemistry and social interactions, through strategic interventions. We evaluate state-of-the-art LLMs, including GPT-4, Gemini, Qwen, Claude, and Llama, and observe a significant performance drop as the problem complexity increases, which suggests an important gap between machine and human intelligence that future development of LLMs need to take into consideration.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) built on transformer architectures have revolutionized natural language processing by surpassing existing models on numerous benchmarks (Narayan et al., 2018; Rajpurkar, 2016; Wang, 2018). Modern LLMs—such as Claude-3-5 (Anthropic, 2024), GPT-4o (OpenAI, 2024), Gemini (DeepMind, 2024), Llama-3.1 (AI,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2024), and Qwen2.5 (Bai et al., 2024)—are trained on trillions of tokens, enabling them to achieve state-of-the-art performance across a diverse range of natural language processing tasks.</p>
<p>LLM models encapsulate vast amounts of world knowledge acquired through extensive training, much like a human who has conducted in-depth research and amassed a broad understanding of the world-knowledge that can drive new discoveries. This raises an obvious question: Can LLM models conduct scientific research and generate novel findings (Lu et al., 2024)? So far, these models have been shown to accelerate various aspects of the research process, such as manuscript writing and coding (Altmäe et al., 2023; Dinu et al., 2024). However, scientific discovery involves much more than refining text or code. It requires models to understand existing knowledge, augment it with new information, make optimal decisions, and, most importantly, employ reasoning skills to explain the roadmap that leads to those decisions.</p>
<p>There is a growing interest in using LLM models for scientific discovery across various domains, such as material discovery (Merchant et al., 2023; Pyzer-Knapp et al., 2022) and synthetic biology (Jumper et al., 2021; Hayes et al., 2025). Although state-of-the-art LLM models have demonstrated promising capabilities for targeted scientific progress (Lu et al., 2024), they are not yet ready for fully autonomous decision-making (Hager et al., 2024). Before entrusting these models with critical applications that require full autonomy, it is essential to identify their strengths and weaknesses in the context of scientific discovery. This necessitates the design of benchmarks which evaluates the models for their ability to perform scientific discoveries.</p>
<p>Numerous benchmarks (Miao et al., 2021; Cobbe et al., 2021; Patel et al., 2021; KoncelKedziorski et al., 2016), related to numerical problem-solving ability have been developed to as-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The framework of our Autonomous Cycle. (A) represents the complete benchmarking cycle. The LLMs are provided with task descriptions, previous interventions they proposed, and the corresponding observations. Based on this information, the LLMs generate adjacency matrices and propose a new intervention to gather additional data. A new observation is then obtained and added to the input. (B) outline the conditions for terminating or continuing the loop. The loop terminates when the generated adjacency matrix matches the underlying causal graph; otherwise, it continues. To simulate real-world scientific problems, we include two experimental settings: chemistry and social networks.</p>
<p>Less reasoning performance, each offering a diverse range of challenges and difficulty levels. Similarly, code completion benchmarks (Xu et al., 2022) and others which focus on sequence completion (Chang et al., 2024; Zhu et al., 2024; Zhang et al., 2024; Valmeekam et al., 2024) exist. LLM models are also evaluated in decision-making applications (Yao et al., 2022), where they engage in iterative interactions with an agent to execute actions. However, these benchmarks do not evaluate LLM models from a scientific discovery perspective—which requires not only understanding but also adapting to environmental changes and making optimal decisions through sound reasoning. This motivates us to design a new benchmark that captures and evaluates the models' abilities in understanding, reasoning, and decision-making.</p>
<p>In this work, we introduce two benchmarks based on causal graph discovery principles. These benchmarks are designed to assess both the capabilities and limitations of various LLM models in decision-making tasks. Decision-making requires understanding, reasoning, and the discovery of new knowledge, and our evaluation focuses on the models' ability to uncover the hidden structures within underlying graphs. This setup features an Oracle with which the models interact continuously to receive feedback that improves their understanding of the graphs. More details regarding the experimental setup are provided in the following sections.</p>
<p>This paper makes the following key contributions:</p>
<ul>
<li>We present a novel benchmark for evaluating scientific research capabilities in LLMs.</li>
<li>We systematically analyze LLM performance across different models, highlighting the limitations of current state-of-the-art approaches.</li>
<li>We investigate the effectiveness of chain-of-thought prompting and identify key failure patterns, offering insights for future architectural improvements.</li>
</ul>
<p>From our experiments, we observed that the complexity of causal graphs significantly affects the performance of LLMs. For example, increasing the number of nodes in both chemistry and social networks leads to a dramatic decrease in the average cycles required to obtain the correct answers. To further analyze this failure, we introduce an experiment on long-term trajectory tracing, which will be</p>
<p>explained in Section 5. We found that as the trajectory length increases, LLMs fail to accurately trace state changes. This finding suggests a significant gap between machine and human intelligence in information processing, which should be addressed in future research.</p>
<h2>2 Related Work</h2>
<p>Existing benchmarks (Miao et al., 2021; Cobbe et al., 2021; Patel et al., 2021; Koncel-Kedziorski et al., 2016) assess the reasoning capabilities of LLMs by posing queries and evaluating their responses. However, these benchmarks do not fully capture the scientific discovery potential of LLMs. For true scientific discovery, LLMs must integrate information gained from critical decisions, which in turn influences their future decision-making performance.</p>
<p>Our benchmark is based on the principles of causal graph structure discovery, albeit with a few relaxations. While it shares similarities with existing methods in causal discovery (Jiralerspong et al., 2024; Long et al., 2023; Choi et al., 2022; Kıcıman et al., 2023), its distinctiveness emerges from the iterative updating of knowledge based on the model’s decisions about the underlying graphs. Traditional LLM-based causal graph discovery methods typically rely on meta-data associated with variables, querying whether an edge exists between two nodes (A, B), sometimes evaluating all possible pairs and other times employing strategies to reduce the number of queries.</p>
<p>Unlike these approaches that primarily focus on querying and response generation, our setup prompts models to suggest optimal interventions, enabling them to uncover hidden structures. This experimental framework is analogous to scientific discovery, where the model continuously updates its knowledge through a series of interventions (experiments) to reveal new insights (causal graph discovery).</p>
<h2>3 Methodology</h2>
<p>In our framework, we introduce two benchmarks: (i) Chemistry and (ii) Social Network. Both benchmarks consist of graphs with nodes, but they differ in how an intervention on one node affects its connected nodes. More details are provided in the following section. The ultimate goal for the LLM model is to uncover the hidden dynamics among the nodes, specifically, to determine their connec-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the Chemistry setting. The brackets indicate (molecule index, molecule state). Figures (a) and (b) illustrate the change in state after an intervention on molecule 0. Figures (c) and (d) present a case where causal graph A and causal graph B result in the same observations.</p>
<p>tions via an adjacency matrix, analogous to making new scientific discoveries.</p>
<p>Scientific breakthroughs require multiple experimental cycles: (1) formulating a hypothesis based on existing observations; (2) designing experiments to test that hypothesis; (3) gathering new observations from these experiments; and (4) refining the hypothesis based on the new data. Similarly, LLM model undergoes a sequence of cycles, referred to as autonomous cycles. It starts with an initial hypothesis (an estimated adjacency matrix) and, based on current state observations, suggests an intervention that leads to new observations. Ultimately, the model aims to converge on a hypothesis (adjacency matrix) that matches the underlying ground truth. Fig. 1 illustrates our benchmarking framework with a fully autonomous cycle. To evaluate the performance of LLMs, we further introduce an assessment of long-term trajectory information tracing using structured matrices.</p>
<h3>3.1 Chemistry Environment</h3>
<p>In this setting, we simulate chemical reactions using Directed Acyclic Graphs (DAGs), as illustrated in Fig. 2(a) and (b). In these graphs, each node represents a molecule, and molecules are connected</p>
<p>by directed edges. For each node a state value is assigned randomly from the set ${0, \ldots, S}$, where $S$ is the maximum state value. An intervention on a molecule causes all downstream molecules to change their state to a random state, while the state of the intervened molecule remains unaffected.</p>
<p>Let $H_{c h} \in{0,1}^{N \times N}$ be the adjacency matrix representing the relationships between molecules, where $N$ denotes the number of molecules. Each element of the matrix is either 1 or 0 , with 1 indicating the presence of a directed edge and 0 indicating its absence. The ultimate goal for the LLMs is to infer this underlying matrix $H_{c h}$.</p>
<p>We will begin our experiment with an initial prompt that encapsulates with all the details like, problem statement, ultimate goal and suggesting the model to request an intervention to learn more about the DAG. More details are provided in the Appendix A. In summary, the model is instructed to understand the current adjacency matrix, with the current state values and prompted to suggest an intervention. After receiving a response from the model, based on the suggested intervention, the Oracle who is aware of the ground truth implements the intervention and note down the new observations. In the consecutive cycles, the prompt includes, description prompting, previous interventions performed by LLMs, and all observations corresponding to these interventions in the form of an observation matrix will be provided to the LLMs. The observation matrix which is denoted as $G_{c h} \in{0, \ldots, S}^{M \times N}$, where $M$ represents the number of cycles, $N$ represents the number of molecules. This cycle will continue once the hypothesis of LLMs $K_{c h}$ matches underlying groundtruth $H_{c h}$ or reaches predefined cycle limitation.</p>
<h3>3.1.1 Evaluation</h3>
<p>For a given DAG, multiple adjacency matrices can exhibit similar causal behavior. As illustrated in Fig. 2(c) and (d), even though the graph structures differ, interventions yield similar effects on the remaining nodes. Therefore, it is essential to consider this property when evaluating the similarity between generated matrices and the ground truth. Where $\left(K_{c h}\right)^{n}$ denotes the matrix $K_{c h}$ raised to the $n^{\text {th }}$ power (i.e., $K_{c h}$ multiplied by itself $n$ times).</p>
<p>$$
H(i, j)= \begin{cases}1, &amp; \text { if } \sum_{n=1}^{M}\left(K_{c h}\right)^{n}(i, j)&gt;0 \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of the Social Network setting. The brackets indicate (person index, person state). Figures (a) and (b) illustrate the change in state after an intervention on person 0 .</p>
<p>Given the hypothesis matrix produced by the model, we compute the corresponding reachability matrix, as demonstrated above, for both the hypothesis and ground truth matrices. Then the resultant matrices are compared to each other to evaluate the similarities.</p>
<h3>3.2 Social Networks</h3>
<p>Similar to our Chemistry setting, to imitate realworld social networks, relationships between individuals are represented using graphs, as illustrated in Fig. 2. The main difference lies in the structure of the connections between nodes and the way they influence each other.</p>
<p>In the social network, the connections between individuals are undirected. An intervention on a specific person increases this person's state by 1 , as well as the states of their neighboring nodes. Therefore, in this case, the adjacency matrix $H_{s o} \in$ ${0,1}^{N \times N}$ should be symmetric, where $N$ represents the number of individuals in the social network. The observation matrix here is denoted as $G_{s o} \in{0,1,2 \ldots}^{M \times N}$, where $M$ represents the number of cycles, $N$ represents the number of individuals.</p>
<h2>4 Experiments</h2>
<p>To ensure a comprehensive comparison, we evaluate several state-of-the-art models, including Claude-3-5 (Anthropic, 2024), GPT-4o (OpenAI, 2024), Gemini 1.5 Pro (DeepMind, 2024), Llama3.1 (70B) (AI, 2024), and Qwen2.5 (72B) (Bai et al., 2024). All experiments are conducted on the OpenRouter platform (OpenRouter, 2025), and the experimental budget is 365 USD. To ensure robust results and minimize the impact of random fluctuations, we conduct 20 independent trials for</p>
<p>Table 1: Performance comparison of multiple LLMs. We present the average number of iterations each LLM requires to obtain the correct answer, along with the success rate.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmarking <br> (Setting)</th>
<th style="text-align: center;">Model used for evaluation (Average Iterations, Successful Rate)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Claude-3-5-haiku</td>
<td style="text-align: center;">Gemini-1.5-pro</td>
<td style="text-align: center;">Llama-3.1-70b-instruct</td>
<td style="text-align: center;">Gpt-4o</td>
<td style="text-align: center;">Qwen2.5-72b-instruct</td>
</tr>
<tr>
<td style="text-align: center;">Social Network <br> (Persons: 3)</td>
<td style="text-align: center;">(1, 20\%)</td>
<td style="text-align: center;">$(3,60 \%)$</td>
<td style="text-align: center;">$(3,70 \%)$</td>
<td style="text-align: center;">$(2,100 \%)$</td>
<td style="text-align: center;">$(2,100 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(Persons: 5)</td>
<td style="text-align: center;">$(\infty, 0 \%)$</td>
<td style="text-align: center;">$(5,50 \%)$</td>
<td style="text-align: center;">$(6,30 \%)$</td>
<td style="text-align: center;">$(4,100 \%)$</td>
<td style="text-align: center;">$(5,100 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(Persons: 10)</td>
<td style="text-align: center;">$(\infty, 0 \%)$</td>
<td style="text-align: center;">$(\infty, 0 \%)$</td>
<td style="text-align: center;">$(\infty, 0 \%)$</td>
<td style="text-align: center;">$(8,30 \%)$</td>
<td style="text-align: center;">$(\infty, 0 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry <br> (Nodes: 3; States:3)</td>
<td style="text-align: center;">(3, 25\%)</td>
<td style="text-align: center;">$(4,95 \%)$</td>
<td style="text-align: center;">$(5,15 \%)$</td>
<td style="text-align: center;">$(4,100 \%)$</td>
<td style="text-align: center;">$(4,100 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(Nodes: 3; States: 5)</td>
<td style="text-align: center;">$(6,5 \%)$</td>
<td style="text-align: center;">$(4,85 \%)$</td>
<td style="text-align: center;">$(4,35 \%)$</td>
<td style="text-align: center;">$(4,100 \%)$</td>
<td style="text-align: center;">$(4,100 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(Nodes: 10; States: 5)</td>
<td style="text-align: center;">$(\infty, 0 \%)$</td>
<td style="text-align: center;">$(11,15 \%)$</td>
<td style="text-align: center;">$(\infty, 0 \%)$</td>
<td style="text-align: center;">$(10,15 \%)$</td>
<td style="text-align: center;">$(\infty, 0 \%)$</td>
</tr>
</tbody>
</table>
<p>each chemistry setup and 10 trials for social network. Below, we provide details on the models, experimental setup, and parameter configurations.</p>
<p>Chemistry To provide a comprehensive evaluation, we consider three different of configurations of $N$ and $S$ values (3,3),(3,5), and (10,5). The purpose of the three different configurations is to measure the trend of reasoning capabilities in LLMs with the change of graph complexity-specifically, as the number of nodes and the diversity of causal properties (satte values) increase. This setup enables us to investigate whether LLMs can maintain consistent reasoning performance as the causal graph becomes more complex, and to determine if there is a threshold beyond which their performance deteriorates.</p>
<p>Social Network In this experiments, similar to Chemistry, we introduce three configurations: (3 persons), (5 persons), and (10 persons). The three different configurations aim to assess the capability of LLMs in managing increasing network complexity as the number of persons and potential relationships grows. This design enables us to investigate whether the reasoning performance of LLMs remains robust when processing larger, more intricate networks.</p>
<h3>4.1 Analysis</h3>
<p>The experimental results are presented in Table 1. In the experiment, we set a predefined cycle limit equal to twice the number of nodes. In a specific round, if the LLM fails to infer the underlying causal graphs within this limit, it is considered to have failed the game for that round. The success rate represents the proportion of rounds in which the LLM successfully obtained the correct answer. The average number of iterations is calculated by averaging all iterations required in successful rounds. We analyze these results from several perspectives.</p>
<p>Social Network Table 1 indicates that GPT-4o and Qwen2.5 demonstrate superior understanding and reasoning capabilities compared to other LLMs, such as Claude-3-5, Gemini-1.5, and Llama-3.1. For instance, GPT-4o and Qwen2.5 achieve 100\% success rate in the settings with 3 and 5 people, whereas the success rate of the other three LLMs remains below $70 \%$.</p>
<p>As expected, as the number of people increases, the success rate of all LLMs decreases. For example, the success rate of Gemini-1.5 is $60 \%, 50 \%$, and $0 \%$ in the settings with 3,5 , and 10 people, respectively. The primary reason for this decline is that existing LLMs struggle to extract key information from complex social networks, leading to inferior performance.</p>
<p>For the most powerful LLMs, GPT-4o and Qwen2.5, performance remains stable in understanding and inferring relationships within the Social Network when the number of people is fewer than 5. However, when the number of people increases to 10 , their performance declines significantly. For instance, GPT-4o achieves $100 \%$ success rate in the setting with 5 people, whereas its success rate drops to only $30 \%$ in the setting with 10 people. This indicates that there is still significant room for improvement in existing LLMs' ability to understand and infer relationships within complex social networks.</p>
<p>Moreover, Table 1 shows the average number of iterations required for LLMs to successfully justify their reasoning. It can be observed that the average number of iterations is very low when LLMs address simple social networks. For example, the average number of iterations for Claude-3-5 and Qwen2.5 is 1 and 2, respectively. As the number of people increases, the average number of iterations increases as well. The primary reason for this is that, when the number of people is high, LLMs may need more iterations to extract key information and</p>
<p>infer relationships. These experiments reveal that the problem-solving capabilities of existing LLMs in complex social networks are inefficient, leaving significant room for improvement.</p>
<p>Based on the above analysis, future LLMs may focus on improving LLMs' ability to efficiently extract key information and infer relationships within complex social networks. This could involve enhancing the models' understanding of network structures, improving their ability to handle largescale relational data, and optimizing their reasoning algorithms. Additionally, even when information extraction and inference are accurate, the speed of inferring relationships decreases as the complexity of the social network increases. Future advancements should aim to address this limitation, ensuring faster and more accurate reasoning in extended interactions or more complex scenarios.
Chemistry Unlike the experiments on the Social Network, which focus on undirected causal graphs, the Chemistry experiments assess the capability of LLMs to address directed causal graphs. The experimental results are shown in Table 1. It can be observed that Gemini-1.5, GPT-4o, and Qwen2.5 demonstrate superior understanding and reasoning capabilities in the Chemistry setting. For example, the success rates of Gemini-1.5, GPT-4o, and Qwen2.5 are ( $95 \%, 85 \%$ ), ( $100 \%, 100 \%$ ), and $(100 \%, 100 \%)$ in the settings of ( 3 nodes, 3 states) and ( 3 nodes, 5 states). In contrast, the success rates of Claude-3-5 and Llama-3.1 are lower than $50 \%$ in both settings.</p>
<p>It is surprising that with the increase in the number of states, the powerful LLMs, GPT-4o and Qwen2.5, can maintain the success rates and do not require additional iterations for analysis. For instance, the average iterations and success rates of GPT-4o and Qwen2.5 are ( $4,100 \%$ ) and ( $4,100 \%$ ) in the settings of ( 3 nodes, 3 states) and ( 3 nodes, 5 states), respectively. This indicates that these two LLMs can handle complex state changes in directed causal graph settings.</p>
<p>As expected, as the number of nodes increases, success rate of all LLMs decreases and average iterations increases. For example, the success rate of Gemini-1.5 is $85 \%$ in the ( 3 nodes, 5 states) setting and decreases to $15 \%$ in the ( 10 nodes, 5 states) setting. The success rate of Gpt-4o is $100 \%$ in the ( 3 nodes, 5 states) setting and decreases to $15 \%$ in the ( 10 nodes, 5 states) setting. Most of LLMs, including Claude-3.5-haiku, Llama-3.1-70b-instruct, and Qwen2.5-72b-instruct, fail to win in every round
of the task. This indicates that understanding and reasoning in complex directed causal graph settings still leaves a large room for improvement for existing LLMs.</p>
<p>As for average iterations, we can find that LLMs need more iterations to address more complex directional causal graphs. Although GPT-4o and Qwen2.5 can quickly find the relations when the number of nodes is low, they struggle to handle many nodes.</p>
<p>Based on the above analysis, future LLMs should focus on improving performance in multiobject interactions, as their performance drops significantly. Additionally, as the number of nodes increases, the analysis speed decreases substantially. Future advancements should aim to address this limitation.
Chemistry vs Social Network Although the overall success rate of LLMs in the Chemistry setting (directional causal graphs) is slightly higher than that in the Social Network setting (non-directional causal graphs), the average number of iterations is similar. Surprisingly, although Gemini-1.5 struggles with non-directional causal graphs (with a success rate of around $50 \%-60 \%$ in simple cases), it can effectively handle simple directional causal graph cases (with a success rate greater than $85 \%$ ). In conclusion, existing LLMs still leave significant room for improvement in understanding and reasoning in complex Social Network and Chemistry settings.</p>
<h2>5 Evaluation on Long-Term Trajectory Tracking</h2>
<p>As mentioned above, we observe that existing LLMs often fail to provide useful feedback due to their limited ability to capture long trajectory information. Consequently, experiments solely based on causal graphs may not fully reflect the true reasoning capabilities of these models. To address this issue, we conduct additional experiments aimed at evaluating the capability of LLMs in capturing long trajectory information. In the following subsections, we first introduce our long-term trajectory measurement, then summarize key findings, and discuss their implications in detail.</p>
<h3>5.1 Task Definition</h3>
<p>In each round, a matrix with shape $M \times N$ is given to the LLM, where $M$ represents the trajectory length and $N$ denotes the number of nodes. Each</p>
<p>element in the matrix is an integer value varies from 0 to $P$, where $P$ represents different color states. Formally, let $X \in{0, \ldots, P}^{M \times N}$ denote the given observation matrix. The task requires an LLM to infer the color trajectory matrix $Y \in$ ${0,1}^{(M-1) \times N}$, where:</p>
<p>$$
Y_{i, j}= \begin{cases}1, &amp; \text { if } X_{i, j} \neq X_{i+1, j} \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>for all $i \in{0, \ldots, M-2}$ and $j \in{0, \ldots, N-1}$. To reduce the impact of outliers, this process will be conducted for $R$ rounds.</p>
<h3>5.2 Prompting and Model Evaluation</h3>
<p>To assess LLMs' ability to comprehend and reason over structured information with long trajectory, we employ a structured zero-shot prompting strategy. The prompt explicitly describes the task, including matrix interpretation and expected output format. We evaluate multiple LLMs, including GPT-4o, Gemini, and Qwen, under identical conditions. Models are queried with the prompt and required to generate structured JSON outputs containing only the color trajectory matrix $Y$.</p>
<p>We conduct systematic evaluations across various trajectory lengths $(M \in{3,5,10,15,20})$ to measure the robustness of each model. To evaluate the performance of each LLM, we develop two types of metrics, noted as Average Trajectory Accuracy (AT-Acc) and Overall Accuracy (OA-Acc).</p>
<p>Overall Accuracy is computed as the fraction of correctly predicted color trajectory matrix:</p>
<p>$$
\text { OA-Acc }=\frac{1}{R} \sum_{i=1}^{R} \mathbb{1}\left(Y_{i}=\hat{Y}_{i}\right)
$$</p>
<p>where $\hat{Y}$ represents the model-generated color trajectory matrix, $Y$ represents the ground-truth color trajectory matrix, $R$ represents number of rounds, and $\mathbb{1}$ is the indicator function. In our experiment, we use $R=100$.</p>
<p>Average Trajectory Accuracy aims to measure the average accuracy of the color change between each pair of rows. For example, the color change between row 0 and row 1 is noted as the first trajectory. To achieve that, a matrix $T \in{0,1}^{R \times(M-1)}$ is further constructed, where:</p>
<p>$$
T_{r, i}= \begin{cases}1, &amp; \text { if } Y_{i, r}=\hat{Y}_{i, r} \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>for all $r \in{0, \ldots, R-1}$ and $i \in{0, \ldots, M-$ 2). $Y_{i, r}$ represents row $i$ of matrix $Y$ in round $r$. The Average Trajectory Accuracy is computed as following:</p>
<p>$$
\begin{aligned}
\text { AT-Acc }<em i="1">{j} &amp; =\frac{1}{R} \sum</em> \
&amp; \forall j \in{1, \ldots, M-1}
\end{aligned}
$$}^{R} T_{i, j</p>
<h3>5.3 Chain-of-Thought Prompting</h3>
<p>Chain-of-Thought (CoT) prompting has been proven effective in enhancing the reasoning ability of large language models (LLMs). To further explore its effectiveness in long-term trajectory tracing, we incorporate CoT prompting before computing the final matrix. In our experiment, we use a simple prompt: "Please also include the reason for your answer." We compare zero-shot and CoTaugmented responses, analyzing accuracy improvements across sequences.</p>
<h3>5.4 Experiment Results</h3>
<p>In this section, we present and analyze the results of our long trajectory measurement. These results offer insights into the understanding and reasoning capabilities of LLMs across different settings, particularly their ability to capture long trajectory information. In the following subsections, we summarize key findings and discuss their implications in detail.</p>
<h3>5.4.1 Analysis of Trajectory Measurement</h3>
<p>The experimental results are presented in Table 2 and Figure 4. We analyze these results from two perspectives:
Overall Accuracy vs Trajectory Length Table 2 shows the Overall Accuracy of various large language models (LLMs) with respect to the trajectory length. The models are evaluated both with and without Chain-of-Thought (CoT) prompting. It can be observed that the accuracy of LLMs significantly degrades as the trajectory length increases. For example, Claude achieves an accuracy of $69 \%$ with 3 observations but drops to $0 \%$ when the trajectory length exceeds 20 .</p>
<p>Additionally, we can observe that for GPT-4o and Qwen2.5, CoT prompting generally improves the models' performance, particularly with shorter observation sequences. However, performance declines as the sequence length increases. For example, with trajectory length of 5, GPT-4o achieves</p>
<p>Table 2: Overall accuracy comparison of multiple LLMs. We report performance across various trajectory lengths, both with and without CoT prompting.</p>
<p>| Trajectory Length | Model used for evaluation (W/O CoT, With CoT) | | | | |
| | Claude-3-5-haiku | Gemini-1.5-pro | Llama-3.1-70b-instruct | Gpt-4o | Qwen2.5-72b-instruct |
| 3 | (69%,52%) | (42%,51%) | (6%,8%) | (41%,99%) | (6%,30%) |
| 5 | (26%,29%) | (18%,18%) | (0%,1%) | (13%,100%) | (1%,31%) |
| 10 | (0%,0%) | (0%,2%) | (0%,0%) | (0%,91%) | (0%,30%) |
| 15 | (1%,0%) | (0%,0%) | (0%,0%) | (0%,28%) | (0%,5%) |
| 20 | (0%,0%) | (0%,0%) | (0%,0%) | (0%,0%) | (0%,3%) |
| 25 | (0%,0%) | (0%,0%) | (0%,0%) | (0%,1%) | (0%,0%) |
| 30 | (0%,0%) | (0%,0%) | (0%,0%) | (0%,0%) | (0%,0%) |</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average Trajectory Accuracy vs. Trajectory
an accuracy of $13 \%$ without using CoT, but this increases to $100 \%$ when CoT is applied. However, CoT prompting does not improve the performance of models with longer observation sequences, meaning that its benefits are limited to shorter input lengths. In future work, exploring more robust methods for handling longer observation sequences may help mitigate the performance degradation observed as input lengths increase.</p>
<h2>Average Trajectory Accuracy vs Trajectory</h2>
<p>Given trajectory length of 20, Figure 4 shows the Average Trajectory Accuracy of various large language models (LLMs) with respect to the trajectories. The results in this figure are averaged from 100 trials. It can be observed that as the trajectory increases, the accuracy of the models gradually decreases. This suggests that existing large language models struggle to handle long-term trajectory information, a phenomenon we refer to as temporal attention decay. Additionally, we find that certain models, such as GPT-4o and Qwen2.5, benefit from CoT prompting, which significantly improves their ability to capture long-term temporal dependencies. However, the accuracy of Qwen2.5 with CoT in the 19th trajectory remains below $70 \%$. This indicates that future work should focus on further improving the models' ability to manage long temporal sequences, potentially by exploring more advanced techniques for enhancing temporal attention mechanisms.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we present Auto-Bench, a novel benchmark designed to evaluate the scientific discovery capabilities of LLMs, which challenges models to uncover hidden structures and make optimal decisions through iterative interactions with an oracle. Specifically, our benchmark is built around two core settings: Chemistry and Social Networks. The Chemistry setting simulates chemical reactions to assess LLMs' ability to understand and reason about directed graphs, while the Social Network setting evaluates their performance in undirected graphs by modeling real-world social interactions. By gradually increasing the complexity of the causal graphs, we conduct a comprehensive analysis of LLMs' capacity for iterative knowledge refinement. Our experiments with state-of-the-art models such as GPT-4, Gemini, Qwen, Claude, and Llama reveal a significant performance drop as the problem complexity increases. Notably, the performance of LLMs is constrained by their ability to capture and maintain long trajectory information. This study not only highlights current limitations but also provides a foundation for future work aimed at enhancing LLMs' iterative reasoning and knowledge updating capabilities in scientific research.</p>
<h2>7 Limitations</h2>
<p>While our benchmark provides a novel framework for evaluating LLMs in scientific discovery through causal reasoning, it has several limitations. First, the framework assumes that causal relationships can be inferred purely from structured matrix observations and predefined interventions, which may not fully capture the complexities of real-world scientific reasoning that often involves unstructured data, domain-specific knowledge, and hypothesis generation beyond direct observations. Second, the benchmark currently focuses on discrete state changes and predefined causal structures, limiting its applicability to more dynamic, continuous, or probabilistic causal systems. Additionally, the evaluation primarily relies on adjacency matrix reconstruction, which may not comprehensively measure an LLM's ability to reason about causality in more abstract or interdisciplinary contexts. Lastly, the benchmark does not account for external knowledge retrieval, a critical component of human scientific discovery, which could lead to underestimating an LLM's full potential. Future work should address these challenges by incorporating more diverse data modalities, probabilistic causal reasoning, and interactive learning environments that better simulate the iterative nature of real-world scientific discovery.</p>
<h2>References</h2>
<p>Meta AI. 2024. Llama 3.1: Open foundation and finetuned chat models. Model Release Notes.</p>
<p>Signe Altmäe, Alberto Sola-Leyva, and Andres Salumets. 2023. Artificial intelligence in scientific writing: a friend or a foe? Reproductive BioMedicine Online, 47(1):3-9.</p>
<p>Anthropic. 2024. Claude 3.5 haiku model card. Accessed: 2024-10-22.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Ruiyang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2024. Qwen2.5: The next generation of large language models with hybrid scaling. arXiv preprint.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):1-45.</p>
<p>Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. 2022. Lmpriors: Pre-trained language models as task-specific priors. arXiv preprint arXiv:2210.12530.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Google DeepMind. 2024. Gemini 1.5: Unlocking multimodal understanding at scale. Technical Report.</p>
<p>Marius-Constantin Dinu, Claudiu Leoveanu-Condrei, Markus Holzleitner, Werner Zellinger, and Sepp Hochreiter. 2024. Symbolicai: A framework for logic-based approaches combining generative models and solvers. arXiv preprint arXiv:2402.00854.</p>
<p>Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, et al. 2024. Evaluation and mitigation of the limitations of large language models in clinical decision-making. Nature medicine, 30(9):26132622.</p>
<p>Thomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Q Tran, Jonathan Deaton, Marius Wiggert, et al. 2025. Simulating 500 million years of evolution with a language model. Science, page eads0018.</p>
<p>Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, and Yoshua Bengio. 2024. Efficient causal graph discovery using large language models. arXiv preprint arXiv:2402.01207.</p>
<p>John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. 2021. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583-589.</p>
<p>Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. 2023. Causal reasoning and large language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1152-1157.</p>
<p>Stephanie Long, Tibor Schuster, and Alexandre Piché. 2023. Can large language models build causal graphs? arXiv preprint arXiv:2303.05279.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292.</p>
<p>Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Nature, 624(7990):80-85.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. A diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772.</p>
<p>Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745.</p>
<p>OpenAI. 2024. Gpt-4o system architecture. Technical Overview.</p>
<p>OpenRouter. 2025. Openrouter: A unified interface for llms. https://openrouter.ai. Accessed: 2025-02-16.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191.</p>
<p>Edward O Pyzer-Knapp, Jed W Pitera, Peter WJ Staar, Seiji Takeda, Teodoro Laíno, Daniel P Sanders, James Sexton, John R Smith, and Alessandro Curioni. 2022. Accelerating materials discovery using artificial intelligence, high performance computing and robotics. npj Computational Materials, 8(1):84.</p>
<p>P Rajpurkar. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.</p>
<p>Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2024. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems, 36.</p>
<p>Alex Wang. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.</p>
<p>Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1-10.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2024. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:39-57.</p>
<p>Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xing Xie. 2024. Promptbench: A unified library for evaluation of large language models. Journal of Machine Learning Research, 25(254):1-22.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">of</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">objects</span>.<span class="err">&quot;</span>
<span class="err">    ],</span>
<span class="w">    </span><span class="s2">&quot;Example&quot;</span>:<span class="w"> </span>[
<span class="w">    </span><span class="s2">&quot;Given three obejcts: object 0, object 1, and object 2.&quot;</span>,
<span class="w">    </span><span class="err">&quot;If intervening on object 0 causes only object 1 to change color, then object 1</span>
<span class="err">is the only downstream of object 0 and we can infer the first row of adjacency matrix is</span>
[<span class="mi">0</span>,<span class="mi">1</span>,<span class="mi">0</span>]<span class="err">&quot;,</span>
<span class="w">    </span><span class="err">&quot;If intervening on object 0 causes no color change in object 1 and object 2,</span>
<span class="err">then object 0 has no causal relationship with object 1 and object 2. Thus we can infer</span>
<span class="nv">the</span><span class="w"> </span><span class="nv">first</span><span class="w"> </span><span class="nv">row</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">adjacency</span><span class="w"> </span><span class="nv">matrix</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span>[<span class="mi">0</span>,<span class="mi">0</span>,<span class="mi">0</span>]<span class="err">&quot;</span>
<span class="err">    ]</span>
<span class="err">    }</span>
<span class="err">}</span>
</code></pre></div>

<p>Here are all observations that you have already obtained, where the first row is the initial observation and the following are the observations corresponding to the actions you made:
・・
{prev_obs}
・・
Here are the previous actions that you have already conducted:
・・
{prev_actions }
・・
Please respond in the following format:</p>
<p>Response JSON:
json
$&lt;$ JSON $&gt;$</p>
<p>In <JSON>, provide your response in JSON format with the following fields:</p>
<ul>
<li>"Trans_Matrix": A matrix wiht shape (M-I,N) showing the color change of each transition. Each row means the observation trainsition ( 0 to 1,1 to $2, \ldots$ ).</li>
</ul>
<p>Each column means different objects ( $0,1,2, \ldots$ ).
There are two elements: 0 or 1 , where 0 means no color changes and 1 means color changes.</p>
<p>Having correct observation is very important, be cautious when you generate Trans_Matrix.</p>
<ul>
<li>"Reason for Trans_Matrix": The reason of your answer in Trans_Matrix.</li>
<li>"Reasoning": Give a detailed analysis on each intervention and their corresponding observation transition. Also provide the reason of your proposed Ad_Matrix.</li>
<li>"Ad_Matrix": This is the adjacency matrix with shape NxMxM, where N is the number of valid hypothesis and $M$ is the number of objects.</li>
</ul>
<p>The element in the Ad_Matrix should be integer. The order in the adjacency matrix follows the numbering of the object.</p>
<h1>A Appendix</h1>
<div class="codehilite"><pre><span></span><code><span class="s2">&quot;task_description&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="s2">&quot;Matrix&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;Rows&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Represent observations (observation 0, observation 1, ...)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;Columns&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Represent objects (object 0, object 1, ...)&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;Elements&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="s2">&quot;Range&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0 to 9&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s2">&quot;Mapping&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="s2">&quot;0&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;red&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;1&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;blue&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;2&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;green&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;3&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;yellow&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;4&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;purple&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;5&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cyan&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;6&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;orange&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;7&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;pink&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;8&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;brown&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;9&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gray&quot;</span>
<span class="w">            </span><span class="p">},</span>
<span class="w">        </span><span class="s2">&quot;Example&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Matrix(1,2)=0 means that in observation 1, the color of object 2 is</span>
<span class="n">red</span><span class="o">.</span><span class="s2">&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;Causal_Graph&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;Definition&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;A hidden adjacency matrix indicating causal relationships between</span>
<span class="n">objects</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">            </span><span class="s2">&quot;Rows&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Represent parent objects (causal sources).&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s2">&quot;Columns&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Represent child objects (causal effects).&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;Intervention&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;Definition&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;You can intervene on a specific object.&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s2">&quot;Effect&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;The intervention will cause downstream objects (determined by the</span>
<span class="n">hidden</span><span class="w"> </span><span class="n">adjacency</span><span class="w"> </span><span class="n">matrix</span><span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">change</span><span class="w"> </span><span class="n">color</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">            </span><span class="s2">&quot;Constraint&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;The object you intervene on will not change color.&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;Gameplay&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;Objective&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Infer causal relationships using color changes from interventions</span>
<span class="ow">and</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">adjacency</span><span class="w"> </span><span class="n">matrix</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">        </span><span class="s2">&quot;Process&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="s2">&quot;In each round, firstly indentify color change in each transition&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s2">&quot;Submit as many correct adjacency matrices as possible, each representing a</span>
<span class="n">possible</span><span class="w"> </span><span class="n">causal</span><span class="w"> </span><span class="n">graph</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">            </span><span class="s2">&quot;Suggest the optimal object index for intervention to gather new causal</span>
<span class="n">information</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">            </span><span class="s2">&quot;After the intervention, receive a new observation reflecting the updated state</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
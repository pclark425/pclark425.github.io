<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5764 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5764</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5764</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-271212697</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.09977v1.pdf" target="_blank">Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis</a></p>
                <p><strong>Paper Abstract:</strong> The reconstruction of Earth's history faces significant challenges due to the nonunique interpretations often derived from rock records. The problem has long been recognized but there are no systematic solutions in practice. This study introduces an innovative approach that leverages Large Language Models (LLMs) along with retrieval augmented generation and real-time search capabilities to counteract interpretation biases, thereby enhancing the accuracy and reliability of geological analyses. By applying this framework to sedimentology and paleogeography, we demonstrate its effectiveness in mitigating interpretations biases through the generation and evaluation of multiple hypotheses for the same data, which can effectively reduce human bias. Our research illuminates the transformative potential of LLMs in refining paleoenvironmental studies and extends their applicability across various sub-disciplines of Earth sciences, enabling a deeper and more accurate depiction of Earth's evolution.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5764.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5764.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paleo Expert Q&A</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paleogeography Expert Question-and-Answer System (Expert Q&A system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LLM-based expert system built to synthesize sedimentology and paleogeography literature, generate multiple working hypotheses, rank them by plausibility, and reduce interpretation bias in rock-record analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-4 (commercial OpenAI large transformer-based LLM). The paper does not report model size or architecture details beyond selecting GPT-4 for superior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Distill generalizable hypotheses, principles, and ranked causal explanations from a large corpus of sedimentology/paleogeography literature to mitigate interpretation bias in paleoenvironmental reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Earth sciences — sedimentology, paleogeography, paleoenvironmental reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompt engineering + chain-of-thought (CoT) decomposition; retrieval-augmented generation (RAG) combining (a) a vector database of embedded domain literature (sentence-transformers/all-MiniLM-L6-v2 embeddings), and (b) live web search (Google) with contextual compression, reranking, and summarization; human-in-the-loop evaluation and ablation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Ranked causal hypotheses and domain-general interpretive rules (e.g., rules linking decreases in benthic foraminifera δ18O to temperature, ice volume, salinity, biological/diagenetic effects; general heuristics for generating multiple plausible depositional/environmental interpretations from observations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative human expert evaluation (domain experts judged output utility/appropriateness), hypothesis ranking agreement with experts, citation density/number of retrieved references, and ablation comparisons between system variants (plain GPT-4, DB-only, web-only, DB+web). No formal numeric accuracy metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>System outputs were judged by domain experts to approximate proficient students/researchers: it produced more content richness, more cited references, and multiple hypotheses ranked by plausibility that generally aligned with expert consensus; ablation showed the full RAG system (DB + web) outperformed plain GPT-4 and partially-augmented variants. Improvements are qualitative; no strict quantitative performance numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop for: (1) curating and providing the literature metadata corpus; (2) prompt engineering and chain-of-thought decomposition design; (3) generating professional test questions; and (4) expert qualitative evaluation of outputs and hypothesis rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Thousands of domain articles (metadata and abstracts) in sedimentology and paleogeography; abstracts segmented into sentences/paragraphs, embedded into a 384-dimensional vector space via sentence-transformers/all-MiniLM-L6-v2, and stored in a vector database; additionally augmented by live web search results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported challenges include LLM hallucination risk, cutoff of pretraining data (need for real-time retrieval), dependency on retrieval quality and accessible relevant data, difficulty handling highly autocorrelated/very long retrieved content (necessitating contextual compression), cost/time to build and update knowledge bases, and the inherently qualitative/under-determined nature of geological inference (no single ground truth). The paper reports primarily qualitative gains and lacks quantitative benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Worked examples include: (1) multi-hypothesis ranked interpretation of decreased benthic foraminifera δ18O (temperature, ice-volume, salinity, biological processes, diagenesis) where GPT-4+RAG produced ranked, referenced hypotheses; (2) a case study on causes of Early Miocene shift to monsoon-dominant climate in China where the system produced multiple ranked hypotheses (Tibetan Plateau uplift highest probability, plus global cooling/ocean circulation/tectonics) and cited sources gathered from DB + web.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5764.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5764.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The large commercial LLM selected and used as the core generative model in the paper's expert Q&A system to synthesize retrieved literature and generate hypotheses and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Closed-source OpenAI multimodal/large transformer LLM chosen for superior generative and reasoning ability; the paper does not specify parameter count or training details.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate multi-step reasoning outputs, synthesize retrieved domain knowledge into organized reports, and produce multiple ranked hypotheses from literature and observations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Applied here to geology (sedimentology, paleogeography) but referenced as a general-purpose LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Invoked inside RAG pipeline and guided with chain-of-thought prompts and prompt engineering; reconsideration step integrates retrieved documents before final answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not a single formal law; used to synthesize general interpretive rules and ranked explanatory hypotheses from literature (e.g., principles governing δ18O interpretation, drivers of Miocene climatic transitions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human expert qualitative assessment, comparison across ablation system variants (plain GPT-4 vs. retrieval-augmented variants).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Plain GPT-4 generated plausible hypotheses but with less citation density and content richness than the full RAG system; when augmented via retrieval and CoT it produced more domain-anchored, citation-rich outputs aligning better with expert judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Humans designed prompts and evaluated outputs; human experts provided test questions and judged answers.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Pretrained on broad web/text corpora (not detailed in paper); used together with curated sedimentology/paleogeography metadata and live web retrieval during RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Susceptible to hallucination if not anchored with retrieval; pretraining cutoff necessitates external retrieval for recent literature; plain model lacks domain-specific grounding without RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Compared plain GPT-4 outputs to RAG-augmented outputs in ablation study—plain GPT-4 produced multiple hypotheses but fewer citations and lower content richness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5764.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5764.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology combining external retrieval (vector DB and web search) with generative LLMs to ground outputs in retrieved documents and reduce hallucination while enabling up-to-date knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (used as the generator in RAG pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>RAG pairs GPT-4 generation with retrieved context from the vector database and search engines; the generator consumes retrieved passages as context to produce answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Anchor LLM outputs in externally retrieved scientific literature to synthesize domain-general rules and ranked hypotheses from large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>General; applied in this paper to geoscience literature (sedimentology/paleogeography).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Pre-retrieval: indexing with sentence embeddings and metadata; Retrieval: vector similarity and web search; Post-retrieval: autocorrelation filtering, contextual compression, reranking, and summarization feeding into LLM generation.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Grounded interpretive rules and ranked causal explanations derived from aggregated literature excerpts (e.g., multi-cause explanations with ranked probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Ablation experiments comparing presence/absence of DB/web retrieval (qualitative expert judgments, citation density), and alignment of hypothesis rankings with domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RAG materially improved citation density, content richness, and plausibility/alignment of hypotheses versus plain generation; the full DB+web RAG pipeline yielded best results in expert evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human curation of the DB, prompt design, and expert evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Vectorized metadata from thousands of sedimentology and paleogeography abstracts plus live web search results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on retrieval relevance and corpus coverage; long or highly autocorrelated documents require contextual compression; retrieval cannot entirely eliminate hallucination and requires careful prompt/formatting to ensure faithful grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Ablation experiment showing System-DB-SE (DB + web) outperformed System-noDB-SE and System-DB-noSE as well as plain GPT-4 when answering expert-level multi-hypothesis questions (e.g., δ18O interpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5764.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5764.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits multi-step internal reasoning from LLMs by instructing them to produce intermediate steps (e.g., 'step by step'), improving complex reasoning needed to assemble multi-part hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (CoT used to structure generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Chain-of-thought is a prompting paradigm, not a separate model; applied to GPT-4 to break questions into subqueries and produce deliberative reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Improve multi-step reasoning, decompose user queries into sub-queries, and replicate expert cognitive processes during hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Applied to geological reasoning and literature synthesis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompt engineering to request stepwise reasoning; LLM decomposes questions into subqueries that are then used by retrieval module.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Enables extraction of structured chains that lead to interpretable, ranked hypotheses (facilitating generation of generalizable interpretive rules).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative assessment of coherence, logical consistency, and alignment of generated reasoning with expert expectations; improvement observed when combined with RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CoT improved consistency and logical coherence of outputs; when combined with RAG it led to more structured, multi-hypothesis outputs that matched expert reasoning patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human designers crafted CoT prompts and inspected the reasoning traces; experts assessed reasoning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Used together with the paper's literature vector DB and web-retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CoT can increase verbosity and expose intermediate hallucinations if retrieval grounding is insufficient; relies on careful prompt engineering to avoid misleading internal rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>System split a user question about Miocene climate into four subqueries and used CoT-guided retrieval+generation to synthesize a structured report with ranked hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5764.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5764.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K2 (foundation LLM for geoscience)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-focused foundation model referenced in the paper as demonstrating potential for geoscience knowledge reasoning and discovery of relationships in geological data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>K2</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Mentioned as a domain-adapted foundation model for geoscience (details available in the cited preprint); the paper references K2 as an example of LLM potential in geology but does not use it experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Reference example of specialized LLMs aimed at geoscience knowledge understanding and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Geoscience (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Mentioned in related work as a model for knowledge reasoning; not used in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Potential to derive geoscience relationships and insights (mentioned as capability).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as demonstration of LLM potential; no results presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5764.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5764.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoGalactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoGalactica: A Scientific Large Language Model in Geoscience</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geoscience-oriented scientific LLM cited in the paper as an example of domain-specialized models that can perform knowledge reasoning and aid discovery in earth science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GeoGalactica: A Scientific Large Language Model in Geoscience.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GeoGalactica</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Referenced as a scientific LLM tailored to geoscience tasks (cited preprint); the paper cites GeoGalactica as related work but does not apply it.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Cited example of domain-specialized LLMs capable of geoscience knowledge synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Geoscience (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Mentioned in related work; not used experimentally in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not specified in this paper; implied potential to extract geoscience rules/relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as relevant prior art; no experimental results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-Augmented Generation for Large Language Models: A Survey <em>(Rating: 2)</em></li>
                <li>GeoGalactica: A Scientific Large Language Model in Geoscience. <em>(Rating: 2)</em></li>
                <li>K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization <em>(Rating: 2)</em></li>
                <li>Large Language Models for Scientific Synthesis <em>(Rating: 2)</em></li>
                <li>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions <em>(Rating: 1)</em></li>
                <li>Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5764",
    "paper_id": "paper-271212697",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "Paleo Expert Q&A",
            "name_full": "Paleogeography Expert Question-and-Answer System (Expert Q&A system)",
            "brief_description": "A retrieval-augmented LLM-based expert system built to synthesize sedimentology and paleogeography literature, generate multiple working hypotheses, rank them by plausibility, and reduce interpretation bias in rock-record analyses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4",
            "llm_model_description": "GPT-4 (commercial OpenAI large transformer-based LLM). The paper does not report model size or architecture details beyond selecting GPT-4 for superior performance.",
            "task_goal": "Distill generalizable hypotheses, principles, and ranked causal explanations from a large corpus of sedimentology/paleogeography literature to mitigate interpretation bias in paleoenvironmental reconstruction.",
            "domain": "Earth sciences — sedimentology, paleogeography, paleoenvironmental reconstruction",
            "methodology": "Prompt engineering + chain-of-thought (CoT) decomposition; retrieval-augmented generation (RAG) combining (a) a vector database of embedded domain literature (sentence-transformers/all-MiniLM-L6-v2 embeddings), and (b) live web search (Google) with contextual compression, reranking, and summarization; human-in-the-loop evaluation and ablation experiments.",
            "type_of_qualitative_law": "Ranked causal hypotheses and domain-general interpretive rules (e.g., rules linking decreases in benthic foraminifera δ18O to temperature, ice volume, salinity, biological/diagenetic effects; general heuristics for generating multiple plausible depositional/environmental interpretations from observations).",
            "evaluation_metrics": "Qualitative human expert evaluation (domain experts judged output utility/appropriateness), hypothesis ranking agreement with experts, citation density/number of retrieved references, and ablation comparisons between system variants (plain GPT-4, DB-only, web-only, DB+web). No formal numeric accuracy metrics reported.",
            "results_summary": "System outputs were judged by domain experts to approximate proficient students/researchers: it produced more content richness, more cited references, and multiple hypotheses ranked by plausibility that generally aligned with expert consensus; ablation showed the full RAG system (DB + web) outperformed plain GPT-4 and partially-augmented variants. Improvements are qualitative; no strict quantitative performance numbers given.",
            "human_involvement": "Human-in-the-loop for: (1) curating and providing the literature metadata corpus; (2) prompt engineering and chain-of-thought decomposition design; (3) generating professional test questions; and (4) expert qualitative evaluation of outputs and hypothesis rankings.",
            "dataset_or_corpus": "Thousands of domain articles (metadata and abstracts) in sedimentology and paleogeography; abstracts segmented into sentences/paragraphs, embedded into a 384-dimensional vector space via sentence-transformers/all-MiniLM-L6-v2, and stored in a vector database; additionally augmented by live web search results.",
            "limitations_or_challenges": "Reported challenges include LLM hallucination risk, cutoff of pretraining data (need for real-time retrieval), dependency on retrieval quality and accessible relevant data, difficulty handling highly autocorrelated/very long retrieved content (necessitating contextual compression), cost/time to build and update knowledge bases, and the inherently qualitative/under-determined nature of geological inference (no single ground truth). The paper reports primarily qualitative gains and lacks quantitative benchmarks.",
            "notable_examples": "Worked examples include: (1) multi-hypothesis ranked interpretation of decreased benthic foraminifera δ18O (temperature, ice-volume, salinity, biological processes, diagenesis) where GPT-4+RAG produced ranked, referenced hypotheses; (2) a case study on causes of Early Miocene shift to monsoon-dominant climate in China where the system produced multiple ranked hypotheses (Tibetan Plateau uplift highest probability, plus global cooling/ocean circulation/tectonics) and cited sources gathered from DB + web.",
            "uuid": "e5764.0",
            "source_info": {
                "paper_title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "The large commercial LLM selected and used as the core generative model in the paper's expert Q&A system to synthesize retrieved literature and generate hypotheses and explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4",
            "llm_model_description": "Closed-source OpenAI multimodal/large transformer LLM chosen for superior generative and reasoning ability; the paper does not specify parameter count or training details.",
            "task_goal": "Generate multi-step reasoning outputs, synthesize retrieved domain knowledge into organized reports, and produce multiple ranked hypotheses from literature and observations.",
            "domain": "Applied here to geology (sedimentology, paleogeography) but referenced as a general-purpose LLM.",
            "methodology": "Invoked inside RAG pipeline and guided with chain-of-thought prompts and prompt engineering; reconsideration step integrates retrieved documents before final answer generation.",
            "type_of_qualitative_law": "Not a single formal law; used to synthesize general interpretive rules and ranked explanatory hypotheses from literature (e.g., principles governing δ18O interpretation, drivers of Miocene climatic transitions).",
            "evaluation_metrics": "Human expert qualitative assessment, comparison across ablation system variants (plain GPT-4 vs. retrieval-augmented variants).",
            "results_summary": "Plain GPT-4 generated plausible hypotheses but with less citation density and content richness than the full RAG system; when augmented via retrieval and CoT it produced more domain-anchored, citation-rich outputs aligning better with expert judgment.",
            "human_involvement": "Humans designed prompts and evaluated outputs; human experts provided test questions and judged answers.",
            "dataset_or_corpus": "Pretrained on broad web/text corpora (not detailed in paper); used together with curated sedimentology/paleogeography metadata and live web retrieval during RAG.",
            "limitations_or_challenges": "Susceptible to hallucination if not anchored with retrieval; pretraining cutoff necessitates external retrieval for recent literature; plain model lacks domain-specific grounding without RAG.",
            "notable_examples": "Compared plain GPT-4 outputs to RAG-augmented outputs in ablation study—plain GPT-4 produced multiple hypotheses but fewer citations and lower content richness.",
            "uuid": "e5764.1",
            "source_info": {
                "paper_title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A methodology combining external retrieval (vector DB and web search) with generative LLMs to ground outputs in retrieved documents and reduce hallucination while enabling up-to-date knowledge.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4 (used as the generator in RAG pipeline)",
            "llm_model_description": "RAG pairs GPT-4 generation with retrieved context from the vector database and search engines; the generator consumes retrieved passages as context to produce answers.",
            "task_goal": "Anchor LLM outputs in externally retrieved scientific literature to synthesize domain-general rules and ranked hypotheses from large corpora.",
            "domain": "General; applied in this paper to geoscience literature (sedimentology/paleogeography).",
            "methodology": "Pre-retrieval: indexing with sentence embeddings and metadata; Retrieval: vector similarity and web search; Post-retrieval: autocorrelation filtering, contextual compression, reranking, and summarization feeding into LLM generation.",
            "type_of_qualitative_law": "Grounded interpretive rules and ranked causal explanations derived from aggregated literature excerpts (e.g., multi-cause explanations with ranked probabilities).",
            "evaluation_metrics": "Ablation experiments comparing presence/absence of DB/web retrieval (qualitative expert judgments, citation density), and alignment of hypothesis rankings with domain experts.",
            "results_summary": "RAG materially improved citation density, content richness, and plausibility/alignment of hypotheses versus plain generation; the full DB+web RAG pipeline yielded best results in expert evaluation.",
            "human_involvement": "Human curation of the DB, prompt design, and expert evaluation.",
            "dataset_or_corpus": "Vectorized metadata from thousands of sedimentology and paleogeography abstracts plus live web search results.",
            "limitations_or_challenges": "Quality depends on retrieval relevance and corpus coverage; long or highly autocorrelated documents require contextual compression; retrieval cannot entirely eliminate hallucination and requires careful prompt/formatting to ensure faithful grounding.",
            "notable_examples": "Ablation experiment showing System-DB-SE (DB + web) outperformed System-noDB-SE and System-DB-noSE as well as plain GPT-4 when answering expert-level multi-hypothesis questions (e.g., δ18O interpretation).",
            "uuid": "e5764.2",
            "source_info": {
                "paper_title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits multi-step internal reasoning from LLMs by instructing them to produce intermediate steps (e.g., 'step by step'), improving complex reasoning needed to assemble multi-part hypotheses.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4 (CoT used to structure generation)",
            "llm_model_description": "Chain-of-thought is a prompting paradigm, not a separate model; applied to GPT-4 to break questions into subqueries and produce deliberative reasoning traces.",
            "task_goal": "Improve multi-step reasoning, decompose user queries into sub-queries, and replicate expert cognitive processes during hypothesis generation.",
            "domain": "Applied to geological reasoning and literature synthesis in this paper.",
            "methodology": "Prompt engineering to request stepwise reasoning; LLM decomposes questions into subqueries that are then used by retrieval module.",
            "type_of_qualitative_law": "Enables extraction of structured chains that lead to interpretable, ranked hypotheses (facilitating generation of generalizable interpretive rules).",
            "evaluation_metrics": "Qualitative assessment of coherence, logical consistency, and alignment of generated reasoning with expert expectations; improvement observed when combined with RAG.",
            "results_summary": "CoT improved consistency and logical coherence of outputs; when combined with RAG it led to more structured, multi-hypothesis outputs that matched expert reasoning patterns.",
            "human_involvement": "Human designers crafted CoT prompts and inspected the reasoning traces; experts assessed reasoning quality.",
            "dataset_or_corpus": "Used together with the paper's literature vector DB and web-retrieved documents.",
            "limitations_or_challenges": "CoT can increase verbosity and expose intermediate hallucinations if retrieval grounding is insufficient; relies on careful prompt engineering to avoid misleading internal rationales.",
            "notable_examples": "System split a user question about Miocene climate into four subqueries and used CoT-guided retrieval+generation to synthesize a structured report with ranked hypotheses.",
            "uuid": "e5764.3",
            "source_info": {
                "paper_title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "K2",
            "name_full": "K2 (foundation LLM for geoscience)",
            "brief_description": "A domain-focused foundation model referenced in the paper as demonstrating potential for geoscience knowledge reasoning and discovery of relationships in geological data.",
            "citation_title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
            "mention_or_use": "mention",
            "llm_model_name": "K2",
            "llm_model_description": "Mentioned as a domain-adapted foundation model for geoscience (details available in the cited preprint); the paper references K2 as an example of LLM potential in geology but does not use it experimentally.",
            "task_goal": "Reference example of specialized LLMs aimed at geoscience knowledge understanding and reasoning.",
            "domain": "Geoscience (referenced)",
            "methodology": "Mentioned in related work as a model for knowledge reasoning; not used in this study.",
            "type_of_qualitative_law": "Potential to derive geoscience relationships and insights (mentioned as capability).",
            "evaluation_metrics": "",
            "results_summary": "Mentioned as demonstration of LLM potential; no results presented in this paper.",
            "human_involvement": "",
            "dataset_or_corpus": "",
            "limitations_or_challenges": "No details in this paper.",
            "notable_examples": "",
            "uuid": "e5764.4",
            "source_info": {
                "paper_title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GeoGalactica",
            "name_full": "GeoGalactica: A Scientific Large Language Model in Geoscience",
            "brief_description": "A geoscience-oriented scientific LLM cited in the paper as an example of domain-specialized models that can perform knowledge reasoning and aid discovery in earth science.",
            "citation_title": "GeoGalactica: A Scientific Large Language Model in Geoscience.",
            "mention_or_use": "mention",
            "llm_model_name": "GeoGalactica",
            "llm_model_description": "Referenced as a scientific LLM tailored to geoscience tasks (cited preprint); the paper cites GeoGalactica as related work but does not apply it.",
            "task_goal": "Cited example of domain-specialized LLMs capable of geoscience knowledge synthesis.",
            "domain": "Geoscience (referenced)",
            "methodology": "Mentioned in related work; not used experimentally in this study.",
            "type_of_qualitative_law": "Not specified in this paper; implied potential to extract geoscience rules/relationships.",
            "evaluation_metrics": "",
            "results_summary": "Cited as relevant prior art; no experimental results in this paper.",
            "human_involvement": "",
            "dataset_or_corpus": "",
            "limitations_or_challenges": "",
            "notable_examples": "",
            "uuid": "e5764.5",
            "source_info": {
                "paper_title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "GeoGalactica: A Scientific Large Language Model in Geoscience.",
            "rating": 2,
            "sanitized_title": "geogalactica_a_scientific_large_language_model_in_geoscience"
        },
        {
            "paper_title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
            "rating": 2,
            "sanitized_title": "k2_a_foundation_language_model_for_geoscience_knowledge_understanding_and_utilization"
        },
        {
            "paper_title": "Large Language Models for Scientific Synthesis",
            "rating": 2,
            "sanitized_title": "large_language_models_for_scientific_synthesis"
        },
        {
            "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "rating": 1,
            "sanitized_title": "a_survey_on_hallucination_in_large_language_models_principles_taxonomy_challenges_and_open_questions"
        },
        {
            "paper_title": "Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy",
            "rating": 1,
            "sanitized_title": "harnessing_the_power_of_adversarial_prompting_and_large_language_models_for_robust_hypothesis_generation_in_astronomy"
        }
    ],
    "cost": 0.013817749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis</p>
<p>Luoqi Wang 
School of Earth Sciences
Zhejiang University
310027HangzhouChina</p>
<p>Zhejiang Provincial Key Laboratory of Geographic Information Science
310028HangzhouChina</p>
<p>Haipeng Li li.haipeng@ddeworld.org 
Deep-time Digital Earth Research Center of Excellence (Suzhou)
215347KunshanChina</p>
<p>Deep-time Digital Earth Research Center of Excellence (Hangzhou)
311121HangzhouChina</p>
<p>Linshu Hu hulinshu1010@zju.edu.cn 
School of Earth Sciences
Zhejiang University
310027HangzhouChina</p>
<p>Zhejiang Provincial Key Laboratory of Geographic Information Science
310028HangzhouChina</p>
<p>Jiarui Cai 
School of Earth Sciences
Zhejiang University
310027HangzhouChina</p>
<p>Zhejiang Provincial Key Laboratory of Geographic Information Science
310028HangzhouChina</p>
<p>Zhenhong Du 
School of Earth Sciences
Zhejiang University
310027HangzhouChina</p>
<p>Zhejiang Provincial Key Laboratory of Geographic Information Science
310028HangzhouChina</p>
<p>Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis
7B88092CBA132730B21B23639D27BC7A
The reconstruction of Earth's history faces significant challenges due to the nonunique interpretations often derived from rock records.The problem has long been recognized but there are no systematic solutions in practice.This study introduces an innovative approach that leverages Large Language Models (LLMs) along with retrieval augmented generation and real-time search capabilities to counteract interpretation biases, thereby enhancing the accuracy and reliability of geological analyses.By applying this framework to sedimentology and paleogeography, we demonstrate its effectiveness in mitigating interpretations biases through the generation and evaluation of multiple hypotheses for the same data, which can effectively reduce human bias.Our research illuminates the transformative potential of LLMs in refining paleoenvironmental studies and extends their applicability across various sub-disciplines of Earth sciences, enabling a deeper and more accurate depiction of Earth's evolution.</p>
<p>INTRODUCTION</p>
<p>The Earth's rock records hold a wealth of information about the planet's history, climate, and the life forms (Boucot et al., 2013;Talent, 2012;Torsvik &amp; Cocks, 2016) that have inhabited it.Geologists interpret these records to reconstruct past environments and understand the processes that have shaped our planet (Bridge &amp; Demicco, 2008;Keighley, 2013).However, interpretation is often fraught with challenges due to the underdetermination of theory by data and human bias.</p>
<p>In Earth science, underdetermination manifests when data do not provide enough information to conclusively support one interpretation over another (Kleinhans et al., 2010;Turner, 2005).The phenomenon of underdetermination has long been recognized (Gilbert, 1886), with various other terms like nonuniqueness (Kim &amp; Ivanov, 2014;Oreskes et al., 1994), equifinality (Chorley, 1962;Nicholas &amp; Quine, 2010), and ambiguity (Hanneson, 2022;Roy, 1962) being used to convey the same idea.Well-known examples of nonunique interpretations include controls on stratigraphic patterns (Burgess &amp; Prince, 2015;Madof et al., 2016), fault-plane solutions (Brinkman et al., 2021), causes for seismic velocity anomaly and tomography (Rawlinson et al., 2014), and implications of δ18O of benthic foraminifera (Boer et al., 2010), to name just a few.</p>
<p>Additionally, we are all biased in terms of education and prior experience, and this affect how we interpret the rock records (Li &amp; Plink-Björklund, 2019).For instance, the early debate on Neptunism and Plutonism is in part due to different field experience of Abraham G. Werner and James Hutton, where Werner's fieldwork is mostly confined to the Erzgebirge and neighboring areas, while Hutton was exposed to the Midland Valley of Scotland that contains numerous basic sills from which he made the critical observations that convinced him of the plutonic origin of rocks (Hallam, 1990).Other examples relate to the various forms of cognitive biases (Tversky &amp; Kahneman, 1974).For example, the availability of different theoretical frameworks across disciplines is a key driver of disciplinary bias, leading to divergent interpretations of the same underlying information.A seismologist is apt to interpret seismic velocity anomalies in terms of temperature variations, while a petrologist is more likely to interpret the anomalies caused by variations in mineralogy, crystal orientation or partial melt content (Anderson, 2007).</p>
<p>To effectively counteract the underdetermination, T. C. Chamberlin (1890) proposed the method of multiple working hypotheses, which is perhaps the best solution in theory.Some of the prominent success of this method include J. T. Wilson's discovery of transform faults, which laid the foundation of plate tectonics (Laudan, 1980).However, due to our own limitation in training, experiences, and psychological reasons, the method is more widely advocated (Chamberlin, 1890;Dalrymple &amp; James, 2010;Platt, 1964) than practiced (Yanco et al., 2020), even for Chamberlin himself in the case of Chamberlin-Moulton planetesimal theory (Brush, 1978).The exponential increase of publications (Bornmann et al., 2021) further worsen the issue; it is becoming exceedingly difficult to keep up with publications, while at the same time getting a good grasp on the current and existing literature serves as an importance source for ideas and hypotheses, especially considering the fact that Earth science relies heavily on case-based reasoning, the situation seems a little bit hopeless if we cannot keep up with the ever-growing cases.</p>
<p>Methods for dealing with human bias in decision have also been proposed, namely through the application of statistical models and algorithms (Green &amp; Chen, 2019;Meehl, 1954).Statistical models and algorithms have been found to consistently outperform experts in many tasks (Dawes et al., 1989;Grove et al., 2000), and biases in algorithms can be corrected much easily compared to changing the human decision-making process.This is because algorithms are often more transparent and explainable than human decision-making processes, and they can be systematically updated and corrected as new data becomes available or as biases are identified.There have been attempts to create algorithms and systems to facilitate paleoenvironmental reconstruction at the outcrop and basin scale based on knowledge engineering (Miller, 1993;H. Wang et al., 2023;Zhang et al., 2023), and they have achieved certain success.However, there are three major challenges.One is the construction of a knowledge base, which often takes a long time and lots of effort; in addition, regularly updating the knowledge base is also no small task.The second is that such a knowledge-based system often requires semantic parsing, which can be difficult for lots of semi-structured or unstructured data in papers and databases such as GeoLexicon (Du et al., 2023).</p>
<p>The rapid development of large language models (LLMs) (Kaddour et al., 2023;Kojima et al., 2022) has brought new opportunities for solving the underdetermination and bias problems in paleoenvironmental reconstruction.Such models, such as ChatGPT and Claude, are trained on a large corpus of text covering essential all areas of human knowledge (Jiang et al., 2023).At the same time, they can be quite effective for understanding natural language queries (Zhao et al., 2023), reasoning about tasks (Wei et al., 2023), and generating ideas and hypotheses (Wei et al., 2022).A preliminary test suggests large language models can be effective in paleoenvironmental reconstruction.When given proper prompt and a description of the field observation, ChatGPT can give the depositional processes, list corresponding possible environments ranked by their probability, modern analogs, and references.</p>
<p>However, there are two main problems need to be addressed before we can use it for paleoenvironmental interpretation.The first problem is what has been called AI hallucination (Huang et al., 2023); put it another way, how can we minimize the proportion of content that is made up by LLMs?The other problem is how to make the LLMs to keep up with the latest research findings when they are initially trained on text that typically has a cut-off date, such as 2022.</p>
<p>To get the most out of LLMs in the application of paleoenvironmental reconstruction, our solution is to use chain-of-thought (Wei et al., 2023), proper prompt based on our aim (Beurer-Kellner et al., 2023), retrieval-augmented generation (Gao et al., 2024), and online search on top of GPT4 API.</p>
<p>DATA AND METHOD</p>
<p>Data and Pre-processing</p>
<p>The quest to reconstruct Earth's history is a complex and intricate endeavor that has long captivated geologists and Earth scientists (Milanovsky, 2007).The rock record, our primary source of information about the Earth's past, is a treasure trove of data that, when interpreted correctly, can reveal the planet's dynamic history (Duarte et al., 2021;Hedberg, 1965).However, this scientific methodology is not without its challenges.Nonunique interpretations often arise from the rock record, complicating our understanding of the geological history of the Earth (Hamilton, 2019;Saltus &amp; Blakely, 2011).Meanwhile, interpreting based on incomplete evidence is actually a rule in geology, rather than an exception (Frodeman, 1995).More reliable research materials can support more robust and truthful theoretical hypotheses, just as the theory of Sea-Floor Spreading is formed and supported by the integration of a large number of observations and hypotheses, and becomes a further development of the Continental Drift (Heirtzler, 1968).Based on this background, we hope to reduce bias in information acquisition and evidence interpretation by integrating relevant data materials and knowledge in the geological field to assist in hypothesis generation and evaluation.</p>
<p>For the above purpose, we searched and obtained thousands of literatures in the field of paleogeography and sedimentology to obtain their meta information, as the essence of domain knowledge.Our metadata dataset contains basic information about articles, including DOI, abstract, title, authors, publication date, etc., for reference.For the obtained metadata, the main focus is on processing the abstract fields.We segmented the abstract text into paragraphs and sentences, and obtained query tables for paragraphs and statements.We introduced a sentence transformer model (sentence-transformers/all-MiniLM-L6-v2) to maps these paragraphs and statements to a 384-dimensional dense vector space, and stored them in a vector database for retrieving.</p>
<p>Expert Question and Answer System</p>
<p>The core abilities of large language models (LLMs), such as ChatGPT, encompass language generation and emergence ability, enabling it to exhibit emergent behavior while generating coherent and contextually relevant text (Ouyang et al., 2022;Wei et al., 2022).Since the advent of ChatGPT, LLMs show great potential in assisting scientific research.The application of LLMs in disciplinary vertical fields, such as law, medical, chemistry and economics, is constantly evolving (Cui et al., 2023;Dan et al., 2023;L. Wang et al., 2023;N. Wang et al., 2023;Xiong et al., 2023).The ability of LLMs to integrate, understand, and apply scientific literatures in reasoning and interpretation has been widely reflected in existing research (Jablonka et al., 2023;Miret &amp; Krishnan, 2024;Zheng et al., 2023).Their ability to summarize and generalize data that far exceeds human capabilities allows them to explore and present diverse perspectives, solutions, and creative ideas.Models such as K2 and GeoGalactica demonstrate the potential for conducting knowledge reasoning and thought generation tasks in the field of geology to discover potential undiscovered relationships (Deng et al., 2023;Lin et al., 2023).This, in turn, brings new insights and possibilities for problem-solving and scientific research.</p>
<p>However, despite its impressive capabilities, LLMs may encounter challenges such as hallucination, which means the information they generate is nonsensical or unfaithful (Ji et al., 2023).This issue arises due to the models' ability to generate text only based on patterns and associations learned from vast amounts of training data, without necessarily verifying the authenticity of the associations (Huang et al., 2023;Ji et al., 2023).Furthermore, while LLMs excel in generating general text, their performance in specialized or professional domains may be insufficient.This limitation stems from their lack of domain-specific knowledge and expertise, making them less reliable for tasks that require in-depth understanding and specialized knowledge (Kaddour et al., 2023b;Zhao et al., 2023).</p>
<p>To enhance the ability of LLMs in professional fields and meet the demands of solving the problem of scientific research bias caused by insufficient knowledge acquisition, simultaneously addressing hallucination issues, several solutions can be considered.These include fine-tuning the models on domain-specific data, incorporating external knowledge sources to improve accuracy, and developing specialized models tailored to specific professional domains (Ciucă et al., 2023;Huang et al., 2023;Ouyang et al., 2022).Additionally, implementing mechanisms for factchecking and verification can help mitigate the issue of hallucination and ensure the generation of reliable and trustworthy information (Pan et al., 2023).In the subsequent discussion, we outline our methodology, which involves configuring specialized knowledge bases along with real-time external knowledge sources, coupled with the rethink mechanism, aimed at alleviating hallucination and bolstering the system's proficiency within professional domains.</p>
<p>Figure 1.The foundational architecture of the expert question and answer system As illustrated in Figure 1, we have developed an expert question and answer (QA) system leveraging LLMs.This system is adept at responding to user queries by accessing and assimilating information from domain-specific knowledge repositories as well as dynamic, real-time external data sources.This augmentation significantly bolsters the analytical and cognitive capabilities of LLMs within targeted domains, thereby facilitating the provision of expert-level responses.The functional architecture of the expert system can be delineated into the following three modules:</p>
<p>(1) LLM module, (2) data retrieval and retrieval module, and (3) generation module.</p>
<p>LLM module</p>
<p>The LLM module functions as the central processing unit of the expert system, receiving diverse inquiries from users and generating responses accordingly.This module enhances the system's cognitive capabilities by leveraging the services of language models such as GPT-4.The core capability of the expert question and answer system is built on the generation and emergence capabilities of LLMs.The generation ability of LLMs is an important feature that enables the expert system to generate text with coherence and contextual relevance.This ability arises from their training on extensive corpora of text data, allowing LLMs to learn patterns, linguistic structures, and semantic relationships (Huang et al., 2023;Kojima et al., 2023;Naveed et al., 2023;Zhao et al., 2023).The concept of emergence refers to the phenomenon where simple components interact and reach a certain scale, resulting in complex behaviors and patterns.In the case of LLMs, emergence is observed as they generate text that exhibits a deep understanding of language, despite not being explicitly programmed for specific tasks.The emergence ability of LLMs is based on the number of model parameters and the amount of computation, and sourced from the vast and diverse text data they are trained on, which provides them with a rich knowledge base to draw upon (Arora &amp; Goyal, 2023;Wei et al., 2022).To enhance the efficacy of LLMs in generating outputs aligned with our expectations and specific problem contexts, the utilization of the "chain of thought" (CoT) technique is prevalent within the system.This approach significantly enhances the multi-step reasoning capabilities of LLMs.Through the strategic insertion of prompts like "step by step," LLMs autonomously establish logical connections among diverse concepts, fostering the construction of coherent and contextually relevant responses (Wei et al., 2022;Yao et al., 2023).</p>
<p>In our expert system, we utilize the CoT to replicate expert cognitive processes for question analysis.Through this approach, user-inputted questions are systematically deconstructed into manageable sub-queries, facilitating a structured examination.The application of CoT is realized through prompt engineering, a method wherein researchers and developers strategically craft input prompts.These prompts are tailored considering linguistic nuances, contextual factors, and structural elements, while also providing explicit instructions or cues.This meticulous design aims to guide the model towards generating responses that adhere to specific criteria or objectives (Beurer-Kellner et al., 2023;Jiang et al., 2023;Zhao et al., 2023).Prompt engineering encompasses diverse strategies, such as incorporating explicit instructions, specifying desired output formats, or integrating supplementary contextual information.By adopting this approach, LLMs are trained to emulate expert reasoning, engaging in a systematic, step-by-step thought process.Through ingeniously crafted prompts, LLMs can develop coherent thought chains, gradually unraveling complex problems.</p>
<p>Data search and retrieval module</p>
<p>The data search and retrieval module provide knowledge support for LLM module.Upon decomposition of user queries into subqueries by LLMs, these subqueries are sequentially inputted into this module.Through the retrieval of information pertinent to each subquery, the module facilitates the provision of highly correlated content, thereby aiding LLMs in problem-solving and reevaluation.</p>
<p>The retrieval module described herein comprises two primary data sources: a vector database housing professional data and search engines for accessing public fundamental data.</p>
<p>The vector database stores disciplinary literature, serving as a repository for professional data.</p>
<p>Through the extraction and embedding of literature data, textual content is transformed into vectors and stored in the database, facilitating efficient retrieval and computation (Pan et al., 2023).</p>
<p>Subsequent retrieval from the vector database involves employing the vector similarity algorithm, wherein the Euclidean Distance metric is utilized for calculating vector similarity.Alternatively, employing Inner Product (IP) or other vector similarity algorithms may yield comparable or superior results.Upon querying the vector database, the algorithm searches for records containing the nearest N sentence or paragraph vectors within the vector knowledge base.Concurrently, search engines are engaged to process subqueries by leveraging common Internet search services, with Google serving as the primary example.Other search engines such as Bing and Baidu can serve as viable alternatives.Information retrieved from both databases and search engines undergoes embedding via a sentence transformer model.Subsequently, autocorrelation calculations are applied to filter out items exhibiting the highest autocorrelation.A notable challenge with information exhibiting high autocorrelation is the typically extended length of their content.Addressing this challenge, contextual compression is employed to retrieve and compress the context of a given content, eliminating extraneous content while retaining the most pertinent information relevant to the query problem.Finally, the returned information is standardized to include three essential fields: source, title, and content.</p>
<p>From the perspective of the respective functions of search engines and vector databases in the data search and retrieval module, search engines play a vital role in furnishing essential reasoning materials for LLMs, thus establishing a foundational basis for expert system capabilities.</p>
<p>Simultaneously, scientific research-based knowledge stored in vector databases enriches and supplements the expertise of expert systems, elevating their proficiency within specialized domains.</p>
<p>Generation module</p>
<p>The generation module serves as the primary functional component within this system, receiving outputs from the data retrieval module and invoking the LLM module to summarize and condense information, ultimately generating conclusions.</p>
<p>Retrieval-augmented generation (RAG) represents an innovative approach amalgamating retrieval-based methods with generative models, typically Language Models (LMs), to enrich the quality and relevance of generated text (Ciucă et al., 2023;Gao et al., 2024).This article's expert question and answer system embodies the concept of RAG.In this paradigm, LLMs retrieve input questions (segmented into subqueries via CoT) from the data retrieval module to acquire information for enhancing generation.Subsequently, LLMs reconsider both the acquired information and original questions.During the reconsideration phase, prompt engineering, leveraging task description, input data, contextual information, and prompt style, enriches and standardizes LLM output (Zhao et al., 2023), enabling LLM to produce answers meeting predefined standards, akin to those of domain experts or proficient students.</p>
<p>In the realm of RAG research, advanced and modular RAG methodologies are evolving from the original or naive RAG approach (Gao et al., 2024).Advanced RAG incorporates additional processing stages pre-and post-retrieval.Pre-retrieval processing concentrates on optimizing data indexing via various methods, including data granularity refinement, index structure optimization, and metadata incorporation to enhance retrieval content quality.Postretrieval processing involves reranking and prompt compression, with embedding playing a crucial role.Optimization strategies encompass fine-tuning embedding or employing dynamic embedding methods.Embracing the design principles of Advanced RAG, we enhance the pre-retrieval stage with query planning and expansion, while bolstering post-retrieval stages with reranking and summarization techniques.</p>
<p>RESULT AND DISCUSSION</p>
<p>System Performance</p>
<p>We developed an expert question and answer (Q&amp;A) system accessible through a userfriendly web application.Users input their queries on the web page and specify the relevant domain, prompting the system to generate structured responses.The interface offers real-time insights into the system's cognitive process and presents the finalized answers, which users can export in Portable Document Format (PDF).</p>
<p>Our system draws upon a comprehensive knowledge base curated from metadata extracted from a vast corpus of sedimentology literature.In selecting the LLM, we opted for GPT-4 due to its superior performance characteristics.</p>
<p>To assess the system's efficacy, we conducted a human evaluation focusing on its utility by engaging domain experts.This evaluation involved posing various questions to the system and qualitatively assessing the appropriateness of its responses.An illustrative example is provided in the Appendix.Feedback from experts in paleogeography and sedimentology indicates that the system's responses closely approximate those of proficient students or researchers in the respective fields.</p>
<p>The expert system showcases its capacity to generalize and comprehend extensive geological knowledge, facilitating multi-perspective analysis of geological phenomena and hypotheses.For instance, when tasked with proposing hypotheses for specific geological phenomena, the system generates multiple hypotheses accompanied by relevant analyses, ranking them based on plausibility.These results generally align with the consensus among domain experts.As shown in Figure 2 (some repeated processes are omitted), the paleogeography expert question and answer system outputs the process of its thinking, making it easier for users to understand how it operates.In the case, the expert system first defines itself as an expert agent in the field of paleogeography, and begins to think about user input question.In system output, it can be seen that the user input question has been split into four queries: 'causes of Miocene climatic</p>
<p>Discussion</p>
<p>The uplift of the Tibetan Plateau stands as the most probable cause for the shift to a monsoon-dominant climate in Early Miocene China.This hypothesis is supported by the temporal correlation between the plateau's uplift and the intensification of the monsoon, as well as the physical mechanism by which the plateau could influence atmospheric circulation… ...</p>
<p>Conclusion</p>
<p>In conclusion, the uplift of the Tibetan Plateau is the most probable cause for the shift to a monsoondominant climate in Early Miocene China, with tectonic activity and regional uplift also playing significant roles.Global cooling and changes in ocean circulation provide additional context for understanding the complex interplay of factors that contributed to this climatic transformation.Further research, particularly involving deep ocean sampling and high-resolution paleoclimatic records, is necessary to refine our understanding of these processes and their relative contributions to the development of the monsoon system.References Plateau.This indicates that our system has indeed enhanced its generated reliability through RAG.</p>
<p>Ablation Experiment</p>
<p>To demonstrate the effectiveness of the retrieval enhanced design in the expert system in generating of LLMs, we conducted an ablation experiment.We separately removed the support of vector database retrieval and search engine search to compare with the complete expert question and answer system.We have invited experts in the field to provide more than ten professional questions for experimentation, all of which require multiple hypothesis explanations and rankings for the given descriptions or questions themselves.We analyzed the effectiveness of retrieval enhancement by comparing the hypothesis ranking of answers between the ablation group and the control group.</p>
<p>Figure 3.Comparison of ablation experiment results.They are respectively plain GPT-4, Expert system that only searches databases (System-DB-noSE), Expert system that only searches web pages (System-noDB-SE), and Expert system that searches databases and web pages (complete expert system, System-DB-SE).</p>
<p>We leverage GPT-4 for LLM selection.As depicted in Figure 3, employing questions and answers crafted by the expert system results in significantly enriched content compared to direct outputs from GPT-4.This improvement stems from the formulation of proper question prompts within the system, which stimulate the generation of multiple hypotheses subsequently ranked A decrease in the δ18O value of carbonate shells of benthic foraminifera can be attributed to several hypotheses, each with varying likelihoods.Here are some possible explanations ranked by their probability:</p>
<ol>
<li>
<p>Temperature change hypothesis: One of the most common reasons for a decrease in the δ18O value is a change in the temperature of the surrounding water.Generally, colder water tends to have lower δ18O values.Therefore, a decrease in the δ18O value of carbonate shells could indicate a period of colder water temperatures.This hypothesis is relatively common and generally has a high probability due to the established relationship between water temperature and δ18O values.</p>
</li>
<li>
<p>Changes in ice volume hypothesis: Another possible explanation is changes in the volume of ice on Earth.During periods of glacial advance, there is a higher proportion of heavy oxygen isotopes (δ18O) in the oceans, resulting in an increase in the δ18O values of foraminifera shells.On the other hand, during periods of deglaciation, the δ18O values decrease as lighter isotopes are favored.Given the established connection between ice volume and δ18O values, this hypothesis is also quite probable.</p>
</li>
</ol>
<p>Changes in salinity hypothesis:</p>
<p>Variations in the salinity of the water can also influence the δ18O values of foraminifera shells.Higher salinity levels tend to result in higher δ18O values, while lower salinity leads to lower δ18O values.However, this hypothesis is usually considered less likely compared to temperature and ice volume changes, as salinity fluctuations often have a more localized effect and may not be as widespread.</p>
<ol>
<li>Biological processes hypothesis: Changes in the metabolic activity of foraminifera or other biogeochemical processes can also impact the δ18O values of their shells.For example, different species of foraminifera may calcify their shells at varying rates, affecting the isotopic composition.While this hypothesis cannot be ruled out entirely, it is generally less favored compared to environmental factors like temperature and ice volume changes due to the less direct and more complex relationship between biology and isotopic values.</li>
</ol>
<p>In summary, a decrease in the δ18O value of carbonate shells of benthic foraminifera is most likely due to changes in temperature or ice volume, followed by possible influences from salinity levels or biological processes.Temperature and ice volume changes have more robust evidence supporting their correlation with δ18O values, making them the leading hypotheses for explaining such variations.</p>
<p>Interpretation of Decreased δ18OValues in Benthic Foraminifera Carbonate Shells Introduction</p>
<p>The stable oxygen isotope ratio, commonly denoted as δ18O, in carbonate shells of benthic foraminifera is a widely utilized proxy in paleoceanography and paleoclimatology.Variations in δ18O values are interpreted to reflect changes in past ocean temperatures and ice volume, as well as local factors such as salinity and diagenesis.A decrease in δ18O values in benthic foraminifera carbonate shells can be indicative of several environmental and geochemical processes.This report aims to explore the different hypotheses that explain a decrease in δ18O values, ranking them by their probability and providing an in-depth analysis of each scenario.</p>
<p>Hypotheses for Decreased δ18O Values</p>
<p>Ocean Warming</p>
<p>The most probable hypothesis for a decrease in δ18O values in benthic foraminifera shells is ocean warming.The δ18O of seawater becomes more negative with increasing temperature, leading to the incorporation of lighter oxygen isotopes (16O) into the carbonate shells of foraminifera.This inverse relationship between temperature and δ18O is well-established and has been used to reconstruct past ocean temperatures (Lear, Elderfield, &amp;Wilson, 2000).</p>
<p>Ice Volume</p>
<p>Analysis of Hypotheses</p>
<p>Ocean Warming</p>
<p>The relationship between temperature and δ18O values is based on the principle that at higher temperatures, the fractionation between 18O and16O is reduced, leading to a greater incorporation of 16O into the carbonate structure.This temperature effect is quantified using the paleotemperature equation, which allows for the estimation of past ocean temperatures fromδ18O measurements (Epstein et al., 1953).The reliability of this method has been confirmed through numerous studies, making it a highly probable explanation for decreased δ18O values.</p>
<p>...</p>
<p>Conclusion</p>
<p>In conclusion, the most probable explanation for a decrease in δ18O values in benthic foraminifera carbonate shells is ocean warming, followed by a decrease in global ice volume.Freshwater input, changes in seawater salinity, diagenetic alteration, and photosynthetic activity are also plausible hypotheses but are generally considered secondary factors.The interpretation of δ18O records requires a comprehensive approach that considers multiple environmental and geochemical proxies to disentangle the complex interplay of factors influencing the isotopic composition of foraminiferal shells.</p>
<p>Introduction</p>
<p>The stable oxygen isotope ratio, commonly expressed as δ18O, in carbonate shells of benthic foraminifera is a widely used proxy in paleoceanography and paleoclimatology.Variations in δ18O values are interpreted to reflect changes in past environmental conditions, such as seawater temperature, ice volume, and salinity.A decrease in δ18O values in foraminiferal shells can signify several different processes, and understanding these changes is crucial for reconstructing Earth's climatic history.This report aims to explore the different hypotheses explaining a decrease in δ18O values of benthic foraminifera carbonate shells, ranking them by their probability based on the current scientific understanding.</p>
<p>Hypotheses Explaining Decrease in δ18OValues</p>
<p>Hypothesis 1: Seawater Temperature Increase</p>
<p>The most widely accepted explanation for a decrease in δ18O values in foraminiferal shells is an increase in seawater temperature.The oxygen isotopic composition of shell carbonate reflects the temperature of the ambient water during shell formation.As water temperature rises, the fractionation of oxygen isotopes changes, leading to a lower δ18O value in the carbonate shells (Epstein et al., 1951;Epstein et al., 1953;Grossman &amp;Ku, 1986).This relationship is well-established and has been used to reconstruct past seawater temperatures from foraminiferal δ18O records (Wefer &amp; Berger, 1991).</p>
<p>Ranking of Hypotheses</p>
<p>Based on the current scientific consensus and the available literature, the hypotheses can be ranked by their probability as follows:</p>
<ol>
<li>
<p>Seawater Temperature Increase: This is the most probable explanation for a decrease in δ18O values in foraminiferal shells, given the well-established temperature dependence of oxygen isotope fractionation in carbonate minerals.</p>
</li>
<li>
<p>Changes in Seawater δ18O Composition: ... 3. Freshwater Influx: ... 4. Vital Effects: ... 5. Diagenetic Alteration: ...</p>
</li>
</ol>
<p>Conclusion</p>
<p>In conclusion, a decrease in δ18O values of benthic foraminifera carbonate shells is most commonly attributed to an increase in seawater temperature or changes in the δ18O composition of seawater.These interpretations are supported by a robust theoretical framework and extensive empirical data.While other factors such as diagenetic alteration, vital effects, and fresh water influx can also contribute to δ18O variability, they are generally considered secondary to the primary drivers of temperature and seawater δ18O composition.Careful consideration of the geological context, alongside multiple lines of evidence, is essential for accurate paleoenvironmental reconstructions based on foraminiferal δ18O records.</p>
<p>References</p>
<p>Epstein, S., Buchsbaum, R., Lowenstam, H. A., &amp; Urey, H. C. (1951).Carbonatewater isotopic temperature scale.Bulletin of the GeologicalSociety of America, 62(4), 417-426.</p>
<p>...</p>
<p>Understanding the Decrease inδ18O Values of Benthic Foraminifera Carbonate Shells</p>
<p>The isotopic composition of foraminifera, particularly the ratio of oxygen isotopes δ18O, serves as a crucial proxy in paleoceanography and paleoclimatology.A decrease in the δ18O value of carbonate shells of benthic foraminifera can be indicative of several environmental changes.This report will explore the various hypotheses that explain a decrease inδ18O values, ranking them by their probability based on the current understanding of geochemical processes and sedimentological records.</p>
<p>Hypotheses for Decrease in δ18O Values</p>
<p>Global Ice Volume Decrease</p>
<p>The most widely accepted hypothesis for a decrease in δ18O values in benthic foraminifera is a reduction in global ice volume.As continental ice sheets melt, the lighter 16O isotope, which is preferentially stored in ice, is released into the ocean, thus decreasing the δ18O value of seawater.This process is well-documented and is considered the primary driver of changes in δ18O values in foraminifera carbonates (Shackleton, 1967).The correlation between ice volume and δ18O values is a cornerstone of paleoclimatic reconstruction, making this hypothesis the most probable.</p>
<p>Evaluation of Hypotheses</p>
<p>The probability of each hypothesis is evaluated based on the consistency of the isotopic record with known environmental and geochemical processes.The global ice volume hypothesis is the most probable due to its strong correlation with the δ18O record and its global impact on seawater composition.The temperature hypothesis is plausible but less likely to be the sole cause of a decrease in δ18O values, especially for benthic foraminifera.Changes in salinity are regionally important but may not always be reflected in the global δ18O record.Diagenetic alteration is a known factor but can be distinguished from primary signals in most cases.Local hydrological changes are significant but typically affect surface waters more than the deep ocean where benthic foraminifera reside.</p>
<p>Conclusion</p>
<p>In conclusion, a decrease in the δ18O value of benthic foraminifera carbonate shells is most likely indicative of a reduction in global ice volume, with secondary influences from sea surface temperature increases, changes in seawater salinity, diagenetic alteration, and local hydrological changes.The integration of multiple geochemical proxies, including δ18O, δ13C, and trace element ratios, along with careful consideration of diagenetic overprints, continues to refine our understanding of past oceanographic and climatic conditions.based on their probability.Consequently, all four groups of answers satisfy the specified questioning criteria.</p>
<p>Regarding writing style and citation practices, the effectiveness of the last three responses is attributed to the proper prompt, where these responses draw upon retrieved references.As illustrated in Figure 3, the comprehensive expert system demonstrates a greater number and density of cited references compared to the ablation group.Notably, the hypotheses proposed by all three systems demonstrate scientific validity.Additionally, the quality of question answering indicates that expert systems utilizing databases and online data retrieved from web pages outperform others.</p>
<p>From a logical mechanism standpoint, the system incorporates CoT.CoT enhances model capability in comprehending and generating coherent text by integrating coherence and logic tasks.Specifically, CoT operates through a series of logic-based subprocesses for generation.This approach necessitates strict logical coherence between model-generated answers and posed questions.Consequently, this training regimen directs the model's attention towards context and logical coherence, thereby improving the consistency and coherence of generated responses.Moreover, the system utilizes RAG, which integrates retrieval and generation methodologies within its mechanism, prioritizing the extraction of information from relevant knowledge during answer generation.Initially, expert system employs a retrieval module to gather relevant documents or passages based on input questions.Subsequently, the generation module utilizes this retrieved knowledge in conjunction with the posed question to formulate answers.This mechanism facilitates the integration of external knowledge, thereby providing more precise and comprehensive information support.By incorporating retrieval knowledge, system achieves higher accuracy and content richness in generated responses.</p>
<p>In summary, the combination of proper prompt, CoT, and RAG enhances response logic and consistency by training models to produce coherent text while leveraging retrieval knowledge to deliver more accurate and content-rich answers.This synergy equips large models with an improved understanding of context, consideration of logical relationships, and integration of external knowledge, thereby enhancing the quality of generated responses.</p>
<p>CONCLUSION</p>
<p>Our study introduces an innovative approach that leverages LLMs along with retrieval augmented generation and real-time search capabilities to counteract interpretation biases.</p>
<p>Through ablation experiments, we have substantiated notable enhancements in response quality.</p>
<p>The integration of LLMs, chain-of-thought, contextually appropriate prompts tailored to our objectives, and retrieval-augmented generation surpasses the efficacy of employing standalone LLMs.Moreover, we emphasize that the quality of responses is intricately linked to the accessibility of pertinent data during retrieval processes.The potential of this approach is profound, poised to revolutionize our research methodologies.Our research illuminates the transformative potential of LLMs in refining paleoenvironmental studies and extends their applicability across various sub-disciplines of Earth sciences, enabling a deeper and more accurate depiction of Earth's evolution.</p>
<p>Figure 2 .
2
Figure 2. System operation example</p>
<p>✅</p>
<p>What are the possible causes for the earliest Miocene shift from a planetary-winddominant type to a monsoon-dominant type in China?List different hypotheses ranked by their probability and explain why.UserPaleogeography Expert Q&amp;A System Thinking about research questions for the task... Running research for 'What are the possible causes for the earliest Miocene shift from a planetarywind-dominant type to a monsoon-dominant type in China?List different hypotheses ranked by their probability and explain why.'... Paleogeography Agent I will conduct my research based on the following queries: ['causes of Miocene climatic shift to monsoon dominance in China', 'hypotheses on Miocene planetary wind to monsoon transition probability', 'explanations for early Miocene monsoon intensification in China 2024', 'What are the possible causes for the earliest Miocene shift from a planetary-wind-dominant type to a monsoon-dominant type in China?List different hypotheses ranked by their probability and explain why.']... Running research for 'causes of Miocene climatic shift to monsoon dominance in China' in internet... Adding source url to research: https://www.sciencedirect.com/science/article/abs/pii/S092181812300320X✅ Adding source url to research: https://en.wikipedia.org/wiki/Miocene... Researching for relevant information... Summarizing: causes of Miocene climatic shift to monsoon dominance in China Source: https://en.wikipedia.org/wiki/MioceneTitle: Miocene Content: The global trend was towards increasing aridity caused primarily by global cooling reducing the ability of the atmosphere to absorb moisture,[19] particularly after 7 to 8 million years ago.[20]Uplift of East Africa in the late Miocene was partly responsible for the shrinking of tropical rain forests in that region,[21] and Australia got drier as it entered a zone of low rainfall in the Late Miocene… … Running research for 'causes of Miocene climatic shift to monsoon dominance in China' in DB… Summarizing: causes of Miocene climatic shift to monsoon dominance in China Source: https://doi.org/10.1016/j.sedgeo.2021.106030Title: Damp-to dry aeolian systems: Sedimentology, climate forcing, and aeolian accumulation in the Late Cretaceous Liyou Basin, South China Content: Aeolian desert systems were widely distributed in South China during the Late Cretaceous.These systems developed under the control of subtropical highs and a monsoon climate.They reveal the evolution of palaeoclimate and palaeogeography and the regional palaeowind pattern in the South China interior during this period.Based on the analyses of sedimentology and facies architecture, this study examines and reassesses the aeolian succession of the Upper Cretaceous Daijiaping Formation in the southern Liyou Basin of southeast China.... ✍ Writing: What are the possible causes for the earliest Miocene shift from a planetary-wind-dominant type to a monsoon-dominant type in China?List different hypotheses ranked by their probability and explain why....The Shift to a Monsoon-Dominant Climate in Early Miocene China: Evaluating Hypotheses Introduction The transition from a planetary-wind-dominant climate to a monsoon-dominant climate in China during the Early Miocene epoch represents a significant climatic shift with profound implications for the region's paleoenvironment and sedimentology.This report examines the potential causes of this climatic transformation, evaluating various hypotheses based on their likelihood and supporting evidence.The Miocene epoch, spanning from approximately 23 to 5 million years ago, was a period of considerable climatic change, marked by fluctuations in global temperatures and atmospheric compositions that influenced precipitation patterns and vegetation distribution (Miao et al., 2019).Hypotheses for the Climatic Shift Uplift of the Tibetan Plateau Probability: High The most widely supported hypothesis for the shift to a monsoon-dominant climate in Early Miocene China is the uplift of the Tibetan Plateau.The collision of the Indian subcontinent with Asia around 50 million years ago initiated the plateau's rise, which intensified during the Miocene… ... Global Cooling and Antarctic Ice Sheet Expansion Probability: Moderate ... Tectonic Activity and Regional Uplift Probability: Moderate to High ... Changes in Ocean Circulation and Sea Surface Temperatures Probability: Low to Moderate ...</p>
<p>Middle</p>
<p>Miocene disruption.(n.d.).Retrieved from https://en.wikipedia.org/wiki/MioceneMiao, Y., Wu, F., Warny, S., Fang, X.,Lu, H., Fu, B., … Shi, P. (2019).Miocene fire intensification linked to continuous aridification on the Tibetan Plateau.Geology, 47(4), 303-307.https://doi.org/10.1130/G45720.dominance in China', 'hypotheses on Miocene planetary wind to monsoon transition probability', 'explanations for early Miocene monsoon intensification in China 2024', 'What are the possible causes for the earliest Miocene shift from a planetary-wind-dominant type to a monsoon-dominant type in China?List different hypotheses ranked by their probability and explain why.'.Then, the expert system starts to query the four questions in turn on the Internet, obtain relevant links, and organize the content of the link page into the format of source, title, and content triplets.Similarly, the expert system then conducted sequential queries and content organization in the database.The final retrieved content is used for generating the final research report.The final generated report is shown on the right side of Figure2(with some paragraphs omitted).The format of this research report is very organized and basically meets the requirements of scientific research texts, including a title, outline structure, and paragraph content.The paragraphs include introduction, possibilities and explanations of various hypotheses, discussions, conclusions, and references.The statement in the report has citation sources, such as a viewpoint in the introduction, "The Miocene epoch, spanning from approximately 23 to 5 million years ago, was a period of considerable climatic change, marked by fluctuations in global temperatures and compositions that influenced precipitation patterns and vegetation distribution", that cited Miao's Miocene fire intensification linked to continuous aridification on the Tibetan</p>
<p>Understanding the Decrease in δ18O Values of Benthic Foraminifera Carbonate Shells References Shackleton. W Dansgaard, S J Johnsen, H B Clausen, D Dahl-Jensen, N S Gundestrup, C U Hammer, . . Jouzel, J , Nature. 36464341993. 1967Nature</p>
<p>D L Anderson, 10.1017/CBO9781139167291New Theory of the Earth. Cambridge University Press20072nd ed.</p>
<p>S Arora, A Goyal, 10.48550/arXiv.2307.15936arXiv:2307.15936A Theory for Emergence of Complex Skills in Language Models. 2023</p>
<p>Prompting Is Programming: A Query Language for Large Language Models. L Beurer-Kellner, M Fischer, M Vechev, 10.1145/3591300Proceedings of the ACM on Programming Languages. 72023</p>
<p>Cenozoic global ice-volume and temperature simulations with 1-D ice-sheet models forced by benthic δ18O records. B D Boer, R S W Wal, Van De, R Bintanja, L J Lourens, E Tuenter, 10.3189/172756410791392736Annals of Glaciology. 51552010</p>
<p>Growth rates of modern science: A latent piecewise growth curve approach to model publication numbers from established and new literature databases. L Bornmann, R Haunschild, R Mutz, 10.1057/s41599-021-00903-wHumanities and Social Sciences Communications. 812021</p>
<p>Phanerozoic Paleoclimate: An Atlas of Lithologic Indicators of Climate. A J Boucot, C Xu, C R Scotese, R J Morley, 10.2110/sepmcsp.112013SEPM Society for Sedimentary Geology</p>
<p>Earth Surface Processes, Landforms and Sediment Deposits. J Bridge, R Demicco, 10.1017/CBO97805118055162008Cambridge University Press</p>
<p>First Focal Mechanisms of Marsquakes. N Brinkman, S C Stähler, D Giardini, C Schmelzbach, A Khan, A Jacob, N Fuji, C Perrin, P Lognonné, E Beucler, M Böse, S Ceylan, C Charalambous, J F Clinton, M Van Driel, F Euchner, A Horleston, T Kawamura, B Knapmeyer-Endrun, W B Banerdt, 10.1029/2020JE006546Journal of Geophysical Research: Planets. 12642021</p>
<p>A Geologist among Astronomers: The Rise and Fall of the Chamberlin-Moulton Cosmogony, Part 1. S G Brush, 10.1177/002182867800900101Journal for the History of Astronomy. 911978</p>
<p>Non-unique stratal geometries: Implications for sequence stratigraphic interpretations. P M Burgess, G D Prince, 10.1111/bre.12082Basin Research. 2732015</p>
<p>The Method of Multiple Working Hypotheses. T C Chamberlin, 10.1126/science.ns-15.366.92ns-15Science. 3661890</p>
<p>. Chorley, 10.3133/PP500B1962Professional Paper</p>
<p>Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy. I Ciucă, Y.-S Ting, S Kruk, K Iyer, 2023</p>
<p>J Cui, Z Li, Y Yan, B Chen, L Yuan, 10.48550/arXiv.2306.16092arXiv:2306.16092ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. 2023</p>
<p>Facies Models 4. R W Dalrymple, N P James, 2010Geological Association of Canada</p>
<p>Y Dan, Z Lei, Y Gu, Y Li, J Yin, J Lin, L Ye, Z Tie, Y Zhou, Y Wang, A Zhou, Z Zhou, Q Chen, J Zhou, L He, X Qiu, 10.48550/arXiv.2308.02773arXiv:2308.02773EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education. 2023</p>
<p>Clinical Versus Actuarial Judgment. R M Dawes, D Faust, P E Meehl, 10.1126/science.2648573Science. 48991989</p>
<p>C Deng, T Zhang, Z He, Y Xu, Q Chen, Y Shi, L Fu, W Zhang, X Wang, C Zhou, Z Lin, J He, 10.48550/arXiv.2306.05064arXiv:2306.05064K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. 2023</p>
<p>Online data service for geologic formations (Lexicons) of China, India, Vietnam and Thailand with one-click visualizations onto East Asia plate reconstructions. W Du, S Mishra, J G Ogg, Y Qian, S Chang, K Oberoi, A Ault, S Zahirovic, H Hou, D S N Raju, O Mamallapalli, G M Ogg, H Li, C R Scotese, B Dong, 10.1002/gdj3.210Geoscience Data Journal. </p>
<p>A dynamic planet. J C Duarte, M Fakhraee, D Gürer, O Kwiecien, C I O Nichols, 10.1038/s43247-021-00246-zCommunications Earth &amp; Environment. 212021</p>
<p>Geological reasoning: Geology as an interpretive and historical science. R Frodeman, 10.1130/0016-7606107&lt;0960:GRGAAI&gt;2.3Geological Society of America Bulletin. 1071995. 1995</p>
<p>Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, Q Guo, M Wang, H Wang, 10.48550/arXiv.2312.10997arXiv:2312.10997Retrieval-Augmented Generation for Large Language Models: A Survey. 2024</p>
<p>The inculcation of scientific method by example, with an illustration drawn from the Quaternary geology of Utah. G K Gilbert, 10.2475/ajs.s3-31.184.284American Journal of Science. 1841886</p>
<p>The Principles and Limits of Algorithm-in-the-Loop Decision Making. B Green, Y Chen, 10.1145/3359152Proceedings of the ACM on Human-Computer Interaction. 3242019</p>
<p>Clinical versus mechanical prediction: A meta-analysis. W M Grove, D H Zald, B S Lebow, B E Snitz, C Nelson, Psychological Assessment. 1212000</p>
<p>Great Geological Controversies. A Hallam, 10.1093/oso/9780198582182.001.00011990Oxford University Press</p>
<p>Toward a myth-free geodynamic history of Earth and its neighbors. W B Hamilton, 10.1016/j.earscirev.2019.102905Earth-Science Reviews. 1981029052019</p>
<p>Coping with ambiguity in geophysical data. J E Hanneson, 10.1080/14432471.2022.2149999Preview. 2212022</p>
<p>Earth History and the Record in the Rocks. H D Hedberg, Proceedings of the American Philosophical Society. 10921965</p>
<p>J R Heirtzler, Sea-Floor Spreading. 1968219</p>
<p>L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, T Liu, 10.48550/arXiv.2311.05232arXiv:2311.05232A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. 2023</p>
<p>14 examples of how LLMs can transform materials science and chemistry: A reflection on a large language model hackathon. K Jablonka, Q Ai, A Al-Feghali, S Badhwar, D Bocarsly, J , M Bran, A Bringuier, S Catherine Brinson, L Choudhary, K Circi, D Cox, S Jong, W A De, L Evans, M Gastellu, N Genzling, J Victoria Gil, M , K Gupta, A Hong, Z Imran, A Blaiszik, B , 10.1039/D3DD00113JDigital Discovery. 252023</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y Bang, D Chen, H S Chan, W Dai, A Madotto, P Fung, 10.1145/3571730ACM Computing Surveys. 55122023</p>
<p>J Jiang, K Zhou, Z Dong, K Ye, W X Zhao, J.-R Wen, 10.48550/arXiv.2305.09645arXiv:2305.09645StructGPT: A General Framework for Large Language Model to Reason over Structured Data. 2023</p>
<p>J Kaddour, J Harris, M Mozes, H Bradley, R Raileanu, R Mchardy, 10.48550/arXiv.2307.10169arXiv:2307.10169Challenges and Applications of Large Language Models. 2023a</p>
<p>J Kaddour, J Harris, M Mozes, H Bradley, R Raileanu, R Mchardy, 10.48550/arXiv.2307.10169arXiv:2307.10169Challenges and Applications of Large Language Models. 2023b</p>
<p>No of pages: xiii + 768. Price US$ 151. ISBN 978-1-4443-4992-4 (hardback). D G Keighley, 10.1002/gj.2429Sedimentology and sedimentary basins: From turbulence to tectonics. Mike Leeder, Wiley-Blackwell, Chichester2013. 201148second edition</p>
<p>On the nonuniqueness of sediment yield at the catchment scale: The effects of soil antecedent conditions and surface shield. J Kim, V Y Ivanov, 10.1002/2013WR014580Water Resources Research. 5022014</p>
<p>Philosophy of Earth Science. M G Kleinhans, C J J Buskes, H W De Regt, 10.1002/9781444315578.ch9Philosophies of the Sciences. John Wiley &amp; Sons, Ltd2010</p>
<p>T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Large Language Models are Zero-Shot Reasoners. </p>
<p>T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, 10.48550/arXiv.2205.11916arXiv:2205.11916Large Language Models are Zero-Shot Reasoners. 2023</p>
<p>The Method of Multiple Working Hypotheses and the Development of Plate Tectonic Theory. R Laudan, 10.1007/978-94-009-9015-9_17Scientific Discovery: Case Studies. T Nickles, NetherlandsSpringer198060</p>
<p>Applying Information Theory and Bayesian Inference to Paleoenvironmental Interpretation. H Li, P Plink-Björklund, 10.1029/2019GL085928Geophysical Research Letters. 46242019</p>
<p>Z Lin, C Deng, L Zhou, T Zhang, Y Xu, Y Xu, Z He, Y Shi, B Dai, Y Song, B Zeng, Q Chen, T Shi, T Huang, Y Xu, S Wang, L Fu, W Zhang, J He, C Zhou, 10.48550/arXiv.2401.00434arXiv:2401.00434GeoGalactica: A Scientific Large Language Model in Geoscience. 2023</p>
<p>Nearshore along-strike variability: Is the concept of the systems tract unhinged?. A S Madof, A D Harris, S D Connell, 10.1130/G37613.1Geology. 4442016</p>
<p>Clinical versus statistical prediction: A theoretical analysis and a review of the evidence. P E Meehl, 10.1037/11281-0001954University of Minnesota Press</p>
<p>Problems of Time in Geology. E Milanovsky, 10.17704/eshi.8.2.r85693514t654187Earth Sciences History. 822007</p>
<p>Object-oriented expert systems and their applications to sedimentary basin analysis. B M Miller, 10.3133/b2048Geological Survey, USGS Map Distribution. 1993. 2048</p>
<p>S Miret, N M A Krishnan, 10.48550/arXiv.2402.05200arXiv:2402.05200Are LLMs Ready for Real-World Materials Discovery?. 2024</p>
<p>H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Akhtar, N Barnes, A Mian, 10.48550/arXiv.2307.06435arXiv:2307.06435A Comprehensive Overview of Large Language Models. 2023</p>
<p>Quantitative assessment of landform equifinality and palaeoenvironmental reconstruction using geomorphic models. A P Nicholas, T A Quine, 10.1016/j.geomorph.2010.04.004Geomorphology. 12132010</p>
<p>Verification, Validation, and Confirmation of Numerical Models in the Earth Sciences. N Oreskes, K Shrader-Frechette, K Belitz, 10.1126/science.263.5147.641Science. 26351471994</p>
<p>L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 10.48550/arXiv.2203.02155arXiv:2203.02155Training language models to follow instructions with human feedback. 2022</p>
<p>J J Pan, J Wang, G Li, arXiv:2310.14021Survey of Vector Database Management Systems. 2023</p>
<p>Strong Inference. J R Platt, 10.1126/science.146.3642.347Science. 14636421964</p>
<p>Chapter One-Seismic Tomography and the Assessment of Uncertainty. N Rawlinson, A Fichtner, M Sambridge, M K Young, 10.1016/bs.agph.2014.08.001Advances in Geophysics. R Dmowska, Elsevier201455</p>
<p>A Roy, 10.1190/1.1438985AMBIGUITY IN GEOPHYSICAL INTERPRETATION. GEOPHYSICS. 196227</p>
<p>Unique geologic insights from "non-unique" gravity and magnetic interpretation. R W Saltus, R J Blakely, 10.1130/G136A.1GSA Today. 21122011</p>
<p>Earth and Life: Global Biodiversity, Extinction Intervals and Biogeographic Perturbations Through Time. J A Talent, 10.1007/978-90-481-3428-12012SpringerNetherlands</p>
<p>T H Torsvik, L R M Cocks, 10.1017/9781316225523Earth History and Palaeogeography. Cambridge University Press2016</p>
<p>Local Underdetermination in Historical Science. D Turner, 10.1086/426851Philosophy of Science. 7212005</p>
<p>Judgment under Uncertainty: Heuristics and Biases. A Tversky, D Kahneman, 10.1126/science.185.4157.1124Science. 18541571974</p>
<p>A knowledge graph for standard carbonate microfacies and its application in the automatical reconstruction of the relative sea-level curve. H Wang, H Zhong, A Chen, K Li, H He, Z Qi, D Zheng, H Zhao, M Hou, 10.1016/j.gsf.2023.101535Geoscience Frontiers. 1451015352023</p>
<p>L Wang, J Zhang, H Yang, Z Chen, J Tang, Z Zhang, X Chen, Y Lin, R Song, W X Zhao, J Xu, Z Dou, J Wang, J.-R Wen, 10.48550/arXiv.2306.02552arXiv:2306.02552When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm. 2023</p>
<p>N Wang, H Yang, C D Wang, 10.48550/arXiv.2310.04793arXiv:2310.04793FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets. 2023</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, 10.48550/arXiv.2206.07682arXiv:2206.07682Emergent Abilities of Large Language Models. 2022</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 10.48550/arXiv.2201.11903arXiv:2201.11903Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2023</p>
<p>H Xiong, S Wang, Y Zhu, Z Zhao, Y Liu, L Huang, Q Wang, D Shen, 10.48550/arXiv.2304.01097arXiv:2304.01097DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task. 2023</p>
<p>A modern method of multiple working hypotheses to improve inference in ecology. S W Yanco, A Mcdevitt, C N Trueman, L Hartley, M B Wunder, 10.1098/rsos.200231Royal Society Open Science. 762020. 200231</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, 10.48550/ARXIV.2305.106012023</p>
<p>Construction of a fluvial facies knowledge graph and its application in sedimentary facies identification. L Zhang, M Hou, A Chen, H Zhong, J G Ogg, D Zheng, 10.1016/j.gsf.2022.101521Geoscience Frontiers. 1421015212023</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, J.-R Wen, arXiv:2303.18223A Survey of Large Language Models. 2023</p>
<p>Large Language Models for Scientific Synthesis. Y Zheng, H Y Koh, J Ju, A T N Nguyen, L T May, G I Webb, S Pan, 10.48550/arXiv.2310.07984arXiv:2310.07984Inference and Explanation. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
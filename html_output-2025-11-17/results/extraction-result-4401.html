<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4401 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4401</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4401</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-267770213</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.13426v1.pdf" target="_blank">Explaining Relationships Among Research Papers</a></p>
                <p><strong>Paper Abstract:</strong> Due to the rapid pace of research publications, keeping up to date with all the latest related papers is very time-consuming, even with daily feed tools. There is a need for automatically generated, short, customized literature reviews of sets of papers to help researchers decide what to read. While several works in the last decade have addressed the task of explaining a single research paper, usually in the context of another paper citing it, the relationship among multiple papers has been ignored; prior works have focused on generating a single citation sentence in isolation, without addressing the expository and transition sentences needed to connect multiple papers in a coherent story. In this work, we explore a feature-based, LLM-prompting approach to generate richer citation texts, as well as generating multiple citations at once to capture the complex relationships among research papers. We perform an expert evaluation to investigate the impact of our proposed features on the quality of the generated paragraphs and find a strong correlation between human preference and integrative writing style, suggesting that humans prefer high-level, abstract citations, with transition sentences between them to provide an overall story.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4401.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4401.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feature-based LLM Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feature-based, LLM-prompting approach for multi-paper related work generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage pipeline introduced in this paper that uses LLMs to extract interpretable natural-language features (faceted summaries, pairwise relationships, enriched citation intent/usages, target TAIC and a main-ideas plan) from a set of papers and then composes those features into prompts to generate paragraph- or section-level literature reviews that explain relationships among multiple scientific papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Feature-based LLM Prompting for Related Work Generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-stage system: (1) feature extraction stage where an LLM (gpt-3.5-turbo-0301 in implementation) is prompted to produce faceted summaries (objective, method, findings, contribution, keywords) for each paper and to synthesize directed pairwise relationships and enriched citation intent/usages from citation spans in the local citation network; (2) composition stage where these natural-language features plus target-paper TAIC and a short guiding 'main ideas' plan are assembled into a structured prompt; (3) generation stage where an LLM (GPT-4 gpt-4-0314 in implementation) generates paragraph- or section-level related work text (citations + expository/transition sentences); (4) optional CTS (cited text span) retrieval and re-generation step where candidate citation spans are retrieved by ROUGE ranking and appended to the prompt for a second-pass generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Feature extraction: gpt-3.5-turbo-0301 (Chat-GPT); Generation: GPT-4 (gpt-4-0314, 8k token limit). The authors also qualitatively tested gpt-3.5-turbo-0613, gpt-4-0613, Anthropic Claude-v2, Google text-bison-32k, and LLaMA-2 70B Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting of LLMs to produce human-interpretable natural language features (faceted summaries from TAIC, synthesis of citation spans into pairwise relationship descriptions, enriched citation intent/usages) rather than numeric embeddings; plus TAIC inclusion for contextualization. Citation spans are extracted with an off-the-shelf citation tagger (Li et al., 2022) and CTS retrieved by ROUGE recall ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Prompt composition and seq2seq LLM generation: combine per-paper faceted summaries, pairwise relationships, enriched citation usages, target TAIC and a guiding main-ideas plan into a single prompt to generate multi-citation paragraphs; multi-pass refinement via retrieved CTS for details.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Varies by related work section; the system handles a set of cited papers per target related-work (from small sets up to dozens), but limited by the LLM input window (GPT-4 8k tokens per pass). The authors chunk very large sets into subsections when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science (majority: natural language processing) in evaluation; approach is general but prompts tuned for CS papers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive literature reviews / related work sections (paragraph- or section-level), including citation sentences, expository and transition sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Automatic: ROUGE-1, ROUGE-2, ROUGE-L. Analysis: coverage and density (extractiveness) against input features. Human evaluation by domain experts on multiple axes: fluency, organization/coherence, relevance to target, relevance to cited papers, factuality (count of non-factual statements), usefulness/informativeness, writing style, overall quality. Kendall's τ correlation analyses between ROUGE, writing-style proportions, and human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Automatic: baseline variant A (full features) achieved ROUGE-1 = 0.513, ROUGE-2 = 0.216, ROUGE-L = 0.248 (Table 2). Human evaluation: across 27 domain experts, baseline outperformed ablations that removed the human-provided 'main idea' plan or target TAIC; human judges rated outputs as useful first drafts in many cases but noted factual errors and organization issues (numerical human scores reported in paper tables; summary: baseline superior to ablations lacking main idea/TAIC). CTS re-generation produced mixed results: some judges reported improved writing style (~44%), some reported improved informativeness (~26%), but some reported decreased factuality (~30%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared mainly against ablated variants of the same pipeline (feature ablations A-H) and qualitatively against a one-shot GPT-4 (Bing Chat) baseline; not compared against a single unified prior trained model baseline because prior works use different datasets/task definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Their full feature-based variant (A) outperformed variants that ablated the human main-ideas plan (B) and target TAIC (C) on both ROUGE and human preference; variant D (using cited abstracts instead of faceted summaries) improved fluency/coherence metrics but baseline A was superior in informativeness and factuality. Compared to one-shot GPT-4 (Bing Chat) baseline, the feature-based approach produced more factual and better-organized drafts (Bing Chat produced generic/ill-organized, hallucinated outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human-interpretable, natural-language features (faceted summaries, pairwise relationships, enriched citation usage) plus a human-provided high-level 'main ideas' plan and target TAIC context significantly improve multi-paper literature-review generation by LLMs. Readers prefer integrative, high-level narrative citations with transition sentences rather than concatenated per-paper summaries. CTS augmentation can increase detail but also increases factuality risk. LLMs need structured guidance for accurate, coherent multi-paper synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination and factual errors in generated claims; over- or under-emphasis of particular cited papers (incorrect prominence); difficulty grouping and organizing papers coherently across subsections; constrained by LLM input length (GPT-4 8k token limit), preprocessing failures when retrieving/ parsing PDFs (paywalls, non-paper sources), inconsistent citation markers, and variability introduced by feature extraction LLM quality; reliance on a human-provided main-ideas plan for good output (fully automated setting performed poorly).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance limited by LLM context window (8k tokens for GPT-4-0314) requiring chunking of large related work sections; authors note that larger context windows (e.g., GPT-4-32k) would reduce chunking and likely improve coherence across more papers. No explicit quantitative scaling curve provided with number of papers or model sizes, but qualitative tests across multiple LLMs showed varied generation quality (some models failed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Relationships Among Research Papers', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4401.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4401.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bing Chat (GPT-4) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bing Chat powered by GPT-4 (one-shot generation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A one-shot prompting baseline where GPT-4 (via Bing Chat) is given title, abstract, and list of reference paper titles to generate a literature-review-style section; used as an informal baseline and diagnostic in this study and shown to hallucinate and produce ill-organized outputs without structured guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bing Chat (GPT-4) one-shot literature review</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Single-pass LLM usage: provide title, abstract, and a list of reference paper titles (fits in context window) and request a literature-review paragraph/section. No intermediate feature extraction or retrieval steps; no structured multi-stage prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (via Bing Chat; exact variant unspecified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>One-shot prompting with paper metadata (title, abstract, reference titles) — no structured feature extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct LLM generation in a single pass from the provided prompt (implicit in-context synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>An unspecified list of reference paper titles supplied in the prompt (fits the model context window).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General; example used was reproducing Section 2 of this paper (related work on related-work generation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive related work paragraph/section</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative human inspection in this paper; judged on factuality, organization, and topicality by comparison to gold related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Produced generic, ill-organized, and non-factual outputs in the example shown; hallucinated approaches and misattributed statements. Considered not competitive as a baseline for this task without additional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively to the paper's feature-based approach (which used multi-stage prompting and feature injection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Worse: Bing Chat's one-shot output contained hallucinations and lacked the coherent story and factual accuracy achieved by the feature-based multi-stage prompting pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla one-shot LLM prompting (even with strong models like GPT-4) is insufficient to generate factually-correct, organized literature reviews across multiple papers; structured guidance (features and a main-ideas plan) is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Tendency to hallucinate, misattribute claims, produce generic expository sentences, and omit needed transition/explanatory content without explicit guidance. Poor factual reliability for multi-document synthesis when used one-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not studied quantitatively here; constrained by prompt content and model context size; authors observed failures on one-shot generation even when input fit in context window.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Relationships Among Research Papers', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4401.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4401.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CTS-enhanced Re-generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cited Text Span (CTS) retrieval augmented LLM re-generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented refinement step where initially generated candidate citation sentences are used as queries to retrieve most-relevant cited text spans (CTS) from cited papers using ROUGE-1/2 recall ranking, and those spans are added to the prompt for a second-pass LLM re-generation to increase citation detail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CTS-augmented LLM re-generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After a first-pass LLM generates candidate citation text, the system uses a citation tagger to extract candidate citation spans, then retrieves top-k (k up to 10, adjusted to fit prompt) sentences from each cited paper by ranking ROUGE-1 and ROUGE-2 recall against the query span. These CTS sentences are appended to the prompt as additional evidence and the LLM re-generates the related work paragraph to incorporate more detailed, paper-text-supported content.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Same LLMs as main pipeline (GPT-4 for generation in the authors' implementation); feature extraction still by gpt-3.5-turbo for earlier stages.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>ROUGE-based retrieval of sentence-level cited text spans from cited paper full texts, using the initial candidate citation span as query; citation spans are identified with a citation tagger (Li et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-augmented generation: combine retrieved CTS evidence with prior natural-language features and prompt the LLM to re-generate improved paragraphs that include more details grounded in cited paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applies for the same set of cited papers in the target related work; number of CTS per cited paper is top-k (k adjusted per case, hard cap at 10) limited by LLM input length.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science related work (as evaluated in this paper), generally applicable to multi-document summarization where cited full texts are available.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>More detailed, citation-grounded related work paragraphs/sections (abstractive but with explicit supporting CTS evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human evaluation on same axes (factuality, informativeness, writing style) and extractiveness metrics (coverage and density).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Mixed: CTS augmentation increased writing-style and informativeness scores for some judges (44% reported improved writing style, 26% reported improved informativeness) but decreased factuality for others (30% reported decreased factuality). Overall human-evaluation variance was high when CTS was used (variant H).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to baseline without CTS augmentation (variant A) and other ablations (variants B-G).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>No consistent net improvement: some judges found CTS helpful for detail, others observed more factual errors; overall high variance means CTS should be used cautiously.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval of actual cited-paper sentences can increase detail and perceived informativeness, but it can also introduce or amplify factuality issues depending on how the LLM uses the retrieved spans; trade-off between detail and factual reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>CTS retrieval depends on availability and accurate parsing of cited paper full texts (paywalls, parsing failures); adding CTS increases prompt length and risk of LLM over-copying (extractiveness/plagiarism concerns) or misinterpreting retrieved spans; mixed impact on factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Limited by LLM context window and k cap (k ≤ 10 per cited paper, adjusted case-by-case). Authors note higher variance and caution when scaling CTS across many cited papers due to prompt-length constraints and coherence issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Relationships Among Research Papers', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4401.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4401.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs Tested (authors' qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs qualitatively tested by the authors (gpt-3.5/gpt-4/Claude/bison/LLaMA-2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports qualitative tests of multiple LLMs besides the main GPT-3.5/GPT-4 pipeline: gpt-3.5-turbo-0613, gpt-4-0613, Anthropic Claude-v2, Google text-bison-32k, and LLaMA-2 70B Chat, noting similar qualitative outputs for some and failures for others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Assorted LLMs (qualitative evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors adapted their prompting templates to run on different LLM backends to assess portability and qualitative behavior: gpt-3.5-turbo variants and gpt-4 variants (OpenAI), Anthropic Claude-v2, Google text-bison-32k, and LLaMA-2 70B Chat. Observed that gpt-3.5/gpt-4 and Claude produced qualitatively similar texts, Google text-bison-32k produced less satisfactory style, and LLaMA-2 70B Chat failed for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-3.5-turbo-0613, gpt-4-0613, Anthropic Claude-v2, Google text-bison-32k, LLaMA-2 70B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Same structured prompting and multi-stage feature pipeline when tested where feasible; qualitative adaptation of prompts to each model.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Prompt-based synthesis as with main pipeline, but output style and factuality varied across models.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Same ranges as main experiments; limited by model context windows (e.g., text-bison-32k had larger window but poorer style; LLaMA-2 failed on relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science (NLP-heavy) evaluation set used for qualitative tests.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive related work paragraphs/sections</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative comparison noted in Limitations: style, factuality and task success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>gpt-3.5/gpt-4 variants and Claude-v2 produced qualitatively similar texts to the main pipeline; Google text-bison-32k produced less satisfactory style; LLaMA-2 70B Chat generated outputs irrelevant to the input (task failure). No quantitative metrics provided for these additional models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Informal comparison across LLM backends versus the authors' primary GPT-4 pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Some models matched qualitatively (Claude-v2), some underperformed on style (text-bison-32k), and some failed (LLaMA-2 70B Chat) when using the same prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt templates and the feature-based approach are portable to other LLMs, but model-intrinsic behavior (style, factuality, adherence to instruction) materially affects outputs; proprietary model changes may require prompt updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Different LLM providers and model sizes yield divergent behaviors; maintenance burden if models are deprecated or change; varying context lengths and token limits affect how many papers can be included per pass.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Behavior varied with model (text-bison-32k has larger context but worse style; LLaMA-2 70B Chat produced irrelevant outputs), indicating that scaling by model size does not guarantee successful multi-document synthesis without prompt and model capability alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Relationships Among Research Papers', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Capturing relations between scientific papers: An abstractive model for related work section generation <em>(Rating: 2)</em></li>
                <li>Explaining relationships between scientific documents <em>(Rating: 2)</em></li>
                <li>CORWA: A citation-oriented related work annotation dataset <em>(Rating: 2)</em></li>
                <li>Automatic related work section generation: experiments in scientific document abstracting <em>(Rating: 1)</em></li>
                <li>Language models are unsupervised multitask learners <em>(Rating: 1)</em></li>
                <li>Get To The Point: Summarization with Pointer-Generator Networks <em>(Rating: 1)</em></li>
                <li>ScisummNet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4401",
    "paper_id": "paper-267770213",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Feature-based LLM Prompting",
            "name_full": "Feature-based, LLM-prompting approach for multi-paper related work generation",
            "brief_description": "A multi-stage pipeline introduced in this paper that uses LLMs to extract interpretable natural-language features (faceted summaries, pairwise relationships, enriched citation intent/usages, target TAIC and a main-ideas plan) from a set of papers and then composes those features into prompts to generate paragraph- or section-level literature reviews that explain relationships among multiple scientific papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Feature-based LLM Prompting for Related Work Generation",
            "system_description": "A multi-stage system: (1) feature extraction stage where an LLM (gpt-3.5-turbo-0301 in implementation) is prompted to produce faceted summaries (objective, method, findings, contribution, keywords) for each paper and to synthesize directed pairwise relationships and enriched citation intent/usages from citation spans in the local citation network; (2) composition stage where these natural-language features plus target-paper TAIC and a short guiding 'main ideas' plan are assembled into a structured prompt; (3) generation stage where an LLM (GPT-4 gpt-4-0314 in implementation) generates paragraph- or section-level related work text (citations + expository/transition sentences); (4) optional CTS (cited text span) retrieval and re-generation step where candidate citation spans are retrieved by ROUGE ranking and appended to the prompt for a second-pass generation.",
            "llm_model_used": "Feature extraction: gpt-3.5-turbo-0301 (Chat-GPT); Generation: GPT-4 (gpt-4-0314, 8k token limit). The authors also qualitatively tested gpt-3.5-turbo-0613, gpt-4-0613, Anthropic Claude-v2, Google text-bison-32k, and LLaMA-2 70B Chat.",
            "extraction_technique": "Structured prompting of LLMs to produce human-interpretable natural language features (faceted summaries from TAIC, synthesis of citation spans into pairwise relationship descriptions, enriched citation intent/usages) rather than numeric embeddings; plus TAIC inclusion for contextualization. Citation spans are extracted with an off-the-shelf citation tagger (Li et al., 2022) and CTS retrieved by ROUGE recall ranking.",
            "synthesis_technique": "Prompt composition and seq2seq LLM generation: combine per-paper faceted summaries, pairwise relationships, enriched citation usages, target TAIC and a guiding main-ideas plan into a single prompt to generate multi-citation paragraphs; multi-pass refinement via retrieved CTS for details.",
            "number_of_papers": "Varies by related work section; the system handles a set of cited papers per target related-work (from small sets up to dozens), but limited by the LLM input window (GPT-4 8k tokens per pass). The authors chunk very large sets into subsections when needed.",
            "domain_or_topic": "Computer science (majority: natural language processing) in evaluation; approach is general but prompts tuned for CS papers.",
            "output_type": "Abstractive literature reviews / related work sections (paragraph- or section-level), including citation sentences, expository and transition sentences.",
            "evaluation_metrics": "Automatic: ROUGE-1, ROUGE-2, ROUGE-L. Analysis: coverage and density (extractiveness) against input features. Human evaluation by domain experts on multiple axes: fluency, organization/coherence, relevance to target, relevance to cited papers, factuality (count of non-factual statements), usefulness/informativeness, writing style, overall quality. Kendall's τ correlation analyses between ROUGE, writing-style proportions, and human preference.",
            "performance_results": "Automatic: baseline variant A (full features) achieved ROUGE-1 = 0.513, ROUGE-2 = 0.216, ROUGE-L = 0.248 (Table 2). Human evaluation: across 27 domain experts, baseline outperformed ablations that removed the human-provided 'main idea' plan or target TAIC; human judges rated outputs as useful first drafts in many cases but noted factual errors and organization issues (numerical human scores reported in paper tables; summary: baseline superior to ablations lacking main idea/TAIC). CTS re-generation produced mixed results: some judges reported improved writing style (~44%), some reported improved informativeness (~26%), but some reported decreased factuality (~30%).",
            "comparison_baseline": "Compared mainly against ablated variants of the same pipeline (feature ablations A-H) and qualitatively against a one-shot GPT-4 (Bing Chat) baseline; not compared against a single unified prior trained model baseline because prior works use different datasets/task definitions.",
            "performance_vs_baseline": "Their full feature-based variant (A) outperformed variants that ablated the human main-ideas plan (B) and target TAIC (C) on both ROUGE and human preference; variant D (using cited abstracts instead of faceted summaries) improved fluency/coherence metrics but baseline A was superior in informativeness and factuality. Compared to one-shot GPT-4 (Bing Chat) baseline, the feature-based approach produced more factual and better-organized drafts (Bing Chat produced generic/ill-organized, hallucinated outputs).",
            "key_findings": "Human-interpretable, natural-language features (faceted summaries, pairwise relationships, enriched citation usage) plus a human-provided high-level 'main ideas' plan and target TAIC context significantly improve multi-paper literature-review generation by LLMs. Readers prefer integrative, high-level narrative citations with transition sentences rather than concatenated per-paper summaries. CTS augmentation can increase detail but also increases factuality risk. LLMs need structured guidance for accurate, coherent multi-paper synthesis.",
            "limitations_challenges": "Hallucination and factual errors in generated claims; over- or under-emphasis of particular cited papers (incorrect prominence); difficulty grouping and organizing papers coherently across subsections; constrained by LLM input length (GPT-4 8k token limit), preprocessing failures when retrieving/ parsing PDFs (paywalls, non-paper sources), inconsistent citation markers, and variability introduced by feature extraction LLM quality; reliance on a human-provided main-ideas plan for good output (fully automated setting performed poorly).",
            "scaling_behavior": "Performance limited by LLM context window (8k tokens for GPT-4-0314) requiring chunking of large related work sections; authors note that larger context windows (e.g., GPT-4-32k) would reduce chunking and likely improve coherence across more papers. No explicit quantitative scaling curve provided with number of papers or model sizes, but qualitative tests across multiple LLMs showed varied generation quality (some models failed).",
            "uuid": "e4401.0",
            "source_info": {
                "paper_title": "Explaining Relationships Among Research Papers",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Bing Chat (GPT-4) baseline",
            "name_full": "Bing Chat powered by GPT-4 (one-shot generation baseline)",
            "brief_description": "A one-shot prompting baseline where GPT-4 (via Bing Chat) is given title, abstract, and list of reference paper titles to generate a literature-review-style section; used as an informal baseline and diagnostic in this study and shown to hallucinate and produce ill-organized outputs without structured guidance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Bing Chat (GPT-4) one-shot literature review",
            "system_description": "Single-pass LLM usage: provide title, abstract, and a list of reference paper titles (fits in context window) and request a literature-review paragraph/section. No intermediate feature extraction or retrieval steps; no structured multi-stage prompting.",
            "llm_model_used": "GPT-4 (via Bing Chat; exact variant unspecified in paper)",
            "extraction_technique": "One-shot prompting with paper metadata (title, abstract, reference titles) — no structured feature extraction.",
            "synthesis_technique": "Direct LLM generation in a single pass from the provided prompt (implicit in-context synthesis).",
            "number_of_papers": "An unspecified list of reference paper titles supplied in the prompt (fits the model context window).",
            "domain_or_topic": "General; example used was reproducing Section 2 of this paper (related work on related-work generation).",
            "output_type": "Abstractive related work paragraph/section",
            "evaluation_metrics": "Qualitative human inspection in this paper; judged on factuality, organization, and topicality by comparison to gold related work.",
            "performance_results": "Produced generic, ill-organized, and non-factual outputs in the example shown; hallucinated approaches and misattributed statements. Considered not competitive as a baseline for this task without additional structure.",
            "comparison_baseline": "Compared qualitatively to the paper's feature-based approach (which used multi-stage prompting and feature injection).",
            "performance_vs_baseline": "Worse: Bing Chat's one-shot output contained hallucinations and lacked the coherent story and factual accuracy achieved by the feature-based multi-stage prompting pipeline.",
            "key_findings": "Vanilla one-shot LLM prompting (even with strong models like GPT-4) is insufficient to generate factually-correct, organized literature reviews across multiple papers; structured guidance (features and a main-ideas plan) is necessary.",
            "limitations_challenges": "Tendency to hallucinate, misattribute claims, produce generic expository sentences, and omit needed transition/explanatory content without explicit guidance. Poor factual reliability for multi-document synthesis when used one-shot.",
            "scaling_behavior": "Not studied quantitatively here; constrained by prompt content and model context size; authors observed failures on one-shot generation even when input fit in context window.",
            "uuid": "e4401.1",
            "source_info": {
                "paper_title": "Explaining Relationships Among Research Papers",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CTS-enhanced Re-generation",
            "name_full": "Cited Text Span (CTS) retrieval augmented LLM re-generation",
            "brief_description": "A retrieval-augmented refinement step where initially generated candidate citation sentences are used as queries to retrieve most-relevant cited text spans (CTS) from cited papers using ROUGE-1/2 recall ranking, and those spans are added to the prompt for a second-pass LLM re-generation to increase citation detail.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CTS-augmented LLM re-generation",
            "system_description": "After a first-pass LLM generates candidate citation text, the system uses a citation tagger to extract candidate citation spans, then retrieves top-k (k up to 10, adjusted to fit prompt) sentences from each cited paper by ranking ROUGE-1 and ROUGE-2 recall against the query span. These CTS sentences are appended to the prompt as additional evidence and the LLM re-generates the related work paragraph to incorporate more detailed, paper-text-supported content.",
            "llm_model_used": "Same LLMs as main pipeline (GPT-4 for generation in the authors' implementation); feature extraction still by gpt-3.5-turbo for earlier stages.",
            "extraction_technique": "ROUGE-based retrieval of sentence-level cited text spans from cited paper full texts, using the initial candidate citation span as query; citation spans are identified with a citation tagger (Li et al., 2022).",
            "synthesis_technique": "Retrieval-augmented generation: combine retrieved CTS evidence with prior natural-language features and prompt the LLM to re-generate improved paragraphs that include more details grounded in cited paper text.",
            "number_of_papers": "Applies for the same set of cited papers in the target related work; number of CTS per cited paper is top-k (k adjusted per case, hard cap at 10) limited by LLM input length.",
            "domain_or_topic": "Computer science related work (as evaluated in this paper), generally applicable to multi-document summarization where cited full texts are available.",
            "output_type": "More detailed, citation-grounded related work paragraphs/sections (abstractive but with explicit supporting CTS evidence).",
            "evaluation_metrics": "Human evaluation on same axes (factuality, informativeness, writing style) and extractiveness metrics (coverage and density).",
            "performance_results": "Mixed: CTS augmentation increased writing-style and informativeness scores for some judges (44% reported improved writing style, 26% reported improved informativeness) but decreased factuality for others (30% reported decreased factuality). Overall human-evaluation variance was high when CTS was used (variant H).",
            "comparison_baseline": "Compared to baseline without CTS augmentation (variant A) and other ablations (variants B-G).",
            "performance_vs_baseline": "No consistent net improvement: some judges found CTS helpful for detail, others observed more factual errors; overall high variance means CTS should be used cautiously.",
            "key_findings": "Retrieval of actual cited-paper sentences can increase detail and perceived informativeness, but it can also introduce or amplify factuality issues depending on how the LLM uses the retrieved spans; trade-off between detail and factual reliability.",
            "limitations_challenges": "CTS retrieval depends on availability and accurate parsing of cited paper full texts (paywalls, parsing failures); adding CTS increases prompt length and risk of LLM over-copying (extractiveness/plagiarism concerns) or misinterpreting retrieved spans; mixed impact on factuality.",
            "scaling_behavior": "Limited by LLM context window and k cap (k ≤ 10 per cited paper, adjusted case-by-case). Authors note higher variance and caution when scaling CTS across many cited papers due to prompt-length constraints and coherence issues.",
            "uuid": "e4401.2",
            "source_info": {
                "paper_title": "Explaining Relationships Among Research Papers",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLMs Tested (authors' qualitative)",
            "name_full": "LLMs qualitatively tested by the authors (gpt-3.5/gpt-4/Claude/bison/LLaMA-2)",
            "brief_description": "The paper reports qualitative tests of multiple LLMs besides the main GPT-3.5/GPT-4 pipeline: gpt-3.5-turbo-0613, gpt-4-0613, Anthropic Claude-v2, Google text-bison-32k, and LLaMA-2 70B Chat, noting similar qualitative outputs for some and failures for others.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Assorted LLMs (qualitative evaluation)",
            "system_description": "Authors adapted their prompting templates to run on different LLM backends to assess portability and qualitative behavior: gpt-3.5-turbo variants and gpt-4 variants (OpenAI), Anthropic Claude-v2, Google text-bison-32k, and LLaMA-2 70B Chat. Observed that gpt-3.5/gpt-4 and Claude produced qualitatively similar texts, Google text-bison-32k produced less satisfactory style, and LLaMA-2 70B Chat failed for this task.",
            "llm_model_used": "gpt-3.5-turbo-0613, gpt-4-0613, Anthropic Claude-v2, Google text-bison-32k, LLaMA-2 70B Chat",
            "extraction_technique": "Same structured prompting and multi-stage feature pipeline when tested where feasible; qualitative adaptation of prompts to each model.",
            "synthesis_technique": "Prompt-based synthesis as with main pipeline, but output style and factuality varied across models.",
            "number_of_papers": "Same ranges as main experiments; limited by model context windows (e.g., text-bison-32k had larger window but poorer style; LLaMA-2 failed on relevance).",
            "domain_or_topic": "Computer science (NLP-heavy) evaluation set used for qualitative tests.",
            "output_type": "Abstractive related work paragraphs/sections",
            "evaluation_metrics": "Qualitative comparison noted in Limitations: style, factuality and task success.",
            "performance_results": "gpt-3.5/gpt-4 variants and Claude-v2 produced qualitatively similar texts to the main pipeline; Google text-bison-32k produced less satisfactory style; LLaMA-2 70B Chat generated outputs irrelevant to the input (task failure). No quantitative metrics provided for these additional models.",
            "comparison_baseline": "Informal comparison across LLM backends versus the authors' primary GPT-4 pipeline.",
            "performance_vs_baseline": "Some models matched qualitatively (Claude-v2), some underperformed on style (text-bison-32k), and some failed (LLaMA-2 70B Chat) when using the same prompt templates.",
            "key_findings": "Prompt templates and the feature-based approach are portable to other LLMs, but model-intrinsic behavior (style, factuality, adherence to instruction) materially affects outputs; proprietary model changes may require prompt updates.",
            "limitations_challenges": "Different LLM providers and model sizes yield divergent behaviors; maintenance burden if models are deprecated or change; varying context lengths and token limits affect how many papers can be included per pass.",
            "scaling_behavior": "Behavior varied with model (text-bison-32k has larger context but worse style; LLaMA-2 70B Chat produced irrelevant outputs), indicating that scaling by model size does not guarantee successful multi-document synthesis without prompt and model capability alignment.",
            "uuid": "e4401.3",
            "source_info": {
                "paper_title": "Explaining Relationships Among Research Papers",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Capturing relations between scientific papers: An abstractive model for related work section generation",
            "rating": 2,
            "sanitized_title": "capturing_relations_between_scientific_papers_an_abstractive_model_for_related_work_section_generation"
        },
        {
            "paper_title": "Explaining relationships between scientific documents",
            "rating": 2,
            "sanitized_title": "explaining_relationships_between_scientific_documents"
        },
        {
            "paper_title": "CORWA: A citation-oriented related work annotation dataset",
            "rating": 2,
            "sanitized_title": "corwa_a_citationoriented_related_work_annotation_dataset"
        },
        {
            "paper_title": "Automatic related work section generation: experiments in scientific document abstracting",
            "rating": 1,
            "sanitized_title": "automatic_related_work_section_generation_experiments_in_scientific_document_abstracting"
        },
        {
            "paper_title": "Language models are unsupervised multitask learners",
            "rating": 1,
            "sanitized_title": "language_models_are_unsupervised_multitask_learners"
        },
        {
            "paper_title": "Get To The Point: Summarization with Pointer-Generator Networks",
            "rating": 1,
            "sanitized_title": "get_to_the_point_summarization_with_pointergenerator_networks"
        },
        {
            "paper_title": "ScisummNet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks",
            "rating": 1,
            "sanitized_title": "scisummnet_a_large_annotated_corpus_and_contentimpact_models_for_scientific_paper_summarization_with_citation_networks"
        }
    ],
    "cost": 0.01814275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explaining Relationships Among Research Papers</p>
<p>Xiangci Li lixiangci8@gmail.com 
Department of Computer Science
University of Texas at Dallas Richardson
75080TX</p>
<p>Jessica Ouyang jessica.ouyang@utdallas.edu 
Department of Computer Science
University of Texas at Dallas Richardson
75080TX</p>
<p>Explaining Relationships Among Research Papers
7D2AA51340BE79D1B6A28E8D5657863BXXX. Method: XXX. Findings: XXX. Contribution: XXX. Keywords: ABC. Faceted Summary Objective: {{objective}} Method: {{method}} Findings: {{findings}} Contribution: {{contribution}} Keywords: {{keywords}}
Due to the rapid pace of research publications, keeping up to date with all the latest related papers is very time-consuming, even with daily feed tools.There is a need for automatically generated, short, customized literature reviews of sets of papers to help researchers decide what to read.While several works in the last decade have addressed the task of explaining a single research paper, usually in the context of another paper citing it, the relationship among multiple papers has been ignored; prior works have focused on generating a single citation sentence in isolation, without addressing the expository and transition sentences needed to connect multiple papers in a coherent story.In this work, we explore a feature-based, LLMprompting approach to generate richer citation texts, as well as generating multiple citations at once to capture the complex relationships among research papers.We perform an expert evaluation to investigate the impact of our proposed features on the quality of the generated paragraphs and find a strong correlation between human preference and integrative writing style, suggesting that humans prefer high-level, abstract citations, with transition sentences between them to provide an overall story.</p>
<p>Introduction</p>
<p>Due to the rapid pace of research publications, including pre-prints that have not yet been peerreviewed, keeping up to date with the latest work is very time-consuming.Even with daily feed tools like the Semantic Scholar Research Feed1 , researchers must still curate, read, and digest all the new papers in their feed.Thus, there is a need for concise, automatically generated literature reviews summarizing the set of new papers in a feed, customized for the researcher whose feed it is.</p>
<p>Unfortunately, there does not exist a dataset of such literature reviews.Survey articles are similar, but they are very long, not customized to a specific reader, and relatively rare.In this work, we use the related work sections of scientific articles as a proxy for the kind of short, customized, daily feed summaries we wish to generate.Related work sections have several advantages: they are concise, usually no more than one page long; they are customized to their parent article, just as a daily feed summary should be customized to its owner; and they are plentiful and can be automatically extracted as the "Related Work," "Literature Review," or "Introduction" sections of scientific articles.To relate back to a daily feed summary, one can easily imagine using the most similar paper authored by the feed owner as the "citing" paper perspective from which to customize the literature review.</p>
<p>The task of automatically generating citations for scientific articles has been explored using both extractive (Hoang and Kan, 2010;Hu and Wan, 2014;Chen and Zhuge, 2019;Wang et al., 2019;Deng et al., 2021) and abstractive techniques (AbuRa'ed et al., 2020;Xing et al., 2020;Ge et al., 2021;Luu et al., 2021;Chen et al., 2021;Li et al., 2022).However, we argue that the dominant approach of generating a single citation sentence in isolation ignores the relationships among cited papers, which are just as important as that between the citing and cited papers.Literature reviews, whether for a related work section or a daily feed summary, are multi-document summaries and should contain the non-citation, expository and transition sentences needed to compose a coherent story (Li et al., 2022).Recent neural approaches frame citation generation as an end-to-end, sequence-to-sequence task; they are thus constrained by the length limitations of their models -research papers are long documents -and are unable to make use of supporting features, such as citation intents or topic information, which would require training additional models.</p>
<p>arXiv:2402.13426v1 [cs.CL] 20 Feb 2024</p>
<p>The task of generating related work sections in scientific papers has been explored by several researchers.The primary focus has been on the automatic generation of citation texts in scholarly papers.Xing et al 1 conducted a pilot study on this topic, demonstrating that citation texts could be automatically generated given the context of a citing paper and a cited paper.They proposed a multi-source pointer-generator network with a cross-attention mechanism for citation text generation, which showed promising results.Building on this, Ge et al2 introduced BACO, a framework for citing sentence generation that leverages both background knowledge and content information.This approach considers structural information from a citation network as background knowledge and uses salience estimation to identify what to cite from the cited paper.The relationship between scientific documents has also been studied.Luu et al 3 addressed the task of explaining relationships between two scientific documents using natural language text.This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing that relationship in text.A meta-study conducted by Li and Ouyang 4 compared existing literature on related work generation from various perspectives such as problem formulation, dataset collection, methodological approach, performance evaluation, and future prospects.This study provided valuable insights into the progress of state-of-the-art studies and suggested directions for future research.Hoang and Kan 5 proposed an approach towards automated related work summarization.They introduced a fully automated approach to generate related work sections by leveraging a seq2seq neural network.Their goal was to improve the abstractive generation of related work by introducing problem and method information.In another study, Li et al 6 presented CORWA, a dataset that labels different types of citation text fragments from different information sources.They trained a strong baseline model that automatically tags the CORWA labels on massive unlabeled related work section texts.Chen et al 7 proposed an optimization approach for the automatic generation of related work sections in scientific papers.They introduced problem and method information as an additional feature to enhance the generation of related work.Radford et al.'s work on Language Models as Unsupervised Multitask Learners 8 demonstrated that language models begin to learn tasks without any explicit supervision when trained on large datasets like WebText.This finding is particularly relevant as it suggests that language models could potentially be used for tasks like related work section generation.Chen et al.'s study 9 proposed an abstractive model for related work section generation that captures relations between scientific papers.They introduced a Relation-aware Related work Generator (RRG) which generates an abstractive related work section from multiple scientific papers in the same research area.Wu et al.'s ToC-RWG 10 explored the combination of topic model and citation information for automatic related work generation.They proposed a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph.</p>
<p>Other notable works in this area include "Automatic generation of related work through summarizing citations" 11 , "Get To The Point: Summarization with Pointer-Generator Networks" 12 , "Longformer: The Long-Document Transformer" 13 , "Automatic Related Work Section Generation: Experiments in Scientific Document Abstracting" 14 , "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" 15 , "Towards Full Related Work Generation" 16 , "Automatic Related Work Section Generation by Sentence Extraction and Reordering" 17 , and "Attention Is All You Need" 18 .These works have contributed significantly to the field of automatic related work section generation.</p>
<p>(a) Bing Chat output ("more precise" setting).</p>
<p>The task of automatic related work generation has been explored by several researchers.Hu and Wan (2014) proposed an optimization approach, ARWG, which uses a PLSA model to split the sentence set of given papers into different topic-biased parts and employs regression models to learn the importance of sentences.This approach was later built upon by Chen and Zhuge (2019), who introduced a method for generating related work sections by summarizing citations.However, these methods often resulted in outputs lacking coherence and stylistic variation.To address this, Wang et al. (2019) proposed a framework, ToC-RWG, that incorporates topic models and citation information for related work generation.Despite these advancements, the challenge of generating a coherent overall story remained.Deng et al. (2021) attempted to address this by developing SERGE, a BERT-based ensemble model for sentence extraction and reordering to generate descriptive related work sections.However, even this approach was unable to fully capture the complex relationships among cited papers.In this work, we build upon these previous studies and propose a feature-based, LLM-prompting approach to generate citation texts and their expository and transition sentences at the paragraph or section level.Recent advancements in abstractive text summarization have paved the way for the development of models capable of generating citation texts.See et al. (2017) introduced a hybrid pointer-generator network with coverage, which significantly improved the accuracy of text summarization and reduced repetition.This work has been widely recognized and cited by several subsequent studies, including those by Xing et al. (2020), Ge et al. (2021), and Luu et al. (2021), among others.Xing et al. (2020) built upon the foundation laid by See et al. (2017) and proposed a multi-source pointer-generator network with a cross-attention mechanism for automatic citation text generation.Their work was further extended by Ge et al. (2021), who introduced a framework for citing sentence generation that considers both background knowledge and content information.Meanwhile, Luu et al. (202) utilized citing sentences as a proxy for explaining relationships between scientific documents, demonstrating the potential of large language models in this domain.These studies collectively highlight the growing trend of using abstractive approaches and large language models for generating citation texts, setting the stage for our feature-based, LLM-prompting approach.The most closely related work to ours is by Chen et al. (2021), who proposed a Relation-aware Related work Generator (RRG) model for generating an abstractive related work section from multiple scientific papers.Their approach, like other abstractive methods, is end-to-end and utilizes a relation-aware multi-document encoder to relate one document to another based on their content dependency in a relation graph.The model iteratively refines the relation graph and document representation during the training process and incorporates the relation graph information in the decoding process to assist in generating the related work section.Their experiments on two large-scale related work generation datasets demonstrated that the RRG model outperforms several strong baselines in terms of ROUGE metrics and human evaluations.However, unlike their approach, our work proposes human-interpretable, natural language features to express the content and relationships of each paper, as well as the discourse role and writing style of each citation.It is worth noting that there is currently no standard benchmark evaluation approach that allows for a direct comparison of methods used in these different prior works, as they employ different datasets and slightly vary in their task definition.</p>
<p>(b) Our approach.</p>
<p>Figure 1: Comparison between GPT4-powered Bing Chat and our approach on reproducing Section 2 of this paper.Bing Chat is given a prompt consisting of the title and abstract of this paper, as well as a list of reference paper titles, which fits comfortably in its context window.Bing Chat's output is generic, ill-organized, and non-factual; statements in red are misattributed or incorrect.</p>
<p>The recent dramatic success of prompting-based methods using LLMs, like Chat-GPT and GPT-4 (OpenAI, 2023), makes it possible to pursue a feature-based approach to generating richer citation texts, as well as generating multiple citations at a time to better capture the complex relationships among research papers.However, as Figure 1 shows, even a SOTA LLM, augmented with a search engine to retrieve papers 2 , cannot generate a factually-correct, on-topic literature review from scratch.Bing Chat with GPT-4 hallucinates cited papers' approaches, and the expository sentences are generic and vague.We can see that the LLM needs guidance in identifying the relevant contributions of each cited paper, as well as how the cited papers relate to one another.</p>
<p>In this work, as a first step towards generating customized daily feed summaries, we explore a feature-based, LLM-prompting approach to generating citation texts and their transition sentences at the paragraph level, using automatically extracted related work sections as the evaluation targets.Our main contributions are as follows:</p>
<p>• We propose features capturing the relationships between cited and citing papers and among papers cited together.We show that these features can be extracted by prompting LLMs and compose them into a new prompt for generating several citations, along with transition sentences, in one pass.</p>
<p>• We conduct experiments on a planning-based setting, where a plan consisting of a few sentences describing the high-level relationships among cited papers is used to guide generation.In this preliminary study, we use a human-provided plan to investigate the impact of these guiding "main ideas" on the organization of citations in the generated paragraphs.</p>
<p>• We perform an expert evaluation to investigate the impact of our proposed features on the quality of the generated paragraphs.We find a strong correlation between human preference and integrative writing style, suggesting that readers prefer more high-level, abstract citations, with transition sentences between them to provide a coherent overall story.</p>
<p>Background and Related Work</p>
<p>Hoang and Kan (2010) proposed the task of automatic related work generation: generating the related work section of a target paper given a list of papers to cite, assuming the rest part of the target paper is available.Early extractive approaches automatically select and concatenate salient sentences from the cited papers (Hu and Wan, 2014;Chen and Zhuge, 2019;Wang et al., 2019;Deng et al., 2021).As a result, their outputs lack coherence among citations and have no overall story, and the sentences lack stylistic variation; transitions and sentences relating back to the target paper are impossible to produce using an extractive approach.</p>
<p>More recently, abstractive approaches have focused on generating a single citation at a time, given the cited paper and assuming the rest of target paper, including the rest of the related work section, is available (AbuRa'ed et al., 2020;Xing et al., 2020;Ge et al., 2021;Luu et al., 2021;Li et al., 2022).These works used a variety of architectures (pointer-generator (See et al., 2017), vanilla Transformer (Vaswani et al., 2017), GPT-2 (Radford et al., 2019) or Longformer-Encoder-Decoder (LED; Beltagy et al., 2020)) to generate citations from cited paper abstracts; the cited paper full texts were not used due to their length.</p>
<p>The most similar work to ours is Chen et al. (2021), who attempt to generate multiple citations at once.However, their approach, like that of other abstractive prior works, is end-to-end; they augment a document encoder with a graph network to learn relationships among document representations.In contrast, we propose human-interpretable, natural language features to express paper content and relationships, as well as the discourse role and writing style of each citation.</p>
<p>Finally, as Li and Ouyang (2022) note, there is no standard benchmark to directly compare methods from these prior works, which use different datasets and vary slightly on the task definition.</p>
<p>Approach</p>
<p>We use multi-stage prompting of LLMs to first collect features from each cited paper and then compose them into a prompt for generating a paragraph of citations with supporting expository and transition sentences.It is well-known that, given a simple prompt, such as "Generate a literature review about XXX," LLMs produce poorly-organized and inaccurate citations; from our Bing Chat example, we can see that the LLM inaccurately describes the approaches and contributions of the cited papers.We must provide the LLM with more detailed and specific information about the cited papers.</p>
<p>We identify two sets of key support features.First, we extract features for the target paper and each cited paper from a local citation network, capturing information about the relationship between the target and each cited paper, as well as between pairs of cited papers (Section 3.1).Second, we extract features from the text of the target paper itself, which provide contextual information to ensure the generated citations stay on topic (Section 3.2); in our experiments, the target paper stands in for the daily feed owner so that the literature review is customized for their perspective.</p>
<p>After extracting these features, we compose a prompt to generate paragraphs3 of citations (Section 3.3).Finally, we use the generated draft to extract candidate cited text spans for each cited paper, add them to the prompt, and re-generate improved paragraphs (Section 3.4).Our prompts are shown in Appendix Tables 6-10; Appendices B &amp; C show examples of features and paragraphs.</p>
<p>Citation Network Features</p>
<p>We identify key features under the framework of a local citation network centered on the target paper.Each node represents a paper, and an edge represents the relationship between two papers.Unlike Chen et al. (2021), we use natural language descriptions as network features, allowing us to leverage the seq2seq nature of LLMs, rather using numerical feature vectors with a graph neural network; the natural language descriptions also improve the interpretability of the citation network.</p>
<p>Faceted summary.Each node in the citation network represents a paper, and its core feature is the faceted summary (Meng et al., 2021), which highlights the key aspects of the paper for rapid understanding: the paper's objective, method, findings, contributions and keywords.Just as a human may quickly skim the title, abstract, introduction, or conclusion (TAIC) sections to get the gist of a paper, we focus on the most important facets of a paper when generating its citation.The faceted summary also provides the practical benefit of reducing the number of tokens needed to represent a paper; the limited input window size of LLMs encourages a compact representation for each node.We prompt the LLM to generate a faceted summary given the TAIC of each paper.</p>
<p>Relationship between paper pairs.Each edge in the citation network represents the relationship between the two paper nodes it connects.Given a pair of papers A and B, we leverage the LLM's strong summarization ability to synthesize information from all citation spans 4 where paper A cites paper B, conditioned on the faceted summaries of A and B, into a single natural language descrip-tion of their (directed) relationship.The incoming edges on paper B's node thus capture how B has been discussed by other works, providing a history of how its ideas have influenced its field; the outgoing edges from likewise capture how B has developed ideas from other works in its field.</p>
<p>Enriched citation intent &amp; usage.Citation intents encode why and how an author cites a paper: to give background information, to use a proposed methodology, or to compare experimental results.Existing work on citation intent focuses on proposing new label sets or modeling approaches to predict those labels, without applying them to downstream applications (Garfield et al., 1965;Teufel et al., 2006;Dong and Schäfer, 2011;Jurgens et al., 2018;Cohan et al., 2019;Tuarob et al., 2019;Zhao et al., 2019), and they framed intent prediction as a classification task to reduce its complexity.However, as Lauscher et al. (2022) point out, simple classification label sets struggle to represent ambiguous, real-world citations.To the best of our knowledge, we are the first to apply citation intent to citation generation and to extend intent labels to rich natural language descriptions.</p>
<p>In addition, Li et al. (2022) distinguish between dominantand reference-type citations.For example, consider the sentence " Luu et al. (2021) fine-tuned GPT-2 (Radford et al., 2019) to predict citation sentences."The emphasis is on the dominant-type citation of Luu et al., whereas the reference-type citation of Radford et al. is not explained in detail, since GPT-2 is being cited as a tool.Ignoring this distinction results in unnaturalsounding paragraphs that treat all cited papers as equally-important, dominant-type citations.</p>
<p>Thus, for each cited paper B, we prompt the LLM to summarize how other papers A i in the network cite B (intent) as well as if the majority usage of B is as an important, dominant cited paper or if it is simply cited for reference.The prompt includes the faceted summaries (node features) of the other citing papers A i , all relationships (edge features) between A i and B, and the text of the citation spans for B in A i .This enriched citation intent/usage feature roughly corresponds to a discursive summary of all edges incident to node B.</p>
<p>Target Paper Features</p>
<p>To generate paragraphs with a coherent overall story, we collect features from the target paper, capturing the context and perspective of the reader.Title, abstract, introduction, and conclusion (TAIC).Despite the powerful zero-shot generation ability of LLMs, they are typically not trained specifically for scientific document generation and lack the necessary domain knowledge to write like a domain expert.We leverage the LLM's strong in-context learning ability by including the full text of the TAIC sections of the target paper.The TAIC provides context to the LLM, so that the story and tone of the TAIC can inform the focus and organization of the citations to be generated; in our experiments, the target paper represents the reader's interests, so a good literature review should be coherent with the target paper.</p>
<p>Guiding plan of main ideas.Intuitively, there can be multiple plausible literature reviews for the same set of cited papers.A reader may prefer one over another, even though they are all factually correct, depending on the perspective given by the target paper.To better capture this information, we experiment with a human-provided plan to guide generation; we leave the automatic generation of a guiding plan to future work.The plan is a short summary of the main ideas to be discussed; to simulate this feature during evaluation, we prompt an LLM to condense the gold related work section of the target paper into a short summary of its main ideas, ignoring citations to avoid information leak.</p>
<p>Related Work Paragraph Generation</p>
<p>With the features above, we prompt the LLM to generate one paragraph, subsection, or when length allows, the entire literature review in one pass (Figure 2).We find section-level generation to be the most robust, as the LLM often does not follow prompt instructions as closely when generating paragraph-by-paragraph, but we are constrained by the length limit of the LLM.During evaluation, for target related work sections with many cited papers, such that the full prompt exceeds the LLM's limit, we manually chunk the gold related work section based on subsections or titled paragraphs, partition the cited papers according to those chunks, and generate each chunk individually, concatenating the generated subsections or paragraphs in the same order as in the gold related work section.</p>
<p>Enhancing Details with CTS</p>
<p>We observe that the generated citations may lack detail compared to human-written ones.This makes sense because the generation prompt con-  tains only summaries of, but no actual text from, each cited paper.To supply more detail, we follow Yasunaga et al. (2019); Wang et al. (2019) in using ROUGE-based ranking (Cao et al., 2015) to retrieve cited text spans (CTS; Jaidka et al., 2018Jaidka et al., , 2019;;AbuRa'ed et al., 2020): the cited paper text span most relevant to the corresponding citation.</p>
<p>We retrieve CTS using a newly generated candidate citation span as query, which we extract using Li et al. (2022)'s citation tagger; we compute the average of ROUGE-1 and -2 recall scores against each sentence in the corresponding cited paper.We take the top-k5 sentences as CTS to augment the prompt and then re-generate the paragraph.</p>
<p>Experimental Settings</p>
<p>For each target related work section, we start with the PDF of the target paper and aim to generate a plausible candidate to replace the gold related work section using our feature-based approach.</p>
<p>As we discuss in Sections 1-2, to the best of our knowledge, there is no prior baseline that can generate the long texts of this task; even a SOTA LLM, Bing Chat with GPT-4, fails to be a competitive baseline.Therefore, we focus on comparing different input feature variations (Section 4.3).</p>
<p>Implementation Details</p>
<p>We use Google search API6 to find and download cited papers and doc2json (Lo et al., 2020) to parse PDFs into JSON format.We use Chat-GPT (gpt-3.5-turbo-0301)for all feature extraction steps and GPT-4 (gpt-4-0314; maximum input length limit of 8k tokens) for the generation step.</p>
<p>We do not perform any training or fine-tuning, and we do not use any datasets; we only use the pre-trained citation tagger of Li et al. (2022) for citation span extraction.We design our prompts using the first author's previous publications as a development set and use our human judges' nominated papers as our test set.</p>
<p>Human Evaluation Settings</p>
<p>Because it is challenging for non-domain experts to evaluate the quality and factuality of scientific texts, we conduct a human evaluation by inviting domain experts to evaluate candidate literature reviews generated for one of their own published papers, or a paper closely related to their own work.Our experts are all fluent in English and are a mix of Ph.D. students and post-doctorate researchers in both academia and industry.We instruct them to nominate a target paper such that they are very familiar with all of the cited papers.</p>
<p>Because the experts were recruited from among our colleagues, about half of the papers evaluated are natural language processing papers, with the other half from other computational fields, including machine learning, speech processing, computer vision, robotics, computer graphics, and programming languages.Almost all papers were published after September 2021 and so were not included in the training data of the LLMs (Appendix Table 12).</p>
<p>The evaluators are asked to score each generated related work section in terms of (1) fluency, (2) organization &amp; coherence, (3) relevance to the target paper, (4) relevance to the cited papers, (5) factuality and the number of non-factual or inaccurate statements, (6) usefulness &amp; informativeness, (7) writing style, and (8) overall quality.</p>
<p>Input Feature Variations</p>
<p>Table 1 shows the input features used by each of our generated variants.A is our baseline, with access to all features, including the human-provided main   cratic process, and the high variance in the human evaluation scores reflects this fact; different variants are preferred by different judges.
A B C D E F G H main idea ✓ -✓ ✓ ✓ ✓ ✓ ✓ target TAIC ✓ ✓ -✓ ✓ ✓ ✓ ✓ faceted summary ✓ ✓ ✓ -✓ ✓ ✓ ✓ cited abstract ---✓ ---- intent/usage ✓ ✓ ✓ ✓ -✓ -✓ relationship ✓ ✓ ✓ ✓ ✓ --✓ CTS -------✓</p>
<p>Importance of Input Features</p>
<p>We integrate the results of Tables 2, 3 &amp; 4 to analyze the usefulness of each input feature.</p>
<p>Main idea plan.All tables show that baseline A outperforms variant B, which ablates the main idea feature.The fact that main idea information is not found in any other feature (Appendix A) confirms the importance of a human-provided main idea to guide the LLM in generating a satisfactory story.</p>
<p>Target paper TAIC.The baseline A also outperforms variant C, which ablates the target paper title, abstract, introduction, and conclusion, confirming our hypothesis that this feature provides crucial context for the generated paragraphs.</p>
<p>Enriched citation usage &amp; relationship between papers.</p>
<p>Comparing A to variants E, F , and G, we find a weak trend that access to either enriched citation usage or relationship between papers is helpful, with the latter slightly preferred; this finding is consistent with the observed coverage and density discussed in Appendix A. Variants with access to both or neither feature underperform, suggesting that, while the usage and relationship features are important, they are also mutually redundant (the usage feature of a cited paper summarizes all its relationships) and the presence of both causes the LLM to over-emphasize this information.</p>
<p>Faceted summaries &amp; cited paper abstracts.</p>
<p>We are surprised to observe that variant D, which uses cited paper abstracts instead of faceted summaries, outperforms A in terms of fluency, coherence, writing, and overall.We note that D still has indirect access to the faceted summaries since the relationship-between-papers feature is derived from faceted summaries.We hypothesize that D is preferred over A because abstracts are humanwritten and contain more narrative-style sentences than the LLM-generated faceted summaries (see Section 5.2).However, the baseline A still outperforms D in terms of informativeness and factuality.</p>
<p>CTS.</p>
<p>Comparing the baseline A with variant H, we observe that re-generating the related work section using cited text spans is a controversial choice that leads to very high variance in human evaluation scores.44% of the judges report improved writing style, and 26% report improved informativeness, while 30% report decreased factuality.Therefore, CTS should be used cautiously.</p>
<p>Analysis of Writing Style</p>
<p>Khoo et al. ( 2011) studied the writing style of literature reviews by categorizing them as integrative or descriptive, depending on whether they focus on high-level ideas or on detailed information from specific studies.Li et al. (2022) extended this distinction to individual citations, distinguishing dominant citations, which focus on and describe cited papers in detail, and reference citations, which are short, highly abstracted, and often tangential to the rest of the sentence.Li et al. also introduced sentence-level discourse roles: transition and narrative sentences provide exposition and high-level observations; single and multi-summarization sentences give specific, detailed information about one or more cited papers; and reflection sentences relate cited papers to the target paper.</p>
<p>To study the writing style of our generated paragraphs, we use Li et al.'s citation tagger to label the usage types and discourse roles of the gold related work sentences and our variants.As Table 5 shows, there is a huge gap between the two writing styles: gold sentences mainly consist of transition and narrative sentences with referencetype citations, while all generated variants have far more explicit single-summary sentences with dominant-type citations.In other words, the generated paragraphs are mostly descriptive, consisting of individual paper summaries, rather than a coherent story integrating all the cited papers.</p>
<p>Correlation Among Human Preference, ROUGE, and Writing Style</p>
<p>As Tables 2, 3 &amp; 5 show, there is a strong correlation between human preference (overall score) and ROUGE-L scores, as well as between ROUGE scores and writing style (proportion of referencetype citations), with Kendall's τ of 0.592 and 0.691, respectively.This suggests that we can use automatic metrics such as ROUGE and the proportion of reference-type citations to estimate human judgments, which is extremely challenging to collect on a large scale.Moreover, this observation emphasizes the importance of having a coherent and organized story consisting of narrative-style sentences with reference-type citations and transition sentences bridging between them.</p>
<p>Qualitative Error Analysis</p>
<p>Despite the overall success of our approach -over half the judges wrote that the generated variants would be good first drafts for the gold related work sections -our collected comments from the judges show that composing a literature review is still a very challenging task.We summarize the typical issues mentioned by the judges below:</p>
<p>Factual errors.While all generated variants have a small absolute number of factual errors (see Table 3), incorrect statements are the most frequently mentioned problem.For example, one judge complained, ". . .two descriptions are false (Hearst patterns not extracted from Wikipedia, and there were no edits in Bowman et al.)."We observe that the overall human evaluation score correlates with the factuality score (Kendall's τ =0.50).</p>
<p>Emphasizing the right cited papers.A good literature review should have a logical story; simply concatenating individual cited paper summaries is not sufficient.Our judges complained about less important papers receiving too much attention:</p>
<p>• ". . .too much detail for the papers and has a paragraph on human-in-the-loop data generation which is not very relevant to the paper (should be mentioned briefly)."• "The descriptions of the cited papers, while accurate, seem less relevant to the citing paper and to the story as a whole, and the papers that get a lengthier description are not the most central ones."• "The focus on traditional decompilation methods is too strong for the paper content."Further, due to the nature of using related work sections as our evaluation targets, the publication dates of the cited papers can vary greatly.While this would not be a problem in a daily feed literature review (since all of the papers would be new), our judges were more likely to complain if the generated literature review focused to much on earlier works: ". . .why does it focus on approaches from 20 years ago than recent approaches?"These comments confirm our finding that dominant, summarization-type citation sentences may not be appropriate for all cited papers.</p>
<p>Paragraph organization.How to group similar works together is a major challenge.While our approach is usually able to generate well-organized texts when the variant includes sufficient features, failure cases significantly impact the human evaluation.For example, one judge wrote, "Second paragraph starts with 'In the pursuit of automating reinforcement learning', but then immediately cite Henderson et al. ( 2018) which talks about reproducibility and not automating RL issues."</p>
<p>Judges also commented that they would prefer more comparisons among cited papers: ". . .some citations did not highlight their difference from other work, such as Schick et al. ( 2021) generates pairs of data."</p>
<p>Other observed issues include insufficient evidence for claims and inconsistent citation formatting.In addition, the LLM does not always follow the prompts, occasionally resulting in some cited papers being silently dropped from the output.</p>
<p>Overall, we find that organization and flow between citations is very important to the judges and is mentioned in most positive comments:</p>
<p>• "Good connection between the explained works.It is not just a list of contributions."• "The organization is close to perfect, and the story flows well in this one.One citation is missing, and, surprisingly, one citation was added (Kondadadi, 2013) -in exactly the right place and with an accurate description!weird but pretty cool!"</p>
<p>Conclusion</p>
<p>We have presented a feature-based approach for prompting LLMs to explain the relationships among cited papers.With the ultimate goal of generating a literature review summarizing the contents of a researcher's daily paper feed, we have conducted a pilot study using the related work sections of scientific articles as a proxy for the kind of literature reviews we wish to generate: short, customized for a particular target paper (standing in for daily feed's owner), and focused on explaining how the cited papers relate to each other and why they are important.Our approach focuses on using the strong natural language understanding and summarization abilities of LLMs to extract interpretable, natural language features describing the content of the cited and target papers, as well as their relationships with each other and with other papers that have cited them in the past.We also propose a "main ideas" plan to guide the LLM to generate a coherent story, using a human-supplied plan in these preliminary experiments.</p>
<p>Our detailed expert evaluation reveals that human judges dislike literature reviews that simply concatenate cited paper summaries together, demonstrating the importance of generating at the paragraph or section level, including transition sentences, rather than focusing on individual citations.Human judges are also sensitive to the relevance of each cited paper and strongly dislike generations that wrongly emphasize less impactful papers.We conclude that accurate descriptions of a cited paper's methodology are not the only important facet of scientific document processing -understanding the rich and sophisticated relationships among papers is the key.</p>
<p>Limitations</p>
<p>Citation retrieval.In their written comments, several judges expressed the wish that our system would help them find other related papers to read.This is a limitation not only of our work, but of all prior work in automatic literature review generation, going back to Hoang and Kan (2010).We suggest that future work can explore integrating citation list optimization with literature review generation, perhaps by generating iteratively candidate drafts and retrieving additional papers to cite.</p>
<p>Length limit of GPT-4.Our experiments used gpt-4-0314, which has a maximum input token length of 8k; we did not have access to gpt-4-32k, which has four times the length limit.For nearly half of the related work sections, we had to iteratively generate subsections and concatenate the outputs, as described in Section 4.1.Consequently, the coherence between subsections is significantly impacted, and they read like a concatenation of different related work sections.This problem will be mitigated as more LLMs have longer maximum input lengths.</p>
<p>Imperfect preprocessing pipeline.We were unable to access the PDFs of some cited works due to several problems: (1) the PDF parser is only able to parse research papers, but not books or websites;</p>
<p>(2) the citation lists are automatically extracted from the parsed PDFs, and some papers may be missing; (3) we were unable to retrieve some cited papers using the Google seach API; and (4) we were unable to download some cited papers due to publisher pay-walls.The missing cited papers limit the performance of our system.</p>
<p>Inconsistent citation markers.Since we use heuristics to extract citation markers from a JSON parsed from the target paper PDF, some of the author last names and publication years may not be accurate.In addition, we observe a few cases of inconsistency in citation styles (e.g.mixing "Smith et al. ( 2023)" and "[1]") across multiple passes of generation.In future work, we will leverage the LLM's code comprehension and generation ability to directly input the bibliography and output related work texts in L A T E Xformat.</p>
<p>Quality of the intermediate outputs.Since there are no gold features introduced in Section 3.1 &amp; 3.2, nor do we have the resource of additional human evaluation for these intermediate output features from the LLM, we have to leave the study of intermediate feature quality and its influence to the final output for future work.</p>
<p>Post-processing layer.In this preliminary study, we only limit our scope to the initial generation process without additional post-processing steps.We leave additional fact-checking and correction, and potential plagiarism avoidance for future work.</p>
<p>Proprietary LLM APIs.Our prompts are designed based on OpenAI gpt-3.5-turbo-0301and gpt-4-0314.As these models may be deprecated, the results may not be replicated in the future.Moreover, the prompts may have to be updated to adapt to newly released models.Nonetheless, we argue that the key input features and general prompt format we propose should be consistently useful across any LLM.We later test our prompts on other LLMs: gpt-3.5-turbo-0613&amp; gpt-4-0613, as well as Anthropic Claude-v27 output qualitatively similar texts, while Google text-bison-32k8 output texts with less satisfactory styles.On the other hand, LLaMA-2 70B Chat (Touvron et al., 2023) fails the task by generating related work sections irrelevant to the input.</p>
<p>Limited field of studies and generalizability.</p>
<p>As Appendix Table 11 shows, our evaluated papers are concentrated on the field of computer science, particularly natural language processing, and our prompts are also designed for computer science papers.We note that it is very challenging and expensive to use authors/experts to evaluate literature reviews and we do not have the resources to tune the LLM prompts and recruit experts in other disciplines.We do evaluate a geology paper because the expert is friends with an author, and we do not see any significant difference from the computer science evaluations.We leave other domains of target papers, and more dynamic prompt template for future work.</p>
<p>Ethics Statement</p>
<p>As an early exploratory work, we use LLMs to automatically generate literature reviews.LLMs may produce inappropriate outputs, such as toxic or non-factual statements.LLMs may also plagiarize the cited papers; however, in our intended use case, generating a literature review summarizing a daily paper feed, this is less of a concern, since the review is shown only to feed owner for the purpose of assisting them in curating their reading list.</p>
<p>Because our experiments are conducted using related work sections as evaluation targets, it is possible that unscrupulous individuals may use our system to "cheat" at writing related work sections for their own publications.We strongly advise against doing so, as this violates the requirement of a fully original piece of work for academic venues.Since there is not yet an established norm around the use of generative systems in writing scientific papers, there may be some risk of harm to the scientific community from careless use of such tools, and their use might be explicitly prohibited in some contexts.Therefore, future researchers, developers, and users must be extra careful about the potential regulations.</p>
<p>From a practical standpoint, the quality of literature reviews generated using our approach is still noticeably lower than human-written ones, especially in terms of organization and writing style.In addition, the human-provided main idea plan is required for higher-quality output, and the fullyautomated setting performs very poorly, which should discourage the malicious use of our system.Our results clearly show that the human thinking process cannot be replaced by an automated system, and human readers are easily able to distinguish and criticize AI-generated content.</p>
<p>A Extractiveness of Generations</p>
<p>We use Grusky et al. (2018)'s coverage and density metrics to investigate how each of the features contributes to the generated related work sections.</p>
<p>Coverage measures how much generated text is extracted from each of the input features.As Figure 5a shows, the features have very different coverage scores.Many, such as faceted summaries, cited paper abstracts, and CTS, have high coverage across all generation variants.Since the information among different features is likely to overlap, the coverage scores do not sum to 1 for each variant; for example, faceted summaries and cited paper abstracts contain highly similar information.</p>
<p>Due to this overlap, certain features, such as  cited paper abstracts and CTS, have high coverage scores even in variants where they are not part of the input prompt (cells in cyan), suggesting they contain information that is highly valued by the LLM.The coverage of each feature varies only slightly across prompt variants, likely due to the same information overlap reason.The only exception is variant B: the human-provided main ideas cannot be found in any other feature.</p>
<p>Density measures the length of extracted fragments.As the columns of Figure 5b show, scores vary significantly among features.Interestingly, despite faceted summaries and cited paper abstracts having similar coverage scores, the former have significantly higher density scores, indicating that the LLM prefers to directly copy from the faceted summaries.The main idea and relationship between paper pairs are also highly extractive.Note that, although this kind of extraction can be vulnerable to plagiarism, it is less of a concern for our approach because the highly extractive features are not taken directly from the cited papers, but from    the feature-extraction LLM's summaries.</p>
<p>Unlike coverage, density scores vary significantly among variants, since the LLM is not able to reconstruct an exact substring that is not present in its input.When features are ablated from a variant, their density scores are much lower (cells in cyan), and the scores of the remaining features are much higher.For example, in variant G, both the enriched citation usage and the relationship between paper pairs are ablated; to compensate, G has very high density scores for the main idea and faceted summaries.</p>
<p>B Examples of Features and Generated Variants</p>
<p>We use Section 2 of this paper as an example target related work section to demonstrate our key support features.For conciseness, we show only one example per feature, as showing all features across all cited papers would be extremely long.</p>
<p>We then generate the corresponding related work section variants A-H.Due to the maximum input length limit of our LLMs, we use the approach described in Section 3.3 to generate the two paragraphs in each variant sequentially.</p>
<p>B.1 Citation Network Features</p>
<p>Faceted Summary of Hoang and Kan (2010).Objective: The objective of this paper is to introduce the problem of automated related work summarization and propose it as a challenge to the automatic summarization community.The paper aims to take initial steps towards solving this problem by dividing the task into general and specific summarization processes.</p>
<p>Method: The paper presents a prototype Related Work Summarization system, ReWoS, which takes in a set of keywords arranged in a hierarchical fashion that describes a target paper's topics, to drive      the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones.The paper also conducts a study of the argumentative patterns used in related work sections to describe the plausible summarization tactics for their creation.</p>
<p>Findings: The paper's initial results show an improvement over generic multi-document summarization baselines in a human evaluation.Contribution: The paper introduces the novel problem of automated related work summarization and proposes it as a challenge to the automatic summarization community.The paper presents a prototype system, ReWoS, that takes initial steps towards solving this problem by dividing the task into general and specific summarization processes.The paper also conducts a study of the argumentative patterns used in related work sections to describe the plausible summarization tactics for their creation.</p>
<p>Keywords: automated summarization, related work summarization, multi-document summarization, citation prediction, keyphrase extraction.Xing et al. (2020) and See et al. (2017).Xing et al. (2020) cite See et al. (2017) as a reference for the pointer-generator network architecture, which they adapt for their own task of automatic citation text generation in scholarly papers.Xing et al. (2020).Xing et al. (2020) is known for proposing a task of automatic citation text generation in scholarly papers and developing a multi-source pointergenerator network with cross attention mechanism to address it.It is cited for related work and as a baseline for comparison in citation sentence generation tasks.</p>
<p>Relationship between</p>
<p>Enriched citation intent &amp; usage of</p>
<p>B.2 Target Paper Features</p>
<p>Main idea of Section 2. The paragraph discusses previous work on automatic related work generation, including Hoang and Kan's proposal of generating the related work section given a list of papers to cite, and later extractive approaches that use probabilistic models or BERT to select and concatenate sentences from cited papers.However, these approaches result in outputs lacking coherence and stylistic variation.</p>
<p>The paragraph discusses recent abstractive approaches that focus on citation text generation, which involves generating a single citation sentence or span given the cited paper(s) and assuming the rest of the related work section and other sections of the target paper are available.These approaches use various architectures and mostly rely on cited paper abstracts rather than full texts.However, there is no standard benchmark evaluation approach to compare methods from different prior works.</p>
<p>B.3 CTS</p>
<p>Extracted from Hoang and Kan (2010).[Structure of Related Work Section] Each article's data consists of the reference related work summary, the collection of the input research articles Inspired by the above observations, we propose a novel strategy for related work summarization with respect to a given topic tree.</p>
<p>C Example Generated Variants</p>
<p>See Figures 6-13 for the reproduced Section 2 of this paper.</p>
<p>In the last decade, several works have addressed the task of automatic related work generation.Hoang et al. (2010) introduced the problem of automated related work summarization and proposed a prototype system, ReWoS, which laid the foundation for subsequent works in the field.Hu et al. (2014) built upon this work by proposing a novel system for automatic related work generation using an optimization approach.Chen et al. (2019) further explored this area by proposing an approach for automatically generating a related work section in scientific papers through summarizing citations.Wang et al. (2019) introduced a new framework, ToC-RWG, for automatic related work generation by incorporating topic model and citation information.Deng et al. (2021) proposed a novel and effective system for generating related work sections based on sentence extraction and reordering using a BERT-based ensemble model.However, these approaches result in outputs lacking coherence and stylistic variation, which our work aims to address by proposing a workflow for generating a full related work section using an abstractive approach and prompting large language models with key support features.</p>
<p>Recent abstractive approaches have focused on citation text generation, which involves generating a single citation sentence or span given the cited paper(s) and assuming the rest of the related work section and other sections of the target paper are available.For instance, Abura'ed et al. (2020) proposed a method for generating related work sections in scientific documents using neural sequence learning, while Xing et al. (2020) introduced a task of automatic citation text generation in scholarly papers and developed a multi-source pointer-generator network with cross attention mechanism to address it.Ge et al. (2021) proposed a framework for citing sentence generation that considers both background knowledge and content information, and Luu et al. (2021) developed a dataset and model for explaining relationships between scientific documents using natural language text.Chen et al. (2021) proposed a relation-aware multi-document encoder for abstractive related work section generation.However, there is no standard benchmark evaluation approach to compare methods from different prior works.In the last decade, several works have addressed the task of automatic related work generation.Hoang et al. (2010) introduced the problem of automated related work summarization and proposed a prototype system, ReWoS, which laid the foundation for subsequent works in the field.Hu et al. (2014) built upon this work by proposing a novel system for automatic related work generation using an optimization approach.Chen et al. (2019) further explored the idea of generating related work sections by summarizing citations, while Wang et al. (2019) proposed a new framework, ToC-RWG, that incorporates topic model and citation information for related work generation.Deng et al. (2021) introduced a novel and effective system for generating related work sections based on sentence extraction and reordering using a BERT-based ensemble model.In contrast to these approaches, our work proposes a workflow for generating a full related work section by prompting large language models (LLMs) with key support features, aiming to produce a related work section that is comparable to a human first draft.</p>
<p>In recent years, several studies have focused on automatic related work generation.Abura'ed et al. (2020) proposed a method for generating related work sections in scientific documents using neural sequence learning.Xing et al. (2020) introduced a task of automatic citation text generation in scholarly papers and developed a multi-source pointer-generator network with cross attention mechanism to address it.Ge et al. (2021) proposed a framework for citing sentence generation that considers both background knowledge and content information.Luu et al. (2021) developed a dataset and model for explaining relationships between scientific documents using natural language text.Chen et al. (2021) proposed a relation-aware multi-document encoder for abstractive related work section generation.Li et al. (2022) developed a dataset for citation-oriented related work annotation and proposed a framework for automatic related work generation.These works have employed various techniques, such as Transformer models (Vaswani et al., 2017), pointer-generator networks (See et al., 2017), and large language models (Radford et al., 2019;Beltagy et al., 2020).Our work builds upon these studies by proposing a workflow for generating a related work section that is comparable to a human draft by extracting and integrating key support features into informative prompts for an LLM.The problem of automated related work summarization was first introduced by Hoang et al. ( 2010), who proposed a prototype system, ReWoS, for generating related work sections.Subsequent works have built upon this foundation, such as Hu et al. (2014), who proposed an optimization approach for automatic related work generation using a PLSA model and regression models.Chen et al. (2019) introduced an approach for generating related work sections by summarizing citations, while Wang et al. (2019) proposed the ToC-RWG framework, which incorporates topic models and citation information.More recently, Deng et al. (2021) developed a BERT-based ensemble model for sentence extraction and reordering in related work sections.However, these extractive approaches often result in outputs lacking coherence and stylistic variation, motivating our work on generating a full related work section by prompting large language models and identifying key support features.</p>
<p>Recent abstractive approaches to citation text generation have focused on generating single citation sentences or spans, given the cited paper(s) and assuming the availability of the rest of the related work section and other sections of the target paper.Abura'ed et al. ( 2020) proposed a method for generating related work sections in scientific documents using neural sequence learning, while Xing et al. (2020) introduced a task of automatic citation text generation in scholarly papers and developed a multi-source pointer-generator network with cross attention mechanism to address it.Ge et al. ( 2021) proposed a framework for citing sentence generation that considers both background knowledge and content information, and Luu et al. (2021) developed a dataset and model for explaining relationships between scientific documents using natural language text.Chen et al. (2021) proposed a relation-aware multi-document encoder for abstractive related work section generation, and Li et al. (2022) developed a dataset for citation-oriented related work annotation and proposed a framework for automatic related work generation.However, there is no standard benchmark evaluation approach to compare methods from different prior works, and most of these approaches rely on cited paper abstracts rather than full texts.In the last decade, several works have addressed the task of automatic related work generation.Hoang et al. ( 2010) introduced the problem of automated related work summarization and proposed a prototype system for it.Subsequent works, such as Hu et al. (2014), proposed a system for automatic related work generation using an optimization approach, while Chen et al. ( 2019) focused on generating related work sections through summarizing citations.Wang et al. (2019) proposed a framework for automatic related work generation by incorporating topic model and citation information.However, these extractive approaches often result in outputs lacking coherence and stylistic variation.Deng et al. (2021) addressed this issue by proposing a novel system for generating related work sections based on sentence extraction and reordering using a BERT-based ensemble model.In contrast, our work explores generating a full related work section using an abstractive approach by prompting large language models with key support features, aiming to produce a related work section that is comparable to a human first draft.</p>
<p>Recent abstractive approaches have focused on citation text generation, which involves generating a single citation sentence or span given the cited paper(s) and assuming the rest of the related work section and other sections of the target paper are available.Xing et al. (2020) proposed a task of automatic citation text generation in scholarly papers and developed a multi-source pointer-generator network with cross attention mechanism to address it.Ge et al. ( 2021) proposed a framework for citing sentence generation that considers both background knowledge and content information.Luu et al. (2021) developed a dataset and model for explaining relationships between scientific documents using natural language text, using citing sentences as a proxy for the task.Abura'ed et al. (2020) proposed a method for generating related work sections in scientific documents using neural sequence learning.Chen et al. (2021) proposed a relation-aware multi-document encoder for abstractive related work section generation.These approaches mostly rely on cited paper abstracts rather than full texts and use various architectures, such as Transformer models (Vaswani et al., 2017) and Longformer (Beltagy et al., 2020).However, there is no standard benchmark evaluation approach to compare methods from different prior works, as highlighted by Li et al. (2022), who conducted a meta-study on automatic related work generation.In the last decade, several works have addressed the task of automatic related work generation.Hoang et al. ( 2010) introduced the problem and proposed a prototype system, ReWoS, for automated related work summarization.Later, extractive approaches emerged, such as Hu et al. (2014), who proposed an optimization approach for automatic related work generation using a PLSA model, and Chen et al. (2019), who focused on summarizing citations for related work generation.Wang et al. (2019) proposed the ToC-RWG framework, which combined topic modeling and citation information for related work generation.Deng et al. (2021) introduced a BERT-based ensemble model for sentence extraction and reordering to generate related work sections.However, these extractive approaches often result in outputs lacking coherence and stylistic variation, motivating our work on a fully abstractive related work generation system using large language models.</p>
<p>Recent abstractive approaches have focused on citation text generation, which involves generating a single citation sentence or span given the cited paper(s) and assuming the rest of the related work section and other sections of the target paper are available.Xing et al. (2020) proposed a new task of automatic citation text generation in scholarly papers and developed a multi-source pointer-generator network with cross attention mechanism to address it.Ge et al. ( 2021) proposed a framework for citing sentence generation that considers both background knowledge and content information, while Abura'ed et al. (2020) designed and evaluated an abstractive related work section generation system.Luu et al. (2021) focused on explaining relationships between scientific documents using natural language text, using citing sentences as a proxy for the task.Chen et al. (2021) addressed the abstractive related work generation task and proposed a relation-aware multi-document encoder.Li et al. (2022) developed a dataset for citation-oriented related work annotation and proposed a framework for automatic related work generation.However, there is no standard benchmark evaluation approach to compare methods from different prior works.The field of automatic related work generation has evolved significantly since Hoang et al. ( 2010) introduced the problem and proposed a prototype system, ReWoS, for automated related work summarization.Subsequent works have explored various approaches to tackle this challenge.Hu et al. (2014) proposed an optimization-based system, ARWG, which leverages a PLSA model and regression models to generate related work sections.Chen et al. (2019) introduced an approach that generates related work sections by summarizing citations, while Wang et al. (2019) presented a framework, ToC-RWG, that incorporates topic models and citation information for related work generation.More recently, Deng et al. (2021) proposed SERGE, a BERT-based ensemble model for sentence extraction and reordering to generate descriptive related work sections.Despite these advancements, existing extractive approaches often result in outputs lacking coherence and stylistic variation, motivating the need for more advanced abstractive methods, such as those based on BERT (Devlin et al., 2019), to further improve the quality of automatically generated related work sections.</p>
<p>Recent abstractive approaches have focused on citation text generation, which involves generating a single citation sentence or span given the cited paper(s) and assuming the rest of the related work section and other sections of the target paper are available.These approaches use various architectures and mostly rely on cited paper abstracts rather than full texts.For instance, Abura 'ed et al. (2020) proposed a method for generating related work sections using neural sequence learning, while Xing et al. (2020) introduced a task of automatic citation text generation and developed a multi-source pointer-generator network with cross attention mechanism.Ge et al. (2021) proposed a framework for citing sentence generation that considers both background knowledge and content information.Luu et al. (2021) developed a dataset and model for explaining relationships between scientific documents using natural language text.Chen et al. (2021) proposed a relation-aware multi-document encoder for abstractive related work section generation.However, there is no standard benchmark evaluation approach to compare methods from different prior works.Some studies, such as Li et al. (2022), have conducted meta-studies on automatic related work generation, comparing existing literature and identifying potential areas for future research.In the last decade, several works have addressed the task of automatic related work generation.Hoang et al. (2010) introduced the problem of automated related work summarization and proposed a prototype system, ReWoS, which generates extractive summaries using keywords arranged in a hierarchical fashion.Hu et al. (2014) proposed an optimization approach, ARWG, which uses a PLSA model to split the sentence set of given papers into different topic-biased parts and employs regression models to learn the importance of sentences.Chen et al. (2019) presented an approach for generating related work sections by summarizing citations, while Wang et al. (2019) proposed a framework, ToC-RWG, that incorporates topic models and citation information for related work generation.More recently, Deng et al. (2021) developed SERGE, a BERT-based ensemble model for sentence extraction and reordering to generate descriptive related work sections.However, these extractive approaches often result in outputs lacking coherence and stylistic variation, motivating the need for more advanced abstractive methods.</p>
<p>Recent abstractive approaches have focused on citation text generation, which involves generating a single citation sentence or span given the cited paper(s) and assuming the rest of the related work section and other sections of the target paper are available.These approaches use various architectures and mostly rely on cited paper abstracts rather than full texts (Abura'ed et al., 2020;Xing et al., 2020;Ge et al., 2021;Luu et al., 2021;Chen et al., 2021).For instance, Abura'ed et al. (2020) designed and evaluated an abstractive related work section generation system, while Xing et al. (2020) proposed a multi-source pointer-generator network with a cross-attention mechanism for citation text generation.Ge et al. ( 2021) introduced a framework called BACO that considers both background knowledge and content information, and Luu et al. (2021) used citing sentences as a proxy for explaining relationships between scientific documents.Chen et al. (2021) proposed a Relation-aware Related work Generator (RRG) for abstractive related work section generation from multiple scientific papers.Li et al. (2022) developed a dataset for citation-oriented related work annotation and proposed a framework for automatic related work generation.However, there is no standard benchmark evaluation approach to compare methods from different prior works.The task of automatic related work generation has been explored in various ways over the past decade.Hoang et al. (2010) introduced the problem and proposed a prototype system, ReWoS, which laid the foundation for subsequent works.Hu et al. (2014) built upon this by proposing an optimization approach for related work generation, while Chen et al. (2019) focused on summarizing citations to generate related work sections.Wang et al. (2019) proposed the ToC-RWG framework, which incorporated topic models and citation information for related work generation.Deng et al. (2021) developed a BERT-based ensemble model for sentence extraction and reordering to generate related work sections.However, these approaches often result in outputs lacking coherence and stylistic variation, motivating our work on generating a full related work section using an abstractive approach with large language models.</p>
<p>In recent years, several abstractive approaches have focused on citation text generation, which involves generating a single citation sentence or span given the cited paper(s) and assuming the rest of the related work section and other sections of the target paper are available.For instance, Abura'ed et al. (2020) proposed a method for generating related work sections in scientific documents using neural sequence learning, while Xing et al. (2020) introduced a task of automatic citation text generation in scholarly papers and developed a multi-source pointer-generator network with cross attention mechanism to address it.Ge et al. (2021) proposed a framework for citing sentence generation that considers both background knowledge and content information, and Luu et al. (2021) developed a dataset and model for explaining relationships between scientific documents using natural language text.Chen et al. (2021) addressed the abstractive related work generation task and proposed a relation-aware multi-document encoder.However, there is no standard benchmark evaluation approach to compare methods from different prior works, and most of these approaches rely on cited paper abstracts rather than full texts.</p>
<p>Figure 2 :
2
Figure 2: Prompt format for generating a literature review paragraph (simplified for length).</p>
<p>Feature</p>
<p>Figure 3 :
3
Figure 3: Bar chart showing the distribution of human evaluation scores.</p>
<p>Figure 4 :
4
Figure 4: Bar chart of the number of factual errors.</p>
<p>Figure 5 :
5
Figure 5: Extractiveness of generated related work sections (n = 38), measured by coverage and density against input features.Scores for features not included in the prompt for a variant are shown in cyan.</p>
<p>Figure 6 :
6
Figure 6: Variant A (baseline).</p>
<p>Figure 7 :
7
Figure 7: Variant B (main idea ablated).</p>
<p>Figure 8 :
8
Figure 8: Variant C (target TAIC ablated).</p>
<p>Figure 9 :
9
Figure 9: Variant D (cited paper abstracts instead of faceted summaries).</p>
<p>Figure 10 :
10
Figure 10: Variant E (citation intent/usage ablated).</p>
<p>Figure 11 :
11
Figure 11: Variant F (relationship between papers ablated).</p>
<p>Figure 12 :
12
Figure 12: Variant G (citation intent/usage and relationship between papers ablated).</p>
<p>Figure 13 :
13
Figure 13: Variant H (baseline with CTS enhancement).</p>
<p>PromptThe title, abstract, introduction and conclusion section of the target paper are as follows: Write a literature review that concisely cites the following papers in a natural way using all of the main ideas as the main story....You can freely reorder the cited papers to adapt to the main ideas.
Title: {{title}}Abstract: {{abstract}}Introduction: {{introduction}}Conclusion: {{conclusion}}... ...Main idea of our literature review:{{main ideas}}List of cited papers:1. {{titleB1}} by {{authorB1}} et al. {{yearB1}}{{Faceted Summary or Abstract of B1}}<Usage> {{Enriched citation usage of B1}}How other papers cite it:{{Relation between Ax and B1}}{{Relation between Ay and B1}}...Potentially useful sentences from the target paper:{{section #1}} {{CTS #1}}{{section #2}} {{CTS #2}}...2.{{titleB2}} by {{authorB2}} et al.{{yearB2}}......</p>
<p>Table 1 :
1
Features in each generation variant.
Variant ROUGE-1 ROUGE-2 ROUGE-LA0.5130.2160.248B0.4460.1310.177C0.5010.2010.235D0.5110.2150.249E0.5140.2230.255F0.5200.2210.252G0.5170.2250.256H0.5130.2150.249</p>
<p>Table 2 :
2
ROUGE scores of generated variants evaluated against the gold related work sections.Bold indicates improvement over baseline A, while italics indicate lowered performance.Coherence 3.30 3.07 3.33 3.70 3.59 3.59 3.52 3.37 Rel target 3.78 3.67 3.89 4.19 4.11 4.00 4.07 4.00 Rel cited 4.22 3.93 4.15 4.22 4.19 4.19 4.00 4.04 Factuality 4.04 3.89 3.74 3.89 3.93 4.30 3.93 3.74 Usefulness 3.74 3.30 3.59 3.52 3.85 3.70 3.59 3.78
5 Results and AnalysesAutomatic Evaluation. Table 2 shows theROUGE scores of our generated variants comparedto the gold related work sections. Overall, mostvariants yield decent scores, indicating that theyare mostly on-topic. Notably, variant B has signif-icantly lower ROUGE scores than other variants,which makes sense because it is the only one with-out the main idea plan. This emphasizes the impor-tant and irreplaceable nature of the guiding plan.Variant C, which has the main ideas but no targetpaper TAIC is the next lowest, again suggestingthat features related to the reader's perspective arethe most important for a good literature review.Human Evaluation. Due to the challenging andexpensive nature of evaluating highly specialized
idea plan.B is the only variant that does not use the main ideas, making it the only variant that could be generated completely automatically.Variant C ablates the TAIC of the target paper, while D replaces each cited paper's faceted summary with its abstract.Variants E, F , and G ablate the enriched citation intent and usage, the relationship between paper pairs, and both, respectively.Finally, variant H adds the CTS-based re-generation step.academicresearchpapers, we are only able to evaluate one target related work section per domain expert judge, with 27 judges in total.Table3shows the average human evaluation scores across all 27 judges.Writing is a highly personal and idiosyn-</p>
<p>Table 3 :
3
Average human evaluation scores.
DiffVrt ♡Vrt% Tie% ♡Bsl%− main storyB22.229.648.1− target TAICC22.237.040.7− faceted summary D + cited abstract29.648.122.2− intent/usageE40.737.022.2− relationshipF40.729.629.6− intent/usage − relationshipG22.233.344.4+ CTSH25.937.037.0</p>
<p>Table 4 :
4
Comparison of human overall scores across variants, with respect to the baseline A.</p>
<p>Table 5 :
5
Li et al. (2022)writing style analysis.Percentage of the discourse role of sentences (top) or citation types (bottom) within each variant.Bold indicates styles used more frequently in generated variants than gold related work sections; italics indicate less frequent styles.
Label% Gld A B C D E F G HTransition 31.1 17.0 10.9 18.5 19.7 17.0 19.3 19.6 20.1Single-Sum 28.2 47.2 59.8 51.7 40.7 45.7 46.2 45.4 40.7Narrative 20.8 11.3 3.5 8.4 13.5 14.5 10.0 14.1 14.1Reflection 15.4 17.0 21.6 15.4 19.3 17.4 16.9 16.9 17.8Multi-Sum 3.6 7.5 3.3 6.0 6.8 5.5 7.4 4.0 6.6Dominant 34.7 70.0 81.4 77.1 64.7 63.6 70.3 60.1 63.2Reference 65.3 30.0 18.6 22.9 35.3 36.4 29.7 39.9 36.8</p>
<p>Table 6 :
6
Prompt and output format for generating faceted summary of a paper.
PromptFaceted summary of the citing paper, {{title A}} by{{author A}} et al. {{year A}}:{{Faceted Summary A}}Faceted summary of the cited paper, {{title B}} by{{author B}} et al. {{year B}}:{{Faceted Summary B}}Citation contexts that {{author A}} et al. {{year A}}cites {{author B}} et al. {{year B}} (which is cited as{{citation marker of B in A}})):1. {{span #1}}2. {{span #2}}......Very briefly explain the relationship between {{authorA}} et al. {{year A}} and {{title B}} by {{author B}}et al. {{year B}}. TLDR:Relation Between Paper Pairs{{author A}} et al. {{year A}} cites {{author B}} et al.{{year B}} ......</p>
<p>Table 7 :
7
Prompt and output format for generating the relationship between paper pairs.The "citation marker of B in A" is how paper A refers to paper B, e.g."B et al. (2023)" or simply "[1]".</p>
<p>Table 8 :
8
Prompt and output format for generating enriched citation intent and usage of cited papers.
PromptOur title: {{title}}Faceted summary of our paper:{{Faceted Summary}}Write a short summary of the main idea of the followingrelated work section paragraphs. Ignore citations.{{Human-written related work section}}Main idea of the target related work section{{main ideas}}</p>
<p>Table 9 :
9
Prompt and output format for generating the main ideas of the target related work section.</p>
<p>Table 10 :
10
Prompt and output format for generating the full target related work section.Cited papers are given in chronological order.</p>
<p>Table 11 :
11
Distribution of the field of study among the human-rated related work sections.
Field of StudyCountNatural Language Processing14Machine Learning4Speech and Audio Processing3Computer Vision2Programming Languages1Robotics1Computer Graphics1Geoscience1Year Count2022 132023720214201912018120161</p>
<p>Table 12 :
12
Distribution of the publication year among the human-rated related work sections.</p>
<p>Table 13 :
13
Average (and standard deviation) of human evaluation scores.</p>
<p>Table 14 :
14
Average and standard deviation of the variation with the best human evaluation overall score.</p>
<p>https://www.semanticscholar.org/faq/ what-are-research-feeds
https://www.bing.com/new
The number of paragraphs we generate in one shot depends on the total number of cited papers; if there are too many, the input prompt becomes too long to generate more than one paragraph at a time.
  4  We useLi et al. (2022)'s citation tagger to extract spans.
We adjust k case-by-case so the prompt length does not exceed the LLM's input window, with a hard cap at k = 10.
https://pypi.org/project/ googlesearch-python/
https://www.anthropic.com/index/claude-2
https://cloud.google.com/vertex-ai/docs/ generative-ai/model-reference/text</p>
<p>A multi-level annotated corpus of scientific papers for scientific document summarization and cross-document relation discovery. Ahmed Abura'ed, Horacio Saggion, Luis Chiruzzo, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2020</p>
<p>Automatic related work section generation: experiments in scientific document abstracting. Ahmed Abura'ed, Horacio Saggion, Alexander Shvets, Àlex Bravo, Scientometrics. 1252020</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020arXiv preprint</p>
<p>Ranking with recursive neural networks and its application to multi-document summarization. Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, Ming Zhou, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201529</p>
<p>Automatic generation of related work through summarizing citations. Jingqiang Chen, Hai Zhuge, Concurrency and Computation: Practice and Experience. 313e42612019</p>
<p>Capturing relations between scientific papers: An abstractive model for related work section generation. Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan, 10.18653/v1/2021.acl-long.473Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Structural scaffolds for citation intent classification in scientific publications. Arman Cohan, Waleed Ammar, Madeleine Van Zuylen, Field Cady, 10.18653/v1/N19-1361Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Automatic related work section generation by sentence extraction and reordering. Zekun Deng, Zixin Zeng, Weiye Gu, Jiawen Ji, Bolin Hua, AII@ iConference. 2021</p>
<p>Ensemble-style self-training on citation classification. Cailing Dong, Ulrich Schäfer, Proceedings of 5th International Joint Conference on Natural Language Processing. 5th International Joint Conference on Natural Language ProcessingChiang Mai, Thailand2011Asian Federation of Natural Language Processing</p>
<p>BACO: A background knowledge-and content-based framework for citing sentence generation. Eugene Garfield, 10.18653/v1/2021.acl-long.116Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics1965. 2021269Statistical association methods for mechanized documentation, symposium proceedings</p>
<p>Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. Max Grusky, Mor Naaman, Yoav Artzi, 10.18653/v1/N18-1065Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers; New Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Towards automated related work summarization. Cong Duy, Vu Hoang, Min-Yen Kan, Coling 2010 Organizing Committee. Beijing, China2010Coling 2010: Posters</p>
<p>Automatic generation of related work sections in scientific papers: An optimization approach. Yue Hu, Xiaojun Wan, 10.3115/v1/D14-1170Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, Qatar2014Association for Computational Linguistics</p>
<p>Insights from cl-scisumm 2016: the faceted scientific document summarization shared task. Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal Rustagi, Min-Yen Kan, International Journal on Digital Libraries. 1922018</p>
<p>Kokil Jaidka, Michihiro Yasunaga, Muthu Kumar Chandrasekaran, Dragomir Radev, Min-Yen Kan, arXiv:1909.00764The cl-scisumm shared task 2018: Results and key insights. 2019arXiv preprint</p>
<p>Measuring the evolution of a scientific field through citation frames. David Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-Farland, Dan Jurafsky, 10.1162/tacl_a_00028Transactions of the Association for Computational Linguistics. 62018</p>
<p>Analysis of the macro-level discourse structure of literature reviews. Jin-Cheon Christopher Sg Khoo, Kokil Na, Jaidka, 2011Online Information Review</p>
<p>MultiCite: Modeling realistic citations requires moving beyond the single-sentence singlelabel setting. Anne Lauscher, Brandon Ko, Bailey Kuehl, Sophie Johnson, Arman Cohan, David Jurgens, Kyle Lo, 10.18653/v1/2022.naacl-main.137Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>CORWA: A citation-oriented related work annotation dataset. Xiangci Li, Biswadip Mandal, Jessica Ouyang, 10.18653/v1/2022.naacl-main.397Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United States2022Association for Computational Linguistics</p>
<p>Automatic related work generation: A meta study. Xiangci Li, Jessica Ouyang, arXiv:2201.018802022arXiv preprint</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Explaining relationships between scientific documents. Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle Lo, Isabel Cachola, Noah A Smith, 10.18653/v1/2021.acl-long.166Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Bringing structure into summaries: a faceted summarization dataset for long scientific documents. Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, Daqing He, 10.18653/v1/2021.acl-short.137Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20212Short Papers). Association for Computational Linguistics</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Get to the point: Summarization with pointergenerator networks. Abigail See, Peter J Liu, Christopher D Manning, 10.18653/v1/P17-1099Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Automatic classification of citation function. Simone Teufel, Advaith Siddharthan, Dan Tidhar, Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. the 2006 Conference on Empirical Methods in Natural Language ProcessingSydney, AustraliaAssociation for Computational Linguistics2006</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Automatic classification of algorithm citation functions in scientific literature. Suppawong Tuarob, Sung Woo Kang, Poom Wettayakorn, Chanatip Pornprasit, Tanakitti Sachati, Saeed-Ul Hassan, Peter Haddawy, IEEE Transactions on Knowledge and Data Engineering. 32102019</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Toc-rwg: Explore the combination of topic model and citation information for automatic related work generation. Pancheng Wang, Shasha Li, Haifang Zhou, Jintao Tang, Ting Wang, IEEE Access. 82019</p>
<p>Automatic generation of citation texts in scholarly papers: A pilot study. Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, 10.18653/v1/2020.acl-main.550Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, Dragomir R Radev, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>A context-based framework for modeling the role and function of on-line resource citations in scientific literature. He Zhao, Zhunchen Luo, Chong Feng, Anqing Zheng, Xiaopeng Liu, 10.18653/v1/D19-1524Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2608 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2608</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2608</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-02bc553b16b643ca2063bd7ae1270fe4288d8009</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/02bc553b16b643ca2063bd7ae1270fe4288d8009" target="_blank">Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> LA-MCTS serves as a meta-algorithm by using existing black- box optimizers as its local models, achieving strong performance in general black-box optimization and reinforcement learning benchmarks, in particular for high-dimensional problems.</p>
                <p><strong>Paper Abstract:</strong> High dimensional black-box optimization has broad applications but remains a challenging problem to solve. Given a set of samples $\{\vx_i, y_i\}$, building a global model (like Bayesian Optimization (BO)) suffers from the curse of dimensionality in the high-dimensional search space, while a greedy search may lead to sub-optimality. By recursively splitting the search space into regions with high/low function values, recent works like LaNAS shows good performance in Neural Architecture Search (NAS), reducing the sample complexity empirically. In this paper, we coin LA-MCTS that extends LaNAS to other domains. Unlike previous approaches, LA-MCTS learns the partition of the search space using a few samples and their function values in an online fashion. While LaNAS uses linear partition and performs uniform sampling in each region, our LA-MCTS adopts a nonlinear decision boundary and learns a local model to pick good candidates. If the nonlinear partition function and the local model fits well with ground-truth black-box function, then good partitions and candidates can be reached with much fewer samples. LA-MCTS serves as a \emph{meta-algorithm} by using existing black-box optimizers (e.g., BO, TuRBO) as its local models, achieving strong performance in general black-box optimization and reinforcement learning benchmarks, in particular for high-dimensional problems.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2608.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2608.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LA-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Action Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-level black-box optimization algorithm that learns adaptive, recursive partitions of the search space (via simple node models: KMeans + SVM) and uses Monte Carlo Tree Search (UCB) to allocate evaluations; inside selected partitions it runs local optimizers (e.g., TuRBO or BO) to pick candidate evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LA-MCTS (Latent Action Monte Carlo Tree Search)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LA-MCTS represents the search domain as a tree of regions (nodes). Each internal node defines a latent action: a boundary learned from the node's samples by clustering (KMeans on [x,f(x)]) to identify high/low performing clusters, then fitting an SVM (linear or nonlinear) to separate them; the learned classifier partitions the parent region into a 'good' child and a 'bad' child. The tree is grown dynamically: when a leaf accumulates more than a threshold θ samples it is split. A UCB-style selection (traversing root→leaf) balances visiting promising vs underexplored regions. For sampling inside a selected leaf, LA-MCTS uses an off-the-shelf local optimizer (default: TuRBO-1) or a standard BO acquisition (EI) constrained to the intersection of SVM constraints along the path. All new evaluations are returned to update node statistics (visit counts n_j and node value v_j = average f over samples in region) and to possibly trigger further splitting. Key hyperparameters are Cp (UCB exploration weight) and θ (split threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General high-dimensional black-box optimization (demonstrated on neural architecture search background, MuJoCo RL policy optimization, synthetic functions, trajectory optimization) — applicable to experimental design/automated discovery where evaluations are expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Region-level allocation via MCTS/UCB: allocate more evaluations to leaves with higher UCB score computed from node statistics (visit count n_j and average value v_j) where Cp controls exploration; tree splitting creates a hierarchical partition so allocation is hierarchical (more samples are assigned to child regions estimated as good); inside a selected region, allocate evaluations using a local optimizer (TuRBO or BO) until its stopping criterion (e.g., trust-region shrinks to zero) — then return all samples to the tree and repeat. Splitting threshold θ determines when to grow/allocate additional modeling capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primary cost metric used is number of black-box evaluations (samples / function evaluations). Wall-clock/runtime is discussed qualitatively (e.g., TuRBO collects 10^4 samples in ≈1 hour on 1 V100 GPU) but the algorithm's design and experiments optimize for sample efficiency rather than explicit FLOPs/dollar costs.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Two forms: (1) region-selection: node value v_j = empirical average f(x) within region (used in UCB), and UCB formula that trades off exploitation (high v_j / visit count) and exploration (√(log n_parent / n_j)); (2) within-region sampling: when LA-MCTS uses regular BO it optimizes an acquisition function (paper states Expected Improvement (EI) is used for BO experiments), which implicitly measures expected improvement/information about better optima.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Multi-level: (a) Tree-level: UCB (Upper Confidence Bound) traversal from root to leaf, using ucb_j = (n_j/n_j) + 2*C_p * sqrt(2*log(n_parent)/n_j) (paper uses UCB1-like term) and Cp as tunable exploration weight; (b) Within-leaf: BO acquisition (EI) or TuRBO trust-region sampling that itself balances exploration/exploitation via its acquisition and trust-region mechanism; Cp controls the extent of exploration (ablation recommends Cp ≈ 0.01–0.1 of max f).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit diversity via the hierarchical partitioning and UCB: the latent-action splits produce disjoint regions (good/bad) and UCB encourages visiting under-sampled regions, producing diverse coverage of hypotheses/regions. There is no separate explicit diversity objective (e.g., explicit novelty score); diversity is achieved by exploration term in UCB and by keeping multiple leaves available to sample from.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-budget in terms of number of function evaluations (samples); experiments also considered wall-clock/time-limited runs (e.g., 3-day runs for some BO baselines) and computational resources (GPU used for TuRBO).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>LA-MCTS handles budget constraints by (1) framing search as sequential allocation of samples via UCB-guided tree traversal, (2) splitting only when a leaf has ≥ θ samples to avoid over-allocating modeling capacity, and (3) running a local optimizer inside a leaf until its internal stopping criterion (e.g., TuRBO trust-region collapse), then returning evaluations to the tree; hyperparameters Cp and θ tune how budget is distributed between exploring new regions vs exploiting known good regions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Breakthroughs are identified by achieving high f(x) values / improvements over prior best; concrete metrics used in experiments include reward thresholds (for MuJoCo tasks) and objective value/regret (e.g., minimization regrets |v_selected - v*|). The paper measures reaching pre-defined reward thresholds and reduction in regret toward global optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics are mainly sample-efficiency and achieved objective values: (a) number of samples (episodes) to reach reward thresholds in MuJoCo tasks (Table 2): Swimmer-v2 threshold 325 reached by LA-MCTS in 126 episodes, Hopper-v2 threshold 3120 reached in 2913 episodes, HalfCheetah-v2 threshold 3430 reached in 3967 episodes; several harder tasks (Walker2d, Ant, Humanoid) did not reach thresholds under allotted budgets with LA-MCTS (reported N/A with best rewards given). (b) Learning curves (objective vs #samples) across benchmarks (MuJoCo, synthetic functions) showing LA-MCTS finds better solutions after ≈30k samples compared to baselines; (c) reduction in region regret |v^+ - v*| as number of splits grows (Figure 7(a)). Runtimes: TuRBO reported to collect 10^4 samples in ≈1 hour on 1 V100 GPU (paper runtime notes).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against numerous baselines: TuRBO (local BO), HesBO (low-dimensional embeddings), BOHB, Bayesian optimization variants, evolutionary algorithms (CMA-ES, Differential Evolution, Shiwa), MCTS/optimistic partitioning methods (VOO, SOO, DOO), Dual Annealing, Random Search, and LaNAS (prior learned-partition method).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LA-MCTS consistently improves over TuRBO and standard BO in high-dimensional problems (Figures 5 and 6); qualitatively, LA-MCTS reduces BO's over-exploration by constraining BO to learned promising partitions and thus fits better local models. Example (Table 2): on Swimmer-v2 LA-MCTS reaches reward 325 in 126 samples (substantially faster than some baselines shown); on high-dimensional locomotion (Ant, Humanoid) LA-MCTS outperforms many baselines in objective achieved by 30k samples (plots show superior final objective). Exact numeric improvements vs TuRBO are presented in plots (Figure 5, Figure 6) rather than as single consolidated percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirically LA-MCTS reduces sample complexity in many benchmarks (e.g., reaches Swimmer-v2 threshold in 126 samples; improves TuRBO's performance on synthetic functions and RL tasks). The paper does not present a single global % improvement; gains are task-dependent and shown in per-task plots and tables — improvements are most pronounced in high-dimensional problems where partitioning reduces the effective region size for local models.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Ablation studies analyze tradeoffs: (1) Cp (UCB exploration weight): too small Cp → over-exploitation and poor performance; too large Cp → over-exploration and inefficiency; recommendation Cp ≈ 0.01–0.1 × max f. (2) Splitting threshold θ: smaller θ grows tree faster (deeper partitions) which helps focus on promising regions in very large spaces but if θ too small boundary estimates become unreliable and performance degrades (θ=10 often good; θ=2 too small). (3) SVM kernel choice: nonlinear kernels (polynomial, RBF) produce more flexible boundaries and often better performance than linear. The paper also notes LA-MCTS bears extra exploration cost relative to gradient methods and thus can be less sample-efficient on tasks where pure exploitation (gradients) is effective.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key principles: (a) allocate more samples to promising regions while retaining systematic exploration via a UCB-style term (Cp tuned to task scale); (b) dynamically learn partitions from samples so allocation adapts to objective structure rather than fixed, objective-independent partitioning; (c) use local optimizers (TuRBO/BO) inside constrained regions to focus computational budget efficiently; (d) split regions only when enough samples exist (threshold θ) to avoid premature, unreliable partitions. Recommendations from experiments: Cp ≈ 1%–10% of max f, θ around 10 for large search domains, prefer nonlinear SVM kernels when sufficient samples are available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2608.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2608.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TuRBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TuRBO (Trust-Region Bayesian Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art local Bayesian optimization method that runs multiple (or single) trust regions, fitting local surrogate models and adapting trust-region sizes to balance local exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scalable global optimization via local bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TuRBO (Trust-Region Bayesian Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TuRBO operates by maintaining one or many local trust regions, fitting Gaussian-process-like or surrogate models inside each trust region, sampling via acquisition functions within trust-region bounding boxes, restarting trust regions periodically, and shrinking/expanding trust radii based on observed progress; it is designed for scalable BO in high-dimensional spaces by focusing modeling power locally.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-dimensional black-box optimization; used as a local sampler inside LA-MCTS and as a baseline in RL and synthetic benchmark tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate evaluations within trust regions centered at current best solutions; can run multiple independent trust regions (TuRBO-20 in some experiments) to parallelize exploration. In LA-MCTS integration, TuRBO is initialized and constrained to the LA-MCTS-selected region and runs until its trust-region collapse stopping criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured in number of evaluations; runtime examples include TuRBO collecting 10^4 samples in ~1 hour on 1 V100 GPU (paper runtime note).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses BO-style acquisition functions (e.g., Expected Improvement) to select within-trust-region samples, implicitly measuring expected improvement/information about better solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Trust-region adaptation: local surrogate and acquisition function drive exploration within region; trust-region expansion/shrinkage and multiple independent regions provide exploration across space vs exploitation inside regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Multiple independent trust regions (when used) provide diverse local searches; in LA-MCTS integration, diversity is obtained indirectly by running separate TuRBO instances inside different LA-MCTS-selected partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of function evaluations; internal stopping when trust-region radius hits zero.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Runs until re-start or until trust-region collapses; returns all samples to LA-MCTS which then re-allocates budget across tree.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement in objective via acquisition-driven sampling and local best improvement (EI); no explicit breakthrough novelty metric in this paper's usage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as baseline and inside LA-MCTS; specific empirical curves in Figures 5 and 6 show that LA-MCTS+TuRBO outperforms standalone TuRBO on many high-dimensional tasks (plots provided rather than single-number summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared as both a standalone solver and as the local-model inside LA-MCTS versus other BO variants, EAs, and partition-based MCTS methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>When used inside LA-MCTS, TuRBO benefits from constrained search domains (better local model fit); LA-MCTS+TuRBO outperforms standalone TuRBO in high-dimensional experiments (shown in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical improvements in objective vs standalone TuRBO across multiple benchmarks; specific numerical gains are shown in plots (Figures 5 and 6).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses that local modeling (TuRBO) works well but benefits from being constrained to learned partitions to avoid over-exploration of whole space; LA-MCTS reduces region size to improve TuRBO model fit.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Initializing TuRBO within LA-MCTS-selected regions and constraining sampling to intersection of bounding box and region helps TuRBO find better candidates and improves sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2608.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2608.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BO (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization using Expected Improvement acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard Bayesian optimization that fits a surrogate (e.g., GP) and selects new evaluations by maximizing an acquisition function; the paper uses Expected Improvement (EI) as the acquisition for BO baselines and as an option inside LA-MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Optimization (Expected Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BO fits a surrogate regression model to observed (x,f(x)) pairs and selects next points by optimizing an acquisition function (EI reported in the paper) that trades off predicted mean and uncertainty; within LA-MCTS, BO is constrained to the SVM-defined intersection region along the selected path and the acquisition is optimized via sampling heuristics (small rectangles around existing points, scaled until a fraction of samples fall outside region).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General black-box optimization; used both as a baseline and as an inner sampler for LA-MCTS on various tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Acquisition-function-driven single-point selection per iteration (EI), i.e., allocate each next evaluation to the point that maximizes EI within the constrained region; in LA-MCTS the constrained region reduces BO's over-exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primarily number of function evaluations; also acquisition optimization cost (sampling-based optimization inside constrained region) which can be expensive in high-dimensional spaces (paper notes BO becomes slow as dimension increases).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement (EI) — an expected utility metric measuring expected improvement over current best.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition function (EI) inherently balances exploration (uncertainty) and exploitation (predicted mean). Within LA-MCTS, BO's exploitation is curtailed by the constrained region size and the tree-level UCB exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond the acquisition's uncertainty term and LA-MCTS tree-level exploration; LA-MCTS's partitioning reduces BO's tendency to over-explore boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of function evaluations; acquisition optimization sometimes bounded by time.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Constrain acquisition optimization to SVM-intersected region and limit sampling for acquisition optimization via the proposed rectangle-scaling heuristic per existing point.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Same as LA-MCTS: objective improvement / reaching reward thresholds; EI targets expected improvement over current best.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Shown in figures; BO works well in low-dimensional tasks but degrades in high dimensions due to over-exploration and acquisition optimization difficulty; LA-MCTS+BO shows performance gains over BO alone (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against TuRBO, LA-MCTS, EAs, and other BO variants (BOHB, HesBO).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BO is competitive in low-dimensions but LA-MCTS reduces BO's over-exploration in high dimensions and improves results when BO is used as local sampler within LA-MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>When wrapped by LA-MCTS, BO achieves better sample efficiency in high-dimensional benchmarks compared to standalone BO (figures show improvements; no single % reported).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses BO's over-exploration in high dimensions and how partitioning (LA-MCTS) constrains BO to promising regions to improve efficiency; acquisition optimization in arbitrary constrained regions is nontrivial and addressed via sampling heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Constraining BO to tree-selected regions (via SVM path intersection) and using sampling heuristics for acquisition optimization yields better practical allocation of samples in high-dimensional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2608.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2608.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaNAS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LaNAS (Learning Action Space for Neural Architecture Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that learns linear partitions (hyperplanes) of the search space and samples uniformly within each partition to guide neural architecture search; LA-MCTS extends LaNAS by using nonlinear boundaries and local models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sample-efficient neural architecture search by learning action space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LaNAS (Learning Action Space)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LaNAS builds a fixed-height tree where each node learns a linear hyperplane partition (latent action) from samples, and then uniformly samples inside each region; it ranks leaves by learned partitions and uses this structure to bias exploration/exploitation. LA-MCTS differs by learning nonlinear SVM boundaries, dynamically growing the tree, and using local optimizers instead of uniform sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Neural architecture search (originally) and discrete architecture spaces; compared as a baseline in black-box optimization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Uniform sampling per region (no local surrogate) after learning linear partitions; splits and tree structure are pre-defined (fixed height) rather than dynamically grown.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of architecture evaluations / samples; LaNAS uses simple sampling (which is computationally cheap per decision) but may require many samples due to uniform sampling strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>No explicit information-gain acquisition is used; allocation is based on learned linear partitions and ranking of leaves.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>LaNAS ranks regions implicitly by learned partitioned labels and can bias sampling toward better regions but lacks the UCB-driven tree traversal and local-model-guided sampling of LA-MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via partitioning but uses uniform sampling inside each region (no explicit diversity-promoting objective).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of evaluations in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Fixed-height tree and uniform sampling; not dynamically reallocated based on UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective improvements / best found architectures; LaNAS aimed to reduce sample complexity in NAS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Compared in experiments; LA-MCTS reported to outperform LaNAS due to nonlinear boundaries and better local sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against LA-MCTS and other black-box optimizers in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LA-MCTS outperforms LaNAS in settings studied (LA-MCTS's nonlinear boundaries and local BO sampler yield better empirical performance).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>LA-MCTS reduces sample complexity compared to LaNAS empirically (no single % provided).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper argues that linear partitions + uniform sampling (LaNAS) are less adaptive and less sample-efficient when function contours are nonlinear; shows benefit of learning nonlinear boundaries and using local modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Uniform sampling within partitions is inferior to using local surrogate-guided sampling; dynamic splitting and UCB allocation improve resource allocation relative to fixed-height linear-partition approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2608.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2608.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UCB1 / MCTS selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Upper Confidence Bound (UCB1) within Monte Carlo Tree Search (MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bandit-derived selection rule used in LA-MCTS to traverse the learned partition tree from root to leaf, balancing visiting high-value (exploitation) and under-visited (exploration) regions via an exploration weight Cp.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finite-time analysis of the multiarmed bandit problem.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>UCB1 selection inside MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each internal node LA-MCTS computes a UCB-style score ucb_j = exploitation_term + 2*C_p * sqrt(2 * log(n_parent) / n_j) (paper uses a UCB1-like formula) where n_j is visits of child j and n_parent is visits of parent; LA-MCTS selects the child with highest ucb to traverse down to a leaf for sampling. Cp is tunable and controls exploration; Cp=0 yields greedy exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General sequential allocation in black-box optimization and active experimental design where hierarchical partitions are used.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates sampling budget by choosing which partition to sample next based on UCB score, thereby allocating more samples to regions with higher empirical reward and/or lower sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Counts of visits/samples (n_j) form part of the allocation; computational overhead of UCB selection is minimal compared to function evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicit; uses statistical confidence term (√(log n_parent / n_j)) as proxy for uncertainty / information deficit in a region.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>UCB balances exploitation (nodes with high observed average v_j) and exploration (nodes with low n_j receive a bonus proportional to sqrt(log(n_parent)/n_j) scaled by Cp).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>By encouraging visits to underexplored nodes (via exploration bonus), UCB promotes coverage of different regions (diversity in hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-evaluations allocation (visits increment as new samples are collected).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>UCB automatically adapts allocation of visits under a finite budget by prioritizing nodes with high UCB scores; Cp tunes how aggressively budget is spread for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvements in region-level empirical value v_j and eventual discovery of high-performing leaves (measured by best f(x) found).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as internal selection mechanism; ablation shows Cp strongly affects performance — too small Cp leads to worst performance; recommended Cp range provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared functionally to greedy left-branch-only exploitation and shown to be superior due to preserving exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>UCB-based traversal outperforms greedy exploitation by preventing premature focus on sub-optimal regions (ablation results in Figure 8(a)).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Using UCB fosters better global search and reduces getting trapped in suboptimal regions; quantitative gains shown in ablation plots but no single global % reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Ablation on Cp shows the classic exploration–exploitation tradeoff: small Cp → over-exploitation and poor global performance; large Cp → over-exploration and inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Set Cp to a small fraction (~1%–10%) of max f(x) to balance exploration and exploitation for best empirical performance in their benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scalable global optimization via local bayesian optimization <em>(Rating: 2)</em></li>
                <li>Sample-efficient neural architecture search by learning action space <em>(Rating: 2)</em></li>
                <li>Monte Carlo tree search in continuous spaces using voronoi optimistic optimization with regret bounds <em>(Rating: 2)</em></li>
                <li>Optimistic optimization of a deterministic function without the knowledge of its smoothness <em>(Rating: 2)</em></li>
                <li>A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning <em>(Rating: 1)</em></li>
                <li>Bayesian optimization in high dimensions via random embeddings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2608",
    "paper_id": "paper-02bc553b16b643ca2063bd7ae1270fe4288d8009",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "LA-MCTS",
            "name_full": "Latent Action Monte Carlo Tree Search",
            "brief_description": "A meta-level black-box optimization algorithm that learns adaptive, recursive partitions of the search space (via simple node models: KMeans + SVM) and uses Monte Carlo Tree Search (UCB) to allocate evaluations; inside selected partitions it runs local optimizers (e.g., TuRBO or BO) to pick candidate evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LA-MCTS (Latent Action Monte Carlo Tree Search)",
            "system_description": "LA-MCTS represents the search domain as a tree of regions (nodes). Each internal node defines a latent action: a boundary learned from the node's samples by clustering (KMeans on [x,f(x)]) to identify high/low performing clusters, then fitting an SVM (linear or nonlinear) to separate them; the learned classifier partitions the parent region into a 'good' child and a 'bad' child. The tree is grown dynamically: when a leaf accumulates more than a threshold θ samples it is split. A UCB-style selection (traversing root→leaf) balances visiting promising vs underexplored regions. For sampling inside a selected leaf, LA-MCTS uses an off-the-shelf local optimizer (default: TuRBO-1) or a standard BO acquisition (EI) constrained to the intersection of SVM constraints along the path. All new evaluations are returned to update node statistics (visit counts n_j and node value v_j = average f over samples in region) and to possibly trigger further splitting. Key hyperparameters are Cp (UCB exploration weight) and θ (split threshold).",
            "application_domain": "General high-dimensional black-box optimization (demonstrated on neural architecture search background, MuJoCo RL policy optimization, synthetic functions, trajectory optimization) — applicable to experimental design/automated discovery where evaluations are expensive.",
            "resource_allocation_strategy": "Region-level allocation via MCTS/UCB: allocate more evaluations to leaves with higher UCB score computed from node statistics (visit count n_j and average value v_j) where Cp controls exploration; tree splitting creates a hierarchical partition so allocation is hierarchical (more samples are assigned to child regions estimated as good); inside a selected region, allocate evaluations using a local optimizer (TuRBO or BO) until its stopping criterion (e.g., trust-region shrinks to zero) — then return all samples to the tree and repeat. Splitting threshold θ determines when to grow/allocate additional modeling capacity.",
            "computational_cost_metric": "Primary cost metric used is number of black-box evaluations (samples / function evaluations). Wall-clock/runtime is discussed qualitatively (e.g., TuRBO collects 10^4 samples in ≈1 hour on 1 V100 GPU) but the algorithm's design and experiments optimize for sample efficiency rather than explicit FLOPs/dollar costs.",
            "information_gain_metric": "Two forms: (1) region-selection: node value v_j = empirical average f(x) within region (used in UCB), and UCB formula that trades off exploitation (high v_j / visit count) and exploration (√(log n_parent / n_j)); (2) within-region sampling: when LA-MCTS uses regular BO it optimizes an acquisition function (paper states Expected Improvement (EI) is used for BO experiments), which implicitly measures expected improvement/information about better optima.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Multi-level: (a) Tree-level: UCB (Upper Confidence Bound) traversal from root to leaf, using ucb_j = (n_j/n_j) + 2*C_p * sqrt(2*log(n_parent)/n_j) (paper uses UCB1-like term) and Cp as tunable exploration weight; (b) Within-leaf: BO acquisition (EI) or TuRBO trust-region sampling that itself balances exploration/exploitation via its acquisition and trust-region mechanism; Cp controls the extent of exploration (ablation recommends Cp ≈ 0.01–0.1 of max f).",
            "diversity_mechanism": "Implicit diversity via the hierarchical partitioning and UCB: the latent-action splits produce disjoint regions (good/bad) and UCB encourages visiting under-sampled regions, producing diverse coverage of hypotheses/regions. There is no separate explicit diversity objective (e.g., explicit novelty score); diversity is achieved by exploration term in UCB and by keeping multiple leaves available to sample from.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-budget in terms of number of function evaluations (samples); experiments also considered wall-clock/time-limited runs (e.g., 3-day runs for some BO baselines) and computational resources (GPU used for TuRBO).",
            "budget_constraint_handling": "LA-MCTS handles budget constraints by (1) framing search as sequential allocation of samples via UCB-guided tree traversal, (2) splitting only when a leaf has ≥ θ samples to avoid over-allocating modeling capacity, and (3) running a local optimizer inside a leaf until its internal stopping criterion (e.g., TuRBO trust-region collapse), then returning evaluations to the tree; hyperparameters Cp and θ tune how budget is distributed between exploring new regions vs exploiting known good regions.",
            "breakthrough_discovery_metric": "Breakthroughs are identified by achieving high f(x) values / improvements over prior best; concrete metrics used in experiments include reward thresholds (for MuJoCo tasks) and objective value/regret (e.g., minimization regrets |v_selected - v*|). The paper measures reaching pre-defined reward thresholds and reduction in regret toward global optimum.",
            "performance_metrics": "Reported metrics are mainly sample-efficiency and achieved objective values: (a) number of samples (episodes) to reach reward thresholds in MuJoCo tasks (Table 2): Swimmer-v2 threshold 325 reached by LA-MCTS in 126 episodes, Hopper-v2 threshold 3120 reached in 2913 episodes, HalfCheetah-v2 threshold 3430 reached in 3967 episodes; several harder tasks (Walker2d, Ant, Humanoid) did not reach thresholds under allotted budgets with LA-MCTS (reported N/A with best rewards given). (b) Learning curves (objective vs #samples) across benchmarks (MuJoCo, synthetic functions) showing LA-MCTS finds better solutions after ≈30k samples compared to baselines; (c) reduction in region regret |v^+ - v*| as number of splits grows (Figure 7(a)). Runtimes: TuRBO reported to collect 10^4 samples in ≈1 hour on 1 V100 GPU (paper runtime notes).",
            "comparison_baseline": "Compared against numerous baselines: TuRBO (local BO), HesBO (low-dimensional embeddings), BOHB, Bayesian optimization variants, evolutionary algorithms (CMA-ES, Differential Evolution, Shiwa), MCTS/optimistic partitioning methods (VOO, SOO, DOO), Dual Annealing, Random Search, and LaNAS (prior learned-partition method).",
            "performance_vs_baseline": "LA-MCTS consistently improves over TuRBO and standard BO in high-dimensional problems (Figures 5 and 6); qualitatively, LA-MCTS reduces BO's over-exploration by constraining BO to learned promising partitions and thus fits better local models. Example (Table 2): on Swimmer-v2 LA-MCTS reaches reward 325 in 126 samples (substantially faster than some baselines shown); on high-dimensional locomotion (Ant, Humanoid) LA-MCTS outperforms many baselines in objective achieved by 30k samples (plots show superior final objective). Exact numeric improvements vs TuRBO are presented in plots (Figure 5, Figure 6) rather than as single consolidated percentages.",
            "efficiency_gain": "Empirically LA-MCTS reduces sample complexity in many benchmarks (e.g., reaches Swimmer-v2 threshold in 126 samples; improves TuRBO's performance on synthetic functions and RL tasks). The paper does not present a single global % improvement; gains are task-dependent and shown in per-task plots and tables — improvements are most pronounced in high-dimensional problems where partitioning reduces the effective region size for local models.",
            "tradeoff_analysis": "Ablation studies analyze tradeoffs: (1) Cp (UCB exploration weight): too small Cp → over-exploitation and poor performance; too large Cp → over-exploration and inefficiency; recommendation Cp ≈ 0.01–0.1 × max f. (2) Splitting threshold θ: smaller θ grows tree faster (deeper partitions) which helps focus on promising regions in very large spaces but if θ too small boundary estimates become unreliable and performance degrades (θ=10 often good; θ=2 too small). (3) SVM kernel choice: nonlinear kernels (polynomial, RBF) produce more flexible boundaries and often better performance than linear. The paper also notes LA-MCTS bears extra exploration cost relative to gradient methods and thus can be less sample-efficient on tasks where pure exploitation (gradients) is effective.",
            "optimal_allocation_findings": "Key principles: (a) allocate more samples to promising regions while retaining systematic exploration via a UCB-style term (Cp tuned to task scale); (b) dynamically learn partitions from samples so allocation adapts to objective structure rather than fixed, objective-independent partitioning; (c) use local optimizers (TuRBO/BO) inside constrained regions to focus computational budget efficiently; (d) split regions only when enough samples exist (threshold θ) to avoid premature, unreliable partitions. Recommendations from experiments: Cp ≈ 1%–10% of max f, θ around 10 for large search domains, prefer nonlinear SVM kernels when sufficient samples are available.",
            "uuid": "e2608.0",
            "source_info": {
                "paper_title": "Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "TuRBO",
            "name_full": "TuRBO (Trust-Region Bayesian Optimization)",
            "brief_description": "A state-of-the-art local Bayesian optimization method that runs multiple (or single) trust regions, fitting local surrogate models and adapting trust-region sizes to balance local exploration and exploitation.",
            "citation_title": "Scalable global optimization via local bayesian optimization",
            "mention_or_use": "use",
            "system_name": "TuRBO (Trust-Region Bayesian Optimization)",
            "system_description": "TuRBO operates by maintaining one or many local trust regions, fitting Gaussian-process-like or surrogate models inside each trust region, sampling via acquisition functions within trust-region bounding boxes, restarting trust regions periodically, and shrinking/expanding trust radii based on observed progress; it is designed for scalable BO in high-dimensional spaces by focusing modeling power locally.",
            "application_domain": "High-dimensional black-box optimization; used as a local sampler inside LA-MCTS and as a baseline in RL and synthetic benchmark tasks.",
            "resource_allocation_strategy": "Allocate evaluations within trust regions centered at current best solutions; can run multiple independent trust regions (TuRBO-20 in some experiments) to parallelize exploration. In LA-MCTS integration, TuRBO is initialized and constrained to the LA-MCTS-selected region and runs until its trust-region collapse stopping criterion.",
            "computational_cost_metric": "Measured in number of evaluations; runtime examples include TuRBO collecting 10^4 samples in ~1 hour on 1 V100 GPU (paper runtime note).",
            "information_gain_metric": "Uses BO-style acquisition functions (e.g., Expected Improvement) to select within-trust-region samples, implicitly measuring expected improvement/information about better solutions.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Trust-region adaptation: local surrogate and acquisition function drive exploration within region; trust-region expansion/shrinkage and multiple independent regions provide exploration across space vs exploitation inside regions.",
            "diversity_mechanism": "Multiple independent trust regions (when used) provide diverse local searches; in LA-MCTS integration, diversity is obtained indirectly by running separate TuRBO instances inside different LA-MCTS-selected partitions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of function evaluations; internal stopping when trust-region radius hits zero.",
            "budget_constraint_handling": "Runs until re-start or until trust-region collapses; returns all samples to LA-MCTS which then re-allocates budget across tree.",
            "breakthrough_discovery_metric": "Improvement in objective via acquisition-driven sampling and local best improvement (EI); no explicit breakthrough novelty metric in this paper's usage.",
            "performance_metrics": "Used as baseline and inside LA-MCTS; specific empirical curves in Figures 5 and 6 show that LA-MCTS+TuRBO outperforms standalone TuRBO on many high-dimensional tasks (plots provided rather than single-number summaries).",
            "comparison_baseline": "Compared as both a standalone solver and as the local-model inside LA-MCTS versus other BO variants, EAs, and partition-based MCTS methods.",
            "performance_vs_baseline": "When used inside LA-MCTS, TuRBO benefits from constrained search domains (better local model fit); LA-MCTS+TuRBO outperforms standalone TuRBO in high-dimensional experiments (shown in figures).",
            "efficiency_gain": "Empirical improvements in objective vs standalone TuRBO across multiple benchmarks; specific numerical gains are shown in plots (Figures 5 and 6).",
            "tradeoff_analysis": "Paper discusses that local modeling (TuRBO) works well but benefits from being constrained to learned partitions to avoid over-exploration of whole space; LA-MCTS reduces region size to improve TuRBO model fit.",
            "optimal_allocation_findings": "Initializing TuRBO within LA-MCTS-selected regions and constraining sampling to intersection of bounding box and region helps TuRBO find better candidates and improves sample efficiency.",
            "uuid": "e2608.1",
            "source_info": {
                "paper_title": "Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "BO (EI)",
            "name_full": "Bayesian Optimization using Expected Improvement acquisition",
            "brief_description": "Standard Bayesian optimization that fits a surrogate (e.g., GP) and selects new evaluations by maximizing an acquisition function; the paper uses Expected Improvement (EI) as the acquisition for BO baselines and as an option inside LA-MCTS.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian Optimization (Expected Improvement)",
            "system_description": "BO fits a surrogate regression model to observed (x,f(x)) pairs and selects next points by optimizing an acquisition function (EI reported in the paper) that trades off predicted mean and uncertainty; within LA-MCTS, BO is constrained to the SVM-defined intersection region along the selected path and the acquisition is optimized via sampling heuristics (small rectangles around existing points, scaled until a fraction of samples fall outside region).",
            "application_domain": "General black-box optimization; used both as a baseline and as an inner sampler for LA-MCTS on various tasks.",
            "resource_allocation_strategy": "Acquisition-function-driven single-point selection per iteration (EI), i.e., allocate each next evaluation to the point that maximizes EI within the constrained region; in LA-MCTS the constrained region reduces BO's over-exploration.",
            "computational_cost_metric": "Primarily number of function evaluations; also acquisition optimization cost (sampling-based optimization inside constrained region) which can be expensive in high-dimensional spaces (paper notes BO becomes slow as dimension increases).",
            "information_gain_metric": "Expected Improvement (EI) — an expected utility metric measuring expected improvement over current best.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition function (EI) inherently balances exploration (uncertainty) and exploitation (predicted mean). Within LA-MCTS, BO's exploitation is curtailed by the constrained region size and the tree-level UCB exploration.",
            "diversity_mechanism": "No explicit diversity mechanism beyond the acquisition's uncertainty term and LA-MCTS tree-level exploration; LA-MCTS's partitioning reduces BO's tendency to over-explore boundaries.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of function evaluations; acquisition optimization sometimes bounded by time.",
            "budget_constraint_handling": "Constrain acquisition optimization to SVM-intersected region and limit sampling for acquisition optimization via the proposed rectangle-scaling heuristic per existing point.",
            "breakthrough_discovery_metric": "Same as LA-MCTS: objective improvement / reaching reward thresholds; EI targets expected improvement over current best.",
            "performance_metrics": "Shown in figures; BO works well in low-dimensional tasks but degrades in high dimensions due to over-exploration and acquisition optimization difficulty; LA-MCTS+BO shows performance gains over BO alone (Figure 6).",
            "comparison_baseline": "Compared against TuRBO, LA-MCTS, EAs, and other BO variants (BOHB, HesBO).",
            "performance_vs_baseline": "BO is competitive in low-dimensions but LA-MCTS reduces BO's over-exploration in high dimensions and improves results when BO is used as local sampler within LA-MCTS.",
            "efficiency_gain": "When wrapped by LA-MCTS, BO achieves better sample efficiency in high-dimensional benchmarks compared to standalone BO (figures show improvements; no single % reported).",
            "tradeoff_analysis": "Paper discusses BO's over-exploration in high dimensions and how partitioning (LA-MCTS) constrains BO to promising regions to improve efficiency; acquisition optimization in arbitrary constrained regions is nontrivial and addressed via sampling heuristics.",
            "optimal_allocation_findings": "Constraining BO to tree-selected regions (via SVM path intersection) and using sampling heuristics for acquisition optimization yields better practical allocation of samples in high-dimensional settings.",
            "uuid": "e2608.2",
            "source_info": {
                "paper_title": "Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "LaNAS",
            "name_full": "LaNAS (Learning Action Space for Neural Architecture Search)",
            "brief_description": "A prior method that learns linear partitions (hyperplanes) of the search space and samples uniformly within each partition to guide neural architecture search; LA-MCTS extends LaNAS by using nonlinear boundaries and local models.",
            "citation_title": "Sample-efficient neural architecture search by learning action space",
            "mention_or_use": "use",
            "system_name": "LaNAS (Learning Action Space)",
            "system_description": "LaNAS builds a fixed-height tree where each node learns a linear hyperplane partition (latent action) from samples, and then uniformly samples inside each region; it ranks leaves by learned partitions and uses this structure to bias exploration/exploitation. LA-MCTS differs by learning nonlinear SVM boundaries, dynamically growing the tree, and using local optimizers instead of uniform sampling.",
            "application_domain": "Neural architecture search (originally) and discrete architecture spaces; compared as a baseline in black-box optimization experiments.",
            "resource_allocation_strategy": "Uniform sampling per region (no local surrogate) after learning linear partitions; splits and tree structure are pre-defined (fixed height) rather than dynamically grown.",
            "computational_cost_metric": "Number of architecture evaluations / samples; LaNAS uses simple sampling (which is computationally cheap per decision) but may require many samples due to uniform sampling strategy.",
            "information_gain_metric": "No explicit information-gain acquisition is used; allocation is based on learned linear partitions and ranking of leaves.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "LaNAS ranks regions implicitly by learned partitioned labels and can bias sampling toward better regions but lacks the UCB-driven tree traversal and local-model-guided sampling of LA-MCTS.",
            "diversity_mechanism": "Implicit via partitioning but uses uniform sampling inside each region (no explicit diversity-promoting objective).",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of evaluations in experiments.",
            "budget_constraint_handling": "Fixed-height tree and uniform sampling; not dynamically reallocated based on UCB.",
            "breakthrough_discovery_metric": "Objective improvements / best found architectures; LaNAS aimed to reduce sample complexity in NAS.",
            "performance_metrics": "Compared in experiments; LA-MCTS reported to outperform LaNAS due to nonlinear boundaries and better local sampling.",
            "comparison_baseline": "Compared against LA-MCTS and other black-box optimizers in experiments.",
            "performance_vs_baseline": "LA-MCTS outperforms LaNAS in settings studied (LA-MCTS's nonlinear boundaries and local BO sampler yield better empirical performance).",
            "efficiency_gain": "LA-MCTS reduces sample complexity compared to LaNAS empirically (no single % provided).",
            "tradeoff_analysis": "Paper argues that linear partitions + uniform sampling (LaNAS) are less adaptive and less sample-efficient when function contours are nonlinear; shows benefit of learning nonlinear boundaries and using local modeling.",
            "optimal_allocation_findings": "Uniform sampling within partitions is inferior to using local surrogate-guided sampling; dynamic splitting and UCB allocation improve resource allocation relative to fixed-height linear-partition approaches.",
            "uuid": "e2608.3",
            "source_info": {
                "paper_title": "Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "UCB1 / MCTS selection",
            "name_full": "Upper Confidence Bound (UCB1) within Monte Carlo Tree Search (MCTS)",
            "brief_description": "A bandit-derived selection rule used in LA-MCTS to traverse the learned partition tree from root to leaf, balancing visiting high-value (exploitation) and under-visited (exploration) regions via an exploration weight Cp.",
            "citation_title": "Finite-time analysis of the multiarmed bandit problem.",
            "mention_or_use": "use",
            "system_name": "UCB1 selection inside MCTS",
            "system_description": "At each internal node LA-MCTS computes a UCB-style score ucb_j = exploitation_term + 2*C_p * sqrt(2 * log(n_parent) / n_j) (paper uses a UCB1-like formula) where n_j is visits of child j and n_parent is visits of parent; LA-MCTS selects the child with highest ucb to traverse down to a leaf for sampling. Cp is tunable and controls exploration; Cp=0 yields greedy exploitation.",
            "application_domain": "General sequential allocation in black-box optimization and active experimental design where hierarchical partitions are used.",
            "resource_allocation_strategy": "Allocates sampling budget by choosing which partition to sample next based on UCB score, thereby allocating more samples to regions with higher empirical reward and/or lower sample counts.",
            "computational_cost_metric": "Counts of visits/samples (n_j) form part of the allocation; computational overhead of UCB selection is minimal compared to function evaluations.",
            "information_gain_metric": "Not explicit; uses statistical confidence term (√(log n_parent / n_j)) as proxy for uncertainty / information deficit in a region.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "UCB balances exploitation (nodes with high observed average v_j) and exploration (nodes with low n_j receive a bonus proportional to sqrt(log(n_parent)/n_j) scaled by Cp).",
            "diversity_mechanism": "By encouraging visits to underexplored nodes (via exploration bonus), UCB promotes coverage of different regions (diversity in hypotheses).",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-number-of-evaluations allocation (visits increment as new samples are collected).",
            "budget_constraint_handling": "UCB automatically adapts allocation of visits under a finite budget by prioritizing nodes with high UCB scores; Cp tunes how aggressively budget is spread for exploration.",
            "breakthrough_discovery_metric": "Improvements in region-level empirical value v_j and eventual discovery of high-performing leaves (measured by best f(x) found).",
            "performance_metrics": "Used as internal selection mechanism; ablation shows Cp strongly affects performance — too small Cp leads to worst performance; recommended Cp range provided.",
            "comparison_baseline": "Compared functionally to greedy left-branch-only exploitation and shown to be superior due to preserving exploration.",
            "performance_vs_baseline": "UCB-based traversal outperforms greedy exploitation by preventing premature focus on sub-optimal regions (ablation results in Figure 8(a)).",
            "efficiency_gain": "Using UCB fosters better global search and reduces getting trapped in suboptimal regions; quantitative gains shown in ablation plots but no single global % reported.",
            "tradeoff_analysis": "Ablation on Cp shows the classic exploration–exploitation tradeoff: small Cp → over-exploitation and poor global performance; large Cp → over-exploration and inefficiency.",
            "optimal_allocation_findings": "Set Cp to a small fraction (~1%–10%) of max f(x) to balance exploration and exploitation for best empirical performance in their benchmarks.",
            "uuid": "e2608.4",
            "source_info": {
                "paper_title": "Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scalable global optimization via local bayesian optimization",
            "rating": 2
        },
        {
            "paper_title": "Sample-efficient neural architecture search by learning action space",
            "rating": 2
        },
        {
            "paper_title": "Monte Carlo tree search in continuous spaces using voronoi optimistic optimization with regret bounds",
            "rating": 2
        },
        {
            "paper_title": "Optimistic optimization of a deterministic function without the knowledge of its smoothness",
            "rating": 2
        },
        {
            "paper_title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Bayesian optimization in high dimensions via random embeddings",
            "rating": 1
        }
    ],
    "cost": 0.01954025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search</h1>
<p>Linnan Wang<br>Brown University<br>linnan_wang@brown.edu</p>
<p>Rodrigo Fonseca<br>Brown University<br>rfonseca@cs.brown.edu</p>
<p>Yuandong Tian<br>Facebook AI Research<br>yuandong@fb.com</p>
<h4>Abstract</h4>
<p>High dimensional black-box optimization has broad applications but remains a challenging problem to solve. Given a set of samples $\left{\mathbf{x}<em i="i">{i}, y</em>\right}$, building a global model (like Bayesian Optimization (BO)) suffers from the curse of dimensionality in the high-dimensional search space, while a greedy search may lead to sub-optimality. By recursively splitting the search space into regions with high/low function values, recently LaNAS [1] shows good performance in Neural Architecture Search (NAS), reducing the sample complexity empirically. In this paper, we coin LA-MCTS that extends LaNAS to other domains. Unlike previous approaches, LA-MCTS learns the partition of the search space using a few samples and their function values in an online fashion. While LaNAS uses linear partition and performs uniform sampling in each region, our LA-MCTS adopts a nonlinear decision boundary and learns a local model to pick good candidates. If the nonlinear partition function and the local model fit well with ground-truth black-box function, then good partitions and candidates can be reached with much fewer samples. LA-MCTS serves as a meta-algorithm by using existing black-box optimizers (e.g., BO, TuRBO [2]) as its local models, achieving strong performance in general black-box optimization and reinforcement learning benchmarks, in particular for high-dimensional problems.</p>
<h2>1 Introduction</h2>
<p>Black-box optimization has been extensively used in many scenarios, including Neural Architecture Search (NAS) [3, 1, 4], planning in robotics [5, 6], hyper-parameter tuning in large scale databases [7] and distributed systems [8], integrated circuit design [9], etc.. In black-box optimization, we have a function $f$ without explicit formulation and the goal is to find $\mathbf{x}^{*}$ such that</p>
<p>$$
\mathbf{x}^{*}=\arg \max _{\mathbf{x} \in X} f(\mathbf{x})
$$</p>
<p>with the fewest samples ( $\mathbf{x}$ ). In this paper, we consider the case that $f$ is deterministic.
Without knowing any structure of $f$ (except for the local smoothness such as Lipschitzcontinuity [10]), in the worst-case, solving Eqn. 1 takes exponential time, i.e. the optimizer needs to search every $\mathbf{x}$ to find the optimum. One way to address this problem is through learning: from a few samples we learn a surrogate regressor $\hat{f} \in \mathcal{H}$ and optimize $\hat{f}$ instead. If the model class $\mathcal{H}$ is small and $f$ can be well approximated within $\mathcal{H}$, then $\hat{f}$ is a good approximator of $f$ with much fewer samples.</p>
<p>Many previous works go that route, such as Bayesian Optimization (BO) and its variants [11, 12, 13, 14]. However, in the case that $f$ is highly nonlinear and high-dimensional, we need to use a very large model class $\mathcal{H}$, e.g. Gaussian Processes (GP) or Deep Neural Networks (DNN), that requires many samples to fit before generalizing well. For example, Oh et al [15] observed that the myopic acquisition in BO over-explores the boundary of a search space, especially in high dimensional problems. To address this issue, recent works start to explore space partitioning [5, 16, 17] and local</p>
<p>modeling [2, 18] that fits local models in promising regions, and achieve strong empirical results in high dimensional problems. However, their space partitions follow a fixed criterion (e.g., $K$-ary uniform partition) that is independent of the objective to be optimized.</p>
<p>Following the path of learning, one under-explored direction is to learn the space partition. Compared to learning a regressor $\hat{f}$ that is expected to be accurate in the region of interest, it suffices to learn a classifier that puts the sample to the right subregion with high probability. Moreover, its quality requirement can be further reduced if done recursively.</p>
<p>In this paper, we propose LA-MCTS, a meta-level algorithm that recursively learns space partition in a hierarchical manner. Given a few samples within a region, it first performs unsupervised $K$-mean algorithm based on their function values, learns a classifier using $K$-mean labels, and partition the region into good and bad sub-regions (with high/low function value). To address the problem of mis-partitioning good data points into bad regions, LA-MCTS uses UCB to balance exploration and exploitation: it assigns more samples to good regions, where it is more likely to find an optimal solution, and exploring other regions in case there are good candidates. Compared to previous space partition method, e.g. using Voronoi graph [5], we learn the partition that is adaptive to the objective function $f(\mathbf{x})$. Compared to the local modeling method, e.g. TuRBO [2], our method dynamically exploits and explores the promising region w.r.t samples using Monte Carlos Tree Search (MCTS), and constantly refine the learned boundaries with new samples.</p>
<p>LA-MCTS extends LaNAS [1] in three aspects. First, while LaNAS learns a hyper-plane, our approach learns a non-linear decision boundary that is more flexible. Second, while LaNAS simply performs uniform sampling in each region as the next sample to evaluate, we make the key observation that local model works well and use existing solvers such as BO to find a promising data point. This makes LA-MCTS a meta-algorithm usable to boost existing algorithms that optimize via building local models. Third, while LaNAS mainly focus on Neural Architecture Search (&lt; 20 discrete parameters), our approach shows strong performance on generic black-box optimization.</p>
<p>We show that LA-MCTS, when paired with TurBO, outperforms various SoTA black-box solvers from Bayesian Optimizations, Evolutionary Algorithm, and Monte Carlo Tree Search, in several challenging benchmarks, including MuJoCo locomotion tasks, trajectory optimization, reinforcement learning, and high-dimensional synthetic functions. We also perform extensive ablation studies, showing LA-MCTS is relatively insensitive to hyper-parameter tuning. As a meta-algorithm, it also substantially improves the baselines.</p>
<p>The implementation of LA-MCTS can be found at https://github.com/facebookresearch/LaMCTS.</p>
<h1>2 Related works</h1>
<p>Bayesian Optimization (BO) has become a promising approach in optimizing the black-box functions [11, 12, 13], despite much of its success is typically limited to less than 15 parameters [19] and a few thousand evaluations [18]. While most real-world problems are high dimensional, and reliably optimizing a complex function requires many evaluations. This has motivated many works to scale up BO, by approximating the expensive Gaussian Process (GP), such as using Random Forest in SMAC [20], Bayesian Neural Network in BOHAMIANN [21], and the tree-structured Parzen estimator in TPE [22]. BOHB [23] further combines TPE with Hyperband [24] to achieve strong any time performance. Therefore, we choose BOHB in comparison. Using a sparse GP is another way to scale up BO [25, 26, 27]. However, sparse GP only works well if there exists sample redundancy, which is barely the case in high dimensional problems. Therefore, scaling up evaluations is not sufficient for solving high-dimensional problems.</p>
<p>There are lots of work to specifically study high-dimensional BO [28, 29, 30, 31, 32, 33, 34, 35, 36, 37]. One category of methods decomposes the target function into several additive structures [32, 35], which limits its scalability by the number of decomposed structures for training multiple GP. Besides, learning a good decomposition remains challenging. Another category of methods is to transform a high-dimensional problem in low-dimensional subspaces. REMBO [34] fits a GP in low-dimensional spaces and projects points back to a high-dimensional space that contains the global optimum with a reasonable probability. Binois et al [38] further improves the distortion from Gaussian projections in REMBO. While REMBO works empirically, HesBO [19] is a theoretical sound framework for BO that optimizes high-dimensional problems on low dimensional sub-spaces embeddings; In BOCK [15],</p>
<p>Table 1: Definition of notations used through this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\mathbf{x}_{i}$</th>
<th style="text-align: left;">the ith sample</th>
<th style="text-align: left;">$f\left(\mathbf{x}_{i}\right)$</th>
<th style="text-align: left;">the evaluation of $\mathbf{x}_{i}$</th>
<th style="text-align: left;">$D_{t}$</th>
<th style="text-align: left;">collected $\left(\mathbf{x}<em i="i">{i}, \mathrm{f}\left(\mathbf{x}</em>\right)\right)$ from iter $1 \rightarrow t$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\Omega$</td>
<td style="text-align: left;">the entire search space</td>
<td style="text-align: left;">$\Omega_{j}$</td>
<td style="text-align: left;">the partition represented by node $j$</td>
<td style="text-align: left;">$D_{t} \cap \Omega_{j}$</td>
<td style="text-align: left;">samples classified in $\Omega_{j}$</td>
</tr>
<tr>
<td style="text-align: left;">$n_{j}$</td>
<td style="text-align: left;">#visits at node $j$</td>
<td style="text-align: left;">$v_{j}$</td>
<td style="text-align: left;">the value of node $j$</td>
<td style="text-align: left;">$u c b_{j}$</td>
<td style="text-align: left;">the ucb score of node $j$</td>
</tr>
</tbody>
</table>
<p>Oh et al observed existing BO spends most evaluations near the boundary of a search space due to the Euclidean geometry, and it proposed transforming the problem into a cylindrical space to avoid over-exploring the boundary. EBO [18] uses an ensemble of local GP on the partitioned problem space. Based on the same principle of local modeling as EBO, recent trust-region BO (TuRBO) [2] has outperformed other high-dimensional BO on a variety of tasks. In comparing to high dimensional BO, we picked SoTA local modeling method TuRBO and dimension reduction method HesBO.</p>
<p>Evolutionary Algorithm (EA) is another popular algorithm for high dimensional black-box optimizations. A comprehensive review of EA can be found in [39]. CMA-ES is a successful EA method that uses co-variance matrix adaption to propose new samples. Differential Evolution (DE) [40] is another popular EA approach that uses vector differences for perturbing the vector population. Recently, Liu et al proposes a metamethod (Shiwa) [41] to automatically selects EA methods based on hyper-parameters such as problem dimensions, budget, and noise level, etc., and Shiwa delivers better empirical results than any single EA method. We choose Shiwa, CMA-ES, and differential evolution in comparisons.</p>
<p>Besides the recent success in games [42, 43, 44, 45], Monte Carlo Tree Search (MCTS) is also widely used in the robotics planning and optimization [6, 46, 47, 48]. Several space partitioning algorithms have been proposed in this line of research. In [16], Munos proposed DOO and SOO. DOO uses a tree structure to partition the search space by recursively bifurcating the region with the highest upper bound, i.e. optimistic exploration, while SOO relaxes the Lipschitz condition of DOO on the objective function. HOO [14] is a stochastic version of DOO. While prior works use K-ary partitions, Kim et al show Voronoi [5] partition can be more efficient than previous linear partitions in high-dimensional problems. In this paper, based on the idea of space partitioning, we extend current works by learning the space partition so that the partition can adapt to the distribution of $f(\mathbf{x})$. Besides, we improve the sampling inside a selected region with BO. This also helps BO from over-exploring by bounding within a small region.</p>
<h1>3 Methodology</h1>
<h3>3.1 Latent Action Monte Carlo Tree Search (LA-MCTS)</h3>
<p>This section describes LA-MCTS that progressively partitions the problem space. Please refer to Table. 1 for definitions of notations in this paper.</p>
<p>The model of MCTS search tree: At any iteration t, we have a dataset $D_{t}$ collected from previous evaluations. Each entry in $D_{t}$ contains a pair of $\left(\mathbf{x}<em i="i">{i}, f\left(\mathbf{x}</em>}\right)\right)$. A tree node (e.g. node A in Fig. 1) represents a region $\Omega_{A}$ in the entire problem space $(\Omega)$, then $D_{t} \cap \Omega_{A}$ represents the samples falling within node A. Each node also tracks two important statistics to calculate UCB1 [49] for guiding the selection: $n_{A}$ represents the number of visits at node A, which is the #sample in $D_{t} \cap \Omega_{A}$; and $v_{i}$ represents the node value that equals to $\frac{1}{n_{i}} \sum f\left(\mathbf{x<em i="i">{i}\right), \forall \mathbf{x}</em>$.} \in D_{t} \cap \Omega_{i</p>
<p>LA-MCTS finds the promising regions by recursively partitioning. Starting from the root, every internal node, e.g. node A in Fig. 1, use latent actions to bifurcate the region represented by itself into a high performing and a low performing disjoint region ( $\Omega_{B}$ and $\Omega_{C}$ ) for its left and right child, respectively (by default we use left child to represent a good region), and $\Omega_{A}=\Omega_{B} \cup \Omega_{C}$. Then a tree enforces the behavior of recursively partitioning from root
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The model of latent actions: each tree nodes represents a region in the search space, and latent action splits the region into a high-performing and a lowperforming region using $\mathbf{x}$ and $f(\mathbf{x})$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The workflow of LA-MCTS: In an iteration, LA-MCTS starts with building the tree via splitting, then it selects a region based on UCB. Finally, on the selected region, it samples by BO.</p>
<p>To leave so that regions represented by tree leaves (Ω<em>leaves</em>) can be easily ranked from the best (the leftmost leaf), the second-best (the sibling of the leftmost leaf) to the worst (the rightmost leaf) due to the partitioning rule. The tree grows as the optimization progress, Ω<em>leaves</em> becomes smaller, better focusing on a promising region (Fig. 7(b)). Please see sec 3.1.1 for the tree construction. By directly optimizing on Ω<em>leaves</em>, it helps BO from over-exploring, hence improving the BO performance especially in high dimensional problems.</p>
<p>Latent actions: Our model defines <em>latent action</em> as a boundary that splits the region represented by a node into a high-performing and a low performing region. Fig. 1 illustrates the concept and the procedures of creating latent actions on a node. Our goal is to learn a boundary from samples in D<sup>t</sup> ∩ Ω<sup>A</sup> to maximize the performance difference of two regions split by the boundary. We apply Kmeans on the feature vector of [x, f(x)] to find a good and a bad performance clusters in D<sup>t</sup> ∩ Ω<sup>A</sup>, then use SVM to learn a decision boundary. Learning a nonlinear decision boundary is a traditional Machine Learning (ML) task, Neural Networks (NN) and Support Vector Machines (SVM) are two typical solutions. We choose SVM for the ease of training, and requiring fewer samples to generalize well in practices. Please note a simple node model is critical for having a tree of them. For the same reason, we choose Kmeans to find two clusters with good and bad performance. The detailed procedures are as follows:</p>
<ol>
<li>At any node A, we prepare ∀[x<sub>i</sub>, f(x<sub>i</sub>)], i ∈ D<sup>t</sup> ∩ Ω<sup>j</sup> as the training data for Kmeans to learn two clusters of different performance (Fig. 1 (b, c)), and get the cluster label l<sub>i</sub> for every x<sub>i</sub> using the learned Kmeans, i.e. [l<sub>i</sub>, x<sub>i</sub>]. So, the cluster with higher average f(x<sub>i</sub>) represents a good performing region, and lower average f(x<sub>i</sub>) represents a bad region.</li>
<li>Given [l<sub>i</sub>, x<sub>i</sub>] from the previous step, we learn a boundary with SVM to generalize two regions to unseen x<sub>i</sub>, and <em>the boundary learnt by SVM forms the latent action</em> (Fig. 1(d)). For example, ∀x<sub>i</sub> ∈ Ω with predicted label equals the high-performing region goes to the left child, and right otherwise.</li>
</ol>
<h3>3.1.1 The search procedures</h3>
<p>Fig. 2 summarizes a search iteration of LA-MCTS that has 3 major steps. 1) <em>Learning and splitting</em> dynamically deepens a search tree using new x<sub>i</sub> collected from the previous iteration; 2) <em>select</em> explores partitioned search space for sampling; and 3) <em>sampling</em> solves <em>minimize f</em>(x<sub>i</sub>), x<sub>i</sub> ∈ Ω<em>selected</em> using BO, and SVMs on the selected path form constraints to bound Ω<em>selected</em>. We omit the back-propagation as it is implicitly done in splitting. Please see [4, 45] for a review of regular MCTS.</p>
<p>Dynamic tree construction via splitting: We estimate the performance of a Ω<sub>i</sub>, i.e. v<sub>i</sub><em> , by v˜<sub>i</sub> = 1/n<sub>i</sub> ∑ f(x<sub>i</sub>), ∀x<sub>i</sub> ∈ D<sup>t</sup> ∩ Ω<sub>i</sub>. At each iteration, new x<sub>i</sub> are collected and the regret of |v˜<sub>i</sub> - v<sub>i</sub></em> | quickly decreases. Once the regret reaches the plateau, new samples are not necessary; then LA-MCTS splits the region using <em>latent actions</em> (Fig. 1) to continue refining the value estimation of two child regions. With more and more samples from promising regions, the tree becomes deeper into good regions, better guiding the search toward the optimum. In practice, we use a threshold θ as a tunable parameter for splitting. If the size of D<sup>t</sup> ∩ Ω<sub>i</sub> exceeds the threshold θ at any leaves, we split the leaf with <em>latent actions</em>. We presents the ablation study on θ in Fig. 8.</p>
<p>The structure of our search tree dynamically changes across iterations, which is different from the pre-defined fixed-height tree used in LaNAS [1]. At the beginning of an iteration, starting from the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The visualization of partitioning $1 \mathrm{~d} \sin (x)$ using LA-MCTS.
root that contains all the samples, we recursively split leaves using latent actions if the sample size of any leaves exceeds the splitting threshold $\theta$, e.g. creating node D and node E for node B in Fig.2(a). We stop the tree splitting until no more leaves satisfy the splitting criterion. Then, the tree is ready to use in this iteration.</p>
<p>Select via UCB: According to the partition rule, a simple greedy based go-left strategy can be used to exclusively exploit the current most promising leaf. This makes the algorithm over-exploiting a region based on existing samples, while the region can be sub-optimal with the global optimum located in a different place. To build an accurate global view of $\Omega$, LA-MCTS selects a partition following Upper Confidence Bound (UCB) for the adaptive exploration; and the definition of UCB for a node is $u c b_{j}=\frac{n_{j}}{n_{j}}+2 C_{p} * \sqrt{2 \log \left(n_{p}\right) / n_{j}}$, where $C_{p}$ is a tunable hyper-parameter to control the extent of exploration, and $n_{p}$ represents #visits of the parent of node j. At a parent node, it chooses the node with the largest $u c b$ score. By following UCB from the root to a leaf, we select a path for sampling (Fig. 2(b)). When $C_{p}=0$, UCB degenerates to a pure greedy based policy, e.g. regression tree. An ablation study on $C_{p}$ in Fig. 8(a) highlights that the exploration is critical to the performance.</p>
<p>Sampling via Bayesian Optimizations: select finds a path from the root to leaf, and SVMs on the path collectively intersects a region for sampling (e.g. $\Omega_{E}$ in Fig. 2(c)). In sampling, LA-MCTS solves $\min f(\mathbf{x})$ on a constrained search space $\Omega_{\text {selected }}$, e.g. $\Omega_{E}$ in Fig. 2(c).</p>
<p>Sampling with TuRBO: here we illustrate the integration of SoTA BO method TuRBO [2] with LA-MCTS. We use TuRBO-1 (no bandit) for solving $\min f(\mathbf{x})$ within the selected region, and make the following changes inside TuRBO, which is summarized in Fig. 2(c). a) At every re-starts, we initialize TuRBO with random samples only in $\Omega_{\text {selected }}$. The shape of $\Omega_{\text {selected }}$ can be arbitrary, so we use the rejected sampling (uniformly samples and reject outliers with SVM) to get a few points inside $\Omega_{\text {selected }}$. Since we only need a few samples for the initialization, the reject sampling is sufficient. b) TuRBO centers a bounding box at the best solution so far, while we restrict the center to be the best solution in $\Omega_{\text {selected }}$. c) TuRBO uniformly samples from the bounding box to feed the acquisition to select the best as the next sample, and we restrict the TuRBO to uniformly sample from the intersection of the bounding box and $\Omega_{\text {selected }}$. The intersection is guaranteed to exist because the center is within $\Omega_{\text {selected }}$. At each iteration, we keep TuRBO running until the size of trust-region goes 0 , and all the evaluations, i.e. $\mathbf{x}<em i="i">{i}$ and $f\left(\mathbf{x}</em>\right)$, are returned to LA-MCTS to refine learned boundaries in the next iteration. Noted our method is also extensible to other solvers by following similar procedures.</p>
<p>Sampling with regular BO: following the steps described in Sec. 3.1.1, we select a leaf for sampling by traversing down from the root. The formulation of sampling with BO is same as using other solvers that $\min f(\mathbf{x}), \mathbf{x} \in \Omega_{\text {selected }}$. $\Omega_{\text {selected }}$ is constrained by SVMs on the selected path. We optimize the acquisition function of BO by sampling, while sampling in a bounded arbitrary $\Omega_{\text {selected }}$ is nontrivial especially in high-dimensional space. For example, rejected sampling can fail to work as the search space is too large to get sufficient random samples in $\Omega_{\text {selected }}$; hit-and-run [50] or Gibbs sampling [51] can be good alternatives. In Fig. 4, we propose a new heuristic for sampling. At every existing samples $\mathbf{x}$ inside $\Omega_{\text {selected }}$, we draw a
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of sampling steps in optimizing the acquisition for Bayesian Optimization. We uniformly draw samples within a hyper-cube, then expand the cube and reject outliers.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Benchmark on MuJoCo locomotion tasks: LA-MCTS consistently outperforms baselines on 6 tasks. With more dimensions, LA-MCTS shows stronger benefits (e.g. Ant and Humanoid). This is also observed in Fig. 6. Thanks to the built-in exploration mechanism, LA-MCTS experiences relatively high variance but achieves better solution after 30k samples, while other methods are quickly trapped into local optima due to insufficient exploration.
rectangle $r^{\delta}$ of length equals to $\delta$ centered at $\mathbf{x}<em i="i">{i}$ (Fig. 4(a)),
and $\mathbf{x}</em>$. Finally, we propose the sample with the largest value calculated from the acquisition function.
Fig. 3 (in Appendix) provides an example of partitioning $1 \mathrm{~d} \sin (\mathrm{x})$ using LA-MCTS.} \in \Omega \cap D_{t}$, where $\delta$ is a small constant (e.g. $10^{-4}$ ). Next, we uniformly draw random samples using sobol sequence [52] inside $r^{\delta}$. Since $\delta$ is a small constant, we assume all the random samples located inside $\Omega_{\text {selected }}$. Then we linearly scale both the rectangle $r^{\delta}$ and samples within $r^{\delta}$ until certain percentages (e.g. 10) of samples located outside of $\Omega_{\text {selected }}$ (Fig. 4(b)). We keep those samples that located inside $\Omega_{\text {selected }}$ (Fig. 4(c)) for optimizing the acquisition, and repeat the procedures for every existing samples in $\Omega_{\text {selected }} \cap D_{t</p>
<h1>4 Experiments</h1>
<p>We evaluate LA-MCTS against the SoTA baselines from different algorithm categories ranging from Bayesian Optimization (TuRBO [2], HesBO [19], BOHB [23]), Evolutionary Algorithm (Shiwa [41], CMA-ES [53], Differential Evolution (DE) [40]), MCTS (VOO [5], SOO [16], and DOO [16]), Dual Annealing [54] and Random Search. In experiments, LA-MCTS is defaulted to use TuRBO for sampling unless state otherwise. For baselines, we used the authors' reference implementations (see the bibliography for the source of implementations). The hyper-parameters of baselines are optimized toward tasks and the setup of each algorithm can be found in Appendix A.1.</p>
<h3>4.1 MuJoCo locomotion tasks</h3>
<p>MuJoCo [55] locomotion tasks (swimmer, hopper, walker-2d, half-cheetah, ant and humanoid) are among the most popular Reinforcement Learning (RL) benchmarks, and learning a humanoid model is considered one of the most difficult control problems solvable by SoTA RL methods [56]. While the push and trajectory optimization problems used in $[2,18]$ only have up to 60 parameters, MuJoCo tasks are more difficult: e.g., the most difficult task humanoid in MuJoCo has 6392 parameters.
Here we chose the linear policy $\mathbf{a}=\mathbf{W s}$ [57], where $\mathbf{s}$ is the state vector, $\mathbf{a}$ is the action vector, and $\mathbf{W}$ is the linear policy. To evaluate a policy, we average rewards from 10 episodes. We want to find $\mathbf{W}$ to maximize the reward. Each component of $\mathbf{W}$ is continuous and in the range of $[-1,1]$.</p>
<p>Table 2: Compare with gradient-based approaches on MuJoCo v1; and the performance on MuJoCo v2 is similar. Despite being a black-box optimizer, LA-MCTS still achieves good sample efficiency in low-dimensional tasks (Swimmer, Hopper and HalfCheetah), but lag behind in high-dimensional tasks due to excessive burden in exploration, which gradient approaches lack.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Reward Threshold</th>
<th style="text-align: center;">The average episodes (#samples) to reach the threshold</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LA-MCTS</td>
<td style="text-align: center;">ARS V2-t [57]</td>
<td style="text-align: center;">NG-fin [58]</td>
<td style="text-align: center;">NG-rbf [58]</td>
<td style="text-align: center;">TRPO-nn [57]</td>
</tr>
<tr>
<td style="text-align: center;">Swimmer-v2</td>
<td style="text-align: center;">325</td>
<td style="text-align: center;">126</td>
<td style="text-align: center;">427</td>
<td style="text-align: center;">1450</td>
<td style="text-align: center;">1550</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">Hopper-v2</td>
<td style="text-align: center;">3120</td>
<td style="text-align: center;">2913</td>
<td style="text-align: center;">1973</td>
<td style="text-align: center;">13920</td>
<td style="text-align: center;">8640</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: center;">HalfCheetah-v2</td>
<td style="text-align: center;">3430</td>
<td style="text-align: center;">3967</td>
<td style="text-align: center;">1707</td>
<td style="text-align: center;">11250</td>
<td style="text-align: center;">6000</td>
<td style="text-align: center;">4250</td>
</tr>
<tr>
<td style="text-align: center;">Walker2d-v2</td>
<td style="text-align: center;">4390</td>
<td style="text-align: center;">N/A $\left(r_{\text {best }}=3523\right)$</td>
<td style="text-align: center;">24000</td>
<td style="text-align: center;">36840</td>
<td style="text-align: center;">25680</td>
<td style="text-align: center;">14250</td>
</tr>
<tr>
<td style="text-align: center;">Ant-v2</td>
<td style="text-align: center;">3580</td>
<td style="text-align: center;">N/A $\left(r_{\text {best }}=2871\right)$</td>
<td style="text-align: center;">20800</td>
<td style="text-align: center;">39240</td>
<td style="text-align: center;">30000</td>
<td style="text-align: center;">73500</td>
</tr>
<tr>
<td style="text-align: center;">Humanoid-v2</td>
<td style="text-align: center;">6000</td>
<td style="text-align: center;">N/A $\left(r_{\text {best }}=3202\right)$</td>
<td style="text-align: center;">142600</td>
<td style="text-align: center;">130000</td>
<td style="text-align: center;">130000</td>
<td style="text-align: center;">unknown</td>
</tr>
</tbody>
</table>
<p>N/A stands for not reaching reward threshold.
$r_{\text {best }}$ stands for the best reward achieved by LA-MCTS under the budget in Fig. 5.
Fig. 5 suggests LA-MCTS consistently out-performs various SoTA baselines on all tasks. In particular, on high-dimensional hard problems such as ant and humanoid ${ }^{1}$, the advantage of LA-MCTS over baselines is the most obvious. Here we use TuRBO-1 to sample $\Omega_{\text {selected }}$ (see sec. 3.1.1). (a) vs TuRBO. LA-MCTS substantially outperforms TuRBO: with learned partitions, LA-MCTS reduces the region size so that TuRBO can fit a better model in small regions. Moreover, LA-MCTS helps TuRBO initialize from a promising region at every restart, while TuRBO restarts from scratch. (b) vs BO. While BO variants (e.g., BOHB) perform very well in low-dimensional problem (Fig. 5), their performance quickly deteriorates with increased problem dimensions (Fig. 5(b) $\rightarrow$ (f)) due to overexploration [15]. LA-MCTS prevents BO from over-exploring by quickly getting rid of unpromising regions. By traversing the partition tree, LA-MCTS also completely removes the step of optimizing the acquisition function, which becomes harder in high dimensions. (c) vs objective-independent space partition. Methods like VOO, SOO, and DOO use hand-designed space partition criterion (e.g., $k$-ary partition) which does not adapt to the objective. As a result, they perform poorly in high-dimensional problems. On the other hand, LA-MCTS learns the space partition that depends on the objective $f(\mathbf{x})$. The learned boundary can be nonlinear and thus can capture the characteristics of complicated objectives (e.g., the contour of $f$ ) quite well, yielding efficient partitioning. (d) vs evolutionary algorithm (EA). CMA-ES generates new samples around the influential mean, which may trap in a local optimum.</p>
<p>Comparison with gradient-based approaches: Table 2 summarizes the sample efficiency of SOTA gradient-based approach on 6 MuJoCo tasks. Note that given the prior knowledge that a gradient-based approach (i.e., exploitation-only) works well in these tasks, LA-MCTS, as a black-box optimizer, will spend extra samples for exploration and is expected to be less sample-efficient than the gradient-based approach for the same performance. Despite that, on simple tasks such as swimmer, LA-MCTS still shows superior sample efficiency than NG and TRPO, and is comparable to ARS ${ }^{2}$. For highdimensional tasks, exploration bears an excessive burden and LA-MCTS is not as sample-efficient as other gradient-based methods in MuJoCo tasks. We leave further improvement for future work.</p>
<p>Comparison with LaNAS: LaNAS lacks a surrogate model to inform sampling, while LA-MCTS samples with BO. Besides, the linear boundary in LaNAS is less adaptive to the nonlinear boundary used in LA-MCTS (e.g. Fig. 8(b)).</p>
<h1>4.2 Small-scale Benchmarks</h1>
<p>The setup of each methods can be found at Sec A. 1 in appendix, and figures are in Appendix A.2.
Synthetic functions: We further benchmark with four synthetic functions, Rosenbrock, Levy, Ackley and Rastrigin. Rosenbrock and Levy have a long and flat valley including global optima, making optimization hard. Ackley and Rastrigin function have many local optima. Fig. 9 in Appendix shows the full evaluations to baselines on the 4 functions at 20 and 100 dimensions, respectively. The</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: LA-MCTS as an effective meta-algorithm. LA-MCTS consistently improves the performance of TuRBO and BO, in particular in high-dimensional cases. We only plot part of the curve (each runs lasts for 3 day) for BO since it runs very slow in high-dimensional space.</p>
<p>result shows the performance of each solver varies a lot w.r.t functions. CMA-ES and TuRBO work well on Ackley, while Dual Annealing is the best on Rosenbrock. However, LA-MCTS consistently improves TuRBO on both functions.</p>
<p>Lunar Landing: the task is to learn a policy for the lunar landing environment implemented in the Open AI gym [59], and we used the same heuristic policy from TuRBO [2] that has 12 parameters to optimize. The state vector contains position, orientation and their time derivatives, and the state of being in contact with the land or not. The available actions are firing engine left, right, up, or idling. Fig. 10 shows LA-MCTS performs the best among baselines.</p>
<p>Rover-60d: the task was proposed in [18] that optimizes 30 coordinates in a trajectory on a 2d plane, so the state vector consists of 60 variables. LA-MCTS still performs the best on this task.</p>
<h3>4.3 Validation of LAMCTS</h3>
<p>LA-MCTS as an effective meta-algorithm: LA-MCTS internally uses TuRBO to pick promising samples from a sub-region. We also try using regular Bayesian Optimization (BO), which utilizes Expected Improvement (EI) for picking the next sample to evaluate. Fig. 6 shows LA-MCTS successfully boosts the performance of TuRBO and BO on Ackley and Rosenbrock function, in particular for high dimensional tasks. This is consistent with our results in MuJoCo tasks (Fig. 5).</p>
<p>Validating LA-MCTS. Starting from the entire search space Ω, the node model in LA-MCTS recursively splits Ω into a high-performing and a low-performing regions. The value of a region v<sup>+</sup> is expected to become closer to the global optimum v<sup><em></sup> with more and more splits. To validate this behavior, we setup LA-MCTS on Ackley-20d in the range of [−5, 10]<sup>20</sup>, and keeps track of the value of a selected partition, v<sup>+</sup><sub>i</sub> = 1/α<sub>i</sub> ∑ f(x<sub>i</sub>), ∀x<sub>i</sub> ∈ D<sub>t</sub> ∩ Ω<sub>selected</sub>, and as well as the number of splits at each steps. The global optimum of Ackley is at v<sup></em></sup> = 0. We plot the progress of regret |v<sup>+</sup><sub>i</sub> − v<sup>*</sup>| in the left axis of Fig. 7(a), and the number of splits in the right axis of Fig. 7(a). Fig. 7 shows the regret decreases as the number of splits increases, which is consistent with the expected behavior. Besides, spikes in the regret curve indicate the exploration of less promising regions from MCTS.</p>
<p>Visualizing the space partition. We further understand LA-MCTS by visualizing space partition inside LA-MCTS on 2d-Ackley in the search range of [−10, 10]<sup>2</sup>, which the global optimum v<sup><em></sup> is marked by a red star at x<sup></em></sup> = 0. First, we visualize the Ω<sub>selected</sub> in first 20 iterations, and show them in Fig. 7(b) and the full plot in Fig. 11(b) at Appendix. The purple indicates a good-performing region, while the yellow indicates a low-performing region. In iteration = 0, Ω<sub>selected</sub> misses v<sup><em></sup> due to the random initialization, but LA-MCTS consistently catches v<sup></em></sup> in Ω<sub>selected</sub> afterwards. The size of Ω<sub>selected</sub> becomes smaller as #splits increases along the search (Fig. 7(a)). Fig. 7(c) shows the selected region is collectively bounded by SVMs on the path, i.e. Ω<sub>F</sub> = Ω<sub>A</sub> ∩ Ω<sub>B</sub> ∩ Ω<sub>D</sub> ∩ Ω<sub>F</sub>.</p>
<h3>4.4 Ablations on hyper-parameters</h3>
<p>Multiple hyper-parameters in LA-MCTS, including C<sub>p</sub> in UCB, the kernel type of SVM, and the splitting threshold (θ), could impact its performance. Here ablation studies on HalfCheetah are provided for practical guidance.</p>
<p>Cp: C<sub>p</sub> controls the amount of exploration. A large C<sub>p</sub> encourages LA-MCTS to visit bad regions more often (exploration). As shown in Fig 8, too small C<sub>p</sub> leads to the worst performance, highlighting the importance of exploration. However, a large C<sub>p</sub> leads to over-exploration which is also undesired. We recommend setting C<sub>p</sub> to 10% to 1% of max f(x).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Validation of LA-MCTS: (a) the value of selected node becomes closer to the global optimum as #splits increases. (b) the visualization of $\Omega_{\text {selected }}$ in the progress of search. (c) the visualization of $\Omega_{\text {selected }}$ that takes the intersection of nodes on the selected path.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Ablation studies on hyper-parameters of LAMCTS.</p>
<p>The SVM kernel: the kernel type decides the shape of the boundary drawn by each SVM. The linear boundary yields a convex polytope, while polynomial and RBF kernel can generate arbitrary region boundary, due to their non-linearity, which leads to better performance (Fig 8(b)).
The splitting threshold $\theta$ : the splitting threshold controls the speed of tree growth. Given the same #samples, smaller $\theta$ leads to a deeper tree. If $\Omega$ is very large, more splits enable LA-MCTS to quickly focus on a small promising region, and yields good results $(\theta=10)$. However, if $\theta$ is too small, the performance and the boundary estimation of the region become more unreliable, resulting in performance deterioration ( $\theta=2$, in Fig. 8).</p>
<h1>5 Conclusion and future research</h1>
<p>The global optimization of high-dimensional black-box functions is an important topic that potentially impacts a broad spectrum of applications. We propose a novel meta method LA-MCTS that learns to partition the search space for Bayesian Optimization so that it can attend on a promising region to avoid over-exploring. Comprehensive evaluations show LA-MCTS is an effective meta-method to improve BO. In the future, we plan to extend the idea of space partitioning into Multi-Objective Optimizations.</p>
<h2>6 Broader impact</h2>
<p>Black-box optimization has a variety of applications in practice, ranging from the hyper-parameter tuning in the distributed system and database, Integrated Circuit(IC) design, Reinforcement Learning (RL), and many more. Most real-world problems are heterogeneous and high-dimensional while existing black-box solvers struggle to yield a reasonable performance in these problems. In this paper, we made our first step to show a gradient-free algorithm partially solves high-dimensional complex MuJoCo tasks, indicating its potential to other high-dimensional tasks in various domains.
Switching to LA-MCTS may improve the productivity at a minimal cost when searching for better performance in a wide variety of applications where the gradient of the function is not known. At the same time, we don't foresee any negative social consequences.</p>
<h1>Acknowledge</h1>
<p>We thank Yiyang Zhao for some follow-up experiments of the paper.</p>
<h2>References</h2>
<p>[1] Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, and Yuandong Tian. Sample-efficient neural architecture search by learning action space. arXiv preprint arXiv:1906.06832, 2019.
[2] David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local bayesian optimization. In Advances in Neural Information Processing Systems, pages 5497-5508, 2019, the implementation is from https://github.com/uber-research/TuRBO.
[3] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.
[4] Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, and Rodrigo Fonseca. Alphax: exploring neural architectures with deep neural networks and monte carlo tree search. arXiv preprint arXiv:1903.11059, 2019.
[5] Beomjoon Kim, Kyungjae Lee, Sungbin Lim, Leslie Pack Kaelbling, and Tomás Lozano-Pérez. Monte carlo tree search in continuous spaces using voronoi optimistic optimization with regret bounds. In AAAI, pages 9916-9924, 2020.
[6] Lucian Buşoniu, Alexander Daniels, Rémi Munos, and Robert Babuška. Optimistic planning for continuousaction deterministic systems. In 2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), pages 69-76. IEEE, 2013.
[7] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma, Prashanth Menon, Todd C Mowry, Matthew Perron, Ian Quah, et al. Self-driving database management systems. In CIDR, volume 4, page 1,2017 .
[8] Lorenz Fischer, Shen Gao, and Abraham Bernstein. Machines tuning machines: Configuring distributed stream processors with bayesian optimization. In 2015 IEEE International Conference on Cluster Computing, pages 22-31. IEEE, 2015.
[9] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Sungmin Bae, et al. Chip placement with deep reinforcement learning. arXiv preprint arXiv:2004.10746, 2020.
[10] AA Goldstein. Optimization of lipschitz continuous functions. Mathematical Programming, 13(1):14-22, 1977.
[11] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010.
[12] Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.
[13] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148-175, 2015.
[14] Sébastien Bubeck, Rémi Munos, Gilles Stoltz, and Csaba Szepesvári. X-armed bandits. Journal of Machine Learning Research, 12(May):1655-1695, 2011.
[15] ChangYong Oh, Efstratios Gavves, and Max Welling. Bock: Bayesian optimization with cylindrical kernels. arXiv preprint arXiv:1806.01619, 2018.
[16] Rémi Munos. Optimistic optimization of a deterministic function without the knowledge of its smoothness. In Advances in neural information processing systems, pages 783-791, 2011.
[17] Ziyu Wang, Babak Shakibi, Lin Jin, and Nando Freitas. Bayesian multi-scale optimistic optimization. In Artificial Intelligence and Statistics, pages 1005-1014, 2014.
[18] Zi Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka. Batched large-scale bayesian optimization in high-dimensional spaces. arXiv preprint arXiv:1706.01445, 2017.
[19] Amin Nayebi, Alexander Munteanu, and Matthias Poloczek. A framework for bayesian optimization in embedded subspaces. In International Conference on Machine Learning, pages 4752-4761, 2019, the implementation is from https://github.com/aminnayebi/HesBO.
[20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In International conference on learning and intelligent optimization, pages 507-523. Springer, 2011.</p>
<p>[21] Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust bayesian neural networks. In Advances in neural information processing systems, pages 4134-4142, 2016.
[22] James S Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, pages 2546-2554, 2011.
[23] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. arXiv preprint arXiv:1807.01774, 2018, the implementation is from https://github.com/automl/HpBandSter.
[24] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765-6816, 2017.
[25] Matthias Seeger, Christopher Williams, and Neil Lawrence. Fast forward selection to speed up sparse gaussian process regression. Technical report, EPFL, 2003.
[26] Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In Advances in neural information processing systems, pages 1257-1264, 2006.
[27] James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. arXiv preprint arXiv:1309.6835, 2013.
[28] Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando De Freitas. Bayesian optimization in high dimensions via random embeddings. In Twenty-Third International Joint Conference on Artificial Intelligence, 2013.
[29] Kenji Kawaguchi, Leslie Pack Kaelbling, and Tomás Lozano-Pérez. Bayesian optimization with exponential convergence. In Advances in neural information processing systems, pages 2809-2817, 2015.
[30] Mitchell McIntire, Daniel Ratner, and Stefano Ermon. Sparse gaussian processes for bayesian optimization. In UAI, 2016.
[31] Bo Chen, Rui Castro, and Andreas Krause. Joint optimization and variable selection of high-dimensional gaussian processes. arXiv preprint arXiv:1206.6396, 2012.
[32] Kirthevasan Kandasamy, Jeff Schneider, and Barnabás Póczos. High dimensional bayesian optimisation and bandits via additive models. In International Conference on Machine Learning, pages 295-304, 2015.
[33] Mickaël Binois, David Ginsbourger, and Olivier Roustant. A warped kernel improving robustness in bayesian optimization via random embeddings. In International Conference on Learning and Intelligent Optimization, pages 281-286. Springer, 2015.
[34] Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, and Nando de Feitas. Bayesian optimization in a billion dimensions via random embeddings. Journal of Artificial Intelligence Research, 55:361-387, 2016.
[35] Jacob Gardner, Chuan Guo, Kilian Weinberger, Roman Garnett, and Roger Grosse. Discovering and exploiting additive structure for bayesian optimization. In Artificial Intelligence and Statistics, pages $1311-1319,2017$.
[36] Paul Rolland, Jonathan Scarlett, Ilija Bogunovic, and Volkan Cevher. High-dimensional bayesian optimization via additive models with overlapping groups. arXiv preprint arXiv:1802.07028, 2018.
[37] Mojmir Mutny and Andreas Krause. Efficient high dimensional bayesian optimization with additivity and quadrature fourier features. In Advances in Neural Information Processing Systems, pages 9005-9016, 2018.
[38] Mickaël Binois, David Ginsbourger, and Olivier Roustant. On the choice of the low-dimensional domain for global optimization via random embeddings. Journal of global optimization, 76(1):69-90, 2020.
[39] Yaochu Jin and Jürgen Branke. Evolutionary optimization in uncertain environments-a survey. IEEE Transactions on evolutionary computation, 9(3):303-317, 2005.
[40] Rainer Storn and Kenneth Price. Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. Journal of global optimization, 11(4):341-359, 1997.
[41] Jialin Liu, Antoine Moreau, Mike Preuss, Baptiste Roziere, Jeremy Rapin, Fabien Teytaud, and Olivier Teytaud. Versatile black-box optimization. arXiv preprint arXiv:2004.14014, 2020, the implementation is from https://github.com/facebookresearch/nevergrad.
[42] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
[43] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.</p>
<p>[44] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354-359, 2017.
[45] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1-43, 2012.
[46] Rémi Munos. From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning. technical report, $\mathrm{x}(\mathrm{x}): \mathrm{x}, 2014$.
[47] Ari Weinstein and Michael L Littman. Bandit-based planning and learning in continuous-action markov decision processes. In Twenty-Second International Conference on Automated Planning and Scheduling, 2012.
[48] Chris Mansley, Ari Weinstein, and Michael Littman. Sample-based planning for continuous action markov decision processes. In Twenty-First International Conference on Automated Planning and Scheduling, 2011.
[49] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235-256, 2002.
[50] Claude JP Bélisle, H Edwin Romeijn, and Robert L Smith. Hit-and-run algorithms for generating multivariate distributions. Mathematics of Operations Research, 18(2):255-266, 1993.
[51] Alan E Gelfand and Adrian FM Smith. Sampling-based approaches to calculating marginal densities. Journal of the American statistical association, 85(410):398-409, 1990.
[52] Il'ya Meerovich Sobol'. On the distribution of points in a cube and the approximate evaluation of integrals. Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki, 7(4):784-802, 1967.
[53] Nikolaus Hansen, Sibylle D Müller, and Petros Koumoutsakos. Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es). Evolutionary computation, 11(1):1-18, 2003, the implementation is from https://github.com/CMA-ES/pycma.
[54] Martin Pincus. Letter to the editor-a monte carlo method for the approximate solution of certain types of constrained optimization problems. Operations Research, 18(6):1225-1228, 1970, the implementation is from https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html.
[55] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.
[56] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
[57] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018, the implementation is from https://github.com/modestyachts/ARS.
[58] Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards generalization and simplicity in continuous control. In Advances in Neural Information Processing Systems, pages $6550-6561,2017$.
[59] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<h1>A Additional Experiment Details</h1>
<h2>A. 1 Hyper-parameter settings for all baselines in benchmarks</h2>
<p>Setup for MuJoCo tasks: Fig. 5 shows 13 methods in total, and here we describe the hyperparameters of each method. Since we're interested in the sample efficiency, the batch size of every method is set to 1 . We reuse the policy and evaluation codes from ARS [57], and the URL to the ARS implementation can be found in the bibliography. The implementations of VOO, SOO, and DOO are from here ${ }^{3}$; methods including CMA-ES, Differential Evolution, Dual Annealing are from the optimize module in scipy, and Shiwa is from Nevergrad ${ }^{4}$. Please see the bibliography for the reference implementations of BOHB and HesBO.</p>
<p>LA-MCTS we use 30 samples for the initialization; and the SVM uses RBF kernel for easy tasks including swimmer, hopper, half-cheetah, and linear kernel for hard tasks including $2 d$ walker, ant and humanoid to get over $3 * 10^{4}$ samples. $C_{p}$ is set to 10 , and the splitting threshold $\theta$ is set to 100. LA-MCTS uses TuRBO-1 for sampling, and the setup of TuRBO-1 is exactly the same as TuRBO described below. TuRBO-1 returns all the samples and their evaluations to LA-MCTS once it hits the re-start criterion.
TuRBO we use 30 samples for the initialization, and CUDA is enabled. The rest hyper-parameters use the default value in TuRBO. In MuJoCo, we used TuRBO-20 that uses 20 independent trust regions for the best $f(x)$.
LaNAS we use 30 samples for the initialization; the height of search tree is 8 , and $C_{p}$ is set to 10 .
VOO default setting in the reference implementation.
DOO default setting in the reference implementation.
SOO default setting in the reference implementation.
CMA-ES the initial standard deviation is set to 0.5 , and the rest parameters are default in Scipy.
Diff-Evo default settings in Scipy.
Shiwa default settings in Nevergrad.
Annealing default settings in Scipy.
BOHB default settings in the reference implementation.
HesBO The tuned embedding dimensions for swimmer, hopper, walker, half-cheetah, ant, and humanoid are $8,17,50,50,400$, and 1000 , respectively.</p>
<p>Setup for synthetic functions, lunar landing, and trajectory optimization: similar to MuJoCo tasks, the batch size of each method is set to 1 . The settings of VOO, DOO, SOO, CMA-ES, Diff-Evo, Dual Annealing, Shiwa, BOHB, TuRBO are the same as the settings from MuJoCo. We modify $C_{p}=1$ and the splitting threshold $\theta=20$ in LA-MCTS. Similarly, we also changed $C_{p}=1$ in LaNAS. We set the upper and lower limits of each dimension in Ackley as [-5, 10], Rosenbrock is set within $[-10,10]$, Rastrigin is set within $[-5.12,5.12]$, Levy is set within $[-10,10]$.
Runtime: LaNAS, VOO, DOO, SOO, CMA-ES, Diff-Evo, Shiwa, Annealing are fairly fast, which can collect thousands of samples in minutes. The runtime performance of LAMCTS and TuRBO are consistent with the result in [2] (see sec.G in appendix) that collects $10^{4}$ samples in an hour using 1 V100 GPU. BOHB and HesBO toke up to a day to collect $10^{4}$ samples for running on CPU.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>A. 2 Additional experiment results</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Evaluations on synthetic functions: the best method varies w.r.t functions, while LAMCTS consistently improves TuRBO and being among top methods among all functions.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Evaluations on Lunar landing and Trajectory Optimization: LA-MCTS consistently outperforms baselines.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: The visualization of LA-MCTS in iterations 1 $\rightarrow$ 20: the purple region is the selected region $\Omega_{\text {selected }}$, and the red star represents the global optimum.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/beomjoonkim/voot
${ }^{4}$ https://github.com/facebookresearch/nevergrad&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
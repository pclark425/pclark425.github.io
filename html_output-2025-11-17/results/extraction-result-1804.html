<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1804 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1804</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1804</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-7637ed79d30d0139901175ae4abedd822c217ab4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7637ed79d30d0139901175ae4abedd822c217ab4" target="_blank">3D-LLM: Injecting the 3D World into Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs that can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1804.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1804.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LLM (BLIP2-FlanT5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LLM using BLIP-2 visual backbone and Flan-T5 language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D-capable multimodal LLM introduced in this paper that ingests 3D point-cloud features (constructed from multi-view rendered images) and language prompts, built by connecting BLIP-2 image-language backbones with a Flan-T5 LLM and augmented with a 3D localization mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-LLM (BLIP2-FlanT5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Architecture: BLIP-2 based visual-language backbone (frozen image encoder outputs -> Q-Former/perceiver) connected to a Flan-T5 LLM (mostly frozen except added location token embeddings); inputs are 3D point features (1408-dim mapped to BLIP-2 feature space) plus sinusoidal 3D position embeddings; outputs are natural language responses or discrete location tokens (AABB token sequence). Trained on the paper's 300k synthetic/auto-generated 3D-language dataset and fine-tuned on downstream 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>language (Flan-T5 pretraining/instruction tuning) + image-language (BLIP-2 pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper initializes from public BLIP-2 checkpoints (LAVIS) and a pre-trained Flan-T5 model; exact pretraining corpora and sizes for those upstream models are not specified in this paper (they reference BLIP-2 and Flan-T5 papers for details). The authors additionally pretrain the 3D-LLM on their collected ~300k 3D-language pairs (generated via GPT-based prompting pipelines) for 100k training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ScanQA, 3DMV-VQA, ScanRefer (3D grounding), 3D dense captioning, object navigation (HM3D / Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>A variety of 3D embodied / scene-understanding tasks: ScanQA and 3DMV-VQA (3D question answering on reconstructed indoor scans), ScanRefer (referring expression -> 3D bounding box grounding), 3D dense captioning (captioning of regions/boxes), and an object navigation formulation where at each timestep the model ingests partial 3D observations and predicts waypoints for a navigation policy in Habitat on HM3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>No explicit discrete action space during upstream Flan-T5 pretraining — the model is trained on token-level language prediction and instruction-following (natural language tokens / instructions / responses).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Task-dependent: for grounding and QA the "actions" are text outputs or discretized location tokens (AABB coordinate tokens); for navigation the 3D-LLM predicts a 3D waypoint (continuous/discretized coordinate) which is then executed by a low-level local policy producing discrete navigation primitives (e.g., go forward, turn left/right); for dense captioning the action is generation of caption tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Mapping is task-specific and learned: (1) For grounding, continuous box corner coordinates are uniformly discretized into voxel integers and represented as discrete location tokens the LLM outputs; (2) For navigation, the 3D-LLM outputs a waypoint (3D coordinate) which is mapped to low-level control by an off-the-shelf local policy (reference [46]) that converts the waypoint to discrete locomotion actions; (3) For other tasks the model directly outputs language tokens (answers/captions).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Requires rendered multi-view RGB (and when available RGB-D) images, camera poses; 3D features are constructed by extracting dense 2D features from rendered views (CLIP/EVA-like features), then mapping/fusing into point/voxel features via Direct Reconstruction / Feature Fusion / Neural Field methods depending on data. For online navigation partial-view 3D feature construction is used.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>ScanQA validation BLEU-1: 39.3 (Table 1); ScanQA test BLEU-1: 38.3 (Table 2). 3DMV-VQA overall: 54.6 (Table 4). ScanRefer grounding ACC@0.25: 30.3, Avg IoU: 24.9, Avg Dist: 1.03 (Table 5). Held-in 3D captioning BLEU-1: 39.8; 3D-assisted dialog BLEU-1: 39.0; task decomposition BLEU-1: 33.9 (Table 3, Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines (non-3D pretrained / 2D or image-only models): ScanQA val BLEU-1: e.g., ScanQA baseline 30.2, VoteNet+MCAN 28.0, BLIP2-flant5-MultiView (image multi-view) 29.7 (Table 1). On 3DMV-VQA baselines: 3D-CLR 57.7 overall (some methods use explicit object representations), flamingo-MultiView overall ~41.6 (Table 4). For grounding ScanRefer, the ScanRefer baseline ACC@0.25 = 41.2 (Table 5) — note methods differ in use of object proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key factors: (1) Aligning 3D features into the same feature space as pretrained 2D image encoders (so BLIP-2 features can be reused); (2) Using pretrained language model Flan-T5 (instruction-tuned) to leverage strong language priors; (3) A 3D localization mechanism (sin/cos 3D position embeddings + discrete location tokens) that bridges spatial coordinates with token outputs; (4) Large synthetic/auto-generated 3D-language pretraining dataset (~300k pairs) for multi-task multimodal tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations noted: reliance on rendering multi-view images for feature extraction (extra processing step), possible disorganization of multi-view image features (multi-view image baselines performed worse), mismatch between holistic 3D features and methods that rely on explicit object proposals (some baselines using object detectors can outperform on certain metrics like ScanRefer ACC@0.25). Exact sample-efficiency gains and ablations on pretraining vs. none are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language (Flan-T5) + image-language (BLIP-2) components can be adapted to 3D embodied tasks by mapping 2D pretrained features into 3D feature space and augmenting the LLM vocabulary with discrete location tokens; this yields substantial improvements on 3D question answering, captioning and some VQA benchmarks versus 2D image-based baselines, though grounding performance versus object-proposal-based methods can vary; the approach demonstrates positive transfer from language/image-language pretraining into 3D reasoning and waypoint-level navigation decisions, but sample complexity numbers are not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1804.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1804.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LLM (BLIP2-Opt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LLM using BLIP-2 visual backbone and OPT language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D-LLM variant that pairs BLIP-2 visual features with an OPT-family LLM (Opt weights largely frozen except location tokens) to perform 3D tasks using mapped 3D features and the 3D localization mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-LLM (BLIP2-Opt)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Architecture: BLIP-2 visual backbone producing 1408-dim features mapped from 2D to 3D, connected to an OPT LLM (kept mostly frozen) with added learnable embeddings for discrete 3D location tokens and added position embeddings on 3D features; trained on the paper's 300k 3D-language dataset and fine-tuned on downstream 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>image-language (BLIP-2) + language (OPT pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Initialized from BLIP-2 checkpoints (LAVIS) and OPT language model weights; exact upstream dataset sizes and corpora for OPT and BLIP-2 are not specified in this paper. The paper's 3D pretraining uses ~300k generated 3D-language pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ScanQA, 3DMV-VQA, ScanRefer (grounding), 3D dense captioning, object navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same suite as other 3D-LLM variants: 3D question answering (ScanQA/3DMV-VQA), 3D referring (ScanRefer), dense captioning and a waypoint-based navigation formulation in Habitat/HM3D.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language token prediction during OPT pretraining (no explicit action symbols in upstream text corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Outputs textual answers and discrete location tokens for grounding; predicts 3D waypoints for navigation which are converted to low-level discrete navigation actions by a separate local policy.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Discrete location tokens (AABB corners discretized to voxel ints) for grounding; for navigation predicted waypoint -> off-the-shelf local policy (e.g., dd-ppo style controller) converts waypoint to discrete actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multi-view rendered RGB/RGB-D, camera poses; creation of point/voxel 3D features via Direct Reconstruction / Fusion / Neural Field methods; online partial map construction for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>ScanQA validation BLEU-1: 35.9 (Table 1). ScanQA test BLEU-1: 37.3 (Table 2). 3DMV-VQA overall: 54.9 (Table 4). Held-in tasks: 3D captioning BLEU-1: 35.7; 3D-assisted dialog BLEU-1: 39.6; task decomposition BLEU-1: 34.1 (Tables 1,2,3,4,6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Comparable 2D/multi-view baselines: BLIP2-flant5-MultiView val BLEU-1 29.7; flamingo-MultiView 25.6 (Table 1). 3D-Feature+LSTM baseline on 3DMV-VQA overall 48.2 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Shared feature space between 2D pretrained image encoders and mapped 3D features; frozen pretrained language (OPT) provides strong language priors and stable generation; added location-token embedding and position embeddings provide spatial grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Paper does not report sample efficiency; limitations include dependence on rendered multi-view inputs and potential mismatch with object-proposal based pipelines which sometimes have advantages for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using BLIP-2 visual features with a pretrained OPT LLM and augmenting with 3D-specific embeddings yields robust gains across 3D QA, captioning and dialog compared to 2D image-based inputs, supporting transfer from language/image-language pretraining when 3D features are mapped into the same representation space.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1804.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1804.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LLM (Flamingo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LLM using Flamingo backbone</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D-LLM variant built on the Flamingo architecture (Perceiver + gated cross-attention) that ingests 3D features mapped into Flamingo's CLIP feature space and is fine-tuned for a range of 3D tasks with added location tokens and position embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-LLM (Flamingo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Architecture: Flamingo 9B style backbone with perceiver resampler and gated cross-attention layers initialized from OpenFlamingo checkpoints; 3D features are 1024-dim mapped to CLIP feature space, appended with 3D sinusoidal position embeddings; localization via discrete location tokens; perceiver and cross-attention layers finetuned while LLM core mostly frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>image-language pretraining (Flamingo / CLIP) + language pretraining of underlying LLM(s)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Initialized from OpenFlamingo weights; exact upstream corpora and sizes for Flamingo/CLIP are not specified in this paper. The authors pretrain their 3D-LLM on their ~300k 3D-language dataset and fine-tune on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ScanQA, 3DMV-VQA, ScanRefer (grounding), 3D dense captioning, object navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same 3D scene understanding and embodied tasks as other variants: 3D question answering (ScanQA/3DMV-VQA), 3D grounding (ScanRefer), dense captioning, and waypoint-based navigation in Habitat/HM3D.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Upstream Flamingo/CLIP pretraining uses image-text pairs and token-level language outputs; no explicit symbolic action space in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Language outputs and discrete location tokens for grounding; predicted 3D waypoints (coordinates) that are then executed by a low-level navigation policy producing discrete locomotion primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Location tokens represent discretized AABB coordinates for grounding; waypoint predictions are translated to low-level controls by a separate local navigation controller; Flamingo's perceiver resampler handles variable-sized 3D input features.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Same as other variants: rendered multi-view RGB/RGB-D inputs with camera poses for constructing point/voxel features; perceiver handles arbitrary sized point clouds / feature sets.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>ScanQA validation BLEU-1: 30.3 (Table 1); ScanQA test BLEU-1: 32.6 (Table 2). 3DMV-VQA overall: 58.6 (Table 4, highest concept and relation in that table). Held-in 3D captioning BLEU-1: 36.1; 3D-assisted dialog BLEU-1: 35.0; task decomposition BLEU-1: 32.9 (Tables 1-4,3,6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines: flamingo-MultiView val BLEU-1 25.6; single-image flamingo 23.8 (Table 1). 3D-Feature+LSTM baseline on 3DMV-VQA 48.2 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Perceiver resampler enables handling variable-sized 3D input mapped into Flamingo/CLIP feature space; leveraging pretrained CLIP-like image features and Flamingo cross-attention yields improved 3D reasoning when 3D features are aligned to the same space; localization token mechanism supports grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Paper reports perceiver is beneficial but does not quantify sample-efficiency; grounding accuracy (ACC@0.25) is lower than some object-proposal-based methods (ScanRefer) in their evaluation, suggesting explicit object proposals remain competitive for certain grounding metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Flamingo-based 3D-LLM successfully leverages pretrained image-language inductive biases when 3D features are projected into the same representation space and augmented with spatial embeddings, improving performance across multiple 3D tasks relative to 2D-only baselines; the perceiver resampler is empirically helpful for training on 3D features.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models <em>(Rating: 2)</em></li>
                <li>Flamingo: a visual language model for few-shot learning <em>(Rating: 2)</em></li>
                <li>Openflamingo <em>(Rating: 2)</em></li>
                <li>CLIP on wheels: Zero-shot object navigation as object localization and exploration <em>(Rating: 2)</em></li>
                <li>Conceptfusion: Open-set multimodal 3D mapping <em>(Rating: 2)</em></li>
                <li>3D concept learning and reasoning from multi-view images <em>(Rating: 2)</em></li>
                <li>dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames <em>(Rating: 2)</em></li>
                <li>3D question answering <em>(Rating: 1)</em></li>
                <li>ScanRefer: 3D object localization in RGB-D scans using natural language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1804",
    "paper_id": "paper-7637ed79d30d0139901175ae4abedd822c217ab4",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "3D-LLM (BLIP2-FlanT5)",
            "name_full": "3D-LLM using BLIP-2 visual backbone and Flan-T5 language model",
            "brief_description": "A 3D-capable multimodal LLM introduced in this paper that ingests 3D point-cloud features (constructed from multi-view rendered images) and language prompts, built by connecting BLIP-2 image-language backbones with a Flan-T5 LLM and augmented with a 3D localization mechanism.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "3D-LLM (BLIP2-FlanT5)",
            "model_agent_description": "Architecture: BLIP-2 based visual-language backbone (frozen image encoder outputs -&gt; Q-Former/perceiver) connected to a Flan-T5 LLM (mostly frozen except added location token embeddings); inputs are 3D point features (1408-dim mapped to BLIP-2 feature space) plus sinusoidal 3D position embeddings; outputs are natural language responses or discrete location tokens (AABB token sequence). Trained on the paper's 300k synthetic/auto-generated 3D-language dataset and fine-tuned on downstream 3D tasks.",
            "pretraining_data_type": "language (Flan-T5 pretraining/instruction tuning) + image-language (BLIP-2 pretraining)",
            "pretraining_data_details": "The paper initializes from public BLIP-2 checkpoints (LAVIS) and a pre-trained Flan-T5 model; exact pretraining corpora and sizes for those upstream models are not specified in this paper (they reference BLIP-2 and Flan-T5 papers for details). The authors additionally pretrain the 3D-LLM on their collected ~300k 3D-language pairs (generated via GPT-based prompting pipelines) for 100k training steps.",
            "embodied_task_name": "ScanQA, 3DMV-VQA, ScanRefer (3D grounding), 3D dense captioning, object navigation (HM3D / Habitat)",
            "embodied_task_description": "A variety of 3D embodied / scene-understanding tasks: ScanQA and 3DMV-VQA (3D question answering on reconstructed indoor scans), ScanRefer (referring expression -&gt; 3D bounding box grounding), 3D dense captioning (captioning of regions/boxes), and an object navigation formulation where at each timestep the model ingests partial 3D observations and predicts waypoints for a navigation policy in Habitat on HM3D environments.",
            "action_space_text": "No explicit discrete action space during upstream Flan-T5 pretraining — the model is trained on token-level language prediction and instruction-following (natural language tokens / instructions / responses).",
            "action_space_embodied": "Task-dependent: for grounding and QA the \"actions\" are text outputs or discretized location tokens (AABB coordinate tokens); for navigation the 3D-LLM predicts a 3D waypoint (continuous/discretized coordinate) which is then executed by a low-level local policy producing discrete navigation primitives (e.g., go forward, turn left/right); for dense captioning the action is generation of caption tokens.",
            "action_mapping_method": "Mapping is task-specific and learned: (1) For grounding, continuous box corner coordinates are uniformly discretized into voxel integers and represented as discrete location tokens the LLM outputs; (2) For navigation, the 3D-LLM outputs a waypoint (3D coordinate) which is mapped to low-level control by an off-the-shelf local policy (reference [46]) that converts the waypoint to discrete locomotion actions; (3) For other tasks the model directly outputs language tokens (answers/captions).",
            "perception_requirements": "Requires rendered multi-view RGB (and when available RGB-D) images, camera poses; 3D features are constructed by extracting dense 2D features from rendered views (CLIP/EVA-like features), then mapping/fusing into point/voxel features via Direct Reconstruction / Feature Fusion / Neural Field methods depending on data. For online navigation partial-view 3D feature construction is used.",
            "transfer_successful": true,
            "performance_with_pretraining": "ScanQA validation BLEU-1: 39.3 (Table 1); ScanQA test BLEU-1: 38.3 (Table 2). 3DMV-VQA overall: 54.6 (Table 4). ScanRefer grounding ACC@0.25: 30.3, Avg IoU: 24.9, Avg Dist: 1.03 (Table 5). Held-in 3D captioning BLEU-1: 39.8; 3D-assisted dialog BLEU-1: 39.0; task decomposition BLEU-1: 33.9 (Table 3, Table 6).",
            "performance_without_pretraining": "Baselines (non-3D pretrained / 2D or image-only models): ScanQA val BLEU-1: e.g., ScanQA baseline 30.2, VoteNet+MCAN 28.0, BLIP2-flant5-MultiView (image multi-view) 29.7 (Table 1). On 3DMV-VQA baselines: 3D-CLR 57.7 overall (some methods use explicit object representations), flamingo-MultiView overall ~41.6 (Table 4). For grounding ScanRefer, the ScanRefer baseline ACC@0.25 = 41.2 (Table 5) — note methods differ in use of object proposals.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Key factors: (1) Aligning 3D features into the same feature space as pretrained 2D image encoders (so BLIP-2 features can be reused); (2) Using pretrained language model Flan-T5 (instruction-tuned) to leverage strong language priors; (3) A 3D localization mechanism (sin/cos 3D position embeddings + discrete location tokens) that bridges spatial coordinates with token outputs; (4) Large synthetic/auto-generated 3D-language pretraining dataset (~300k pairs) for multi-task multimodal tuning.",
            "transfer_failure_factors": "Limitations noted: reliance on rendering multi-view images for feature extraction (extra processing step), possible disorganization of multi-view image features (multi-view image baselines performed worse), mismatch between holistic 3D features and methods that rely on explicit object proposals (some baselines using object detectors can outperform on certain metrics like ScanRefer ACC@0.25). Exact sample-efficiency gains and ablations on pretraining vs. none are not reported.",
            "key_findings": "Pretrained language (Flan-T5) + image-language (BLIP-2) components can be adapted to 3D embodied tasks by mapping 2D pretrained features into 3D feature space and augmenting the LLM vocabulary with discrete location tokens; this yields substantial improvements on 3D question answering, captioning and some VQA benchmarks versus 2D image-based baselines, though grounding performance versus object-proposal-based methods can vary; the approach demonstrates positive transfer from language/image-language pretraining into 3D reasoning and waypoint-level navigation decisions, but sample complexity numbers are not reported.",
            "uuid": "e1804.0"
        },
        {
            "name_short": "3D-LLM (BLIP2-Opt)",
            "name_full": "3D-LLM using BLIP-2 visual backbone and OPT language model",
            "brief_description": "A 3D-LLM variant that pairs BLIP-2 visual features with an OPT-family LLM (Opt weights largely frozen except location tokens) to perform 3D tasks using mapped 3D features and the 3D localization mechanism.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "3D-LLM (BLIP2-Opt)",
            "model_agent_description": "Architecture: BLIP-2 visual backbone producing 1408-dim features mapped from 2D to 3D, connected to an OPT LLM (kept mostly frozen) with added learnable embeddings for discrete 3D location tokens and added position embeddings on 3D features; trained on the paper's 300k 3D-language dataset and fine-tuned on downstream 3D tasks.",
            "pretraining_data_type": "image-language (BLIP-2) + language (OPT pretraining)",
            "pretraining_data_details": "Initialized from BLIP-2 checkpoints (LAVIS) and OPT language model weights; exact upstream dataset sizes and corpora for OPT and BLIP-2 are not specified in this paper. The paper's 3D pretraining uses ~300k generated 3D-language pairs.",
            "embodied_task_name": "ScanQA, 3DMV-VQA, ScanRefer (grounding), 3D dense captioning, object navigation",
            "embodied_task_description": "Same suite as other 3D-LLM variants: 3D question answering (ScanQA/3DMV-VQA), 3D referring (ScanRefer), dense captioning and a waypoint-based navigation formulation in Habitat/HM3D.",
            "action_space_text": "Language token prediction during OPT pretraining (no explicit action symbols in upstream text corpora).",
            "action_space_embodied": "Outputs textual answers and discrete location tokens for grounding; predicts 3D waypoints for navigation which are converted to low-level discrete navigation actions by a separate local policy.",
            "action_mapping_method": "Discrete location tokens (AABB corners discretized to voxel ints) for grounding; for navigation predicted waypoint -&gt; off-the-shelf local policy (e.g., dd-ppo style controller) converts waypoint to discrete actions.",
            "perception_requirements": "Multi-view rendered RGB/RGB-D, camera poses; creation of point/voxel 3D features via Direct Reconstruction / Fusion / Neural Field methods; online partial map construction for navigation.",
            "transfer_successful": true,
            "performance_with_pretraining": "ScanQA validation BLEU-1: 35.9 (Table 1). ScanQA test BLEU-1: 37.3 (Table 2). 3DMV-VQA overall: 54.9 (Table 4). Held-in tasks: 3D captioning BLEU-1: 35.7; 3D-assisted dialog BLEU-1: 39.6; task decomposition BLEU-1: 34.1 (Tables 1,2,3,4,6).",
            "performance_without_pretraining": "Comparable 2D/multi-view baselines: BLIP2-flant5-MultiView val BLEU-1 29.7; flamingo-MultiView 25.6 (Table 1). 3D-Feature+LSTM baseline on 3DMV-VQA overall 48.2 (Table 4).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Shared feature space between 2D pretrained image encoders and mapped 3D features; frozen pretrained language (OPT) provides strong language priors and stable generation; added location-token embedding and position embeddings provide spatial grounding.",
            "transfer_failure_factors": "Paper does not report sample efficiency; limitations include dependence on rendered multi-view inputs and potential mismatch with object-proposal based pipelines which sometimes have advantages for grounding.",
            "key_findings": "Using BLIP-2 visual features with a pretrained OPT LLM and augmenting with 3D-specific embeddings yields robust gains across 3D QA, captioning and dialog compared to 2D image-based inputs, supporting transfer from language/image-language pretraining when 3D features are mapped into the same representation space.",
            "uuid": "e1804.1"
        },
        {
            "name_short": "3D-LLM (Flamingo)",
            "name_full": "3D-LLM using Flamingo backbone",
            "brief_description": "A 3D-LLM variant built on the Flamingo architecture (Perceiver + gated cross-attention) that ingests 3D features mapped into Flamingo's CLIP feature space and is fine-tuned for a range of 3D tasks with added location tokens and position embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "3D-LLM (Flamingo)",
            "model_agent_description": "Architecture: Flamingo 9B style backbone with perceiver resampler and gated cross-attention layers initialized from OpenFlamingo checkpoints; 3D features are 1024-dim mapped to CLIP feature space, appended with 3D sinusoidal position embeddings; localization via discrete location tokens; perceiver and cross-attention layers finetuned while LLM core mostly frozen.",
            "pretraining_data_type": "image-language pretraining (Flamingo / CLIP) + language pretraining of underlying LLM(s)",
            "pretraining_data_details": "Initialized from OpenFlamingo weights; exact upstream corpora and sizes for Flamingo/CLIP are not specified in this paper. The authors pretrain their 3D-LLM on their ~300k 3D-language dataset and fine-tune on downstream tasks.",
            "embodied_task_name": "ScanQA, 3DMV-VQA, ScanRefer (grounding), 3D dense captioning, object navigation",
            "embodied_task_description": "Same 3D scene understanding and embodied tasks as other variants: 3D question answering (ScanQA/3DMV-VQA), 3D grounding (ScanRefer), dense captioning, and waypoint-based navigation in Habitat/HM3D.",
            "action_space_text": "Upstream Flamingo/CLIP pretraining uses image-text pairs and token-level language outputs; no explicit symbolic action space in pretraining.",
            "action_space_embodied": "Language outputs and discrete location tokens for grounding; predicted 3D waypoints (coordinates) that are then executed by a low-level navigation policy producing discrete locomotion primitives.",
            "action_mapping_method": "Location tokens represent discretized AABB coordinates for grounding; waypoint predictions are translated to low-level controls by a separate local navigation controller; Flamingo's perceiver resampler handles variable-sized 3D input features.",
            "perception_requirements": "Same as other variants: rendered multi-view RGB/RGB-D inputs with camera poses for constructing point/voxel features; perceiver handles arbitrary sized point clouds / feature sets.",
            "transfer_successful": true,
            "performance_with_pretraining": "ScanQA validation BLEU-1: 30.3 (Table 1); ScanQA test BLEU-1: 32.6 (Table 2). 3DMV-VQA overall: 58.6 (Table 4, highest concept and relation in that table). Held-in 3D captioning BLEU-1: 36.1; 3D-assisted dialog BLEU-1: 35.0; task decomposition BLEU-1: 32.9 (Tables 1-4,3,6).",
            "performance_without_pretraining": "Baselines: flamingo-MultiView val BLEU-1 25.6; single-image flamingo 23.8 (Table 1). 3D-Feature+LSTM baseline on 3DMV-VQA 48.2 (Table 4).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Perceiver resampler enables handling variable-sized 3D input mapped into Flamingo/CLIP feature space; leveraging pretrained CLIP-like image features and Flamingo cross-attention yields improved 3D reasoning when 3D features are aligned to the same space; localization token mechanism supports grounding.",
            "transfer_failure_factors": "Paper reports perceiver is beneficial but does not quantify sample-efficiency; grounding accuracy (ACC@0.25) is lower than some object-proposal-based methods (ScanRefer) in their evaluation, suggesting explicit object proposals remain competitive for certain grounding metrics.",
            "key_findings": "Flamingo-based 3D-LLM successfully leverages pretrained image-language inductive biases when 3D features are projected into the same representation space and augmented with spatial embeddings, improving performance across multiple 3D tasks relative to 2D-only baselines; the perceiver resampler is empirically helpful for training on 3D features.",
            "uuid": "e1804.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "rating": 2
        },
        {
            "paper_title": "Flamingo: a visual language model for few-shot learning",
            "rating": 2
        },
        {
            "paper_title": "Openflamingo",
            "rating": 2
        },
        {
            "paper_title": "CLIP on wheels: Zero-shot object navigation as object localization and exploration",
            "rating": 2
        },
        {
            "paper_title": "Conceptfusion: Open-set multimodal 3D mapping",
            "rating": 2
        },
        {
            "paper_title": "3D concept learning and reasoning from multi-view images",
            "rating": 2
        },
        {
            "paper_title": "dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames",
            "rating": 2
        },
        {
            "paper_title": "3D question answering",
            "rating": 1
        },
        {
            "paper_title": "ScanRefer: 3D object localization in RGB-D scans using natural language",
            "rating": 1
        }
    ],
    "cost": 0.01827775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>3D-LLM: Injecting the 3D World into Large Language Models</h1>
<p>Yining Hong<br>University of California, Los Angeles<br>Heoyu Zhen<br>Shanghai Jiao Tong University<br>Peihao Chen<br>South China University of Technology<br>Shuhong Zheng<br>University of Illinois Urbana-Champaign<br>Yilun Du<br>Massachusetts Institute of Technology<br>Zhenfang Chen<br>MIT-IBM Watson AI Lab<br>Chuang Gan<br>UMass Amherst and MIT-IBM Watson AI Lab</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multiview images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by $9 \%$ ). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.</p>
<h2>1 Introduction</h2>
<p>In the past several years, we have witnessed a surge of large language models (LLMs) (e.g., GPT4 [33]) that excel at multiple tasks, such as communication and commonsense reasoning. Recent works have explored aligning images and videos with LLM for a new generation of multi-modal LLMs (e.g., Flamingo [14], BLIP-2 [30]) that equip LLMs with the ability to understand and reason about 2D images. However, as powerful as the models can be in communication and reasoning, they are not grounded in the real 3D physical world, which involves richer concepts such as spatial</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples from our generated 3D-language data, which covers multiple 3D-related tasks.
relationships, affordances, physics and interaction so on. Therefore, such LLMs pale in comparison with the robots depicted in sci-fi movies - the assistants that could understand the 3D environments, as well as perform reasoning and planning based on the 3D understandings.</p>
<p>To this end, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs that could take 3D representations (i.e., 3D point clouds with their features) as input, and perform a series of 3D-related tasks. By taking the 3D representations of scenes as input, LLMs are blessed with twofold advantages: (1) long-term memories about the entire scene can be stored in the holistic 3D representations, instead of episodic partial-view observations. (2) 3D properties such as affordances and spatial relationships can be reasoned from 3D representations, far beyond the scope of language-based or 2D image-based LLMs.</p>
<p>One major challenge of training the proposed 3D-LLMs lies in data acquisition. Unlike the vast amount of paired 2D-images-and-text data on the Internet, the scarcity of 3D data hinders the development of 3D-based foundation models. 3D data paired with language descriptions are even harder to obtain. To address this, we propose a set of unique data generation pipelines that could generate large-scale 3D data paired with language. Specifically, we make use of ChatGPT [33] and devise three efficient prompting procedures for communication between 3D data and language. In this way, we are able to obtain 300k 3D-language data covering a diverse set of tasks, including but not limited to 3D captioning, dense captioning, 3D question answering, 3D task decomposition, 3D grounding, 3D-assisted dialog, navigation and so on, as shown in Figure 1.</p>
<p>The next challenge resides in how to obtain meaningful 3D features that could align with language features for 3D-LLMs. One way is to train 3D encoders from scratch using a similar contrastive-learning paradigm for the alignment between 2D images and language (e.g., CLIP [37]). However, this paradigm consumes tremendous data, time, and GPU resources. From another perspective, there are numerous recent works that build 3D features from 2D multi-view images (e.g., concept fusion [26], 3D-CLR [20]). Inspired by this, we also utilize a 3D feature extractor that constructs 3D features from the 2D pretrained features of rendered multi-view images. Recently, there are also quite a few visual-language models (e.g., BLIP-2 [30], Flamingo [14]) utilizing the 2D pretrained CLIP features for training their VLMs. Since our extracted 3D features are mapped to the same feature space as 2D pretrained features, we can seamlessly use 2D VLMs as our backbones and input the 3D features for the efficient training of 3D-LLMs.</p>
<p>One crucial aspect of 3D-LLMs, different from vanilla LLMs and 2D VLMs, is that 3D-LLMs are expected to have a underlying 3D spatial sense of information. Thus, we develop a 3D localization mechanism that bridges the gap between language and spatial locations. Specifically, we append 3D position embeddings to the extracted 3D features to better encode spatial information. In addition, we append a series of location tokens to the 3D-LLMs, and localization can be trained via outputting location tokens given the language descriptions of specific objects in the scenes. In this way, 3D-LLMs could better capture 3D spatial information.</p>
<p>To sum up, our paper has the following contributions:</p>
<ul>
<li>We introduce a new family of 3D-based Large Language models (3D-LLMs) that can take 3D points with features and language prompts as input, and perform a variety of 3D-related tasks. We focus on tasks beyond the scope of vanilla LLMs or 2D-LLMs, such as tasks about holistic scene understanding, 3D spatial relationships, affordances and 3D planning.</li>
<li>We devise novel data collection pipelines that could generate large-scale 3D-language data. Based on the pipelines, we collect a dataset that has over 300k 3D-language data that cover a diverse set of 3D-related tasks, including but not limited to 3D captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on.</li>
<li>We use a 3D feature extractor that extracts meaningful 3D features from rendered multi-view images. We utilize 2D pretrained VLMs as our backbones for efficient training. We introduce a 3D localization mechanism for training the 3D-LLMs to better capture 3D spatial information.</li>
<li>Experiments on held-out evaluation dataset, ScanQA, outperform state-of-the-art baselines. In particular, 3D LLMs outperform baselines by a large margin on ScanQA (e.g., 9% for BLEU-1). Experiments on held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative studies further demonstrate that our model is able to handle a diverse set of tasks.</li>
<li>We plan to release our 3D-LLMs, the 3D-language dataset, and language-aligned 3D features of the dataset for future research development.</li>
</ul>
<h1>2 Related Works</h1>
<p>Large Language Models. Our work is closely related to large language models [4, 13, 38, 9, 34] (LLMs) like GPT-3 [4] and PaLM [9], which are able to handle different language tasks with a single model and show strong generalization abilities. These models are typically trained on massive textual data with self-supervised training targets like predicting the next tokens [4, 38] or reconstructing the masked tokens [13, 39]. To better align these LLMs' predictions to human instructions, improve the models' generalization abilities on unseen tasks, a series of instruction tuning methods [35, 44] and datasets [10, 12] have been proposed. In this work, we aim to inject the 3D world into large language models, understanding rich 3D concepts such as spatial relations, affordances, and physics.</p>
<p>Vision-Language Pre-trained Models. Our work is also related to vision-language pre-trained models that connect images and natural language [31, 32, 17, 37, 27]. Some research [37, 27] learn to train models from scratch with massive image-language pairs and apply them to downstream tasks like visual question answering [18, 51], captioning [7], and referring expression comprehension [50] with finetuning. Other researchers have connected pre-trained vision models and pre-trained LLMs with additional learnable neural modules like perceiver [2] and QFormers [31], leveraging perception abilities in pre-trained vision models, and reasoning and generalization capacities in LLMs. Inspired by these previous works, we plan to build an AI assistant that could understand the 3D world and perform corresponding 3D reasoning and planning. This is not trivial and we need to overcome</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: 3D-language data generation pipelines.
obstacles like how to handle the problem of data sparsity, how to align the 3D world with 2D images, and how to capture 3D spatial information.
3D \&amp; Language. Another line of research that is similar to ours is 3D and language [5, 49, 8, 20, 1, $15,24,49,3,21,19]$. ScanQA [49] requires a model to answer questions related to the 3D world; ScanRefer [5] asks a model to localize a region that the text expression refer to; 3D captioning [8] tests models' abilities to generate captions describing the 3D scenes. However, these 3D tasks and their corresponding models are usually task-specific and could only handle cases within the same distribution of the training sets without generalization. Different from them, we aim to build a 3D model that could handle different tasks at the same time and enable new abilities like 3D-assistant dialog and task decomposition.</p>
<h1>3 3D-Language Data Generation</h1>
<p>The community has witnessed the proliferation of multi-modal data thanks to easy access to a tremendous amount of 2D image and text pairs on the internet. However, when it comes to 3D-related data, obtaining multimodal resource is not easy, due to not only the scarcity of 3D assets, but also the difficulty of providing language data for 3D assets. There are some existing datasets that contain 3D-language data (e.g., ScanQA [49], ScanRefer [5]). However, they are limited with regard to both quantity and diversity, restricted to only one task per dataset. How to generate a 3D-language dataset that can be utilized for all kinds of 3D-related tasks is well worth delving into.</p>
<p>Inspired by the recent success of large language models like GPT [33], we propose to leverage such models for 3D-language data collection. Specifically, as shown in Figure 7, we have three ways to prompt a text-only GPT for generating data. 1) boxes-demonstration-instruction based prompting. We input the axis-aligned bounding boxes (AABB) of both the rooms and the objects in the 3D scenes, providing information about the semantics and spatial locations of the scene. We then provide specific instructions to the GPT model to generate diverse data. We give 0-3 few-shot demonstration examples of the GPT model showing what kind of data it is instructed to generate. 2) ChatCaptioner based prompting. We utilize techniques similar to [52], in which ChatGPT is prompted to ask a series</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overview of our 3D-LLM framework. The first two columns show our 3D feature extractor. We first render a few multi-view images from the 3D scene, extract 2D dense features, and then construct 3D features from these multi-view images using three kinds of methods. And then, the 3D features and input language prompts are input to the 3D-LLMs to generate responses. We also propose a 3D localization mechanism to better capture 3D spatial information.
of informative questions about an image and BLIP-2 [30] answers the questions. In order to collect 3D-related data, we input images from different views to BLIP-2, and ChatGPT is instructed to ask questions and collect information of different regions to form a global 3D description of the entire scene. 3) Revision based prompting. It can be used for transfer one type of 3D data to another,.
Given the prompting pipelines, GPT is able to generate various types of 3D-language data as summarized in Figure 1. We show detailed prompts to generate all types of data in the Appendix.
We mainly establish our 3D-language dataset upon several 3D assets:</p>
<ul>
<li>Objaverse is a universe of 800 K 3D objects. However, since the language descriptions were extracted from online sources and not examined by humans, most objects have very noisy descriptions (e.g., with urls) or no descriptions. We utilize ChatCaptioner based prompting to generate high-quality 3D-related descriptions for the scenes.</li>
<li>Scannet [11] is a richly-annotated dataset of approximately 1 k 3D indoor scenes. It provides semantics and bounding boxes of the objects in the scenes.</li>
<li>Habitat-Matterport (HM3D) [41] is a dataset of 3D environments of embodied AI. HM3DSem [47] further adds semantic annotations and bounding boxes for more than 200 scenes of HM3D.</li>
</ul>
<h1>4 3D-LLM</h1>
<h3>4.1 Overview</h3>
<p>In this section, we introduce how we train our 3D-LLMs. We argue that it's hard to train 3D-LLMs from scratch, since our collected 3D-language dataset is still not the size of billion-scale imagelanguage dataset used to train 2D VLMs. Furthermore, for 3D scenes, there are no available pretrained encoders like those for 2D images (e.g., CLIP ViT encoders). Thus, retraining 3D-language models from scratch is data-inefficient and resource-heavy. Recently, researchers have proposed to extract 3D features from 2D multi-view images [26, 20]. Using these alignment methods, we could use pretrained image encoders to extract image features, and then map the features to the 3D data. Since the pretrained image features serve as inputs to 2D VLMs, the mapped 3d features of the same feature space can also be seamlessly fed into the pretrained 2D VLMs, which we use as our backbones to train 3D-LLMs. We also propose a 3D localization mechanism to boost the model's ability to capture 3D spatial information. Figure 3 shows our framework.</p>
<h3>4.2 3D Feature Extractor</h3>
<p>The first step of training 3D-LLMs is to build meaningful 3D features that could be aligned with language features. For 2D images, there exist feature extractors like CLIP, which learn visual models from language supervision. The models are pretrained using billion-scale internet data of imagelanguage pairs. It's hard to pre-train such feature learners from scratch, since there are no 3D-language assets comparable to internet-scale image-language pairs in terms of quantity and diversity.</p>
<p>On the contrary, numerous methods have been proposed to extract 3D features from 2D multi-view images [26, 20, 16, 23]. Inspired by these works, we extract features for 3D points by rendering the 3D scenes in several different views, and construct 3D features from rendered image features.</p>
<p>We first extract pixel-aligned dense features for rendered images following [26]. Then, we utilize three methods to construct 3D features from rendered image features. These methods are designed for different types of 3D data.</p>
<ul>
<li>Direct Reconstruction. We directly reconstruct point cloud from rgbd images rendered from the 3D data using ground-truth camera matrixes. The features are directly mapped to the reconstructed 3D points. This method is suitable for rendered rgbd data with perfect camera poses and intrinsics.</li>
<li>Feature Fusion. Similar to [26], we fuse 2D features into 3D maps using gradslam [28]. Different from dense mapping methods, the features are fused in addition to depths and colors. This method is suitable for 3D data with noisy depth map renderings, or noisy camera poses and intrinsics.</li>
<li>Neural Field. We utilize [20], which constructs 3D compact representation using neural voxel field [43]. Specifically, each voxel in the field has a feature in addition to density and color. Then we align 3D features in the rays and 2D features in the pixels using MSE loss. This method is for 3D data with RGB renderings but no depth data, and noisy camera poses and intrinsics.</li>
</ul>
<p>In this way, we are able to obtain the $<N, \mathcal{D}_{v}>$-dim 3D features of each 3D scene, where $N$ is the number of points in the point cloud, and $\mathcal{D}_{v}$ is the feature dimension.</p>
<h1>4.3 Training 3D-LLMs</h1>
<h3>4.3.1 2D VLMs as backbones</h3>
<p>In addition to the feature extractor, training 3D-LLMs from scratch is also non-trivial. In fact, according to [30, 14], the training of 2D VLMs only begins to show "signs of life" after consuming half a billion images. They usually use frozen and pre-trained image encoders such as CLIP to extract features for 2D images. Considering that with 3D feature extractor, the 3D features can be mapped into the same feature space as 2D images, it's reasonable to use these 2D VLMs as our backbones.
The perceiver architecture proposed by [25] leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to handle very large inputs of arbitrary input sizes, thus can tackle different modalities. This architecture is utilized in VLMs like Flamingo [14]. BLIP-2 [30] also utilizes a similar structure called QFormer. The 2D image features, output from frozen image encoders, are flattened and sent to the perceiver to generate a fixed-sized input. Given that our 3D features are in the same feature space as the 2D features by the 3D feature extractor, and that perceiver is able to handle inputs of arbitrary input sizes of the same feature dimension, point cloud features with arbitrary sizes could also be fed into the perceiver. Therefore, we use the 3D feature extractor to extract the 3D features in the same feature space as the features of the frozen image encoders. Then, we use pretrained 2D VLMs as our backbones, input the aligned 3D features to train 3D-LLMs with our collected 3D-language dataset.</p>
<h3>4.3.2 3D Localization Mechanism</h3>
<p>Apart from building 3D features, which can be aligned with language semantics, it's also essential to capture 3D spatial information. To this end, we propose a 3D localization mechanism that boosts 3D LLMs' abilities to absorb spatial information. It consists of two parts:
Augmenting 3D features with position embeddings Besides the 3D features aggregated from 2D multi-view features, we also add position embeddings to the features. Suppose the feature dim is $\mathcal{D}<em v="v">{v}$. We generate $\sin / \cos$ position embeddings of the three dimensions, each has an embedding size $\mathcal{D}</em> / 3$. We concatenate the embeddings of all three dimensions, and concatenate them to the 3D features.
Augmenting LLM vocabularies with location tokens In order to align 3D spatial locations with LLMs, we propose to embed 3D locations in the vocabularies, following [6] and [45]. To be specific, the region to be grounded can be denoted as a sequence of discrete tokens representing the bounding box in the form of AABB. The continuous corner coordinates of the bounding boxes are uniformly discretized to voxel integers as location tokens $\left\langle x_{\text {min }}, y_{\text {min }}, z_{\text {min }}, x_{\text {max }}, y_{\text {max }}, z_{\text {max }}\right\rangle$. After adding these additional location tokens, we unfreeze the weights for these tokens in the input and output embeddings of language models.</p>
<p>5 Experiments</p>
<p>We first introduce the architecture, and training and evaluation protocols. In Sec 5.1, we analyze the held-out experiments on ScanQA Dataset. Sec 5.2 covers more analysis on held-in evaluation and qualitative examples. Due to page limit, we put the following content into Appendix: 1) Held-Out Experiments on 3DMV-VQA and object navigation; 2) Held-In Experiments on grounding and dense captioning; 3) More ablative studies; 4) More qualitative examples.</p>
<p>Architecture We experiment on three backbone 2D VLMs for 3D-LLMs: Flamingo 9B, BLIP-2 Vit-g Opt2.7B, BLIP-2 Vit-g FlanT5-XL. For BLIP-2, during pre-training the 3D-LLMs, we initialize the model from BLIP-2 checkpoints released in LAVIS library [29], and finetune the parameters for the QFormer. 3D features are 1408-dim features, same as EVA_CLIP hidden feature dim used by BLIP-2. We keep most parts of the LLMs (i.e., Opt and FlanT5) frozen, except the weights for the newly-added location tokens in the input and the output embeddings. For Flamingo, we initialize the model from the Flamingo9B checkpoint released in OpenFlamingo repository [2]. We finetune the parameters for perceiver, gated cross attention layers, and the weights for additional location tokens in the input and output embeddings. 3D features are 1024-dim features, same as CLIP hidden feature dim used by Flamingo.</p>
<p>Training &amp; Evaluation Datasets &amp; Protocols We split our datasets into two genres, held-in datasets and held-out datasets. Specifically, our 3D-language data generation pipeline generates the held-in datasets of multiple tasks. we split the datasets into train/val/test sets (8:1:1). We utilize training sets of held-in datasets for pre-training foundation 3D-LLMs, and their validation and test sets can be applied for held-in evaluation. During pre-training, we mix the held-in datasets of all tasks. The models are trained with the standard language modeling loss to output responses. Held-out datasets, on the other hand, are not used in training the foundation 3D-LLMs. We use two held-out 3D question answering datasets for held-out evaluation: ScanQA and 3DMV-VQA. We put the analysis of experiments of 3DMV-VQA [20] in the supplementary material.</p>
<h3>5.1 Held-Out Evaluation</h3>
<p>We finetune our pretrained 3D-LLMs on the ScanQA dataset and compare with baseline models.</p>
<p>Baselines &amp; Evaluation Metrics We include representative baseline models on the benchmark. Particularly, ScanQA is the state-of-the-art method on the benchmark that uses VoteNet to obtain object proposals, and then fuse them with language embeddings. ScanRefer+MCAN is a baseline that identifies the referred object and the MCAN model is applied to the image surrounding the localized object. VoteNet+MCAN detects objects in a 3D space, extracts their features, and uses them in a standard VQA model. Notably, these baseline models all extract explicit object representations from a pretrained localization module. In addition to these baselines, we also design several LLM-based baselines. LLaVA is a visual instruction tuning that connects a vision encoder and LLM for general-purpose visual and language understanding. We use its pretrained model and do zero-shot evaluation on our dataset. We use a single random image as input. We use LLaVA 13B model. Single Image + Pretrained VLMs use our 2D VLM backbones (i.e., flamingo and BLIP-2), replace the 3D inputs of 3D-LLMs with single image features to train the models, and then finetune on ScanQA dataset. Multi-View Image + Pretrained VLMs use our 2D VLM backbones, replace the 3D inputs of 3D-LLMs with concatenated features of multi-view images to train the models, and then finetune on ScanQA dataset. We report BLEU, ROUGE-L, METEOR, CIDEr for robust answer matching. We also use exact match (EM) metric.</p>
<p>Result Analysis We report our results on ScanQA validation set in Table 1, and results on test set in Table 2. We observe a significant increase in the evaluation metrics. For example, for BLEU-1, our model outperforms the state-of-the-art ScanQA model by $\sim 9\%$ for validation set and $\sim 7\%$ for test set. For CIDER, we report a $\sim 5\%$ gain compared to ScanQA, and much higher than other 3D-based baselines. These results show that by injecting 3D into LLMs, the models can generate answers that are much more similar to the ground-truth answers. Furthermore, 3D-based baselines use object detectors like VoteNet to segment the objects, and then send per-object features into their models, while our inputs are holistic 3D features without explicit object representations. This shows that our model could perform visual reasoning about objects and their relationships even without explicit object representations. We then examine whether 2D VLMs have the same ability. We find that by taking single-view images or multi-view images as inputs, the performances drop much compared to 3D-LLMs. Specifically, multi-view images also contain information about the whole scene. However,</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">B-1</th>
<th style="text-align: left;">B-2</th>
<th style="text-align: left;">B-3</th>
<th style="text-align: left;">B-4</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">ROUHE-L</th>
<th style="text-align: left;">CIDER</th>
<th style="text-align: right;">EM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VoteNet+MCAN*</td>
<td style="text-align: left;">28.0</td>
<td style="text-align: left;">16.7</td>
<td style="text-align: left;">10.8</td>
<td style="text-align: left;">6.2</td>
<td style="text-align: left;">11.4</td>
<td style="text-align: left;">29.8</td>
<td style="text-align: left;">54.7</td>
<td style="text-align: right;">17.3</td>
</tr>
<tr>
<td style="text-align: left;">ScanRefer+MCAN*</td>
<td style="text-align: left;">26.9</td>
<td style="text-align: left;">16.6</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">7.9</td>
<td style="text-align: left;">11.5</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">55.4</td>
<td style="text-align: right;">18.6</td>
</tr>
<tr>
<td style="text-align: left;">ScanQA*</td>
<td style="text-align: left;">30.2</td>
<td style="text-align: left;">20.4</td>
<td style="text-align: left;">15.1</td>
<td style="text-align: left;">10.1</td>
<td style="text-align: left;">13.1</td>
<td style="text-align: left;">33.3</td>
<td style="text-align: left;">64.9</td>
<td style="text-align: right;">$\mathbf{2 1 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA(zero-shot)</td>
<td style="text-align: left;">7.1</td>
<td style="text-align: left;">2.6</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">0.3</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">12.3</td>
<td style="text-align: left;">5.7</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">flamingo-SingleImage</td>
<td style="text-align: left;">23.8</td>
<td style="text-align: left;">14.5</td>
<td style="text-align: left;">9.2</td>
<td style="text-align: left;">8.5</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">29.6</td>
<td style="text-align: left;">52</td>
<td style="text-align: right;">16.9</td>
</tr>
<tr>
<td style="text-align: left;">flamingo-MultiView</td>
<td style="text-align: left;">25.6</td>
<td style="text-align: left;">15.2</td>
<td style="text-align: left;">9.2</td>
<td style="text-align: left;">8.4</td>
<td style="text-align: left;">11.3</td>
<td style="text-align: left;">31.1</td>
<td style="text-align: left;">55</td>
<td style="text-align: right;">18.8</td>
</tr>
<tr>
<td style="text-align: left;">BLIP2-flant5-SingleImage</td>
<td style="text-align: left;">28.6</td>
<td style="text-align: left;">15.1</td>
<td style="text-align: left;">9.0</td>
<td style="text-align: left;">5.1</td>
<td style="text-align: left;">10.6</td>
<td style="text-align: left;">25.8</td>
<td style="text-align: left;">42.6</td>
<td style="text-align: right;">13.3</td>
</tr>
<tr>
<td style="text-align: left;">BLIP2-flant5-MultiView</td>
<td style="text-align: left;">29.7</td>
<td style="text-align: left;">16.2</td>
<td style="text-align: left;">9.8</td>
<td style="text-align: left;">5.9</td>
<td style="text-align: left;">11.3</td>
<td style="text-align: left;">26.6</td>
<td style="text-align: left;">45.7</td>
<td style="text-align: right;">13.6</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (flamingo)</td>
<td style="text-align: left;">30.3</td>
<td style="text-align: left;">17.8</td>
<td style="text-align: left;">12.0</td>
<td style="text-align: left;">7.2</td>
<td style="text-align: left;">12.2</td>
<td style="text-align: left;">32.3</td>
<td style="text-align: left;">59.2</td>
<td style="text-align: right;">20.4</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (BLIP2-opt)</td>
<td style="text-align: left;">35.9</td>
<td style="text-align: left;">22.5</td>
<td style="text-align: left;">16.0</td>
<td style="text-align: left;">9.4</td>
<td style="text-align: left;">13.8</td>
<td style="text-align: left;">34.0</td>
<td style="text-align: left;">63.8</td>
<td style="text-align: right;">19.3</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (BLIP2-flant5)</td>
<td style="text-align: left;">$\mathbf{3 9 . 3}$</td>
<td style="text-align: left;">$\mathbf{2 5 . 2}$</td>
<td style="text-align: left;">$\mathbf{1 8 . 4}$</td>
<td style="text-align: left;">$\mathbf{1 2 . 0}$</td>
<td style="text-align: left;">$\mathbf{1 4 . 5}$</td>
<td style="text-align: left;">$\mathbf{3 5 . 7}$</td>
<td style="text-align: left;">$\mathbf{6 9 . 4}$</td>
<td style="text-align: right;">20.5</td>
</tr>
</tbody>
</table>
<p>Table 1: Experimental results on ScanQA validation set. * Means the models use explicit object representations. B-1, B-2, B-3, B-4 denote BLEU-1, BLEU-2, BLEU-3, BLEU-4 respectively. Our model outperforms all baseline models for all evaluation metrics except for the EM metric.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">BLEU-1</th>
<th style="text-align: left;">BLEU-4</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">ROUHE-L</th>
<th style="text-align: left;">CIDER</th>
<th style="text-align: left;">EM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SingleImage+MCAN</td>
<td style="text-align: left;">16.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">8.4</td>
<td style="text-align: left;">21.5</td>
<td style="text-align: left;">38.6</td>
<td style="text-align: left;">15.8</td>
</tr>
<tr>
<td style="text-align: left;">VoteNet+MCAN*</td>
<td style="text-align: left;">29.5</td>
<td style="text-align: left;">6.0</td>
<td style="text-align: left;">12.0</td>
<td style="text-align: left;">30.9</td>
<td style="text-align: left;">58.2</td>
<td style="text-align: left;">19.7</td>
</tr>
<tr>
<td style="text-align: left;">ScanRefer+MCAN*</td>
<td style="text-align: left;">27.9</td>
<td style="text-align: left;">7.5</td>
<td style="text-align: left;">11.9</td>
<td style="text-align: left;">30.7</td>
<td style="text-align: left;">57.4</td>
<td style="text-align: left;">20.6</td>
</tr>
<tr>
<td style="text-align: left;">ScanQA*</td>
<td style="text-align: left;">31.6</td>
<td style="text-align: left;">$\mathbf{1 2 . 0}$</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">34.3</td>
<td style="text-align: left;">67.3</td>
<td style="text-align: left;">$\mathbf{2 3 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (flamingo)</td>
<td style="text-align: left;">32.6</td>
<td style="text-align: left;">8.4</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">34.8</td>
<td style="text-align: left;">65.6</td>
<td style="text-align: left;">23.2</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (BLIP2-opt)</td>
<td style="text-align: left;">37.3</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">14.3</td>
<td style="text-align: left;">34.5</td>
<td style="text-align: left;">67.1</td>
<td style="text-align: left;">19.1</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (BLIP2-flant5)</td>
<td style="text-align: left;">$\mathbf{3 8 . 3}$</td>
<td style="text-align: left;">11.6</td>
<td style="text-align: left;">$\mathbf{1 4 . 9}$</td>
<td style="text-align: left;">$\mathbf{3 5 . 3}$</td>
<td style="text-align: left;">$\mathbf{6 9 . 6}$</td>
<td style="text-align: left;">19.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results on ScanQA test set. * Means the models use explicit object representations. Our model outperforms all baseline models for most of the evaluation metrics.
they have much lower performances compared to 3D-LLMs, probably because features of multi-view images are disorganized, thus losing 3D-related information.</p>
<h1>5.2 More Extensive Evaluation</h1>
<p>Held-In Evaluation We carry out experiments on held-in datasets of three tasks: 3D captioning, 3D-assited dialog and task decomposition. The baselines include 2D VLMs as for the held-out evaluation. We add one language-only baseline: FlanT5, which examines LLMs' ability to complete these tasks without any visual input. To evaluate the quality of responses, we include BLEU, ROUGEL, METEOR, CIDEr as our metrics. We report the held-in evaluation performances in Table 3. From the table, we could see that 3D-LLMs could generate high-quality responses, outperforming both 2D VLMs and language-only LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">BLEU-1</th>
<th style="text-align: center;">BLEU-2</th>
<th style="text-align: center;">BLEU-3</th>
<th style="text-align: center;">BLEU-4</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">ROUGH-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3D Captioning</td>
<td style="text-align: center;">flamingo-SingleImage</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flamingo-MultiView</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLIP2-flant5-SingleImage</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLIP2-flant5-MultiView</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">35.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (flamingo)</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (BLIP2-opt)</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">40.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (BLIP2-t5)</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">42.6</td>
</tr>
<tr>
<td style="text-align: center;">3D-assisted Dialog</td>
<td style="text-align: center;">flant5</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flamingo-SingleImage</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">26.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flamingo-MultiView</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">27.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLIP2-flant5-SingleImage</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLIP2-flant5-MultiView</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">29.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (flamingo)</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">34.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (BLIP2-opt)</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">38.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (BLIP2-flant5)</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">39.3</td>
</tr>
<tr>
<td style="text-align: center;">Task Decomposition</td>
<td style="text-align: center;">flant5</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">28.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flamingo-SingleImage</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">30.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">flamingo-MultiView</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLIP2-flant5-SingleImage</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">31.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLIP2-flant5-MultiView</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">34.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (flamingo)</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (BLIP2-opt)</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">35.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3D-LLM (BLIP2-flant5)</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">37.8</td>
</tr>
</tbody>
</table>
<p>Table 3: Experimental Results on Held-In Datasets. 3D-LLMs outperform 2D VLMs.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative examples of 3D-LLM's prediction.</p>
<p>Qualitative Examples In Figure 4, we show qualitative examples of 3D-LLM's predictions. We can see that our 3D-LLM is able to perform a variety of tasks.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we propose a new family of 3D-LLMs that can take 3D representations as inputs and generate responses. We introduce a series of 3D-language data generation pipelines to generate a dataset of 300K 3D-language pairs to train our 3D-LLMs, including dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Our 3D-LLMs leverage 2D pretrained VLMs as backbones and a novel 3D localization mechanism. Experiments show that our 3D-LLMs outperform state-of-the-art baseline models on ScanQA datasets, and could perform a diverse set of 3D-related tasks. A limitation is that the 3D feature extractor relies on 2D multi-view images, and thus all 3D scenes need to be rendered so that they can be trained in 3D-LLMs, which introduces an additional rendering process.</p>
<h1>References</h1>
<p>[1] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. J. Guibas. ReferIt3D: Neural listeners for fine-grained 3D object identification in real-world scenes. In ECCV, 2020.
[2] A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre, J. Jitsev, S. Kornblith, P. W. Koh, G. Ilharco, M. Wortsman, and L. Schmidt. Openflamingo, Mar. 2023.
[3] D. Azuma, T. Miyanishi, S. Kurita, and M. Kawanabe. ScanQA: 3D question answering for spatial scene understanding. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19107-19117, 2022.
[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, pages 1877-1901, 2020.
[5] D. Z. Chen, A. X. Chang, and M. Nießner. ScanRefer: 3D object localization in RGB-D scans using natural language. 16th European Conference on Computer Vision (ECCV), 2020.
[6] T. Chen, S. Saxena, L. Li, D. J. Fleet, and G. E. Hinton. Pix2seq: A language modeling framework for object detection. ArXiv, abs/2109.10852, 2021.
[7] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.
[8] Z. Chen, A. Gholami, M. Nießner, and A. X. Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3193-3203, 2021.
[9] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[10] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[11] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nießner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2432-2443, 2017.
[12] Databricks. Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023.
[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[14] J.-B. A. et al. Flamingo: a visual language model for few-shot learning. 2022.
[15] M. Feng, Z. Li, Q. Li, L. Zhang, X. Zhang, G. Zhu, H. Zhang, Y. Wang, and A. S. Mian. Free-form description guided 3d visual graph network for object grounding in point cloud. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3702-3711, 2021.
[16] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. CLIP on wheels: Zero-shot object navigation as object localization and exploration. ArXiv, abs/2203.10421, 2022.
[17] T. Gong, C. Lyu, S. Zhang, Y. Wang, M. Zheng, Q. Zhao, K. Liu, W. Zhang, P. Luo, and K. Chen. MultiModal-GPT: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.
[18] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
[19] Y. Hong, Y. Du, C. Lin, J. Tenenbaum, and C. Gan. 3d concept grounding on neural fields. Advances in Neural Information Processing Systems, 35:7769-7782, 2022.
[20] Y. Hong, C. Lin, Y. Du, Z. Chen, J. B. Tenenbaum, and C. Gan. 3D concept learning and reasoning from multi-view images, 2023.</p>
<p>[21] Y. Hong, C. Lin, Y. Du, Z. Chen, J. B. Tenenbaum, and C. Gan. 3d concept learning and reasoning from multi-view images. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.
[22] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell. Natural language object retrieval, 2016.
[23] C. Huang, O. Mees, A. Zeng, and W. Burgard. Visual language maps for robot navigation, 2023.
[24] P.-H. Huang, H.-H. Lee, H.-T. Chen, and T.-L. Liu. Text-guided graph neural networks for referring 3D instance segmentation. In AAAI, 2021.
[25] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira. Perceiver: General perception with iterative attention. In International Conference on Machine Learning, 2021.
[26] K. M. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, S. Li, G. Iyer, S. Saryazdi, N. Keetha, A. Tewari, J. B. Tenenbaum, C. M. de Melo, M. Krishna, L. Paull, F. Shkurti, and A. Torralba. Conceptfusion: Open-set multimodal 3D mapping, 2023.
[27] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021.
[28] J. Krishna Murthy, S. Saryazdi, G. Iyer, and L. Paull. gradslam: Dense slam meets automatic differentiation. arXiv, 2020.
[29] D. Li, J. Li, H. Le, G. Wang, S. Savarese, and S. C. H. Hoi. LAVIS: A library for language-vision intelligence, 2022.
[30] J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.
[31] J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.
[32] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.
[33] OpenAI. GPT-4 technical report, 2023.
[34] OpenAI. GPT-4 technical report. ArXiv, abs/2303.08774, 2023.
[35] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[36] C. Qi, O. Litany, K. He, and L. J. Guibas. Deep hough voting for 3d object detection in point clouds. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9276-9285, 2019.
[37] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.
[38] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[39] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.
[40] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv preprint arXiv:2109.08238, 2021.
[41] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao, and D. Batra. Habitat-matterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.
[42] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, et al. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9339-9347, 2019.</p>
<p>[43] C. Sun, M. Sun, and H.-T. Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5459-5469, June 2022.
[44] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv e-prints, pages arXiv-2305, 2023.
[45] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, 2022.
[46] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. arXiv preprint arXiv:1911.00357, 2019.
[47] K. Yadav, R. Ramrakhya, S. K. Ramakrishnan, T. Gervet, J. Turner, A. Gokaslan, N. Maestre, A. X. Chang, D. Batra, M. Savva, et al. Habitat-matterport 3D semantics dataset. arXiv preprint arXiv:2210.05633, 2022.
[48] Z. Yang, B. Gong, L. Wang, W. Huang, D. Yu, and J. Luo. A fast and accurate one-stage approach to visual grounding, 2019.
[49] S. Ye, D. Chen, S. Han, and J. Liao. 3D question answering. IEEE transactions on visualization and computer graphics, PP, 2021.
[50] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context in referring expressions. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69-85. Springer, 2016.
[51] P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and D. Parikh. Yin and Yang: Balancing and answering binary visual questions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[52] D. Zhu, J. Chen, K. Haydarov, X. Shen, W. Zhang, and M. Elhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions, 2023.</p>
<h1>A 3D-Language Data</h1>
<h2>A. 1 Prompts</h2>
<p>In figure 5 and 6, we show two exemplar prompts for generating task decomposition data and 3D-assisted dialog data. Specifically, they are generated using the boxes-demonstration-instruction method described in the paper. For each sample in the few shot samples, the "content" has the bounding boxes of the scenes, and the "response" refers to human-written responses for demonstration. For query, it consists of the bounding boxes of scenes that we query the ChatGPT to give us responses.</p>
<div class="codehilite"><pre><span></span><code>messages=[(&quot;role&quot;: &quot;system&quot;, &quot;content&quot; &quot;You are an A]
visual assistant that can analyze a 3D scene. All object instances in this 3D
scene are given, along with their center point position. The center points
are represented by a 3D coordinate (x, y, z) with units of meters. Using the
provided object instance information, design a high-level task that can be
performed in this 3D scene. Besides, decomposing this high-level task into
a sequence of action steps that can be performed using the instances in this
3D scene. With Remenber, the high-level task and action steps must be able
to be performed in the 3D scene using the given object instances.
&quot;!]
for sample in fewshot_samples:
    messages.append((&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
sample[&#39;content&#39;]))
    messages.append((&quot;role&quot;: &quot;assistant&quot;,
&quot;content&quot;: sample[&#39;response&#39;]))
messages.append((&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
&quot;in&quot;.join(query)))
</code></pre></div>

<p>Figure 5: Prompts on generating task decomposition data</p>
<div class="codehilite"><pre><span></span><code>messages=[(&quot;role&quot;: &quot;system&quot;, &quot;content&quot; &quot;You are a
</code></pre></div>

<p>conversation generator in a room. All object instances in this room are given, along with their center point position. The center points are represent by a 3D coordinate ( $\mathrm{x}, \mathrm{y}, \mathrm{z}$ ) with units of meters. You need to generate $4-10$ round conversation between a human and a robot assistant. 'n'nThe human know all information in this room, including all objects described above and all small things that are not visible now. The human will ask the robot to do a high-level task. The robot will tell its observation and its state (e.g., location) to the human and will ask for help when it is ambiguous about the task. Remenber, the high-level task should be done in this room. "]</p>
<div class="codehilite"><pre><span></span><code>for sample in fewshot_samples:
    messages.append((&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
sample[&#39;content&#39;]))
    messages.append((&quot;role&quot;: &quot;assistant&quot;,
&quot;content&quot;: sample[&#39;response&#39;]))
messages.append((&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
&quot;in&quot;.join(query)))
</code></pre></div>

<p>Figure 6: Prompts on generating 3D-assisted dialog data</p>
<h2>A. 2 Data Distribution</h2>
<p>In figure 7, we show the distribution of our data.</p>
<h2>B Experiments</h2>
<h2>B. 1 Implementation Details</h2>
<p>Using Pretrained BLIP-2 as backbones, we train 3D-LLMs for 100 K steps, and validate every 1 K step. We run the models on 8 nodes, where each node has 8 V100s. The batch size is 16 for each node. The AdamW optimizer is used, with $\beta_{1}=0.9, \beta_{2}=0.999$, and a weight decay of 0.05 . Additionally, we apply a linear warmup of the learning rate during the initial 1 K steps, increasing from $10^{-8}$ to $10^{-5}$, followed by a cosine decay with a minimum learning rate of 0 .
3D-LLMs based on pretrained flamingo are trained using the AdamW optimizer with global norm clipping of 1 , no weight decay for the perceiver resampler and weight decay of 0.1 for the other</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Data distribution on different types for our 3D-language data
trainable parameters. The learning rate is increased linearly from 0 to $10^{-4}$ up over the first 5000 steps then held constant for the duration of training. The model is trained on 8 A100s. The batch size is 16 . We use Distributed Data Parallel (DDP) to train the model.</p>
<h1>B. 2 Held-Out Evaluation</h1>
<h2>B.2.1 3DMV-VQA</h2>
<p>We finetune our pretrained 3D-LLMs on the 3DMV-VQA dataset and compare with baseline models.
Baselines \&amp; Evaluation Metrics We include representative baseline models on the benchmark. NS-VQA is the neuro-symbolic method that first extracts object proposals and then perform neurosymoblic reasoning. 3D-Feature + LSTM is a baseline that extracts 3D features first and then concatenates with LSTM to output final answers. 3D-CLR is the state-of-the-art method that extracts 3D feature fields first, and then perform neuro-symbolic reasoning. In addition to these baselines, we also add 2D VLMs baselines like we did for ScanQA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Concept</th>
<th style="text-align: center;">Counting</th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">Comparison</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NS-VQA*</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: left;">3D-Feature+LSTM</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">48.2</td>
</tr>
<tr>
<td style="text-align: left;">3D-CLR*</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">$\mathbf{4 1 . 3}$</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">$\mathbf{7 2 . 3}$</td>
<td style="text-align: center;">57.7</td>
</tr>
<tr>
<td style="text-align: left;">flamingo-SingleImage</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">40.3</td>
</tr>
<tr>
<td style="text-align: left;">flamingo-MultiView</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">41.6</td>
</tr>
<tr>
<td style="text-align: left;">BLIP-SingleImage</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">43.1</td>
</tr>
<tr>
<td style="text-align: left;">BLIP-MultiView</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">47.1</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (flamingo)</td>
<td style="text-align: center;">$\mathbf{6 8 . 9}$</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">$\mathbf{6 1 . 6}$</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">$\mathbf{5 8 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (BLIP5-opt)</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">54.9</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (BLIP2-flanT5)</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">54.6</td>
</tr>
</tbody>
</table>
<p>Table 4: Experimental results on 3DMV-VQA dataset. * denotes using explicit object representations and neuro-symbolic reasoning.</p>
<p>Result Analysis Table 4 shows the performances on 3DMV-VQA. We can see that 3D-LLMs outperform state-of-the-art baseline model in the question types of concept and relation, and also in the overall performance. Our model also outperforms 3D-Feature+LSTM, demonstrating the power of LLMs over vanilla language models with similar 3D features as inputs. Overall, 3D-based methods outshine 2D-based versions of the methods. Our 3D-LLMs outperform their corresponding 2D VLMs with image input, further demonstrating the importance of 3D representations for 3D-LLMs.</p>
<h1>B.2.2 3D Grounding (Referring) on ScanRefer</h1>
<p>In order to examine 3D-LLMs' 3D localization abilities, we carry out a held-out experiment on ScanRefer benchmark. Specifically, ScanRefer benchmark requires the models to output object locations given a referring sentence of the objects. We finetune 3D-LLMs on ScanRefer training set and report the results on ScanRefer validation sets.
Baselines We include the baseline models in the original ScanRefer paper. Specifically, OCRand(OracleCatRand) use an oracle with ground truth bounding boxes of objects, and predict the box by simply selecting a random box that matches the object category. Vote+Rand(VoteNetRand) uses the predicted object proposals of the VoteNet [36] backbone and selects a box randomly with the correct semantic class label. SCRC \&amp; One-stage are 2D image baselines for referring expression comprehension by extending SCRC [22] and One-stage [48] to 3D using back-projection. Since 2D referring expression methods operate on a single image frame, we construct a 2D training set by using the recorded camera pose associated with each annotation to retrieve the frame from the scan video with the closest camera pose. At inference time, we sample frames from the scans (using every 20th frame) and predict the target 2D bounding boxes in each frame. We then select the 2D bounding box with the highest confidence score from the bounding box candidates and project it to 3D using the depth map for that frame. ScanRefer uses a pretrained VoteNet backbone with a trained GRU for selecting a matching bounding box.
Evaluation Metrics To evaluate the performance of our method, we measure the thresholded accuracy where the positive predictions have higher intersection over union (IoU) with the ground truths than the thresholds. Similar to work with 2D images, we use ACC@kIoU as our metric, where the threshold value k for IoU is set to 0.25 . In addition to that, we also report the Average IoUs of our 3D-LLMs. Since our model focuses on 3D localization of objects, we also report the distances from the centers of predicted bounding boxes to the ground-truth bounding boxes.
Result Analysis In Table 5, we show the results on ScanRefer. As we can see, our 3D-LLMs can have decent performances on grounding and referring, and outperform most of the baselines, showing that 3D-LLMs have the ability of 3D localization. Notably, the baseline models use ground-truth bounding boxes, or a pre-trained object detector to propose bounding boxes and classes for object proposals. Then, they use scoring modules to vote for the most likely candidate. Our method does not use any explicit object proposal module or ground truth bounding boxes, but outputs the locations of the bounding boxes directly using LLM losses for predicting tokens, while still outperforming most of the baselines. We could also see from the Avg. Dist metric the bounding boxes we predict is very close to the ground-truth bounding boxes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">OCRand</th>
<th style="text-align: center;">Vote+Rand</th>
<th style="text-align: center;">SCRC</th>
<th style="text-align: center;">One-stage</th>
<th style="text-align: center;">ScanRefer</th>
<th style="text-align: center;">3D-LLM <br> (flamingo)</th>
<th style="text-align: center;">3D-LLM <br> (BLIP2-opt)</th>
<th style="text-align: center;">3D-LLM <br> (BLIP2-flant5)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ACC@0.25</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">30.3</td>
</tr>
<tr>
<td style="text-align: left;">Avg. IoU</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">24.9</td>
</tr>
<tr>
<td style="text-align: left;">Avg. Dist.</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">1.03</td>
</tr>
</tbody>
</table>
<p>Table 5: Experimental Results on ScanRefer</p>
<h2>B.2.3 Object Navigation</h2>
<p>We show the ability of our 3D-LLM to progressively understand the environment and navigate to a target object. We formulate the navigation process as a conversation. At each time step, we online build a 3D feature from the partially observed scene. We feed this feature, current agent location, and history location to the 3D-LLM for predicting a 3D waypoint the agent should go for. We then use an off-the-shelf local policy [46] to determine a low-level action (e.g., go forward, turn left or right) for navigating to the waypoint. The 3D-LLM predicts "stop" if it believes the agent has reached the target object.</p>
<p>In Figure 8, we visualize a conversation process and its corresponding navigation trajectory. At the beginning when the target object is not observed, the 3D-LLM predicts a waypoint that leads the agent to explore the area most likely containing the target object. When the agent observes the target object (i.e., red box in the partially observed scene), the 3D-LLM predicts a waypoint leading the agent to it. The example episode is performed on the HM3D dataset [40] using Habitat simulator [42].</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Visualization of an object navigation episode.</p>
<h1>B. 3 Held-In Evaluation</h1>
<h2>B.3.1 3D Dense Captioning</h2>
<p>In Table 6, we show the results of 3D dense captioning. Specifically, given a 3D bounding box, models are expected to output the caption describing what's in that region. We can see that our 3D-LLMs outperform image-based baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">BLEU-1</th>
<th style="text-align: left;">BLEU-2</th>
<th style="text-align: left;">BLEU-3</th>
<th style="text-align: left;">BLEU4</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">ROUGH-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">flamingo-SingleImage</td>
<td style="text-align: left;">21.5</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">6.9</td>
<td style="text-align: left;">4.1</td>
<td style="text-align: left;">11.1</td>
<td style="text-align: left;">23.4</td>
</tr>
<tr>
<td style="text-align: left;">flamingo-MultiView</td>
<td style="text-align: left;">24.4</td>
<td style="text-align: left;">12.3</td>
<td style="text-align: left;">7.1</td>
<td style="text-align: left;">4.6</td>
<td style="text-align: left;">11.9</td>
<td style="text-align: left;">25.8</td>
</tr>
<tr>
<td style="text-align: left;">BLIP-SingleImage</td>
<td style="text-align: left;">23.0</td>
<td style="text-align: left;">11.7</td>
<td style="text-align: left;">7.7</td>
<td style="text-align: left;">4.6</td>
<td style="text-align: left;">11.3</td>
<td style="text-align: left;">23.8</td>
</tr>
<tr>
<td style="text-align: left;">BLIP-MultiView</td>
<td style="text-align: left;">25.3</td>
<td style="text-align: left;">14.1</td>
<td style="text-align: left;">9.0</td>
<td style="text-align: left;">5.6</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">24.9</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (flamingo)</td>
<td style="text-align: left;">29.6</td>
<td style="text-align: left;">16.8</td>
<td style="text-align: left;">10.6</td>
<td style="text-align: left;">5.9</td>
<td style="text-align: left;">11.4</td>
<td style="text-align: left;">29.9</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (BLIP2-opt)</td>
<td style="text-align: left;">32.5</td>
<td style="text-align: left;">18.7</td>
<td style="text-align: left;">11.9</td>
<td style="text-align: left;">6.5</td>
<td style="text-align: left;">11.7</td>
<td style="text-align: left;">31.5</td>
</tr>
<tr>
<td style="text-align: left;">3D-LLM (BLIP2-flant5)</td>
<td style="text-align: left;">34.3</td>
<td style="text-align: left;">20.5</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">8.1</td>
<td style="text-align: left;">13.1</td>
<td style="text-align: left;">33.2</td>
</tr>
</tbody>
</table>
<p>Table 6: Experimental Results on Held-In 3D Dense Captioning Dataset.</p>
<h2>B. 4 More Ablative Studies</h2>
<h2>B.4.1 Ablative Studies on Flamingo Perceiver</h2>
<p>We first examine how the perceiver resampler of Flamingo benefits the training. We carry out an ablative experiment where we take out the perceiver of the flamingo model. Table 7 shows the results. From the table, we can see that the perceiver module is indeed beneficial for the training of 3D-LLM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">BLEU-1</th>
<th style="text-align: left;">BLEU-2</th>
<th style="text-align: left;">BLEU-3</th>
<th style="text-align: left;">BLEU-4</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">ROUGH_L</th>
<th style="text-align: left;">CIDER</th>
<th style="text-align: left;">EM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">wo/ perceiver</td>
<td style="text-align: left;">29.2</td>
<td style="text-align: left;">17.2</td>
<td style="text-align: left;">11.2</td>
<td style="text-align: left;">7.4</td>
<td style="text-align: left;">11.4</td>
<td style="text-align: left;">30.4</td>
<td style="text-align: left;">58.9</td>
<td style="text-align: left;">20.6</td>
</tr>
<tr>
<td style="text-align: left;">w/ perceiver</td>
<td style="text-align: left;">30.3</td>
<td style="text-align: left;">17.8</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">7.2</td>
<td style="text-align: left;">12.2</td>
<td style="text-align: left;">32.3</td>
<td style="text-align: left;">59.2</td>
<td style="text-align: left;">20.4</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablative Study on the Perceiver of Flamingo Model.</p>
<h2>B. 5 More Qualitative Examples</h2>
<p>We show more qualitative examples in Figure 9, 10, 11.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: Qualitative Examples on 3D Captioning</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: Qualitative Examples on 3D-Assisted Dialog, 3D Dense Captioning and Question Answering.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11: Qualitative Examples on Task Decomposition and Grounding (Referring).</p>            </div>
        </div>

    </div>
</body>
</html>
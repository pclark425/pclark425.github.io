<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-425 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-425</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-425</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-268041662</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.17766v3.pdf" target="_blank">ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</a></p>
                <p><strong>Paper Abstract:</strong> This paper presents ShapeLLM, the first 3D Multimodal Large Language Model (LLM) designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages. ShapeLLM is built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding. By utilizing ReCon++ as the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed instruction-following data and tested on our newly human-curated benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D geometry understanding and language-unified 3D interaction tasks, such as embodied visual grounding. Project page: https://qizekun.github.io/shapellm/</p>
                <p><strong>Cost:</strong> 0.031</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e425.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e425.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShapeLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ShapeLLM (3D Multimodal Large Language Model for Embodied Interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal LLM that integrates a geometry-rich 3D point-cloud encoder (ReCon++) with a LLaMA-based language model, trained with instruction-following data to perform embodied 3D tasks (visual Q&A, embodied planning, 6-DoF outputs and part grounding). It maps local/global 3D embeddings and explicit position encodings into the language space so the LLM can generate executable spatial and procedural outputs (including formatted 6-DoF boxes).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShapeLLM (LLaMA-based MLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA backbone (Vicuna checkpoints) as the LLM combined with ReCon++ 3D point-cloud encoder. Local and global 3D embeddings are projected via MLPs into LLM token space; absolute position encodings (APE) are appended; prefix-tuning (learnable prompts) modulates semantics. Trained in two stages: multimodal alignment and instruction-following supervised fine-tuning using GPT-4V-generated multi-view Q&A and GAPartNet part annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>3D MM-Vet (zero-shot 3D multimodal comprehension & embodied interaction benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-aspect evaluation (General Recognition, Knowledge & Language Generation, Spatial Awareness, Embodied Interaction) over 59 curated 3D models and 232 human-written Q&As; also includes 3D MM-Vet-C corruptions (single-view, jitter, rotate) to test robustness under partial/occluded sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / embodied interaction / spatial awareness / 6-DoF pose estimation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Self-supervised pretraining on 3D datasets (Objaverse, ShapeNet, ABO, 3D-FUTURE) + supervised instruction tuning on GPT-4V-created multi-view Q&A and GAPartNet part-level data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Zero-shot prompting for evaluation; supervised fine-tuning (instruction tuning); prefix/prompt tuning; multimodal alignment training</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Local and global point-cloud embeddings projected into text-token space; explicit numeric absolute position encodings added as tokens; spatial outputs encoded as structured textual 6-DoF bounding-box lists; learnable prefix prompts modulate semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Total accuracy (%) on 3D MM-Vet (per-capability sub-scores), robustness accuracy on 3D MM-Vet-C variants</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Zero-shot on 3D MM-Vet: ShapeLLM-13B Total = 53.1%; ShapeLLM-7B Total = 47.4% (Table 3). Robustness (3D MM-Vet-C) for ShapeLLM-13B (clean / single-view / jitter / rotate) = 53.1% / 43.6% / 47.8% / 49.3% (Table 4). Embodied interaction (Emb) subscore: ShapeLLM-13B = 68.4% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Learns to output valid structured 6-DoF coordinates and multi-step embodied plans; strong embodied interaction reasoning and part-level object-relational understanding; robust to single-view occlusion relative to earlier point-cloud LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Residual limitations in fine-grained spatial awareness and referring-expression grounding (6-DoF IoU metrics remain modest); performance drops under single-view/partial input compared to multi-view; some hallucination risk (though reduced compared to image-only baselines) and dependence on quality/scale of SFT data.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms PointLLM-13B (Total 46.6%) and PointLLM-7B; image-based GPT-4V (4-view) reaches higher total (63.4%) on 2D inputs but uses multi-view imagery; language-only LLM baselines (LLaMA2, GPT-4-Turbo) perform very poorly (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Replacing PointLLM's encoder with ReCon++ yields +4.2% improvement; ablations (Table 7) show that combining APE, local and global embeddings, and prompting yields large cumulative gains (3D MM-Vet ~30.8% with minimal features -> 53.1% with full design). Table 12 shows scaling and multi-view alignment both critical (combined gives best zero-shot/3D MM-Vet gains).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mapping geometry-rich 3D embeddings into language space plus explicit numeric position tokens and instruction tuning enables LLMs to encode and utilize spatial, procedural and object-relational knowledge for embodied tasks; multi-view distillation and a dedicated 3D encoder (ReCon++) are crucial to provide the geometry priors the LLM needs to plan and output actionable coordinates even under partial sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ShapeLLM: Universal 3D Object Understanding for Embodied Interaction', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e425.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e425.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReCon++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReCon ++ (improved 3D point-cloud encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An enhanced 3D point-cloud encoder that extends ReCon with multi-view image distillation, adaptive selective matching via bipartite (Hungarian) matching, and scaled-up pretraining to produce pose-aware local and global 3D representations suited for mapping into language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReCon++ point-cloud encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>S / B / L variants (Large -L used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ViT-style point-cloud encoder with local-guided stop-gradient and separate global branch. Incorporates multi-view image query tokens (RGB + depth), bipartite matching (Hungarian) for cross-view alignment (learning implicit pose), contrast-with-reconstruct pretraining, and scaled datasets for robust geometry representations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>3D representation learning for classification, zero-shot recognition and multimodal grounding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learn geometry-aware local and global embeddings from point clouds (10k points) via multi-view distillation to support downstream recognition, zero-shot open-world classification, and embodied grounding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation learning / perception for manipulation and grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (geometry, implicit pose) + object-relational (parts)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Self-supervised pretraining on Objaverse, ShapeNet, ABO, 3D-FUTURE plus distillation from multi-view images</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Self-supervised masked reconstruction + contrastive learning; cross-modal distillation with multi-view images using bipartite matching</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Per-point/local embeddings and global queries; multi-scale tokens; learned cross-view matched queries encode pose-consistent global geometry</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Fine-tuned classification accuracy, zero-shot Top-1 accuracy on open-world datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Fine-tuned: 95.25% on ScanObjectNN and ~95.0% on ModelNet40 (reported). Zero-shot: 53.7% on Objaverse-LVIS and 65.4% on ScanObjectNN (reported improvements over prior work). ReCon++ variants set new SOTA transfer results (Table 1/2).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Produces geometry-accurate embeddings yielding better transfer, zero-shot open-world recognition, and supports downstream LLM grounding for part localization and pose-sensitive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still constrained by pretraining data distribution and single-view incompleteness; needs multi-view signals and scaling to maximize geometry fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms prior ReCon and other encoders (ULIP-2, PointGPT); reported gains: +1.85% vs previous best on ScanObjectNN in fine-tune, and competitive zero-shot metrics vs ensembled pretraining methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations (Table 12) show both scaling up and multi-view alignment individually improve performance; combined they produce the largest gains (zero-shot and 3D MM-Vet increases).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit multi-view selective distillation and adaptive matching enable learning of pose-aware global features; combining local per-point details with global cross-view queries yields representations that bridge to language models for embodied spatial and object-relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ShapeLLM: Universal 3D Object Understanding for Embodied Interaction', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e425.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e425.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absolute Position Encoding (APE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear projection of raw 3D xyz coordinates appended to the LLM input as explicit numeric positional tokens to provide precise low-level geometric signals that aid tasks such as 6-DoF pose estimation and precise referring outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShapeLLM (uses APE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>APE implemented as an MLP mapping (x,y,z) coordinates to embedding tokens, concatenated with ReCon++ local/global projected embeddings; modulated via prefix-tuning and fed into the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Geometry-sensitive outputs (6-DoF pose estimation, spatial awareness)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide an explicit numeric positional signal so the LLM can produce precise coordinate outputs and better low-level geometry reasoning required for manipulation and pose tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial / 6-DoF pose estimation / embodied interaction</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Direct projection of sensor point coordinates (from input point clouds)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Concatenated encoded tokens and prefix-tuning during instruction fine-tuning; ablated in experiments to test necessity</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit numeric embeddings derived from raw xyz coordinates (tokenized into LLM input space)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>3D MM-Vet accuracy and GAPartNet grounding metrics in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Ablation (Table 7) shows that configurations with APE combined with local/global embeddings and prompts reach the best 3D MM-Vet score (53.1%); APE alone yields modest single-component gains (e.g., APE-only config produced 30.8% in one ablation row).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Helps the LLM generate more precise numeric coordinate outputs and improves low-level geometry tasks when combined with semantic 3D embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Not sufficient alone â€” requires combination with learned 3D embeddings and prompting to reach strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Configurations lacking APE or other components score significantly lower in ablations (e.g., ~30.8% vs 53.1% with full set), indicating APE contributes importantly in combination.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Table 7 ablations explicitly demonstrate that APE combined with prompt/local/global embeddings provides large cumulative gains; removing APE reduces geometry-sensitive task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing explicit coordinate encodings as tokens is an effective mechanism to inject low-level geometric knowledge into language models; best results occur when numerical position tokens are fused with semantic 3D embeddings and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ShapeLLM: Universal 3D Object Understanding for Embodied Interaction', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e425.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e425.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>6DoF-text-coords</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>6-DoF bounding-box textual encoding ([[x,y,z], ...])</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A textual representation format used in instruction data where part positions are encoded as eight 3D vertex coordinates (lists of [x,y,z]) so the LLM learns to produce exact 6-DoF bounding boxes as text outputs for manipulation and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShapeLLM (LLM output format)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM is trained to output 6-DoF bounding boxes and part semantics in a strict textual list format (canonical object-space ordering), enabling the LLM to internalize spatial coordinate patterns during supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied part understanding & zero-shot 6-DoF pose estimation (textual output)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Training and evaluation task where the model must answer part-level questions by returning both semantic labels and precise 6-DoF bounding-box coordinates in text for downstream manipulation guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation / spatial reasoning / embodied interaction</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>GAPartNet annotated parts with 6-DoF boxes used to construct SFT examples; multi-view GPT-4V Q&A augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Supervised instruction tuning to map from point-cloud-derived embeddings to formatted coordinate outputs</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit symbolic numeric coordinate lists embedded in natural-language responses</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Validity/format correctness of coordinate outputs; downstream grounding accuracy (referring, IoU thresholds)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitatively, ShapeLLM produces consistently valid coordinate responses and improved embodied interaction outputs; the paper reports that LLMs 'easily grasp such patterns and consistently produce valid coordinates' (examples in Fig.5/6), but exact numeric 6-DoF accuracy metrics for this encoding are modest and limited by GAPartNet scale.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>LLM reliably generates syntactically valid coordinate lists and can localize handles/parts for manipulation tasks in examples; generalization to unseen objects shown qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Quantitative 6-DoF pose accuracy remains constrained, requiring more large-scale part-level SFT data for higher precision; precise IoU-based grounding scores are low compared to object recognition metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Image-only methods fail at zero-shot geometry-necessary 6-DoF estimation; ShapeLLM outperforms such image-only baselines qualitatively and in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Instruction tuning on coordinate-output format is essential; removing SFT part data degrades embodied interaction performance significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoding positions as structured numeric lists in language is a practical and learnable way for LLMs to represent and produce spatially precise, actionable outputs for manipulation, enabling embodied instructions without separate geometric decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ShapeLLM: Universal 3D Object Understanding for Embodied Interaction', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e425.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e425.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V (used-as-generator/evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V(ision) (used to construct instruction-following data and evaluate responses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-enabled multimodal GPT-4 variant used in this work to generate multi-view Q&A instruction-following data (~45K samples) from rendered views and to evaluate generated responses (GPT-4 used for scoring), enabling efficient creation and assessment of embodied instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained multimodal model with image understanding; used with in-context prompts to synthesize diverse Q&A across six aspects (including embodied interaction) and to score/model-evaluation via GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SFT data generation & open-ended evaluator for 3D MM-Vet</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given four multi-view renders per object, GPT-4V generated question-answer pairs spanning semantic, spatial and embodied aspects; GPT-4 (text) used to produce stable evaluation scores (K=5 rounds) for open-ended responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>data generation / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+spatial+object-relational (as encoded into language)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Pretrained multimodal knowledge from GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>In-context prompting and few-shot style prompts to generate structured Q&A and to provide evaluation judgments</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Natural language descriptions including formatted structured coordinates; used to produce instruction-following examples for LLM SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Number of generated samples; evaluation stability (std dev across scoring rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Approximately 45K instruction-following samples generated from Objaverse-LVIS renders and ~30K part-level examples from GAPartNet; estimated cost ~$900 for data generation; GPT-4 used for evaluation with standard deviation <0.1 across scoring rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides diverse, high-quality instruction-style SFT data including procedural steps and coordinate annotations which significantly improve ShapeLLM's embodied capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Generated data may still carry biases/hallucinations of the generator; SFT quality depends on GPT-4V alignment and prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a strong multimodal LLM as a synthetic data generator and evaluator is an effective bootstrapping strategy to inject procedural and spatial patterns into 3D MLLMs via language instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ShapeLLM: Universal 3D Object Understanding for Embodied Interaction', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e425.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e425.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-view point-cloud</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-view 3D point cloud (partial/occluded input)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical sensory scenario where the agent perceives only a single partial point cloud (from an RGB-D frame), testing the model's ability to infer missing geometry and still produce spatial/procedural outputs for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShapeLLM (evaluated on single-view inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ShapeLLM processes single-view partial point clouds via ReCon++ and APE; evaluated under 3D MM-Vet-C single-view corruption to measure robustness under incomplete sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied reasoning and planning under partial observability</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot evaluation where only one randomly sampled viewpoint point cloud is available (simulating real-world occlusion), requiring spatial inference, part localization and procedural plan generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial awareness / embodied interaction under limited sensory input</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Pretrained 3D encoder + instruction tuning SFT data in canonical object space (imparts priors to infer missing geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Zero-shot evaluation on corruption benchmark 3D MM-Vet-C (single-view)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Same projected local/global embeddings + APE; model relies on priors learned during pretraining and SFT to fill missing information</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) on 3D MM-Vet-C single-view corruption</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>ShapeLLM-13B single-view 3D MM-Vet-C accuracy = 43.6% (vs clean 53.1%); ShapeLLM-7B single-view = 38.3% (vs clean 47.4%) (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Maintains substantial embodied interaction and coordinate-output capability despite occlusion; can infer plausible part locations and produce procedural steps in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Performance degrades relative to clean multi-view inputs; fine-grained spatial relations that require unseen geometry are often missed or less accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>More robust than prior point-cloud LLM baselines (PointLLM) under single-view corruption, but still below image-based multi-view models like GPT-4V given richer visual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A geometry-aware encoder plus language instruction priors enables LLMs to generalize to partial sensory inputs; explicit geometry tokens and multi-scale embeddings mitigate but do not eliminate performance loss from single-view occlusion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ShapeLLM: Universal 3D Object Understanding for Embodied Interaction', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e425.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e425.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language-only baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-only LLMs without sensory input (LLaMA, GPT-3.5/GPT-4-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of pure-text LLMs on the 3D MM-Vet benchmark (no 3D or image input) showing they cannot reliably perform spatially-grounded or coordinate-producing embodied tasks and often hallucinate or refuse to answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B, GPT-3.5-Turbo, GPT-4-Turbo (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / null / null</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard language models operating only on textual prompts (no direct sensory data). Used as ablation to show necessity of geometry inputs for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>3D MM-Vet language-only evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot prompting baseline where LLMs answer the same 3D MM-Vet questions without any visual/3D input to test what spatial/procedural/object-relational knowledge is present in the language-only model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>baseline / knowledge probing</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+conceptual (not grounded spatial)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Pretraining on large text corpora (implicit knowledge in weights)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in model weights; natural language descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Total accuracy (%) on 3D MM-Vet when no visual input provided</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Language-only results in Table 9: LLaMA2-7B-Chat variant = 16.2% total; GPT-3.5-Turbo = 11.7%; GPT-4-Turbo = 9.8% (these low scores demonstrate poor grounded spatial/procedural ability without sensory inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Capable of answering generic knowledge or procedural questions that don't require grounding in a particular geometry instance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails at spatially-grounded Q&As, coordinate output, and part-level embodied instructions; prone to hallucination when asked about specific unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Far below ShapeLLM (47.4% and 53.1% totals for 7B/13B respectively) and point-cloud-enabled baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-only LLMs cannot reliably encode or produce the detailed spatial and object-relational knowledge required for embodied manipulation/planning when they lack direct geometric sensory inputs; sensory-grounded 3D encoders or explicit numeric geometry tokens are necessary to bridge that gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ShapeLLM: Universal 3D Object Understanding for Embodied Interaction', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PointLLM: Empowering large language models to understand point clouds <em>(Rating: 2)</em></li>
                <li>Point-bind & Point-LLM: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following <em>(Rating: 2)</em></li>
                <li>ReCon: Contrast with reconstruct: Contrastive 3D representation learning guided by generative pretraining <em>(Rating: 2)</em></li>
                <li>GAPartNet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts <em>(Rating: 2)</em></li>
                <li>PALM-E: An embodied multimodal language model (Palm-e) and related embodied LLM works <em>(Rating: 1)</em></li>
                <li>ACT: Cross-modal geometry understanding through 2D or language foundation models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-425",
    "paper_id": "paper-268041662",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "ShapeLLM",
            "name_full": "ShapeLLM (3D Multimodal Large Language Model for Embodied Interaction)",
            "brief_description": "A multimodal LLM that integrates a geometry-rich 3D point-cloud encoder (ReCon++) with a LLaMA-based language model, trained with instruction-following data to perform embodied 3D tasks (visual Q&A, embodied planning, 6-DoF outputs and part grounding). It maps local/global 3D embeddings and explicit position encodings into the language space so the LLM can generate executable spatial and procedural outputs (including formatted 6-DoF boxes).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ShapeLLM (LLaMA-based MLLM)",
            "model_size": "7B / 13B",
            "model_description": "LLaMA backbone (Vicuna checkpoints) as the LLM combined with ReCon++ 3D point-cloud encoder. Local and global 3D embeddings are projected via MLPs into LLM token space; absolute position encodings (APE) are appended; prefix-tuning (learnable prompts) modulates semantics. Trained in two stages: multimodal alignment and instruction-following supervised fine-tuning using GPT-4V-generated multi-view Q&A and GAPartNet part annotations.",
            "task_name": "3D MM-Vet (zero-shot 3D multimodal comprehension & embodied interaction benchmark)",
            "task_description": "Multi-aspect evaluation (General Recognition, Knowledge & Language Generation, Spatial Awareness, Embodied Interaction) over 59 curated 3D models and 232 human-written Q&As; also includes 3D MM-Vet-C corruptions (single-view, jitter, rotate) to test robustness under partial/occluded sensory input.",
            "task_type": "instruction following / embodied interaction / spatial awareness / 6-DoF pose estimation",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "Self-supervised pretraining on 3D datasets (Objaverse, ShapeNet, ABO, 3D-FUTURE) + supervised instruction tuning on GPT-4V-created multi-view Q&A and GAPartNet part-level data",
            "has_direct_sensory_input": true,
            "elicitation_method": "Zero-shot prompting for evaluation; supervised fine-tuning (instruction tuning); prefix/prompt tuning; multimodal alignment training",
            "knowledge_representation": "Local and global point-cloud embeddings projected into text-token space; explicit numeric absolute position encodings added as tokens; spatial outputs encoded as structured textual 6-DoF bounding-box lists; learnable prefix prompts modulate semantics.",
            "performance_metric": "Total accuracy (%) on 3D MM-Vet (per-capability sub-scores), robustness accuracy on 3D MM-Vet-C variants",
            "performance_result": "Zero-shot on 3D MM-Vet: ShapeLLM-13B Total = 53.1%; ShapeLLM-7B Total = 47.4% (Table 3). Robustness (3D MM-Vet-C) for ShapeLLM-13B (clean / single-view / jitter / rotate) = 53.1% / 43.6% / 47.8% / 49.3% (Table 4). Embodied interaction (Emb) subscore: ShapeLLM-13B = 68.4% (Table 3).",
            "success_patterns": "Learns to output valid structured 6-DoF coordinates and multi-step embodied plans; strong embodied interaction reasoning and part-level object-relational understanding; robust to single-view occlusion relative to earlier point-cloud LLM baselines.",
            "failure_patterns": "Residual limitations in fine-grained spatial awareness and referring-expression grounding (6-DoF IoU metrics remain modest); performance drops under single-view/partial input compared to multi-view; some hallucination risk (though reduced compared to image-only baselines) and dependence on quality/scale of SFT data.",
            "baseline_comparison": "Outperforms PointLLM-13B (Total 46.6%) and PointLLM-7B; image-based GPT-4V (4-view) reaches higher total (63.4%) on 2D inputs but uses multi-view imagery; language-only LLM baselines (LLaMA2, GPT-4-Turbo) perform very poorly (Table 9).",
            "ablation_results": "Replacing PointLLM's encoder with ReCon++ yields +4.2% improvement; ablations (Table 7) show that combining APE, local and global embeddings, and prompting yields large cumulative gains (3D MM-Vet ~30.8% with minimal features -&gt; 53.1% with full design). Table 12 shows scaling and multi-view alignment both critical (combined gives best zero-shot/3D MM-Vet gains).",
            "key_findings": "Mapping geometry-rich 3D embeddings into language space plus explicit numeric position tokens and instruction tuning enables LLMs to encode and utilize spatial, procedural and object-relational knowledge for embodied tasks; multi-view distillation and a dedicated 3D encoder (ReCon++) are crucial to provide the geometry priors the LLM needs to plan and output actionable coordinates even under partial sensory input.",
            "uuid": "e425.0",
            "source_info": {
                "paper_title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReCon++",
            "name_full": "ReCon ++ (improved 3D point-cloud encoder)",
            "brief_description": "An enhanced 3D point-cloud encoder that extends ReCon with multi-view image distillation, adaptive selective matching via bipartite (Hungarian) matching, and scaled-up pretraining to produce pose-aware local and global 3D representations suited for mapping into language models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ReCon++ point-cloud encoder",
            "model_size": "S / B / L variants (Large -L used in experiments)",
            "model_description": "ViT-style point-cloud encoder with local-guided stop-gradient and separate global branch. Incorporates multi-view image query tokens (RGB + depth), bipartite matching (Hungarian) for cross-view alignment (learning implicit pose), contrast-with-reconstruct pretraining, and scaled datasets for robust geometry representations.",
            "task_name": "3D representation learning for classification, zero-shot recognition and multimodal grounding",
            "task_description": "Learn geometry-aware local and global embeddings from point clouds (10k points) via multi-view distillation to support downstream recognition, zero-shot open-world classification, and embodied grounding tasks.",
            "task_type": "representation learning / perception for manipulation and grounding",
            "knowledge_type": "spatial (geometry, implicit pose) + object-relational (parts)",
            "knowledge_source": "Self-supervised pretraining on Objaverse, ShapeNet, ABO, 3D-FUTURE plus distillation from multi-view images",
            "has_direct_sensory_input": true,
            "elicitation_method": "Self-supervised masked reconstruction + contrastive learning; cross-modal distillation with multi-view images using bipartite matching",
            "knowledge_representation": "Per-point/local embeddings and global queries; multi-scale tokens; learned cross-view matched queries encode pose-consistent global geometry",
            "performance_metric": "Fine-tuned classification accuracy, zero-shot Top-1 accuracy on open-world datasets",
            "performance_result": "Fine-tuned: 95.25% on ScanObjectNN and ~95.0% on ModelNet40 (reported). Zero-shot: 53.7% on Objaverse-LVIS and 65.4% on ScanObjectNN (reported improvements over prior work). ReCon++ variants set new SOTA transfer results (Table 1/2).",
            "success_patterns": "Produces geometry-accurate embeddings yielding better transfer, zero-shot open-world recognition, and supports downstream LLM grounding for part localization and pose-sensitive outputs.",
            "failure_patterns": "Still constrained by pretraining data distribution and single-view incompleteness; needs multi-view signals and scaling to maximize geometry fidelity.",
            "baseline_comparison": "Outperforms prior ReCon and other encoders (ULIP-2, PointGPT); reported gains: +1.85% vs previous best on ScanObjectNN in fine-tune, and competitive zero-shot metrics vs ensembled pretraining methods.",
            "ablation_results": "Ablations (Table 12) show both scaling up and multi-view alignment individually improve performance; combined they produce the largest gains (zero-shot and 3D MM-Vet increases).",
            "key_findings": "Explicit multi-view selective distillation and adaptive matching enable learning of pose-aware global features; combining local per-point details with global cross-view queries yields representations that bridge to language models for embodied spatial and object-relational reasoning.",
            "uuid": "e425.1",
            "source_info": {
                "paper_title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "APE",
            "name_full": "Absolute Position Encoding (APE)",
            "brief_description": "A linear projection of raw 3D xyz coordinates appended to the LLM input as explicit numeric positional tokens to provide precise low-level geometric signals that aid tasks such as 6-DoF pose estimation and precise referring outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ShapeLLM (uses APE)",
            "model_size": "7B / 13B",
            "model_description": "APE implemented as an MLP mapping (x,y,z) coordinates to embedding tokens, concatenated with ReCon++ local/global projected embeddings; modulated via prefix-tuning and fed into the LLM.",
            "task_name": "Geometry-sensitive outputs (6-DoF pose estimation, spatial awareness)",
            "task_description": "Provide an explicit numeric positional signal so the LLM can produce precise coordinate outputs and better low-level geometry reasoning required for manipulation and pose tasks.",
            "task_type": "spatial / 6-DoF pose estimation / embodied interaction",
            "knowledge_type": "spatial",
            "knowledge_source": "Direct projection of sensor point coordinates (from input point clouds)",
            "has_direct_sensory_input": true,
            "elicitation_method": "Concatenated encoded tokens and prefix-tuning during instruction fine-tuning; ablated in experiments to test necessity",
            "knowledge_representation": "Explicit numeric embeddings derived from raw xyz coordinates (tokenized into LLM input space)",
            "performance_metric": "3D MM-Vet accuracy and GAPartNet grounding metrics in ablations",
            "performance_result": "Ablation (Table 7) shows that configurations with APE combined with local/global embeddings and prompts reach the best 3D MM-Vet score (53.1%); APE alone yields modest single-component gains (e.g., APE-only config produced 30.8% in one ablation row).",
            "success_patterns": "Helps the LLM generate more precise numeric coordinate outputs and improves low-level geometry tasks when combined with semantic 3D embeddings.",
            "failure_patterns": "Not sufficient alone â€” requires combination with learned 3D embeddings and prompting to reach strong performance.",
            "baseline_comparison": "Configurations lacking APE or other components score significantly lower in ablations (e.g., ~30.8% vs 53.1% with full set), indicating APE contributes importantly in combination.",
            "ablation_results": "Table 7 ablations explicitly demonstrate that APE combined with prompt/local/global embeddings provides large cumulative gains; removing APE reduces geometry-sensitive task performance.",
            "key_findings": "Providing explicit coordinate encodings as tokens is an effective mechanism to inject low-level geometric knowledge into language models; best results occur when numerical position tokens are fused with semantic 3D embeddings and instruction tuning.",
            "uuid": "e425.2",
            "source_info": {
                "paper_title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "6DoF-text-coords",
            "name_full": "6-DoF bounding-box textual encoding ([[x,y,z], ...])",
            "brief_description": "A textual representation format used in instruction data where part positions are encoded as eight 3D vertex coordinates (lists of [x,y,z]) so the LLM learns to produce exact 6-DoF bounding boxes as text outputs for manipulation and grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ShapeLLM (LLM output format)",
            "model_size": "7B / 13B",
            "model_description": "LLM is trained to output 6-DoF bounding boxes and part semantics in a strict textual list format (canonical object-space ordering), enabling the LLM to internalize spatial coordinate patterns during supervised fine-tuning.",
            "task_name": "Embodied part understanding & zero-shot 6-DoF pose estimation (textual output)",
            "task_description": "Training and evaluation task where the model must answer part-level questions by returning both semantic labels and precise 6-DoF bounding-box coordinates in text for downstream manipulation guidance.",
            "task_type": "object manipulation / spatial reasoning / embodied interaction",
            "knowledge_type": "spatial+object-relational",
            "knowledge_source": "GAPartNet annotated parts with 6-DoF boxes used to construct SFT examples; multi-view GPT-4V Q&A augmentation",
            "has_direct_sensory_input": true,
            "elicitation_method": "Supervised instruction tuning to map from point-cloud-derived embeddings to formatted coordinate outputs",
            "knowledge_representation": "Explicit symbolic numeric coordinate lists embedded in natural-language responses",
            "performance_metric": "Validity/format correctness of coordinate outputs; downstream grounding accuracy (referring, IoU thresholds)",
            "performance_result": "Qualitatively, ShapeLLM produces consistently valid coordinate responses and improved embodied interaction outputs; the paper reports that LLMs 'easily grasp such patterns and consistently produce valid coordinates' (examples in Fig.5/6), but exact numeric 6-DoF accuracy metrics for this encoding are modest and limited by GAPartNet scale.",
            "success_patterns": "LLM reliably generates syntactically valid coordinate lists and can localize handles/parts for manipulation tasks in examples; generalization to unseen objects shown qualitatively.",
            "failure_patterns": "Quantitative 6-DoF pose accuracy remains constrained, requiring more large-scale part-level SFT data for higher precision; precise IoU-based grounding scores are low compared to object recognition metrics.",
            "baseline_comparison": "Image-only methods fail at zero-shot geometry-necessary 6-DoF estimation; ShapeLLM outperforms such image-only baselines qualitatively and in the paper's comparisons.",
            "ablation_results": "Instruction tuning on coordinate-output format is essential; removing SFT part data degrades embodied interaction performance significantly.",
            "key_findings": "Encoding positions as structured numeric lists in language is a practical and learnable way for LLMs to represent and produce spatially precise, actionable outputs for manipulation, enabling embodied instructions without separate geometric decoders.",
            "uuid": "e425.3",
            "source_info": {
                "paper_title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4V (used-as-generator/evaluator)",
            "name_full": "GPT-4V(ision) (used to construct instruction-following data and evaluate responses)",
            "brief_description": "A vision-enabled multimodal GPT-4 variant used in this work to generate multi-view Q&A instruction-following data (~45K samples) from rendered views and to evaluate generated responses (GPT-4 used for scoring), enabling efficient creation and assessment of embodied instruction data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_size": null,
            "model_description": "Pretrained multimodal model with image understanding; used with in-context prompts to synthesize diverse Q&A across six aspects (including embodied interaction) and to score/model-evaluation via GPT-4.",
            "task_name": "SFT data generation & open-ended evaluator for 3D MM-Vet",
            "task_description": "Given four multi-view renders per object, GPT-4V generated question-answer pairs spanning semantic, spatial and embodied aspects; GPT-4 (text) used to produce stable evaluation scores (K=5 rounds) for open-ended responses.",
            "task_type": "data generation / evaluation",
            "knowledge_type": "procedural+spatial+object-relational (as encoded into language)",
            "knowledge_source": "Pretrained multimodal knowledge from GPT-4V",
            "has_direct_sensory_input": true,
            "elicitation_method": "In-context prompting and few-shot style prompts to generate structured Q&A and to provide evaluation judgments",
            "knowledge_representation": "Natural language descriptions including formatted structured coordinates; used to produce instruction-following examples for LLM SFT",
            "performance_metric": "Number of generated samples; evaluation stability (std dev across scoring rounds)",
            "performance_result": "Approximately 45K instruction-following samples generated from Objaverse-LVIS renders and ~30K part-level examples from GAPartNet; estimated cost ~$900 for data generation; GPT-4 used for evaluation with standard deviation &lt;0.1 across scoring rounds.",
            "success_patterns": "Provides diverse, high-quality instruction-style SFT data including procedural steps and coordinate annotations which significantly improve ShapeLLM's embodied capabilities.",
            "failure_patterns": "Generated data may still carry biases/hallucinations of the generator; SFT quality depends on GPT-4V alignment and prompt design.",
            "baseline_comparison": "",
            "ablation_results": "",
            "key_findings": "Using a strong multimodal LLM as a synthetic data generator and evaluator is an effective bootstrapping strategy to inject procedural and spatial patterns into 3D MLLMs via language instruction tuning.",
            "uuid": "e425.4",
            "source_info": {
                "paper_title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Single-view point-cloud",
            "name_full": "Single-view 3D point cloud (partial/occluded input)",
            "brief_description": "A practical sensory scenario where the agent perceives only a single partial point cloud (from an RGB-D frame), testing the model's ability to infer missing geometry and still produce spatial/procedural outputs for embodied tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ShapeLLM (evaluated on single-view inputs)",
            "model_size": "7B / 13B",
            "model_description": "ShapeLLM processes single-view partial point clouds via ReCon++ and APE; evaluated under 3D MM-Vet-C single-view corruption to measure robustness under incomplete sensory input.",
            "task_name": "Embodied reasoning and planning under partial observability",
            "task_description": "Zero-shot evaluation where only one randomly sampled viewpoint point cloud is available (simulating real-world occlusion), requiring spatial inference, part localization and procedural plan generation.",
            "task_type": "spatial awareness / embodied interaction under limited sensory input",
            "knowledge_type": "spatial+procedural",
            "knowledge_source": "Pretrained 3D encoder + instruction tuning SFT data in canonical object space (imparts priors to infer missing geometry)",
            "has_direct_sensory_input": true,
            "elicitation_method": "Zero-shot evaluation on corruption benchmark 3D MM-Vet-C (single-view)",
            "knowledge_representation": "Same projected local/global embeddings + APE; model relies on priors learned during pretraining and SFT to fill missing information",
            "performance_metric": "Accuracy (%) on 3D MM-Vet-C single-view corruption",
            "performance_result": "ShapeLLM-13B single-view 3D MM-Vet-C accuracy = 43.6% (vs clean 53.1%); ShapeLLM-7B single-view = 38.3% (vs clean 47.4%) (Table 4).",
            "success_patterns": "Maintains substantial embodied interaction and coordinate-output capability despite occlusion; can infer plausible part locations and produce procedural steps in many cases.",
            "failure_patterns": "Performance degrades relative to clean multi-view inputs; fine-grained spatial relations that require unseen geometry are often missed or less accurate.",
            "baseline_comparison": "More robust than prior point-cloud LLM baselines (PointLLM) under single-view corruption, but still below image-based multi-view models like GPT-4V given richer visual inputs.",
            "ablation_results": "",
            "key_findings": "A geometry-aware encoder plus language instruction priors enables LLMs to generalize to partial sensory inputs; explicit geometry tokens and multi-scale embeddings mitigate but do not eliminate performance loss from single-view occlusion.",
            "uuid": "e425.5",
            "source_info": {
                "paper_title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Language-only baseline",
            "name_full": "Language-only LLMs without sensory input (LLaMA, GPT-3.5/GPT-4-Turbo)",
            "brief_description": "Evaluation of pure-text LLMs on the 3D MM-Vet benchmark (no 3D or image input) showing they cannot reliably perform spatially-grounded or coordinate-producing embodied tasks and often hallucinate or refuse to answer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-7B, GPT-3.5-Turbo, GPT-4-Turbo (text-only)",
            "model_size": "7B / null / null",
            "model_description": "Standard language models operating only on textual prompts (no direct sensory data). Used as ablation to show necessity of geometry inputs for embodied tasks.",
            "task_name": "3D MM-Vet language-only evaluation",
            "task_description": "Zero-shot prompting baseline where LLMs answer the same 3D MM-Vet questions without any visual/3D input to test what spatial/procedural/object-relational knowledge is present in the language-only model weights.",
            "task_type": "baseline / knowledge probing",
            "knowledge_type": "procedural+conceptual (not grounded spatial)",
            "knowledge_source": "Pretraining on large text corpora (implicit knowledge in weights)",
            "has_direct_sensory_input": false,
            "elicitation_method": "Zero-shot prompting",
            "knowledge_representation": "Implicit in model weights; natural language descriptions",
            "performance_metric": "Total accuracy (%) on 3D MM-Vet when no visual input provided",
            "performance_result": "Language-only results in Table 9: LLaMA2-7B-Chat variant = 16.2% total; GPT-3.5-Turbo = 11.7%; GPT-4-Turbo = 9.8% (these low scores demonstrate poor grounded spatial/procedural ability without sensory inputs).",
            "success_patterns": "Capable of answering generic knowledge or procedural questions that don't require grounding in a particular geometry instance.",
            "failure_patterns": "Fails at spatially-grounded Q&As, coordinate output, and part-level embodied instructions; prone to hallucination when asked about specific unseen objects.",
            "baseline_comparison": "Far below ShapeLLM (47.4% and 53.1% totals for 7B/13B respectively) and point-cloud-enabled baselines.",
            "ablation_results": "",
            "key_findings": "Text-only LLMs cannot reliably encode or produce the detailed spatial and object-relational knowledge required for embodied manipulation/planning when they lack direct geometric sensory inputs; sensory-grounded 3D encoders or explicit numeric geometry tokens are necessary to bridge that gap.",
            "uuid": "e425.6",
            "source_info": {
                "paper_title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PointLLM: Empowering large language models to understand point clouds",
            "rating": 2,
            "sanitized_title": "pointllm_empowering_large_language_models_to_understand_point_clouds"
        },
        {
            "paper_title": "Point-bind & Point-LLM: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following",
            "rating": 2,
            "sanitized_title": "pointbind_pointllm_aligning_point_cloud_with_multimodality_for_3d_understanding_generation_and_instruction_following"
        },
        {
            "paper_title": "ReCon: Contrast with reconstruct: Contrastive 3D representation learning guided by generative pretraining",
            "rating": 2,
            "sanitized_title": "recon_contrast_with_reconstruct_contrastive_3d_representation_learning_guided_by_generative_pretraining"
        },
        {
            "paper_title": "GAPartNet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts",
            "rating": 2,
            "sanitized_title": "gapartnet_crosscategory_domaingeneralizable_object_perception_and_manipulation_via_generalizable_and_actionable_parts"
        },
        {
            "paper_title": "PALM-E: An embodied multimodal language model (Palm-e) and related embodied LLM works",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model_palme_and_related_embodied_llm_works"
        },
        {
            "paper_title": "ACT: Cross-modal geometry understanding through 2D or language foundation models",
            "rating": 1,
            "sanitized_title": "act_crossmodal_geometry_understanding_through_2d_or_language_foundation_models"
        }
    ],
    "cost": 0.031176999999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>12 Jul 2024
12 Jul 2024B584FC1E5032276FE2D7F1D1ECA205EFarXiv:2402.17766v3[cs.CV]3D Point CloudsLarge Language ModelsEmbodied Intelligence3D Representation LearningZero-shot Learning
This paper presents ShapeLLM, the first 3D Multimodal Large Language Model (LLM) designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages.ShapeLLM is built upon an improved 3D encoder by extending ReCon[135]to ReCon ++ that benefits from multi-view image distillation for enhanced geometry understanding.By utilizing ReCon ++ as the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed instruction-following data and tested on our newly humancurated benchmark, 3D MM-Vet.ReCon ++ and ShapeLLM achieve state-of-the-art performance in 3D geometry understanding and languageunified 3D interaction tasks, such as embodied visual grounding.</p>
<p>Fig. 1: Demonstrations of ShapeLLM and ReCon ++ .We present ShapeLLM, the first 3D LLM designed for embodied interaction and spatial intelligence.</p>
<p>Introduction</p>
<p>3D shape understanding, serving as a fundamental capability for molding intelligent systems in both digital and physical worlds, has witnessed tremendous progress in graphics, vision, augmented reality, and embodied robotics.However, to be effectively deployed by real-world agents, several critical criteria must be fulfilled: (i) Sufficient 3D geometry information needs to be captured for accurate spatial and structure processing [10,13,82,132].(ii) Models should be endowed with a foundational knowledge of the embodied interaction fashion with objectsoften physically -for functional comprehension [55, 68-70, 83, 131, 200, 201].(iii) A universal interface is required as a bridge between information encoding and decoding, which could help translate high-order instructions for agent reactions like dialogue response and embodied feedback [28,75,202].</p>
<p>Recent advancements in Large Language Models (LLMs) [11,119,139,140,156] have demonstrated unprecedented success of foundational knowledge and unified reasoning capabilities across tasks [7,21,29,36,40,73,76,81,130].It makes it possible to utilize language as a universal interface that enables the comprehensive commonsense knowledge embedded in LLMs to enhance understanding of 3D shapes.This is particularly evident in physically-grounded tasks, where the wealth of commonsense knowledge simplifies the interpretation of an object's functionality, mobility, and dynamics, etc.However, the aforementioned challenges remain when incorporating LLMs for 3D object understanding -especially embodied interaction that relies on precise geometry -currently under-explored.</p>
<p>The question is: What makes better 3D representations that bridge language models and interaction-oriented 3D object understanding?In this work, we introduce ShapeLLM that meets the requirements, which is established based on the following three designing policies: i. 3D Point Clouds as Inputs Some concurrent works [57] recently propose to use point cloud-rendered images [193] as multimodal LLMs' inputs and demonstrate effectiveness.However, these works fail to achieve accurate 3D geometry understanding and often suffer from a well-known visual hallucination issue [90,143,204].Compared to 2D images, 3D point clouds provide a more accurate representation of the physical environment, encapsulating sparse yet highly precise geometric data [1,37,133].Moreover, 3D point clouds are crucial in facilitating embodied interactions necessitating accurate 3D structures like 6-DoF object pose estimation [88,160,162,166,173]. ii.Selective Multi-View Distillation Interacting with objects typically necessitates an intricate 3D understanding that involves knowledge at various levels and granularities.For instance, a whole-part high-level semantic understanding is needed for interactions like opening a large cabinet, while detailed, high-resolution (i.e., low-level ) semantics are crucial for smaller objects like manipulating a drawer handle [181].However, existing works mainly distill single-view high-resolution object features from 2D foundation models [138], providing a complementary understanding [37,135,175].The potential of multi-view images, which offer abundant multi-level features due to view variation and geometry consistency [9,61,66,82,103,149], is often neglected.ShapeLLM extends ReCon [135] to ReCon ++ as the 3D encoder by integrating multi-view distillation.To enable the model to selectively distill views that enhance optimization and generalization, inspired by DETR [12], ReCon ++ is optimized through adaptive selective matching using the Hungarian algorithm [85].iii.3D Visual Instruction Tuning Instruction tuning has been proven effective in improving LLMs' alignment capability [122,126].To realize various 3D understanding tasks with a universal language interface, ShapeLLM is trained through instruction-following tuning on constructed language-output data.However, similar to 2D visual instruction tuning [4,96], the data-desert issue [37] is even worse since no object-level VQA data is available, unlike 2D [95].To validate the efficacy of ShapeLLM, we first construct âˆ¼45K instruction-following data using the advanced GPT-4V(ision) [120] on the processed Objaverse dataset [30] and 30K embodied part understanding data from GAPartNet [50] for supervised fine-tuning.Following MM-Vet [185], we further develop a novel evaluation benchmark named 3D MM-Vet.This benchmark is designed to assess the core vision-language capabilities, including embodied interaction in a 3D context, thereby stimulating future research.The 3D MM-Vet benchmark comprises 59 diverse Internet8 3D objects and 232 human-written question-answer pairs.</p>
<p>Through extensive experimentation, we first demonstrate that our improved 3D encoder ReCon ++ sets a new state-of-the-art representation transferring on both downstream fine-tuned and zero-shot 3D object recognition.Specifically, ReCon ++ has obtained 95.25% and 95.0% fine-tuned accuracy on ScanOb-jectNN and ModelNet40, surpassing previous best records by +1.85% on the most challenging ScanObjectNN.Besides, ReCon ++ achieved 53.7% and 65.4% zero-shot accuracy on Objaverse-LVIS and ScanObjectNN, which is +0.6% and +1.6% higher than previous best.By utilizing our ReCon ++ as ShapeLLM's    3D encoder, ShapeLLM successfully unifies various downstream tasks, including 3D captioning, 3D VQA, embodied task planning &amp; decomposition, 3D embodied visual grounding, and 3D precise referring dialogue (See Fig. 1).On our newly constructed 3D MM-Vet benchmark, 42.7% and 49.3%Total accuracy have been achieved by ShapeLLM-7B and ShapeLLM-13B, surpassing previous best records [172] that also uses 3D point clouds by +2.1% and +5.1%, respectively.This work initiates a first step towards leveraging LLMs for embodied object interaction, and we hope our ShapeLLM and proposed 3D MM-Vet benchmark could spur more related future research.
â€¢â€¢â€¢â€¢â€¢ â€¢â€¢â€¢â€¢â€¢â€¢ Multi-View Images Embodied / GPT-4V Instruction [[x 1 ,y 1 ,z 1 ], â€¢â€¢â€¢â€¢â€¢â€¢ [x 8 ,y</p>
<p>ShapeLLM</p>
<p>In this section, we first introduce the overall architecture of ShapeLLM.Then, we delve into two critical challenges faced in interactive 3D understanding: data desert [37] and representation of 3D point clouds.We present the detailed design of our method to tackle these challenges, respectively.</p>
<p>Overall Architecture</p>
<p>The main objective of this work is interactive 3D understanding by using the LLM as a universal interface.Drawing inspiration from recent work in visual understanding [96], the proposed ShapeLLM consists a pre-trained 3D encoder and an LLM for effective 3D representation learning and understanding, respectively.Specifically, we adopt LLaMA [156] as our LLM, building upon the success of previous work [25,36,96  (a) Construction illustration of instructfollowing data using GPT-4V [120].Four perspective views are input into GPT-4V.Incontext prompts focusing on different topics are explicitly incorporated to ensure data diversity.[Question 3 Type: Embodied Interaction] As an AI assistant, please give me some steps to put her in all the protective equipment.</p>
<p>[Answer 3 ]</p>
<p>Step 1: Put her in the protective clothing and tie the knot.</p>
<p>Step 2: Put her hair up and cover it with the hat.</p>
<p>Step 3: Open the gloves and put them on her.</p>
<p>(b) 3D MM-Vet dataset sample.A wealth of precise evaluation metrics enable a comprehensive assessment.with multiple improvements as the 3D understanding generally demands more information, such as accurate spatial and multi-view details, etc.To ensure compatibility with the LLM inputs, the representation of a 3D object obtained from ReCon ++ undergoes a linear projection before being fed into the LLM.To further improve low-level geometry understanding, which benefits tasks like 6-DoF pose estimation, we append the absolute position encoding (APE) obtained by linear projection of 3D coordinates.Besides, we use prefix-tuning with learnable prompts [36,37,79,87] to adaptively modulate the different semantics of APE and ReCon ++ representations.</p>
<p>How to alleviate interactive 3D understanding Data Desert?</p>
<p>Most published 3D data is typically presented as 3D object-caption pairs, lacking an interactive style.Although a few concurrent works [65,172] have attempted to construct interactive 3D understanding datasets, the questions-and-answers (Q&amp;As) are primarily based on annotated captions, often providing a limited perspective without sufficient details.Additionally, those works have generally been limited to semantic understanding without considering embodied interaction.</p>
<p>To address these limitations, our work constructs question-and-answer pairs based on multi-view images of a 3D object using GPT-4V(ision) [120].For data diversity, we explicitly introduce six aspects as prompts, as illustrated Fig. 3a.</p>
<p>In the following, we provide the details about data collection and construction regarding general semantic understanding and embodied object understanding, respectively.Data Objaverse-LVIS [30,110] and GAPartNet [50] are data sources.Objaverse-LVIS covers 1,156 LVIS [58] categories, and we sample Top-10 "likes" 9 3D objects per category and generate Q&amp;A pairs per sample.After filtering out noisy Q&amp;As, we obtain âˆ¼45K instruction-following samples.We use 12 categories from GAPartNet by removing "Remote" to avoid too many tiny boxes, which leads to filtered âˆ¼30K Q&amp;A samples constructed from âˆ¼8K parts of the âˆ¼4K objects states covering âˆ¼1.1K different objects.</p>
<p>General Semantic Understanding This aims to enhance the model's generalization abilities in visual recognition, knowledge integration, spatial understanding, and other aspects.We prompt GPT4-V to generate Q&amp;As in six different aspects based on images captured from four different views, as illustrated in Fig. 3a.</p>
<p>Embodied Object Understanding A comprehensive understanding of the spatial positions and semantics at the part level is crucial to facilitate effective object grasping and interaction in embodied scenarios.Fortunately, the GAPart-Net [50] provides rich part annotations, including semantics and poses, which are instrumental in constructing instruction-tuning data for embodied interactive parts of a subject.Specifically, given a 3D object, questions are formulated based on the semantics of its different parts, and answers are constructed in both the semantics and 3D positions.The positions are represented as 6-DoF 3D bounding boxes in a straightened Python multidimensional list format, denoted as [[x1, y1, z1], [x2, y2, z2], ..., [x8, y8, z8]], to meet characteristics of the textual dialogues response in LLMs.The canonical space of the object determines the sequence of coordinates.Using bounding box coordinates leverages the inherent spatial relationship, allowing LLMs to readily learn these patterns and generate accurate output coordinates.This approach can offer specific position information for embodied manipulation, as illustrated in Fig. 4.</p>
<p>9</p>
<p>"Likes" statistics can be found at Sketchfab.</p>
<p>ReCon ++ : Scaling Up 3D Representation Learning</p>
<p>Interaction with objects such as object grasping [99,160,173] typically requires accurate perception of 3D shape information at multi-level and multi-granularity.This imposes heightened requirements on 3D representations, calling for a higher standard of a holistic understanding of 3D geometry.</p>
<p>However, existing 3D cross-modal representation learning methods [97,176] mainly distill high-resolution object features from single-view 2D foundation models, resulting in a unilateral shape understanding.Besides, they generally employ multi-view images as data augmentation, imposing the learned representation to the average representation of all views.Thus, the accurate 3D shape information is missing.Recently, ReCon [135] utilizes contrast guided by reconstruction to address the pattern disparities between local masked data modeling and global cross-modal alignment.This results in remarkable performance in various tasks, including transfer learning, zero-shot classification, and part segmentation.However, its potential is hindered by the scarcity of pretraining data [13].</p>
<p>To address the above limitations, this paper proposes ReCon ++ with multiple improvements.First, multi-view image query tokens collaboratively comprehend the semantic information of 3D objects across different views, encompassing both RGB images and depth maps.Considering the disorderliness of pretraining data in terms of pose, we propose a cross-modal alignment method based on bipartite matching, which implicitly learns the pose estimation of 3D objects.Second, we scale up the parameters of ReCon and broaden the scale of the pretraining dataset [18,30,110] for robust 3D representations.</p>
<p>Denote N as the number of multi-view images, I i is the image feature from i-th view, and Q i represents the global query of i-th view.Following DETR [12], we search for an optimal permutation Ïƒ of N elements with the lowest cost:
Ïƒ = arg min Ïƒ N i L match (I i , Q Ïƒ(i) ),(1)
where
L match (I i , Q Ïƒ(i)
) is a pair-wise matching cost between i-th view image features I i and matched query Q Ïƒ(i) with the permutation Ïƒ.In practice, we employ cosine similarity as the matching cost.In this fashion, the query of each view is learned to gather accurate 3D shape information from the 3D point clouds.</p>
<p>Concatenating the features from the local 3D point cloud encoder and global 3D point cloud decoder together provides comprehensive information for 3D understanding of multimodal LLMs.</p>
<p>3D MM-Vet: Benchmarking 3D Comprehension</p>
<p>A wide range of diverse visual-language capabilities is essential to develop a multimodal large language model tailored for embodied scenarios, particularly addressing task and action planning.The model's proficiency in processing point clouds enables it to perform general recognition tasks effortlessly, demonstrating a broad understanding of colored point clouds.This capability serves as the groundwork for more intricate tasks.Beyond 3D recognition, the LLM should exhibit competence in addressing tasks in real-world embodied scenarios.This entails unifying the aforementioned abilities to generate decomposed task actions step-by-step in an instructionfollowing fashion, addressing specific problems.Hence, to formulate an evaluation system aligned with the aforementioned task description, we establish a multi-level evaluation task system encompassing four-level tasks: General Recognition, Knowledge and Language Generation, Spatial Awareness, and Embodied Interaction.This framework systematically and comprehensively assesses the model's proficiency in information comprehension and language generation when processing interactive objects.The detailed descriptions of the tasks are listed as follows:</p>
<p>i. General Recognition: Following MM-Vet [185], we assess the fundamental comprehension abilities of LLMs involving both coarse-and fine-grained aspects.Coarse-grained recognition focuses on basic object attributes such as color, shape, action, etc.While fine-grained recognition delves into details like subparts and counting, etc. ii.Knowledge Capability &amp; Language Generation: To examine the models' capacity to understand and utilize knowledge, drawing inspiration from MMBench [102], we integrate its reasoning components.This includes knowledge spanning natural and social reasoning, physical properties, sequential prediction, math, etc., evaluating gauges whether multimodal LLMs possess the requisite expertise and capacity to solve intricate tasks.We utilize</p>
<p>What can this object be used to do?</p>
<p>This object can be used to play music, as it appears to be a stringed instrument.</p>
<p>What kind of material might be used to produce this object?</p>
<p>The object appears to be made of wood, which is commonly used for stringed instruments</p>
<p>User User</p>
<p>What is this?Which country does it come from?This is sushi, a traditional Japanese dish.It is made from vinegared rice, often served with raw fish or other ingredients, wrapped in seaweed or wrapped in paper.</p>
<p>As an Embodied AI assistant, please give me steps to feed me noodles.customized prompts to stimulate models and extract detailed responses to evaluate language generation.iii.Spatial Awareness: In 3D, spatial awareness holds heightened significance compared to 2D due to the provided geometry information.The point clouds contain location information crucial for discerning spatial relationships between different parts.In 2D, achieving the same information intensity level would necessitate multi-view images.Therefore, our evaluation includes questions probing the ability of LLMs to understand spatial relations.iv.Embodied Interaction: The utilization scope of MLLMs extends into the field of embodied interaction, facilitated by the utilization of instructionfollowing data.Our evaluation system tests their capacity by formally requesting LLMs to provide execution steps toward an instruction.This approach aims to establish connections for handling Embodied Interaction tasks [40,73].</p>
<p>To prevent any overlap with training data, our collection of 3D models is sourced exclusively from Turbosquid [148], a platform not included in the acquisition lists of Objaverse [30] and ShapeNet [13].We meticulously curated a dataset of 59 3D models, generating 232 Q&amp;As for evaluation purposes.In our pursuit of a precise assessment of single-task capabilities, each question is designed to test only one specific capacity outlined earlier.Every question is paired with a corresponding answer tailored to the particular 3D model, serving as the ground truth.Dataset samples are illustrated in Fig. 3b.More details and analysis can be found in the supplemental material.</p>
<p>Experiments</p>
<p>3D Representation Transferring with ReCon ++</p>
<p>Fine-tuned 3D Object Recognition In Tab. 1, we first evaluate the representation transfer learning capabilities of self-supervised ReCon ++ by fine-tuning on ScanObjectNN [157] and ModelNet [170], which are currently the two most Table 2: Zero-shot 3D recognition on Objaverse-LVIS [30], ModelNet40 [170] and ScanObjectNN [157].Ensembled [97]: pretraining with four datasets, Objaverse [30], ShapeNet [13], ABO [23] and 3D-FUTURE [44].â€  : Uni3D employs a larger EVA-CLIP-E [152] teacher, while other methods employ OpenCLIP-bigG [77].</p>
<p>Method</p>
<p>Objaverse-LVIS ModelNet40 ScanObjectNN</p>
<p>Top1 Top3 Top5 Top1 Top3 Top5 Top1 Top3 Top5</p>
<p>2D Inference without 3D Training</p>
<p>PointCLIP [193] 1.9 challenging 3D object datasets.ScanObjectNN is a collection of âˆ¼15K 3D object point clouds from the real-world scene dataset ScanNet [24], which involves 15 categories.ModelNet is one of the most classical 3D object datasets collected from clean 3D CAD models, which includes âˆ¼12K meshed 3D CAD models covering 40 categories.Following PointGPT [18], we adopt the intermediate fine-tuning strategy and use the post-pretraining stage to transfer the general semantics learned through self-supervised pretraining on ShapeNetCore [13].For a fair comparison, our Base and Large models adopt the same architecture as PointGPT regarding layers, hidden size, and attention heads.Tab. 1 shows that: (i) ReCon ++ exhibits representation performance significantly surpassing that of other baselines, achieving state-of-the-art results.(ii) Particularly, ReCon ++ achieves a remarkable accuracy of 95.25% on the most challenging ScanObjectNN PB_T50_RS benchmark, boosting the Transformer baseline by +16.14%.</p>
<p>Zero-Shot 3D Open-World Recognition Similar to CLIP [138], our model aligns the feature space of languages and other modalities, which results in a zero-shot open-world recognition capability.In Tab. 2, we compare the zeroshot 3D open-world object recognition models to evaluate the generalizable recognition capability.Following OpenShape [97], we evaluate on ModelNet [170], ScanObjectNN [157], and Objaverse-LVIS [30].Objaverse-LVIS is a benchmark involving âˆ¼47K clean 3D models of 1,156 LVIS categories [58].We compare ReCon ++ with 2D inference methods, ShapeNet pretrained methods, and "Ensembled" datasets-pretrained methods.It can be concluded from Tab. 2: i) Compared to 2D inference and ShapeNet-pretrained methods, ReCon ++ demonstrates significantly superior performance, showing the necessity of 3D point clouds as inputs and scaling up.ii) Compared to state-of-the-art methods trained on "Ensembled" datasets, ReCon ++ demonstrates superior or on-par performance across all benchmarks.Notably, ReCon ++ -L achieves a remarkable Top-1 accuracy, which is +0.6% and +7.2% higher than Uni3D-L on the most challenging Objaverse-LVIS and ScanObjectNN benchmarks, respectively.</p>
<p>Multimodal Comprehension with ShapeLLM</p>
<p>Quantitative Analysis To assess the comprehensive capabilities of ShapeLLM, we first quantitatively compare various baselines and our model on the proposed 3D MM-Vet using GPT-4.Following ModelNet-C [142] and ModelNet40-C [150], we construct 3D MM-Vet-C to benchmark the robustness against 3D corruptions.ii) 3D MM-Vet-C.Following the ModelNet-C [142] and ModelNet40-C [150], we construct 3D MM-Vet-C to benchmark the robustness against 3D corruptions.Tab. 4 shows the comparison of robustness against "single-view", "jitter", and "rotate" corruptions, which are the most common in real scenarios.The "single-view" issue is the most critical challenge since obtaining the complete point clouds is non-trivial, similar to multi-view images.Therefore, everyday real-world robots only get single-view 3D perceptions with sensors such as RGB-D [59].The results demonstrate sig- Yes, all the chairs are completely placed under the table.</p>
<p>User</p>
<p>single-view point cloud original mesh (only for reference)</p>
<p>What is she doing now?</p>
<p>The figure appears to be in mid-motion, as if she's in the act of reaching or pointing towards something.ii) Improvement from data.As stated in Sec.2.2, we have constructed instruction-following data for supervised fine-tuning (SFT) using GPT-4V involving diverse topics.By further using the SFT data curated by us, PointLLM's performance gap to ShapeLLM has been fulfilled.This demonstrates the superiority of our SFT data, where the decent quality comes from the advanced GPT4-V using multi-view images and the topics covered in the data.Qualitative Analysis Fig. 5 illustrates qualitative examples of ShapeLLM in multimodal dialogue.ShapeLLM can support general VQA, embodied task and action planning, and 6-DoF pose estimation.Notably, LLMs easily grasp such patterns and consistently produce valid coordinates due to the strict spatial relationship inherent in 6-DoF bounding box coordinates.Fig. 6 shows the examples of ShapeLLM-13B's response using single-view point cloud inputs, demonstrating surprisingly outstanding robustness in processing such occlusion.This is crucial for the practical deployment of real machines, as single-view point clouds can be easily obtained from RGB-D cameras.</p>
<p>Discussions</p>
<p>Is ShapeLLM grounded in physical worlds?</p>
<p>Tab. 6 compares ShapeLLM with image-only methods on 3D referring expression grounding (REG) of 6-DoF poses on GAPartNet [50].The results show that: i) Image-only methods cannot perform zero-shot geometry-necessary 6-DoF pose estimation.ii) Compared to image-only methods with 2D to 6-DoF pose estimation fine-tuning or in-context prompting, ShapeLLM still performs significantly better.It demonstrates the necessity of geometry and the difficulty of the ill-posed 2D to 6-DoF pose estimation problem, as well as the importance of using 3D point clouds as input for spatial intelligence.</p>
<p>Can ShapeLLM generalize to unseen objects?</p>
<p>Where is its hat?Interaction-Oriented 3D Understanding Interaction with 3D objects typically involves concept-only interaction and physical-grounded interaction [15].The former works focus on 3D perception and semantic parsing, such as 3D object recognition and scene perception [104,132,133,163,165].By utilizing language for open-ended interaction in 3D, a number of works demonstrate successful 3D scene QA [111,180], grounding [16], and captioning [17].Recently, some works propose to utilize foundation models like LLMs or CLIP for open-ended 3D object recognition [37,97,193,206] and scene segmentation [127,188].Guo &amp; Zhang et al. [57] utilizes ImageBind [52] and LLaMA-Adapter [194] to realize point cloud-based interactive QA.Following LLaVA, PointLLM [172] conducts supervised fine-tuning by constructing a visual instruction-following dataset.Other works focus on scene-level tasks utilizing comprehensive 2D features [71,207] or 3D features distilled from 2D images into LLMs [65,71,207].The second kind of interaction typically requires physical understanding in 3D, such as part understanding [50,98,108,116], 6-DoF pose estimation [88,100,162,166,181], particularly useful for human-object interaction (HOI) and robotic manipulation [20, 48-51, 53, 89, 99, 117, 134, 145, 160, 173, 184] and complex robotic planning [14,35,40,74,93,147].In this work, we focus on both physical and conceptual interactions with 3D shapes for embodied understanding.</p>
<p>Multimodal Large Language Models Multimodal comprehension, which allows human interaction with textual and visual elements, has witnessed significant advancements, particularly in extending LLMs like LLaMA [22,155,156].The early efforts predominantly revolved around integrating LLMs with various downstream systems by employing it as an agent [6,60,91,146,154,161,167,177,178].Significant success has been demonstrated within this plugin-style framework.Due to the remarkable capabilities of LLMs, aligning the visual semantic space with language through parameter-efficient tuning [2,67,86,179,194,205] and instruction tuning [25,36,96,174] has emerged as the prevailing approach in current research.To further enhance interactive capabilities, some approaches have been developed towards visual-interactive multimodal comprehension by precisely referring to instruction tuning [19,129,196,199].Another family advances the developments of LLMs endowed with content creation beyond comprehension [36,47,84,123,151,153,168].</p>
<p>Conclusions</p>
<p>This paper introduces ShapeLLM, the first 3D MLLM for embodied interaction, excelling in generalizable recognition and interaction comprehension.We present ReCon ++ , a novel 3D point cloud encoder leveraging multi-view distillation and advanced 3D representation learning, forming the basis for ShapeLLM.We perform 3D visual instruction tuning on curated instruction-following data for broad and embodied comprehension.Additionally, we establish 3D MM-Vet, a benchmark to evaluate four levels of capacity in embodied interaction scenarios, from fundamental recognition to control statement generation.Visual Prompt Number Fig. 8 shows the performance of ShapeLLM using different numbers of prompts, including 1, 8, 16, 32, and 64.This ablation study has shown that a different number of prompts leads to varied improvements, and the optimal setting is 32.This observation is similar to VPT [79] where the prompts used to modulate Transformer attention should be studied [62].</p>
<p>No. of Learnable Prompts</p>
<p>A.2 Multimodal Comprehension with ShapeLLM</p>
<p>Generative 3D Object Recognition &amp; Captioning Following PointLLM [172], we conduct generative 3D recognition and captioning experiments.Tab. 8 shows 3D object classification overall accuracy (%) and captioning performance evaluated by GPT-4 and data-driven metrics: Sentence-BERT (S-BERT) [141] and SimCSE [45].It can be observed that ShapeLLM consistently outperforms other methods across all metrics, demonstrating robust recognition and instructionfollowing capabilities.Note that similar to PointLLM's findings, we also notice that the 3D captioning performance evaluated by traditional metrics like BLEU-1 [125], ROUGE-L [92], and METEIOR [5] are highly unreliable in accurately revealing the response quality.This is further demonstrated by human-oriented evaluation, such as the preference win rate comparison presented next.</p>
<p>Human Win Rate Comparison GPT-4 [119] is widely used as an evaluator in natural language and vision language processing, as seen in recent modern benchmarks like MM-Bench and MM-Vet.Recent studies [169] have demonstrated that ChatGPT-based evaluation is more closely aligned with human preferences compared to traditional metrics.With GPT4-turbo, the standard deviation of 3D MM-Vet is less than 0.1.To further verify the soundness of the models' response, we also conduct human evaluation and report the win rate in Fig. 9, where ShapeLLM demonstrates superior preference by humans.This is a 3D model of a traditional grand piano, painted in a rich black hue.The piano, known for producing a wide range of sounds, boasts a full set of white and black keys, indicating a full octave is available.A solitary lamp, possibly to be used for reading or near vision work, rests on its rack.The piano, with its gleaming black surface, forms a striking contrast against the lamp.This model suggests uses in professional settings, such as music performances and practice, as well as leisure activities like reading and studying.</p>
<p>(b) PointLLM</p>
<p>Describe the 3D object in detail.</p>
<p>Is there any other object in it?</p>
<p>Yes, there is a set of keys in the grand piano.The keys are arranged in a horizontal row, and the body has a smooth surface with a glossy finish.
User User original mesh
(only for reference)
point cloud input
The 3D model presents a grand piano, known for its elegant and richly textured design.Characterized by its black and white keys, the piano showcases a legsupporting stand at one end and a music rack to hold the sheet music at the other end.The piano, typically used in classical music, appears to be full-sized and is likely capable of producing a wide range of sounds.</p>
<p>(c) ShapeLLM Visual Hallucination Visual hallucination is a well-known issue in LLMs and MLLMs that generate non-existent objects or identities from the input data, significantly compromising their multimodal comprehension capabilities [26,56,94,204] and may pose safety risks [113,144].Recent research [197] suggests that hallucination may stem from biases in training data, particularly within supervised fine-tuning data, or inappropriate generation strategies.In Fig. 10, we qualitatively demonstrate the illusion evaluation of ShapeLLM compared to other methods.We assess the model's ability to counteract illusions by prompting it with detailed captions and misleading questions.The results in Fig. 10 demonstrate that previous methods Point-Bind&amp;Point-LLM [57] and PointLLM [172] suffer from the problems of mis-recognition and mis-associating non-existing identities.</p>
<p>Language-Only Baseline To verify whether the capabilities of our model stem solely from large language models rather than from 3D perception abilities, we also conducted a Language-Only Baseline evaluation.This involved testing the performance of 3D MMVet on large language models such as ChatGPT [118] and LLaMA [156].Results is shown in Tab. 9. Our intentional usage of pronouns (e.g., "it") for referring to unknown objects in questions leads to poor performance, and LLaMA hallucinates but scores higher than ChatGPT, which rejects to answer without specific inputs Fig. 11.</p>
<p>Ground Truth:</p>
<p>A lesser force.</p>
<p>Ground Truth:</p>
<p>It's a black bowler hat.</p>
<p>Not input to the model Only for reference Fig. 11: Language-only baseline analysis.</p>
<p>A.3 Representation Learning with ReCon ++ Linear SVM Linear SVM evaluation [8,158] can be used to evaluate the discriminative quality of pretrained features [54].The results on ModelNet40 are shown in Tab.10.The results show that our ReCon ++ outperforms both plain and hierachical Transformer methods by a clear margin.Few-Shot 3D Object Recognition Few-shot learning is critical for evaluating the representation transferring capabilities in data and training efficiency.We conduct few-shot 3D object recognition experiments on ModelNet40, and the results are shown in Tab.11.Our ReCon ++ achieves state-of-the-art performance in all the benchmarks compared to previous works.96.9 Â± 1.9 98.3 Â± 1.5 93.0 Â± 4.0 95.4 Â± 3.1 ReCon [135] 97.3 Â± 1.9 98.9 Â± 1.2 93.3 Â± 3.9 95.8 Â± 3.0 PointGPT [18] 98.0 Â± 1.9 99.0 Â± 1.0 94.1 Â± 3.</p>
<p>ReCon++ Key Modifications Analysis</p>
<p>We conduct an ablation study on the two key modifications of ReCon ++ , namely scaling up and multi-view alignment, and the results are presented in Tab.12.The results demonstrate that: i) scaling up 3D representation is critical for both 3D representation learning, and stonger 3D representation understanding brought by ReCon ++ consistently yields better 3D multimodal comprehension; ii) the proposed multi-view distillation further leads to significant improvement.Unlike classification or regression tasks, language generation tasks lack a definitive ground truth that can comprehensively cover diverse real-life scenarios.Therefore, evaluating the alignment of model-generated results with the question and assessing their appropriateness becomes a challenging problem, requiring a reasonable quantitative score.Fortunately, we have observed the recent surge in the popularity of GPT, providing us with a dependable tool for conducting open-ended evaluations.</p>
<p>To enhance the performance of GPT, we employ a few-shot style in-context prompt.This involves feeding GPT with prompts from evaluative examples and instructing it to generate scores.Specifically, we present prompts to obtain a score ranging from 0 to 1, indicating the degree of similarity between the modelgenerated answers and the ground truths we provided.When implementing this approach, we observed that results generated multiple times may vary a lot.To address it, we apply the same evaluation setting to a single answer for K iterations, obtaining the average result as the final score for a precise answer.The score of an answer S a and the total score S t of answer set A are calculated by:
S a = K i=1 s ai K , S t = aâˆˆA S a N .
Here we set K = 5, and s ai is the score of the i th test of answer a.The average score for a specific capability is the sum of scores in category C answer set A C :
S c = aâˆˆA C S a N c ,
where N c is the number of answers in each capability set.</p>
<p>To mitigate excessive standard deviation, we opt for GPT-4 in a series of K scoring rounds to get rounds of outputs with a standard deviation below 0.1.This choice is motivated by the enhanced stability offered by GPT-4 [119], in contrast to GPT-3.5 [118], where scores across different rounds exhibit significant variability.</p>
<p>B.2 Analysis</p>
<p>The 3D MM-Vet evaluation benchmark consists of 5 different categories of questions.In Fig. 13 we report the distribution of problem categories.The knowledge and General Visual Recognition parts contain multiple subparts that comprehensively evaluate these capacities and thus hold higher proportions.Fig. 14 shows an example of how we prompt GPT-4 for 3D MM-Vet evaluation.Fig. 15 and Fig. 16 illustrate additional examples of 3D MM-Vet Q&amp;As.</p>
<p>B.3 ChatGPT Costs</p>
<p>In constructing the Supervised Finetuning dataset for ShapeLLM and conducting inference on 3D MMVet using the GPT-4 or GPT-4V API, we have roughly estimated the costs.For ShapeLLM's training data, which contains over 50k Q&amp;A pairs, with each request yielding 5-6 Q&amp;A pairs, the estimated cost is approximately $900.As for inference on 3D MMVet, with only 232 samples and averaging five requests per sample, the cost is estimated to be around $12.</p>
<p>C Implementation details</p>
<p>ReCon ++ Following the standard ViT [39] architecture, we design four different model structures consistent with prior work [97,135,203].The model parameters are shown in Tab.14.Following OpenShape [97], we employ four datasets as pretraining data, namely Objaverse [30], ShapeNet [13], ABO [23], and 3D-FUTURE [44].Each point cloud sample has a size of 10,000Ã—6, where the first three dimensions represent xyz coordinates, and the latter three dimensions represent rgb values.Regarding the masked modeling strategy, we experimented with both random masking strategies and the latest causal masking strategy.Using causal masking as initialization significantly improves transfer learning capability, as shown in the ablation experiments in Tab. 15.Specifically, the point encoder of ShapeLLM still employs the original local-guided stop-gradient strategy [135].Additionally, to enhance global classification and retrieval capabilities, we backpropagate gradients from the global branch to the local branch in open vocabulary zero-shot experiments, as demonstrated in the ablation experiments in Tab. 15.</p>
<p>ShapeLLM</p>
<p>We use the LLaMA model [156] as our LLM backbone, with the 7B and 13B Vicuna-1.1 [22] checkpoint as the default settings.We partitioned the point clouds into 512 patches using furthest point sampling and k-nearest neighbors.Similar to other MLLMs [36,96,172], we employ a 3-layer MLP with GELU [64] as the projector, with hidden layer sizes of 1,024 and 2,048, respectively.Note that different projector parameters are utilized for absolute positional encoding, local, and global features.Through training the projector, multi-scale and multi-mode features of the point cloud are mapped into the text space.After adding two special tokens, the vocabulary size becomes 32,003.</p>
<p>D Training details</p>
<p>ReCon ++ Due to the sensitivity of the Chamfer Distance [43] loss to accuracy, all experiments were conducted at FP32 precision using 8 Ã— 80G A800 GPUS.We still use the strategy of contrast with reconstruct [135].To save parameter tuning time and improve performance, we divide the training process into two stages: the reconstruction stage based on mask modeling and the cross-modal alignment stage based on knowledge distillation.For transfer learning classification tasks, ReCon ++ is pretrained on 1,024 points.For zero-shot tasks and ShapeLLM tasks, ReCon ++ is pretrained on 10,000 points.Further details regarding the hyperparameter settings are documented in Tab.16.</p>
<p>ShapeLLM All experiments were conducted using 8 Ã— 80G A800 GPUs with a BF16 data type.During the multimodal alignment stage, we train our model for one epoch with a batch size 256 and a learning rate 2e-3.During the instruction tuning stage, we train our model for one epoch with a batch size of 128 and a learning rate 2e-5.Throughout both stages, we employ flash-attention [27], the AdamW [107] optimizer, and a cosine learning rate scheduler [106].For the entire training process, the 7B and 13B models require approximately 10 and 20 hours, respectively.Further hyper-parameters are documented in Tab.16.Various methods have been proposed to tackle 3D Representation Learning, including point-based [132,133], voxel-based [115], and multiview-based approaches [61,149].Point-based methods [41,137] have gained prominence in object classification [157,170] due to their sparsity yet geometry-informative representation.On the other hand, voxel-based methods [31,136,183] offer dense representation and translation invariance, leading to a remarkable performance in object detection [24] and segmentation [3,182].The evolution of attention mechanisms [159] has also contributed to the development of effective representations for downstream tasks, as exemplified by the emergence of 3D Transformers [41,105,114].Notably, 3D self-supervised representation learning has garnered significant attention in recent studies.PointContrast [171] utilizes contrastive learning across different views to acquire discriminative 3D scene representations.Innovations such as Point-BERT [186] and Point-MAE [124] introduce masked modeling [32,63] pretraining into the 3D domain.ACT [37] pioneers cross-modal geometry understanding through 2D or language foundation models such as CLIP [138] or BERT [32].Following ACT, ReCon [135] further proposes a learning paradigm that unifies generative and contrastive learning, which can be applied to both single-modal or cross-modal settings.Additionally, leveraging foundation vision-language models like CLIP [37,138] has spurred the exploration of a new direction in open-world 3D representation learning.This line of work seeks to extend the applicability and adaptability of 3D representations in diverse and open-world/vocabulary scenarios by distilling the open-world knowledge within foundation models [33,34,42,109,127,188], with which it is now possible to perceive the 3D physical scenes using human languages.</p>
<p>F Future Works</p>
<p>ShapeLLM has made significant progress in advancing 3D shape understanding and embodied perception through MLLMs.Future endeavors aim to scale up embodied understanding training using datasets larger than GAPartNet, potentially leading to open-vocabulary part-level comprehension, including 6-DoF pose estimation.To this end, the first possibility is to empower the training data and benchmarking data with more advanced MLLMs such as GPT4-o [121], which are more human-aligned intelligent agents [128,169].Excitingly, there is a vision to establish a unified framework capable of comprehending not only 3D shapes but also entire 3D scenes.To enhance real-world applications on robots, a promising approach involves a robotics co-design that effectively connects 3D representations with downstream language-based tasks [20,74,80].Additionally, addressing efficiency for real-time deployment is crucial, emphasizing techniques like model compression [38,78,[189][190][191].</p>
<p>Fig. 2 :
2
Fig. 2: Overview of our ShapeLLM framework.(a) The introduced ReCon ++ pipeline incorporates the required 3D encoder.(b) The comprehensive design of the MLLM, featuring an instruction-mode tokenizer and the integration of an aligned multi-modal representation, equips the MLLM with the capability to effectively handle 3D vision language tasks.</p>
<p>You are a 3D Computer visual assistant.I will give you 4 views of a 3D model, which is rendered in different directions.Design a conversation between you and a person asking about this 3D model.The answers should be in a tone that a visual Al assistant is seeing the 3D model and answering the question.Ask diverse questions and give corresponding answers.You need to consider the spatial relation in different views.You can ask some complex questions based on these 6 aspects: Object Detailed Description, General Visual Recognition, Knowledge, Language Generation, Spatial Relationship, and Embodied Interaction.I will give you several examples, and please generate questions and answer pairs with the format of my examples.For example: Q: <Example Q1>A: <Example A1> Q: <Example Q2> A: <Example A2> Provide detailed answers when answering complex questions.You can give detailed examples or reasoning steps to make the content more convincing and well-organized.You can include multiple paragraphs if necessary.Do not ask about uncertain details.Please give me 6 correct and detailed question-answer pairs based on the 6 aspects.You should try to avoid asking "Q: Is there xxx?A: Yes, xxx.".</p>
<p>[Point Clouds] [Question 1 1 ][Question 2 2 ]
1122
Type: Knowledge] What is this woman's occupation?[Answer She is a medical worker.Type: General Visual Recognition] What clothes are the women wearing and what colors are they?[Answer She is wearing protective clothing and two medical gloves.the protective clothing is blue while the medical gloves are green.</p>
<p>Fig. 3 :
3
Fig. 3: Qualitative visualization of the instruction-following and 3D MM-Vet data.</p>
<p>Fig. 4 :
4
Fig. 4: Qualitative examples of the embodied interaction data.</p>
<p>Grab and pull the line fixed handle in the direction that allows it to turn or move.Yes! Look for the hinge door in the storage furniture, precisely located within the bounding box [[-0.90, -0.30, -0.46], [-0.90, -0.30, 0.39], [-0.40, -0.65, 0.39], [-0.40, -0.65, -0.46], [-0.88, -0.28, -0.46], [-0.88, -0.28, 0.39], [-0.38, -0.63, 0.39], [-0.38, -0.63, -0.46]].It's a movable element you can interact with.</p>
<p>Fig. 5 :
5
Fig. 5: Selected multimodal dialogue examples.ShapeLLM possesses robust capabilities in knowledge representation, reasoning, and instruction-following dialogue.With its powerful point cloud encoder ReCon ++ , ShapeLLM can even make accurate predictions about minute interactive components, e.g., handle.The rendered mesh images are solely for visual reference here and do not constitute input data.</p>
<p>single-view point cloud original mesh (only for reference) As a robot, Where should you position the robotic arm to grasp it?As a robot, you should position the robotic arm around the middle of the spherical object to grasp it securely.User clean point cloud (only for reference) single-view point cloud original mesh (only for reference) Give me some steps to use this thing to get a basin of water.To use this thing to get a basin of water, you would follow these steps: 1. Turn the handle counter-clockwise to open the valve and allow water to flow from the spout.2. Fill the basin with water.3. Once the basin is full, turn the handle clockwise to close the valve and stop the flow of water.4. Enjoy your freshly filled basin of water.User single-view point cloud original mesh (only for reference) Are all the chairs completely placed under the table?</p>
<p>Fig. 6 :
6
Fig. 6: 3D multimodal dialogue using single-view point cloud inputs.All answers are generated by ShapeLLM-13B with single-view occluded inputs.ShapeLLM achieves outstanding robustness against such commonly met occlusion in the real world.</p>
<p>Table 6 :
6
3D referring expression grounding on GAPartNet[50].Accuracy with an IoU threshold of 0.25 is reported.â€  : Fine-tuned on GAPartNet images.â€¡ : Inference with 3 in-context demonstrations.ShapeLLM-13B 3D Point Cloud 7.6 26.7 11.5 6.7 6.8 11.1 11.7</p>
<p>Fig. 7 :
7
Fig. 7: Part understanding examples of unseen objects beyond GAPartNet.</p>
<p>Fig. 7
7
Fig. 7 shows the part understanding examples of unseen objects.While ShapeLLM's 6-DoF pose estimation and spatial awareness are trained on GAPartNet, which primarily consists of indoor articulated furniture objects.It has demonstrated promising generalization potential of spatial understanding on the open-world objects, paving ways for scaling up spatial awareness training.</p>
<p>Fig. 8 :
8
Fig. 8: Ablation study on visual prompt number.The performance of ShapeLLM-13B on 3D MM-Vet is reported.</p>
<p>Fig. 9 :
9
Fig. 9: Win rate comparison.</p>
<p>Fig. 10 :
10
Fig. 10: Qualitative comparison examples of visual hallucination.</p>
<p>8 Fig. 12 :
812
Fig. 12: Visualization of multi-view query results.The distinct colors serve to denote distinct image queries.</p>
<p>Fig. 13 :
13
Fig. 13: The number of diverse questions of core VL capabilities on 3D MM-Vet.Rec: General Visual Recognition, Know: Knowledge, Gen: Language Generation, Spat: Spatial Awareness, Emb: Embodied Interaction.</p>
<p>Table 1 :
1
[18]]tuned 3D recognition on ScanObjectNN and ModelNet40.Overall accuracy (%) with voting[101]is reported.â€ :Results with a post-pretraining stage[18].
MethodScanObjectNN OBJ_BG OBJ_ONLY PB_T50_RS 1k P ModelNet40 8k PSupervised Learning OnlyPointNet [132]73.379.268.089.290.8PointNet++ [133]82.384.377.990.791.9DGCNN [163]82.886.278.192.9-PointMLP [112]--85.494.5-PointNeXt [137]--87.794.0-with Self-Supervised Representation LearningPoint-BERT [186]87.4388.1283.0793.293.8Point-MAE [124]90.0288.2985.1893.894.0Point-M2AE [192]91.2288.8186.4394.0-Point2Vec [187]91.290.487.594.8-ACT [37]93.2991.9188.2193.794.0TAP [164]--88.594.0-VPP [136]93.1191.9189.2894.194.3I2P-MAE [195]94.1591.5790.1194.1-ULIP-2 [176]--91.5--ReCon [135]95.3593.8091.2694.594.7PointGPT-B  â€  [18]95.895.291.994.494.6PointGPT-L  â€  [18]97.296.693.494.794.9ReCon ++ -B  â€ 98.6296.2193.3494.694.8ReCon ++ -L  â€ 98.8097.5995.2594.895.0</p>
<p>Table 3 :
3
Zero-shot 3D multimodal comprehension of core VL capabilities in 3D context on 3D MM-Vet.Rec: General Visual Recognition, Know: Knowledge, Gen: Language Generation, Spat: Spatial Awareness, Emb: Embodied Interaction.
MethodInputRec Know Gen Spat Emb TotalLLaVA-13B [96]1-View 2D Image 40.0 55.3 51.3 43.2 51.1 47.9DreamLLM-7B [36]4-View 2D Image 42.2 54.4 50.8 48.9 54.5 50.3GPT-4V [120]1-View 2D Image 53.7 59.5 61.1 54.7 59.0 57.4GPT-4V [120]4-View 2D Image 65.1 69.1 61.4 52.9 65.5 63.4PointBind&amp;LLM [57] 3D Point Cloud 16.9 13.0 18.5 32.9 40.4 23.5PointLLM-7B [172]3D Point Cloud 40.6 49.5 34.3 29.1 48.7 41.2PointLLM-13B [172]3D Point Cloud 46.6 48.3 38.8 45.2 50.9 46.6ShapeLLM-7B3D Point Cloud 45.7 42.7 43.4 39.9 64.5 47.4ShapeLLM-13B3D Point Cloud 46.8 53.0 53.9 45.3 68.4 53.1</p>
<p>Table 4 :
4i) 3D MM-Vet. Tab. 3 shows thedetailed results of ShapeLLM on dif-ferent tasks of 3D MM-Vet. It is ob-served that ShapeLLM significantlyoutperforms PointLLM [172] acrossvarious metrics, particularly in Em-bodied Tasks. This substantiates ourmodel's versatile capability in address-ing real-world tasks.Method3D MM-Vet-C VariantsClean Single-View Jitter RotatePointBind&amp;LLM [57] 23.520.419.719.5PointLLM-7B [172]41.233.638.840.6PointLLM-13B [172] 46.641.342.344.2ShapeLLM-7B47.438.345.8 42.7ShapeLLM-13B 53.143.647.8 49.3
Zero-shot 3D multimodal comprehension of robustness on 3D MM-Vet-C.Clean: no corruptions.Single-View: randomly select a camera viewpoint within the unit sphere and generate a single viewpoint within the FoV on polar coordinates.Jitter: Gaussian jittering with noise Ïµ âˆ¼ N (0, Ïƒ 2 ) and Ïƒ = 0.01.Rotate: random SO(3) rotation sampling over X-Y-Z Euler angle (Î±, Î², Î³) âˆ¼ U (âˆ’Î¸, Î¸) and Î¸ = Ï€/6.</p>
<p>Table 5 :
5
[172]ion study on baseline improvements.Results are tested on 3D MM-Vet with the baseline model PointLLM-13B[172]using different point encoders and SFT data.
i) Improvement from encoder.First, by changing PointLLM'sencoder to ReCon ++ , a signifi-cant improvement of +4.20% isobtained. This demonstrates theEncoderSFT Data Rec Know Gen Spat Emb Totalsignificantly better 3D representa-ULIP-2 [176] PointLLM 46.6 48.3 38.8 45.2 50.9 46.6tion extraction of ReCon ++ com-ReCon ++ PointLLM 47.5 52.8 43.6 44.9 54.5 50.8pared to ULIP-2. It is consistentReCon ++Ours 46.8 53.0 53.9 45.3 68.4 53.1with previous findings in Tab. 1and Tab. 2 that ReCon ++ outperforms ULIP-2 by a large margin regarding 3Drepresentation transferring learning and zero-shot learning.</p>
<p>Table 7 :
7
Ablation study on the dedicated designs of ShapeLLM architecture.The performance of multimodal comprehension on 3D MM-Vet and referring expression grounding on GAPartNet with ShapeLLM-13B is reported.Note that E global is calculated with both global queries and cross-attention with local 3D embeddings.
EAPE Eprompt Elocal Eglobal 3D MM-Vet GAPartNetâœ“âœ—âœ—âœ—30.812.3âœ“âœ“âœ—âœ—32.011.4âœ—âœ—âœ“âœ—42.210.0âœ—âœ“âœ—âœ“50.310.5âœ“âœ—âœ“âœ“52.310.5âœ“âœ“âœ—âœ“50.311.7âœ—âœ—âœ—âœ“52.411.7âœ—âœ—âœ“âœ“49.610.1âœ—âœ“âœ“âœ“51.710.1âœ“âœ“âœ“âœ“53.111.7</p>
<p>Table 8 :
8
Generative 3D recognition and captioning.The accuracy (%) averaged under the instruction-typed prompt "What is this?" and the completion-typed prompt "This is an object of" is reported.
MethodInputClassification MN-40 Objaverse GPT-4 S-BERT SimCSE CaptioningInstructBLIP-7B [25]1-View 2D Image25.5143.5045.34 47.4148.48InstructBLIP-13B [25] 1-View 2D Image28.6934.2544.97 45.9048.86LLaVA-7B [96]1-View 2D Image39.7150.0046.71 45.6147.10LLaVA-13B [96]1-View 2D Image36.5951.7538.28 46.3745.903D-LLM [65]3D Object + Multi-View 2D Image-45.2533.42 44.4843.68PointLLM-7B [172]3D Point Cloud52.6353.0044.85 47.4748.55PointLLM-13B [172]3D Point Cloud52.7854.0048.15 47.9149.12ShapeLLM-7B3D Point Cloud53.08 54.50 46.92 48.20 49.23ShapeLLM-13B3D Point Cloud52.96 54.00 48.94 48.52 49.98</p>
<p>Table 9 :
9
Language-only baseline results on 3D MM-Vet.When grasping an object, you should apply a lesser force to ensure that you don't drop it or cause any damage to the object or yourself.A greater force could cause the object to slip out of your grasp or even break, so it's important to be gentle and careful when handling objects.
MethodRecKnowGenSpatEmbTotalLLaMA2-7B-Chat11.810.622.114.625.816.2GPT-3.5-Turbo2.95.417.310.227.811.7GPT-4-Turbo1.73.616.16.626.09.8ShapeLLM-7B36.535.136.935.152.539.0ShapeLLM-13B46.853.053.945.368.453.1</p>
<p>.5 (Score: 32)</p>
<p>When grasping an object, the force you apply should be just enough to securely hold the object without damaging it.If you apply too much force, you may end up crushing or breaking the object.Conversely, if you apply too little force, the object may slip out of your grasp.So, the key is to find the right balance and apply a moderate amount of force when grasping an object.To provide the best advice, I'll need more context about what "it" refers to.The force needed when grasping depends on the object's material, weight, size, and the purpose for which you are holding it.Could you please specify what you're referring to?It seems like you are referring to something specific, but I don't have enough context to accurately answer your question.Could you please provide more details or specify what "it" refers to?
ChatGPT-4 (Score: 6)Question:LLaMA2 (Score: 88)ChatGPT-3.5 (Score: 0)ChatGPT-4 (Score: 0)What is on its head?Answer: A hat.Could you please provide more context orspecify what "it" you are referring to so I canassist you better?</p>
<p>Table 10 :
10
Linear SVM classification on ModelNet40.Overall accuracy (%) without voting is reported.
MethodHierachical ModelNet40Point-BERT [186]âœ—87.4PointMAE [124]âœ—91.0PointM2AE [192]âœ“92.9ACT [37]âœ—93.1I2P-MAE [195]âœ“93.4ReCon [135]âœ—93.4ReCon ++âœ—93.6</p>
<p>Table 11 :
11
Few-shot classification results on ModelNet40.Overall accuracy (%) without voting is reported.
Method5-way 10-shot 20-shot10-way 10-shot 20-shotTransformer [159] 87.8 Â± 5.2 93.3 Â± 4.3 84.6 Â± 5.5 89.4 Â± 6.3Point-BERT [186] 94.6 Â± 3.1 96.3 Â± 2.7 91.0 Â± 5.4 92.7 Â± 5.1Point-MAE [124] 96.3 Â± 2.5 97.8 Â± 1.8 92.6 Â± 4.1 95.0 Â± 3.0Point-M2AE [192] 96.8 Â± 1.8 98.3 Â± 1.4 92.3 Â± 4.5 95.0 Â± 3.0ACT [37]96.8 Â± 2.3 98.0 Â± 1.4 93.3 Â± 4.0 95.6 Â± 2.8VPP [136]</p>
<p>Table 12 :
12
Ablation study on scaling and multi-view alignment.
scalingmulti-viewZero-Shot3D MM-Vetâœ—âœ—6.715.8âœ—âœ“10.321.9âœ“âœ—51.548.2âœ“âœ“53.753.1</p>
<p>Table 13 :
13
Sample categories of 3D MM-Vet data.
Category Characters Life Art Architecture AnimalsNumber111610139</p>
<p>Table 14 :
14
[39]n ++ model variants, which follow ViT[39].
ModelLayers Hidden size MLP size HeadsReCon ++ -S 1238415366ReCon ++ -B 12768307212ReCon ++ -L 241024409616</p>
<p>Table 15 :
15
[30]]ion study on mask type &amp; stop gradient.transfer:fine-tuned3Drecognitionon ScanObjectNN[157].zero-shot: zero-shot 3D recognition on Objaverse-LVIS[30].All experiments are conducted on ReCon ++ -L and ShapeLLM-13B.
Mask Type Stop Grad Fine-Tune Zero-Shot 3D MM-VetRandomâœ“92.552.853.1Randomâœ—93.653.752.9Causalâœ“95.349.850.7Causalâœ—92.851.051.6</p>
<p>Table 16 :
16
Training recipes for ReCon ++ and ShapeLLM.
ReCon ++ShapeLLMConfigHyBrid/Ensembled ScanObjectNN ModelNet Cap3D LVIS/GAPartNetoptimizerAdamWAdamWAdamW AdamWAdamWlearning rate5e-52e-51e-52e-32e-5weight decay5e-25e-25e-2--learning rate schedulercosinecosinecosinecosinecosinetraining epochs30030030031warmup epochs1010100.030.03batch size5123232256128drop path rate0.10.20.2--number of points1024/1000020481024/10000 1000010000number of point patches64/51212864/512512512point patch size3232323232augmentationRot&amp;Scale&amp;TransRotScale&amp;Trans--GPU device8Ã—A8001Ã—A8001Ã—A800 8Ã—A8008Ã—A800E Additional Related Work
E.1 3D Representation Learning</p>
<p>URL &amp; License.
AcknowledgmentsThe work was supported by the Dushi Program from Tsinghua University, the National Key R&amp;D Program of China (2022YFB2804103), and the National Science and Technology Major Project of China (2023ZD0121300).A Additional ExperimentsA.1 ShapeLLM Architecture Let F Î¸ be the multimodal LLM parameterized by Î¸, we use a ReCon ++ encoder H Ï• as ShapeLLM's 3D point cloud encoder, followed by three MLP projection layers M Î¶ local and M Î¶ global for 3D embedding projection of ReCon ++ 's local and global representations, respectively.To facilitate geometry-necessary tasks like 6-DoF pose estimation, we use absolute position encoding (APE) with an MLP projection M Î¶ APE to provide additional precise low-level geometric information.Given the original 3D point cloud inputs P = {p i |i = 1, 2, . . ., N } âˆˆ R N Ã—3 with N coordinates encoded in a (x, y, z) Cartesian space.Following previous works[37,135,186], N s seed points are first sampled using farthest point sampling (FPS).The point cloud P is then grouped into N s neighborhoods N = {N i |i = 1, 2, . . ., N s } âˆˆ R NsÃ—KÃ—3 with group centroids from the seed point set P s .The APE representation can be written as(The local and transformation-invariant 3D embeddingsfor P s i , i = 1, 2, . . ., N s is used as 3D token embeddings of ReCon ++ , where Î¦ Î³ is a per-point MLP point feature extractor[132,133]and Î¾ i,j is the feature of j-th neighbour point p i,j in the neighbourhood N i .Let {g image q } G q=1 be G multi-view image global queries and g text be the global text query.ReCon ++ outputs the local and global 3D point cloud representations by taking 3D embeddings and global queries as inputs:and the representation to ShapeLLM is:In addition, inspired by prefix-tuning[87]and dream queries[36], we append Formally, the encoded 3D representations to ShapeLLM can be written as:Input Components Tab. 7 shows the ablation study of each input component by supervised fine-tuning with different input representations, demonstrating that it is necessary to employ all designs for achieving decent performance on both 3D comprehension and real-world grounding.[System Prompt]You are a helpful AI assistant.[User Prompt]Now I will give you a question, the type of the question, an answer from model, and an answer from label.All you need to do is focus on these two answers and figure out whether they are saying the same thing about the specific type of question.Your response should only be a confidence score ranging from 0 to 100.Remember the confidence score is to evaluate how much two answers are describing the same thing.Your response confidence score should follow the scoring standard of the prompt I gave.Firstly I will give you several question-and-answer pairs as long as their confidence score: question 1: How many oranges will there be if  Step 1: Pour the milk into the bowl.Step 2: Put the cookies into the bowl.Step 3: Stir with a spoon.[Question 3 Type: Knowledge] Describe the physical properties of the milk.[Answer 3 ]The milk is a kind of liquid with a white color, whose density and boiling point is higher than water while the freezing point is lower than water, has a mild, slightly sweet odor and taste.[Point Clouds][Question 1 Type: Embodied Interaction] Please give me steps to roll the dice until it hits 6.[Answer 1 ]Step 1: Roll the dice.Step 2: If the dice hits 1, 2, 3, 4, or 5, roll it again.Step 3: If the dice hits 6, stop rolling and show the roll of the dice.[Question 2 Type: Spatial Awareness] Change the 1 and 5 on the object, which number will be the opposite of the number 5?Change the number 1 and number 5 on this dice, number 6 will be on the opposite of number 5.[Question 3 Type: Knowledge] At which places will this object be used?This object is a dice, so it might be used at places like bars and gambling houses.[Point Clouds][Question 1 Type: Embodied Interaction] I want to change the place of the spoon and the fork, please give me steps.[Answer 1 ]Step 1: Pick up the fork and the spoon.Step 2: Put down the spoon at the place of the fork.Step 3: Put down the fork at the place of the spoon.[Question 2 Type: Language Generation] What are these objects commonly used to do in usual life?Please speak in detail.[Answer 2 ] Forks, spoons, and table knives are common eating utensils used in everyday life for various purposes.Forks are primarily used for piercing and picking up solid food item.Spoons are used for scooping and conveying liquids, semi-liquids, and foods with a sauce or broth.Table knives, also known as dinner knives, are used for cutting and slicing food on your plate.They are typically not as sharp as kitchen knives, as their primary purpose is to assist with cutting while dining.Step 1:Clamp the rusty barrel.Step 2:Take it down from the height.Step 3:Turn around and take it away from the pack.The barrels are stacked in two layers, the bottom layer is three yellow barrels, and the top layer is a yellow barrel and a rusted barrel in the gap between the bottom three buckets.
Learning representations and generative models for 3d point clouds. P Achlioptas, O Diamanti, I Mitliagkas, L J Guibas, Int. Conf. Mach. Learn. (ICML). 2018</p>
<p>Flamingo: a visual language model for few-shot learning. J Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, R Ring, E Rutherford, S Cabi, T Han, Z Gong, S Samangooei, M Monteiro, J Menick, S Borgeaud, A Brock, A Nematzadeh, S Sharifzadeh, M Binkowski, R Barreira, O Vinyals, A Zisserman, K Simonyan, Adv. Neural Inform. Process. Syst. (NeurIPS). 2022</p>
<p>3d semantic parsing of large-scale indoor spaces. I Armeni, IEEE/CVF Conf. Comput. Vis. Pattern Recog.O Sener, IEEE/CVF Conf. Comput. Vis. Pattern Recog.A R Zamir, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Jiang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.I Brilakis, IEEE/CVF Conf. Comput. Vis. Pattern Recog.M Fischer, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Savarese, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2016CVPR</p>
<p>Sequential modeling enables scalable learning for large vision models. Y Bai, IEEE/CVF Conf. Comput. Vis. Pattern Recog.X Geng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.K Mangalam, IEEE/CVF Conf. Comput. Vis. Pattern Recog.A Bar, IEEE/CVF Conf. Comput. Vis. Pattern Recog.A L Yuille, IEEE/CVF Conf. Comput. Vis. Pattern Recog.T Darrell, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Malik, IEEE/CVF Conf. Comput. Vis. Pattern Recog.A A Efros, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2024CVPR</p>
<p>METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. S Banerjee, A Lavie, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005. the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005Ann Arbor, Michigan, USAJune 29, 2005. 2005</p>
<p>Improving image generation with better captions. J Betker, G Gabriel, L Jing, T Brooks, J Wang, L Li, L Ouyang, J Zhuang, J Lee, Y Guo, W Manassra, P Dhariwal, C Chu, Y Jiao, A Ramesh, 2023</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N S Chatterji, A S Chen, K Creel, J Q Davis, D Demszky, C Donahue, M Doumbouya, E Durmus, S Ermon, J Etchemendy, K Ethayarajh, L Fei-Fei, C Finn, T Gale, L Gillespie, K Goel, N D Goodman, S Grossman, N Guha, T Hashimoto, P Henderson, J Hewitt, D E Ho, J Hong, K Hsu, J Huang, T Icard, S Jain, D Jurafsky, P Kalluri, S Karamcheti, G Keeling, F Khani, O Khattab, P W Koh, M S Krass, R Krishna, R Kuditipudi, CoRR abs/2108.07258On the opportunities and risks of foundation models. 2021</p>
<p>A training algorithm for optimal margin classifiers. B E Boser, I Guyon, V Vapnik, ACM Conf. Comput. Learn. Theory (COLT). ACM1992</p>
<p>Recognition of 3-d objects from multiple 2-d views by a self-organizing neural architecture. G Bradski, S Grossberg, From Statistics to Neural Networks: Theory and Pattern Recognition Applications. Springer1994</p>
<p>Shape google: Geometric words and expressions for invariant shape retrieval. A M Bronstein, M M Bronstein, L J Guibas, M Ovsjanikov, ACM Trans. Graph. 301202011</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Adv. Neural Inform. Process. Syst. (NeurIPS). 2020</p>
<p>Endto-end object detection with transformers. N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, S Zagoruyko, Eur. Conf. Comput. 2020</p>
<p>A X Chang, T A Funkhouser, L J Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, J Xiao, L Yi, F Yu, CoRR abs/1512.03012Shapenet: An information-rich 3d model repository. 2015</p>
<p>GOAT: GO to any thing. M Chang, RSST Gervet, RSSM Khanna, RSSS Yenamandra, RSSD Shah, RSSS Y Min, RSSK Shah, RSSC Paxton, RSSS Gupta, RSSD Batra, RSSR Mottaghi, RSSJ Malik, RSSD S Chaplot, RSSRobotics: Science and Systems. 2024</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Z Xu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Kirmani, IEEE/CVF Conf. Comput. Vis. Pattern Recog.B Ichter, IEEE/CVF Conf. Comput. Vis. Pattern Recog.D Driess, IEEE/CVF Conf. Comput. Vis. Pattern Recog.P Florence, IEEE/CVF Conf. Comput. Vis. Pattern Recog.D Sadigh, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Guibas, IEEE/CVF Conf. Comput. Vis. Pattern Recog.F Xia, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2024CVPR</p>
<p>Scanrefer: 3d object localization in RGB-D scans using natural language. D Z Chen, A X Chang, M NieÃŸner, Eur. Conf. Comput. 2020</p>
<p>Scan2cap: Context-aware dense captioning in RGB-D scans. D Z Chen, A Gholami, M NieÃŸner, A X Chang, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2021</p>
<p>Pointgpt: Autoregressively generative pre-training from point clouds. G Chen, M Wang, Y Yang, K Yu, L Yuan, Y Yue, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>Shikra: Unleashing multimodal llm's referential dialogue magic. K Chen, Z Zhang, W Zeng, R Zhang, F Zhu, R Zhao, CoRR abs/2306.151952023</p>
<p>Sugar: Pre-training 3d visual representations for robotics. S Chen, R Garcia, I Laptev, C Schmid, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Pali-x: On scaling up a multilingual vision and language model. X Chen, J Djolonga, P Padlewski, B Mustafa, S Changpinyo, J Wu, C R Ruiz, S Goodman, X Wang, Y Tay, S Shakeri, M Dehghani, D Salz, M Lucic, M Tschannen, A Nagrani, H Hu, M Joshi, B Pang, C Montgomery, P Pietrzyk, M Ritter, A J Piergiovanni, M Minderer, F Pavetic, A Waters, G Li, I Alabdulmohsin, L Beyer, J Amelot, K Lee, A P Steiner, Y Li, D Keysers, A Arnab, Y Xu, K Rong, A Kolesnikov, M Seyedhosseini, A Angelova, X Zhai, N Houlsby, R Soricut, Int. Conf. Learn. Represent. (ICLR). 2023</p>
<p>W L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>Abo: Dataset and benchmarks for real-world 3d object understanding. J Collins, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Goel, IEEE/CVF Conf. Comput. Vis. Pattern Recog.K Deng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.A Luthra, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Xu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.E Gundogdu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.X Zhang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.T F Y Vicente, IEEE/CVF Conf. Comput. Vis. Pattern Recog.T Dideriksen, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Arora, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2022CVPR</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M NieÃŸner, IEEE/CVF Conf. Comput. Vis. Pattern Recog. 2017</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. W Dai, J Li, D Li, A M H Tiong, J Zhao, W Wang, B Li, P Fung, S C H Hoi, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>Plausible may not be faithful: Probing object hallucination in vision-language pre-training. W Dai, Z Liu, Z Ji, D Su, P Fung, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, Croatiathe Association for Computational Linguistics2023. May 2-6, 2023 (2023</p>
<p>Flashattention: Fast and memoryefficient exact attention with io-awareness. T Dao, D Fu, S Ermon, A Rudra, C RÃ©, Adv. Neural Inform. Process. Syst. (NeurIPS). 2022</p>
<p>Visual dialog. A Das, S Kottur, K Gupta, A Singh, D Yadav, S Lee, J M F Moura, D Parikh, D Batra, IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI). 52019</p>
<p>Commonsense knowledge mining from pretrained models. J Davison, J Feldman, A M Rush, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaNovember 3-7, 2019. 2019</p>
<p>Objaverse: A universe of annotated 3d objects. M Deitke, IEEE/CVF Conf. Comput. Vis. Pattern Recog.D Schwenk, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Salvador, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Weihs, IEEE/CVF Conf. Comput. Vis. Pattern Recog.O Michel, IEEE/CVF Conf. Comput. Vis. Pattern Recog.E Vanderbilt, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Schmidt, IEEE/CVF Conf. Comput. Vis. Pattern Recog.K Ehsani, IEEE/CVF Conf. Comput. Vis. Pattern Recog.A Kembhavi, IEEE/CVF Conf. Comput. Vis. Pattern Recog.A Farhadi, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2023CVPR</p>
<p>Voxel r-cnn: Towards high performance voxel-based 3d object detection. J Deng, S Shi, P Li, W Zhou, Y Zhang, H Li, AAAI Conf. 2021</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational LinguisticsJune 2-7, 2019. 20191</p>
<p>PLA: language-driven open-vocabulary 3d scene understanding. R Ding, J Yang, C Xue, W Zhang, S Bai, X Qi, IEEE/CVF Conf. Comput. Vis. Pattern Recog. 2023</p>
<p>Lowis3d: Language-driven open-world instance-level 3d scene understanding. R Ding, J Yang, C Xue, W Zhang, S Bai, X Qi, IEEE Trans. Pattern Anal. Mach. Intell. 2024</p>
<p>Task and motion planning with large language models for object rearrangement. Y Ding, X Zhang, C Paxton, S Zhang, IEEE/RSJ Int. Conf. Intell. Robot. and Syst. (IROS). 2023</p>
<p>DreamLLM: Synergistic multimodal comprehension and creation. R Dong, C Han, Y Peng, Z Qi, Z Ge, J Yang, L Zhao, J Sun, H Zhou, H Wei, X Kong, X Zhang, K Ma, L Yi, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning?. R Dong, Z Qi, L Zhang, J Zhang, J Sun, Z Ge, L Yi, K Ma, Int. Conf. Learn. Represent. (ICLR). 2023</p>
<p>Finding the task-optimal low-bit sub-distribution in deep neural networks. R Dong, Z Tan, M Wu, L Zhang, K Ma, Int. Conf. Mach. Learn. (ICML). 2022</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, Int. Conf. Learn. Represent. (ICLR). 2021</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S M Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, Y Chebotar, P Sermanet, D Duckworth, S Levine, V Vanhoucke, K Hausman, M Toussaint, K Greff, A Zeng, I Mordatch, P Florence, Int. Conf. Mach. Learn. (ICML). 2023</p>
<p>Point transformer. N Engel, V Belagiannis, K Dietmayer, IEEE Access. 92021</p>
<p>Point-gcc: Universal self-supervised 3d scene pre-training via geometry-color contrast. G Fan, Z Qi, W Shi, K Ma, CoRR abs/2305.196232023</p>
<p>A point set generation network for 3d object reconstruction from a single image. H Fan, H Su, L J Guibas, IEEE/CVF Conf. Comput. Vis. Pattern Recog. 2017</p>
<p>3d-future: 3d furniture shape with texture. H Fu, R Jia, L Gao, M Gong, B Zhao, S Maybank, D Tao, International Journal of Computer Vision. 1292021</p>
<p>Simcse: Simple contrastive learning of sentence embeddings. T Gao, X Yao, D Chen, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican Republic7-11 November, 2021 (2021</p>
<p>Sculpting holistic 3d representation in contrastive language-image-3d pre-training. Y Gao, Z Wang, W S Zheng, C Xie, Y Zhou, IEEE/CVF Conf. Comput. Vis. Pattern Recog. 2024</p>
<p>Planting a SEED of vision in large language model. Y Ge, Y Ge, Z Zeng, X Wang, Y Shan, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Partmanip: Learning cross-category generalizable part manipulation policy from point cloud observations. H Geng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Z Li, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Y Geng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Chen, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Dong, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Wang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2023CVPR</p>
<p>Sage: Bridging semantic and actionable parts for generalizable articulated-object manipulation under language instructions. H Geng, RSSS Wei, RSSC Deng, RSSB Shen, RSSH Wang, RSSL Guibas, RSSRobotics: Science and Systems. 2024</p>
<p>Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts. H Geng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Xu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.C Zhao, IEEE/CVF Conf. Comput. Vis. Pattern Recog.C Xu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Yi, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Huang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Wang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2023CVPR</p>
<p>Rlafford: End-to-end affordance learning for robotic manipulation. Y Geng, B An, H Geng, Y Chen, Y Yang, H Dong, IEEE Int. Conf. Robot. Autom. (ICRA). 2023</p>
<p>Imagebind: One embedding space to bind them all. R Girdhar, A El-Nouby, Z Liu, M Singh, K V Alwala, A Joulin, I Misra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>ARNOLD: A benchmark for languagegrounded task learning with continuous states in realistic 3d scenes. R Gong, J Huang, Y Zhao, H Geng, X Gao, Q Wu, W Ai, Z Zhou, D Terzopoulos, S Zhu, B Jia, S Huang, Int. Conf. Comput. Vis. (ICCV). 2023</p>
<p>Scaling and benchmarking selfsupervised visual representation learning. P Goyal, D Mahajan, A Gupta, I Misra, Int. Conf. Comput. Vis. (ICCV). IEEE2019</p>
<p>What makes a chair a chair?. H Grabner, J Gall, L V Gool, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2011</p>
<p>Detecting and preventing hallucinations in large vision language models. A Gunjal, J Yin, E Bas, AAAI Conf. 2024</p>
<p>Point-bind &amp; point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. Z Guo, R Zhang, X Zhu, Y Tang, X Ma, J Han, K Chen, P Gao, X Li, H Li, P Heng, CoRR abs/2309.006152023</p>
<p>Lvis: A dataset for large vocabulary instance segmentation. A Gupta, P Dollar, R Girshick, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2019</p>
<p>Learning rich features from RGB-D images for object detection and segmentation. S Gupta, R B Girshick, P A ArbelÃ¡ez, J Malik, Eur. Conf. Comput. Vis. 2014ECCV</p>
<p>Visual programming: Compositional visual reasoning without training. T Gupta, A Kembhavi, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2023</p>
<p>MVTN: multi-view transformation network for 3d shape recognition. A Hamdi, S Giancola, B Ghanem, Int. Conf. Comput. Vis. (ICCV). IEEE2021</p>
<p>Towards a unified view of parameter-efficient transfer learning. J He, C Zhou, X Ma, T Berg-Kirkpatrick, G Neubig, Int. Conf. Learn. Represent. (ICLR). 2021</p>
<p>Masked autoencoders are scalable vision learners. K He, X Chen, S Xie, Y Li, P DollÃ¡r, R B Girshick, IEEE/CVF Conf. Comput. Vis. Pattern Recog. 2022</p>
<p>Gaussian error linear units (gelus). D Hendrycks, K Gimpel, CoRR abs/1606.084152016</p>
<p>-llm: Injecting the 3d world into large language models. Y Hong, H Zhen, P Chen, S Zheng, Y Du, Z Chen, C Gan, Adv. Neural Inform. Process. Syst. (NeurIPS). 32023</p>
<p>J Hou, S Xie, B Graham, A Dai, M NieÃŸner, Pri3d: Can 3d priors help 2d representation learning? In: Int. Conf. Comput. Vis. (ICCV). IEEE2021</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Int. Conf. Learn. Represent. (ICLR). 2022</p>
<p>Learning how objects function via co-analysis of interactions. R Hu, O Van Kaick, B Wu, H Huang, A Shamir, H Zhang, ACM Trans. Graph. 354132016</p>
<p>Learning to predict part mobility from a single static snapshot. R Hu, W Li, O Van Kaick, A Shamir, H Zhang, H Huang, ACM Trans. Graph. 366132017</p>
<p>Interaction context (ICON): towards a geometric functionality descriptor. R Hu, C Zhu, O Van Kaick, L Liu, A Shamir, H Zhang, ACM Trans. Graph. 344122015</p>
<p>An embodied generalist agent in 3d world. J Huang, S Yong, X Ma, X Linghu, P Li, Y Wang, Q Li, S Zhu, B Jia, S Huang, Int. Conf. Mach. Learn. (ICML). 2024</p>
<p>Clip2point: Transfer CLIP to point cloud classification with image-depth pre-training. T Huang, B Dong, Y Yang, X Huang, R W H Lau, W Ouyang, W Zuo, Int. Conf. Comput. Vis. (ICCV). 2023</p>
<p>One policy to control them all: Shared modular policies for agent-agnostic control. W Huang, I Mordatch, D Pathak, Int. Conf. Mach. Learn. (ICML). 2020</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, Annu. Conf. Robot. Learn. (CoRL). 2023</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, T Jackson, N Brown, L Luu, S Levine, K Hausman, B Ichter, Annu. Conf. Robot. Learn. (CoRL). 2022</p>
<p>Do as I can, not as I say: Grounding language in robotic affordances. B Ichter, A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, D Kalashnikov, S Levine, Y Lu, C Parada, K Rao, P Sermanet, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, M Yan, N Brown, M Ahn, O Cortes, N Sievers, C Tan, S Xu, D Reyes, J Rettinghouse, J Quiambao, P Pastor, L Luu, K Lee, Y Kuang, S Jesmonth, N J Joshi, K Jeffrey, R J Ruano, J Hsu, K Gopalakrishnan, B David, A Zeng, C K Fu, Annu. Conf. Robot. Learn. (CoRL). 2022</p>
<p>. G Ilharco, M Wortsman, R Wightman, C Gordon, N Carlini, R Taori, A Dave, V Shankar, H Namkoong, J Miller, H Hajishirzi, A Farhadi, L Schmidt, Jul 2021Openclip</p>
<p>Compressing llms: The truth is rarely pure and never simple. A Jaiswal, Z Gan, X Du, B Zhang, Z Wang, Y Yang, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Visual prompt tuning. M Jia, L Tang, B Chen, C Cardie, S J Belongie, B Hariharan, S Lim, Eur. Conf. Comput. 2022</p>
<p>VIMA: general robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, Annu. Conf. Robot. Learn. (CoRL). 2023</p>
<p>How can we know what language models know. Z Jiang, F F Xu, J Araki, G Neubig, Trans. Assoc. Comput. Linguistics. 82020</p>
<p>A stereo matching algorithm with an adaptive window: Theory and experiment. T Kanade, M Okutomi, IEEE Trans. Pattern Anal. Mach. Intell. 1691994</p>
<p>Shape2pose: humancentric shape analysis. V G Kim, S Chaudhuri, L J Guibas, T A Funkhouser, ACM Trans. Graph. 334122014</p>
<p>Generating images with multimodal language models. J Y Koh, D Fried, R Salakhutdinov, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>The hungarian method for the assignment problem. H W Kuhn, Naval research logistics quarterly. 21-21955</p>
<p>BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S C H Hoi, Int. Conf. Mach. Learn. (ICML). 2023</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Category-level articulated object pose estimation. X Li, H Wang, L Yi, L J Guibas, A L Abbott, S Song, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2020</p>
<p>X Li, M Zhang, Y Geng, H Geng, Y Long, Y Shen, R Zhang, J Liu, H Dong, Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. 2023</p>
<p>Evaluating object hallucination in large vision-language models. Y Li, Y Du, K Zhou, J Wang, X Zhao, J R Wen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Completing tasks by connecting foundation models with millions of apis. Y Liang, C Wu, T Song, W Wu, Y Xia, Y Liu, Y Ou, S Lu, L Ji, S Mao, Intelligent Computing. Taskmatrix. ai3632024</p>
<p>Rouge: A package for automatic evaluation of summaries. C Y Lin, Proc. Workshop on Text Summariation Branches Out, Post-Conference Workshop of ACL 2004. Workshop on Text Summariation Branches Out, Post-Conference Workshop of ACL 20042004</p>
<p>Text2motion: From natural language instructions to feasible plans. K Lin, C Agia, T Migimatsu, M Pavone, J Bohg, Autonomous Robots. 4782023</p>
<p>Aligning large multi-modal model with robust instruction tuning. F Liu, K Lin, L Li, J Wang, Y Yacoob, L Wang, CoRR abs/2306.145652023</p>
<p>Improved baselines with visual instruction tuning. H Liu, C Li, Y Li, Y J Lee, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2024</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>Openshape: Scaling up 3d shape representation towards open-world understanding. M Liu, R Shi, K Kuang, Y Zhu, X Li, S Han, H Cai, F Porikli, H Su, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>Few-shot physically-aware articulated mesh generation via hierarchical deformation. X Liu, B Wang, H Wang, L Yi, Int. Conf. Comput. Vis. (ICCV). 2023</p>
<p>GeneOH diffusion: Towards generalizable hand-object interaction denoising via denoising diffusion. X Liu, L Yi, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Self-supervised categorylevel articulated object pose estimation with part-level SE(3) equivariance. X Liu, J Zhang, R Hu, H Huang, H Wang, L Yi, Int. Conf. Learn. Represent. (ICLR). 2023</p>
<p>Relation-shape convolutional neural network for point cloud analysis. Y Liu, B Fan, S Xiang, C Pan, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2019</p>
<p>Mmbench: Is your multi-modal model an all. Y Liu, H Duan, Y Zhang, B Li, S Zhang, W Zhao, Y Yuan, J Wang, C He, Z Liu, K Chen, D Lin, CoRR abs/2307.062812023</p>
<p>Syncdreamer: Generating multiview-consistent images from a single-view image. Y Liu, C Lin, Z Zeng, X Long, L Liu, T Komura, W Wang, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Leaf: Learning frames for 4d point cloud sequence understanding. Y Liu, J Chen, Z Zhang, J Huang, L Yi, Int. Conf. Comput. Vis. (ICCV). 2023</p>
<p>Group-free 3d object detection via transformers. Z Liu, Z Zhang, Y Cao, H Hu, X Tong, Int. Conf. Comput. Vis. (ICCV). 2021</p>
<p>SGDR: stochastic gradient descent with warm restarts. I Loshchilov, F Hutter, Int. Conf. Learn. Represent. (ICLR. 2017</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, Int. Conf. Learn. Represent. (ICLR). 2019</p>
<p>Beyond holistic object recognition: Enriching image understanding with part states. C Lu, H Su, Y Li, Y Lu, L Yi, C Tang, L J Guibas, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2018</p>
<p>OVIR-3D: openvocabulary 3d instance retrieval without training on 3d data. S Lu, H Chang, E P Jing, A Boularias, K E Bekris, Annu. Conf. Robot. Learn. (CoRL). 2023</p>
<p>Scalable 3d captioning with pretrained models. T Luo, C Rockwell, H Lee, J Johnson, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>SQA3D: situated question answering in 3d scenes. X Ma, S Yong, Z Zheng, Q Li, Y Liang, S Zhu, S Huang, Int. Conf. Learn. Represent. (ICLR). 2023</p>
<p>Rethinking network design and local geometry in point cloud: A simple residual MLP framework. X Ma, C Qin, H You, H Ran, Y Fu, Int. Conf. Learn. Represent. (ICLR). OpenReview.net. 2022</p>
<p>Understanding blind people's experiences with computer-generated captions of social media images. H Macleod, C L Bennett, M R Morris, E Cutrell, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. the 2017 CHI Conference on Human Factors in Computing SystemsNew York, NY, USAAssociation for Computing Machinery201717</p>
<p>Voxel transformer for 3d object detection. J Mao, Y Xue, M Niu, H Bai, J Feng, X Liang, H Xu, C Xu, Int. Conf. Comput. Vis. (ICCV). 2021</p>
<p>Voxnet: A 3d convolutional neural network for realtime object recognition. D Maturana, S A Scherer, IEEE/RSJ Int. Conf. Intell. Robot. and Syst. 2015IEEE</p>
<p>Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. K Mo, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Zhu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.A X Chang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Yi, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Tripathi, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L J Guibas, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Su, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2019CVPR</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Y Mu, Q Zhang, M Hu, W Wang, M Ding, J Jin, B Wang, J Dai, Y Qiao, P Luo, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>. OpenAI: Introducing chatgpt. 2022</p>
<p>CoRR abs/2303.08774OpenAI: GPT-4 technical report. 2023</p>
<p>Gpt-4v(ision) system card. 2023OpenAI</p>
<p>OpenAI: Introducing gpt-4o and more tools to chatgpt free users. 2024</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P F Christiano, J Leike, R Lowe, Adv. Neural Inform. Process. Syst. (NeurIPS). 2022</p>
<p>Kosmos-g: Generating images in context with multimodal large language models. X Pan, L Dong, S Huang, Z Peng, W Chen, F Wei, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Masked autoencoders for point cloud self-supervised learning. Y Pang, W Wang, F E H Tay, W Liu, Y Tian, L Yuan, Eur. Conf. Comput. 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W Zhu, 2002</p>
<p>Instruction tuning with GPT-4. B Peng, C Li, P He, M Galley, J Gao, CoRR abs/2304.032772023</p>
<p>Openscene: 3d scene understanding with open vocabularies. S Peng, K Genova, C M Jiang, A Tagliasacchi, M Pollefeys, T A Funkhouser, IEEE/CVF Conf. Comput. Vis. Pattern Recog. 2023</p>
<p>Dreambench++: A human-aligned benchmark for personalized image generation. Y Peng, Y Cui, H Tang, Z Qi, R Dong, J Bai, C Han, Z Ge, X Zhang, S T Xia, CoRR abs/2406.168552024</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Z Peng, W Wang, L Dong, Y Hao, S Huang, S Ma, F Wei, CoRR abs/2306.148242023</p>
<p>Language models as knowledge bases?. F Petroni, T RocktÃ¤schel, S Riedel, P S H Lewis, A Bakhtin, Y Wu, A H Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaNovember 3-7, 2019. 2019</p>
<p>Understanding and exploiting object interaction landscapes. S Pirk, V Krs, K Hu, S D Rajasekaran, H Kang, Y Yoshiyasu, B Benes, L J Guibas, ACM Trans. Graph. 363142017</p>
<p>Pointnet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2017</p>
<p>Pointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, Adv. Neural Inform. Process. Syst. 2017</p>
<p>In-hand object rotation via rapid motor adaptation. H Qi, A Kumar, R Calandra, Y Ma, J Malik, Annu. Conf. Robot. Learn. (CoRL). 2023</p>
<p>Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. Z Qi, R Dong, G Fan, Z Ge, X Zhang, K Ma, L Yi, Int. Conf. Mach. Learn. (ICML). 2023</p>
<p>VPP: efficient conditional 3d generation via voxel-point progressive representation. Z Qi, M Yu, R Dong, K Ma, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>Pointnext: Revisiting pointnet++ with improved training and scaling strategies. G Qian, Y Li, H Peng, J Mai, H A A K Hammoud, M Elhoseiny, B Ghanem, Adv. Neural Inform. Process. Syst. (NeurIPS). 2022</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, Proceedings of Machine Learning Research. Machine Learning ResearchPMLR2021139</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaNovember 3-7, 2019. 2019</p>
<p>Benchmarking and analyzing point cloud classification under corruptions. J Ren, L Pan, Z Liu, Int. Conf. Mach. Learn. (ICML). 2022</p>
<p>Object hallucination in image captioning. A Rohrbach, L A Hendricks, K Burns, T Darrell, K Saenko, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumOctober 31 -November 4, 2018. 2018</p>
<p>Object hallucination in image captioning. A Rohrbach, L A Hendricks, K Burns, T Darrell, K Saenko, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Distilled feature fields enable few-shot language-guided manipulation. W Shen, G Yang, A Yu, J Wong, L P Kaelbling, P Isola, Annu. Conf. Robot. Learn. (CoRL). 2023</p>
<p>Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>Robocook: Long-horizon elasto-plastic object manipulation with diverse tools. H Shi, H Xu, S Clarke, Y Li, J Wu, Annu. Conf. Robot. Learn. (CoRL). 2023</p>
<p>Shutterstock: Turbosquid. </p>
<p>Multi-view convolutional neural networks for 3d shape recognition. H Su, S Maji, E Kalogerakis, E G Learned-Miller, Int. Conf. Comput. Vis. (ICCV). 2015</p>
<p>Modelnet40-c: A robustness benchmark for 3d point cloud recognition under corruption. J Sun, Q Zhang, B Kailkhura, Z Yu, C Xiao, Z M Mao, ICLR 2022 Workshop on Socially Responsible Machine Learning. </p>
<p>Generative multimodal models are in-context learners. Q Sun, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Y Cui, IEEE/CVF Conf. Comput. Vis. Pattern Recog.X Zhang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.F Zhang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Q Yu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Z Luo, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Y Wang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Y Rao, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Liu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.T Huang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.X Wang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2024CVPR</p>
<p>EVA-CLIP: improved training techniques for CLIP at scale. Q Sun, Y Fang, L Wu, X Wang, Y Cao, CoRR abs/2303.153892023</p>
<p>Emu: Generative pretraining in multimodality. Q Sun, Q Yu, Y Cui, F Zhang, X Zhang, Y Wang, H Gao, J Liu, T Huang, X Wang, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Vipergpt: Visual inference via python execution for reasoning. D SurÃ­s, S Menon, C Vondrick, Int. Conf. Comput. Vis. (ICCV). 2023</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M Lachaux, T Lacroix, B RoziÃ¨re, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, CoRR abs/2302.139712023</p>
<p>Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. M A Uy, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)Q H Pham, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)B S Hua, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)T Nguyen, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)S K Yeung, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)2019</p>
<p>Statistical learning theory. V Vapnik, 1998Wiley</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Adv. Neural Inform. Process. Syst. 2017</p>
<p>Unidexgrasp++: Improving dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-specialist learning. W Wan, H Geng, Y Liu, Z Shan, Y Yang, L Yi, H Wang, Int. Conf. Comput. Vis. (ICCV). 2023</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, T. Mach. Learn. Res. (TMLR). 2024</p>
<p>Normalized object coordinate space for category-level 6d object pose and size estimation. H Wang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Sridhar, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Huang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Valentin, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Song, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L J Guibas, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2019CVPR</p>
<p>Dynamic graph CNN for learning on point clouds. Y Wang, Y Sun, Z Liu, S E Sarma, M M Bronstein, J M Solomon, ACM Trans. Graph. 385122019</p>
<p>Take-a-photo: 3d-to-2d generative pre-training of point cloud models. Z Wang, X Yu, Y Rao, J Zhou, J Lu, Int. Conf. Comput. Vis. (ICCV). 2023</p>
<p>Point primitive transformer for longterm 4d point cloud video understanding. H Wen, Y Liu, J Huang, B Duan, L Yi, Eur. Conf. Comput. 2022</p>
<p>CAPTRA: category-level pose tracking for rigid and articulated objects from point clouds. Y Weng, H Wang, Q Zhou, Y Qin, Y Duan, Q Fan, B Chen, H Su, L J Guibas, Int. Conf. Comput. Vis. (ICCV). 2021</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. C Wu, S Yin, W Qi, X Wang, Z Tang, N Duan, CoRR abs/2303.046712023</p>
<p>Next-gpt: Any-to-any multimodal LLM. S Wu, H Fei, L Qu, W Ji, T Chua, Int. Conf. Mach. Learn. (ICML). 2024</p>
<p>Gpt-4v(ision) is a human-aligned evaluator for text-to-3d generation. T Wu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.G Yang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Z Li, IEEE/CVF Conf. Comput. Vis. Pattern Recog.K Zhang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Z Liu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L J Guibas, IEEE/CVF Conf. Comput. Vis. Pattern Recog.D Lin, IEEE/CVF Conf. Comput. Vis. Pattern Recog.G Wetzstein, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2024CVPR</p>
<p>3d shapenets: A deep representation for volumetric shapes. Z Wu, S Song, A Khosla, F Yu, L Zhang, X Tang, J Xiao, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2015</p>
<p>Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. S Xie, J Gu, D Guo, C R Qi, L J Guibas, O Litany, Eur. Conf. Comput. Vis. (ECCV). Lecture Notes in Computer Science. Springer202012348</p>
<p>Pointllm: Empowering large language models to understand point clouds. R Xu, X Wang, T Wang, Y Chen, J Pang, D Lin, CoRR abs/2308.169112023</p>
<p>Unidexgrasp: Universal robotic dexterous grasping via learning diverse proposal generation and goal-conditioned policy. Y Xu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.W Wan, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Zhang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Liu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Z Shan, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Shen, IEEE/CVF Conf. Comput. Vis. Pattern Recog.R Wang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Geng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Y Weng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Chen, IEEE/CVF Conf. Comput. Vis. Pattern Recog.T Liu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Yi, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Wang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2023CVPR</p>
<p>Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Z Xu, Y Shen, L Huang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics (ACL)20231</p>
<p>ULIP: learning unified representation of language, image and Z. Qi et al. point cloud for 3d understanding. L Xue, IEEE/CVF Conf. Comput. Vis. Pattern Recog.M Gao, IEEE/CVF Conf. Comput. Vis. Pattern Recog.C Xing, IEEE/CVF Conf. Comput. Vis. Pattern Recog.R MartÃ­n-MartÃ­n, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Wu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.C Xiong, IEEE/CVF Conf. Comput. Vis. Pattern Recog.R Xu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J C Niebles, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Savarese, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2023CVPR</p>
<p>ULIP-2: towards scalable multimodal pre-training for 3d understanding. L Xue, IEEE/CVF Conf. Comput. Vis. Pattern Recog.N Yu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Zhang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Li, IEEE/CVF Conf. Comput. Vis. Pattern Recog.R MartÃ­n-MartÃ­n, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Wu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.C Xiong, IEEE/CVF Conf. Comput. Vis. Pattern Recog.R Xu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J C Niebles, IEEE/CVF Conf. Comput. Vis. Pattern Recog.S Savarese, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2024CVPR</p>
<p>Gpt4tools: Teaching large language model to use tools via self-instruction. R Yang, L Song, Y Li, S Zhao, Y Ge, X Li, Y Shan, Adv. Neural Inform. Process. Syst. (NeurIPS). 2023</p>
<p>MM-REACT: prompting chatgpt for multimodal reasoning and action. Z Yang, L Li, J Wang, K Lin, E Azarnasab, F Ahmed, Z Liu, C Liu, M Zeng, L Wang, CoRR abs/2303.113812023</p>
<p>Q Ye, H Xu, G Xu, J Ye, M Yan, Y Zhou, J Wang, A Hu, P Shi, Y Shi, C Li, Y Xu, H Chen, J Tian, Q Qi, J Zhang, F Huang, mplugowl: Modularization empowers large language models with multimodality. 2023</p>
<p>3d question answering. S Ye, D Chen, S Han, J Liao, IEEE Transactions on Visualization and Computer Graphics. 2022</p>
<p>Deep part induction from articulated object pairs. L Yi, H Huang, D Liu, E Kalogerakis, H Su, L J Guibas, ACM Trans. Graph. 3762092018</p>
<p>A scalable active framework for region annotation in 3d shape collections. L Yi, V G Kim, D Ceylan, I C Shen, M Yan, H Su, C Lu, Q Huang, A Sheffer, L Guibas, ACM Trans. Graph. 3562016</p>
<p>Syncspeccnn: Synchronized spectral CNN for 3d shape segmentation. L Yi, H Su, X Guo, L J Guibas, IEEE/CVF Conf. Comput. Vis. Pattern Recog. 2017</p>
<p>Make a donut: Language-guided hierarchical emd-space planning for zero-shot deformable object manipulation. Y You, B Shen, C Deng, H Geng, H Wang, L J Guibas, CoRR abs/2311.027872023</p>
<p>Mm-vet: Evaluating large multimodal models for integrated capabilities. W Yu, Z Yang, L Li, J Wang, K Lin, Z Liu, X Wang, L Wang, Int. Conf. Mach. Learn. (ICML). 2024</p>
<p>Point-bert: Pre-training 3d point cloud transformers with masked point modeling. X Yu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Tang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Y Rao, IEEE/CVF Conf. Comput. Vis. Pattern Recog.T Huang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Zhou, IEEE/CVF Conf. Comput. Vis. Pattern Recog.J Lu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2022CVPR</p>
<p>Point2vec for self-supervised representation learning on point clouds. K A Zeid, J Schult, A Hermans, B Leibe, DAGM German Conference on Pattern Recognition. Springer2023</p>
<p>CLIP-FO3D: learning free open-world 3d scene representations from 2d dense CLIP. J Zhang, R Dong, K Ma, Int. Conf. Comput. Vis. Worksh. (ICCV Workshop. 2023</p>
<p>Self-distillation: Towards efficient and compact neural networks. L Zhang, C Bao, K Ma, IEEE Trans. Pattern Anal. Mach. Intell. 4482022</p>
<p>Region-aware knowledge distillation for efficient image-to-image translation. L Zhang, X Chen, R Dong, K Ma, Vis. Conf. (BMVC). Brit, Mach, 2023</p>
<p>Pointdistiller: Structured knowledge distillation towards efficient and compact 3d detection. L Zhang, R Dong, H Tai, K Ma, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2023</p>
<p>Point-m2AE: Multi-scale masked autoencoders for hierarchical point cloud pre-training. R Zhang, Z Guo, P Gao, R Fang, B Zhao, D Wang, Y Qiao, H Li, Adv. Neural Inform. Process. Syst. (NeurIPS). 2022</p>
<p>Pointclip: Point cloud understanding by CLIP. R Zhang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Z Guo, IEEE/CVF Conf. Comput. Vis. Pattern Recog.W Zhang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.K Li, IEEE/CVF Conf. Comput. Vis. Pattern Recog.X Miao, IEEE/CVF Conf. Comput. Vis. Pattern Recog.B Cui, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Y Qiao, IEEE/CVF Conf. Comput. Vis. Pattern Recog.P Gao, IEEE/CVF Conf. Comput. Vis. Pattern Recog.H Li, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2022CVPR</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y Qiao, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. R Zhang, L Wang, Y Qiao, P Gao, H Li, IEEE/CVF Conf. Comput. Vis. Pattern Recog. 2023</p>
<p>S Zhang, P Sun, S Chen, M Xiao, W Shao, W Zhang, K Chen, P Luo, CoRR abs/2307.03601Gpt4roi: Instruction tuning large language model on region-of-interest. 2023</p>
<p>Siren's song in the AI ocean: A survey on hallucination in large language models. Y Zhang, Y Li, L Cui, D Cai, L Liu, T Fu, X Huang, E Zhao, Y Zhang, Y Chen, L Wang, A T Luu, W Bi, F Shi, S Shi, CoRR abs/2309.012192023</p>
<p>TAMM: triadapter multi-modal learning for 3d shape understanding. Z Zhang, S Cao, Y Wang, IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR). 2024</p>
<p>Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. L Zhao, E Yu, Z Ge, J Yang, H Wei, H Zhou, J Sun, Y Peng, R Dong, C Han, X Zhang, Int. Joint Conf. Artif. Intell. (IJCAI. 2024</p>
<p>Indexing 3d scenes using the interaction bisector surface. X Zhao, H Wang, T Komura, ACM Trans. Graph. 333142014</p>
<p>CAMS: canonicalized manipulation spaces for category-level functional hand-object manipulation synthesis. J Zheng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Q Zheng, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Fang, IEEE/CVF Conf. Comput. Vis. Pattern Recog.Y Liu, IEEE/CVF Conf. Comput. Vis. Pattern Recog.L Yi, IEEE/CVF Conf. Comput. Vis. Pattern Recog.2023CVPR</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, Adv. Neural Inform. Process. Syst. (NeurIPS). 2024</p>
<p>Uni3d: Exploring unified 3d representation at scale. J Zhou, J Wang, B Ma, Y Liu, T Huang, X Wang, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Analyzing and mitigating object hallucination in large vision-language models. Y Zhou, C Cui, J Yoon, L Zhang, Z Deng, C Finn, M Bansal, H Yao, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, Int. Conf. Learn. Represent. (ICLR). 2024</p>
<p>X Zhu, R Zhang, B He, Z Guo, Z Zeng, Z Qin, S Zhang, P Gao, Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. 2023Int. Conf. Comput. Vis. (ICCV)</p>
<p>3d-vista: Pre-trained transformer for 3d vision and text alignment. Z Zhu, X Ma, Y Chen, Z Deng, S Huang, Q Li, Int. Conf. Comput. Vis. (ICCV). 2023</p>            </div>
        </div>

    </div>
</body>
</html>
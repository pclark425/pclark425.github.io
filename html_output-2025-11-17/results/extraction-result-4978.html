<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4978 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4978</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4978</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-1358f90705b05cdb20ebe6799b02196205e7e9f0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1358f90705b05cdb20ebe6799b02196205e7e9f0" target="_blank">Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels.</p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7%), commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning tasks (+2.5%). The code is available at https://github.com/SHUMKASHUN/Automate-CoT.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4978.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4978.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automate-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought (Automate-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic pipeline that (1) augments model-generated chain-of-thought exemplars from labeled data, (2) prunes exemplars whose final answers disagree with ground truth, and (3) selects an optimal small set of exemplars via a variance-reduced policy-gradient optimization over categorical exemplar distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002, code-davinci-002, gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessible OpenAI GPT-3/CodeX / ChatGPT-class models used as black-box generators and evaluators; exact sizes not provided in paper (text-davinci-002 = InstructGPT variant; code-davinci-002 = Codex variant; gpt-3.5-turbo = chat model).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Automate-CoT (selected diverse CoT exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Automatic augmentation: generate many rationale chains per training question; prune chains whose final answers mismatch labels; build a pool; train categorical selection distributions over candidate exemplars using a variance-reduced policy gradient estimator to pick 4–8 exemplars (and their order) that maximize validation accuracy. Diversity arises from sampling many machine-generated exemplars of varying complexity and style and selecting a heterogeneous combination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (GSM8K, SVAMP, AQuA, ASDiv, SingleOp, CSQA, StrategyQA, Letter(4), OpenBookQA, e-SNLI, SST-2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic, commonsense, symbolic reasoning and non-reasoning benchmarks; exact-match accuracy is reported for all.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example highlights: text-davinci-002 average over 11 tasks: 70.6% (Automate-CoT) vs Manual-CoT 68.0%; GSM8K text-davinci-002: 49.7% (Automate-CoT) vs Manual-CoT 46.9%; code-davinci-002 average: 76.9% (Automate-CoT) vs Manual-CoT 74.2%; GSM8K code-davinci-002: 67.6% (Automate-CoT) vs Manual-CoT 63.1%; gpt-3.5-turbo GSM8K: 68.0% (Automate-CoT) vs Manual-CoT 63.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to Manual-CoT (single / fixed human exemplars): +2.6% to +2.7% average improvements depending on model; Compared to Self-Consistency (SC): Automate-CoT+SC gives further gains (e.g., text-davinci-002 GSM8K SC=38.2% -> SC+Automate-CoT=67.8%; code-davinci-002 SC=78.0% -> SC+Automate-CoT=82.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automatically constructing and selecting a diverse set of CoT exemplars (Automate-CoT) consistently improves accuracy across arithmetic, commonsense, symbolic, and some non-reasoning tasks compared to fixed manual CoT; combining Automate-CoT with self-consistency (diverse path ensembling) yields additional gains; diverse exemplar complexity (mix of short and long chains) enables a complexity–diversity trade-off that improves both simple and complex question performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>When pool size is very small (e.g., 10 exemplars), Automate-CoT can be worse or comparable to Manual-CoT; selection benefits rely on having a sufficiently large and diverse pool (>=20 observed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4978.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4978.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Manual-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting (manual exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard chain-of-thought in-context prompting using human-written exemplars that include intermediate reasoning steps for each example.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002, code-davinci-002, gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same API models as used throughout the experiments; human-authored CoT exemplars are provided as few-shot context.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Manual-CoT (human-authored CoT exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Provide a small fixed set (4–8) of human-written chain-of-thought exemplars to the LLM as in-context demonstrations; typically simple (<=3 hops) human rationale chains are used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (e.g., GSM8K, SVAMP, CSQA, Letter(4), etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as above; used as baseline few-shot CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example highlights: text-davinci-002 average: 68.0%; GSM8K text-davinci-002: 46.9%; code-davinci-002 average: 74.2%; GSM8K code-davinci-002: 63.1%; gpt-3.5-turbo average (reported): 73.6% in one table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Automate-CoT: e.g., text-davinci-002 average 70.6% (+2.6%); code-davinci-002 average 76.9% (+2.7%); SC (self-consistency) sometimes outperforms Manual-CoT on several settings (SC avg 72.8% text-davinci-002 > Manual-CoT 68.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human-written CoT exemplars are effective but sensitive to order, complexity, diversity, and writing style; human exemplars tend to be simple and can underperform on complex questions. Automate-CoT mitigates many of these sensitivities.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Manual-CoT's performance can vary substantially across annotators (style sensitivity) and exemplar order; simple human exemplars can perform poorly on complex multi-hop questions compared to more diverse/complex exemplar mixtures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4978.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4978.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (sampling and majority/consensus over multiple reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples many chain-of-thought reasoning paths from the model (stochastic decoding) and chooses the most consistent answer (mode) across sampled chains to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002, code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs probed under stochastic decoding (temperature > 0) to produce multiple diverse reasoning traces; paper used 40 samples for SC experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (ensemble of sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple independent chain-of-thought outputs via sampling (e.g., 40 draws at temperature 0.7) and take the most frequent final answer; diversity arises from different sampled reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (notably GSM8K, arithmetic and commonsense tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks; SC used to improve robustness by ensembling sampled reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples: text-davinci-002 SC average: 72.8% (Table 1); GSM8K text-davinci-002 SC: 38.2% (noted), but SC+Automate-CoT: 67.8% (large gain from combining selection and sampling); code-davinci-002 SC GSM8K: 78.0% and SC+Automate-CoT: 82.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Manual-CoT (single deterministic CoT): often lower (Manual-CoT GSM8K 46.9% text-davinci-002); Automate-CoT can be combined with SC giving further improvements over SC alone (e.g., text-davinci-002 GSM8K SC 38.2% -> SC+Automate-CoT 67.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-consistency (sampling and ensembling) is complementary to Automate-CoT: selecting better exemplars increases the effectiveness of self-consistency, yielding large joint improvements; diversity across sampled reasoning paths can substantially help, but its benefit depends on exemplar quality and model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>In some reported numbers (text-davinci-002, GSM8K), SC alone (38.2%) underperformed Manual-CoT (46.9%) — indicating that naive sampling without good exemplars or settings can fail; combining selection (Automate-CoT) is crucial to realize SC benefits in those settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4978.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4978.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Chain-of-Thought Prompting (Auto-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed automatic exemplar construction method that clusters questions and generates rationale chains for cluster representatives to build exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chain of thought prompting in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002, text-davinci-002 (as reported / compared)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-CoT uses clustering (question feature-based) to choose diverse representative questions and then generates CoT exemplars for them.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Auto-CoT (clustering-based exemplar selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Cluster pool of questions (e.g., via k-means), sample representative questions from clusters, generate CoT for those, and use them as few-shot exemplars; diversity arises from cluster-based semantic diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and other arithmetic tasks (reported comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automatic selection approach intended to generalize exemplar choice without human curation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: code-davinci-002 GSM8K reported Auto-CoT 62.8% (Table 1); Automate-CoT outperforms Auto-CoT (e.g., Automate-CoT GSM8K code-davinci-002 67.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Automate-CoT: e.g., code-davinci-002 GSM8K Automate-CoT 67.6% (> Auto-CoT 62.8%); Manual-CoT sometimes lower or comparable depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Clustering-based automatic exemplar selection (Auto-CoT) improves over naive manual selection but can overlook complexity and style diversity; Automate-CoT that augments, prunes, and optimizes exemplar selection obtains further gains by explicitly addressing complexity/diversity/style sensitivities.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Auto-CoT can miss beneficial exemplar complexity diversity and thus underperform methods that explicitly search or select for complexity-diversity trade-offs (as Automate-CoT does).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4978.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4978.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-Selection-from-Pool</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random selection of model-generated exemplars from the candidate pool</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that randomly samples few-shot exemplars from the pruned pool and forms prompts without optimization or ordering control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002, text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLMs used to generate and evaluate; exemplars chosen at random from the generated-pruned pool.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Random exemplar selection (diverse but uncontrolled)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Randomly sample exemplars from an automatically generated pool of pruned CoT exemplars and use them as few-shot prompts; diversity arises incidentally from pool content but selection is not optimized.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, CSQA and others (experiments shown for several datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as an ablation/baseline to measure importance of deliberate exemplar selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Observed high variance and generally lower accuracy than Automate-CoT; sometimes the random average can outperform Manual-CoT on some datasets (e.g., GSM8K, CSQA) but with high variance (plots & discussion in §6.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Manual-CoT: random selection on average is lower and higher variance; Automate-CoT: consistently better and lower variance than random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random selection suffers from high variance and usually lower mean accuracy compared to optimized selection and Manual-CoT; however, randomly sampled model-generated exemplars can occasionally beat Manual-CoT on some datasets, indicating that model-generated exemplars can be strong but require selection to be reliably useful.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Random selection occasionally outperforms Manual-CoT on particular datasets (GSM8K, CSQA) but is unreliable due to high variance; thus diversity alone without selection is not reliably superior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4978.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4978.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complex-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complex-CoT (all high-hop exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hand-constructed baseline consisting of exemplars that all have large chain complexity (e.g., 9 hops) to test whether uniformly complex exemplars help multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (experiments shown), code-davinci-002 comparisons implied</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs prompted with few-shot exemplars where every exemplar has many reasoning steps (high-hop chains).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Complex-CoT (uniformly complex exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Use few-shot demonstration set where each exemplar is a complex/high-hop reasoning chain; similarity arises because exemplars are uniformly high-complexity and stylistically similar.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (analyzed by hop/difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math multi-hop reasoning benchmark with tests stratified by number of hops required.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On GSM8K overall Complex-CoT achieved 59.4% exact-match (Table 9) compared to Manual-CoT 61.8% and Automate-CoT 68.2% (code/davinci specific numbers vary by model). Complex-CoT helps on hardest (7–9 hop) but hurts simple questions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Manual-CoT: 61.8% overall on GSM8K; Automate-CoT: 68.2% overall on GSM8K (Automate-CoT outperforms Complex-CoT and Manual-CoT); Complex-CoT improves performance on high-hop questions but degrades on low-hop ones.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Uniformly providing complex exemplars improves performance on hard/many-hop questions but degrades performance on simple questions; hence, a mixture of complexities (diversity) is preferable to uniformly similar high-complexity exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Complex-CoT underperforms overall relative to Automate-CoT and even Manual-CoT for GSM8K due to poor performance on simpler questions; demonstrates that 'more complex' exemplars are not universally better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4978.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4978.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-Shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Chain-of-Thought (Zero-Shot-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot prompting trick in which a single instruction such as "Let's think step by step." is appended to the question to induce the model to produce a chain-of-thought without exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (experiments reported), Automate-CoT also applied in zero-shot setting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs prompted in zero-shot fashion with a short instruction to elicit intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-Shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>No few-shot exemplars; prepend or append an instruction like "Let's think step by step." to the test question so the model generates a chain-of-thought in a zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP, Letter(4) (reported comparisons in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks where zero-shot CoT can be assessed against few-shot methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3: Zero-Shot-CoT: GSM8K 40.7%, SVAMP 62.1%, Letter(4) 57.6% (text-davinci-002). Zero-Shot-Automate-CoT (apply Automate-CoT pipeline seeded only by zero-shot prompt) improved to GSM8K 49.1%, SVAMP 74.3%, Letter(4) 59.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Manual-CoT GSM8K 46.9% (Zero-Shot-CoT 40.7% < Manual); Automate-CoT (few-shot seed) 49.7% > Zero-Shot 40.7%; Zero-Shot-Automate-CoT can reach or exceed Manual-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero-shot elicitation of CoT can produce reasonable chains, but Automate-CoT applied in a zero-shot seed setting (Zero-Shot-Automate-CoT) can substantially improve zero-shot baseline and in some cases exceed Manual-CoT; automated selection mitigates noisy zero-shot chains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Zero-Shot-CoT by itself underperforms Manual-CoT on some settings (e.g., GSM8K), showing that zero-shot chain elicitation is not always sufficient without selection/augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4978.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4978.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25 Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 retrieval-based exemplar selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval baseline that selects few-shot exemplars most textually similar to the test question using BM25 lexical retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Probabilistic Relevance Framework: BM25 and Beyond</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (compared in Table 6 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BM25 is an off-the-shelf lexical retrieval algorithm; chosen exemplars are then used to prompt LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>BM25 retrieval-based exemplar selection</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>For a given test question, retrieve the most similar training questions via BM25 and use their CoT exemplars as demonstrations; similarity-focused selection yields exemplars similar in semantics to the test item.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (reported under gpt-3.5-turbo experiments; average comparisons in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieval-based exemplar selection evaluated against clustering and Automate-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Under gpt-3.5-turbo Table 6 shows BM25 average 72.6% vs Manual-CoT 73.6% and Automate-CoT 76.4% (BM25 slightly worse on average), with per-task variation (e.g., GSM8K BM25 64.2% vs Automate-CoT 68.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Automate-CoT: consistently higher (e.g., +3.8% avg over BM25); Clustering (k-means) also compared and often outperformed BM25.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-by-similarity (BM25) tends to select semantically similar exemplars but overlooks complexity/style diversity, leading to weaker average performance than Automate-CoT which optimizes for exemplar usefulness rather than raw similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>BM25 on average performed slightly worse than Manual-CoT in the reported experiments and substantially worse than Automate-CoT, indicating similarity-based retrieval can be suboptimal for CoT exemplar selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4978.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4978.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptPG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PromptPG (dynamic example-selection via policy gradient)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning-based retrieval/selection method that dynamically learns which exemplars to present for each input using policy gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (compared in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural policy that picks exemplars per input; trained with policy gradient to maximize downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>PromptPG (learned exemplar retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Learn a policy (via reinforcement learning / policy gradient) to select exemplars conditioned on the input; aims for per-instance optimal exemplar choice rather than a fixed few-shot set.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (gpt-3.5-turbo experiments in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Baseline for dynamic exemplar selection compared to Automate-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>gpt-3.5-turbo Table 6: PromptPG average 74.0% vs Automate-CoT 76.4% (Automate-CoT outperforms PromptPG on average).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Automate-CoT: +2.4% avg improvement over PromptPG in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Policy-gradient based dynamic selection (PromptPG) improves over naive methods but Automate-CoT's augmentation + pruning + variance-reduced selection achieved higher average performance in experiments, suggesting that constructing a broad high-quality pool and optimizing categorical selection distributions per exemplar-position is effective.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PromptPG did not beat Automate-CoT in these experiments; no detailed negative cases beyond being lower-avg in Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning <em>(Rating: 1)</em></li>
                <li>The Probabilistic Relevance Framework: BM25 and Beyond <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4978",
    "paper_id": "paper-1358f90705b05cdb20ebe6799b02196205e7e9f0",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Automate-CoT",
            "name_full": "Automatic Prompt Augmentation and Selection with Chain-of-Thought (Automate-CoT)",
            "brief_description": "An automatic pipeline that (1) augments model-generated chain-of-thought exemplars from labeled data, (2) prunes exemplars whose final answers disagree with ground truth, and (3) selects an optimal small set of exemplars via a variance-reduced policy-gradient optimization over categorical exemplar distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002, code-davinci-002, gpt-3.5-turbo",
            "model_description": "API-accessible OpenAI GPT-3/CodeX / ChatGPT-class models used as black-box generators and evaluators; exact sizes not provided in paper (text-davinci-002 = InstructGPT variant; code-davinci-002 = Codex variant; gpt-3.5-turbo = chat model).",
            "reasoning_method_name": "Automate-CoT (selected diverse CoT exemplars)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Automatic augmentation: generate many rationale chains per training question; prune chains whose final answers mismatch labels; build a pool; train categorical selection distributions over candidate exemplars using a variance-reduced policy gradient estimator to pick 4–8 exemplars (and their order) that maximize validation accuracy. Diversity arises from sampling many machine-generated exemplars of varying complexity and style and selecting a heterogeneous combination.",
            "task_name": "Multiple (GSM8K, SVAMP, AQuA, ASDiv, SingleOp, CSQA, StrategyQA, Letter(4), OpenBookQA, e-SNLI, SST-2)",
            "task_description": "Arithmetic, commonsense, symbolic reasoning and non-reasoning benchmarks; exact-match accuracy is reported for all.",
            "performance": "Example highlights: text-davinci-002 average over 11 tasks: 70.6% (Automate-CoT) vs Manual-CoT 68.0%; GSM8K text-davinci-002: 49.7% (Automate-CoT) vs Manual-CoT 46.9%; code-davinci-002 average: 76.9% (Automate-CoT) vs Manual-CoT 74.2%; GSM8K code-davinci-002: 67.6% (Automate-CoT) vs Manual-CoT 63.1%; gpt-3.5-turbo GSM8K: 68.0% (Automate-CoT) vs Manual-CoT 63.1%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to Manual-CoT (single / fixed human exemplars): +2.6% to +2.7% average improvements depending on model; Compared to Self-Consistency (SC): Automate-CoT+SC gives further gains (e.g., text-davinci-002 GSM8K SC=38.2% -&gt; SC+Automate-CoT=67.8%; code-davinci-002 SC=78.0% -&gt; SC+Automate-CoT=82.4%).",
            "key_findings": "Automatically constructing and selecting a diverse set of CoT exemplars (Automate-CoT) consistently improves accuracy across arithmetic, commonsense, symbolic, and some non-reasoning tasks compared to fixed manual CoT; combining Automate-CoT with self-consistency (diverse path ensembling) yields additional gains; diverse exemplar complexity (mix of short and long chains) enables a complexity–diversity trade-off that improves both simple and complex question performance.",
            "counter_examples_or_negative_results": "When pool size is very small (e.g., 10 exemplars), Automate-CoT can be worse or comparable to Manual-CoT; selection benefits rely on having a sufficiently large and diverse pool (&gt;=20 observed).",
            "uuid": "e4978.0",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Manual-CoT",
            "name_full": "Chain-of-Thought Prompting (manual exemplars)",
            "brief_description": "Standard chain-of-thought in-context prompting using human-written exemplars that include intermediate reasoning steps for each example.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "text-davinci-002, code-davinci-002, gpt-3.5-turbo",
            "model_description": "Same API models as used throughout the experiments; human-authored CoT exemplars are provided as few-shot context.",
            "reasoning_method_name": "Manual-CoT (human-authored CoT exemplars)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Provide a small fixed set (4–8) of human-written chain-of-thought exemplars to the LLM as in-context demonstrations; typically simple (&lt;=3 hops) human rationale chains are used.",
            "task_name": "Multiple (e.g., GSM8K, SVAMP, CSQA, Letter(4), etc.)",
            "task_description": "Same benchmarks as above; used as baseline few-shot CoT.",
            "performance": "Example highlights: text-davinci-002 average: 68.0%; GSM8K text-davinci-002: 46.9%; code-davinci-002 average: 74.2%; GSM8K code-davinci-002: 63.1%; gpt-3.5-turbo average (reported): 73.6% in one table.",
            "comparison_with_other_method": true,
            "performance_other_method": "Automate-CoT: e.g., text-davinci-002 average 70.6% (+2.6%); code-davinci-002 average 76.9% (+2.7%); SC (self-consistency) sometimes outperforms Manual-CoT on several settings (SC avg 72.8% text-davinci-002 &gt; Manual-CoT 68.0%).",
            "key_findings": "Human-written CoT exemplars are effective but sensitive to order, complexity, diversity, and writing style; human exemplars tend to be simple and can underperform on complex questions. Automate-CoT mitigates many of these sensitivities.",
            "counter_examples_or_negative_results": "Manual-CoT's performance can vary substantially across annotators (style sensitivity) and exemplar order; simple human exemplars can perform poorly on complex multi-hop questions compared to more diverse/complex exemplar mixtures.",
            "uuid": "e4978.1",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency (sampling and majority/consensus over multiple reasoning paths)",
            "brief_description": "A method that samples many chain-of-thought reasoning paths from the model (stochastic decoding) and chooses the most consistent answer (mode) across sampled chains to improve robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "text-davinci-002, code-davinci-002",
            "model_description": "LLMs probed under stochastic decoding (temperature &gt; 0) to produce multiple diverse reasoning traces; paper used 40 samples for SC experiments.",
            "reasoning_method_name": "Self-Consistency (ensemble of sampled reasoning paths)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple independent chain-of-thought outputs via sampling (e.g., 40 draws at temperature 0.7) and take the most frequent final answer; diversity arises from different sampled reasoning traces.",
            "task_name": "Multiple (notably GSM8K, arithmetic and commonsense tasks)",
            "task_description": "Same benchmarks; SC used to improve robustness by ensembling sampled reasoning chains.",
            "performance": "Examples: text-davinci-002 SC average: 72.8% (Table 1); GSM8K text-davinci-002 SC: 38.2% (noted), but SC+Automate-CoT: 67.8% (large gain from combining selection and sampling); code-davinci-002 SC GSM8K: 78.0% and SC+Automate-CoT: 82.4%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Manual-CoT (single deterministic CoT): often lower (Manual-CoT GSM8K 46.9% text-davinci-002); Automate-CoT can be combined with SC giving further improvements over SC alone (e.g., text-davinci-002 GSM8K SC 38.2% -&gt; SC+Automate-CoT 67.8%).",
            "key_findings": "Self-consistency (sampling and ensembling) is complementary to Automate-CoT: selecting better exemplars increases the effectiveness of self-consistency, yielding large joint improvements; diversity across sampled reasoning paths can substantially help, but its benefit depends on exemplar quality and model.",
            "counter_examples_or_negative_results": "In some reported numbers (text-davinci-002, GSM8K), SC alone (38.2%) underperformed Manual-CoT (46.9%) — indicating that naive sampling without good exemplars or settings can fail; combining selection (Automate-CoT) is crucial to realize SC benefits in those settings.",
            "uuid": "e4978.2",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Auto-CoT",
            "name_full": "Automatic Chain-of-Thought Prompting (Auto-CoT)",
            "brief_description": "A previously proposed automatic exemplar construction method that clusters questions and generates rationale chains for cluster representatives to build exemplars.",
            "citation_title": "Automatic chain of thought prompting in large language models",
            "mention_or_use": "use",
            "model_name": "code-davinci-002, text-davinci-002 (as reported / compared)",
            "model_description": "Auto-CoT uses clustering (question feature-based) to choose diverse representative questions and then generates CoT exemplars for them.",
            "reasoning_method_name": "Auto-CoT (clustering-based exemplar selection)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Cluster pool of questions (e.g., via k-means), sample representative questions from clusters, generate CoT for those, and use them as few-shot exemplars; diversity arises from cluster-based semantic diversity.",
            "task_name": "GSM8K and other arithmetic tasks (reported comparisons)",
            "task_description": "Automatic selection approach intended to generalize exemplar choice without human curation.",
            "performance": "Example: code-davinci-002 GSM8K reported Auto-CoT 62.8% (Table 1); Automate-CoT outperforms Auto-CoT (e.g., Automate-CoT GSM8K code-davinci-002 67.6%).",
            "comparison_with_other_method": true,
            "performance_other_method": "Automate-CoT: e.g., code-davinci-002 GSM8K Automate-CoT 67.6% (&gt; Auto-CoT 62.8%); Manual-CoT sometimes lower or comparable depending on dataset.",
            "key_findings": "Clustering-based automatic exemplar selection (Auto-CoT) improves over naive manual selection but can overlook complexity and style diversity; Automate-CoT that augments, prunes, and optimizes exemplar selection obtains further gains by explicitly addressing complexity/diversity/style sensitivities.",
            "counter_examples_or_negative_results": "Auto-CoT can miss beneficial exemplar complexity diversity and thus underperform methods that explicitly search or select for complexity-diversity trade-offs (as Automate-CoT does).",
            "uuid": "e4978.3",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Random-Selection-from-Pool",
            "name_full": "Random selection of model-generated exemplars from the candidate pool",
            "brief_description": "Baseline that randomly samples few-shot exemplars from the pruned pool and forms prompts without optimization or ordering control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002, text-davinci-002",
            "model_description": "Same LLMs used to generate and evaluate; exemplars chosen at random from the generated-pruned pool.",
            "reasoning_method_name": "Random exemplar selection (diverse but uncontrolled)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Randomly sample exemplars from an automatically generated pool of pruned CoT exemplars and use them as few-shot prompts; diversity arises incidentally from pool content but selection is not optimized.",
            "task_name": "GSM8K, CSQA and others (experiments shown for several datasets)",
            "task_description": "Used as an ablation/baseline to measure importance of deliberate exemplar selection.",
            "performance": "Observed high variance and generally lower accuracy than Automate-CoT; sometimes the random average can outperform Manual-CoT on some datasets (e.g., GSM8K, CSQA) but with high variance (plots & discussion in §6.1).",
            "comparison_with_other_method": true,
            "performance_other_method": "Manual-CoT: random selection on average is lower and higher variance; Automate-CoT: consistently better and lower variance than random selection.",
            "key_findings": "Random selection suffers from high variance and usually lower mean accuracy compared to optimized selection and Manual-CoT; however, randomly sampled model-generated exemplars can occasionally beat Manual-CoT on some datasets, indicating that model-generated exemplars can be strong but require selection to be reliably useful.",
            "counter_examples_or_negative_results": "Random selection occasionally outperforms Manual-CoT on particular datasets (GSM8K, CSQA) but is unreliable due to high variance; thus diversity alone without selection is not reliably superior.",
            "uuid": "e4978.4",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Complex-CoT",
            "name_full": "Complex-CoT (all high-hop exemplars)",
            "brief_description": "A hand-constructed baseline consisting of exemplars that all have large chain complexity (e.g., 9 hops) to test whether uniformly complex exemplars help multi-hop reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (experiments shown), code-davinci-002 comparisons implied",
            "model_description": "LLMs prompted with few-shot exemplars where every exemplar has many reasoning steps (high-hop chains).",
            "reasoning_method_name": "Complex-CoT (uniformly complex exemplars)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Use few-shot demonstration set where each exemplar is a complex/high-hop reasoning chain; similarity arises because exemplars are uniformly high-complexity and stylistically similar.",
            "task_name": "GSM8K (analyzed by hop/difficulty)",
            "task_description": "Math multi-hop reasoning benchmark with tests stratified by number of hops required.",
            "performance": "On GSM8K overall Complex-CoT achieved 59.4% exact-match (Table 9) compared to Manual-CoT 61.8% and Automate-CoT 68.2% (code/davinci specific numbers vary by model). Complex-CoT helps on hardest (7–9 hop) but hurts simple questions.",
            "comparison_with_other_method": true,
            "performance_other_method": "Manual-CoT: 61.8% overall on GSM8K; Automate-CoT: 68.2% overall on GSM8K (Automate-CoT outperforms Complex-CoT and Manual-CoT); Complex-CoT improves performance on high-hop questions but degrades on low-hop ones.",
            "key_findings": "Uniformly providing complex exemplars improves performance on hard/many-hop questions but degrades performance on simple questions; hence, a mixture of complexities (diversity) is preferable to uniformly similar high-complexity exemplars.",
            "counter_examples_or_negative_results": "Complex-CoT underperforms overall relative to Automate-CoT and even Manual-CoT for GSM8K due to poor performance on simpler questions; demonstrates that 'more complex' exemplars are not universally better.",
            "uuid": "e4978.5",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Zero-Shot-CoT",
            "name_full": "Zero-Shot Chain-of-Thought (Zero-Shot-CoT)",
            "brief_description": "A zero-shot prompting trick in which a single instruction such as \"Let's think step by step.\" is appended to the question to induce the model to produce a chain-of-thought without exemplars.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (experiments reported), Automate-CoT also applied in zero-shot setting",
            "model_description": "LLMs prompted in zero-shot fashion with a short instruction to elicit intermediate reasoning steps.",
            "reasoning_method_name": "Zero-Shot-CoT",
            "reasoning_method_type": "other",
            "reasoning_method_description": "No few-shot exemplars; prepend or append an instruction like \"Let's think step by step.\" to the test question so the model generates a chain-of-thought in a zero-shot manner.",
            "task_name": "GSM8K, SVAMP, Letter(4) (reported comparisons in Table 3)",
            "task_description": "Benchmarks where zero-shot CoT can be assessed against few-shot methods.",
            "performance": "Table 3: Zero-Shot-CoT: GSM8K 40.7%, SVAMP 62.1%, Letter(4) 57.6% (text-davinci-002). Zero-Shot-Automate-CoT (apply Automate-CoT pipeline seeded only by zero-shot prompt) improved to GSM8K 49.1%, SVAMP 74.3%, Letter(4) 59.3%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Manual-CoT GSM8K 46.9% (Zero-Shot-CoT 40.7% &lt; Manual); Automate-CoT (few-shot seed) 49.7% &gt; Zero-Shot 40.7%; Zero-Shot-Automate-CoT can reach or exceed Manual-CoT.",
            "key_findings": "Zero-shot elicitation of CoT can produce reasonable chains, but Automate-CoT applied in a zero-shot seed setting (Zero-Shot-Automate-CoT) can substantially improve zero-shot baseline and in some cases exceed Manual-CoT; automated selection mitigates noisy zero-shot chains.",
            "counter_examples_or_negative_results": "Zero-Shot-CoT by itself underperforms Manual-CoT on some settings (e.g., GSM8K), showing that zero-shot chain elicitation is not always sufficient without selection/augmentation.",
            "uuid": "e4978.6",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "BM25 Retrieval",
            "name_full": "BM25 retrieval-based exemplar selection",
            "brief_description": "A retrieval baseline that selects few-shot exemplars most textually similar to the test question using BM25 lexical retrieval.",
            "citation_title": "The Probabilistic Relevance Framework: BM25 and Beyond",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (compared in Table 6 experiments)",
            "model_description": "BM25 is an off-the-shelf lexical retrieval algorithm; chosen exemplars are then used to prompt LLMs.",
            "reasoning_method_name": "BM25 retrieval-based exemplar selection",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "For a given test question, retrieve the most similar training questions via BM25 and use their CoT exemplars as demonstrations; similarity-focused selection yields exemplars similar in semantics to the test item.",
            "task_name": "Multiple (reported under gpt-3.5-turbo experiments; average comparisons in Table 6)",
            "task_description": "Retrieval-based exemplar selection evaluated against clustering and Automate-CoT.",
            "performance": "Under gpt-3.5-turbo Table 6 shows BM25 average 72.6% vs Manual-CoT 73.6% and Automate-CoT 76.4% (BM25 slightly worse on average), with per-task variation (e.g., GSM8K BM25 64.2% vs Automate-CoT 68.0%).",
            "comparison_with_other_method": true,
            "performance_other_method": "Automate-CoT: consistently higher (e.g., +3.8% avg over BM25); Clustering (k-means) also compared and often outperformed BM25.",
            "key_findings": "Retrieval-by-similarity (BM25) tends to select semantically similar exemplars but overlooks complexity/style diversity, leading to weaker average performance than Automate-CoT which optimizes for exemplar usefulness rather than raw similarity.",
            "counter_examples_or_negative_results": "BM25 on average performed slightly worse than Manual-CoT in the reported experiments and substantially worse than Automate-CoT, indicating similarity-based retrieval can be suboptimal for CoT exemplar selection.",
            "uuid": "e4978.7",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "PromptPG",
            "name_full": "PromptPG (dynamic example-selection via policy gradient)",
            "brief_description": "A reinforcement-learning-based retrieval/selection method that dynamically learns which exemplars to present for each input using policy gradients.",
            "citation_title": "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (compared in Table 6)",
            "model_description": "Neural policy that picks exemplars per input; trained with policy gradient to maximize downstream performance.",
            "reasoning_method_name": "PromptPG (learned exemplar retrieval)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Learn a policy (via reinforcement learning / policy gradient) to select exemplars conditioned on the input; aims for per-instance optimal exemplar choice rather than a fixed few-shot set.",
            "task_name": "Multiple (gpt-3.5-turbo experiments in Table 6)",
            "task_description": "Baseline for dynamic exemplar selection compared to Automate-CoT.",
            "performance": "gpt-3.5-turbo Table 6: PromptPG average 74.0% vs Automate-CoT 76.4% (Automate-CoT outperforms PromptPG on average).",
            "comparison_with_other_method": true,
            "performance_other_method": "Automate-CoT: +2.4% avg improvement over PromptPG in reported experiments.",
            "key_findings": "Policy-gradient based dynamic selection (PromptPG) improves over naive methods but Automate-CoT's augmentation + pruning + variance-reduced selection achieved higher average performance in experiments, suggesting that constructing a broad high-quality pool and optimizing categorical selection distributions per exemplar-position is effective.",
            "counter_examples_or_negative_results": "PromptPG did not beat Automate-CoT in these experiments; no detailed negative cases beyond being lower-avg in Table 6.",
            "uuid": "e4978.8",
            "source_info": {
                "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
            "rating": 1
        },
        {
            "paper_title": "The Probabilistic Relevance Framework: BM25 and Beyond",
            "rating": 1
        }
    ],
    "cost": 0.02094725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</h1>
<p>Kashun Shum ${ }^{\circ <em>}$, Shizhe Diao ${ }^{\circ </em>}$, Tong Zhang ${ }^{\circ}$<br>${ }^{\circ}$ The Hong Kong University of Science and Technology<br>{ksshumab, sdiaoaa, tongzhang}@ust.hk</p>
<h4>Abstract</h4>
<p>Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, AutomateCoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machinegenerated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning ( $+2.7 \%$ ), commonsense reasoning ( $+3.4 \%$ ), symbolic reasoning ( $+3.2 \%$ ), and nonreasoning tasks $(+2.5 \%){ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The recent success in large language models (LLMs) has shown that properly prompted LLMs demonstrate emergent capabilities on complex understanding and question-answering tasks (Wei et al., 2022a). Especially, with the recently proposed chain-of-thought (CoT) prompting (Wei et al., 2022b), LLMs are capable of solving reasoning tasks including arithmetic reasoning, commonsense reasoning, and symbolic reasoning. The</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>basic idea of CoT prompting is adding a few rationale chains to the answer as exemplars to illustrate the intermediate reasoning steps. Following CoT, several recent studies improve it by leveraging self-consistency (Wang et al., 2023), explanation learning (Lampinen et al., 2022), complexity-based prompting (Fu et al., 2023), self-training (Huang et al., 2022), voting verifier (Li et al., 2022a), and bootstrapping (Zelikman et al., 2022).
However, most of them are constrained to a few fixed human-written exemplars, which require significant human efforts to create and adapt to new datasets. The annotation process is nontrivial because humans need to not only select the questions but also carefully design the reasoning steps for each question. In the process of searching for the perfect exemplars, we identify four critical factors that affect the performance of chain-of-thought prompting and require large human effort to deal with: (1) order sensitivity (Zhao et al., 2021): the order combination of the exemplars; (2) complexity (Sugawara et al., 2018; Lai et al., 2021; Fu et al., 2023): the number of reasoning steps of the rationale chains; (3) diversity: the combination of different complex-level exemplars; (4) style sensitivity (Papadopoulos et al., 2010): the writing/linguistic style of the rationale chains. Detailed analysis of the four factors is covered in Section 2. All of these sensitivities make human-based prompt engineering costly and motivate us to find an automatic and task-agnostic way to adapt chain-ofthought exemplars to any downstream tasks.
In this paper, we solve these problems by a CoT augmentation and selection process to find suitable exemplars automatically. This can be divided into three steps: (1) Augment: The language model generates multiple pseudo-chains for query questions automatically. (2) Prune: Based on an assumption: Generating correct reasoning is a necessary condition for generating correct answers. This assumption is natural because the answer is</p>
<p>generated after several reasoning steps. When a correct answer is generated, the rationale chain of these steps is most likely correct, contributing to the final correctness. As a result, We prune the pseudo-chains according to the consistency between generated and ground-truth answers to reduce the noise. (3) Select: Given that all the data have been annotated with rationale paths, we propose to apply a variance-reduced policy gradient strategy <em>Williams (1992); Dong et al. (2020); Zhou et al. (2021); Diao et al. (2022)</em> to estimate the gradients and optimize the selection process to find the most helpful chain-of-thought for each task. Compared to prior manually written CoT, AutomateCoT could find the optimal and diverse CoT automatically, adaptable to any task without human effort. Compared with Auto-CoT <em>Zhang et al. (2023)</em>, which samples diverse questions by clustering and generates rationale chains, AutomateCoT considers and mitigates the aforementioned sensitivity issues, while achieving a greater performance boost for each task. Automate-CoT is a fully automatic pipeline for finding better chain-of-thought prompts, mitigating the sensitivity issues of manually written exemplars, and further improving the performance by a large margin. Experimental results demonstrate the effectiveness of Automate-CoT on arithmetic reasoning (+2.7%), commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning tasks (+2.5%).</p>
<h2>2 Motivation</h2>
<p>Recent studies observed sensitivity issues of GPT-3’s few-shot learning caused by different selections of in-context examples such as order instability <em>Zhao et al. (2021); Zhang et al. (2022); Liu et al. (2022); Lu et al. (2022)</em>. Based on their findings, we first investigate whether these sensitivities still exist in chain-of-thought methods. Then we further explore other factors that would not only affect the performance but require human efforts to deal with. We conclude with the following four factors:</p>
<ul>
<li>Order Sensitivity: Different orders of few-shot exemplars may cause a huge impact on the performance in traditional few-shot prompting <em>Lu et al. (2022)</em>. Thus we conduct experiments on GPT-3 to test if there is such sensitivity in chain-of-thought methods. Although Manual-CoT <em>Wei et al. (2022b)</em> reports that the human-written CoT is robust to order changes ( $&lt;2 \%$ ) with the LaMDA model, we observed that the performance of GPT-3</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The performance across different numbers of hops (reasoning steps of rationale chains) on GSM8K. Manual-CoT refers to the human-written chain-of-thought by <em>Wei et al. (2022b)</em>. Complex-CoT refers to the chain-of-thought using 9-hop rationale chains.</p>
<p>fluctuates with different orders of chain-of-though exemplars. For the GSM8K dataset, we simply randomly shuffle the order of the exemplars in Manual-CoT 10 times and the lowest accuracy can be $59.8 \%$ which is $3.3 \%$ lower than the average accuracy ( $63.1 \%$ ) they report, suggesting that order sensitivity still exists.</p>
<ul>
<li>Complexity: We first define complexity as the number of hops (reasoning steps) in an exemplar where more steps indicate greater complexity. It is observed that human-written CoT tends to be simple ( $\leq 3$ hops), achieving good accuracy in simple math questions while suffering from complex questions, as shown in Figure 1. In addition, a previous study <em>Fu et al. (2023)</em> suggested that using all complex exemplars can improve CoT performance. However, in our experiments (Figure 1), we found that Complex-CoT can improve the accuracy of complex questions, but perform poorly in simple questions. Therefore, we conjecture that the inconsistency between the hops of provided exemplars and the required hops of the real question causes the performance drop, suggesting that determining the appropriate complexity level of exemplars is crucial.</li>
<li>Diversity: Based on the above discovery about complexity, a natural question is what combination of different complex-level exemplars is most effective. However, testing various combinations is a challenging task for humans and requires significant effort to determine the optimal one. In our</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustrations of our proposed approach. The left and middle parts of the figure contain two steps of our method: (1) <strong>Augment and Prune</strong> and (2) <strong>Select</strong>. The right part illustrates the training stage (right top) and the inference stage (right bottom), respectively.</p>
<p>experiments (Figure 1), we found that a combination of different complex-level exemplars outperforms CoT with all complex exemplars, suggesting a complexity-diversity trade-off.</p>
<ul>
<li><strong>Style Sensitivity</strong>: Previous research in educational psychology found that different learning styles would limit the cognitive benefit for students from the prompting (Papadopoulos et al., 2010). We further argue that students with specific learning styles benefit to varying degrees from different styles of prompting. In addition, the empirical evidence from Manual-CoT (Wei et al., 2022b) shows that different annotators can cause up to 28.2% accuracy difference in a symbolic reasoning task, verifying our conjecture. As a result, some <em>bad</em> styles may lead to a huge performance drop. However, humans cannot determine the performance of a particular style beforehand, so it requires trial and error by checking on the validation set, which further increases the effort of writing chain-of-thought exemplars.</li>
</ul>
<p>In light of this empirical evidence, we are motivated to design a framework not only to augment rationale chains but also to select helpful chains adaptively. With this framework, it is expected to bypass the order and style sensitivities and reach a better complexity-diversity trade-off without human effort, finally boosting performance.</p>
<h2>3 Approach</h2>
<p>Our approach receives a training dataset <em>D</em> containing <em>n</em> questions <em>Q</em> = {<em>q</em>1, <em>q</em>2, ..., <em>q</em>n}, and <em>n</em> answers <em>A</em> = {<em>a</em>1, <em>a</em>2, ..., <em>a</em>n}. The overall architecture of our approach is shown in Figure 2. In this section, we start with a detailed description of <em>augment and prune</em> operation and end with an illustration of <em>select</em> operation.</p>
<h3>3.1 Augment and Prune</h3>
<p>Inspired by Wang et al. (2022), which shows that the generated rationale chains are of comparable quality to the human-annotated ones, we aim to automatically generate the rationale chains to augment the candidate exemplars. Given <em>m</em> fixed rationale chains <em>C</em> = {<em>c</em>1, <em>c</em>2, ..., <em>c</em>m}, a question <em>q</em>, we ask the large language model <em>G</em> to generate <em>k</em> rationale chains for each <em>q</em>. A larger <em>k</em> can form a larger pool and some post-processes can be done to improve the quality of the pool. Considering the cost and efficiency, we choose <em>k</em> = 1 for our experiments. Our method works well even without <em>C</em> (i.e., <em>m</em> = 0), which is based on zero-shot prompting. Then we prune those incorrect ones out and only keep those with the correct final answer. In other words, the final answer should be consistent with the ground-truth answer. After pruning, we</p>
<p>obtain a pool of $K$ high-quality exemplars.</p>
<h3>3.2 Select</h3>
<p>With a large pool of high-quality exemplars, we cannot directly apply all of them due to four considerations: (1) context length limit: the maximum length is 2,048 for GPT-3, so we cannot feed too many exemplars into the model. (2) fair comparison: existing studies usually take 4-8 questionanswer pairs as exemplars following Wei et al. (2022b). (3) sensitivity: the model performance may be sensitive to the contexts (Jiang et al., 2020), orders (Lu et al., 2022) and lengths (Lester et al., 2021) from the observation of prompt learning literature. (4) adaptation: different downstream tasks may require different exemplars. Therefore, a natural idea is to select the most suitable 4-8 exemplars automatically.
The process can be deemed as optimizing a supervised model with latent variables. For each chain-of-thought index $i$, we initialize a latent variable $j_{i} \sim \operatorname{Cat}\left(\boldsymbol{p}<em i="i">{i}\right)$. The random variable $j</em>}$ is sampled with the probability distribution $\boldsymbol{p<em 1="1" i_="i,">{i}=$ $\left[p</em>}, \cdots, p_{i, N}\right]$ over the $N$ candidate demonstration indexes, where $\boldsymbol{p<em 1="1">{i} \in \mathcal{C}$ and $\mathcal{C}=\left{\boldsymbol{p}:|\boldsymbol{p}|</em>}=\right.$ $1,0 \preceq \boldsymbol{p} \preceq 1}$. Since $\boldsymbol{p<em i="1">{i}$ is independent of each other, the joint probability of the whole input exemplars is $P(T)=\Pi</em>([T, S], y))$ is not possible because of the inaccessible gradients, where $y$ is the label. We resort to the variance-reduced policy gradient estimator (VRPGE) (Williams, 1992; Dong et al., 2020; Zhou et al., 2021; Diao et al., 2022), a kind of reinforcement learning method to optimize the loss function via forward propagation with:}^{n} P\left(t_{i}\right)=\Pi_{i=1}^{n} p_{i, j_{i}}$. The loss is formulated as $\mathcal{L}(\mathcal{G}([T, S], y))$, where $T$ represents the full few-shot exemplars, $t_{i}$ denotes the i-th exemplar, and $S$ is the current question (user's query). However, directly updating the prompts by back-propagating through $\nabla_{\boldsymbol{p}_{i}} \mathcal{L}(\mathcal{G</p>
<p>$$
\mathbb{E}_{T}[\mathcal{L}(T)]=\int \mathcal{L}(T) P(T) \mathrm{d} T
$$</p>
<p>and estimate the gradient of $\boldsymbol{p}<em _boldsymbol_p="\boldsymbol{p">{i}$ by:
$\boldsymbol{g}</em><em k="1">{i}}^{c r}=\frac{1}{I-1} \sum</em>}^{I}\left(\mathcal{L}\left(T^{(k)}\right)-\frac{1}{I} \sum_{j=1}^{I} \mathcal{L}\left(T^{(j)}\right)\right) \nabla_{\boldsymbol{p<em i="i">{i}} \log P\left(t</em>\right)$
where $T^{(k)}, k=1, \cdots, I$ are sampled independently from $P(T)$. Therefore, the exemplar distribution $\boldsymbol{p}_{i}$ can be updated by a projected stochastic gradient descent algorithm:</p>
<p>$$
\boldsymbol{p}<em _mathcal_C="\mathcal{C">{i} \leftarrow \operatorname{proj}</em>}}\left(\boldsymbol{p<em _boldsymbol_p="\boldsymbol{p">{i}-\eta \cdot \boldsymbol{g}</em>\right), i=1, \cdots, n
$$}_{i}}^{c s</p>
<p>where $\eta$ is the learning rate, $I$ is the sample size, and $\operatorname{proj}_{\mathcal{C}}$ is the projection calculation (details are presented in the Appendix A).</p>
<h2>4 Experimental Settings</h2>
<p>In this section, we first introduce the setting of eleven datasets and their corresponding evaluation metrics (§ 4.1). Then the baseline models (§ 4.2) and implementation details (§ 4.3) are presented in the following two subsections, respectively. Full details about the experimental setting are illustrated in Appendix B.</p>
<h3>4.1 Datasets and Evaluation Metrics</h3>
<p>Following Wei et al. (2022b), we conduct our experiments on eight reasoning tasks, including five math word problem datasets: GSM8K, ASDiv, SVAMP, AQuA, and SingleOp; two commonsense reasoning datasets: CommonsenseQA (CSQA) and StrategyQA, and one symbolic reasoning task: Last Letter Concatenation (Letter (4)). We also generalize our method to non-reasoning tasks including one question-answering task (OpenBookQA), one natural language inference task (e-SNLI), and one sentiment analysis task (SST-2). The detailed statistics of the datasets are listed in Table 5. The evaluation metric for all tasks is the exact match accuracy. First, we conduct pre-processing for predictions to remove all the special symbols. For example, "\$100,000" will be processed to "100000". Then we check if it has the same value as the ground truth to calculate the exact match accuracy.</p>
<h3>4.2 Baselines</h3>
<p>We compare our method with the following baseline methods: chain-of-thought (ManualCoT) (Wei et al., 2022b), self-consistency (SC) (Wang et al., 2023), and Auto-CoT (Zhang et al., 2023). And we utilize the public APIs from OpenAI's services ${ }^{2}$ and test with text-davinci-002 and code-davinci-002.</p>
<h3>4.3 Implementation</h3>
<p>Augment and Prune: Following Wei et al. (2022b) and Wang et al. (2022), we keep the same number of exemplars (4-8) listed in Table 5. For main experiments, we augment and prune a pool of 100 high-quality exemplars for all datasets.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">AQUA</th>
<th style="text-align: center;">SingleOp</th>
<th style="text-align: center;">CSQA</th>
<th style="text-align: center;">STQA</th>
<th style="text-align: center;">Letter (4)</th>
<th style="text-align: center;">OBQA</th>
<th style="text-align: center;">e-SNLI</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prior Best*</td>
<td style="text-align: center;">$55.0^{*}$</td>
<td style="text-align: center;">$75.3^{b}$</td>
<td style="text-align: center;">$57.4^{*}$</td>
<td style="text-align: center;">$37.9^{d}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$91.2^{*}$</td>
<td style="text-align: center;">$73.9^{f}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$97.5^{g}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Auto-CoT</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Manual-CoT</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;">+ Automate-CoT</td>
<td style="text-align: center;">49.7 $\uparrow$</td>
<td style="text-align: center;">74.2 $\uparrow$</td>
<td style="text-align: center;">73.3 $\uparrow$</td>
<td style="text-align: center;">37.9 $\uparrow$</td>
<td style="text-align: center;">90.0 $\uparrow$</td>
<td style="text-align: center;">76.1 $\uparrow$</td>
<td style="text-align: center;">67.9 $\uparrow$</td>
<td style="text-align: center;">58.9 $\uparrow$</td>
<td style="text-align: center;">79.1 $\uparrow$</td>
<td style="text-align: center;">82.3 $\uparrow$</td>
<td style="text-align: center;">87.5 $\uparrow$</td>
<td style="text-align: center;">70.6 $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">72.8</td>
</tr>
<tr>
<td style="text-align: center;">+ Automate-CoT</td>
<td style="text-align: center;">67.8 $\uparrow$</td>
<td style="text-align: center;">78.9 $\uparrow$</td>
<td style="text-align: center;">80.5 $\uparrow$</td>
<td style="text-align: center;">43.4 $\uparrow$</td>
<td style="text-align: center;">91.9 $\uparrow$</td>
<td style="text-align: center;">80.2 $\uparrow$</td>
<td style="text-align: center;">76.3 $\uparrow$</td>
<td style="text-align: center;">60.8 $\uparrow$</td>
<td style="text-align: center;">84.8 $\uparrow$</td>
<td style="text-align: center;">86.4 $\uparrow$</td>
<td style="text-align: center;">90.6 $\uparrow$</td>
<td style="text-align: center;">76.5 $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Auto-CoT</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Manual-CoT</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">74.2</td>
</tr>
<tr>
<td style="text-align: center;">+ Automate-CoT</td>
<td style="text-align: center;">67.6 $\uparrow$</td>
<td style="text-align: center;">83.1 $\uparrow$</td>
<td style="text-align: center;">78.2 $\uparrow$</td>
<td style="text-align: center;">47.8 $\uparrow$</td>
<td style="text-align: center;">92.4 $\uparrow$</td>
<td style="text-align: center;">81.3 $\uparrow$</td>
<td style="text-align: center;">75.3 $\uparrow$</td>
<td style="text-align: center;">75.0 $\uparrow$</td>
<td style="text-align: center;">83.2 $\uparrow$</td>
<td style="text-align: center;">71.2 $\uparrow$</td>
<td style="text-align: center;">90.8 $\uparrow$</td>
<td style="text-align: center;">76.9 $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">80.6</td>
</tr>
<tr>
<td style="text-align: center;">+ Automate-CoT</td>
<td style="text-align: center;">82.4 $\uparrow$</td>
<td style="text-align: center;">88.9 $\uparrow$</td>
<td style="text-align: center;">87.8 $\uparrow$</td>
<td style="text-align: center;">55.6 $\uparrow$</td>
<td style="text-align: center;">94.0 $\uparrow$</td>
<td style="text-align: center;">84.0 $\uparrow$</td>
<td style="text-align: center;">80.6 $\uparrow$</td>
<td style="text-align: center;">76.2 $\uparrow$</td>
<td style="text-align: center;">89.7 $\uparrow$</td>
<td style="text-align: center;">78.3 $\uparrow$</td>
<td style="text-align: center;">92.8 $\uparrow$</td>
<td style="text-align: center;">82.8 $\uparrow$</td>
</tr>
</tbody>
</table>
<p>Table 1: The overall performance of Automate-CoT and the comparison against existing models on eleven downstream tasks. Manual-CoT and SC represent chain-of-thought [wei2022chain] and self-consistency [wang2023self] methods. Bold denotes the best in code-davinci-002-based methods and Underline denotes the best in text-davinci-002-based methods. *: Prior Best is the best performance before CoT comes out. $a$ : [cobbe2021coT], $b$ : [lan2022coT], $c$ : [yi2022coT], $d$ : [amini2019coT], $e$ : [xu2022coT], $f$ : [chowdhery2022coT], $g$ : [raffel2020coT]. Most statistics of Manual-CoT and SC have been obtained directly from their latest version. For some entries they did not report, we obtain the result from DIVERSE [li2022diverse].</p>
<p>Select: Both the training and validation sets have a size of 100 to reach a performance and cost tradeoff. Then by utilizing the log probability returned by API calls, we calculate the cross-entropy loss of the answer token. Finally, we optimize the latent variables by AdamW [Loshchilov and Hutter, 2019] for 5 epochs with a learning rate of $1 \times 10^{-3}$ and batch size of 10. After optimization, we choose the exemplars combination $\left(\arg \max \boldsymbol{p}_{i}\right)$ with the highest validation accuracy to be further evaluated on the test set. By default, we query the language model once to get the answer. Under the selfconsistency setting, similar to [wang2023self], we query the language model 40 times and choose the most consistent one as the final answer.</p>
<p>Hyper-parameter Setting: Under few-shot setting, we set max_tokens $=256$ for all augmentation, selection and inference. In addition, we set logprobs $=5$ when training. Moreover, we set temperature $=0.7$ for evaluation under self-consistency while temperature $=0$ for all other cases.</p>
<h2>5 Experimental Results</h2>
<p>The experimental results are shown in Table 1. We discuss our results in three sections based on the task categories. Automate-CoT are averaged over three runs, and the variance over different runs is reported in Appendix Table 7. Overall, AutomateCoT achieves superior results on all tasks. With text-davinci-002, Automate-CoT outperforms Manual-CoT and SC by $2.6 \%$ and $3.7 \%$ on average.
With code-davinci-002, Automate-CoT also outperforms Manual-CoT and SC by $2.7 \%$ and $2.2 \%$, respectively.</p>
<p>Arithmetic Reasoning: For text-davinci-002, Automate-CoT improves Manual-CoT by 2.7\% over five arithmetic reasoning tasks. In addition, under the self-consistency setting, AutomateCoT improves SC by a large margin by an average of $3.3 \%$. Moreover, compared to AutoCoT, Automate-CoT also outperforms it on all three arithmetic tasks (GSM8K, SVAMP, and AQuA). While for code-davinci-002, AutomateCoT achieves an average of $2.4 \%$ improvement across all five arithmetic reasoning tasks, illustrating the effectiveness of our proposed approach with different language models. Additionally, AutomateCoT outperforms Auto-CoT in GSM8K by 4.8\%, since Auto-CoT only constructs experiments on GSM8K under code-davinci-002. AutomateCoT demonstrates consistent improvement over arithmetic tasks, especially on GSM8K, where it can outperform Manual-CoT by a large margin. Finally, under the self-consistency setting, AutomateCoT also shows similar trends to improve the SC baseline, demonstrating the synergistic effects of our proposed method and self-consistency method.</p>
<p>Commonsense and Symbolic Reasoning Similarly, on commonsense and symbolic reasoning tasks, Automate-CoT demonstrates significant improvement over Manual-CoT, SC, and Auto-CoT. It achieves an average of $2.5 \%$ and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparisons between Random Selection, Manual-CoT and Automate-CoT on six datasets.
$3.4 \%$ improvement on text-davinci-002 and code-davinci-002 respectively, demonstrating that our method is effective on different task types. More surprisingly, the improvement in the Letter (4) is significant, demonstrating our method's robustness to deal with out-of-distribution data.</p>
<p>Non-Reasoning Tasks Automate-CoT has also reached great success on question answering (OpenBookQA), natural language inference (eSNLI), and sentiment analysis (SST-2) tasks by an improvement of $2.8 \%, 3.4 \%$ and $1.3 \%$, respectively. The results show that our method can be generalized to various task types and is not limited to reasoning tasks.</p>
<h2>6 Additional Experiments and Analysis</h2>
<p>We further conduct several experiments to evaluate the effectiveness of Automate-CoT and analyze the contributions of each module. Since queries to text-davinci-002 are limited and expensive, most additional experiments are conducted with code-davinci-002.</p>
<h3>6.1 Effects of Selection Algorithm</h3>
<p>After obtaining a large pool of exemplars, a natural question would be what is the performance if we randomly select from the pool regardless of order. In Figure 3, we compare the accuracy obtained by random selection, human-written (Manual-CoT), and our Automate-CoT. For random selection, we randomly sample exemplars from the pool and combine them regardless of order to form the prompts. We repeat this process five times and report the accuracy with an error bar. The results show that random selection suffers from high variance and relatively low accuracy compared to Manual-CoT and Automate-CoT. Surprisingly, we observed the average performance of a random selection from modelgenerated exemplars can outperform Manual-CoT in some datasets (e.g. GSM8K, CSQA). This also suggests that manual prompt engineering needs to take efforts to design carefully in terms of difficulty,
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The performance across different pool sizes of Automate-CoT compare with Manual-CoT. Pool size refers to the number of exemplars in the pool.
diversity, and style. In conclusion, if we simply randomly select the exemplars from the pool, it is very likely to obtain a much lower accuracy than the manually written method. However, our AutomateCoT can consistently outperform random selection and Manual-CoT which shows the effectiveness of our method.</p>
<h3>6.2 Effects of Pool Size</h3>
<p>We further conduct a set of experiments to test different pool sizes. As shown in Figure 4, if the pool size is limited to only 10 , the performance of Automate-CoT is worse than Manual-CoT or comparable with Manual-CoT. It turns out that if the pool size is small, Automate-CoT is unable to select a good combination to beat carefully designed Manual-CoT. However, Automate-CoT can outperform Manual-CoT when the pool size reaches 20 or larger. The trends show that the performance would be better as pool size keeps increasing. This is intuitive and matches our hypothesis because as pool size increases, there would be more complex, diverse exemplars to choose from. It is expected that the performance would keep increasing, but since more queries for GPT-3 are time-consuming and expensive, we limited these additional experiments to have a max pool size of 150.</p>
<h3>6.3 Effects of Chain Complexity</h3>
<p>It is observed that exemplars written by human are rather simple, so we further explore how chain complexity affect performance. We randomly pick</p>
<table>
<thead>
<tr>
<th>RUN</th>
<th>GSM8K</th>
<th>SVAM</th>
<th>Letter(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rand(Training Set)</td>
<td>67.55</td>
<td>78.2</td>
<td>75.0</td>
</tr>
<tr>
<td>Rand(Training Set)</td>
<td>67.93</td>
<td>77.8</td>
<td>76.6</td>
</tr>
<tr>
<td>Rand(Training Set)</td>
<td>67.25</td>
<td>77.6</td>
<td>75.8</td>
</tr>
<tr>
<td>Variance</td>
<td>0.077</td>
<td>0.062</td>
<td>0.426</td>
</tr>
</tbody>
</table>
<p>Table 2: The effect of different randomly chosen training set on performance over three datasets.</p>
<p>8 exemplars with complex rationale chains (each has 9 hops) and refer to them as Complex-CoT. For human-written exemplars (Manual-CoT) wei2022human, exemplars are all 2-3 hops. We compare them with our Automate-CoT which has an average hop of 4 and ranges from 2-hop to 6-hop on GSM8K dataset. From Figure 1, Manual-CoT has an overall accuracy of 62%, achieving good results on simple questions. However, it suffers from complex math questions, especially 7-hop and 8-hop questions. Complex-CoT can improve the accuracy on complex questions by a large margin but it performs poorly on simple questions, which only has an overall accuracy of 60%. In contrast, our Automate-CoT can select a combination of different complex-level exemplars automatically. It achieves good results on simple questions and reasonable results on complex questions at the same time, outperforming both Manual-CoT and Complex-CoT by a large margin. The result shows the superiority of our method because it can automatically achieve a complexity-diversity trade-off.</p>
<h3>6.4 Effects of Training Example Selection</h3>
<p>Since training examples to construct CoT are randomly chosen, we also measure the performance vary regarding this random selection. Three different randomly chosen training sets are used to train Automate-CoT and the results are reported in Table 2. According to the result, Automate-CoT shows its robustness to training examples. Randomly chosen training examples have quite a small impact on the result.</p>
<h3>6.5 Bypass Manual Effort by Zero-shot-CoT</h3>
<p>Starting with 4-8 manually constructed chain-of-thought exemplars, our methods show great success in automatically generating, pruning, and selecting suitable exemplars for each task. After that, we raise a new question: Can we further bypass the effort of writing the initial chain-of-thought exemplars? Based on current research of Zero-ShotCoT kojima2022, we found it is possible.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">SVAM</th>
<th style="text-align: center;">Letter (4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-Shot-CoT</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">57.6</td>
</tr>
<tr>
<td style="text-align: left;">Manual-CoT</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">56.6</td>
</tr>
<tr>
<td style="text-align: left;">Auto-CoT</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">$\mathbf{5 9 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-Automate-CoT</td>
<td style="text-align: center;">$\mathbf{4 9 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 3}$</td>
<td style="text-align: center;">59.3</td>
</tr>
<tr>
<td style="text-align: left;">Automate-CoT</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">58.9</td>
</tr>
</tbody>
</table>
<p>Table 3: The performance of Automate-CoT in zeroshot setting compared with other baselines. Lightgray highlights our main model which uses a manually constructed chain-of-thought and is not intended for comparison. We list it here only for reference.</p>
<p>Instead of using 4-8 manual-written exemplars to generate the chains, we simply add "Let's think step by step." and let LLMs generate the chains. We test the result under text-davinci-002 model on GSM8K, SVAM, and Letter (4) and compare it with Zero-shot-CoT, Manual-CoT and Auto-CoT. Surprisingly, we observe the result can be comparable and even outperform Manual-CoT and AutoCoT a bit as shown in Table 3. The results further demonstrate that our method can effectively select a suitable combination of exemplars even from a pool that may contain low-quality chains. In conclusion, if a dataset already has manually written chains, our method can be applied to boost the performance. If a dataset does not have manually written chains, our method can still be used to achieve higher accuracy than if it had manually written chains, demonstrating the superiority of our method.</p>
<h2>7 Ablation Study</h2>
<p>In this section, we further conduct ablation experiments to verify the advantage of the generated prompts on four factors, respectively.</p>
<p>Advantage over Order Factor The advantages of Automate-CoT on order factor can be viewed in two ways. Firstly, it requires a large human effort to determine a good order by trying many different orders on validation sets. However, Automate-CoT can automatically construct the exemplars without further adjustment to have a good result. Secondly, Automate-CoT is less affected by the order sensitivity. We further conduct an experiment to compare selected exemplars and random permutations of Automate-CoT's selected exemplars as shown in Table 4. We randomly permutate the selected exemplars to see how performance varies compared to the selected order by Automate-CoT. It is observed that the order sensitivity still exists and our se-</p>
<table>
<thead>
<tr>
<th>RunS</th>
<th>GSM8K</th>
<th>SVAMP</th>
<th>Letter(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td>perm(Automate-CoT)</td>
<td>66.7</td>
<td>77.2</td>
<td>73.0</td>
</tr>
<tr>
<td>perm(Automate-CoT)</td>
<td>66.6</td>
<td>78.4</td>
<td>72.6</td>
</tr>
<tr>
<td>perm(Automate-CoT)</td>
<td>66.9</td>
<td>78.0</td>
<td>72.0</td>
</tr>
<tr>
<td>perm(Automate-CoT)</td>
<td>67.8</td>
<td>78.2</td>
<td>74.2</td>
</tr>
<tr>
<td>perm(Automate-CoT)</td>
<td>67.5</td>
<td>78.1</td>
<td>75.0</td>
</tr>
<tr>
<td>Automate-CoT</td>
<td>68.4</td>
<td>78.7</td>
<td>75.2</td>
</tr>
<tr>
<td>Mean_{Δstd}</td>
<td>67.3_{±0.64}</td>
<td>78.2_{±0.46}</td>
<td>73.7_{±1.21}</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of different permutations orders of Automate-CoT's selected examplars.
lected exemplars have better performance than that of all 5 random permutation runs, demonstrating Automate-CoT can automatically choose a good order without any human effort.</p>
<p>Advantage over Complexity Factor As discussed in the complexity factor of Section 2, we show that the complexity of manually written chains is quite simple (less than or equal to 3 hops). It would require more human effort to design complex rationales. However, Automate-CoT can automatically augment and select examples with different complexity, reaching a better trade-off accuracy between simple questions and complex questions (Appendix Table 9).</p>
<p>Advantage over Diversity Factor The diversity of Manual-CoT or Complexity-CoT is limited. For example, every exemplar of Complexity-CoT has the same complexity and every exemplar of Manual-CoT ranges from 1-3 hops as illustrated in the motivation section. However, Automate-CoT can automatically select an optimal combination of complexity that best suits the dataset. For example, our selected exemplars on GSM8K have an average hop of 5.4 and range from 3 hops to 8 hops as shown in Appendix G. It contains both simple exemplars and complex exemplars which reach the best performance.</p>
<p>Advantage over Style Factor Our extensive experience with multiple experiments indicates that a good linguistic style is typically formal and detailed. This style entails the use of (1) explicit and logical connection words (e.g., "so", "that means"), (2) detailed reasoning steps within a single sentence, (3) symbols when appropriate (e.g., using the \$ symbol to denote monetary values), and (4) minimizing the use of abbreviations. We further conduct an ablation experiment to test how our method can choose the examples with better style. Firstly, we use Automate-CoT to select 8 rationale exem- plars $S_{1}=\left[A_{1}, B_{1}, C_{1}, D_{1}, E_{1}, F_{1}, G_{1}, H_{1}\right]$ for GSM8K. Then we copy this set and edit its written / linguistic style manually to be worse while keeping the order, complexity, and diversity the same which gives $S_{2}=\left[A_{2}, B_{2}, C_{2}, D_{2}, E_{2}, F_{2}, G_{2}, H_{2}\right]$. Now we have 16 examplars says $S=$ $\left[A_{1}, A_{2}, B_{1}, B_{2}, \ldots, H_{1}, H_{2}\right]$. A-H represents the No.1-8 exemplars. Subscript 1 represents the originally selected exemplars and 2 represents the edited ones. Then, Automate-CoT selects 8 exemplars from the previous 16 exemplars. Note that we limit Automate-CoT to select exactly one of $\left[A_{1}, A_{2}\right]$ and $\left[B_{1}, B_{2}\right] \ldots$ and keep the same order A-H. Subsequently, when we perform Automate-CoT algorithm, we observe that Automate-CoT is able to successfully select the original exemplars $S_{1}$. Furthermore, we find that the selected exemplars can outperform the non-selected exemplars by $2 \%$.</p>
<h2>8 Related Work</h2>
<p>In this section, we first review the recent progress of prompt-based learning (§8.1) and chain-of-thought prompting (§8.2), and then discuss the black-box optimization methods (§8.3).</p>
<h3>8.1 Prompt-based Learning</h3>
<p>Prompt-based Learning (Prompting) aims to leverage large language models (LLMs) (Devlin et al., 2018; Liu et al., 2019; He et al., 2021; Diao et al., 2020, 2021) to trigger helpful knowledge for downstream tasks. Existing prompting methods can be categorized into two types based on their nature: 1) discrete prompts (Wallace et al., 2019; Shin et al., 2020; Jiang et al., 2020; Yuan et al., 2021; Haviv et al., 2021; Gao et al., 2021; Ben-David et al., 2022; Davison et al., 2019; Su et al., 2022; Diao et al., 2022; Guo et al., 2023) and continuous prompts (Zhong et al., 2021; Qin and Eisner, 2021; Hambardzumyan et al., 2021; Liu et al., 2021; Han et al., 2021; Li and Liang, 2021; Yang et al., 2023). Discrete prompts optimize a sequence of discrete tokens, while continuous prompts optimize a sequence of vectors. One of the most important advantages of prompting is saving fine-tuning costs by refraining from the parameter changes of large language models, and we only need to optimize a small set of parameters.</p>
<h3>8.2 Chain-of-thought Prompting</h3>
<p>Chain-of-thought (Wei et al., 2022b) introduces a chain of rationale steps for each exemplar of</p>
<p>in-context learning and significantly improves the performance on several complex tasks like arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Based on this simple yet effective idea, many following works propose different strategies to improve it: self-consistency (Wang et al., 2023), explanation learning (Lampinen et al., 2022), complexity-based prompting (Fu et al., 2023), self-training (Huang et al., 2022), voting verifier (Li et al., 2022a), zero-shot prompting (Kojima et al., 2022; Fung et al., 2022), and bootstrapping (Zelikman et al., 2022).</p>
<h3>8.3 Black-box Optimization</h3>
<p>Nowadays, large language models provide services as commercial APIs deployed in the cloud, such as OpenAI's GPT-3 (Brown et al., 2020) and ChatGPT $^{3}$. It usually accepts query inputs and outputs the predictions with a web interface. Their model parameters and gradients are not accessible, causing difficulties in optimization with gradients. Previous research on black-box optimization mainly focuses on score-based black-box adversarial attack (Ilyas et al., 2018, 2019; Huang and Zhang, 2020; Andriushchenko et al., 2020; Cheng et al., 2019). Most recently, black-box prompt learning (Diao et al., 2022; Sun et al., 2022; Prasad et al., 2022) is introduced, aiming to optimize the prompts without accessing gradients, but their models suffer from limited reasoning abilities and are limited to zero-shot settings with classification task.</p>
<h2>9 Conclusion</h2>
<p>In this paper, we proposed a chain-of-thought optimization method consisting of three steps: augment, prune, and select. Automate-CoT first generates rationale chains according to the standard CoT process with several exemplars, and then prunes those incorrect ones according to the consistency of the predicted answer and ground-truth answer. Finally, we apply a variance-reduced policy gradient strategy to estimate the gradients and optimize the latent variables to select better CoTs. Experimental results demonstrate the effectiveness of our method on arithmetic reasoning, commonsense reasoning, symbolic reasoning tasks, and non-reasoning tasks.</p>
<h2>10 Limitations</h2>
<p>It is shown that Automate-CoT demonstrates superior performance over previous chain-of-thought</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>prompting methods. However, despite these exciting results, there are still some limitations to our current work, as well as potential opportunities for future research.</p>
<p>Comparision with Fine-tuning : Our main baselines include original chain-of-thought (Wei et al., 2022b), self-consistency (Wang et al., 2023) which are manual-written based prompt method. In addition, we also compare the clustering-based and retrieval-based methods to select the prompt exemplars like Auto-CoT (Zhang et al., 2023), BM25 (Robertson, 2009), PromptPG (Lu et al., 2023). As large language models are dominating the field, the performance of training the large language models by using these labeled data might be interesting. However, it is not covered in this study due to the prompt setting of this study and limited resources.</p>
<p>Prompt Style Definition : Another limitation of this work is that it does not provide a rigorous definition of what constitutes good versus bad linguistic style. While we have observed several patterns of good and bad style during numerous experiments, and the results show that Automate-CoT is able to mitigate style sensitivity in Manual-CoT, we cannot determine what perfect style entails. As such, we acknowledge that defining what constitutes good versus bad linguistic style can be a challenging task and an important area for further exploration and development.</p>
<h2>Acknowledgments</h2>
<p>We thank the anonymous reviewers for their valuable suggestions. This work was supported by the General Research Fund (GRF) of Hong Kong (No. 16310222). Shizhe Diao was supported by the Hong Kong Ph.D. Fellowship Scheme (HKPFS).</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. 2020. Square at-</p>
<p>tack: a query-efficient black-box adversarial attack via random search. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXIII, pages 484-501. Springer.</p>
<p>Eyal Ben-David, Nadav Oved, and Roi Reichart. 2022. PADA: Example-based prompt learning for on-thefly adaptation to unseen domains. Transactions of the Association for Computational Linguistics, 10:414433.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 9560-9572.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.</p>
<p>Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. 2019. Improving black-box adversarial attacks with a transfer-based prior. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 10932-10942.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv.</p>
<p>Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and Yonggang Wang. 2020. Zen: Pre-training chinese text encoder enhanced by n-gram representations. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4729-4740.</p>
<p>Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, and Tong Zhang. 2022. Black-box prompt learning for pre-trained language models. ArXiv preprint, abs/2201.08531.</p>
<p>Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan Song, and Tong Zhang. 2021. Taming pre-trained language models with n-gram representations for lowresource domain adaptation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3336-3349.</p>
<p>Zhe Dong, Andriy Mnih, and George Tucker. 2020. Disarm: An antithetic gradient estimator for binary latent variables. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. Complexity-based prompting for multi-step reasoning. In International Conference on Learning Representations.</p>
<p>Yi R Fung, Tuhin Chakraborty, Hao Guo, Owen Rambow, Smaranda Muresan, and Heng Ji. 2022. Normsage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly. arXiv preprint arXiv:2210.08604.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830, Online. Association for Computational Linguistics.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the</p>
<p>Association for Computational Linguistics, 9:346361.</p>
<p>Chunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li, Zhihua Wen, Kaixuan Wang, and Ting Wang. 2023. Retrieval-augmented gpt-3.5-based text-to-sql framework with sample-aware prompting and dynamic revision chain. arXiv preprint arXiv:2307.05074.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4921-4933, Online. Association for Computational Linguistics.</p>
<p>Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021. PTR: Prompt Tuning with Rules for Text Classification. ArXiv preprint, abs/2105.11259.</p>
<p>Adi Haviv, Jonathan Berant, and Amir Globerson. 2021. BERTese: Learning to speak to BERT. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3618-3623, Online. Association for Computational Linguistics.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DEBERTA: Decodingenhanced bert with disentangled attention. In International Conference on Learning Representations.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. ArXiv preprint, abs/2210.11610.</p>
<p>Zhichao Huang and Tong Zhang. 2020. Black-box adversarial attack with transferable model-based embedding. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-box adversarial attacks with limited queries and information. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2142-2151. PMLR.</p>
<p>Andrew Ilyas, Logan Engstrom, and Aleksander Madry. 2019. Prior convictions: Black-box adversarial attacks with bandits and priors. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics.</p>
<p>Yuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang, and Dongyan Zhao. 2021. Why machine reading comprehension models learn shortcuts? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 989-1002, Online. Association for Computational Linguistics.</p>
<p>Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. 2022. Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022. Association for Computational Linguistics.</p>
<p>Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. 2022. Mwptoolkit: an open-source framework for deep learning-based math word problem solvers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1318813190.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Linguistics.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022a. On the advance of making language models better reasoners. ArXiv preprint, abs/2206.02336.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022b. On the advance of making language models better reasoners.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word</p>
<p>problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT Understands, Too. ArXiv preprint, abs/2103.10385.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR).</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086-8098, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online. Association for Computational Linguistics.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381-2391, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instructions with human feedback. ArXiv preprint, abs/2203.02155.</p>
<p>Pantelis Papadopoulos, Stavros Demetriadis, Ioannis Stamelos, and Ioannis Tsoukalas. 2010. The effect of prompting to students with different learning styles. Multicultural Education and Technology Journal, 4:198-213.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, Jian-Guang Lou, and Weizhu Chen. 2022. Reasoning like program executors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 761779. Association for Computational Linguistics.</p>
<p>Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2022. Grips: Gradient-free, edit-based instruction search for prompting large language models. ArXiv preprint, abs/2203.07281.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212, Online. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.
S. Robertson. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 3(4):333-389.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv preprint, abs/2206.04615.</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. ArXiv preprint, abs/2209.01975.</p>
<p>Saku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. 2018. What makes reading comprehension questions easier? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208-4219, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 20841-20855. PMLR.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153-2162, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Rationaleaugmented ensembles in language models. ArXiv preprint, abs/2207.00747.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Transactions
on Machine Learning Research. Survey Certification.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3):229-256.</p>
<p>Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, and Xuedong Huang. 2022. Human parity on commonsenseqa: Augmenting self-attention with external attention. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 2762-2768. Main Track.</p>
<p>Ke Yang, Charles Yu, Yi R Fung, Manling Li, and Heng Ji. 2023. Adept: A debiasing prompt framework. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10780-10788.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems.</p>
<p>Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 91349148 .</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In International Conference on Learning Representations.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.</p>
<p>Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [MASK]: Learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5017-5033, Online. Association for Computational Linguistics.</p>
<p>Xiao Zhou, Weizhong Zhang, Zonghao Chen, Shizhe Diao, and Tong Zhang. 2021. Efficient neural network training via forward and backward propagation sparsification. In Advances in Neural Information Processing Systems, volume 34, pages 15216-15229.</p>
<p>Algorithm 1 The black-box optimization procedures.
Require: Input batch $S$, Label batch $Y$, Parameter of categorical distribution $\boldsymbol{p}<em n="n">{1}, \cdots, \boldsymbol{p}</em>$.
for $k \leq I$ do
$\operatorname{Sample} j_{1}^{(k)} \sim \operatorname{Cat}\left(\boldsymbol{p}}$, Prediction model $\mathcal{G}$, Loss function $\mathcal{L<em n="n">{1}\right), \cdots, j</em>}^{(k)} \sim \operatorname{Cat}\left(\boldsymbol{p<em 1="1">{n}\right)$
$T^{(k)}=t</em>\right]$
end for
$\mathcal{L}}^{(k)} \cdots t_{n}^{(k)}=\mathcal{V}\left[j_{1}^{(k)}\right] \cdots \mathcal{V}\left[j_{n}^{(k)<em k="1">{\text {avg }}=\frac{1}{I} \sum</em>, S\right], Y\right)$
for $i \leq n$ do
$\boldsymbol{g}}^{I} \mathcal{L}\left(\mathcal{G}\left[T^{(k)<em i="i">{\boldsymbol{p}</em>}}^{c o}=\frac{1}{I-1} \sum_{k=1}^{I} \nabla_{\boldsymbol{p<em i="i">{i}} \log P\left(t</em>}^{(k)}\right)\left(\mathcal{L}\left(\mathcal{G}\left[T^{(k)}, S\right], Y\right)-\mathcal{L<em i="i">{\text {avg }}\right)$
$\boldsymbol{p}</em>} \leftarrow \operatorname{proj<em i="i">{\mathcal{C}}\left(\boldsymbol{p}</em>}-\eta \cdot \boldsymbol{g<em i="i">{\boldsymbol{p}</em>\right)$
end for
10: return $\boldsymbol{p}}}^{c o<em n="n">{1}, \cdots \boldsymbol{p}</em>$</p>
<h1>A Algorithm Details</h1>
<p>In this section, we provide more details about the derivation of the equation (1) in Section 3.2. Given the loss function:</p>
<p>$$
\mathbb{E}_{T}[\mathcal{L}(T)]=\int \mathcal{L}(T) P(T) \mathrm{d} T
$$</p>
<p>We can estimate the gradient of $\boldsymbol{p}_{i}$ by:</p>
<p>$$
\begin{aligned}
\nabla_{\boldsymbol{p}<em T="T">{i}} \mathbb{E}</em>}[\mathcal{L}(T)] &amp; =\int \mathcal{L}(T) \nabla_{\boldsymbol{p<em _boldsymbol_p="\boldsymbol{p">{i}} P(T) \mathrm{d} T \
&amp; =\int \mathcal{L}(T) \frac{P(T)}{P(T)} \nabla</em><em _boldsymbol_p="\boldsymbol{p">{i}} P(T) \mathrm{d} T \
&amp; =\int P(T) \mathcal{L}(T) \nabla</em><em P_T_="P(T)">{i}} \log P(T) \mathrm{d} T \
&amp; =\mathbb{E}</em>}\left[\mathcal{L}(T) \nabla_{\boldsymbol{p<em j="1">{i}} \log \Pi</em>\right)\right] \
&amp; =\mathbb{E}}^{n} P\left(t_{j<em _boldsymbol_p="\boldsymbol{p">{P(T)}\left[\mathcal{L}(T) \nabla</em><em j="1">{i}} \sum</em>\right)\right] \
&amp; =\mathbb{E}}^{n} \log P\left(t_{j<em _boldsymbol_p="\boldsymbol{p">{P(T)}\left[\mathcal{L}(T) \nabla</em><em i="i">{i}} \log P\left(t</em>\right)\right]
\end{aligned}
$$</p>
<p>The $j$-th component of $\nabla_{\boldsymbol{p}<em i="i">{i}} \log P\left(t</em>\right)$ could be solved explicitly by:</p>
<p>$$
\nabla_{p_{i, j}} \log P\left(t_{i}\right)=\nabla_{p_{i, j}} \log p_{i, j_{i}}
$$</p>
<p>When $j=j_{i}$, it is obvious that $\nabla_{p_{i, j}} \log P\left(t_{i}\right)=\frac{1}{p_{i, j_{i}}}$. When $j \neq j_{i}$, equation (6) is calculated by:</p>
<p>$$
\begin{aligned}
\nabla_{p_{i, j}} \log P\left(t_{i}\right) &amp; =\nabla_{p_{i, j}} \log \left(1-\sum_{k=1, k \neq j_{i}}^{N} p_{i, k}\right) \
&amp; =-\frac{1}{1-\sum_{k=1, k \neq j_{i}}^{N} p_{i, k}} \
&amp; =-\frac{1}{p_{i, j_{i}}}
\end{aligned}
$$</p>
<p>Therefore, we adopted a variance-reduced policy gradient estimator (VR-PGE) as described in Williams (1992); Dong et al. (2020); Zhou et al. (2021) to mitigate the high-variance issue of PGE. The estimated gradient is calculated by:</p>
<table>
<thead>
<tr>
<th>$\boldsymbol{g}<em i="i">{\boldsymbol{p}</em>}}^{\nu r}=\frac{1}{I-1}\sum_{k=1}^{I}\left(\mathcal{L}(T^{(k)})-\frac{1}{I}\sum_{j=1}^{I}\mathcal{L}(T^{(j)})\right)\nabla_{\boldsymbol{p<em i="i">{i}}\log P(t</em>)$</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>where $T^{(k)},k=1,\cdots,I$ are sampled independently from $P(T)$.</td>
<td></td>
</tr>
<tr>
<td>Thus, the prompt token distribution $\boldsymbol{p}_{i}$ can be updated by a projected stochastic gradient descent</td>
<td></td>
</tr>
<tr>
<td>algorithm:</td>
<td></td>
</tr>
<tr>
<td>$\boldsymbol{p}<em _mathcal_C="\mathcal{C">{i}\leftarrow\operatorname{proj}</em>}}(\boldsymbol{p<em _boldsymbol_p="\boldsymbol{p">{i}-\eta\cdot\boldsymbol{g}</em>),i=1,\cdots,n$}_{i}}^{\nu r</td>
<td></td>
</tr>
<tr>
<td>where $\eta$ is the learning rate of prompt learning, $I$ is the sample size, and $\operatorname{proj}_{\mathcal{C}}$ is the projection calculation.</td>
<td></td>
</tr>
<tr>
<td>The detailed training procedure of our VR-PGE algorithm is displayed in Algorithm 1.</td>
<td></td>
</tr>
</tbody>
</table>
<h1>B Detailed Experimental Setting</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">DATASET</th>
<th style="text-align: center;">TASK TYPE</th>
<th style="text-align: center;"># Ex.</th>
<th style="text-align: center;"># EVAL.</th>
<th style="text-align: center;">EVAL. SPLIT</th>
<th style="text-align: center;">TRANSFERRED</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GSM8K (Cobbe et al., 2021)</td>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">ASDiv (Miao et al., 2020)</td>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2096</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">SVAMP (Patel et al., 2021)</td>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">AQuA (Ling et al., 2017)</td>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">SingleOp ${ }^{\text {® }}$</td>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">562</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">CSQA ${ }^{\text {® }}$ (Talmor et al., 2019)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1221</td>
<td style="text-align: center;">Validation</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA ${ }^{\text {® }}$ (Geva et al., 2021)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1880</td>
<td style="text-align: center;">Validation</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">Letter (4) (Wei et al., 2022b)</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">Test (OOD)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">OpenBookQA (Mihaylov et al., 2018)</td>
<td style="text-align: center;">Question Answering</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">e-SNLI ${ }^{\text {® }}$ (Camburu et al., 2018)</td>
<td style="text-align: center;">Narural Language Inference</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">SST-2 ${ }^{\text {® }}$ (Socher et al., 2013)</td>
<td style="text-align: center;">Sentiment Analysis</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">872</td>
<td style="text-align: center;">Validation</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
</tbody>
</table>
<p>Table 5: The overall statistics of the datasets. # Ex.: the number of few-shot chain-of-thought exemplars used to prompt each task. # EVAL.: the number of evaluation data. EVAL. SPLIT: evaluation split. TRANSFERRED: a checkmark means that the exemplars are generated and trained from other datasets and then applied to this task. ${ }^{\text {® }}$ : SingleOp is a subset of MAWPS (Koncel-Kedziorski et al., 2016). ${ }^{\text {® }}$ : CSQA, StrategyQA, and SST-2 do not have publicly available test set labels, so we simply follow the setting by Wei et al. (2022b) and Wang et al. (2022) to evaluate the performance of the validation set. ${ }^{\text {® }}$ : Following Wang et al. (2022), we evaluate the first 1,000 data points for a fair comparison.</p>
<h2>B. 1 Datasets and Evaluation Metrics</h2>
<p>Following Wei et al. (2022b), we conduct our experiments on eight reasoning tasks, including five math word problem datasets: GSM8K, ASDiv, SVAMP, AQuA, and SingleOp; two commonsense reasoning datasets: CommonsenseQA (CSQA) and StrategyQA, and one symbolic reasoning task: Last Letter Concatenation (Letter (4)). We also generalize our method to non-reasoning tasks including one questionanswering task (OpenBookQA), one natural language inference task (e-SNLI), and one sentiment analysis task (SST-2). The detailed statistics of the datasets are listed in Table 5.
To make a fair comparison with our baselines, we use the same number of exemplars as Wei et al. (2022b) and Wang et al. (2022), as shown in Table 5. We keep the same setting for the evaluation split as well. By default, we use the test split for evaluation, and for datasets that do not have publicly available test set labels, we evaluate the validation set instead. In addition, for last letter concatenation, since the model has already achieved almost $100 \%$ accuracy under the in-distribution setting, we only test the out-of-distribution (OOD) setting, Letter (4), where prompts are 2-letters, and test examples are 4-letters. The evaluation metric for all tasks is the exact match accuracy. First, we conduct pre-processing for predictions to remove all the special symbols. For example, "\$100,000" will be processed to "100000". Then we check if it has the same value as the ground truth to calculate the exact match accuracy.</p>
<h2>B. 2 Baselines</h2>
<p>In our experiments, the following three methods serve as the main baselines:</p>
<ul>
<li>
<p>chain-of-thought (Manual-CoT) (Wei et al., 2022b): standard chain-of-thought prompting which provides manual-written intermediate reasoning steps.</p>
</li>
<li>
<p>self-consistency (SC) (Wang et al., 2023): an improved version of CoT. Instead of greedy decoding, it samples a diverse set of reasoning paths and chooses the most common answer.</p>
</li>
<li>Auto-CoT (Zhang et al., 2023): an automatic exemplars construction method that applies clustering techniques to sample questions and then generates chains.
Our experiments are conducted with two popular large language models:</li>
<li>GPT-3 (Brown et al., 2020): we test an advanced version of GPT-3, text-davinci-002, which corresponds to InstructGPT (Ouyang et al., 2022) model.</li>
<li>CodeX (Chen et al., 2021): we test code-davinci-002 which has better code representation ability.</li>
</ul>
<p>We utilize the public APIs directly from OpenAI's services ${ }^{4}$. In our main experiments, we test on both text-davinci-002 and code-davinci-002 engines. However, in additional experiments, we mainly test on code-davinci-002 for two reasons : (1) It is the most capable model available at the time we were conducting our experiments, consistent with the observations in previous studies (Wei et al., 2022b; Wang et al., 2023; Miao et al., 2020). (2) Compared to costly text-davinci-002, it is free of charge because we are in the initial limited beta period during our experiments process.</p>
<h1>B. 3 Implementation</h1>
<p>Augment and Prune: Following Wei et al. (2022b) and Wang et al. (2022), we keep the same number of exemplars (4-8) listed in Table 5. For main experiments, we augment and prune a pool of 100 high-quality exemplars for all datasets. Firstly, pool construction questions are randomly sampled and then fed to LLMs to construct model-generated answers with rationale chains. Given that some datasets only have the test split, we use the pool result of GSM8K and transferred it to these datasets for further inference. Here for arithmetic reasoning tasks, pool construction questions are randomly sampled from the training split of GSM8K and AQuA. For CSQA and StrategyQA, exemplars are randomly sampled from the official training split (Talmor et al., 2019) and question-only set from BIG-bench collaboration (Srivastava et al., 2022). For letter concatenation, exemplars are randomly sampled from the 2-letter set. After the pool is constructed, we use labels to prune the incorrect model-generated exemplars and retain 100 high-quality exemplars.
Select: The train set and validation set are also randomly sampled following the same rule as above except Letter (4) dataset. Since LLM has already reached almost $100 \%$ accuracy on the 2-letter set, we choose to optimize the model based on the 3-letter OOD set. Thus the train set and validation set are randomly sampled from the 3-letter set. Both the train and validation sets have a size of 100 to reach a performance and cost trade-off. Then by utilizing the log probability returned by API calls, we calculate the cross-entropy loss of the answer token. Finally, we optimize the latent variables by AdamW (Loshchilov and Hutter, 2019) for 5 epochs with a learning rate of $1 \times 10^{-3}$ and batch size of 10. After optimization, as shown in Figure 2 inference stage, we choose the exemplars combination $\left(\arg \max \boldsymbol{p}_{i}\right)$ with the highest validation accuracy to be further evaluated on the test set. By default, we query the language model once to get the answer. Under the self-consistency setting, similar to Wang et al. (2023), we query the language model 40 times and choose the most consistent one as the final answer.</p>
<p>Hyper-parameter Setting: Under few-shot setting, we set max_tokens $=256$ for all augmentation, selection and inference. In addition, we set logprobs $=5$ when training. Moreover, we set temperature $=$ 0.7 for evaluation under self-consistency while temperature $=0$ for all other cases. Under zero-shot setting (§6.5), we keep the same hyper-parameters as Kojima et al. (2022) which first uses max_tokens $=128$ for generating the rationale chains and then uses max_tokens $=32$ for generating the answers to construct the pool. The hyper-parameters for selecting and evaluating are the same as the few-shot setting above.</p>
<h2>C More Experiment Results</h2>
<h2>C. 1 Experiments under ChatGPT</h2>
<p>To further verify the effectiveness of Automate-CoT, we further conduct the experiments on gpt-3.5-turbo. Automate-CoT also shows consistent improvement on each task with $2.8 \%$ improvement on arithmetic</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">AQUA</th>
<th style="text-align: center;">SingleOp</th>
<th style="text-align: center;">CSQA</th>
<th style="text-align: center;">STQA</th>
<th style="text-align: center;">Letter (4)</th>
<th style="text-align: center;">OBQA</th>
<th style="text-align: center;">e-SNLI</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Manual-CoT</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">73.6</td>
</tr>
<tr>
<td style="text-align: left;">+ BM25</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: left;">+ PromptPG</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">74.0</td>
</tr>
<tr>
<td style="text-align: left;">+ K-Means</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">73.8</td>
</tr>
<tr>
<td style="text-align: left;">+ Automate-CoT</td>
<td style="text-align: center;">$\mathbf{6 8 . 0} \uparrow_{4.0}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 7} \uparrow_{4.0}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 1} \uparrow_{3.0}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 9} \uparrow_{2.0}$</td>
<td style="text-align: center;">$\mathbf{9 1 . 5} \uparrow_{3.2}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 5} \uparrow_{3.0}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 5} \uparrow_{4.0}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 2} \uparrow_{3.2}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 0} \uparrow_{3.0}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 4} \uparrow_{0.5}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 7} \uparrow_{2.4}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 4} \uparrow_{2.8}$</td>
</tr>
</tbody>
</table>
<p>Table 6: The overall performance of Automate-CoT under gpt-3.5-turbo and the comparison with retrieval-based and clustering-based exemplars selection methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">AQUA</th>
<th style="text-align: center;">SingleOp</th>
<th style="text-align: center;">CSQA</th>
<th style="text-align: center;">STQA</th>
<th style="text-align: center;">Letter (4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Automate-CoT</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">Automate-CoT(SC)</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">code-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Automate-CoT</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: left;">Automate-CoT(SC)</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.08</td>
</tr>
</tbody>
</table>
<p>Table 7: The variance of the results in Table 1 over 3 runs. (SC) denotes under self-consistency setting.
reasoning, $3.9 \%$ improvement on commonsense reasoning, $3.2 \%$ on symbolic reasoning, and $2.8 \%$ improvement overall as shown in Table 6.</p>
<h1>C. 2 Comparison with Retrieval Methods</h1>
<p>We also compare Automate-CoT with simple retrieval method BM25 (Robertson, 2009) and reinforcement learning-based retrieval method PromptPG (Lu et al., 2023). We first implemented a BM25 selection method and tested the performance on all the datasets. The results are shown in Table 6. It indicates that retrieval-based methods can only select examples with similar meaning to the query question while the diversity is overlooked. As shown in the table, the average performance of the BM25 retrieval-based method even has a $1 \%$ degradation compared to Manual-CoT, and 3.8\% lower than Automate-CoT. A similar phenomenon is observed in Auto-CoT (Zhang et al., 2023), which indicates that with similar questions being sampled for test questions, Retrieval-Q-CoT is negatively affected by misleading by similarity.
In addition, we also compare with PromptPG (Lu et al., 2023), a dynamic example-selection baseline. We adopt the same setting as ours for PomptPG, where the number of training examples is 100 , the size of the candidate pool is 100 , and the backbone model is gpt-3.5-turbo. Further, we keep the same prompt format as the original chain-of-thought and ours. The other settings we use are consistent with the settings provided by their original code. The results are shown in Table 6. It indicates that Automate-CoT outperforms PromptPG.</p>
<h2>C. 3 Comparison with Clustering Methods</h2>
<p>We further conduct additional experiments to compare Automate-CoT with methods selecting demonstration exemplars through clustering. We use K-Means as the clustering method and create k clusters according to the number of exemplars specified in Table 5. Then we use these $k$ representative exemplars as the demonstration exemplars to prompt the language models. The results are shown in Table 6. It indicates that clustering-based methods can select examples with different semantic meanings and generally perform better than Manual-CoT. However, the complexity and diversity are overlooked. For example, most of the selected few-shot exemplars in GSM8K have around 3-4 hops where complex questions and moderately difficult questions are overlooked. As a result, it generally performs worse than Automate-CoT with a $2.6 \%$ gap.</p>
<h2>C. 4 Variance Report</h2>
<p>Since Automate-CoT's results in Table 1 are averaged over three runs, we also report the variance in Table 7 here. It is observed that Automate-CoT achieves quite a low variance, especially compared to the large variance of Manual-CoT as shown in § 2 Motivation.</p>
<h1>D Additional Comparison with Fine-tuning</h1>
<p>Since our method uses a training-based pipeline, we also compare it with fine-tuning large language models in terms of the number of parameters, training cost, estimated total training cost, and required training set size. As shown in the study of Cobbe et al. (2021), fine-tuning on gpt-3 requires thousands (e.g., 8000) of training examples to be effective while Automate-CoT only needs 100 training examples. In addition, fine-tuning has a larger training and inference cost than Automate-CoT because it not only requires a one-off fine-tuning cost but also has a higher unit price on subsequent usage.
For Automate-CoT, under the setting of gpt-3.5-turbo, the direct usage is $\$ 0.0015$ / 1k tokens for input and $\$ 0.002 / 1 \mathrm{k}$ tokens for output. With the training epochs of 3 , a training set size of 100 and a validation set size of 100, an input length of around 750 tokens and an average output length of 150 tokens, it takes about $(750 / 1000 \cdot 0.0015+150 / 1000 \cdot 0.002) \cdot 100 \cdot 10 \cdot 3+(750 / 1000 \cdot 0.0015+150 / 1000 \cdot 0.002) \cdot 100 \cdot 3=$ $\$ 4.7$. However, for fine-tuneing, given the training price of gpt-3.5-turbo is $\$ 0.008 / 1 \mathrm{~K}$ tokens, the usage of finetuned gpt-3.5-turbo is $\$ 0.0015$ / 1 K tokens for input and $\$ 0.002$ / 1 K tokens for output tokens. Under the finetuning setting, suppose the average length of training examples is 300 tokens, and training a whole training set of 8000 examples for 3 epochs takes about $300 / 1000 \cdot 8000 \cdot 3 \cdot 0.008=\$ 57.6$, which costs 12 x more than Automate-CoT.
It is also worth noting that the further usage of finetuned gpt-3.5-turbo is $\$ 0.012 / 1 \mathrm{~K}$ tokens for input and $\$ 0.016 / 1 \mathrm{~K}$ tokens for output while Automate-CoT remains the normal cost, which is 8 x less cost than fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">METHOD</th>
<th style="text-align: center;"># of Training Params</th>
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">Est. Total Cost</th>
<th style="text-align: center;">Train Set Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">\$0.008/1K tokens (Train)</td>
<td style="text-align: center;">\$ 9.1</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$ 12.7</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">but should $\geq 175 \mathrm{~B}$</td>
<td style="text-align: center;">\$0.012/1K tokens (Input Usage)</td>
<td style="text-align: center;">\$ 20.0</td>
<td style="text-align: center;">2000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$0.016/1K tokens (Output Usage)</td>
<td style="text-align: center;">\$ 34.3</td>
<td style="text-align: center;">4000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$0.016/1K tokens (Output Usage)</td>
<td style="text-align: center;">\$ 63.1</td>
<td style="text-align: center;">8000</td>
</tr>
<tr>
<td style="text-align: center;">Automate-CoT</td>
<td style="text-align: center;"># of exemplars $\times$ Pool Size</td>
<td style="text-align: center;">\$0.0015/1K tokens (Input Usage)</td>
<td style="text-align: center;">\$ 6.6</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$0.002/1K tokens (Output Usage)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison between Fine-tuning and Automate-CoT on GSM8K. The cost is copied from the OpenAI official website. ${ }^{3}$</p>
<h2>E Additional Analysis</h2>
<p>We list some additional analysis here that cannot be put in the main section because of the page limit.</p>
<h2>E. 1 Effects of Several Tricks</h2>
<p>Previous studies have found some tricks like add "Let's think step by step." before each rationale chain and replace "Q:" with "Question:" (Fu et al., 2023; Kojima et al., 2022) can boost the performance on top of Manual-CoT. Following their settings, we also test Automate-CoT with tricks on GSM8K as an additional experiment. By adding tricks, Automate-CoT can further boost the accuracy to $69.8 \%$ (+2.2\%) under the normal setting and $83.0 \%(+0.6 \%)$ under the self-consistency setting, respectively.</p>
<h2>F Exact Match Number over Each Hop</h2>
<p>The exact match number over each hop of Figure 1 is reported in Table 9.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">HOP</th>
<th style="text-align: center;">Total Number</th>
<th style="text-align: center;">Manual-CoT</th>
<th style="text-align: center;">Automate-CoT</th>
<th style="text-align: center;">Complex-CoT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">326</td>
<td style="text-align: center;">$269=82.52 \%$</td>
<td style="text-align: center;">$\mathbf{2 7 2 = 8 3 . 4 4 \%}$</td>
<td style="text-align: center;">$227=69.63 \%$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">370</td>
<td style="text-align: center;">$265=71.62 \%$</td>
<td style="text-align: center;">$\mathbf{2 6 8 = 7 2 . 4 4 \%}$</td>
<td style="text-align: center;">$232=62.70 \%$</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">298</td>
<td style="text-align: center;">$159=53.35 \%$</td>
<td style="text-align: center;">$\mathbf{1 9 6 = 6 5 . 7 7 \%}$</td>
<td style="text-align: center;">$166=55.70 \%$</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">174</td>
<td style="text-align: center;">$83=47.70 \%$</td>
<td style="text-align: center;">$\mathbf{1 1 0 = 6 3 . 2 1 \%}$</td>
<td style="text-align: center;">$98=56.32 \%$</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">$26=29.54 \%$</td>
<td style="text-align: center;">$37=42.05 \%$</td>
<td style="text-align: center;">$\mathbf{3 9 = 4 4 . 3 2 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$10=25 \%$</td>
<td style="text-align: center;">$12=30 \%$</td>
<td style="text-align: center;">$\mathbf{1 6 = 4 0 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$3=15 \%$</td>
<td style="text-align: center;">$4=20 \%$</td>
<td style="text-align: center;">$\mathbf{5 = 2 5 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\mathbf{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">$815=61.8 \%$</td>
<td style="text-align: center;">$\mathbf{8 9 9 = 6 8 . 2 \%}$</td>
<td style="text-align: center;">$784=59.4 \%$</td>
</tr>
</tbody>
</table>
<p>Table 9: The exact match number across the different numbers of hops on GSM8K. Bold represents the best among each hop. The percentage accuracy is calculated for each hop.</p>
<h1>G Full Exemplars generated by Automate-CoT</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">DATASET</th>
<th style="text-align: center;">Exemplars</th>
<th style="text-align: center;">Hops</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">Q: Ralph is going to practice playing tennis with a tennis ball machine that shoots out tennis balls for Ralph to hit. He loads up the machine with 175 tennis balls to start with. Out of the first 100 balls, he manages to hit $2 / 5$ of them. Of the next 75 tennis balls, he manages to hit $1 / 3$ of them. Out of all the tennis balls, how many did Ralph not hit? <br> A: Ralph started with 175 tennis balls. He hit $2 / 5$ of the first 100 balls, so he hit $2 / 5 * 100=40$ balls. He hit $1 / 3$ of the next 75 balls, so he hit $1 / 3 * 75=25$ balls. In total he hit $40+25=65$ balls. He did not hit $175-65=110$ balls. The answer is 110 . <br> Q: Hans booked a room in a hotel. The hotel has 10 floors with 10 identical rooms on each floor. Because of an accident, the last floor is unavailable for the guests. Considering there are no other guests, in how many different rooms could Hans be checked in? <br> A: There are 10 floors with 10 rooms each. The last floor is unavailable. So there are $9 * 10=90$ rooms available. The answer is 90 . <br> Q: James earns $\$ 20$ an hour while working at his main job. He earns $20 \%$ less while working his second job. He works 30 hours at his main job and half that much at his second job. How much does he earn per week? <br> A: James earns $\$ 20$ an hour at his main job. He earns $20 \%$ less at his second job. That means he earns $\$ 20-20 \%$ of $\$ 20=\$ 20-\$ 4=\$ 16$ an hour at his second job. He works 30 hours at his main job and half that much at his second job. That means he works $30 / 2=15$ hours at his second job. In total he works $30+15=45$ hours. He earns $\$ 20$ an hour at his main job and $\$ 16$ an hour at his second job. In total he earns $\$ 20 * 30+\$ 16 * 15=\$ 600+\$ 240=\$ 840$. The answer is 840 . <br> Q: Leah earned $\$ 28$ working odd jobs around the neighborhood. She spent a seventh of it on a milkshake and put half of the rest in her savings account. She left the remaining money in her wallet. Her dog got abold of her wallet and shredded all the money inside but $\$ 1$. How many dollars did Leah lose? <br> A: Leah earned $\$ 28$. She spent a seventh of it on a milkshake, so she spent $\$ 28 / 7=\$ 4$ on the milkshake. She put half of the rest in her savings account, so she put $\$ 28-\$ 4 / 2=\$ 12$ in her savings account. She left the remaining money in her wallet. She lost all the money in her wallet except $\$ 1$. So she lost $\$ 28-$ $\$ 4-\$ 12-\$ 1=\$ 11$. The answer is 11 . <br> Q: Sam and Jeff had a skipping competition at recess. The competition was split into four rounds. Sam completed 1 more skip than Jeff in the first round. Jeff skipped 3 fewer times than Sam in the second round. Jeff skipped 4 more times than Sam in the third round. Jeff got tired and only completed half the number of skips as Sam in the last round. If Sam skipped 16 times in each round, what is the average number of skips per round completed by Jeff? <br> A: Sam skipped 16 times in each round. In the first round, Jeff skipped $16-1=15$ times. In the second round, Jeff skipped $16-3=13$ times. In the third round, Jeff skipped $16+4=20$ times. In the last round, Jeff skipped $16 / 2=8$ times. The average number of skips per round completed by Jeff is $(15+13+20$ $+8) / 4=56 / 4=14$. The answer is 14 . <br> Q: A Statistics student wants to find out the average daily allowance of the middle school students. According to his survey, $2 / 3$ of the students receive an average of $\$ 6$ allowance per day while the rest gets an average of $\$ 4$ a day. If he surveyed 60 students, what is the total amount of money those 60 students get in a day? <br> A: $2 / 3$ of the students receive $\$ 6$ a day. The rest receive $\$ 4$ a day. The total number of students is 60 . So, $2 / 3$ of 60 students receive $\$ 6$ a day. The rest of the students, which is $60-2 / 3 * 60=20$ students, receive $\$ 4$ a day. The total amount of money those 60 students get in a day is $2 / 3 * 60 * \$ 6+20 * \$ 4=\$ 240+$ $\$ 80=\$ 320$. The answer is 320 . <br> Q: Ali had a collection of seashells. He started with 180 seashells. He then gave away 40 seashells to his friends. He also gave 30 seashells to his brothers. If he sold half of the remaining seashells, how many seashells did he have left? <br> A: He started with 180 seashells. He gave away 40 to his friends and 30 to his brothers. That means he gave away $40+30=70$ seashells. He has $180-70=110$ seashells left. He sold half of them, so he has $110 / 2=55$ seashells left. The answer is 55 . <br> Q: Marvin and Tina were selling candy bars to help fund their class trip. The candy bars cost $\$ 2$ each. Marvin sold 35 candy bars total. Tina sold three times the number of candy bars as Marvin. How much more money did Tina make for the class trip selling candy bars compared to Marvin? <br> A: Marvin sold 35 candy bars. Tina sold 3 times as many, so she sold $35 * 3=105$ candy bars. Each candy bar cost $\$ 2$, so Marvin made $35 * \$ 2=\$ 70$. Tina made $105 * \$ 2=\$ 210$. The difference is $\$ 210-$ $\$ 70=\$ 140$. The answer is 140 .</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 10: One example of selected model-generated exemplars with rationale chains of average hops $=5.4$. This set of exemplars is trained and selected on GSM8K and transferred to other arithmetic reasoning tasks.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://openai.com/pricing&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
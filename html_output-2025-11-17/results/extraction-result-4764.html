<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4764 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4764</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4764</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-269302480</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14812v2.pdf" target="_blank">Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns</a></p>
                <p><strong>Paper Abstract:</strong> Chain of Thought (CoT) prompting can encourage language models to engage in multi-step logical reasoning. The quality of the provided demonstrations significantly influences the success of downstream inference tasks. Current unsupervised CoT methods primarily select examples based on the semantics of the questions, which can introduce noise and lack interpretability. In this paper, we propose leveraging reasoning patterns to enhance CoT prompting effectiveness. Reasoning patterns represent the process by which language models arrive at their final results. By utilizing prior knowledge and prompt-based methods from large models, we first construct task-specific pattern sets. We then select diverse demonstrations based on different reasoning patterns. This approach not only mitigates the impact of noise but also provides explicit interpretability to help us understand the mechanisms of CoT. Extensive experiments demonstrate that our method is more robust and consistently leads to improvements across various reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4764.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4764.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pattern-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pattern-based Chain-of-Thought Demonstration Selection (Pattern-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A demonstration selection method that extracts task-specific reasoning operation tokens from rationales, clusters the resulting reasoning patterns, and samples diverse demonstrations from each cluster to construct CoT prompts that expose varied reasoning strategies to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (7B and 13B); also evaluated on GPT-3.5-turbo and Qwen-7B (mentioned as tested)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-2 chat models of 7 billion and 13 billion parameters used as the primary reasoning engines; experiments also reported for GPT-3.5-turbo and Qwen-7B to test compatibility.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Pattern-CoT (diverse reasoning patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Extract operation tokens representing reasoning operations from generated or dataset rationales (p_i sequences), encode these pattern sequences, cluster them (k-means) and sample representative demonstrations from each cluster so the prompt contains a diverse set of reasoning templates/operations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, GSM8K, AddSub, AQuA, SingleEq, SVAMP, Coin-Flip, BIG-bench Date Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard reasoning / math-word-problem / symbolic reasoning benchmarks used for CoT evaluation (arithmetic, multi-step math, date reasoning, coin flips, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLaMA-2 7B (Pattern-CoT, non-adaptive k) — MultiArith: 79.66% ; GSM8K: 27.45% ; AddSub: 65.06% ; AQuA: 28.34% ; SingleEq: 71.85% ; SVAMP: 48.50% ; Coin: 59.40% ; Date: 45.79%. LLaMA-2 13B (Pattern-CoT, non-adaptive k) — MultiArith: 83.16% ; GSM8K: 37.68% ; AddSub: 65.82% ; AQuA: 26.37% ; SingleEq: 74.80% ; SVAMP: 56.39% ; Coin: 57.40% ; Date: 56.91%. (Adaptive-k variants reported further gains on some datasets.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared against unsupervised baselines (Zero-Shot-CoT, Random-CoT, Auto-CoT, Auto-CoT-RA, Self-Consistency) as reported in Table 2. Pattern-CoT consistently outperforms these baselines across most datasets, especially arithmetic tasks with limited operation sets; on broader-action-space datasets (GSM8K, AQuA) gains are smaller unless adaptive k is used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Selecting demonstrations based on diverse reasoning patterns yields more robust and consistent improvements in CoT prompting than semantics-based selection. Pattern diversity reduces noise and bias from irrelevant semantic similarity and better covers different solution strategies; this is particularly effective for arithmetic tasks with a limited operation vocabulary. Even when demonstrations contain incorrect final answers, providing diverse pattern templates helps the model follow the correct reasoning chain.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Gains are less pronounced on datasets with large or diverse action spaces (GSM8K, AQuA) unless the number of clusters/demonstrations is increased (adaptive k). Some datasets (e.g., AQuA) had high demonstration error rates (authors report up to 100% incorrect in selected demos) yet still benefited, indicating limitations in relying on label correctness alone. Pattern-CoT did not uniformly dominate in every single configuration; adaptive choices matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4764.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4764.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT-RA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-CoT with Rationale-Augmented Clustering (Auto-CoT-RA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of Auto-CoT that replaces question embeddings with rationale embeddings for clustering, intended to uncover underlying reasoning patterns via clustering on rationale semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (7B and 13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-2 chat models used for evaluation (7B and 13B sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Auto-CoT-RA (rationale-embedding clustering)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Clustering demonstration examples using embeddings of generated rationales (instead of question embeddings) to select demonstrations for in-context learning, hypothesized to implicitly capture reasoning-pattern similarities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, GSM8K, AddSub, AQuA, SingleEq, SVAMP, Coin-Flip, Date</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of reasoning and math benchmarks used for CoT evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLaMA-2 7B (Auto-CoT-RA): reported per Table 2 roughly MultiArith: 74.83% ; GSM8K: 26.76% ; AddSub: 63.29% ; AQuA: 23.80% ; SingleEq: 66.92% ; SVAMP: 45.19% ; Coin: 48.00% ; Date: 43.08%. LLaMA-2 13B (Auto-CoT-RA): roughly MultiArith: 82.16% ; GSM8K: 37.04% ; AddSub: 62.08% ; AQuA: 27.74% ; SingleEq: 66.14% ; SVAMP: 52.10% ; Coin: 62.80% ; Date: 54.47%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared with Auto-CoT and Pattern-CoT in Table 2. Auto-CoT-RA sometimes outperforms Auto-CoT but is inconsistent; overall Pattern-CoT usually surpasses Auto-CoT-RA by emphasizing explicit operation-token pattern extraction and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Clustering on rationale embeddings can reveal some reasoning-structure similarities and sometimes outperform question-embedding based Auto-CoT, but it remains less interpretable and less consistently beneficial than explicit pattern-extraction + clustering (Pattern-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Auto-CoT-RA does not consistently improve over Auto-CoT across all datasets; improvements are dataset-dependent and sometimes small.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4764.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4764.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-CoT (automatic CoT demonstration selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised demonstration selection method that clusters examples using (semantic) question embeddings to automatically produce CoT demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (7B and 13B); also evaluated elsewhere in literature</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used in this paper as a baseline; original Auto-CoT is an unsupervised method (Zhang et al. 2023 referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Auto-CoT (semantic clustering of examples)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Cluster training or seed examples by semantic similarity (question or answer embedding) and sample cluster representatives as CoT demonstrations; selection is driven by semantic closeness rather than explicit reasoning-operation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same reasoning / math benchmarks (MultiArith, GSM8K, AddSub, AQuA, SingleEq, SVAMP, Coin, Date)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks requiring multi-step reasoning and arithmetic operations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in Table 2 as an unsupervised baseline; performance is generally lower or less stable than Pattern-CoT across many datasets (specific dataset-level numbers are reported in Table 2 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared against Pattern-CoT, Zero-Shot-CoT, Random-CoT, Self-Consistency. Pattern-CoT generally outperforms Auto-CoT; in some cases Auto-CoT performs poorly (even worse than direct answering), presumably due to noise from semantics-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantic-based selection (Auto-CoT) can introduce irrelevant patterns and noise; it may distort LLM reasoning if many irrelevant or repetitive patterns are introduced. Diverse pattern-based selection (Pattern-CoT) reduces such bias and yields more consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Auto-CoT occasionally outperforms other unsupervised baselines on some datasets or with larger models, but its performance is less stable and sometimes worse than even random demonstration selection, indicating sensitivity to irrelevant pattern noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4764.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4764.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-Shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Chain-of-Thought prompting (Zero-Shot-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach that elicits chain-of-thought style reasoning in LLMs without few-shot examples by instructing the model to 'think step by step' in a zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (7B and 13B) (evaluated as baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot CoT prompts are applied to LLaMA-2 7B and 13B models in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-Shot-CoT (single-shot chain-of-thought elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompt the model with an instruction to produce step-by-step reasoning without providing few-shot demonstrations; produces a single chain-of-thought per input (no ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, GSM8K, AddSub, AQuA, SingleEq, SVAMP, Coin, Date</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning/math datasets used throughout the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in Table 2. Example (LLaMA-2 13B Zero-Shot-CoT): MultiArith ~77.50% ; GSM8K ~34.49% ; AddSub ~60.75% ; AQuA ~15.74% ; SingleEq ~69.29% ; SVAMP ~49.40% ; Coin ~47.40% ; Date ~46.07% (as reported in the paper's Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared with Pattern-CoT and Auto-CoT; Zero-Shot-CoT is often outperformed by Pattern-CoT and sometimes by Auto-CoT depending on dataset and model size. Self-consistency (multiple sampled chains) can boost Zero-Shot-CoT performance in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero-shot CoT provides a simple way to elicit reasoning but is typically inferior to few-shot methods with well-chosen demonstrations; adding self-consistency (multiple sampled reasoning paths) can improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Zero-Shot-CoT performance can be improved by self-consistency (ensemble of multiple sampled chains), and in a few settings some automated selection methods may outperform plain zero-shot; however, Zero-Shot-CoT is generally weaker than Pattern-CoT in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4764.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4764.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (ensemble of sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple reasoning paths (chains-of-thought) for the same question and aggregates answers (majority or scoring) to improve robustness and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (7B and 13B) — used as a baseline (self-consistency set with 5 paths in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-consistency is applied on top of chain-of-thought generation by sampling multiple stochastic chain outputs (temperature > 0) and aggregating.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (ensemble of diverse sampled chains)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple reasoning trajectories (chains-of-thought) via sampling (stochastic decoding) and combine outputs to produce a final answer, leveraging diversity in sampled reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same benchmarks; used e.g., with Zero-Shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning and math problem datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as a baseline in Table 2; self-consistency improved Zero-Shot-CoT results in some cases (authors set number of sampled paths to 5). Exact dataset-level numbers for self-consistency are reported in Table 2 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared with Pattern-CoT and other baselines; while self-consistency boosts performance of single-prompt methods, Pattern-CoT (diverse demonstrations) yields more consistent improvements across datasets according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Ensembling multiple sampled chains (self-consistency) helps improve robustness and accuracy, but combining diverse demonstrations (Pattern-CoT) is a complementary and often more stable way to expose reasoning diversity in the prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-consistency requires sampling multiple generations (compute cost) and its gains depend on the base prompting quality; it does not fully replace gains from carefully selected diverse demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4764.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4764.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Chain-of-Thought demonstration selection (Random-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that constructs CoT prompts by randomly sampling demonstration examples from available seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (7B and 13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as a baseline in experiments to contrast with heuristic and pattern-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Random-CoT (random demonstration selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Selects demonstrations uniformly at random from a seed pool without any clustering or pattern-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same reasoning datasets used in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks for multi-step and arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as a baseline in Table 2. The authors note that random demonstration selection can sometimes surpass Auto-CoT but improvements are inconsistent across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to Auto-CoT and Pattern-CoT: Random-CoT is unstable — occasionally beats Auto-CoT but is generally outperformed by Pattern-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random demonstration selection can unexpectedly outperform some semantics-based automated methods in isolated cases, highlighting that factors other than pure semantic similarity (e.g., reasoning patterns) influence in-context learning effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Random-CoT's performance is inconsistent and not reliably better than pattern-aware selection; it does not offer interpretability and can be outperformed by structured approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Automatic Chain of Thought Prompting in Large Language Models <em>(Rating: 2)</em></li>
                <li>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4764",
    "paper_id": "paper-269302480",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Pattern-CoT",
            "name_full": "Pattern-based Chain-of-Thought Demonstration Selection (Pattern-CoT)",
            "brief_description": "A demonstration selection method that extracts task-specific reasoning operation tokens from rationales, clusters the resulting reasoning patterns, and samples diverse demonstrations from each cluster to construct CoT prompts that expose varied reasoning strategies to LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (7B and 13B); also evaluated on GPT-3.5-turbo and Qwen-7B (mentioned as tested)",
            "model_description": "Open-source LLaMA-2 chat models of 7 billion and 13 billion parameters used as the primary reasoning engines; experiments also reported for GPT-3.5-turbo and Qwen-7B to test compatibility.",
            "reasoning_method_name": "Pattern-CoT (diverse reasoning patterns)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Extract operation tokens representing reasoning operations from generated or dataset rationales (p_i sequences), encode these pattern sequences, cluster them (k-means) and sample representative demonstrations from each cluster so the prompt contains a diverse set of reasoning templates/operations.",
            "task_name": "MultiArith, GSM8K, AddSub, AQuA, SingleEq, SVAMP, Coin-Flip, BIG-bench Date Understanding",
            "task_description": "Standard reasoning / math-word-problem / symbolic reasoning benchmarks used for CoT evaluation (arithmetic, multi-step math, date reasoning, coin flips, etc.).",
            "performance": "LLaMA-2 7B (Pattern-CoT, non-adaptive k) — MultiArith: 79.66% ; GSM8K: 27.45% ; AddSub: 65.06% ; AQuA: 28.34% ; SingleEq: 71.85% ; SVAMP: 48.50% ; Coin: 59.40% ; Date: 45.79%. LLaMA-2 13B (Pattern-CoT, non-adaptive k) — MultiArith: 83.16% ; GSM8K: 37.68% ; AddSub: 65.82% ; AQuA: 26.37% ; SingleEq: 74.80% ; SVAMP: 56.39% ; Coin: 57.40% ; Date: 56.91%. (Adaptive-k variants reported further gains on some datasets.)",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared against unsupervised baselines (Zero-Shot-CoT, Random-CoT, Auto-CoT, Auto-CoT-RA, Self-Consistency) as reported in Table 2. Pattern-CoT consistently outperforms these baselines across most datasets, especially arithmetic tasks with limited operation sets; on broader-action-space datasets (GSM8K, AQuA) gains are smaller unless adaptive k is used.",
            "key_findings": "Selecting demonstrations based on diverse reasoning patterns yields more robust and consistent improvements in CoT prompting than semantics-based selection. Pattern diversity reduces noise and bias from irrelevant semantic similarity and better covers different solution strategies; this is particularly effective for arithmetic tasks with a limited operation vocabulary. Even when demonstrations contain incorrect final answers, providing diverse pattern templates helps the model follow the correct reasoning chain.",
            "counter_examples_or_negative_results": "Gains are less pronounced on datasets with large or diverse action spaces (GSM8K, AQuA) unless the number of clusters/demonstrations is increased (adaptive k). Some datasets (e.g., AQuA) had high demonstration error rates (authors report up to 100% incorrect in selected demos) yet still benefited, indicating limitations in relying on label correctness alone. Pattern-CoT did not uniformly dominate in every single configuration; adaptive choices matter.",
            "uuid": "e4764.0",
            "source_info": {
                "paper_title": "Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Auto-CoT-RA",
            "name_full": "Auto-CoT with Rationale-Augmented Clustering (Auto-CoT-RA)",
            "brief_description": "A variant of Auto-CoT that replaces question embeddings with rationale embeddings for clustering, intended to uncover underlying reasoning patterns via clustering on rationale semantics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (7B and 13B)",
            "model_description": "Open-source LLaMA-2 chat models used for evaluation (7B and 13B sizes).",
            "reasoning_method_name": "Auto-CoT-RA (rationale-embedding clustering)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Clustering demonstration examples using embeddings of generated rationales (instead of question embeddings) to select demonstrations for in-context learning, hypothesized to implicitly capture reasoning-pattern similarities.",
            "task_name": "MultiArith, GSM8K, AddSub, AQuA, SingleEq, SVAMP, Coin-Flip, Date",
            "task_description": "Same set of reasoning and math benchmarks used for CoT evaluation.",
            "performance": "LLaMA-2 7B (Auto-CoT-RA): reported per Table 2 roughly MultiArith: 74.83% ; GSM8K: 26.76% ; AddSub: 63.29% ; AQuA: 23.80% ; SingleEq: 66.92% ; SVAMP: 45.19% ; Coin: 48.00% ; Date: 43.08%. LLaMA-2 13B (Auto-CoT-RA): roughly MultiArith: 82.16% ; GSM8K: 37.04% ; AddSub: 62.08% ; AQuA: 27.74% ; SingleEq: 66.14% ; SVAMP: 52.10% ; Coin: 62.80% ; Date: 54.47%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared with Auto-CoT and Pattern-CoT in Table 2. Auto-CoT-RA sometimes outperforms Auto-CoT but is inconsistent; overall Pattern-CoT usually surpasses Auto-CoT-RA by emphasizing explicit operation-token pattern extraction and diversity.",
            "key_findings": "Clustering on rationale embeddings can reveal some reasoning-structure similarities and sometimes outperform question-embedding based Auto-CoT, but it remains less interpretable and less consistently beneficial than explicit pattern-extraction + clustering (Pattern-CoT).",
            "counter_examples_or_negative_results": "Auto-CoT-RA does not consistently improve over Auto-CoT across all datasets; improvements are dataset-dependent and sometimes small.",
            "uuid": "e4764.1",
            "source_info": {
                "paper_title": "Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Auto-CoT",
            "name_full": "Auto-CoT (automatic CoT demonstration selection)",
            "brief_description": "An unsupervised demonstration selection method that clusters examples using (semantic) question embeddings to automatically produce CoT demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (7B and 13B); also evaluated elsewhere in literature",
            "model_description": "Used in this paper as a baseline; original Auto-CoT is an unsupervised method (Zhang et al. 2023 referenced).",
            "reasoning_method_name": "Auto-CoT (semantic clustering of examples)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Cluster training or seed examples by semantic similarity (question or answer embedding) and sample cluster representatives as CoT demonstrations; selection is driven by semantic closeness rather than explicit reasoning-operation patterns.",
            "task_name": "Same reasoning / math benchmarks (MultiArith, GSM8K, AddSub, AQuA, SingleEq, SVAMP, Coin, Date)",
            "task_description": "Benchmarks requiring multi-step reasoning and arithmetic operations.",
            "performance": "Reported in Table 2 as an unsupervised baseline; performance is generally lower or less stable than Pattern-CoT across many datasets (specific dataset-level numbers are reported in Table 2 of the paper).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared against Pattern-CoT, Zero-Shot-CoT, Random-CoT, Self-Consistency. Pattern-CoT generally outperforms Auto-CoT; in some cases Auto-CoT performs poorly (even worse than direct answering), presumably due to noise from semantics-based selection.",
            "key_findings": "Semantic-based selection (Auto-CoT) can introduce irrelevant patterns and noise; it may distort LLM reasoning if many irrelevant or repetitive patterns are introduced. Diverse pattern-based selection (Pattern-CoT) reduces such bias and yields more consistent gains.",
            "counter_examples_or_negative_results": "Auto-CoT occasionally outperforms other unsupervised baselines on some datasets or with larger models, but its performance is less stable and sometimes worse than even random demonstration selection, indicating sensitivity to irrelevant pattern noise.",
            "uuid": "e4764.2",
            "source_info": {
                "paper_title": "Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Zero-Shot-CoT",
            "name_full": "Zero-Shot Chain-of-Thought prompting (Zero-Shot-CoT)",
            "brief_description": "A prompting approach that elicits chain-of-thought style reasoning in LLMs without few-shot examples by instructing the model to 'think step by step' in a zero-shot manner.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (7B and 13B) (evaluated as baseline in this paper)",
            "model_description": "Zero-shot CoT prompts are applied to LLaMA-2 7B and 13B models in the experiments.",
            "reasoning_method_name": "Zero-Shot-CoT (single-shot chain-of-thought elicitation)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Prompt the model with an instruction to produce step-by-step reasoning without providing few-shot demonstrations; produces a single chain-of-thought per input (no ensembling).",
            "task_name": "MultiArith, GSM8K, AddSub, AQuA, SingleEq, SVAMP, Coin, Date",
            "task_description": "Same reasoning/math datasets used throughout the paper.",
            "performance": "Reported in Table 2. Example (LLaMA-2 13B Zero-Shot-CoT): MultiArith ~77.50% ; GSM8K ~34.49% ; AddSub ~60.75% ; AQuA ~15.74% ; SingleEq ~69.29% ; SVAMP ~49.40% ; Coin ~47.40% ; Date ~46.07% (as reported in the paper's Table 2).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared with Pattern-CoT and Auto-CoT; Zero-Shot-CoT is often outperformed by Pattern-CoT and sometimes by Auto-CoT depending on dataset and model size. Self-consistency (multiple sampled chains) can boost Zero-Shot-CoT performance in some cases.",
            "key_findings": "Zero-shot CoT provides a simple way to elicit reasoning but is typically inferior to few-shot methods with well-chosen demonstrations; adding self-consistency (multiple sampled reasoning paths) can improve accuracy.",
            "counter_examples_or_negative_results": "Zero-Shot-CoT performance can be improved by self-consistency (ensemble of multiple sampled chains), and in a few settings some automated selection methods may outperform plain zero-shot; however, Zero-Shot-CoT is generally weaker than Pattern-CoT in these experiments.",
            "uuid": "e4764.3",
            "source_info": {
                "paper_title": "Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (ensemble of sampled reasoning paths)",
            "brief_description": "A method that samples multiple reasoning paths (chains-of-thought) for the same question and aggregates answers (majority or scoring) to improve robustness and correctness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (7B and 13B) — used as a baseline (self-consistency set with 5 paths in experiments)",
            "model_description": "Self-consistency is applied on top of chain-of-thought generation by sampling multiple stochastic chain outputs (temperature &gt; 0) and aggregating.",
            "reasoning_method_name": "Self-Consistency (ensemble of diverse sampled chains)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple reasoning trajectories (chains-of-thought) via sampling (stochastic decoding) and combine outputs to produce a final answer, leveraging diversity in sampled reasoning paths.",
            "task_name": "Same benchmarks; used e.g., with Zero-Shot-CoT",
            "task_description": "Multi-step reasoning and math problem datasets.",
            "performance": "Reported as a baseline in Table 2; self-consistency improved Zero-Shot-CoT results in some cases (authors set number of sampled paths to 5). Exact dataset-level numbers for self-consistency are reported in Table 2 of the paper.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared with Pattern-CoT and other baselines; while self-consistency boosts performance of single-prompt methods, Pattern-CoT (diverse demonstrations) yields more consistent improvements across datasets according to the paper.",
            "key_findings": "Ensembling multiple sampled chains (self-consistency) helps improve robustness and accuracy, but combining diverse demonstrations (Pattern-CoT) is a complementary and often more stable way to expose reasoning diversity in the prompt context.",
            "counter_examples_or_negative_results": "Self-consistency requires sampling multiple generations (compute cost) and its gains depend on the base prompting quality; it does not fully replace gains from carefully selected diverse demonstrations.",
            "uuid": "e4764.4",
            "source_info": {
                "paper_title": "Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Random-CoT",
            "name_full": "Random Chain-of-Thought demonstration selection (Random-CoT)",
            "brief_description": "A baseline method that constructs CoT prompts by randomly sampling demonstration examples from available seeds.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (7B and 13B)",
            "model_description": "Used as a baseline in experiments to contrast with heuristic and pattern-based selection.",
            "reasoning_method_name": "Random-CoT (random demonstration selection)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Selects demonstrations uniformly at random from a seed pool without any clustering or pattern-based selection.",
            "task_name": "Same reasoning datasets used in the paper",
            "task_description": "Benchmarks for multi-step and arithmetic reasoning.",
            "performance": "Reported as a baseline in Table 2. The authors note that random demonstration selection can sometimes surpass Auto-CoT but improvements are inconsistent across datasets.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to Auto-CoT and Pattern-CoT: Random-CoT is unstable — occasionally beats Auto-CoT but is generally outperformed by Pattern-CoT.",
            "key_findings": "Random demonstration selection can unexpectedly outperform some semantics-based automated methods in isolated cases, highlighting that factors other than pure semantic similarity (e.g., reasoning patterns) influence in-context learning effectiveness.",
            "counter_examples_or_negative_results": "Random-CoT's performance is inconsistent and not reliably better than pattern-aware selection; it does not offer interpretability and can be outperformed by structured approaches.",
            "uuid": "e4764.5",
            "source_info": {
                "paper_title": "Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study",
            "rating": 2,
            "sanitized_title": "what_makes_chainofthought_prompting_effective_a_counterfactual_study"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Automatic Chain of Thought Prompting in Large Language Models",
            "rating": 2,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
            "rating": 1,
            "sanitized_title": "automatic_prompt_augmentation_and_selection_with_chainofthought_from_labeled_data"
        }
    ],
    "cost": 0.0173825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns</p>
<p>Yufeng Zhang yufeng.zhang@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Xuepeng Wang xuepeng.wang@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Wuhan AI Research
WuhanChina</p>
<p>Lingxiang Wu lingxiang.wu@nlpr.ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Wuhan AI Research
WuhanChina</p>
<p>Jinqiao Wang jqwang@nlpr.ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Wuhan AI Research
WuhanChina</p>
<p>Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns
6A228DEC222D3E18B94A16AF2CED6D79
Chain of Thought (CoT) prompting can encourage language models to engage in multi-step logical reasoning.The quality of the provided demonstrations significantly influences the success of downstream inference tasks.Current unsupervised CoT methods primarily select examples based on the semantics of the questions, which can introduce noise and lack interpretability.In this paper, we propose leveraging reasoning patterns to enhance CoT prompting effectiveness.Reasoning patterns represent the process by which language models arrive at their final results.By utilizing prior knowledge and prompt-based methods from large models, we first construct task-specific pattern sets.We then select diverse demonstrations based on different reasoning patterns.This approach not only mitigates the impact of noise but also provides explicit interpretability to help us understand the mechanisms of CoT.Extensive experiments demonstrate that our method is more robust and consistently leads to improvements across various reasoning tasks.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of language tasks.In general question-answering tasks (Kwiatkowski et al. 2019), LLMs hold a distinct advantage over other language models due to their robust writing capabilities.However, when it comes to more advanced tasks such as logical reasoning, mathematical computation, and symbolic reasoning, LLMs often fall short (Qiao et al. 2023;Huang and Chang 2023).</p>
<p>One effective approach to addressing these challenges is Chain of Thought (CoT) prompting (Wei et al. 2022b).By providing several demonstration examples that include a problem, intermediate reasoning steps, and an answer, CoT prompting serves as a contextual guide for downstream tasks.This approach encourages LLMs to generate multistep logical reasoning, thereby maximizing the likelihood of producing more plausible answers.The advantage of this method lies in its simplicity and efficiency; unlike finetuning, it does not require extensive gradient updates or alter the model's inherent capabilities.Instead, it acts as an external augmentation of knowledge.For different reasoning tasks, we can route the model to the appropriate context, and then easily switch the demonstration sets to activate the relevant knowledge and abilities in the corresponding domain.</p>
<p>However, we argue that existing unsupervised CoT prompting methods have two major shortcomings.First, there remains a significant gap between the selected demonstration sets and the reasoning targets.Although extensive research (Zhang et al. 2023;Levy, Bogin, and Berant 2023;Yang et al. 2023;Shum, Diao, and Zhang 2023a) has explored ways to provide CoT demonstrations to enhance LLMs' reasoning capabilities, these methods largely rely on the semantic features of the problem or the answer.Such features introduce irrelevant noise on a global scale, which can obscure the logical information needed for reasoning.Consequently, the constructed demonstration sets do not effectively represent the domain-specific logical knowledge, and struggle to adequately trigger correct reasoning in LLMs.Second, some demonstration selection methods lack interpretability and scalability.These methods are primarily based on heuristic design (Wang et al. 2022;Zheng et al. 2023) or leverage the model itself to generate additional demonstrations (Zhong et al. 2024;Yasunaga et al. 2024).The demonstration sets chosen through these means inherently lack clear explanations, making it challenging to assess their effectiveness or determine the direction for further optimization.This limitation can be particularly problematic in scenarios where interpretability is crucial.</p>
<p>To better select a demonstration subset for a reasoning task, we believe that considering the logical patterns of reasoning is essential.Inspired by the work of (Min et al. 2022) and (Madaan, Hermann, and Yazdanbakhsh 2023), we observe that LLMs are more influenced by the templates and patterns in the context than by the correctness of the demonstrations themselves.Building on this insight, we investigate the selection of demonstrations based on Reasoning Patterns.This approach offers a dual benefit.First, it helps to eliminate bias introduced by irrelevant information, thereby reducing the gap between the demonstration set and the reasoning task.Second, it provides explicit interpretability, allowing us to gain a deeper understanding of how CoT prompting functions.This interpretability can also serve as a clue for attribution analysis and visualization.</p>
<p>In this work, we propose Pattern-CoT 1 , a CoT demonstration selection method based on reasoning patterns.Unlike previous approaches that focus on overall semantics, our method targets finer-grained logical reasoning operations.For instance, in mathematical reasoning, addition and multiplication represent distinct operations, while multiple sequential operators may indicate more complex operational patterns, as shown in Figure 1.Inspired by recent studies (Yang et al. 2023), a diverse range of these patterns should be incorporated into CoT.Specifically, for a given reasoning task, we first obtain a set of seed demonstrations with rationale (intermediate reasoning steps).These examples can be sourced from the training set or generated using a zeroshot approach.We then obtain specific operation tokens tailored to different task types, which help us extract reasoning patterns from the rationales.Here, we incorporate prior knowledge and guide the LLMs in generating these operation tokens.Based on the extracted reasoning patterns, we apply clustering techniques to merge similar patterns and design metrics to automatically assess the number of demonstration categories.Finally, we select representative demonstrations from each category to enrich the diversity and construct context prompts for LLMs.Notably, by incorporating task-specific knowledge, our method improves interpretability and facilitates further scalability.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We introduce the use of diverse reasoning patterns to enhance CoT prompting and design a demonstration selection method to reduce the gap between the demonstration set and the task.• Our method strengthens the interpretability of CoT in unsupervised scenarios, and can be utilized for further attribution analysis.• Extensive experiments demonstrate that our method con-1 https://github.com/Magicat128/Pattern-CoT.</p>
<p>sistently enhances performance across multiple reasoning tasks and various models.</p>
<p>Related Work Chain-of-Thought Prompting</p>
<p>Large language models have demonstrated significant ability in comprehending context and responding to prompts (Brown et al. 2020;Ouyang et al. 2022).Recent studies highlight that LLMs can achieve improved task completion without fine-tuning, particularly on reasoning tasks, when provided with few-shot demonstrations (Wei et al. 2022b).For instance, when presented with an example like Q: Mary has 9 yellow marbles.John has 3 yellow marbles.How many yellow marbles do they have in all?A: They have 9 + 3 = 12 yellow marbles.The answer is 12, LLMs are expected to emulate such a format, deconstruct the question, engage in multi-step reasoning, and refrain from generating random answers in subsequent tasks.This process is commonly referred to as chain-of-thought prompting or in-context learning (Wei et al. 2022a;Xie et al. 2022).However, implementing this practice often involves the manual design of prompts at a labour cost.Consequently, researchers are exploring more efficient example selection strategies to streamline this process.</p>
<p>Demonstration Selection and Refinement</p>
<p>Several CoT studies are directed towards automating the generation of demonstrations, such as retrieval-based (Rubin, Herzig, and Berant 2022), zero-shot (Kojima et al. 2022), clustering-based (Zhang et al. 2023), and self-prompt (Shao et al. 2023;Yasunaga et al. 2024).However, many of these approaches encounter challenges in achieving performance comparable to Manual-CoT, primarily due to the absence of supervision in example selection.In another branch of research, efforts are focused on enhancing the quality of CoT demonstrations.They incorporate elements such as knowledge-infusion (Zhao et al. 2023;Weng et al. 2023;Li et al. 2024), self-consistency (Wang et al. 2023a), complexity-based (Fu et al. 2022), contrastive-based (Chia et al. 2023), and progressive-hint (Zheng et al. 2023).The primary goal of these strategies is to ensure that LLMs adhere to the correct prompt and avoid being misled.</p>
<p>Role of In-Context Patterns</p>
<p>To understand the underlying mechanism of ICL, (Min et al. 2022) and (Madaan, Hermann, and Yazdanbakhsh 2023) employ counterfactual prompting methods.These methods involve substituting question-answer mapping, token distributions, answer patterns, and many other factors.Their findings consistently show that the correctness of examples is not the most crucial factor, but rather the distribution or pattern (e.g.equations, templates, sentence structure) of the examples.In this paper, we continue to uncover the power of CoT patterns and show how they can improve the reasoning process.</p>
<p>Seed Demonstration Collection</p>
<p>For a given task Q = {q 1 , q 2 , ..., q N } with N questions, we first need to obtain their rationales and answers {q i , r i , a i } that can be used as context for CoT prompting.For data from existing training sets, we can directly use the training data.However, in practical applications, complete training sets may not always be available.In such cases, we refer to methods like (Zhang et al. 2023;Shum, Diao, and Zhang 2023b) and leverage the zero-shot (Kojima et al. 2022) capabilities of LLMs to generate the corresponding rationales.It is important to note that we do not require the answers to be correct or labelled; our focus is on whether the generated rationales contain meaningful reasoning patterns.</p>
<p>Pattern Discovery</p>
<p>Based on the rationale set Ra = {r 1 , r 2 , ..., r N } that we have obtained, we next identify the reasoning operations T associated with the task.For tasks with a relatively limited action space, we can define reasoning operations using prior knowledge, as these operations represent the fundamental units of reasoning tasks.For example, in arithmetic problems, we refer to a glossary of possible operators from sources like Wikipedia2 , including basic arithmetic operations, square roots, comparison symbols, etc.For tasks with less clearly defined operations, we adapt definitions from arithmetic problems to guide LLMs in generating the corresponding reasoning operations.We design the prompt as: 'Similar to operators used in arithmetic such as (+, -, *, /), which operators do you think best represent the [TASK]?Example of [TASK]: ...'</p>
<p>For each rationale r i ∈ Ra, we extract the reasoning operation tokens or phrases t j ∈ T to form its reasoning pattern:
p i = f (r i , T ) = {t i1 , t i2 , ..., t ij }
(1) where f denotes the function used to extract the reasoning path.In this context, p i represents how LLMs apply these operations step-by-step to reach the final result, and t ij can repeated.</p>
<p>Pattern Wise Demonstration Selection</p>
<p>Once we have identified the task-relevant patterns, we use them to select better demonstration sets.Following (Zhang et al. 2023), we cluster all the p i patterns while preserving diversity.Although p i is a simplified sequence of tokens, it still contains substantial semantic information that can be used to uncover underlying similarities.For instance, a sequence of addition operations is likely to be closer to a single addition operation than to a single multiplication operation.</p>
<p>To leverage this, we use a language model to encode these patterns.We then apply the k-means clustering algorithm to generate k clusters and sample from each cluster:
p i = encode(p i )(2)return d c 1 , c 2 , ..., c k = cluster( p 1 , p 2 , ..., p i ) (3) d = {q m , r m , a m | p m ∈ c m , m = 1, 2, ..., k}(4)
where d denotes the demonstration set, c k denotes the kth cluster.Specifically, we use patterns primarily to select demonstrations rather than directly as context for downstream tasks.We utilize the original problem q k and rationale r k corresponding to the p k patterns as the CoT input.</p>
<p>Number of Demonstrations</p>
<p>Since previous methods lack knowledge-based guidance, the choice of k is often based on heuristic values.However, having too many demonstrations does not proportionally enhance the performance (Wei et al. 2022b;Agarwal et al. 2024), while too few may fail to adequately capture the task's characteristics.By incorporating reasoning operations, we can use the number of these operations to inform a more reasonable choice for k:
k = ⌈ 1 2 × n × (1 + log(N ))⌉(5)
where n denotes the number of identified operations, and ⌈⌉ represents the ceiling function that rounds up to the nearest integer.This formula empirically takes into account the impact of the number of operation types on the number of demonstrations and further adjusts based on the sample size.</p>
<p>Experiments</p>
<p>In this section, our objective is to evaluate the effectiveness of our proposed method and answer the following research questions:</p>
<p>• RQ1: Does incorporating reasoning patterns enhance the effectiveness of CoT prompting?• RQ2: How do the reasoning patterns influence the outputs of LLMs?</p>
<p>Experimental Setup</p>
<p>Datasets.We adopt eight representative datasets for our reasoning tasks: MultiArith (Roy and Roth 2015), GSM8K (Cobbe et al. 2021), AddSub (Hosseini et al. 2014), AQUA-RAT (Ling et al. 2017), SingleEq (Koncel-Kedziorski et al. 2015), SVAMP (Patel, Bhattamishra, and Goyal 2021), Coin-Flip (Wei et al. 2022b), and BIG-bench Date Understanding (Srivastava et al. 2023).They require certain reasoning steps and are commonly used for CoT method comparisons (Wei et al. 2022b;Kojima et al. 2022;Zhang et al. 2023;Wang et al. 2023b;Fu et al. 2022).</p>
<p>For tasks MultiArith, AddSub, SingleEq, and SVAMP, we define the set of operation tokens based on a glossary from Wikipedia, as the operations involved are relatively straightforward.For tasks GSM8K and AQUA, we expand the operation token vocabulary manually based on data distribution.For tasks Coin-Flip and BIG-bench Date Understanding, we prompt GPT-4 to generate the corresponding operation tokens.The specific details of the datasets can be found in Table 1.</p>
<p>Language Models.To facilitate subsequent interpretability analysis, we select open-source models as our reasoning engine.Specifically, we use models from the LLaMA-2 family due to their foundational logical reasoning capabilities and support for CoT prompting.These models are deployed on our local server, which is equipped with 8 RTX 3090 GPUs, each with 24GB of memory.Due to hardware constraints, we test only the 7B and 13B models.Experiments with larger models or those from other families are discussed in subsequent sections.</p>
<p>We use the inference functions of these models, and the process does not involve training or fine-tuning.Additionally, we set the hyperparameters with a temperature of 0.4 and top p of 0.9 to manage the model's randomness (Xu et al. 2022) Table 2: Accuracy (%) on eight reasoning datasets.We present the mean value obtained from five runs.* denotes the situation where k does not change, and results are copied from above.For the Random-CoT method, we report the best result since we are concerned about the potential of CoT.For the self-consistency method, we set the number of paths as 5 (Wang et al. 2023a).</p>
<p>Baselines.We primarily compare our methods with unsupervised methods including Zero-Shot-CoT (Kojima et al. 2022), Random-CoT, Auto-CoT (Zhang et al. 2023), and Self-Consistency (Wang et al. 2023a).Building on Auto-CoT, we introduce an additional variant, Auto-CoT-RA, which replaces the original question embedding with the rationale embedding for clustering.The purpose of this modification is to investigate whether this subtle shift can implicitly uncover the underlying patterns in reasoning.Unless otherwise specified, our method uses the same k value as the baseline in experiments.Additionally, we conduct experiments using our method with the adaptive k value that we designed.</p>
<p>Main Results (RQ1)</p>
<p>Table 2 presents the overall performance of various methods on the 7B and 13B models.Since our primary goal is to evaluate whether focusing on diverse patterns provides more benefit to reasoning than semantic information, we are not concerned with identifying which model achieves stateof-the-art performance.Based on these results, we make the following observations:</p>
<p>• Overall, our method consistently outperforms the baseline approaches.This stable improvement indicates that by introducing diverse reasoning patterns, we can identify more representative demonstration sets, where each example embodies a different reasoning strategy.Using these diverse examples as context for LLMs can further enhance their ability to solve downstream tasks.• We observe that for arithmetic problems with a limited set of operation tokens, such as MultiArith, AddSub, Sin-gleEq, and SVAMP, our method achieves more significant improvements compared to methods based on semantic information.This suggests that the demonstration sets we construct can effectively cover the majority of reasoning paths, thereby providing comprehensive guidance for LLMs to select appropriate reasoning patterns.• For datasets with a relatively broader action space, like GSM8K and AQuA, the improvements are less significant.This implies that a limited number of examples do not fully capture the diversity of reasoning patterns.However, when we recalculate the number of clusters using adaptive k and expand the demonstration set, we observe additional gains on these two datasets.• Surprisingly, we find that for datasets like Coin and Date, where the operation patterns are not explicitly defined, our method actually lead to greater improvements.We hypothesize that this is because the questions in these datasets are quite similar, making it difficult to distinguish them based on semantic features alone.In contrast, leveraging reasoning patterns allows us to identify representative examples from a different dimension.Additionally, both of these datasets show further performance improvements when using adaptive k.</p>
<p>There are several additional observations.For instance, in some cases, Auto-CoT-RA outperforms Auto-CoT, while in others it does not.This suggests that simply shifting from Auto-CoT Ours Q: Mark's father gave him $85.Mark bought 10 books, each of which cost $5.How much money does Mark have left?A: Let's think step by step.If Mark has $85 and he buys 10 books at $5 each, then he will spend a total of $50 (10 x $5).So, after buying the books, Mark will have $35 ($85 -$50) left.The answer is 35.</p>
<p>Q: Tom was at the beach for 5 days and found 7 seashells every day.How many seashells did Tom find during the beach trip?A: Let's think step by step.If Tom found 7 seashells every day, then he found a total of 7 x 5 = 35 seashells during his entire beach trip.The answer is 35.Q: Mike has 48 books.Alyssa has 8 times more books than Mike.How many dozen books does Alyssa have?A: Let's think step by step.If Mike has 48 books, then Alyssa has 8 times that number = 8 x 48 = 384 books.So Alyssa has 384 books.The answer is 384.</p>
<p>Q: Nancy has saved 4900 cents from selling lemonade.How many dollars does Nancy have?A: Let's think step by step.Nancy saved 4900 cents, which means she saved 4900 / 100 = 49 dollars.The answer is 49.</p>
<p>Q:</p>
<p>The value of a sport utility vehicle this year is 16,000 dollars , which is 0.8 of what its value was last year.How much is the value of the vehicle last year?Q: The value of a sport utility vehicle this year is 16,000 dollars , which is 0.8 of what its value was last year.How much is the value of the vehicle last year?A: Let's think step by step.The problem tells us that the value of the sport utility vehicle this year is 16,000 dollars, which is 0.8 of what it was last year.So, if we multiply the value of the vehicle this year by 0.8, we get the value of the vehicle last year = 16,000 x 0.8 = 12,800 dollars.The answer is 12,800.</p>
<p>A: Let's think step by step.The problem tells us that the value of the sport utility vehicle this year is $16,000, which is 0.8 times its value last year.This means that the value last year is $16,000 / 0.8 = $20,000.The answer is $20,000.</p>
<p>Impact of Operation Tokens (RQ1)</p>
<p>To further assess the impact of reasoning patterns, we conduct additional experiments.Given that GSM8K and AQuA datasets utilize additional operation tokens, we removed some of these tokens to determine their influence.Specifically, we categorize the expanded operation tokens into a basic operation subset, such as {+, −, ×, /}, similar to other arithmetic tasks, and the remaining tokens as supplementary subsets.These subsets represent only a portion of the reasoning patterns within these two datasets.</p>
<p>Figure 3 shows the results of using different subsets on the 7B model.The experimental results demonstrate that using operation subsets as reasoning pattern tokens can degrade overall performance.The primary reason for this is that these subsets do not sufficiently cover the task's logical scope.It leads to a lack of diversity.However, when the full set of operations is utilized, a broader range of scenarios can be activated, allowing the model to better adapt to the task.</p>
<p>Case Study (RQ2)</p>
<p>To gain a deeper understanding of CoT prompting, we perform a case study.Table 3 presents a typical instance analysis.We observe that Auto-CoT, due to its introduction of numerous irrelevant patterns, tends to distort the reasoning results of LLMs.In contrast, our method, which includes a diverse set of reasoning pattern templates, enables the model to generate correct responses.</p>
<p>Feature Attribution (RQ2)</p>
<p>Following the previous case study, we seek to understand why different contextual reasoning patterns alter the output of LLMs.Specifically, we employ a perturbation-based feature attribution analysis method (Winter 2002) to aid in this understanding.Traditional attention-based analysis methods have been criticized for their inability to identify the most significant features (Wiegreffe and Pinter 2019;Zhao et al. 2024), which is why we turned to this perturbation-based approach.By masking portions of the input tokens, we recompute the generation probabilities for each output token to as-</p>
<p>Model</p>
<p>AddSub AQuA SingleEq sess the input's attribution impact on these output tokens.We use Captum (Miglani et al. 2023) to achieve this visualization.Figure 4 presents the attribution analysis matrix for the case study.According to the visualization results, we find that when a particular pattern is overly dense in the examples, the model tends to activate related knowledge, which can lead to biased reasoning processes.Conversely, when these patterns are more diverse, the model is more likely to activate the correct reasoning pathways.Our method, by enhancing the diversity of patterns in the demonstrations, effectively reduces the distance to the reasoning task objectives.</p>
<p>Error Robustness (RQ3)</p>
<p>It is worth mentioning that we do not impose supervision on the labels of the demonstrations.Therefore, we proceed to count the number of incorrect instances within the selected set, as shown in Table 4.It is intriguing to notice that the majority of our provided demonstrations are imperfect, with</p>
<p>AQuA even exhibiting a 100% error rate.This phenomenon suggests that LLMs struggle to discern incorrect instances from correct ones.Instead, they learn from how the example approaches problem-solving, which we refer to as 'pattern'.</p>
<p>Our method encourages LLMs to follow the most probable reasoning chain towards the final answer and thus leads to a significant improvement.</p>
<p>Results on Other Models (RQ3)</p>
<p>To determine whether our method is applicable to different models, we test it on various LLM branches.Specifically, we select the GPT series to represent larger and more advanced models, and Qwen to represent multilingual models.</p>
<p>For the sake of hardware resources and budget constraints, we experiment with the GPT-3.5-turboand Qwen-7B models.Table 5 presents the performance of several methods on these models.Notably, the experiments show that Auto-CoT, in some cases, underperforms compared to direct answering on these models.We attribute this to the inherent noise in semantics-based methods.Our approach mitigates this noise, resulting in more consistent performance improvements.</p>
<p>Conclusion</p>
<p>This paper aims to address the noise issue inherent in unsupervised semantic-based CoT methods and proposes a reasoning pattern-based approach for CoT demonstration selection.Our method explicitly enhances the interpretability of reasoning processes and illustrates how LLMs can be guided toward generating accurate answers.Extensive experiments validate the effectiveness, robustness, and compatibility of our approach.</p>
<p>Figure 1 :
1
Figure 1: Example of the chain-of-thought prompting.The prompt influences how LLMs arrive at the final answer.</p>
<p>Figure 2 :
2
Figure 2: Illustration of our proposed framework.We first extract different patterns from the original rationales.Then clustering is used to produce a group of demonstrations.This enables LLMs to perceive diverse reasoning patterns and to select a proper solution path.It avoids LLMs being biased by monotonous reasoning mode.</p>
<p>Figure 3 :
3
Figure 3: Comparison of different operation sets.</p>
<p>Figure 4 :
4
Figure 4: Visualization of token attribution for the case study.The left part stands for the score matrix of patterns from Auto-CoT, and the right part stands for the score matrix from our method.The upper column denotes each individual prompt, and the row denotes the generated token sequence.Higher scores (positive) indicate that the input has a greater impact on the output.</p>
<p>Demonstration list d = [d 1 , d 2 , ..., d k ] 1: Acquire operation token set T with LLMs prompting or domain knowledge based on Q 2: for q i ∈ Q do Cluster all [ p 1 , p 2 , ..., p i ] into k clusters 14: Sample d = [d 1 , d 2 , ..., d k ] from each cluster 15:
3:Generate rationale r i with Zero-Shot-CoT4:p i = []5:for each token t ij ∈ r i do6:if t ij ∈ T then7:Update p i with t ij8:end if9:end for10:p i = encode(p i )11: end for12: Select proper k13:
Algorithm 1: Pattern-CoT Demonstration Selection Require: A set of task questions Q Ensure:</p>
<p>Table 1 :
1
• RQ3: Is our method robust and scalable to other models?The number of samples and operation tokens.
DatasetSamplesOperation TokensGSM8K1319+, −, ×, / 'more', 'less', 'twice', 'half'AQuA254+, −, ×, /, π,√x, x n , x • , logMultiArith600AddSub SingleEq395 508+, −, ×, /SVAMP1000Coin500'heads up', 'tails up''day', 'week',Date369'month', 'year''yesterday', 'tomorrow'</p>
<p>. To maintain consistency with(Zhang  et al. 2023), we use Sentence-BERT (Reimers and Gurevych 2019) as our encoder and select the 'all-MiniLM-L6-v2' model for semantic vector representation.This model has also been proven effective in our experiments.
LLaMA-2 ModelMultiArith GSM8K AddSub AQuA SingleEq SVAMP Coin DateZero-Shot-CoT72.3321.0057.9724.0157.6741.9044.60 39.29(+ SC)79.8327.1462.7821.6568.1147.6052.80 40.377b-chat-hfRandom-CoT Auto-CoT76.16 76.0024.41 26.9965.59 58.4822.44 24.0166.14 64.9646.59 43.8048.00 44.44 51.20 44.71Auto-CoT-RA74.8326.7663.2923.8066.9245.1948.00 43.08Ours79.6627.4565.0628.3471.8548.5059.40 45.79Ours (Adaptive k)79.66<em>28.0567.0829.1371.85</em>48.50<em>58.40 46.34Zero-Shot-CoT77.5034.4960.7515.7469.2949.4047.40 46.07Auto-CoT82.1636.7763.0325.1970.6755.5054.20 53.9313b-chat-hfAuto-CoT-RA82.1637.0462.0827.7466.1452.1062.80 54.47Ours83.1637.6865.8226.3774.8056.3957.40 56.91Ours (Adaptive k)83.16</em>38.4464.8131.4974.80<em>56.39</em>67.80 60.97</p>
<p>Table 3 :
3
Case study of Auto-CoT and our method for demonstration selection and downstream inference.The upper questions and answers are demonstrations constructed by two methods, and the lower part contains how LLMs solve the inference task.
DatasetDemos Incorrect Error RateMultiArith8225.0%GSM8K8562.5%AddSub8337.5%AQuA44100%SingleEq8225.0%SVAMP8675%Coin8337.5%Date8112.5%Table 4: The number of demonstrations and their error ratefor each dataset.question semantics to rationale semantics does not necessar-ily narrow the gap between demonstrations and the reason-ing task. Deeper reasoning patterns can still be obscured byirrelevant information. Moreover, in certain situations, us-ing a random demonstration set can also surpass Auto-CoT,although this improvement is inconsistent. This indirectlyhighlights that other factors, such as underlying reasoningpatterns, can influence the effectiveness of examples. Ourmethod, in most cases, demonstrates a more stable ability touncover these factors.
The glossary of arithmetic operators refers to the Wikipedia: https://en.wikipedia.org/wiki/Glossary of mathematical symbols
AcknowledgementsThis work was supported by the National Key R&amp;D Program of China (Grant No.2023ZD0120400), Beijing Natural Science Foundation (L247028), National Natural Science Foundation of China (No. 62276260, 62076235), Beijing Municipal Science and Technology Project (Z231100007423004).We sincerely thank all reviewers and ACs for their insightful comments, time and efforts.
R Agarwal, A Singh, L M Zhang, B Bohnet, L Rosias, S Chan, B Zhang, A Anand, Z Abbas, A Nova, J D Co-Reyes, E Chu, F Behbahani, A Faust, H Larochelle, arXiv:2404.11018Many-Shot In-Context Learning. 2024</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Y K Chia, G Chen, L A Tuan, S Poria, L Bing, arXiv:2311.09277Contrastive Chain-of-Thought Prompting. 2023</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, arXiv:2110.14168Training Verifiers to Solve Math Word Problems. 2021</p>
<p>Y Fu, H Peng, A Sabharwal, P Clark, T Khot, arXiv:2210.00720Complexity-based prompting for multi-step reasoning. 2022arXiv preprint</p>
<p>Learning to Solve Arithmetic Word Problems with Verb Categorization. M J Hosseini, H Hajishirzi, O Etzioni, N Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. A Moschitti, B Pang, W Daelemans, the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarAssociation for Computational Linguistics2014</p>
<p>Towards Reasoning in Large Language Models: A Survey. J Huang, K C Chang, -C , Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, 2023</p>
<p>. Canada Toronto, Association for Computational Linguistics</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. 202235</p>
<p>Parsing Algebraic Word Problems into Equations. R Koncel-Kedziorski, H Hajishirzi, A Sabharwal, O Etzioni, S D Ang, Transactions of the Association for Computational Linguistics. 32015</p>
<p>Diverse Demonstrations Improve In-context Compositional Generalization. T Kwiatkowski, J Palomaki, O Redfield, M Collins, A Parikh, C Alberti, D Epstein, I Polosukhin, J Devlin, K Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2019. 20237Natural questions: a benchmark for question answering research</p>
<p>Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. X Li, R Zhao, Y K Chia, B Ding, S Joty, S Poria, L Bing, International Conference on Learning Representations ICLR 2024. 2024</p>
<p>Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. W Ling, D Yogatama, C Dyer, P Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL). R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational Linguistics (ACL)Vancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study. A Madaan, K Hermann, A Yazdanbakhsh, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work. V Miglani, A Yang, A H Markosyan, D Garcia-Olano, N Kokhlikyan, S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, arXiv:2312.05491Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2023. 2022Using Captum to Explain Generative Language Models</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Are NLP Models really able to Solve Simple Math Word Problems?. A Patel, S Bhattamishra, N Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterOnline: Association for Computational Linguistics2021</p>
<p>Reasoning with Language Model Prompting: A Survey. S Qiao, Y Ou, N Zhang, X Chen, Y Yao, S Deng, C Tan, F Huang, H Chen, Reimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational Linguistics20231Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</p>
<p>Solving General Arithmetic Word Problems. S Roy, D Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)Lisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Learning To Retrieve Prompts for In-Context Learning. O Rubin, J Herzig, J Berant, Proceedings of the 2022 Conference of the North American Chapter. M Carpuat, M.-C De Marneffe, I V Meza Ruiz, the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Synthetic prompting: Generating chain-ofthought demonstrations for large language models. Z Shao, Y Gong, Y Shen, M Huang, N Duan, W Chen, arXiv:2302.006182023arXiv preprint</p>
<p>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. K Shum, S Diao, T Zhang, Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational Linguistics2023a</p>
<p>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. K Shum, S Diao, T Zhang, Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational Linguistics2023b</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations ICLR 2023. 2023a</p>
<p>Large language models are latent variable models: Explaining and finding good demonstrations for incontext learning. X Wang, J Wei, D Schuurmans, Q Le, E Chi, D Zhou, X Wang, W Zhu, M Saxon, M Steyvers, W Y Wang, arXiv:2207.00747Thirty-seventh Conference on Neural Information Processing Systems. 2022. 2023bRationale-Augmented Ensembles in Language Models</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, Emergent Abilities of Large Language Models. Transactions on Machine Learning Research. 2022aSurvey Certification</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Large Language Models are Better Reasoners with Self-Verification. Y Weng, M Zhu, F Xia, B Li, S He, S Liu, B Sun, K Liu, J Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Attention is not not Explanation. S Wiegreffe, Y Pinter, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. K Inui, J Jiang, V Ng, X Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong; ChinaAssociation for Computational Linguistics2019</p>
<p>The shapley value. Handbook of game theory with economic applications. E Winter, 20023</p>
<p>An Explanation of In-context Learning as Implicit Bayesian Inference. S M Xie, A Raghunathan, P Liang, T Ma, International Conference on Learning Representations ICLR. 2022. 2022</p>
<p>A systematic evaluation of large language models of code. F F Xu, U Alon, G Neubig, V J Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine Programming2022</p>
<p>Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process. Z Yang, Y Zhang, D Sui, C Liu, J Zhao, K Liu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Automatic Chain of Thought Prompting in Large Language Models. M Yasunaga, X Chen, Y Li, P Pasupat, J Leskovec, P Liang, E H Chi, D Zhou, Z Zhang, A Zhang, M Li, A Smola, H Zhao, H Chen, F Yang, N Liu, H Deng, H Cai, S Wang, D Yin, M Du, The Eleventh International Conference on Learning Representations ICLR 2023. 2024. 2023. 202415Explainability for large language models: A survey</p>
<p>Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework. R Zhao, X Li, S Joty, C Qin, L Bing, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational Linguistics (ACL)Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>C Zheng, Z Liu, E Xie, Z Li, Y Li, arXiv:2304.09797Progressive-Hint Prompting Improves Reasoning in Large Language Models. 2023</p>
<p>Q Zhong, K Wang, Z Xu, J Liu, L Ding, B Du, D Tao, arXiv:2404.14963Achieving 97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3727 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3727</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3727</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-374dd173491a59a10bbb2b3519ebcfe3649f529d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/374dd173491a59a10bbb2b3519ebcfe3649f529d" target="_blank">Teaching Models to Express Their Uncertainty in Words</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> It is shown that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits -- for the first time.</p>
                <p><strong>Paper Abstract:</strong> We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g."90% confidence"or"high confidence"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words ("verbalized probability") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3727.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3727.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verbalized probability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verbalized probability (natural-language expression of epistemic uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where a language model is trained or prompted to output a human-readable probability or confidence (e.g. "61%" or "high confidence") representing its estimated probability that its own answer is correct. In this paper GPT-3 is fine-tuned to produce such verbalized probabilities and evaluated for calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model (GPT-3 "davinci" family, 175 billion parameters) accessed via the OpenAI API and used zero-shot for answers; finetuned (supervised) to output verbalized probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate the probability that GPT-3's own zero-shot answer to an arithmetic question from the CalibratedMath suite is correct (i.e., per-question self-confidence about correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Supervised fine-tuning to map (question, GPT-3 zero-shot answer) pairs to calibrated verbal probability labels (either numeric percentages or discrete words); few-shot experiments using in-context examples and Expected Value (EV) decoding over top tokens were also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CalibratedMath — a benchmark of 21 programmatically generated arithmetic sub-tasks (addition, subtraction, multiplication, division, rounding, sequences, multi-answer tasks such as "name a prime < N"); training on an Add-subtract source set (~10k examples sampled across sub-tasks) and evaluation on Multi-answer and Multiply-divide sets to test distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Metrics: Mean Squared Error (MSE, equivalent to Brier score when binary) and Mean Absolute Deviation calibration error (MAD). Reported (main table) for verbalized numbers (finetuned): Multi-answer MSE 22.0%, MAD 16.4%; Multiply-divide MSE 15.5%, MAD 19.0%. Few-shot EV decoding (50-shot) approaches finetuned performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Verbalized probability (finetuned) outperformed the zero-shot answer logit and the indirect-logit finetuned model on the Multi-answer evaluation (e.g., MSE 22.0% vs answer-logit 37.4% on Multi-answer). On Multiply-divide the answer-logit and indirect-logit sometimes outperformed verbalized probability (answer-logit MSE 10.4% on Multiply-divide vs verbalized 15.5%), and the constant baseline matched or approached verbalized performance on some shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Labels for supervision are empirical sub-task accuracies (Ĥ_p_T), which are coarse proxies and can be suboptimal for individual questions; verbalized probabilities can overfit to training distribution (noted underconfidence on Multi-answer because evaluation answers are easier than training); limited tokenization effect (model tends to output exact tokens seen in training, harming smoothness); experiments restricted to arithmetic tasks (no real-world forecasting experiments); method depends on having a model that can emit natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>GPT-3 can be trained to express calibrated epistemic uncertainty in natural language (verbalized probability) without using internal logits; this generalizes moderately under distribution shift (better than some logit-based baselines on Multi-answer), few-shot in-context learning (≈50-shot) can approach finetuned calibration, and latent pre-trained embeddings contain features useful for predicting correctness (linear probes on embeddings achieve nontrivial generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Models to Express Their Uncertainty in Words', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3727.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3727.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answer logit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answer logit (normalized log-probability of the model's answer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A logit-based proxy for confidence computed from the model's normalized token log-probability for the produced answer (commonly used in prior calibration work). In this paper it is used as a zero-shot baseline for calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-3 model; here used zero-shot and confidence derived from the model's output log-probabilities for the generated answer string.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Zero-shot estimate of probability that the model's generated answer (a string) is correct, derived from the normalized log-probability of that answer.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Extract normalized log-probability (logit) of the answer string produced by the model and interpret it as a confidence score (no fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CalibratedMath (same as above), evaluated on Multi-answer and Multiply-divide to test calibration under shift.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MSE and MAD. Reported: Multi-answer MSE 37.4%, MAD 33.7%; Multiply-divide MSE 10.4%, MAD 9.4% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>On the Multi-answer evaluation, the answer-logit baseline performed substantially worse (more miscalibrated) than finetuned verbalized probability (37.4% vs 22.0% MSE). On Multiply-divide, the answer-logit performed best by MSE (10.4%) versus verbalized (15.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Log-probabilities reflect uncertainty over token sequences (surface forms) rather than epistemic uncertainty about claims; paraphrasability and multi-answer questions can break the mapping from token logprob to claim correctness; does not apply to models without tractable output likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Answer-logit can be a competitive zero-shot baseline in some settings but fails to capture epistemic uncertainty robustly across distribution shifts, especially when paraphrase/multi-answer phenomena are present.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Models to Express Their Uncertainty in Words', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3727.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3727.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Indirect logit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Indirect logit (logprob of appended 'True' token)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuning approach where the model is given its answer followed by a true/false token; the model's log-probability for the 'True' token serves as an estimate of confidence that the preceding answer is correct. This is used as an alternative logit-based finetuned method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 fine-tuned in supervised fashion to predict a True/False token appended after the answer; cross-entropy loss trained against ground-truth booleans for correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate per-question probability that GPT-3's zero-shot answer is correct by modeling the probability the appended 'True' token would be generated.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Supervised fine-tuning with cross-entropy where label is whether the model's zero-shot answer is correct; at inference the log-probability of the 'True' token is used as confidence (indirectly getting a scalar from logits).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CalibratedMath (same training/evaluation splits as verbalized experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MSE and MAD. Reported: Multi-answer MSE 33.7%, MAD 38.4%; Multiply-divide MSE 11.7%, MAD 7.1% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Indirect logit outperforms some baselines on Multiply-divide (e.g., best MAD 7.1% on Multiply-divide), but on Multi-answer it is worse than verbalized probability (33.7% vs 22.0% MSE). This suggests sensitivity to overfitting and dependence on training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential for overfitting in the finetuning setup used; depends on likelihoods (not applicable for models without tractable output probabilities); less robust on multi-answer evaluation than verbalized probability under the studied training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Indirect-logit fine-tuning can yield strong calibration on some out-of-distribution evaluation sets (e.g., Multiply-divide) but did not generalize as well as verbalized probability on Multi-answer in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Models to Express Their Uncertainty in Words', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3727.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3727.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Forecasting / real-world events (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forecasting and estimation of real-world/scientific event likelihoods (suggested application)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper mentions forecasting (economic forecasts, open scientific problems) as motivating examples where models must express epistemic uncertainty, and suggests evaluating whether verbalized calibration generalizes to forecasting tasks in future work, but contains no forecasting experiments or results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (discussed as candidate); more generally pre-trained LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Paper suggests applying the verbalized probability approach to models (e.g., GPT-3 or other pre-trained LLMs) beyond arithmetic tasks, including forecasting tasks, but does not perform such experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Hypothetical: estimating probabilities of events without known ground truth (examples cited: economic forecasts, open problems in science and mathematics, long-form forecasting), i.e., forecasting future real-world or scientific events.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Proposed/mentioned approaches include verbalized probability (fine-tune to produce natural-language probabilities), few-shot elicitation, or reinforcement learning using proper scoring rules; no experimental method in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>No dataset used in this paper for forecasting; authors suggest future work to evaluate calibration on other domains (history, biology) and forecasting formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No experimental metrics reported for forecasting in this paper; paper's calibration metrics (MSE/Brier, MAD) are suggested as applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>No empirical comparisons provided for forecasting; authors note prior work on calibration and abstention in NLP and suggest extending to forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper explicitly notes that current experiments are limited to arithmetic tasks and that generalization to forecasting/other subject areas is an open question; supervised finetuning labels used here (subtask empirical accuracies) may be inadequate for forecasting where ground truth is rare or unavailable; additional research (including RL with scoring rules) is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>The authors propose that verbalized probability could enable models to convey calibrated epistemic uncertainty in domains without known ground truth (e.g., forecasting), but they do not present experimental evidence on scientific discovery or real-world event forecasting in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Models to Express Their Uncertainty in Words', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering <em>(Rating: 2)</em></li>
                <li>Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift <em>(Rating: 2)</em></li>
                <li>Calibration of pre-trained transformers <em>(Rating: 2)</em></li>
                <li>GPT-3 nonfiction - calibration <em>(Rating: 1)</em></li>
                <li>Truthful AI: Developing and governing AI that does not lie <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3727",
    "paper_id": "paper-374dd173491a59a10bbb2b3519ebcfe3649f529d",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "Verbalized probability",
            "name_full": "Verbalized probability (natural-language expression of epistemic uncertainty)",
            "brief_description": "A method where a language model is trained or prompted to output a human-readable probability or confidence (e.g. \"61%\" or \"high confidence\") representing its estimated probability that its own answer is correct. In this paper GPT-3 is fine-tuned to produce such verbalized probabilities and evaluated for calibration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci, 175B)",
            "model_description": "Autoregressive transformer language model (GPT-3 \"davinci\" family, 175 billion parameters) accessed via the OpenAI API and used zero-shot for answers; finetuned (supervised) to output verbalized probabilities.",
            "prediction_task": "Estimate the probability that GPT-3's own zero-shot answer to an arithmetic question from the CalibratedMath suite is correct (i.e., per-question self-confidence about correctness).",
            "method_of_probability_estimation": "Supervised fine-tuning to map (question, GPT-3 zero-shot answer) pairs to calibrated verbal probability labels (either numeric percentages or discrete words); few-shot experiments using in-context examples and Expected Value (EV) decoding over top tokens were also evaluated.",
            "dataset_or_benchmark": "CalibratedMath — a benchmark of 21 programmatically generated arithmetic sub-tasks (addition, subtraction, multiplication, division, rounding, sequences, multi-answer tasks such as \"name a prime &lt; N\"); training on an Add-subtract source set (~10k examples sampled across sub-tasks) and evaluation on Multi-answer and Multiply-divide sets to test distribution shift.",
            "performance_metrics": "Metrics: Mean Squared Error (MSE, equivalent to Brier score when binary) and Mean Absolute Deviation calibration error (MAD). Reported (main table) for verbalized numbers (finetuned): Multi-answer MSE 22.0%, MAD 16.4%; Multiply-divide MSE 15.5%, MAD 19.0%. Few-shot EV decoding (50-shot) approaches finetuned performance.",
            "comparison_to_baselines": "Verbalized probability (finetuned) outperformed the zero-shot answer logit and the indirect-logit finetuned model on the Multi-answer evaluation (e.g., MSE 22.0% vs answer-logit 37.4% on Multi-answer). On Multiply-divide the answer-logit and indirect-logit sometimes outperformed verbalized probability (answer-logit MSE 10.4% on Multiply-divide vs verbalized 15.5%), and the constant baseline matched or approached verbalized performance on some shifts.",
            "limitations_or_challenges": "Labels for supervision are empirical sub-task accuracies (Ĥ_p_T), which are coarse proxies and can be suboptimal for individual questions; verbalized probabilities can overfit to training distribution (noted underconfidence on Multi-answer because evaluation answers are easier than training); limited tokenization effect (model tends to output exact tokens seen in training, harming smoothness); experiments restricted to arithmetic tasks (no real-world forecasting experiments); method depends on having a model that can emit natural language.",
            "notable_findings": "GPT-3 can be trained to express calibrated epistemic uncertainty in natural language (verbalized probability) without using internal logits; this generalizes moderately under distribution shift (better than some logit-based baselines on Multi-answer), few-shot in-context learning (≈50-shot) can approach finetuned calibration, and latent pre-trained embeddings contain features useful for predicting correctness (linear probes on embeddings achieve nontrivial generalization).",
            "uuid": "e3727.0",
            "source_info": {
                "paper_title": "Teaching Models to Express Their Uncertainty in Words",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Answer logit",
            "name_full": "Answer logit (normalized log-probability of the model's answer)",
            "brief_description": "A logit-based proxy for confidence computed from the model's normalized token log-probability for the produced answer (commonly used in prior calibration work). In this paper it is used as a zero-shot baseline for calibration.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci, 175B)",
            "model_description": "Same GPT-3 model; here used zero-shot and confidence derived from the model's output log-probabilities for the generated answer string.",
            "prediction_task": "Zero-shot estimate of probability that the model's generated answer (a string) is correct, derived from the normalized log-probability of that answer.",
            "method_of_probability_estimation": "Extract normalized log-probability (logit) of the answer string produced by the model and interpret it as a confidence score (no fine-tuning).",
            "dataset_or_benchmark": "CalibratedMath (same as above), evaluated on Multi-answer and Multiply-divide to test calibration under shift.",
            "performance_metrics": "MSE and MAD. Reported: Multi-answer MSE 37.4%, MAD 33.7%; Multiply-divide MSE 10.4%, MAD 9.4% (Table 1).",
            "comparison_to_baselines": "On the Multi-answer evaluation, the answer-logit baseline performed substantially worse (more miscalibrated) than finetuned verbalized probability (37.4% vs 22.0% MSE). On Multiply-divide, the answer-logit performed best by MSE (10.4%) versus verbalized (15.5%).",
            "limitations_or_challenges": "Log-probabilities reflect uncertainty over token sequences (surface forms) rather than epistemic uncertainty about claims; paraphrasability and multi-answer questions can break the mapping from token logprob to claim correctness; does not apply to models without tractable output likelihoods.",
            "notable_findings": "Answer-logit can be a competitive zero-shot baseline in some settings but fails to capture epistemic uncertainty robustly across distribution shifts, especially when paraphrase/multi-answer phenomena are present.",
            "uuid": "e3727.1",
            "source_info": {
                "paper_title": "Teaching Models to Express Their Uncertainty in Words",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Indirect logit",
            "name_full": "Indirect logit (logprob of appended 'True' token)",
            "brief_description": "A finetuning approach where the model is given its answer followed by a true/false token; the model's log-probability for the 'True' token serves as an estimate of confidence that the preceding answer is correct. This is used as an alternative logit-based finetuned method.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci, 175B)",
            "model_description": "GPT-3 fine-tuned in supervised fashion to predict a True/False token appended after the answer; cross-entropy loss trained against ground-truth booleans for correctness.",
            "prediction_task": "Estimate per-question probability that GPT-3's zero-shot answer is correct by modeling the probability the appended 'True' token would be generated.",
            "method_of_probability_estimation": "Supervised fine-tuning with cross-entropy where label is whether the model's zero-shot answer is correct; at inference the log-probability of the 'True' token is used as confidence (indirectly getting a scalar from logits).",
            "dataset_or_benchmark": "CalibratedMath (same training/evaluation splits as verbalized experiments).",
            "performance_metrics": "MSE and MAD. Reported: Multi-answer MSE 33.7%, MAD 38.4%; Multiply-divide MSE 11.7%, MAD 7.1% (Table 1).",
            "comparison_to_baselines": "Indirect logit outperforms some baselines on Multiply-divide (e.g., best MAD 7.1% on Multiply-divide), but on Multi-answer it is worse than verbalized probability (33.7% vs 22.0% MSE). This suggests sensitivity to overfitting and dependence on training distribution.",
            "limitations_or_challenges": "Potential for overfitting in the finetuning setup used; depends on likelihoods (not applicable for models without tractable output probabilities); less robust on multi-answer evaluation than verbalized probability under the studied training regime.",
            "notable_findings": "Indirect-logit fine-tuning can yield strong calibration on some out-of-distribution evaluation sets (e.g., Multiply-divide) but did not generalize as well as verbalized probability on Multi-answer in this setup.",
            "uuid": "e3727.2",
            "source_info": {
                "paper_title": "Teaching Models to Express Their Uncertainty in Words",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Forecasting / real-world events (mentioned)",
            "name_full": "Forecasting and estimation of real-world/scientific event likelihoods (suggested application)",
            "brief_description": "The paper mentions forecasting (economic forecasts, open scientific problems) as motivating examples where models must express epistemic uncertainty, and suggests evaluating whether verbalized calibration generalizes to forecasting tasks in future work, but contains no forecasting experiments or results.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (discussed as candidate); more generally pre-trained LLMs",
            "model_description": "Paper suggests applying the verbalized probability approach to models (e.g., GPT-3 or other pre-trained LLMs) beyond arithmetic tasks, including forecasting tasks, but does not perform such experiments.",
            "prediction_task": "Hypothetical: estimating probabilities of events without known ground truth (examples cited: economic forecasts, open problems in science and mathematics, long-form forecasting), i.e., forecasting future real-world or scientific events.",
            "method_of_probability_estimation": "Proposed/mentioned approaches include verbalized probability (fine-tune to produce natural-language probabilities), few-shot elicitation, or reinforcement learning using proper scoring rules; no experimental method in this paper.",
            "dataset_or_benchmark": "No dataset used in this paper for forecasting; authors suggest future work to evaluate calibration on other domains (history, biology) and forecasting formats.",
            "performance_metrics": "No experimental metrics reported for forecasting in this paper; paper's calibration metrics (MSE/Brier, MAD) are suggested as applicable.",
            "comparison_to_baselines": "No empirical comparisons provided for forecasting; authors note prior work on calibration and abstention in NLP and suggest extending to forecasting.",
            "limitations_or_challenges": "Paper explicitly notes that current experiments are limited to arithmetic tasks and that generalization to forecasting/other subject areas is an open question; supervised finetuning labels used here (subtask empirical accuracies) may be inadequate for forecasting where ground truth is rare or unavailable; additional research (including RL with scoring rules) is recommended.",
            "notable_findings": "The authors propose that verbalized probability could enable models to convey calibrated epistemic uncertainty in domains without known ground truth (e.g., forecasting), but they do not present experimental evidence on scientific discovery or real-world event forecasting in this work.",
            "uuid": "e3727.3",
            "source_info": {
                "paper_title": "Teaching Models to Express Their Uncertainty in Words",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "rating": 2
        },
        {
            "paper_title": "Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift",
            "rating": 2
        },
        {
            "paper_title": "Calibration of pre-trained transformers",
            "rating": 2
        },
        {
            "paper_title": "GPT-3 nonfiction - calibration",
            "rating": 1
        },
        {
            "paper_title": "Truthful AI: Developing and governing AI that does not lie",
            "rating": 1
        }
    ],
    "cost": 0.01263825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Teaching models to express their uncertainty in words</h1>
<p>Stephanie Lin<br>sylin07@gmail.com<br>University of Oxford<br>Jacob Hilton<br>jhilton@openai.com<br>OpenAI<br>Owain Evans<br>owaine@gmail.com<br>University of Oxford</p>
<h4>Abstract</h4>
<p>We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language - without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. " $90 \%$ confidence" or "high confidence"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words ("verbalized probability") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.</p>
<h2>1 Introduction</h2>
<p>Current state-of-the-art language models perform well on a wide range of challenging question-answering tasks (Brown et al., 2020; Chowdhery et al., 2022; Hoffmann et al., 2022). They can even outperform the average human on the MMLU benchmark (which consists of exam-like questions across 57 categories) and on BIG-Bench (which consists of 150+ diverse tasks). Yet when models generate long-form text, they often produce false statements or "hallucinations" (Lin et al., 2021; Maynez et al., 2020; Shuster et al., 2021). This reduces their value to human users, as users cannot tell when a model is being truthful or not.</p>
<p>The problem of truthfulness motivates calibration for language models (Nguyen \&amp; O'Connor, 2015). If models convey calibrated uncertainty about their statements, then users know how much to trust a given statement. This is important for current models (which often hallucinate falsehoods) but also for any model that makes statements where there is no known ground truth (e.g. economic forecasts, open problems in science or mathematics).</p>
<p>Previous work on calibration focuses on the model log-probabilities or "logits" (Guo et al., 2017; Jiang et al., 2021). Yet the log-probabilities of models like GPT-3 represent uncertainty over tokens (ways of expressing a claim) and not epistemic uncertainty over claims themselves. If a claim can be paraphrased in many different ways, then each paraphrase may have a low log-probability. ${ }^{1}$ By contrast, when humans express uncertainty, this is epistemic uncertainty about the claim itself. ${ }^{2}$ In this paper, we finetune models to express epistemic uncertainty using natural language. We call this "verbalized probability".</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Q: What is the remainder when 23 is divided by 4? $\leftarrow$ Prompt</h1>
<p>A: $3 \leftarrow$ Answer generated by GPT3 (greedy decoding)
Confidence: Medium $\leftarrow$ Confidence generated by GPT3 (greedy decoding)</p>
<p>Figure 1: Illustration of verbalized probability and the CalibratedMath task. The prompt is in bold and GPT-3's output is in blue. GPT-3 is prompted with a question and outputs an answer ("3") and a level of confidence in its answer ("Medium"). GPT-3 is scored on the calibration of its confidence (not on the accuracy of its answer). In this example, the answer is correct but the confidence is only "Medium". Using our MSE metric (Section 2.3), this confidence would score $(1-0.5)^{2}=0.25$.</p>
<p>The goal of verbalized probability is to express uncertainty in a human-like way but not to directly mimic human training data. Models should be calibrated about their own uncertainty, which differs from human uncertainty. For example, GPT-3 outperforms most humans on a computer security quiz (Hendrycks et al., 2020) but is much worse at arithmetic questions of the form " $2 \times 3 \times 7=$ ?". Thus, we expect pre-trained models will need to be finetuned to produce calibrated verbalized probabilities.</p>
<p>Training models in verbalized probability is a component of making models "honest" (Evans et al., 2021; Askell et al., 2021a; Christiano, 2021). We define a model as honest if it can communicate everything it represents internally in natural language (and will not misrepresent any internal states). Honesty helps with AI alignment: if an honest model has a misinformed or malign internal state, then it could communicate this state to humans who can act accordingly. Calibration is compatible with a certain kind of dishonesty, because a model could be calibrated by simply imitating a calibrated individual (without having the same "beliefs" as the individual). However, if GPT-3 achieves good calibration on diverse questions after finetuning as in Section 3.1, it seems unlikely that it dishonestly misrepresents its confidence.</p>
<h3>1.1 Contributions</h3>
<p>We introduce a new test suite for calibration. CalibratedMath is a suite of elementary mathematics problems. For each question, a model must produce both a numerical answer and a confidence in its answer (see Figure 1). There are many types of question, which vary substantially in content and in difficulty for GPT-3. This allows us to test how calibration generalizes under distribution shifts (by shifting the question type) and makes for a challenging test (see Figure 3). Since GPT-3's math abilities differ greatly from humans, GPT-3 cannot simply imitate human expressions of uncertainty.
GPT-3 can learn to express calibrated uncertainty using words ("verbalized probability"). We finetune GPT-3 to produce verbalized probabilities. It achieves reasonable calibration both in- and out-of-distribution, outperforming a fairly strong baseline (Figure 5 and Table 1).
This calibration performance is not explained by learning to output logits. GPT-3 does not simply learn to output the uncertainty information contained in its logits (Section 3.4). We also show that certain superficial heuristics (e.g. the size of the integers in the arithmetic question) cannot explain the performance of verbalized probability.</p>
<p>We compare verbalized probability to finetuning the model logits. We show how to finetune GPT-3 to express epistemic uncertainty via its model logits (see "Indirect logit" in Table 2) and find that this also generalizes calibration under distribution shift (Table 1).</p>
<h2>2 Setup</h2>
<h3>2.1 Calibration and Three Kinds of Probability</h3>
<p>We want to test the calibration of language models for uncertainty over their own answers to questions. The basic idea is that if a calibrated model assigns $90 \%$ to an answer, then the answer is correct $90 \%$ of the time.</p>
<table>
<thead>
<tr>
<th>Kind of probability</th>
<th>Definition</th>
<th>Example</th>
<th>Supervised objective</th>
<th>Desirable properties</th>
</tr>
</thead>
<tbody>
<tr>
<td>Verbalized (number / word)</td>
<td>Express uncertainty in language ('61\%’ or 'medium confidence')</td>
<td>Q: What is 952 - 55? A: 897 $\leftarrow$ Answer from GPT3 (greedy) Confidence: 61\% / Medium $\leftarrow$ Confidence from GPT3</td>
<td>Match 0 -shot empirical accuracy on math subtasks</td>
<td>Handle multiple correct answers; Express continuous distributions</td>
</tr>
<tr>
<td>Answer logit (zero-shot)</td>
<td>Normalized logprob of the model’s answer</td>
<td>Q: What is 952 - 55? A: 897 $\leftarrow$ Normalized logprob for GPT3’s answer</td>
<td>None</td>
<td>Requires no training</td>
</tr>
<tr>
<td>Indirect logit</td>
<td>Logprob of ‘True’ token when appended to model’s answer</td>
<td>Q: What is 952 - 55? A: 897 $\leftarrow$ Answer from GPT3 (greedy) True/false: True $\leftarrow$ Logprob for “True” token</td>
<td>Cross-entropy loss against groundtruth</td>
<td>Handles multiple correct answers</td>
</tr>
</tbody>
</table>
<p>Figure 2: Three kinds of probability used in this paper. Prior work on calibration focuses on the answer logit. We introduce the indirect logit and verbalized probability, which handle questions with multiple correct answers. Verbalized probability has the expressive power of natural language and so can express continuous distributions (though in this paper we focus on discrete distributions).</p>
<p>Formally, let $M$ be a model, $q$ be a question, $a_{M}$ be the model’s answer, and $p_{M}=\operatorname{Pr}(a_{M}|q)$ be the assigned probability that $a_{M}$ is correct. Then these assigned probabilities are (perfectly) calibrated if:</p>
<p>$\operatorname{Pr}(a_{M}|p_{M}=p)=p$</p>
<p>for $p \in[0,1]$ (Guo et al., 2017). In this paper, we test calibration on different sets of questions to evaluate how well calibration generalizes under distribution shift (Ovadia et al., 2019).</p>
<p>We consider three sources for the probability $p_{M}$ that the model’s answer is correct, as shown in Figure 2. Two of the kinds of probability (“answer logit” and “indirect logit”) are based on the log-probabilities that a language model assigns to tokens. Thus they cannot be used for models without a tractable likelihood on outputs (e.g. information retrieval models that call out to external resources). By contrast, verbalized probabilities apply to any model that outputs natural language. Moreover, verbalized probabilities mirror human expression of uncertainty. This allows models to respond to prompts from non-technical users (e.g. “How sure are you about what you just said?”, “I’ve told you my confidence on a scale from 1-5. Can you do the same?”). This also allows models to decide when and how to provide uncertainty information (depending on the human audience).</p>
<h3>2.2 CalibratedMath</h3>
<p>CalibratedMath is a test suite consisting of 21 arithmetic tasks, including addition, multiplication, rounding, arithmetic progressions, and finding remainders (see full details in Table 3). For each task, questions and answers are programmatically generated. The answers are always integers and for some tasks there are multiple correct answers (e.g. “Name any prime number below 208?”). The 21 tasks are further divided into sub-tasks based on the number of digits in each operand and the number format. The sub-tasks vary in difficulty for GPT-3. For example, multiplication is harder than addition and gets more difficult as the number of digits is increased. The fact that some sub-tasks are predictably easier or harder for GPT-3 is crucial for a challenging test of calibration.</p>
<p>As in prior work on calibration in ML (Ovadia et al., 2019; Karandikar et al., 2021), we focus on how well calibration generalizes under distribution shift. Our main experiments use the “Add-subtract” training set (Figure 3). This consists of tasks in CalibratedMath that involve addition or subtraction and have a unique correct answer. The evaluation set (called “Multi-answer”) consists of questions with multiple correct answers that sometimes involve multiplication and division. There is a distribution shift between training and evaluation, with the following two aspects:</p>
<p>Training: Add-subtract</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Q: What is $952-55$ ? <br> A: 897 <br> Confidence: $\underline{61 \%}$</th>
<th style="text-align: center;">Q: Name any number smaller than 621? <br> A: 518 <br> Confidence: $\qquad$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q: What comes next: 3, 12, 21, 30...? <br> A: 42 <br> Confidence: $\underline{22 \%}$</td>
<td style="text-align: center;">Q: Name any prime number smaller than 56? <br> A: 7 <br> Confidence: $\qquad$</td>
</tr>
<tr>
<td style="text-align: center;">Q: What is $6+5+7$ ? <br> A: 17 <br> Confidence: $\underline{36 \%}$</td>
<td style="text-align: center;">Q: Name two numbers that sum to 76 ? <br> A: 69 and 7 <br> Confidence: $\qquad$</td>
</tr>
</tbody>
</table>
<p>Figure 3: Examples from training and one of the evaluation sets for CalibratedMath. GPT-3 is finetuned on the Add-subtract training set (left). Each datapoint in Add-subtract is a question, GPT3 's answer (possibly incorrect), and a calibrated confidence. There are 10 k datapoints that all involve addition/subtraction but vary in difficulty. Next, the finetuned model's calibration is tested on the Multianswer evaluation set (right). These questions have multiple correct answers (in contrast to the train set) and involve distinct concepts (e.g. prime numbers). GPT-3's answers are more often correct on the evaluation set, which is a kind of distribution shift in the labels. (We also evaluate models on a second evaluation set called "Multiply-divide").</p>
<ul>
<li>Shift in task difficulty: GPT-3 is more likely to answer questions in the evaluation set (Multianswer) correctly than the training set (Add-subtract). Median accuracy is $65 \%$ for Multi-answer and $21 \%$ for Add-subtract (for full details see Figure 8). Thus, to be well calibrated, the model should assign higher probabilities on average to answers in the evaluation set than the training set. This is essentially a shift in the "label distribution" from training to evaluation. (We expect language models other than GPT-3 to have a similar distribution shift for the same reason.)</li>
<li>Shift in content: The training and evaluation sets differ in the mathematical concepts they employ and whether or not there are multiple correct answers.</li>
</ul>
<p>Though not shown in Figure 3, models trained on Add-subtract are also evaluated on a second evaluation set called "Multiply-divide". Questions in Multiply-divide have unique correct answers but are more difficult than those in Add-subtract and include distinct concepts related to multiplication and division (Table 3).</p>
<h1>2.3 Metrics</h1>
<p>Our goal is to measure the model's calibration when expressing uncertainty about its own zero-shot answers. In all our experiments, the model's zero-shot answers are held fixed. The goal is not to improve the model's answers but instead to improve calibration in expressing uncertainty over these answers. ${ }^{3}$ Calibration is measured using two metrics:
Mean squared error (MSE). Following Section 2.1, for each question the model $M$ assigns a probability $p_{M}$ to its own answer $a_{M}$ being correct. The MSE compares $p_{M}$ to the groundtruth of whether $a_{M}$ is correct or not:</p>
<p>$$
\mathbb{E}<em M="M">{q}\left[\left(p</em>\right]
$$}-\mathbb{I}\left(a_{M}\right)\right)^{2</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Note that a model can be perfectly calibrated (per Equation 1) and not have a MSE of zero. The MSE combines calibration error with "sharpness" (Kuleshov \&amp; Liang, 2015), while the MAD (below) just measures the former. (The MSE is called the "Brier Score" in probabilistic forecasting.)</p>
<p>Mean absolute deviation calibration error (MAD). The MAD estimates how closely the model approximates Equation 1 based on a finite sample. Model probabilities are divided into $K$ bins with equal numbers of samples, so the bins have denser coverage where there are more samples (Nguyen \&amp; O'Connor, 2015). Within each bin $b_{i}$, we calculate the proportion of correct answers (" $\operatorname{acc}\left(b_{i}\right)$ " or "accuracy") and average probability assigned to answers in $b_{i}$ ("conf $\left(b_{i}\right)$ " or the "average confidence"). Then the MAD is given by:</p>
<p>$$
\frac{1}{K} \sum_{i=1}^{K}\left|\operatorname{acc}\left(b_{i}\right)-\operatorname{conf}\left(b_{i}\right)\right|
$$</p>
<p>While this is not a proper scoring rule, it offers a simple numeric summary of the calibration curves shown in Section 3 (Hendrycks et al., 2018; Nixon et al., 2019).</p>
<h1>3 Experiments</h1>
<p>For our experiments, we used the 175 -billion parameter GPT-3 model ("davinci") via the OpenAI API (Brown et al., 2020). We tried out smaller models but their performance on arithmetic questions is too weak for CalibratedMath to be challenging. ${ }^{4}$</p>
<p>How can we finetune a pre-trained model to output calibrated verbalized probabilities? We finetune GPT-3 using supervised learning. This approach is less principled and flexible than using reinforcement learning (with rewards derived from a proper scoring rule). However, supervised learning was easier to implement using OpenAI's API, and provides an interesting test of generalization outside the training distribution.</p>
<h3>3.1 Supervised finetuning</h3>
<p>To finetune GPT-3 to produce verbalized probabilities, we need a labeled training set. Each input is a question followed by GPT-3's answer and the label is a (calibrated) confidence (see Figure 3). The basic intuition is that for questions GPT-3 is likely to get wrong, its confidence should be low. Thus, we use GPT-3's empirical accuracy on each type of question as the label. We recognize that this approach can lead to suboptimal labels. For example, it might use a low-confidence label for " $10 \times 10=100$ " because most two-digit multiplications are hard for GPT-3. But we will show that the approach works well enough for our purposes.</p>
<p>Formally, let $q$ be a question from sub-task $T$. Let $a_{M}$ be GPT-3's answer to $q$. We define $\hat{p}<em M="M">{T}$ associated with the input $\left(q, a</em>\right)$ to be GPT-3's empirical accuracy on sub-task $T$ :</p>
<p>$$
\hat{p}<em T="T" _in="\in" q="q">{T}=\mathbb{E}</em>\right)\right]
$$}\left[\mathbb{I}\left(a_{M</p>
<p>which we estimate using random samples generated from $T$. The full training set is then constructed as follows. For each sub-task $T$ we randomly sample 100 questions and generate GPT-3's zero-shot answers (using greedy decoding) for a total of $|T| \times 100 \approx 10 \mathrm{k}$ inputs. We then compute the $\hat{p}_{T}$ for each $T$ and use it to construct the label for each sample from $T$.</p>
<p>The label is a simple transformation of $\hat{p}<em T="T">{T}$. For the "verbalized numbers" setup, the label is given by $\left\lfloor 100 * \hat{p}</em>$ to one of five words corresponding to probability intervals of width 0.2 . Categories can then be mapped back to probability values by taking the}\right\rfloor$. In the "verbalized words" setup, we use a set of five words (e.g. "lowest", "low", "medium", "high", "highest") to express the degree of confidence. We map $\hat{p}_{T</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 4: Calibration scores on the Multi-answer and Multiply-divide evaluation sets. The same results are shown in Table 1 below.</p>
<p>Table 1: Calibration scores on evaluation sets. The finetuned setups were trained on the Add-subtract set. We test how well calibration generalizes under distribution shift. Scores are in percentage terms and lower is better. Note: the MSE is not for answers to questions but for the probability the answers are correct.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setup</th>
<th style="text-align: left;">Multi-answer</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Multiply-divide</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MSE</td>
<td style="text-align: left;">MAD</td>
<td style="text-align: left;">MSE</td>
<td style="text-align: left;">MAD</td>
</tr>
<tr>
<td style="text-align: left;">Verbalized numbers (finetune)</td>
<td style="text-align: left;">$\mathbf{2 2 . 0}$</td>
<td style="text-align: left;">$\mathbf{1 6 . 4}$</td>
<td style="text-align: left;">15.5</td>
<td style="text-align: left;">19.0</td>
</tr>
<tr>
<td style="text-align: left;">Answer logit (zero-shot)</td>
<td style="text-align: left;">37.4</td>
<td style="text-align: left;">33.7</td>
<td style="text-align: left;">$\mathbf{1 0 . 4}$</td>
<td style="text-align: left;">9.4</td>
</tr>
<tr>
<td style="text-align: left;">Indirect logit (finetune)</td>
<td style="text-align: left;">33.7</td>
<td style="text-align: left;">38.4</td>
<td style="text-align: left;">11.7</td>
<td style="text-align: left;">$\mathbf{7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Constant baseline</td>
<td style="text-align: left;">34.1</td>
<td style="text-align: left;">31.1</td>
<td style="text-align: left;">15.3</td>
<td style="text-align: left;">8.5</td>
</tr>
</tbody>
</table>
<p>midpoint of the corresponding interval. (We found that using meaningful words - such as "lowest" etc. worked slightly less well than meaningless names. See Appendix B.1.)</p>
<h1>3.1.1 Indirect logit and baselines</h1>
<p>For the indirect logit (defined in Figure 2), we use the same random sample of 100 questions from each sub-task (along with GPT-3's zero-shot answer). However, in this case the label for each individual questionanswer pair is the boolean True/False value indicating whether the model's answer was correct, for which we have the groundtruth. Thus we can optimize the cross-entropy loss. Further details for the supervised finetuning setup are given in Appendix B.3.</p>
<p>We compare the two finetuned setups (verbalized probability and indirect logit) to the "zero-shot answer logit" (see Fig. 2). We also include a "constant baseline". This baseline uses a constant probability on the evaluation set, where the value of the constant is the best-scoring value on the training set (in terms of MSE) ${ }^{5}$. Metrics are shown in Table 1 and Figure 4, while calibration curves are in Figure 5.</p>
<h3>3.2 Results</h3>
<p>Verbalized probability generalizes well to both eval sets. The main result is shown in Table 1 and Figures 4 and 5. After finetuning on the Add-subtract training set, verbalized probabilities generalize reasonably well to both the Multiply-divide and Multi-answer evaluation sets. So the model remains moderately calibrated under a substantial distribution shift. In terms of MSE, the model outperforms the two logit</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: Calibration curves for training (left) and evaluation (center and right). Curves are generated using the same procedure as the MAD (Section 2.3). The probabilities for each question are divided into bins, and the y-value for a bin is the proportion of questions for which the answer was true (i.e. the model accuracy). The size of markers indicates the bin size. We see that the two logit setups are very underconfident on the Multi-answer evaluation, while all three setups are better calibrated on the Multiply-divide evaluation.
setups on Multi-answer and matches the constant baseline on Multiply-divide. ${ }^{6}$ We ran an additional experiment to probe generalization, where we flipped around the training set (training on Multiply-divide and evaluating on both Add-subtract and Multi-answer). Again, verbalized probability generalizes reasonably well and outperforms other setups on Multi-answer (see Appendix C.3). Finally, we find that verbalized probability performs similarly whether the model outputs tokens for words or numbers (see Appendix C.4).
Verbalized probability overfits to training. Calibration for verbalized probability is much better indistribution. The model is underconfident in its answers to Multi-answer because these answers are more likely to be correct than those for the Add-subtract training set. ${ }^{7}$
Indirect logit generalizes well to Multiply-divide. The indirect logit achieves impressive calibration on the Multiply-divide evaluation set, where it outperforms other models. However, it does worse than verbalized probability on the Multi-answer evaluation. This is likely because it is more difficult to avoid overfitting given our setup. ${ }^{8}$ Further work could explore how the indirect logit compares to verbalized probability with different training setups (e.g. a more diverse distribution on probabilities and questions).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Few-shot: Multi-answer
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: Calibration curves for few-shot learning (verbalized probability). Compares stochastic $k$-shot for varying $k$ (using Expected Value decoding) to supervised finetuning (10k datapoints with greedy decoding) on the evaluation sets. 50 -shot is almost as calibrated as the finetuned setup.</p>
<h1>3.3 Stochastic Few-shot</h1>
<p>In order to learn more about how verbalized probability generalizes, we tested GPT-3's calibration in a stochastic $k$-shot setting, while varying $k$ from 1 to 50 . We used the following procedure. For each question in the evaluation set, we randomly sample $k$ new examples from the Add-subtract training set and include them in the context. ${ }^{9}$ In order to generate verbalized probabilities, we do not use greedy decoding (as in the finetuning experiments) but instead find the weighted sum of the model's top five tokens (where the weights are the model probabilities for the tokens). This "Expected Value decoding" is less in the spirit of verbalized probabilities, but gives us a sense of the model's capabilities (see Appendix C.2). The resulting calibration curves are shown in Figure 6.</p>
<p>On both evaluation sets, GPT-3 starts out visibly uncalibrated, but begins to show improvement at $k=25$ and above. At $k=50$, performance is already close to that of the finetuned models, which are trained on over 2.5 k samples. One potential explanation is that GPT-3 already has latent representations for questions and answers that relate to calibrated confidence, and the few-shot examples allow it to locate the task (Reynolds $\&amp;$ McDonell, 2021). We discuss this in the following section.</p>
<h3>3.4 Explaining the performance of verbalized probability</h3>
<p>We have shown that GPT-3 learns to express uncertainty in words and generalize calibration to new tasks. But what exactly has GPT-3 learned and would the learned features enable generalization beyond our experiments?</p>
<p>Does GPT-3 just learn to output the logits? One possibility is that the verbalized probability results are fully explained by GPT-3 learning to output information in its logits. However, we have already seen that verbalized probability generalizes better than the answer logit on the Multi-answer evaluation. Moreover, on the Multiply-divide evaluation, the correlation in performance between verbalized probability and answer</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Calibration performance of alternative models. Verbalized probability outperforms simple heuristics, but the linear probe on pre-trained embedding model performs well.</p>
<table>
<thead>
<tr>
<th>Setup</th>
<th>Multi-answer</th>
<th></th>
<th>Multiply-divide</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>MSE</td>
<td>MAD</td>
<td>MSE</td>
<td>MAD</td>
</tr>
<tr>
<td>Verbalized probability (finetune)</td>
<td>29.0</td>
<td>24.0</td>
<td>12.7</td>
<td>10.6</td>
</tr>
<tr>
<td>Log. reg. with heuristic features</td>
<td>29.7</td>
<td>31.2</td>
<td>17.7</td>
<td>18.5</td>
</tr>
<tr>
<td>Linear probe on GPT3 embedding</td>
<td>31.2</td>
<td>30.1</td>
<td>14.0</td>
<td>14.2</td>
</tr>
</tbody>
</table>
<p>logit across sub-tasks is only modest (see Appendix C.4). So GPT-3 must be using more than just the information in the logits.</p>
<p>Does GPT-3 just learn simple heuristics (e.g. low probability for questions with large integers)? Another possibility is that verbalized probability results are explained by GPT-3 learning simple heuristics for the difficulty of questions. For example, suppose GPT-3 simply learned to output lower probabilities for questions with larger integers (because they are more difficult). This would not lead to robust generalization, as some questions with small integers are difficult. We ran an experiment to test whether simple heuristics can generate calibrated probabilities. We trained a logistic regression model on the Add-subtract training set with the same target probabilities as in Section 3.1. The model has hand-crafted features that we know are predictive of difficulty for GPT-3: the number of digits of integers in the question, the operator (e.g. "+" or "round to nearest 10"), and the number format (e.g. "1000" or "1,000"). This heuristic model performed worse than verbalized probability on both the Multi-answer and Multiply-divide evaluation sets (Table 2). So the results for verbalized probability cannot be fully explained by these heuristics.</p>
<p>Evidence that GPT-3 uses latent (pre-existing) features of questions. So what does explain GPT3's ability to generalize calibration? There is tentative evidence that GPT-3 learns to use features of inputs that it already possessed before finetuning. We refer to these features as "latent" representations, because they are not "active" in pre-trained GPT-3 (which is poorly calibrated). This supports our claim that GPT-3 learns to express its own (pre-existing) uncertainty about answers and exhibits "honesty" (i.e. communicating its actual epistemic state in words).</p>
<p>Via OpenAI's Embeddings API (Neelakanta, 2022), we can extract an embedding for each question-answer pair in CalibratedMath using a GPT-3 model finetuned for semantic similarity. ${ }^{10}$ Figure 7 shows a (trained) projection of GPT-3's embeddings into two dimensions on the Multiply-divide evaluation set, where we see that samples are already reasonably well separated into correct and incorrect classes. Since a linear 2D projection is able to uncover this structure, we view this as evidence that the embedding already encoded features that were relevant to calibration.</p>
<p>The "Linear probe" row in Table 2 explores this further by attaching a linear probe to GPT-3's embeddings and predicting whether GPT-3's embedded answer was correct or incorrect. While performance is worse than the finetuned verbalized model, the probe still exhibits generalization to the Multiply-divide evaluation set, again indicating that GPT-3 learned relevant features during pre-training that are now present in the embedding.</p>
<p>Finally, from Section 3.3, GPT-3 is able to generalize its calibration on both evaluation sets after seeing only $k=50$ examples. Given the high number of tasks and difficulty levels in CalibratedMath, a context containing 50 examples can only cover a tiny fraction of the space of inputs. It would therefore be difficult to meta-learn new features that would generalize robustly to the evaluation sets.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: Linear projection of GPT-3 embeddings into two dimensions with colors denoting true (green) or false (blue). Each point is the embedding of an input pair of form (question, GPT-3 answer) from the Multiply-divide evaluation set that has been projected into 2D. A point is green if the GPT-3 answer is correct and blue otherwise. We see the classes become better separated as training progresses and after 5 epochs they are reasonably well separated by a linear boundary.</p>
<h1>4 Discussion</h1>
<h3>4.1 Directions for future work</h3>
<p>Our results show that GPT-3 has some ability to generalize (verbalized) calibration under distribution shift. However, while our training and evaluation sets differed significantly in the label distribution, the content and format of questions did not shift much. Future work could test whether calibration generalizes to other subject areas (e.g. history or biology) and to other formats (e.g. chat, long-form question answering, forecasting). It would also be valuable to test language models other than GPT-3, especially models that have a better grasp of probability before being finetuned. While we finetuned models using supervised learning, future work could explore the more flexible approach of reinforcement learning (Stiennon et al., 2020; Wu et al., 2021).</p>
<h2>5 Related work</h2>
<p>Calibration in new domains. Prior work on calibration focuses primarily on the classification setting, where models output a probability distribution over the set of possible classes (Guo et al., 2017; Mukhoti et al., 2020; Minderer et al., 2021), corresponding to what we call the "answer logit". To generalize calibration to a new target domain, methods often require samples from the target or from additional source domains (Gong et al., 2021; Csurka, 2017; Wang et al., 2021). We study how calibration generalizes when a pre-trained model is finetuned on a single source domain and must generalize zero-shot to a new domain.</p>
<p>Pre-trained language models. Hendrycks et al. (2020) analyze GPT-3's behavior on a benchmark of tasks that vary in both subject matter and difficulty, showing that GPT-3's calibration (for the answer logit) generalizes fairly poorly in both the zero-shot and few-shot settings. To improve the calibration of pre-trained language models, Desai \&amp; Durrett (2020) use label smoothing to reduce overconfidence on out-ofdomain data. Kong et al. (2020) introduce on- and off-manifold regularization to handle in-distribution and out-of-distribution calibration, respectively, but focus on OOD detection rather than generalization. Other work focuses on the closely related problem of teaching models to abstain from answering when a model has high uncertainty about its answer. Kamath et al. (2020) train an auxiliary "calibrator" to predict whether the primary model correctly answers any given question using a mix of in-domain and out-of-domain data. In cases where the calibrator predicts an error, the model can refuse to answer. Additional studies explore the use of manually crafted prompts that instruct models to defer or qualify their answers when uncertain (Askell et al., 2021b; Lin et al., 2021). These methods typically correct for models being overconfident on out-of-domain examples. In comparison, GPT-3's accuracy on our target domain is much higher than its accuracy on the source domain; its predictions therefore tend to be underconfident. The shift between target and source is also much larger, where we move from a single-answer to a multi-answer setting.</p>
<p>Natural language generation. In the specific case of natural language generation, Jiang et al. (2021) study calibration by framing multiple-choice and extractive QA as generative tasks, where a language model's uncertainty can be extracted from its logits over all tokens in an answer sequence. The authors introduce methods for both fine-tuning and post-hoc calibration of logits. To handle answers that can be worded in more than one way, a round-trip translation model is used to generate paraphrases for each answer, and the model's uncertainty is calculated as its total probability across all such paraphrases. While this approach leads to better calibration, it adds additional overhead and doesn't handle the situation where a question has multiple answers that can't be exhaustively listed.</p>
<p>Verbalized uncertainty. Branwen (2020) demonstrates GPT-3's ability to express verbalized uncertainty on simple trivia questions in the in-domain, few-shot setting, using an instructive prompt.</p>
<h1>Acknowledgments</h1>
<p>We thank William Saunders, Dan Hendrycks, Mark Xue, Jeff Wu, Paul Christiano, Daniel Ziegler, Collin Burns and Rai (Michael Pokorny) for helpful comments and discussions.</p>
<h2>References</h2>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment, 2021a.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment, 2021b. URL https://arxiv.org/abs/2112.00861.</p>
<p>Gwern Branwen. Gpt-3 nonfiction - calibration, 2020. https://www.gwern.net/GPT-3-nonfiction# calibration, Last accessed on 2022-04-24.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Paul Christiano. ARC's first technical report: Eliciting latent knowledge, 2021. https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/ arc-s-first-technical-report-eliciting-latent-knowledge, Last accessed on 2022-04-30.</p>
<p>Gabriela Csurka. Domain adaptation for visual applications: A comprehensive survey, 2017. URL https: //arxiv.org/abs/1702.05374.</p>
<p>Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 295-302, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.21. URL https://aclanthology.org/2020.emnlp-main. 21.</p>
<p>Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders. Truthful AI: Developing and governing AI that does not lie. arXiv preprint arXiv:2110.06674, 2021. URL https://arxiv.org/abs/2110.06674.</p>
<p>Yunye Gong, Xiao Lin, Yi Yao, Thomas G. Dietterich, Ajay Divakaran, and Melinda Gervasio. Confidence calibration for domain generalization under covariate shift. 2021. doi: 10.48550/ARXIV.2104.00742. URL https://arxiv.org/abs/2104.00742.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks, 2017 .</p>
<p>Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure, 2018. URL https://arxiv.org/abs/1812.04606.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering. Transactions of the Association for Computational Linguistics, 9:962-977, 09 2021. ISSN 2307-387X. doi: 10.1162/tacl_a_00407. URL https://doi.org/10.1162/tacl_a_00407.</p>
<p>Amita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5684-5696, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.503. URL https: //aclanthology.org/2020.acl-main.503.</p>
<p>Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael C Mozer, and Becca Roelofs. Soft calibration objectives for neural networks. arXiv preprint arXiv:2108.00106, 2021.</p>
<p>Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, and Chao Zhang. Calibrated language model fine-tuning for in- and out-of-distribution data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1326-1340, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.102. URL https://aclanthology. org/2020.emnlp-main. 102 .</p>
<p>Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. Advances in Neural Information Processing Systems, 28, 2015.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. URL https://arxiv.org/abs/2109.07958.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.</p>
<p>Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 15682-15694. Curran Associates, Inc., 2021. URL https: //proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf.</p>
<p>Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania. Calibrating deep neural networks using focal loss. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.</p>
<p>15288-15299. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/aeb7b30ef1d024a76f21a1d40e30c302-Paper.pdf.</p>
<p>Arvind Neelakanta. Introducing text and code embeddings in the openai api, 2022. https://openai.com/ blog/introducing-text-and-code-embeddings/, Last accessed on 2022-04-30.</p>
<p>Khanh Nguyen and Brendan O'Connor. Posterior calibration and exploratory analysis for natural language processing models. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1587-1598, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / \mathrm{D} 15-1182$. URL https://aclanthology.org/D15-1182.</p>
<p>Jeremy Nixon, Mike Dusenberry, Ghassen Jerfel, Timothy Nguyen, Jeremiah Liu, Linchuan Zhang, and Dustin Tran. Measuring calibration in deep learning, 2019. URL https://arxiv.org/abs/1904.01685.</p>
<p>OpenAI. Fine-tuning, 2021. https://beta.openai.com/docs/guides/fine-tuning/advanced-usage, Last accessed on 2022-04-30.</p>
<p>Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift. Curran Associates Inc., Red Hook, NY, USA, 2019.</p>
<p>Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm, 2021. URL https://arxiv.org/abs/2102.07350.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 4627-4635. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/628. URL https://doi.org/10. 24963/ijcai.2021/628. Survey Track.</p>
<p>Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback, 2021. URL https://arxiv.org/abs/2109.10862.</p>
<h1>A CalibratedMath</h1>
<p>Table 3: Breakdown of tasks in the CalibratedMath benchmark. '# Levels' refers to the count of difficulty levels within each operation, where the difficulty is determined by the number of digits in each operand and the formatting used for the numbers. Models are trained on tasks from the 'Add/Sub' group, then evaluated on either the 'Mult/Div' or the 'Multi[-answer]' group.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Group</th>
<th style="text-align: center;">Operation</th>
<th style="text-align: center;"># Levels</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Add/Sub</td>
<td style="text-align: center;">Addition</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Q: What is $14+27$ ? A: 41</td>
</tr>
<tr>
<td style="text-align: center;">Add/Sub</td>
<td style="text-align: center;">Subtraction</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Q: What is 109 - 3 ? A: 106</td>
</tr>
<tr>
<td style="text-align: center;">Mult/Div</td>
<td style="text-align: center;">Multiplication</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Q: What is 8 * 64 ? A: 512</td>
</tr>
<tr>
<td style="text-align: center;">Mult/Div</td>
<td style="text-align: center;">Division</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Q: What is 512 / 8 ? A: 64</td>
</tr>
<tr>
<td style="text-align: center;">Mult/Div</td>
<td style="text-align: center;">Floor division</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Q: What is 515 / 8 ? A: 64</td>
</tr>
<tr>
<td style="text-align: center;">Mult/Div</td>
<td style="text-align: center;">Modulo</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Q: What is 515 mod 8 ? A: 3</td>
</tr>
<tr>
<td style="text-align: center;">Mult/Div</td>
<td style="text-align: center;">Remainder</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Q: What is the remainder when 515 is divided by 8 ? A: 3</td>
</tr>
<tr>
<td style="text-align: center;">Mult/Div</td>
<td style="text-align: center;">Percentages</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Q: What is $25 \%$ of 1024 ? A: 256</td>
</tr>
<tr>
<td style="text-align: center;">Mult/Div</td>
<td style="text-align: center;">Fraction reduction</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Q: What is $15 / 24$ in reduced form? A: $5 / 8$</td>
</tr>
<tr>
<td style="text-align: center;">Add/Sub</td>
<td style="text-align: center;">Rounding</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Q: What is 10,248 rounded to the nearest 10 ? A: 10,250</td>
</tr>
<tr>
<td style="text-align: center;">Add/Sub</td>
<td style="text-align: center;">Arithmetic sequences</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Q: What comes next: $4,14,24,34 \ldots$ ? A: 44</td>
</tr>
<tr>
<td style="text-align: center;">Add/Sub</td>
<td style="text-align: center;">3 -step addition</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Q: What is $2+3+7$ ? A: 12</td>
</tr>
<tr>
<td style="text-align: center;">Mult/Div</td>
<td style="text-align: center;">3 -step multiplication</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Q: What is 2 * 3 * 7 ? A: 42</td>
</tr>
<tr>
<td style="text-align: center;">Add/Sub</td>
<td style="text-align: center;">Addition (alt)</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Q: What is 10 more than 23,298 ? A: 23,308</td>
</tr>
<tr>
<td style="text-align: center;">Add/Sub</td>
<td style="text-align: center;">Subtraction (alt)</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Q: What is 24 less than 96 ? A: 72</td>
</tr>
<tr>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Less than</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: Name any number smaller than 100 ? A: 37</td>
</tr>
<tr>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Greater than</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: Name any number larger than 100 ? A: 241</td>
</tr>
<tr>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Prime</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: Name any prime number smaller than 100 ? A: 7</td>
</tr>
<tr>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Square</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: Name any perfect square smaller than 100 ? A: 64</td>
</tr>
<tr>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Two-sum</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: Name two numbers that sum to 25 ? A: 11 and 14</td>
</tr>
<tr>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Multiple</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Q: Name a single multiple of 7 between 80 and 99 ? A: 91</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Distribution shift of GPT-3's zero-shot ability to answer arithmetic questions between training (Add-subtract) and evaluation sets (Multi-answer and Multiply-divide). For the training set "Add-subtract", we calculate the accuracy ( $\%$ of correct answers) across each task and level of difficulty (see Table 3) and display this as a histogram. We see that the most frequent accuracies are close to 0 (which are question types such that GPT-3 gets nearly all instances wrong). The same process is repeated for the evaluation sets (Multi-answer and Multiply-divide). We see that GPT-3 does even worse on Multiply-divide but does much better on Multi-answer. Thus to be well calibrated on the Multi-answer evaluation set, GPT-3 would need to use higher probabilities (on average) than on the training set.</p>
<h1>B Experimental setup</h1>
<h2>B. 1 Verbalized probability with words</h2>
<p>In one version of verbalized probability, models express uncertainty using words rather than numbers (see Figure 1 for an example). This leaves the question of which words to use for supervised finetuning. While we tried ordered categories (Confidence: "lowest", "low", "medium", "high", "highest"), we found that using random names without explicit orderings ("john", "sam", "matt", "dan", "tom") led to very slightly better performance. So we use these random names throughout.</p>
<h2>B. 2 Prompts</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">57368</span><span class="w"> </span><span class="n">rounded</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">nearest</span><span class="w"> </span><span class="mi">100</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">57</span><span class="o">,</span><span class="mi">400</span>
<span class="n">Confidence</span><span class="o">:</span><span class="w"> </span><span class="mi">19</span><span class="o">%</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="n">less</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="mi">58</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">51</span>
<span class="n">Confidence</span><span class="o">:</span><span class="w"> </span><span class="mi">44</span><span class="o">%</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">877</span><span class="o">+</span><span class="mi">47</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">924</span>
<span class="n">Confidence</span><span class="o">:</span><span class="w"> </span><span class="mi">59</span><span class="o">%</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">517</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">898</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mi">381</span>
<span class="n">Confidence</span><span class="o">:</span><span class="w"> </span><span class="mi">67</span><span class="o">%</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">247</span><span class="w"> </span><span class="n">less</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="mi">4895</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">2352</span>
<span class="n">Confidence</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="o">%</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">5</span><span class="o">*</span><span class="mi">145</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">725</span>
<span class="n">Confidence</span><span class="o">:</span>
</code></pre></div>

<p>Figure 9: Few-shot prompt. The example shows a 5 -shot prompt. The answers and target probabilities come from the estimation step described in Section 3. The prompt is randomized before every query.</p>
<h2>B. 3 Supervised fine-tuning</h2>
<p>The supervised fine-tuning dataset consists of approximately 10 k examples, where 100 examples are sampled from each sub-task in the training set. Models are trained for one epoch to prevent overfitting, using the default hyperparameters from OpenAI's fine-tuning API with learning_rate_multiplier $=0.1$ (OpenAI, 2021). We additionally carry out a form of early stopping that takes into account the difference between the sub-task level targets $\hat{p}_{T}$, and a model's binary accuracy of $0 / 1$ on any individual question.</p>
<p>Consider a sub-task $T$ from which we sample two questions, the first of which the model answers correctly. Then $\hat{p}<em T="T">{T}$ would equal 0.5 . If the model correctly gives uncertainties of 1 and 0 on the two samples, its per-sample MSE would be 0 . However, it would incur a loss against the target $\hat{p}</em>$ is a proxy for what the model's}$. Reducing this loss would lead to worse performance on the per-sample MSE. This happens because $\hat{p}_{T</p>
<p>uncertainty should be on any given question. As we continue to fit to $\hat{p}<em T="T">{T}$, we see that per-sample MSE flattens or increases on the training set, even though the loss against $\hat{p}</em>$ continues to decrease. We use this as a signal to stop training after around $n=2700$ examples. A comparison of calibration by the number of samples seen is shown in Figure 11 on the two evaluation sets, although we use the training set only to determine the stopping point.</p>
<h1>C Additional results</h1>
<h2>C. 1 Verbalized calibration curves by number of training samples</h2>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 10: Calibration curves by number of training examples. We train the model to produce verbalized probabilities (numbers) on the Add-subtract training set. Curves show calibration performance for the Multiply-divide (top) and Multi-answer (bottom) evaluation sets using Expected Value decoding over output tokens (rather than greedy decoding). Beyond around $n=2700$, continuing to train does not improve generalization.</p>
<h1>C. 2 Comparing results using greedy and EV uncertainties</h1>
<p>By verbally expressing uncertainty using a number (e.g. "Confidence: $84 \%$ "), models can cover a wide range of probability values even if greedy decoding is used. In comparison, expressing uncertainty using words limits models to five categories in our setup, corresponding to the discrete confidence scores $[10 \%, 30 \%, 50 \%$, $70 \%, 90 \%]$. Taking an expected value (EV) over output tokens allows models to give intermediate scores (e.g. $0.5 \times$ "High" $(70 \%)+0.5 \times$ "Medium" $(50 \%)=60 \%$ confidence). The difference between greedy and EV uncertainties is more pronounced when the number of finetuning or $k$-shot examples is low.</p>
<p>Table 4: Performance of finetuned models using greedy and EV uncertainties.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setup</th>
<th style="text-align: center;">Multi-answer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multiply-divide</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAD</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAD</td>
</tr>
<tr>
<td style="text-align: center;">Verbalized numbers (greedy)</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">19.0</td>
</tr>
<tr>
<td style="text-align: center;">Verbalized numbers (EV)</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">18.9</td>
</tr>
<tr>
<td style="text-align: center;">Verbalized words (greedy)</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">10.6</td>
</tr>
<tr>
<td style="text-align: center;">Verbalized words (EV)</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">13.3</td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 11: Calibration curves using greedy and EV uncertainties.</p>
<h1>C. 3 Changing the training set from Add-subtract to Multiply-divide</h1>
<p>Table 5: Calibration performance of models with a different training set. In contrast to the results in the main text (where models are trained on Add-subtract), here we train models on the Multiply-divide set and we evaluate on both Add-subtract and Multi-answer. We find that calibration on the Multi-answer evaluation set is worse than when training on Add-subtract. One reason is that there is a bigger shift in the "label distribution" from training to evaluation. GPT-3's answers are less accurate on Multiply-divide and so probabilities above $50 \%$ are barely represented in the training set but make up most tasks in Multianswer. The label distributions (i.e. distribution of accuracy for GPT-3 on the arithmetic tasks) are shown in Figure 8.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setup</th>
<th style="text-align: left;">Add-subtract</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Multi-answer</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MSE</td>
<td style="text-align: left;">MAD</td>
<td style="text-align: left;">MSE</td>
<td style="text-align: left;">MAD</td>
</tr>
<tr>
<td style="text-align: left;">Verbalized numbers (finetune)</td>
<td style="text-align: left;">17.0</td>
<td style="text-align: left;">9.9</td>
<td style="text-align: left;">36.3</td>
<td style="text-align: left;">40.7</td>
</tr>
<tr>
<td style="text-align: left;">Verbalized words (finetune)</td>
<td style="text-align: left;">16.4</td>
<td style="text-align: left;">$\mathbf{6 . 8}$</td>
<td style="text-align: left;">$\mathbf{3 0 . 5}$</td>
<td style="text-align: left;">$\mathbf{3 0 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Answer logit (zero-shot)</td>
<td style="text-align: left;">$\mathbf{1 5 . 5}$</td>
<td style="text-align: left;">14.3</td>
<td style="text-align: left;">37.4</td>
<td style="text-align: left;">33.7</td>
</tr>
<tr>
<td style="text-align: left;">Indirect logit (finetune)</td>
<td style="text-align: left;">17.3</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">43.9</td>
<td style="text-align: left;">49.9</td>
</tr>
<tr>
<td style="text-align: left;">Constant baseline</td>
<td style="text-align: left;">20.1</td>
<td style="text-align: left;">8.5</td>
<td style="text-align: left;">40.1</td>
<td style="text-align: left;">39.5</td>
</tr>
</tbody>
</table>
<h2>C. 4 Correlations between probability types</h2>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 12: Correlation between verbalized probability and logit setups. Using the Multiply-divide evaluation set, we calculate each setup's MSE on each task and difficulty level, then plot the results. The colorbar shows GPT-3's accuracy on the arithmetic questions. While correlation between the two verbalized uncertainty types - expressing uncertainty either in numbers (e.g. $45 \%$ ) or words ("Confidence: Low") is high, correlation to the other two types is moderate. This provides more evidence that the finetuned verbalized model isn't simply reproducing the answer logit.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ While the embeddings come from a finetuned GPT-3 model, we expect the results would be similar if embeddings came from the pre-trained model.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4817 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4817</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4817</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-259144942</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.07934v1.pdf" target="_blank">BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information</a></p>
                <p><strong>Paper Abstract:</strong> Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4817.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4817.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoardgameQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic natural-language dataset of board-game style scenarios that require multi-hop defeasible reasoning with contradictory rules and incomplete background knowledge; includes rule types that require spatial reasoning such as 3D volume/fit checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BoardgameQA board-game reasoning tasks (includes Volume/fitting subproblems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Synthetic board-game scenarios expressed in natural language where questions ask whether a proposition is proved/disproved/unknown given facts, rules, and pairwise preferences; includes a 'Volume' category that requires spatial reasoning about 3D objects (e.g., whether a ball/notebook fits into a box given dimensions) and other spatial-like checks (places/cities → countries).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Not a model; dataset is designed to test mechanisms including finetuning to predict labels, generating explicit proofs (chain-of-thought style), few-shot CoT prompting, soft prompt-tuning, and instruction-finetuning. It controls conflict type (Type1/Type2), depth (hop count), missing information, and distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Dataset includes an explicit 'Volume' incomplete-information category requiring the model to compute geometric containment (e.g., determine if a ball of given radius fits inside a box with given dimensions). Several failure examples in the paper show models attempting and failing these checks (see examples where a model mis-evaluates whether a ball of radius 29 inches fits in a 26.3x25.6x24.2 in box).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>NA (dataset description). The dataset is intended for classification accuracy (labels: proved/disproved/unknown) and proof-level metrics (Rule F1, Conflict F1, manual Overall Proof Accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dataset limited to binary contradictions and modus ponens style rules; spatial checks are synthetic and limited to containment/volume/fit and unit conversions; small prompt assumption (all facts and rules fit in prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Majority-class baseline ~33% (3-way balanced labels); comparisons in paper among multiple LMs (see following entries).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4817.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4817.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-540B (few-shot, CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 540B (few-shot in-context learning with chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large decoder-only language model (PaLM) with ~540B parameters used in few-shot settings with chain-of-thought demonstrations to attempt generating proofs and labels for BoardgameQA examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (PaLM family) with ~540 billion parameters (as stated in the paper); used in few-shot in-context learning with one demonstration per label due to long examples; some experiments use the instruction-finetuned variant FLAN-PaLM 540B.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BoardgameQA (includes Volume/fit spatial subproblems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Natural-language board-game reasoning problems requiring multi-hop defeasible reasoning and sometimes spatial computations (e.g., fit-in-box volume checks, unit conversions, city→country membership).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Few-shot in-context learning with chain-of-thought (CoT) proofs provided as demonstrations in the prompt; the model outputs a proof chain and a final label. Also used instruction-finetuned variant (FLAN-PaLM 540B) with CoT in few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative examples show PaLM-540B attempting spatial reasoning steps (e.g., reasoning about ball radii and box dimensions) but failing in several cases: mis-classifying object types (football vs basketball) and incorrectly concluding fit relations. The paper reports these concrete failure cases as evidence that the model attempted but did not reliably perform geometric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in the paper as classification accuracy plots (figures) and proof-level metrics; exact numeric accuracies are presented in figures but not enumerated in text. Qualitative trend: PaLM-540B few-shot performs poorly across depths 1–3 relative to tuned models, though it outperforms PaLM-62B few-shot (paper states PaLM-540B performs better than PaLM-62B). Proof metrics: few-shot models perform poorly on conflict resolution (low Conflict F1) and produce many incorrect proofs even when the label is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to reliably resolve preferences/conflicts in few-shot mode, hallucinates or misapplies rules, mistakes object types (football vs basketball), and fails numeric/spatial checks (fit-in-box). Few-shot setup used only 3 demonstrations because of long example length, limiting in-context learning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to prompt-tuned PaLM-62B and finetuned T5/BERT: few-shot PaLM-540B shows better capacity than PaLM-62B but worse faithfulness on proofs than tuned models; instruction-finetuned FLAN-PaLM 540B still performs poorly out-of-the-box on defeasible conflict resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4817.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4817.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-62B (prompt-tuned w/ CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 62B (prompt-tuned with chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medium-large decoder-only PaLM model (~62B parameters) used with soft prompt-tuning and chain-of-thought demonstration format to learn to generate proofs and labels on BoardgameQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 62B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (PaLM family) with ~62 billion parameters; in experiments the base LM parameters are frozen and a 100-token soft prompt is learned on training data (prompt-tuning) with CoT-style outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BoardgameQA (Volume/spatial subproblems included)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>BoardgameQA examples that sometimes require spatial computations (3D containment), unit conversions, lexical entailment and world-knowledge linking.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Prompt-tuning: a learnable soft prompt embedding (size 100) is trained while keeping LM parameters frozen; training objective is to have model generate chain-of-thought style proofs and the label.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Prompt-tuned PaLM-62B sometimes performs better at proof faithfulness (higher proof-level metrics relative to few-shot), suggesting it internalizes rules including some incomplete-knowledge reasoning such as volume checks, but the paper includes failure examples where prompt-tuned PaLM-62B fills missing info incorrectly (spatial or non-spatial).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively and in plots: prompt-tuned PaLM-62B achieves reasonable performance at low depths and maintains performance better than few-shot baselines as amount of missing world-knowledge increases; exact numeric accuracies are shown in figures (not enumerated in text). Proof-level metrics (Rule F1 and Conflict F1) are higher for tuned models than for few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still fails on harder depths and on examples with many conflicts or distractors; specific errors include incorrectly filling missing information, misunderstanding antecedents, and failing conjunction proofs involving spatial checks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms few-shot PaLM-62B and PaLM-540B on some proof metrics; performs worse than larger tuned models in some settings. Compared to finetuned T5 and BERT, prompt-tuning shows different tradeoffs (better proof faithfulness than few-shot, but overall accuracy varies by depth and conflict level).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4817.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4817.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-XXL (finetuned w/ proofs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 1.1 XXL (finetuned to generate proofs then labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large encoder-decoder T5 model (1.1 XXL) finetuned to output full proofs (chain-of-thought style) followed by a final label for BoardgameQA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 1.1 XXL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer (T5 family) '1.1 XXL' variant (very large) finetuned on BoardgameQA to generate explicit proof chains and then output the classification label; training hyperparameters reported (batch size 8, learning rate 0.001, up to 50K epochs) in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BoardgameQA (includes Volume spatial subproblems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Natural-language board-game reasoning tasks with potential spatial computations and incomplete knowledge requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Finetuning to generate full textual proofs (intermediate reasoning steps) before the final label; explicit supervision on proof tokens is used (finetuning with proofs).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>T5 is trained to generate the proof steps, which include spatial computations when the example requires it (e.g., converting units or checking geometric fit). The paper provides failure examples where T5 hallucinates spatial facts or derives wrong spatial conclusions, indicating T5 attempts these computations but does not always succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as classification accuracy per dataset variation and depth (figures); finetuned T5 performs reasonably at depth 1 but accuracy degrades at depths 2–3; on proof-level metrics, tuned models (including T5) select correct rules at reasonable rates (Rule F1), but generate incorrect proofs relatively often (manual Overall Proof Accuracy lower than label accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Hallucination of facts/rules in proofs, incorrect handling of missing world-knowledge (e.g., unit conversion or volume checks), and decreased performance with higher depth/conflict/distractor counts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to BERT (classification) and PaLM variants: finetuned T5 with proofs yields better interpretability (proofs) and sometimes higher label accuracy than few-shot approaches, but still struggles with defeasible/conflicting inputs relative to monotonic reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4817.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4817.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-large (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-large (finetuned classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-only transformer (BERT-large) finetuned with a classification head to directly predict proved/disproved/unknown labels on BoardgameQA without explicit proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer pretrained BERT-large finetuned with a classification head (batch size 16, learning rate 4.6e-5, 20 epochs as reported) to directly predict labels from the textual theory and question.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BoardgameQA (includes spatial Volume checks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>BoardgameQA classification problems; some examples require spatial reasoning (volume/fit), but BERT's setup does not require generating proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Direct finetuning as a classifier (no explicit proof generation).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Because BERT is trained only to output labels (no proofs), evidence of explicit spatial reasoning is limited; the paper reports BERT performs above-random on NoConflict and LowConflict cases but drops to near-random on Medium/High conflict cases and when missing information (spatial checks) is required, indicating limited ability to perform the spatial computations needed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in figures: BERT-large achieves reasonable accuracy on depth 1 NoConflict/LowConflict but drops near-random for higher conflict levels and higher depths. Exact numeric values are shown in paper figures (not enumerated in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performs poorly when additional world knowledge or explicit spatial computation is required (e.g., volume containment, unit conversion); cannot produce proofs to inspect reasoning and tends to fail for Type2 conflicts that require showing an antecedent cannot be proved.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to T5 finetuned with proofs and prompt-tuned PaLM: BERT performs worse on deeper, conflicting, and incomplete-information examples; finetuned proof-generating models achieve better interpretability and often better performance on structured reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Lambada: Backward chaining for automated reasoning in natural language <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 1)</em></li>
                <li>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4817",
    "paper_id": "paper-259144942",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "BoardgameQA",
            "name_full": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
            "brief_description": "A synthetic natural-language dataset of board-game style scenarios that require multi-hop defeasible reasoning with contradictory rules and incomplete background knowledge; includes rule types that require spatial reasoning such as 3D volume/fit checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "puzzle_name": "BoardgameQA board-game reasoning tasks (includes Volume/fitting subproblems)",
            "puzzle_description": "Synthetic board-game scenarios expressed in natural language where questions ask whether a proposition is proved/disproved/unknown given facts, rules, and pairwise preferences; includes a 'Volume' category that requires spatial reasoning about 3D objects (e.g., whether a ball/notebook fits into a box given dimensions) and other spatial-like checks (places/cities → countries).",
            "mechanism_or_strategy": "Not a model; dataset is designed to test mechanisms including finetuning to predict labels, generating explicit proofs (chain-of-thought style), few-shot CoT prompting, soft prompt-tuning, and instruction-finetuning. It controls conflict type (Type1/Type2), depth (hop count), missing information, and distractors.",
            "evidence_of_spatial_reasoning": "Dataset includes an explicit 'Volume' incomplete-information category requiring the model to compute geometric containment (e.g., determine if a ball of given radius fits inside a box with given dimensions). Several failure examples in the paper show models attempting and failing these checks (see examples where a model mis-evaluates whether a ball of radius 29 inches fits in a 26.3x25.6x24.2 in box).",
            "performance_metrics": "NA (dataset description). The dataset is intended for classification accuracy (labels: proved/disproved/unknown) and proof-level metrics (Rule F1, Conflict F1, manual Overall Proof Accuracy).",
            "limitations_or_failure_cases": "Dataset limited to binary contradictions and modus ponens style rules; spatial checks are synthetic and limited to containment/volume/fit and unit conversions; small prompt assumption (all facts and rules fit in prompt).",
            "comparison_baseline": "Majority-class baseline ~33% (3-way balanced labels); comparisons in paper among multiple LMs (see following entries).",
            "uuid": "e4817.0",
            "source_info": {
                "paper_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "PaLM-540B (few-shot, CoT)",
            "name_full": "PaLM 540B (few-shot in-context learning with chain-of-thought)",
            "brief_description": "A large decoder-only language model (PaLM) with ~540B parameters used in few-shot settings with chain-of-thought demonstrations to attempt generating proofs and labels for BoardgameQA examples.",
            "citation_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
            "mention_or_use": "use",
            "model_name": "PaLM 540B",
            "model_description": "Decoder-only transformer (PaLM family) with ~540 billion parameters (as stated in the paper); used in few-shot in-context learning with one demonstration per label due to long examples; some experiments use the instruction-finetuned variant FLAN-PaLM 540B.",
            "puzzle_name": "BoardgameQA (includes Volume/fit spatial subproblems)",
            "puzzle_description": "Natural-language board-game reasoning problems requiring multi-hop defeasible reasoning and sometimes spatial computations (e.g., fit-in-box volume checks, unit conversions, city→country membership).",
            "mechanism_or_strategy": "Few-shot in-context learning with chain-of-thought (CoT) proofs provided as demonstrations in the prompt; the model outputs a proof chain and a final label. Also used instruction-finetuned variant (FLAN-PaLM 540B) with CoT in few-shot.",
            "evidence_of_spatial_reasoning": "Qualitative examples show PaLM-540B attempting spatial reasoning steps (e.g., reasoning about ball radii and box dimensions) but failing in several cases: mis-classifying object types (football vs basketball) and incorrectly concluding fit relations. The paper reports these concrete failure cases as evidence that the model attempted but did not reliably perform geometric reasoning.",
            "performance_metrics": "Reported in the paper as classification accuracy plots (figures) and proof-level metrics; exact numeric accuracies are presented in figures but not enumerated in text. Qualitative trend: PaLM-540B few-shot performs poorly across depths 1–3 relative to tuned models, though it outperforms PaLM-62B few-shot (paper states PaLM-540B performs better than PaLM-62B). Proof metrics: few-shot models perform poorly on conflict resolution (low Conflict F1) and produce many incorrect proofs even when the label is correct.",
            "limitations_or_failure_cases": "Fails to reliably resolve preferences/conflicts in few-shot mode, hallucinates or misapplies rules, mistakes object types (football vs basketball), and fails numeric/spatial checks (fit-in-box). Few-shot setup used only 3 demonstrations because of long example length, limiting in-context learning capacity.",
            "comparison_baseline": "Compared to prompt-tuned PaLM-62B and finetuned T5/BERT: few-shot PaLM-540B shows better capacity than PaLM-62B but worse faithfulness on proofs than tuned models; instruction-finetuned FLAN-PaLM 540B still performs poorly out-of-the-box on defeasible conflict resolution.",
            "uuid": "e4817.1",
            "source_info": {
                "paper_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "PaLM-62B (prompt-tuned w/ CoT)",
            "name_full": "PaLM 62B (prompt-tuned with chain-of-thought)",
            "brief_description": "A medium-large decoder-only PaLM model (~62B parameters) used with soft prompt-tuning and chain-of-thought demonstration format to learn to generate proofs and labels on BoardgameQA.",
            "citation_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
            "mention_or_use": "use",
            "model_name": "PaLM 62B",
            "model_description": "Decoder-only transformer (PaLM family) with ~62 billion parameters; in experiments the base LM parameters are frozen and a 100-token soft prompt is learned on training data (prompt-tuning) with CoT-style outputs.",
            "puzzle_name": "BoardgameQA (Volume/spatial subproblems included)",
            "puzzle_description": "BoardgameQA examples that sometimes require spatial computations (3D containment), unit conversions, lexical entailment and world-knowledge linking.",
            "mechanism_or_strategy": "Prompt-tuning: a learnable soft prompt embedding (size 100) is trained while keeping LM parameters frozen; training objective is to have model generate chain-of-thought style proofs and the label.",
            "evidence_of_spatial_reasoning": "Prompt-tuned PaLM-62B sometimes performs better at proof faithfulness (higher proof-level metrics relative to few-shot), suggesting it internalizes rules including some incomplete-knowledge reasoning such as volume checks, but the paper includes failure examples where prompt-tuned PaLM-62B fills missing info incorrectly (spatial or non-spatial).",
            "performance_metrics": "Reported qualitatively and in plots: prompt-tuned PaLM-62B achieves reasonable performance at low depths and maintains performance better than few-shot baselines as amount of missing world-knowledge increases; exact numeric accuracies are shown in figures (not enumerated in text). Proof-level metrics (Rule F1 and Conflict F1) are higher for tuned models than for few-shot.",
            "limitations_or_failure_cases": "Still fails on harder depths and on examples with many conflicts or distractors; specific errors include incorrectly filling missing information, misunderstanding antecedents, and failing conjunction proofs involving spatial checks.",
            "comparison_baseline": "Outperforms few-shot PaLM-62B and PaLM-540B on some proof metrics; performs worse than larger tuned models in some settings. Compared to finetuned T5 and BERT, prompt-tuning shows different tradeoffs (better proof faithfulness than few-shot, but overall accuracy varies by depth and conflict level).",
            "uuid": "e4817.2",
            "source_info": {
                "paper_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "T5-XXL (finetuned w/ proofs)",
            "name_full": "T5 1.1 XXL (finetuned to generate proofs then labels)",
            "brief_description": "A large encoder-decoder T5 model (1.1 XXL) finetuned to output full proofs (chain-of-thought style) followed by a final label for BoardgameQA tasks.",
            "citation_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
            "mention_or_use": "use",
            "model_name": "T5 1.1 XXL",
            "model_description": "Encoder-decoder transformer (T5 family) '1.1 XXL' variant (very large) finetuned on BoardgameQA to generate explicit proof chains and then output the classification label; training hyperparameters reported (batch size 8, learning rate 0.001, up to 50K epochs) in the paper.",
            "puzzle_name": "BoardgameQA (includes Volume spatial subproblems)",
            "puzzle_description": "Natural-language board-game reasoning tasks with potential spatial computations and incomplete knowledge requirements.",
            "mechanism_or_strategy": "Finetuning to generate full textual proofs (intermediate reasoning steps) before the final label; explicit supervision on proof tokens is used (finetuning with proofs).",
            "evidence_of_spatial_reasoning": "T5 is trained to generate the proof steps, which include spatial computations when the example requires it (e.g., converting units or checking geometric fit). The paper provides failure examples where T5 hallucinates spatial facts or derives wrong spatial conclusions, indicating T5 attempts these computations but does not always succeed.",
            "performance_metrics": "Reported as classification accuracy per dataset variation and depth (figures); finetuned T5 performs reasonably at depth 1 but accuracy degrades at depths 2–3; on proof-level metrics, tuned models (including T5) select correct rules at reasonable rates (Rule F1), but generate incorrect proofs relatively often (manual Overall Proof Accuracy lower than label accuracy).",
            "limitations_or_failure_cases": "Hallucination of facts/rules in proofs, incorrect handling of missing world-knowledge (e.g., unit conversion or volume checks), and decreased performance with higher depth/conflict/distractor counts.",
            "comparison_baseline": "Compared to BERT (classification) and PaLM variants: finetuned T5 with proofs yields better interpretability (proofs) and sometimes higher label accuracy than few-shot approaches, but still struggles with defeasible/conflicting inputs relative to monotonic reasoning tasks.",
            "uuid": "e4817.3",
            "source_info": {
                "paper_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "BERT-large (finetuned)",
            "name_full": "BERT-large (finetuned classifier)",
            "brief_description": "Encoder-only transformer (BERT-large) finetuned with a classification head to directly predict proved/disproved/unknown labels on BoardgameQA without explicit proof generation.",
            "citation_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
            "mention_or_use": "use",
            "model_name": "BERT-large",
            "model_description": "Encoder-only transformer pretrained BERT-large finetuned with a classification head (batch size 16, learning rate 4.6e-5, 20 epochs as reported) to directly predict labels from the textual theory and question.",
            "puzzle_name": "BoardgameQA (includes spatial Volume checks)",
            "puzzle_description": "BoardgameQA classification problems; some examples require spatial reasoning (volume/fit), but BERT's setup does not require generating proofs.",
            "mechanism_or_strategy": "Direct finetuning as a classifier (no explicit proof generation).",
            "evidence_of_spatial_reasoning": "Because BERT is trained only to output labels (no proofs), evidence of explicit spatial reasoning is limited; the paper reports BERT performs above-random on NoConflict and LowConflict cases but drops to near-random on Medium/High conflict cases and when missing information (spatial checks) is required, indicating limited ability to perform the spatial computations needed.",
            "performance_metrics": "Reported in figures: BERT-large achieves reasonable accuracy on depth 1 NoConflict/LowConflict but drops near-random for higher conflict levels and higher depths. Exact numeric values are shown in paper figures (not enumerated in-text).",
            "limitations_or_failure_cases": "Performs poorly when additional world knowledge or explicit spatial computation is required (e.g., volume containment, unit conversion); cannot produce proofs to inspect reasoning and tends to fail for Type2 conflicts that require showing an antecedent cannot be proved.",
            "comparison_baseline": "Compared to T5 finetuned with proofs and prompt-tuned PaLM: BERT performs worse on deeper, conflicting, and incomplete-information examples; finetuned proof-generating models achieve better interpretability and often better performance on structured reasoning.",
            "uuid": "e4817.4",
            "source_info": {
                "paper_title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Lambada: Backward chaining for automated reasoning in natural language",
            "rating": 2,
            "sanitized_title": "lambada_backward_chaining_for_automated_reasoning_in_natural_language"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 1,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge",
            "rating": 1,
            "sanitized_title": "leapofthought_teaching_pretrained_models_to_systematically_reason_over_implicit_knowledge"
        }
    ],
    "cost": 0.015487749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information
13 Jun 2023</p>
<p>Mehran Kazemi mehrankazemi@google.com 
Quan Yuan yquan@google.com 
Deepti Bhatia bhatiad@google.com 
Najoung Kim njkim@google.com 
Xin Xu 
Vaiva Imbrasaite vimbrasaite@google.com 
Deepak Ramachandran ramachandrand@google.com 
BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information
13 Jun 202376A39B27CD1B3BDAC8C1B878C8D37B65arXiv:2306.07934v1[cs.CL]
Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems.Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning.However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason.When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise.One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference.In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting.BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications.We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs.While performance can be improved with finetuning, it nevertheless remains poor.Preprint.Under review.</p>
<p>Introduction</p>
<p>A fundamental goal of AI since its early days has been automatically applying logical or deductive reasoning to draw new conclusions from existing knowledge [28,20].Since a large amount of knowledge is available in the form of natural language, tremendous effort has been put into developing models that can understand and reason over natural language [22,40,52,30,12] (see [33] for a survey).Recent years have seen substantial improvements in this direction thanks to advancements in pretrained language models (LMs) [8,9] that can handle unstructured data more flexibly, combined with advanced prompting techniques [50,29], and modular reasoning approaches [22,12].</p>
<p>Existing work in automated reasoning in natural language usually assumes that the provided knowledge is consistent and reliable.But in many applications, the collection of information one has to reason with is inconsistent and contradictory.This is the case, for instance, when reasoning is performed with information found in different online sources or social media (e.g., retrievalaugmented LMs [17,3]).When input sources are contradictory, one can consider various strategies to resolve the contradictions.One simple and practical formulation, which we adopt in this work, is to resolve the conflicts based on preferences over the information sources: when a con-flict arises, the information from the source with a higher preference should be used to solve the reasoning problem.Depending on the application, preferences can be assigned based on different criteria, e.g., based on the credibility of websites or social media users, or based on the recency of the information with newer information being preferred over older information.Exceptions to generics can also be expressed as preferences; for example, generic knowledge such as "birds fly" (see also [6]) should be overridden by exceptions such as "penguins are birds but do not fly" (see also [1]) when reasoning about penguins.Figure 1 demonstrates an example of a reasoning problem with conflicting information, where the conflict is resolved based on recency.</p>
<p>All travelers entering Y from X need to show negative covid19 test results.</p>
<p>Facts: Fiona wants to travel from country X to country Y and stay for 5 days.Does Fiona need to book a covid19 test appointment?</p>
<p>Travelers visiting Y for less than a month do not require covid19 tests anymore.</p>
<p>Info.Source 1 (date: 2022-04-20)</p>
<p>Info.Source 2 (date: 2022-04-27)</p>
<p>Reasoning: Fiona is traveling from X to Y and wants to stay for less than a month so both sources apply.The conclusions from the two sources conflict, but source 2 may be preferred since it is more recent.So Fiona doesn't need a covid test, and so does not need to book an appointment.</p>
<p>Figure 1: A reasoning problem with contradictory information (conflict resolved based on recency).</p>
<p>Reasoning with conflicting information guided by preferences can be formulated as a form of the classical defeasible reasoning problem [31,19,27].In this work, we study the reasoning ability of LMs in this setting.Toward this goal, we create a synthetic dataset where each example contains a defeasible theory (a set of input facts, possibly contradictory rules, and preferences over the rules), and a question about that theory.Answering the questions in the dataset requires multi-hop reasoning and conflict resolution over the input theory.The difficulty level (e.g., the depth, amount and type of conflicts, etc.) of the examples in the dataset can be controlled automatically, enabling targeted comparisons of various aspects of reasoning.</p>
<p>We also note that while a large number of logical reasoning benchmarks provide all the knowledge needed to answer questions [46,39,40,18], such benchmarks do not reflect common real-world scenarios where implicit background knowledge plays an important role in reasoning.Moreover, models that translate the textual examples into logical form and then leverage off-the-shelf solvers may excel on these datasets, which does not reflect the true performance of such models in real-world applications.For these reasons, in BoardgameQA only part of the knowledge required to solve the problem is provided as input to the LM; the missing knowledge has to come from the LM itself.</p>
<p>The problems in our dataset are formulated as scenarios of a board game, hence we name it BoardgameQA 1 .A board game theme allows us to create synthetic scenarios with complex defeasible rules to reason about that seem natural when stated in text and hence allows background commonsense world knowledge to also be used.To the best of our knowledge, BoardgameQA is the first dataset for multi-hop reasoning with contradictory inputs.Figure 2 shows a sample example from the dataset where the conflict resolution and missing knowledge have been highlighted.</p>
<p>We benchmark various LMs on BoardgameQA and measure their defeasible reasoning capacity.Most notably, our results reveal that LMs perform poorly when reasoning with conflicting sources, especially in the few-shot setting (compared to the finetuning setting) suggesting that preference understanding and defeasible reasoning capacities do not surface out-of-the-box in pretrained LMs.Secondly, we find that smaller LMs perform poorly when not all of the required information is provided as input.These results highlight a critical gap in the reasoning capacity of current LMs, considering that reasoning over contradicting and incomplete sets of information is a common scenario in many applications, and is key for developing robust AI systems.</p>
<p>Related Work</p>
<p>Our work spans three dimensions: 1-text-based logical reasoning, 2-reasoning with conflicting sources, and 3-reasoning with incomplete information.In the following section, we briefly summarize the literature on each of these axes that relate to our work.</p>
<p>Text-based logical reasoning approaches: Earlier works on natural language logical reasoning have finetuned LMs to directly provide answers to logical reasoning questions [11,4,38,18].Later work showed that explicitly generating the entire proof leads to substantial improvements both in Rules: R1: If the frog has more money than the lion and the dog combined, then the frog builds a power plant close to the green fields of the cat.R2: If something attacks the green fields of the cat, then it does not build a power plant near the green fields of the cat.[...]
Preferences: R2 is preferred over R1. [...]
Question: Does the frog build a power plant near the green fields of the cat?</p>
<p>Proof: We know the frog has $81, 81 is more than 3+48=51 which is the total money of the dog and lion combined, and according to R1 [...], and for the conflicting and higher priority rule R2 we cannot prove the antecedent "the frog attacks the green fields of the cat", so we can conclude "the frog builds a power plant close to the green fields of the cat".So the statement "the frog builds a power plant near the green fields of the cat" is proved and the answer is "yes".the case of finetuning and in the case of few-shot learning [29,13,55,56].In addition, modular reasoning approaches where the LM is used as a tool within a reasoning algorithm [22,12,49,23] have been shown to achieve both performance gains and more precise intermediate proof chains.In this paper, we experiment with four types of approaches: 1-finetuning without explicit reasoning steps, 2-finetuning with explicit reasoning steps, 3-prompt-tuning with chain-of-thought (CoT) prompting [50], and 4-few-shot in-context learning with CoT.</p>
<p>Text-based logical reasoning datasets: Many datasets have been created to measure the logical reasoning ability of NLP models [46,40,58,18,43].In Table 1, we provide a comparison of (a subset of) these datasets along three desired features in this work.All datasets compared contain only facts and rules that are non-contradicting.The dataset closest to our work is the ConditionalQA dataset [45] where the answer to the questions follows a "If X then yes, if Y then no" format.</p>
<p>Reasoning with conflicts: From the early days of AI, reasoning with conflicting information has been an important topic and many approaches have been developed to handle such conflicts [32,31,35].</p>
<p>The problem we study in this paper is an instance of defeasible reasoning [31,19,27] which has applications in various domains (especially in legal reasoning) [41,16,7] and has been argued to be one of the most important future directions in a recent survey of LM reasoning literature [54].In defeasible reasoning, there are preferences over the rules and in the case of conflict between two rules, the conclusion from the higher preference rule is accepted.Previous work on defeasible reasoning with natural language has studied the problem of adjusting the probability of a conclusion based on new (single-hop) evidence [37,26].Our work extends this line of work by developing a dataset for multi-hop defeasible reasoning with preferences over sources.</p>
<p>Reasoning with incomplete information: Several existing reasoning benchmarks adopt a setup where part of the required information is missing and needs to come from the model itself [44,5,2,48].Some datasets also employ a setup in which none of the required rules are provided as input [47,15,43,21].Our work focuses mainly on cases where part of the knowledge needs to come from the model and another part of the knowledge is provided as input.</p>
<p>Background and Notation</p>
<p>We let E = {e 1 , . . ., e N } and P = {p 1 , . . ., p M } represent a set of entities and predicates.We represent a fact in the logical form using the triple notation (e i , p j , e k ), where e i , e k ∈ E and p j ∈ P, and a rule as r : r b → r h where r b represents the body of the rule and r h represents the head.We use ! to indicate negation.A monotonic theory T = (F, R) is a tuple containing a set F of (positive or negative) facts, and a set R = {r 1 , . . ., r |R| } of rules.We let T ⊨ f represent that the fact f can be derived from the theory T using the standard inference rules of logic (See [42]).For a monotonic theory T = (F, R), if T ⊨ f , then for any theory T ′ such that T ′ = (F ∪ F ′ , R), we also have T ′ ⊨ f (that is, adding new facts does not change previously derived facts).</p>
<p>Defeasible Theory: A defeasible theory T (d) = (F, R, O) is a triple containing a set F of facts, a set R = {r 1 , . . ., r |R| } of rules, and a set O = {r t1 &gt; r t2 , . . ., r t3 &gt; r t4 } of pair-wise relative priorities/preferences between rules. 2 The rules hold defeasibly, meaning the conclusion from a rule may be defeated by contrary evidence from a higher priority rule.This happens, for example, when one rule implies something is true but another rule with a higher priority implies it is false; in
Contradictory Information ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ∼ ✓ Incomplete Information ✗ ∼ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ Auto. Diff. Control ✓ ✓ ✗ ✓ ✓ ✗ ✗ ∼ ✗ ✓
Table 1: A comparison of BoardgameQA with some of the widely-used logical reasoning datasets (bAbI 15 [51], CLUTRR [43], FOLIO [18], ProofWriter [46], PrOntoQA-OOD [40], AR-LSAT [58], ENWN [44], leap-of-thought [48], and ConditionalQA [45]) in terms of three key features.We use ∼ in the case of incomplete information for CLUTRR because there is only a fixed set of information that needs to come from the model (i.e., kinship relations), in the case of automatic difficulty control for leap-of-thought because the depth of reasoning is fixed (difficulty is added through distractors), and in the case of contradictory information for ConditionalQA because while the answer to a question can be yes under one set of conditions and no under another set of conditions, the two sets of conditions are mutually exclusive.</p>
<p>such cases, we accept the conclusion from the higher priority rule (see Figure 1).We let T (d) ⊨ f represent that f can be derived from a defeasible theory T (d) after resolving conflicts.Note that the initial facts F are internally consistent and always have priority over the derived facts.We assume the theory is defeasibly consistent, meaning whenever a conflict arises, the preferences can be used to resolve it.An example of a defeasible theory T (d) is as follows: Example 3.1.F = {Tweety is a penguin.},R = {r 1 : Penguins are birds.r 2 : Birds fly.r 3 : Penguins do not fly.},O = {r 3 &gt; r 2 }.From the theory, one can first use r 1 to derive that "Tweety is a bird".Then, one can use r 2 to derive that "Tweety flies".However, one can also use r 3 to derive that "Tweety does not fly", which is in conflict with the previous derivations.Since r 3 &gt; r 2 , we accept the derivation that "Tweety does not fly".</p>
<p>Conflict types: Conflicts can arise for rules whose heads cannot be simultaneously true, e.g., for two rules r : r b → z and r ′ : r ′ b →!z.For a theory T (d) with these two rules, T (d) ⊨ z in two cases: (a) r has higher priority than r ′ and we can prove r b , and (b) r has lower priority than r ′ and we can prove r b but we cannot prove r ′ b .In the first case, one does not need to take into account r ′ b for conflict resolution, but in the second case it is critical to take r ′ b into account.We name the first type of conflict Type1 conflict and the second type Type2.</p>
<p>The BoardgameQA Dataset</p>
<p>We now describe how we create a dataset for measuring the ability of LMs in reasoning with conflicting inputs in a defeasible setup.Our dataset creation follows a backward story generation strategy similar to [53,22].Each example in the dataset contains a (defeasible) theory T (d) and a question q.The goal is to predict whether T (d) ⊨ q, or T (d) ⊨ !q, or neither.Therefore, the label space for each question is {proved, disproved, unknown}.We next describe how we generate examples with the label proved; examples with the label disproved or unknown are created by modifying the examples with label proved.</p>
<p>The facts of each theory describe the current state of a board game, the rules of each theory represent the rules of the board game, and the questions are about the game.In the design of BoardgameQA, we include several variables that can be used to sample examples with varying levels of difficulty with respect to several finer-grained properties (e.g., depth, number and types of conflicts).</p>
<p>Entities and predicates:</p>
<p>We start with a predefined set of entities E (e.g., dog, cat, lion, etc.) and a predefined set of predicates P (e.g., invite for dinner, attack the fields, etc.) that we sample from to generate facts and rules.We use the animals as entities and the boardgame-inspired verbs/operations as our predicates.Using these entities and predicates, we can create facts such as the dog attacks the fields of the lion.To make the problem more challenging, we use different entities and predicates across training and test similar to [24].The full list of entities and predicates is provided in Appendix C.3.</p>
<p>Rule types:</p>
<p>We adopt a set of 6 rule templates containing existential and universal quantifiers, conjunctions, and missing information.The rules are as follows: 1-∀X : (X, p 1 , e 1 ) ⇒ (X, p 2 , e 2 ), If the dog has a name that starts with the same letter as the name of the cat, then ... Numeric Comparisons Some numbers are required to be summed and then compared to other numbers.</p>
<p>The dog has two friends that are nice and five that are not</p>
<p>If the dog has less than 10 friends, then ...</p>
<p>Lexical Entailment</p>
<p>The fact and the rule body are not identical but the fact entails the rule body.</p>
<p>The dog assassinated the mayor If the dog killed the mayor, then ...</p>
<p>World Knowledge</p>
<p>Some knowledge about the world is needed to connect the fact to the rule body.</p>
<p>The dog is currently in Montreal.</p>
<p>If the dog is currently in Canada, then ...</p>
<p>Event Times</p>
<p>Knowledge about times of events is needed to connect the fact to the rule body.</p>
<p>The dog is watching a movie that was released in 2005.</p>
<p>If the dog is watching a movie that was released after Covid19 started, then ...</p>
<p>Part Of</p>
<p>The fact and the rule body have a part of relation.</p>
<p>The dog is a nurse If the dog works in healthcare, then ...</p>
<p>Affordance</p>
<p>The rule body is about a certain feature/affordance of the fact.</p>
<p>The dog has a knife If the dog has a sharp object, then ...</p>
<p>Volumes Knowledge of what objects fit in what other</p>
<p>objects is required.</p>
<p>The dog has a ball with a radius of 15 inches.</p>
<p>If the dog has a ball that fits in a 28 x 35 x 35 inches, then ... addToFacts(q) 3: else 4:</p>
<p>Sample sub-questions Q = {q 1 , ..., q n } and rule r s.t.q can be derived from Q and r.Sample sub-questions Q ′ = {q ′ 1 , ..., q ′ m } and rule r ′ s.t.!q can be derived from Q ′ and r ′ .
Q = Q + SubSample(Q ′ ) 11: addToPreferences(r, r ′ ) 12: else 13: Q = Q + RemoveOneSubquestion(Q ′ ) 14: addToPreferences(r ′ , r) 15: for q i in Q do 16: GenerateTheory(q i , d-1)
Selecting a question: To generate each example, we first sample a question q = (e i , p j , e k ) that should be proved or disproved, where e i and e k are sampled from E and p j is sampled from P. We also sample the sign of the question (positive or negative).For example, we might sample the question !(dog, attack the fields, lion) asking whether the dog does not attack the fields of the lion.The question is then converted into natural language using a template (see Appendix C.3).</p>
<p>Theory generation:</p>
<p>The theory generation is the main component of the dataset generation that constructs the facts, rules and question to be used in each example.A high-level description is provided in Algorithm 16 and an example generation is shown in Appendix C. We first sample some sub-questions Q = q 1 , . . ., q n and a rule r which has Q in its body and q in its head, such that q can be derived from Q and r.The sampling is done by first selecting one of the aforementioned rule types, then matching the head of the rule to the question q, and then sampling sub-questions Q based on the body of the rule.For example for the question !(dog, attack the fields, lion), we might sample the first rule type (see the six types above), then p 2 will be mapped to attack the fields and e 2 will be mapped to lion, and we also sample a sub-question such as (dog, unite with, cat) and add the rule ∀X : (X, unite with, cat) ⇒!(X, attacks the fields, lion) to our set of rules.We then make a recursive call for each q i to generate new rules and facts for them.</p>
<p>We then decide whether a conflict should be introduced or not, by using a biased coin flip with p Conf representing the probability of conflict.If the decision is to produce conflicts, then we generate another set of sub-questions Q ′ = q ′ 1 , . . ., q ′ m and another rule r ′ such that !q can be derived from Q ′ and r ′ .Then we probabilistically decide if we want to generate a Type1 or a Type2 conflict using a biased coin flip with probability p ConfType1 .If the first case is selected, then r &gt; r ′ is added to the preferences.In this case, we can make recursive calls for all or a subset of the facts in Q ′ .Otherwise, r ′ &gt; r is added to the preferences.In this case, we make recursive calls for all but one of the facts in Q ′ (selecting randomly) to ensure that r ′ does not activate.</p>
<p>Proofs: We keep track of the facts, rules, and preferences during the generation process and turn them into proofs for the examples.</p>
<p>Stopping criterion: Every time we make a recursive call to the function in Algorithm 16, the example will contain one extra hop in its proof.We set the stopping criterion as the number of hops in the proof.Toward this goal, we included an argument d in Algorithm 16 which corresponds to the target maximum number of hops in the proof; d decreases by one every time we make a recursive call.When the algorithm is called with d = 0, instead of generating rules and sub-questions for the input question q, we simply add q to our set of facts.</p>
<p>Incomplete information:</p>
<p>We generate examples with incomplete information where part of the knowledge should come from the LM (corresponds to rule type 5).For a question q in the theory generation phase, we sample sub-questions Q and rule r such that Q can be derived based on Q and q can be derived from Q and r.We then hide Q from the model so the model has to derive it itself.We use a separate body of world knowledge, commonsense knowledge, mathematical, and orthography reasoning for generating Q and Q (see Table 2 for a high-level description and Appendix C.2 for more details).For example, for the goal "the dog unites with the cat" we generate the sub-question "The dog is in Montreal."and the rule "If the dog is in Canada, then the dog unites with the cat.".Then, an extra reasoning step is needed from the model to recognize that Montreal is in Canada.</p>
<p>We generate sub-questions and rules that require extra knowledge and reasoning with probability p MissInfo ; otherwise, we create sub-questions and rules that require no extra knowledge and reasoning.To make the problem more challenging, we only include some categories of extra knowledge and reasoning in the training set; this ensures that the models cannot simply learn the extra knowledge from the training set and use it in the test set.</p>
<p>Conversion to natural language: Finally, once we generate the facts, rules, preferences, and question, we use manually constructed templates to turn each of them into a textual format.To make the problem more challenging, we use multiple templates per rule type and use some of the templates only in the test set (see Appendix C.3 for details).A comparison of BoardgameQA with other prominent deductive reasoning datasets in terms of the average length of examples and the average number of unique tokens per example is provided in Figure 3.A comparison of BoardgameQA with ProofWriter [46] and PrOntoQA [39] in terms of average length of examples and average number of unique tokens per example on depth 3 of the datasets.</p>
<p>Disproved and unknown examples: So far, we described how to generate examples with the label proved.Generating examples with the label disproved can be done simply by first generating an example with the label proved and then negating the question.Also, generating examples with the label unknown can be done by perturbing the theory until the statement in the question cannot be derived from the theory (e.g., reducing the amount of money of the frog to 50 dollars in the example of Figure 2).We randomly select and apply the following perturbations to the theory and run a defeasible solver implemented based on the scalable solver in [27] on the resulting theory until the label becomes unknown: 1-change the predicate of a fact or a rule, 2change the sign of a fact or an element of the rule, 3-replace a fact with a new fact, and 4-flip the order of a preference.</p>
<p>Experiments</p>
<p>One of the primary goals of our experiments is to verify if LMs are capable of reasoning in a defeasible setup.For this reason, we conduct experiments with various LM architectures (encoder-only, encoderdecoder, and decoder-only) and various pre-training and learning paradigms (finetune with and without proofs, prompt tuning, few-shot in-context learning, and instruction-tuned).Specifically, we test 1) finetuning BERT-large [14] with a classification head to predict the label directly, 2) finetuning T5 1.1 XXL [34] to generate the entire proof and then the label, 3) few-shotting PaLM 62B and PaLM 540B [9] where we provide demonstration examples and chain-of-thought (CoT) in the prompt (the CoT corresponds to the proof), 4) few-shotting the instruction-finetuned FLAN-PaLM 540B [10] with CoT, and 5) soft prompt-tuning [25] PaLM 62B with CoT where instead of providing a static prompt, we make the prompt embedding learnable and tune its parameters using the training data (the rest of the LM parameters are frozen).We report classification accuracy as the metric.We also report the majority class baseline (∼33% since our labels are balanced).</p>
<p>Dataset sizes: To gain a more detailed understanding of the models' defeasible reasoning capacity, we create several variations of BoardgameQA.The nature of the variation will be discussed in the remainder of this section with each experiment.For each variation, we sample 1000 examples for train, 500 for validation, and 1000 for test.We sample an equal number of examples from each label.</p>
<p>Can LMs Reason with Contradictory Inputs?</p>
<p>As explained in Section 4, BoardgameQA makes use of a number of variables that control various aspects of the dataset such as the amount and types of conflict and the amount of extra knowledge required.We start by creating a default version of the dataset that exhibits each of these properties to some degree by setting p Conf = 0.5, p ConfType1 = 0.5, and p MissInfo = 0.5.We then generate three datasets with depth 1-3 (i.e., requiring 1-3 hop(s) of reasoning, respectively), and measure the performance of our baselines on these datasets.</p>
<p>The results are in Figure 4.The tuned models perform reasonably on depth 1, but their performance substantially degrades on depths 2-3.This contrasts with previous observations for monotonic reasoning (e.g., in [11,46]) where finetuned LMs reach near-perfect performance even on higher depths.This indicates that reasoning with contradictory inputs is more difficult even with finetuning.Moreover, we see that the few-shot models perform poorly across all depths showing that conflict resolution is not achieved out-of-the-box with pretrained models.This includes both PaLM and instruction-finetuned FLAN PaLM models.PaLM 540B performs better than PaLM 62B showing that larger models may have higher capacity for defeasible reasoning.More insights from full confusion matrices can be found in Appendix A.</p>
<p>Hereafter, due to inference costs, we only experiment with finetuned BERT and T5, prompt-tuned PaLM 62B, and few-shot PaLM 540B, and with examples of depth 2 to keep a medium level of difficulty in terms of reasoning hops and enable measuring the effect of the other factors.</p>
<p>Does Correct Label Prediction Mean Correct Proof?</p>
<p>Recently, it has been shown that although large LMs achieve high accuracy on label prediction for (monotonic) reasoning task, they do so by generating spurious proofs that do not represent valid steps of reasoning [22].There is also evidence that LMs frequently exploit spurious correlations in the data distribution to achieve high label accuracy, rather than reasoning purely deductively [57].Hence we design evaluation metrics to reflect a more rigorous measure of accurate defeasible reasoning.In the case where a model predicts the label correctly, and the label is one of proved or disproved (where an actual proof exists), we measure whether the proof generated by the model is correct or not.For this purpose, we compute two automated proof accuracy metrics (named Rule F1 and Conflict F1) and one manual metric (named Overall Proof Accuracy) as described below.For Rule F1, we extract the rules used in the golden proof and the ones in the proof generated by the model that are used to derive new facts (and ultimately, the goal).Then we compute the F1-score of the overlap of the two sets.For Conflict F1, we extract the conflict resolutions (corresponding to pairs of rules) used in the gold proof and the ones in the proof generated by the model, and compute the F1-score of their overlap.For Overall Proof Accuracy, we manually verify whether the proof is correct for 50 sampled examples per model.We compute these metrics on depth 2 of the dataset.According to the results in Figure 5, all models perform relatively well in selecting the correct set of rules for the proof.The few-shot model performs poorly on conflict resolution whereas the tuned models perform substantially better, suggesting that preference understanding and conflict resolution do not surface with simple few-shot prompting, and tuning is required for models to exhibit this capacity.Second, the models often generate wrong proofs, even when they predict the label correctly.The issue is less severe in the case of the prompt-tuned model but becomes more severe for the finetuned and few-shot models.We provide examples of proof failures in Appendix A.  The results of the models on these datasets are reported in Figure 6.The performance of all models monotonically degrades as the number of conflicts increases, showing that conflict resolution is indeed a major factor in the difficulty of the problems.For example, BERT performs above-random for the NoConflict and LowConflict cases, but the model performance drops to near-random on MediumConflict and HighConflict cases.To test which type of conflict (See sec.4) is more difficult for the models, we create three versions of the dataset with varying proportions of Type1 vs Type2 conflicts, by setting p ConfType1 to 0.2, 0.5, and 0.8 respectively.The first dataset mostly contains conflicts of Type1, the second contains both conflicts in a similar amount, and the third dataset contains mostly Type2 conflicts.The other factors are kept constant across the datasets.</p>
<p>Do Conflicts</p>
<p>The results of the models are reported in Figure 7.We see that models perform slightly better on the dataset with mostly Type1 conflicts.This discrepancy between performance on Type1 and Type2 conflicts is intuitive because in the case of Type1 conflicts, the model can ignore the conflicting rule and whether its body can be proved, but in the case of Type2 conflicts, the model has to show that at least one of the elements in the body of the conflicting rule cannot be proved.In the case of tuned models, we furthermore observe that biasing the dataset toward one conflict type results in better performance overall.This might be because the model mostly needs to learn to resolve one type of conflict which may be easier than learning both.</p>
<p>Does Information Incompleteness Make Reasoning More Difficult?</p>
<p>As described in Section 4, we can control the amount of information incompleteness using a parameter which we named p MissInfo .To test how the information incompleteness affects the performance of various models, we create three versions of our dataset with p MissInfo set to 0.2, 0.5 and 0.8, which we name KnowledgeLight, KnowledgeMedium and KnowledgeHeavy, respectively.The results are reported in Figure 8.We observe that as the amount of required knowledge increases, the performance of the finetuned models decreases accordingly.However, the performance of the prompt-tuned and few-shot models remain relatively unchanged, likely due to the larger size of the model and the extra amount of knowledge that is present in the model, as well as the fact that working with real-world knowledge might be easier for these models than with artificial knowledge.We also measure the effect of distracting facts and rules on model performance.A distracting fact or rule is one that does not appear in the proof and does not change the label.In Figure 2, for example, "the frog has a knife" is a distracting fact.To this end, each time we call Algorithm 16, besides the sampled sub-questions, we also sample some distracting sub-questions and add them to the set of sub-questions.We create three versions of the BoardgameQA dataset where we add 0, 1, and 2 distracting facts in each step, which we name NoDistractors, SomeDistractors, and ManyDistractors, respectively.</p>
<p>Do Distractors Make Reasoning More Difficult?</p>
<p>According to the results in Figure 9, the performance of the tuned models does not substantially degrade with a small number of distractors, potentially because the distractors can help the model avoid learning spurious correlations.However, their performance drops substantially with more distractors.Also, with more distractors, the performance of the few-shot model decreases monotonically, although only marginally (this observation is consistent with the results of [40]).This shows that distractors (that are typically common in real applications) can also compound the problem difficulty.</p>
<p>Conclusion</p>
<p>In this work, we introduced BoardgameQA, a dataset for measuring the natural language reasoning ability of language models (LMs) in the presence of conflicting input sources.Our dataset furthermore includes scenarios in which the knowledge required for reasoning is only partially provided as input and additional information needs to come from the model itself.We tested several types of LMs on different variations of the dataset and observed that LMs perform poorly when reasoning with conflicting inputs.In the case of smaller models, the performance was also poor when additional knowledge from the LM is needed.Since reasoning over contradicting and incomplete sets of information is a common scenario in real-world applications, our results highlight an important gap in the reasoning capacity of current LMs.We hope our dataset can guide future work developing methodology to improve the reasoning ability of LMs under this setup, or finding alternative formulations of conflict resolution that better facilitate LM reasoning.</p>
<p>A More Experimental Results and Analysis</p>
<p>Binary Classification: It was observed in [22] that reasoning with unknown labels is particularly challenging for few-shot LMs, because providing a natural chain-of-thought for unknown is difficult.</p>
<p>To measure if the poor performance is merely due to the existence of examples with unknown label or due to conflict resolution being difficult for these models, we also created a binary version of the dataset for depths 1, 2, and 3 where only examples with proved and disproved labels are included. 3he results are reported in Figure 10.We overall see similar patterns as the binary case, except for some improvements for the T5 model on depth 2.</p>
<p>Examples of Model Failures:</p>
<p>To showcase some of the reasons why models struggle with the BoardgameQA dataset, in Figures 12-21, we provide examples where the model generated wrong proofs.Some of the dominant error cases (showcased in the examples) include: 1-hallucinating or misunderstanding conflicts and preferences, 2-not being able to correctly fill in the incomplete information, 3-misunderstanding logical rules, 4-failing to prove both elements in a conjunction, 5-getting distracted by distracting facts and rules and going on a wrong proof path, and 6-being unfaithful to the provided facts and rules and changing them so a proof can be found in the case where no proof exists.</p>
<p>Confusion Matrices:</p>
<p>The confusion matrices for the model predictions with respect to the golden labels on the BoardgameQA dataset is provided in Figure 11 for various depths.One interesting observation is that for models tuned with proofs, while the model perform well at predicting unknown labels in lower depths, in higher depths they tend to generate proofs (with proved or disproved labels) even when the label is unknown (e.g., for examples with label unknown in depth 1 the prompt-tuned model predicts unknown for 268 instances, while in depth 2 it predicts unknown only for 2 instances).This may be because when the search space for a proof increases, LMs cannot verify all possible solutions and decide that the label is unknown.Instead, they start a path in the hopes that it ends in a proof.</p>
<p>B Experimental Details</p>
<p>We conducted our experiments on v3 TPUs for all the models, except for the 540B models where we used v4 TPUs due to their larger size.All the experiments were done using the T5X framework [36] available at https://github.com/google-research/t5x.</p>
<p>For the fewshot experiments, to ensure the demonstrations and the question fit in the prompt, due to the large size of the examples in BoardgameQA, we only included one example per label as demonstration (i.e. 3 examples one with label proved, one with disproved, and one with unknown in the case of 3-way classification datasets).For each example in the test set, we selected the demonstrations randomly from the training set, while ensuring that they contain both types of conflict resolution.For the prompt-tuning experiments, we used a prompt-size of 100 in all experiments, as it worked best in the experiments of [25].We allowed a maximum of 50K training steps with a batch size of 8, a learning rate of 0.1, and a weight decay rate of 0.0001.For the T5 experiments, we also set the batch size to 8 but set the learning rate to 0.001 and allowed 50K epochs (since it required more time to converge).For BERT experiments, we set the batch size to 16, learning rate to 4.6e − 5, and ran the training for 20 epochs.In all experiments involving turning, we reported the results for the epoch that achieved best validation accuracy.</p>
<p>Facts:</p>
<p>The seahorse has a basketball with a diameter of 27 inches, and is currently in Istanbul.The wolf does not take over the emperor of the snake.And the rules of the game are as follows.</p>
<p>Rules: [...] Rule2: If the seahorse has a basketball that fits in a 37.1 x 37.4 x 23.9 inches box, then the seahorse surrenders to the swan.Rule3: The seahorse will surrender to the swan if it (the seahorse) is in Turkey at the moment.[...] Preferences: Rule5 is preferred over Rule4.</p>
<p>Question: Does the swan swear to the woodpecker?</p>
<p>Prediction: Since the seahorse is in Istanbul and according to Rule3 if the seahorse is in Turkey at the moment then the seahorse surrenders to the swan so the seahorse surrenders to the swan.Since the seahorse surrenders to the swan and according to Rule2 if the seahorse has a basketball that fits in a 37.1 x 37.4 x 23.9 inches box then the seahorse surrenders to the swan so the seahorse has a basketball that fits in a 37.1 x 37.4 x 23.9 inches box.[...] Figure 12: An example of a wrong proof generated by PaLM 540B (fewshot) where the error is due to misunderstanding a logical rule (given a fact f and a rule f ′ → f the model concludes that f ′ must be true.</p>
<p>Facts:</p>
<p>The mule dances with the fish.[...]</p>
<p>Rules: [...] Rule2: There exists an animal which dances with the fish?Then, the dachshund definitely does not capture the king (i.e. the most important piece) of the owl.Rule3: If something does not capture the king (i.e. the most important piece) of the owl and additionally not disarm the zebra, then it disarms the woodpecker.[...]</p>
<p>Preferences: Rule1 is preferred over Rule5.Rule3 is preferred over Rule4.</p>
<p>Question: Does the dachshund disarm the woodpecker?</p>
<p>Prediction: [...] and according to Rule4 [...] and for the conflicting and higher priority rule Rule3 we cannot prove the antecedent "the dachshund does not capture the king of the owl", [...].</p>
<dl>
<dt>Figure 13</dt>
<dd>
<p>An example of a wrong proof generated by PaLM 62B (prompt-tuned) where the error is due to assuming an antecedent of a high priority rule cannot be proved, whereas it can indeed be proved.</p>
</dd>
</dl>
<p>C BoardgameQA Details</p>
<p>Here, we provide more in depth details about the generation and properties of the BoardgameQA dataset.A sample of theory and question generation from Algorithm 16 is provided in Figure 22.</p>
<p>C.1 Consistency of the Dataset</p>
<p>A defeasible theory is called consistent if whenever a conflict arises, the preferences can be used to resolve the conflict.In BoardgameQA, we aim to produce consistent theories.To avoid inconsistencies and loops, each time we call the function in Algorithm 16, we only allow it to sample from the entities that have not been used in other rules and (sub-)questions.As an example, if we have a rule such as a ∧ b ⇒ c, then when we call the algorithm for a and b recursively, we use separate entities in the facts and rules produced for the sub-branch of a and for the sub-branch of b.This way, we ensure that when we derive new facts in the sub-branch of a, it does not defeat some of the derivations in the sub-branch of b (and the derivations in the later stages of the proof do not defeat the earlier derivations).That is because the the set of facts used in the rule bodies are for separate entities, and are therefore disjoint.We also apply defeasible reasoning on the final logical theory to ensure that the question can be derived from the theory.</p>
<p>C.2 Incomplete Information</p>
<p>Here, we provide more information about the nature and type of incomplete information in BoardgameQA.</p>
<p>• Age: We first generate a positive integer x corresponding to the age of one of the players expressed in days.Then we decide if we want to use a more than or less than relationship.</p>
<p>In the case of the former, we next generate another integer y &lt; x and in the case of the latter y &gt; x.Then, we randomly decide a target unit (days, weeks, months, or years) for each integer and convert them to that unit.Let x ′ and y ′ represent the obtained values measured with the new units.Then we add a fact [PLAYER] is x ′ [unit] old and a rule if the [PLAYER] is [more than/less than] y ′ [unit] old then ....The model has to be able to convert units of time and then compare them.</p>
<p>Facts:</p>
<p>The mule dances with the fish.[...]</p>
<p>Rules: [...] Rule2: There exists an animal which dances with the fish?Then, the dachshund definitely does not capture the king (i.e. the most important piece) of the owl.Rule3: If something does not capture the king (i.e. the most important piece) of the owl and additionally not disarm the zebra, then it disarms the woodpecker.[...] Preferences: Rule1 is preferred over Rule5.Rule3 is preferred over Rule4.</p>
<p>Question: Does the dachshund disarm the woodpecker?</p>
<p>Prediction: [...] and according to Rule4 [...] and for the conflicting and higher priority rule Rule3 we cannot prove the antecedent "the dachshund does not capture the king of the owl", [...].Facts: The cougar manages to convince the ostrich, and smiles at the beetle.The dalmatian is named Chickpea.The pelikan has 8 friends.The seal is named Charlie.</p>
<p>Rules: Rule1: If something smiles at the beetle and manages to convince the ostrich, then it negotiates a deal with the badger.Rule2: If the dalmatian has a name whose first letter is the same as the first letter of the seal's name, then the dalmatian invests in the company whose owner is the beetle.[...] Rule4: From observing that an animal brings an oil tank for the shark, one can conclude the following: that animal does not shout at the badger.Rule5: If the cougar negotiates a deal with the badger and the pelikan shouts at the badger, then the badger falls on a square of the mouse.</p>
<p>Preferences: Rule4 is preferred over Rule3.</p>
<p>Question: Does the badger fall on a square of the mouse?</p>
<p>Prediction: Since the cougar manages to convince the ostrich and smiles at the beetle and according to Rule1 [...] so the cougar negotiates a deal with the badger.Since the cougar negotiates a deal with the badger and according to Rule5 if the cougar negotiates a deal with the badger and the pelikan shouts at the badger then the badger falls on a square of the mouse and Rule5 is preferred over the conflicting and lower priority rule Rule4 so the badger falls on a square of the mouse.So the statement the badger falls on a square of the mouse is proved and the answer is "yes".• Money: We first generate a positive integer x corresponding to the amount of money a player has, then we randomly decide if we want the comparison to be between two or three players.We also decide if we want to use a more than or less than relation.If the comparison is between two players and more than is used, then we generate another integer y &lt; x and if less than is used then y &gt; x; if the comparison is between three players and more than is used then we generate y, z such that y + z &lt; x, and if less than is used then y + z &gt; x.</p>
<p>We then generate facts of the form The [PLAYER i] has x dollars.and rules of the form If [PLAYER i] has [more than/less than] than [PLAYER j] and [PLAYER k] combined, then ....The model has to do a summation and decide which quantity is more or less.</p>
<p>• Textual Entailment: We manually write multiple pairs of sentences where one implies the other.Examples include (assassinated the mayor, killed the mayor), (struggles to find food, has difficulty to find food), and (purchased a luxury aircraft, owns a luxury aircraft).The first element of the pair is used in the fact and the second element in the body of a rule.The model has to identify the entailment.</p>
<p>• Places: We manually write a list of cities and the countries they are located in.• Names: We assign a name (from a list of manually written names) to two of the players and then write rules in the form of If [PLAYER i] has a name that starts with the same letter as [PLAYER 2], then ... .</p>
<p>• Jobs:</p>
<p>We manually write a list of pairs of jobs and the industry they belong to.Examples include (nurse, healthcare), (high school teacher, education), and (sales manager, marketing).</p>
<p>We use the job in the fact and the industry in the rule body.The model has to know which job is part of which industry.</p>
<p>• Volume: The facts mention that one of the players has an object (a notebook or a ball) and give the dimensions of the object (the height and width for notebook and the radius or diameter for the ball).The rule body asks whether the object fit in a box with some given dimensions.The model has to understand how 3D objects fit inside each other to be able to connect the fact to the rule.</p>
<p>• Events: We manually write a list of world events and the year when they occurred.Facts: The reindeer has three friends that are energetic and 1 friend that is not.The reindeer will turn 18 months old in a few minutes.The finch does not build a power plant near the green fields of the reindeer.</p>
<p>Rules: Rule1: For the reindeer, if the belief is that the german shepherd disarms the reindeer and the finch builds a power plant near the green fields of the reindeer, then you can add that "the reindeer is not going to acquire a photograph of the finch" to your conclusions.Rule2: The leopard trades one of its pieces with the cobra whenever at least one animal builds a power plant close to the green fields of the finch.Rule3: Regarding the reindeer, if it is more than five years old, then we can conclude that it acquires a photo of the finch.Rule4: Here is an important piece of information about the reindeer: if it has fewer than 5 friends then it acquires a photo of the finch for sure.</p>
<p>Preferences: Rule3 is preferred over Rule1.Rule4 is preferred over Rule1.</p>
<p>Question: Does the leopard trade one of its pieces with the cobra?</p>
<p>Prediction: Since [...] and according to Rule1 [...] and Rule1 is preferred over Rule3 so the reindeer is not going to acquire a photograph of the finch.[...] Figure 18: An example of a wrong proof generated by PaLM 540B (fewshot) where the error is due to misunderstanding a preference.</p>
<p>Facts:</p>
<p>The liger has a card that is violet in color.The worm invests in the company whose owner is the mouse.The goat does not create one castle for the pigeon.</p>
<p>Rules: Rule3: The living creature that does not create a castle for the pigeon will never hug the liger.[...] Preferences: Rule2 is preferred over Rule1.Rule6 is preferred over Rule5.</p>
<p>Question: Does the liger invest in the company whose owner is the mermaid?</p>
<p>Prediction: [...] We know the goat does not hug the liger, and according to Rule3 "if the goat does not hug the liger, then the liger does not hug the liger" generating sub-questions and rules for questions such as The dog is named Paco leading to unnatural rules such as "If ... then the dog is named Paco.</p>
<p>C.3 Entities, Predicates, and Templates</p>
<p>Table 3 presents the set of entities, predicates, and templates used in BoardgameQA.To make the problem slightly more challenging in terms of language complexity, we use different entities, predicates and templates in the test set.</p>
<p>D Limitations and Negative Societal Impact</p>
<p>We identify the following limitations in our current work:</p>
<p>• Our dataset, in its current form, focuses primarily on deductive logical entailment, where the problem is a classification problem of whether a the answer to a question is proved, disproved, or unknown based on a theory, and the contradictions are also binary (i.e. one rule suggesting something is True and the other suggesting it is False).Future work can extend BoardgameQA and the analysis provided in this work to non-classification cases where 1one needs to apply defeasible logical reasoning to answer questions such as "Who will be attacked by the dog?", and 2-one needs to resolve non-binary conflicts where, e.g., one rule suggests "the dog is currently in Canada" and the other suggests "the dog is currently in Australia".</p>
<p>• The current work assumes the initial state (facts) and the rules of the game are small enough to be included in the prompt.Future work can extend BoardgameQA and our analyses to the cases where not all the facts and rules can be included in the prompt due to the limitation in the prompt length, and retrieval is required to retrieve relevant facts and rules.</p>
<p>• The current work is limited to deductive reasoning with the modus ponens rule; future work can expand BoardgameQA and the analysis provided in this paper to other types of rules such as proof by contradiction, disjunction elimination, etc (see [40]).</p>
<p>Facts:</p>
<p>The llama creates one castle for the camel.The llama has a card that is white in color.The monkey is a public relations specialist.</p>
<p>Rules: [...] Rule2: Regarding the llama, if it has a card whose color appears in the flag of Italy, then we can conclude that it does not suspect the truthfulness of the mermaid.Rule3: If the monkey manages to convince the mermaid, then the mermaid is not going to pay money to the seahorse.Rule4: For the mermaid, if the belief is that the dinosaur dances with the mermaid and the llama does not suspect the truthfulness of the mermaid, then you can add "the mermaid pays some $$$ to the seahorse" to your conclusions.Rule5: If the monkey works in marketing, then the monkey manages to convince the mermaid.</p>
<p>Preferences: Rule1 is preferred over Rule2.Rule4 is preferred over Rule3.</p>
<p>Question: Does the mermaid pay money to the seahorse?</p>
<p>Prediction: We know the llama has a card that is white in color, white appears in the flag of Italy, and according to Rule2 [...]</p>
<p>Facts:</p>
<p>The otter has a 10 x 18 inches notebook.The otter is 15 months old.The duck does not tear down the castle that belongs to the otter.</p>
<p>Rules: Rule1: If the otter does not enjoy the companionship of the dugong, then the dugong captures the king (i.e. the most important piece) of the bear.Rule2: Regarding the otter, if it is more than four years old, then we can conclude that it does not want to see the dugong.Rule3: Regarding the otter, if it has a notebook that fits in a 18.1 x 12.8 inches box, then we can conclude that it does not want to see the dugong.</p>
<p>Preferences: None</p>
<p>Question: Does the dugong capture the king of the bear?</p>
<p>Prediction: Since the otter is 15 months old and 15 months is less than 4 years and according to Rule2 if the otter is less than 4 years old then it does not want to see the dugong so the otter does not want to see the dugong.[...] Figure 21: An example of a wrong proof generated by PaLM 540B (fewshot) where the error is due to changing a rule in such a way that a proof can be found, when a proof does not exist.</p>
<p>• In this work, we only studied one simple but highly practical solution to conflict resolution (i.e. based on preferences).Future work can extend BoardgameQA and the analysis in this paper to other natural types of conflict resolution.• In some applications, preferences for conflict resolution have to be assigned with great care and diligence to avoid unfair treatment of information sources.</p>
<p>Facts:</p>
<p>A few players are playing a boardgame.Here is the current state of the game.The dog has $3.The lion has $48.The frog has 81 dollars, and has a knife.[...]</p>
<p>Figure 2 :
2
Figure 2: A sample example from BoardgameQA that requires one hop of reasoning.The text in violet highlights conflict resolution and the text in blue highlights the missing information.</p>
<p>6 :
6
if CoinFlip(p Conf ) == Conflict then 7:</p>
<p>if CoinFlip(p ConfType1 ) == Type1 then 10:</p>
<p>Figure 3:A comparison of BoardgameQA with ProofWriter[46] and PrOntoQA[39] in terms of average length of examples and average number of unique tokens per example on depth 3 of the datasets.</p>
<p>FineutnedFigure 4 :
4
Figure 4: The model performances on depths 1-3 of the BoardgameQA dataset.Many models struggle on this dataset, especially with higher depths.</p>
<p>Fineutned</p>
<p>Figure 5 :
5
Figure 5: Proof accuracy metrics for various models on depth 2 of the dataset, when the label is predicted correctly.</p>
<p>Figure 6 :
6
Figure 6: The model performances on four versions of the BoardgameQA dataset with various amounts of conflicts in them.We create four versions of BoardgameQA named NoConflict, LowConflict, Medium-Conflict, and HighConflict, with p Conf set to 0.0, 0.2, 0.5 and 0.8 respectively; other factors are kept the same.Note that the Medi-umConflict corresponds to the dataset in Fig-ure 4. The results of the models on these datasets are reported in Figure6.The performance of all models monotonically degrades as the number of conflicts increases, showing that conflict resolution is indeed a major factor in the difficulty of the problems.For example, BERT performs above-random for the NoConflict and LowConflict cases, but the model performance drops to near-random on MediumConflict and HighConflict cases.</p>
<ol>
<li>4 Fineutned
4
Which Conflict Type is More Difficult to Resolve?</li>
</ol>
<p>Figure 7 :
7
Figure 7: The model performances on three versions of the BoardgameQA dataset with different distributions on the type of conflicts.</p>
<p>FineutnedFigure 8 :
8
Figure 8: The model performances on three versions of BoardgameQA with various degrees of incomplete information.</p>
<p>FineutnedFigure 9 :
9
Figure 9: The model performances on three versions of BoardgameQA with various amounts of distracting facts and rules.</p>
<p>FineutnedFigure 10 :
10
Figure 10: The model performances on various depths of a binary version of the BoardgameQA dataset.</p>
<p>Figure 11 :
11
Figure 11: Confusion matrices for various models on the BoardgameQA dataset.</p>
<p>Figure 14 :
14
Figure 14: An example of a wrong proof generated by PaLM 62B (prompt-tuned) where the error is due to filling the missing information incorrectly.</p>
<p>Figure 15 :
15
Figure 15: An example of a wrong proof (but correct label) generated by PaLM 540B (fewshot)where the error is due to 1-failing to prove one element of the conjunction and also identifying a non-existence conflict between two rules.</p>
<p>Figure 17 :
17
Figure 17: An example of a wrong proof generated by PaLM 540B (fewshot) where the error is due to starting with a distracting fact that took the proof on a wrong path (the correct proof is to first use the fact The woodpecker swears to the duck and Rule7 to conclude that The woodpecker leaves the houses occupied by the dragon, and then use Rule5 to conclude that The vampire does not disarm the bulldog.</p>
<p>Figure 19 :
19
Figure 19: An example of a wrong proof generated by T5 where the error is due to hallucinating facts and rules.</p>
<p>Figure 20 :
20
Figure 20: An example of a wrong proof generated by T5 where the the model got distracted and ended up on a wrong proof path.</p>
<p>Table 2 :
2
Categories, descriptions, and examples of incomplete information in BoardgameQA.For lexical entailment, world knowledge, event times, and affordance, a list of examples is written manually from which the sampling procedure can select.In others, examples are generated automatically.</p>
<p>2-∀X : (X, p 1 , e 1 ) ∧ (X, p 2 , e 2 ) ⇒ (X, p 3 , e 3 ), 3-(e 1 , p 1 , e 2 ) ⇒ (e 2 , p 2 , e 3 ), 4-(e 1 , p 1 , e 2 ) ∧ (e 3 , p 2 , e 2 ) ⇒ (e 2 , p 3 , e 4 ), 5-(e 1 , p, ê) ⇒ (e 1 , p 2 , e 2 ), and 6-∃X(X, p 1 , e 1 ) ⇒ (e 2 , p 2 , e 3 ), where X represents a universally or existentially bounded variable, each e i represents an entity, and each p j represents a predicate.The fifth rule template corresponds to a rule where the predicate (or object entity) in the rule body may not be an element of P (resp.E).For more information, see below.Algorithm 1 GenerateTheory Input: Question q, Depth d 1: if d == 0 then 2:</p>
<p>Make Reasoning More Difficult?
Accuracy40 60 80Majority Class No Conflict Low Conflict Medium Conflict High Conflict200Fineutned w/o proofs (BERT Large)Fineutned w/ proofs (T5 XXL)Prompt-tuned w/ CoT (PaLM 62B)(PaLM 540B) Fewshot w/ CoT</p>
<p>The city names are used in the facts (The [PLAYER] is in [CITY] right now.) and the countries are used in the rule bodies (If the [PLAYER] is in [COUNTRY] right now, then ....The model has to know which city is in which country.The monkey has a card that is orange in color.The monkey has a football with a radius of 29 inches, and is a farm worker.Rules: [...] Rule4: If the monkey has a basketball that fits in a 26.3 x 25.6 x 24.2 inches box, then the monkey swims in the pool next to the house of the crab.[...] Does the monkey pay money to the rhino?Prediction: Since the monkey has a football with a radius of 29 inches and according to Rule4 if the monkey has a basketball that fits in a 26.3 x 25.6 x 24.2 inches box then the monkey swims in the pool next to the house of the crab [...] Figure16: An example of a wrong proof generated by PaLM 540B (fewshot) where the error is due to 1-mistaking a football with a basketball, and 2-not being able to fill in the incomplete information by realizing that a ball with a radius of 29 inches does not fit in a 26.3 x 25.6 x 24.2 inches box.The gorilla is watching a movie from 1952.The woodpecker has a football with a radius of 28 inches, and swears to the duck.The zebra swims in the pool next to the house of the butterfly.Rule1: The gorilla will not enjoy the company of the vampire if it (the gorilla) works in healthcare.[...] Rule4: Here is an important piece of information about the woodpecker: if it has a football that fits in a 58.4 x 61.5 x 59.6 inches box then it does not call the vampire for sure.Rule5: If at least one animal leaves the houses that are occupied by the dragon, then the vampire does not disarm the bulldog.[...] Rule7: If something swears to the duck, then it leaves the houses that are occupied by the dragon, too.Since the woodpecker has a football with a radius of 28 inches and according to Rule4 if the woodpecker has a football that fits in a 58.4 x 61.5 x 59.6 inches box then it does not call the vampire so the woodpecker does not call the vampire.[...]
Preferences: Rule5 is preferred over Rule1.Rules: Preferences: Rule1 is preferred over Rule2. Rule3 is preferred over Rule4. Rule6 is preferred over Rule2.
Facts: Question: Facts: Question: Does the vampire disarm the bulldog?Prediction:</p>
<p>Examples include (world war 1 started, 1914), (the first man landed on moon, 1969) and (Obama's presidency started, 2009).Then we write facts of the form The [PLAYER] is watching a movie from [YEAR] and rules of the form If the [PLAYER] is watching a movie that was released [before/after] [EVENT], then ....The model has to know the time for major world events to be able to connect the fact and the rule.•Friends:We first generate a positive integer x corresponding to the number of friends a player has.Then, we either generate a fact such as The [PLAYER] has x friends or The [PLAYER] has x 1 friends that are [ADJECTIVE] and x 2 that are not where x 1 + x 2 = x.Then we decide if we want to use a more than or less than relation.In the former case, we</p>
<p>generate a number y &lt; x and in the latter case y &gt; x.Then we generate a rule of the form If the [PLAYER] has [more than/less than] y friends, then .... Due to the nature of the extra knowledge and reasoning cases we consider, we only add such cases at the last theory generation step of Algorithm 16 (i.e. when d = 1); Otherwise, we will need to follow</p>
<p>Available at: https://storage.googleapis.com/gresearch/BoardgameQA/BoardgameQA.zip. License: CC BY.
Note: Many types of preferences can be converted into pair-wise relative preferences.
In this case, we set pConf = 1.0 for the first call we make to Algorithm 16; Otherwise, the dataset will have a spurious correlation that can be exploited without doing any reasoning.
Initially, the question has been selected to be (dog, attack, cat).The input depth D = 1 indicates that a theory with one hop of reasoning should be generated.Then a fact (dog, unite, lion) and a rule R1: (X, unite, lion) ⇒ (X, attack, cat) have been generated.Notice that the combination of the fact and the rule conclude (dog, attack, cat).Next we randomly decide if a conflict should be generated.The decision is yes, so we generate another fact (dog, respect, cat) and rule (dog, respect, cat) ⇒ !(dog, attack, cat).Notice that the two rules have contradictory conclusions now.We next decide randomly the type of the conflict, and Type2 is selected in this case.Therefore, we add R2 &gt; R1 to our preferences, remove one of the facts generated for the conflicting rule, and make recursive calls for the remaining facts, which is only (dog, unite, lion).This call is made with D = 0, therefore the stopping criterion triggers and we add (dog, unite, lion) to our set of facts.Train entities cat, dog, pig, parrot, eagle, squirrel, penguin, lion, tiger, donkey, leopard, cheetah, grizzly bear, polar bear, sun bear, panda bear, black bear, turtle, crocodile, elephant, panther, cow, rabbit, hare, buffalo, baboon, sheep, whale, jellyfish, carp, goldfish, viperfish, starfish, catfish, oscar, zander, sea bass, swordfish, salmon, halibut, blobfish, doctorfish, tilapia, kangaroo, octopus, phoenix, aardvark, amberjack, eel, hummingbird, canary, hippopotamus, snail, caterpillar, mosquito, bat, ferret, gecko, kudu, moose, cockroach, cricket, grasshopper, meerkat, spider, lobster, squid, puffin, raven, kiwi, koala, wolverine Test entities akita, bear, camel, coyote, snake, monkey, leopard, fish, ostrich, pigeon, dolphin, frog, goat, goose, wolf, gorilla, beaver, lizard, flamingo, swan, elk, duck, reindeer, bison, shark, mouse, owl, llama, cobra, zebra, otter, crab, peafowl, rhino, dinosaur, dove, badger, chinchilla, cougar, crow, seal, worm, ant, bee, butterfly, dragonfly, dragon, gadwall, mule, liger, german shepherd, bulldog, husky, poodle, chihuahua, dachshund, basenji, dalmatian, mermaid, seahorse, fangtooth, dugong, walrus, vampire, stork, swallow, songbird, woodpecker, starling, mannikin, pelikan, beetle, finch Train predicates owe money to, give a magnifier to, learn the basics of resource management from, know the defensive plans of, show all her cards to, prepare armor for, sing a victory song for, need support from, respect, raise a peace flag for, become, an enemy of, roll the dice for, hold the same number of points as, offer a job to, wink at, steal five points from, knock down the fortress of, burn the warehouse of, eat the food of, attack the green fields whose owner is, proceed to the spot that is right after the spot of, remove one of the pieces of Test predicates tear down the castle that belongs to, bring an oil tank for, reveal a secret to, enjoy the company of, neglect, want to see, swear to, refuse to help, manage to convince, call, stop the victory of, dance with, shout at, smile at, pay money to, unite with, hug, destroy the wall constructed by, create one castle for, disarm, acquire a photograph of, borrow one of the weapons of, fall on a square of, suspect the truthfulness of, invest in the company whose owner is, leave the houses occupied by, hide the cards that she has from, swim in the pool next to the house of, negotiate a deal with, trade one of its pieces with, build a power plant near the green fields of, take over the emperor of, capture the king of, surrender to
Penguins don't fly: Reasoning about generics through instantiations and exceptions. Emily Allaway, Jena D Hwang, Chandra Bhagavatula, Kathleen Mckeown, Doug Downey, Yejin Choi, arXiv:2205.116582022arXiv preprint</p>
<p>Conversational neuro-symbolic commonsense reasoning. Forough Arabshahi, Jennifer Lee, Mikayla Gawarecki, Kathryn Mazaitis, Amos Azaria, Tom Mitchell, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Can retriever-augmented language models reason? the blame game between the retriever and the language model. Parishad Behnamghader, Santiago Miret, Siva Reddy, arXiv:2212.091462022arXiv preprint</p>
<p>Critical thinking for language models. Gregor Betz, Christian Voigt, Kyle Richardson, Proceedings of the 14th International Conference on Computational Semantics (IWCS). the 14th International Conference on Computational Semantics (IWCS)Groningen, The NetherlandsAssociation for Computational LinguisticsJune 2021</p>
<p>Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Wen-Tau Yih, Yejin Choi, arXiv:1908.05739Abductive commonsense reasoning. 2019arXiv preprint</p>
<p>Genericskb: A knowledge base of generic statements. Sumithra Bhakthavatsalam, Chloe Anastasiades, Peter Clark, arXiv:2005.006602020arXiv preprint</p>
<p>Argumentation and defeasible reasoning in the law. Marco Billi, Roberta Calegari, Giuseppe Contissa, Francesca Lagioia, Giuseppe Pisano, Galileo Sartor, Giovanni Sartor, J. 442021</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, arXiv:2204.02311David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav Petrovand Noah Fiedel. PaLM: Scaling language modeling with pathways</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'202021</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Defeasible reasoning in web-based forms through argumentation. Alejandro Sergio, Carlos Gómez, Guillermo Ricardo Iván Chesnevar, Simari, International Journal of Information Technology &amp; Decision Making. 7012008</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Natural language reasoning with first-order logic. 2022</p>
<p>On a flexible representation for defeasible reasoning variants. Abdelraouf Hecham, Pierre Bisquert, Madalina Croitoru, AAMAS 2018-17th International Conference on Autonomous Agents and MultiAgent Systems, number AAMAS'18. 2018</p>
<p>Planner: A language for proving theorems in robots. Carl Hewitt, Proceedings of the 1st International Joint Conference on Artificial Intelligence, IJCAI'69. the 1st International Joint Conference on Artificial Intelligence, IJCAI'69San Francisco, CA, USAMorgan Kaufmann Publishers Inc1969</p>
<p>Inferring implicit relations in complex questions with language models. Uri Katz, Mor Geva, Jonathan Berant, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Lambada: Backward chaining for automated reasoning in natural language. Najoung Seyed Mehran Kazemi, Deepti Kim, Xin Bhatia, Deepak Xu, Ramachandran, ACL. 2023</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Entity tracking in language models. Najoung Kim, Sebastian Schuster, arXiv:2305.023632023arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Think about it! improving defeasible reasoning by first modeling the question scenario. Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, Eduard Hovy, arXiv:2110.123492021arXiv preprint</p>
<p>Rethinking defeasible reasoning: A scalable approach. J Michael, Ilias Maher, Grigoris Tachmazidis, Stephen Antoniou, Long Wade, Cheng, Theory and Practice of Logic Programming. 2042020</p>
<p>Programs with common sense. John Mccarthy, Proceedings of the Teddington Conference on the Mechanization of Thought Processes. the Teddington Conference on the Mechanization of Thought ProcessesLondonHer Majesty's Stationary Office1959</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, Deep Learning for Code Workshop. 2022</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Defeasible reasoning. John L Pollock, Cognitive science. 1141987</p>
<p>A logical framework for default reasoning. David Poole, Artificial intelligence. 3611988</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, arXiv:2212.09597Reasoning with language model prompting: A survey. 2022arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Nonmonotonic reasoning. Raymond Reiter, Exploring artificial intelligence. Elsevier1988</p>
<p>Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Jacob Marc Van Zee, Sebastian Austin, Livio Baldini Goodman, Haitang Soares, Sasha Hu, Aakanksha Tsvyashchenko, Jasmijn Chowdhery, Jannis Bastings, Xavier Bulian, Jianmo Garcia, Andrew Ni, Chen, arXiv:2203.17189Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x and seqio. Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp2022arXiv preprint</p>
<p>Thinking like a skeptic: Defeasible inference in natural language. Rachel Rudinger, Vered Shwartz, Jena D Hwang, Chandra Bhagavatula, Maxwell Forbes, Le Ronan, Noah A Bras, Yejin Smith, Choi, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>RuleBERT: Teaching soft rules to pre-trained language models. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He, arXiv:2305.152692023arxiv preprint</p>
<p>Defeasibility in legal reasoning. Giovanni Sartor, 1995Springer</p>
<p>Mathematical Logic. J R Shoenfield, 2001Taylor &amp; Francis</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, arXiv:1908.06177Clutrr: A diagnostic benchmark for inductive reasoning from text. 2019arXiv preprint</p>
<p>Natural language deduction with incomplete information. Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Conditionalqa: A complex reading comprehension dataset with conditional answers. Haitian Sun, William W Cohen, Ruslan Salakhutdinov, arXiv:2110.068842021arXiv preprint</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsAugust 2021</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937Commonsenseqa: A question answering challenge targeting commonsense knowledge. 2018arXiv preprint</p>
<p>Leap-ofthought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems. 202033</p>
<p>Iteratively prompt pre-trained language models for chain of thought. Boshi Wang, Xiang Deng, Huan Sun, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, arXiv:1502.056982015arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Anbang Ye, Christopher Cui, Taiwei Shi, Mark O Riedl, arXiv:2212.08718Neural story planning. 2022arXiv preprint</p>
<p>Nature language reasoning, a survey. Fei Yu, Hongbo Zhang, Benyou Wang, arXiv:2303.147252023arXiv preprint</p>
<p>STaR: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Improved logical reasoning of language models via differentiable symbolic programming. Hanlin Zhang, Ziyang Li, Jiani Huang, Mayur Naik, Eric Xing, First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022. 2022</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, arXiv:2205.11502On the paradox of learning to reason from data. 2022</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, arXiv:2104.06598AR-LSAT: Investigating analytical reasoning of text. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
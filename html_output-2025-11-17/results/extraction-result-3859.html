<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3859 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3859</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3859</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-269484318</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.00099v4.pdf" target="_blank">Creative Beam Search: LLM-as-a-Judge For Improving Response Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models are revolutionizing several areas, including artificial creativity. However, the process of generation in machines profoundly diverges from that observed in humans. In particular, machine generation is characterized by a lack of intentionality and an underlying creative process. We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation. The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques. We also show that the response validation step is a necessary complement to the response generation step.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3859.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3859.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (CBS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge used within Creative Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses a model-internal evaluation step (LLM-as-a-Judge) to rank K candidate responses produced by Diverse Beam Search and select the final output; positional-bias mitigation (balanced position calibration) and vote aggregation over rotations are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-Judge for selecting the best candidate among K Diverse Beam Search outputs (inference-time self-evaluation with balanced position calibration).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>creative text generation / co-creative response generation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Llama 2 7B (RLHF-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>The paper notes that LLM self-evaluation is not a human judgement: it returns what the model has learned to be likely rather than a personal belief or intentional assessment; positional order of candidates can change rankings (positional bias) and some human-evaluated subtleties (intentionality, genuine domain-motivated judgments) are not captured by model self-eval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Positional bias in rankings (order of candidates affects LLM judgments); lack of consciousness/intent — self-eval does not reflect personal belief; evaluations may simply reflect model likelihood biases; sensitivity to prompt structure and limited candidate set; potential fairness/evaluator weaknesses (cited literature).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>In this work, using the LLM-as-a-Judge improved end-user preferences: CBS (which includes the LLM self-eval step) was preferred by human evaluators 45% of the time versus 29% for standard sampling, suggesting the LLM-judged selection can produce outputs humans prefer; the self-evaluation step also notably changed which candidate was chosen versus DBS scoring (see overlap metric).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Mitigate positional bias via balanced position calibration (rotate candidate order and aggregate votes); use greedy decoding for self-assessment to reduce randomness; examine prompt structure effects and expand candidate sets in future evaluations; combine with human evaluation for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Creative Beam Search: LLM-as-a-Judge for Improving Response Generation</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creative Beam Search: LLM-as-a-Judge For Improving Response Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3859.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3859.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alignment claim (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reported alignment between strong LLM evaluators and human experts (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior work (Chiang & Lee 2023; Zheng et al. 2023) reporting that evaluations from strong LLMs align with those from human experts, while also noting associated biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Reported/alleged alignment of LLM-based evaluations with human expert judgements (cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>general model-output evaluation (multiple domains implied)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Cited work claims alignment, but the paper emphasizes that LLM evaluations can be biased (e.g., positional bias) and are not equivalent to human intentional judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Positional bias and other evaluation biases reported in cited work; LLM evaluation still reflects model-learned likelihood patterns rather than human-like intentional assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Cited studies are reported to find alignment with human experts, indicating LLM evaluators can approximate human judgments in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>The paper adopts balanced position calibration (rotating candidate order and aggregating votes) to mitigate positional bias reported in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>cited (Chiang & Lee 2023; Zheng et al. 2023) as discussed in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creative Beam Search: LLM-as-a-Judge For Improving Response Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3859.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3859.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positional bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Positional bias in LLM-as-a-Judge evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper highlights that LLM ranking/evaluation of candidate responses is affected by candidate ordering (positional bias) and therefore raw single-prompt votes can be unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-Judge ranking of candidate outputs; positional bias mitigation via balanced position calibration (rotations and vote aggregation) is applied in the method.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>evaluation/ranking of generated text</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Positional bias causes LLM judges to change their preferred candidate when candidate order changes — unlike robust human judgments which are less order-dependent when asked carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Raw LLM votes can be skewed by candidate ordering; positional bias can misrepresent quality ranking unless mitigated.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Balanced position calibration (rotate candidate order so each candidate appears in all positions and aggregate) is an effective mitigation adopted in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use balanced position calibration: create K prompts by rotating candidates so each occupies every position, then aggregate votes; in general, design evaluation prompts to minimize order effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>cited (Wang et al. 2023) as discussed in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creative Beam Search: LLM-as-a-Judge For Improving Response Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3859.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3859.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human preference study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qualitative human evaluation comparing CBS outputs vs standard sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A user study with 31 graduate students (217 comparisons) in which participants chose between CBS output (DBS + LLM-as-a-Judge) and standard sampling outputs; CBS was preferred in 45% of comparisons, standard in 29%, and 'too similar' in 26%.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Human evaluation of creative outputs: participants freely provided prompts and chose which of two presented outputs (CBS vs standard sampling) was more creative or declared them too similar.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>creative text generation / subjective creativity preference</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Llama 2 7B (RLHF-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Humans sometimes judged outputs as too similar (≈25%); CBS did not always diverge strongly from standard sampling, suggesting some aspects of diversity/creativity are not dramatically altered by CBS in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Evaluation limited to short outputs (256 tokens max), small participant pool (31 grad students), resource-limited model (Llama 2 7B), and only K=4 candidates evaluated — external validity is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Despite constraints, human evaluators on average preferred the CBS outputs, implying the LLM-as-a-Judge selection helped produce outputs perceived as more creative.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Expand experimental evaluation (more participants, broader prompts, larger/more creative LLMs), examine prompt structure impact, consider larger/more diverse candidate sets for the self-evaluation step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Creative Beam Search: LLM-as-a-Judge for Improving Response Generation</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creative Beam Search: LLM-as-a-Judge For Improving Response Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3859.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3859.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-eval vs DBS overlap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Overlap between LLM self-evaluation selection and Diverse Beam Search selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors measured how often the candidate selected by the model's self-evaluation matched the candidate with top DBS score: overlap was 29%, which is lower than the 35.3% expected from random selection in their setup, indicating self-eval often changes the DBS-preferred choice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Internal comparison between the candidate chosen by the LLM-as-a-Judge and the candidate that DBS would select by its combined likelihood/diversity score.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>ranking/selection among generated candidates (creative generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Llama 2 7B (RLHF-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>29% overlap (self-eval vs DBS); compared against 35.3% expected from random selection (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Self-evaluation subverted DBS scoring often (i.e., chose different candidates than DBS top-scoring), indicating the LLM judge applies different selection criteria than DBS's likelihood/diversity objective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Overlap below random baseline suggests complex interactions — interpretation requires caution; does not directly measure agreement with human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Lower-than-random overlap indicates the self-evaluation stage is not a trivial/noisy replay of DBS scoring and actively changes selections; combined with human preference results, this suggests the self-eval can improve human-perceived creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Analyze and report overlap metrics when combining search and self-evaluation to understand how often model judgement diverges from the search objective; consider larger candidate sets to study effect magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Creative Beam Search: LLM-as-a-Judge for Improving Response Generation</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creative Beam Search: LLM-as-a-Judge For Improving Response Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Quality-Diversity through AI feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3859",
    "paper_id": "paper-269484318",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge (CBS)",
            "name_full": "LLM-as-a-Judge used within Creative Beam Search",
            "brief_description": "The paper uses a model-internal evaluation step (LLM-as-a-Judge) to rank K candidate responses produced by Diverse Beam Search and select the final output; positional-bias mitigation (balanced position calibration) and vote aggregation over rotations are applied.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-Judge for selecting the best candidate among K Diverse Beam Search outputs (inference-time self-evaluation with balanced position calibration).",
            "task_or_domain": "creative text generation / co-creative response generation",
            "llm_model_name": "Llama 2 7B (RLHF-tuned)",
            "agreement_rate": null,
            "qualitative_differences": "The paper notes that LLM self-evaluation is not a human judgement: it returns what the model has learned to be likely rather than a personal belief or intentional assessment; positional order of candidates can change rankings (positional bias) and some human-evaluated subtleties (intentionality, genuine domain-motivated judgments) are not captured by model self-eval.",
            "limitations_or_failure_cases": "Positional bias in rankings (order of candidates affects LLM judgments); lack of consciousness/intent — self-eval does not reflect personal belief; evaluations may simply reflect model likelihood biases; sensitivity to prompt structure and limited candidate set; potential fairness/evaluator weaknesses (cited literature).",
            "counterexamples_or_strengths": "In this work, using the LLM-as-a-Judge improved end-user preferences: CBS (which includes the LLM self-eval step) was preferred by human evaluators 45% of the time versus 29% for standard sampling, suggesting the LLM-judged selection can produce outputs humans prefer; the self-evaluation step also notably changed which candidate was chosen versus DBS scoring (see overlap metric).",
            "recommendations_or_best_practices": "Mitigate positional bias via balanced position calibration (rotate candidate order and aggregate votes); use greedy decoding for self-assessment to reduce randomness; examine prompt structure effects and expand candidate sets in future evaluations; combine with human evaluation for validation.",
            "citation": "Creative Beam Search: LLM-as-a-Judge for Improving Response Generation",
            "uuid": "e3859.0",
            "source_info": {
                "paper_title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Alignment claim (prior work)",
            "name_full": "Reported alignment between strong LLM evaluators and human experts (cited)",
            "brief_description": "The paper cites prior work (Chiang & Lee 2023; Zheng et al. 2023) reporting that evaluations from strong LLMs align with those from human experts, while also noting associated biases.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_setting": "Reported/alleged alignment of LLM-based evaluations with human expert judgements (cited prior work).",
            "task_or_domain": "general model-output evaluation (multiple domains implied)",
            "llm_model_name": null,
            "agreement_rate": null,
            "qualitative_differences": "Cited work claims alignment, but the paper emphasizes that LLM evaluations can be biased (e.g., positional bias) and are not equivalent to human intentional judgments.",
            "limitations_or_failure_cases": "Positional bias and other evaluation biases reported in cited work; LLM evaluation still reflects model-learned likelihood patterns rather than human-like intentional assessment.",
            "counterexamples_or_strengths": "Cited studies are reported to find alignment with human experts, indicating LLM evaluators can approximate human judgments in some settings.",
            "recommendations_or_best_practices": "The paper adopts balanced position calibration (rotating candidate order and aggregating votes) to mitigate positional bias reported in prior work.",
            "citation": "cited (Chiang & Lee 2023; Zheng et al. 2023) as discussed in the paper",
            "uuid": "e3859.1",
            "source_info": {
                "paper_title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Positional bias",
            "name_full": "Positional bias in LLM-as-a-Judge evaluations",
            "brief_description": "The paper highlights that LLM ranking/evaluation of candidate responses is affected by candidate ordering (positional bias) and therefore raw single-prompt votes can be unreliable.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_setting": "LLM-as-a-Judge ranking of candidate outputs; positional bias mitigation via balanced position calibration (rotations and vote aggregation) is applied in the method.",
            "task_or_domain": "evaluation/ranking of generated text",
            "llm_model_name": null,
            "agreement_rate": null,
            "qualitative_differences": "Positional bias causes LLM judges to change their preferred candidate when candidate order changes — unlike robust human judgments which are less order-dependent when asked carefully.",
            "limitations_or_failure_cases": "Raw LLM votes can be skewed by candidate ordering; positional bias can misrepresent quality ranking unless mitigated.",
            "counterexamples_or_strengths": "Balanced position calibration (rotate candidate order so each candidate appears in all positions and aggregate) is an effective mitigation adopted in this work.",
            "recommendations_or_best_practices": "Use balanced position calibration: create K prompts by rotating candidates so each occupies every position, then aggregate votes; in general, design evaluation prompts to minimize order effects.",
            "citation": "cited (Wang et al. 2023) as discussed in the paper",
            "uuid": "e3859.2",
            "source_info": {
                "paper_title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Human preference study",
            "name_full": "Qualitative human evaluation comparing CBS outputs vs standard sampling",
            "brief_description": "A user study with 31 graduate students (217 comparisons) in which participants chose between CBS output (DBS + LLM-as-a-Judge) and standard sampling outputs; CBS was preferred in 45% of comparisons, standard in 29%, and 'too similar' in 26%.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Human evaluation of creative outputs: participants freely provided prompts and chose which of two presented outputs (CBS vs standard sampling) was more creative or declared them too similar.",
            "task_or_domain": "creative text generation / subjective creativity preference",
            "llm_model_name": "Llama 2 7B (RLHF-tuned)",
            "agreement_rate": null,
            "qualitative_differences": "Humans sometimes judged outputs as too similar (≈25%); CBS did not always diverge strongly from standard sampling, suggesting some aspects of diversity/creativity are not dramatically altered by CBS in all cases.",
            "limitations_or_failure_cases": "Evaluation limited to short outputs (256 tokens max), small participant pool (31 grad students), resource-limited model (Llama 2 7B), and only K=4 candidates evaluated — external validity is limited.",
            "counterexamples_or_strengths": "Despite constraints, human evaluators on average preferred the CBS outputs, implying the LLM-as-a-Judge selection helped produce outputs perceived as more creative.",
            "recommendations_or_best_practices": "Expand experimental evaluation (more participants, broader prompts, larger/more creative LLMs), examine prompt structure impact, consider larger/more diverse candidate sets for the self-evaluation step.",
            "citation": "Creative Beam Search: LLM-as-a-Judge for Improving Response Generation",
            "uuid": "e3859.3",
            "source_info": {
                "paper_title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Self-eval vs DBS overlap",
            "name_full": "Overlap between LLM self-evaluation selection and Diverse Beam Search selection",
            "brief_description": "The authors measured how often the candidate selected by the model's self-evaluation matched the candidate with top DBS score: overlap was 29%, which is lower than the 35.3% expected from random selection in their setup, indicating self-eval often changes the DBS-preferred choice.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Internal comparison between the candidate chosen by the LLM-as-a-Judge and the candidate that DBS would select by its combined likelihood/diversity score.",
            "task_or_domain": "ranking/selection among generated candidates (creative generation)",
            "llm_model_name": "Llama 2 7B (RLHF-tuned)",
            "agreement_rate": "29% overlap (self-eval vs DBS); compared against 35.3% expected from random selection (reported in paper).",
            "qualitative_differences": "Self-evaluation subverted DBS scoring often (i.e., chose different candidates than DBS top-scoring), indicating the LLM judge applies different selection criteria than DBS's likelihood/diversity objective.",
            "limitations_or_failure_cases": "Overlap below random baseline suggests complex interactions — interpretation requires caution; does not directly measure agreement with human preference.",
            "counterexamples_or_strengths": "Lower-than-random overlap indicates the self-evaluation stage is not a trivial/noisy replay of DBS scoring and actively changes selections; combined with human preference results, this suggests the self-eval can improve human-perceived creativity.",
            "recommendations_or_best_practices": "Analyze and report overlap metrics when combining search and self-evaluation to understand how often model judgement diverges from the search objective; consider larger candidate sets to study effect magnitude.",
            "citation": "Creative Beam Search: LLM-as-a-Judge for Improving Response Generation",
            "uuid": "e3859.4",
            "source_info": {
                "paper_title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "Quality-Diversity through AI feedback",
            "rating": 1,
            "sanitized_title": "qualitydiversity_through_ai_feedback"
        }
    ],
    "cost": 0.012535499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Creative Beam Search: LLM-as-a-Judge for Improving Response Generation</p>
<p>Giorgio Franceschelli giorgio.franceschelli@unibo.it 
Mirco Musolesi m.musolesi@ucl.ac.uk </p>
<p>Department of Computer Science and Engineering Alma Mater Studiorum
Università di Bologna</p>
<p>Department of Computer Science
University College London</p>
<p>Department of Computer Science and Engineering Alma Mater Studiorum
Università di Bologna</p>
<p>Creative Beam Search: LLM-as-a-Judge for Improving Response Generation
D92742DA421038CA99981FB5D5DDC30A
Large language models are revolutionizing several areas, including artificial creativity.However, the process of generation in machines profoundly diverges from that observed in humans.In particular, machine generation is characterized by a lack of intentionality and an underlying creative process.We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation.The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques.We also show that the response validation step is a necessary complement to the response generation step.</p>
<p>Introduction</p>
<p>Recent advancements in deep learning have led to a wave of generative models, in particular large language models (LLMs), capable of impacting society at multiple levels (Bommasani et al. 2021).Thanks to the quality of their outputs, the impact of LLMs on creative fields has been substantial (Newton and Dhole 2023; Weidinger et al. 2022).However, LLMs are still far from being creative due to their lack of intentionality (Shanahan 2024) and the absence of a genuinely creative process in their production (Franceschelli and Musolesi 2023).</p>
<p>In this paper, we introduce Creative Beam Search (CBS), a novel generate-and-test sampling scheme designed to artificially replicate certain aspects of the creative process.According to the framework proposed in (Amabile 1983), creativity should involve the following steps: task presentation (from internal or external stimuli); preparation; response generation (thanks to creativity-relevant skills); and response validation (thanks to domain-relevant skills).In particular, CBS first simulates the response generation phase through Diverse Beam Search (DBS) (Vijayakumar et al. 2018), generating a more diverse set of possible solutions.Then, it performs a self-evaluation phase in LLM-as-a-Judge style (Zheng et al. 2023) to select the final output.We evaluate our method against the classic sampling strategy with a qualitative assessment study, finding that endusers find our approach preferable and, on average, CBS (as a generate-and-test approach with DBS) provides better solutions than DBS alone.</p>
<p>The remainder of the article is structured as follows.First, we review the relationship between LLMs and creativity and we introduce the key concepts at the basis of Creative Beam Search.Then, we detail our proposed method and present our qualitative experiment results.Finally, we discuss our findings and the limitations of the proposed approach, and we conclude with final remarks.</p>
<p>Related Work</p>
<p>LLMs and Creativity</p>
<p>The potential impact of LLMs on creative fields has been evident since the advent of GPT models (Brown et al. 2020;OpenAI 2023) and their competitors, e.g.(Touvron et al. 2023).Research has been conducted to determine whether LLMs can pass human creativity tests, such as the Alternate Uses Test (Stevenson et al. 2022), and to explore ways to improve their results (Goes et al. 2023).However, their intrinsic lack of intentionality and consciousness should prevent them from being truly creative (Franceschelli and Musolesi 2023).Another area of research is focused on enhancing the ability of LLMs to generate creative outputs.For example, LLMs can be fine-tuned (Sawicki et al. 2023b) or used in zero-shot settings (Sawicki et al. 2023a) to write in the style of famous authors.Another possibility is to use Reinforcement Learning from Human Feedback (RLHF) (Christiano et al. 2017) to teach an LLM to write haikus that human evaluators would find more creative (Pardinas et al. 2023).Finally, active divergence techniques (Berns and Colton 2020) can also be used.Quality-diversity algorithms can help find more creative solutions by leveraging human feedback (Ding et al. 2023) or AI feedback (Bradley et al. 2023) to measure quality.</p>
<p>Beam Search</p>
<p>Beam Search (Ott et al. 2018) is a text generation strategy that maintains several hypotheses (known as the beam budget B) at each time step and eventually chooses the hypothesis with the overall highest probability under the model.This approach, rather than focusing on single tokens (which can lead to sub-optimal or even degenerated solutions), considers the likelihood of the entire sequence (Caccia et al. 2020).However, Beam Search often focuses on a single highly valued beam, resulting in final candidates that are merely minor variations of a single sequence.Diverse Beam Search (Vijayakumar et al. 2018) proposes to overcome this issue by dividing the beam budget into G groups.It enforces diversity between different groups by penalizing candidates that share tokens with other beams.This guarantees increased diversity in the final solutions.Other variants of Beam Search have been proposed as well, to enforce a certain constraint over the output (Hokamp and Liu 2017) or to substitute the likelihood with a self-evaluation scheme (Xie et al. 2023).</p>
<p>LLM-as-a-Judge</p>
<p>The LLM-as-a-Judge approach involves the LLM evaluating its own responses.(Chiang and Lee 2023;Zheng et al. 2023) show that evaluations from strong LLMs align with those from human experts.However, these evaluations suffer from positional bias, i.e., altering the order of candidate responses can affect their quality ranking (Wang et al. 2023).This new capability has led to the adoption of self-evaluation during training, replacing human feedback for RLHF (Bai et al. 2022;Lee et al. 2023) or for other learning strategies (Chen et al. 2024;Yuan et al. 2024).In addition, LLMas-a-Judge can be applied at inference time.It can guide quality-diversity search algorithms (Bradley et al. 2023) or improve responses for creativity tests (Goes et al. 2023;Summers-Stay, Voss, and Lukin 2023).</p>
<p>Creative Beam Search</p>
<p>Drawing from the componential model of creativity (Amabile 1983), we propose a method, namely Creative Beam Search (CBS), to better simulate (parts of) the human creative process during text generation.In particular, after a task presentation step where an external stimulus is provided in the form of a user prompt and a preparation step where a pre-trained language model is loaded (bringing along the facts and information already acquired), CBS is articulated in two steps: response generation and response validation.The full process is summarized in Figure 1.</p>
<p>Response Generation</p>
<p>During the response generation phase, an individual generates response possibilities by searching through the available pathways, exploring features that are relevant to the task at hand (Amabile 1983).This process requires creativityrelevant skills as well as a method to limit the search to feasible and relevant solutions.</p>
<p>We propose to simulate these aspects using Diverse Beam Search for sequence generation.During beam search, a better collection of options is generated thanks to a diversity penalty.The beam budget B is divided into G groups.At each generation step, the B G solutions for a given group are selected among all possible B G • |V| candidates (where V is the vocabulary).These solutions optimize an objective consisting of two terms: the standard sequence likelihood under the model and a dissimilarity term that encourages diversity across groups.Commonly, Hamming diversity is considered, where each token receives a penalty proportional to the number of times that same token has been selected in other groups at the same step.Therefore, DBS can be seen A prompt is given by the user.
B C A D C D B A D A C B A B D C DBS B A C D B A C D
Figure 1: The Creative Beam Search method.Given a user prompt (step 0), DBS samples K candidate solutions from a pre-trained language model (step 1).Then, K evaluative prompts are composed by altering the order of the candidates and are passed to the model as inputs (step 2).The candidate with the most preferences is finally outputted.</p>
<p>as guided by two forces: the diversity penalty, which represents a simplified creativity-oriented skill, and the likelihood under the model, which helps focus the search to feasible and relevant paths.</p>
<p>Response Validation</p>
<p>During the response validation phase, the response possibilities are tested for quality and appropriateness, using the knowledge and assessment criteria from domain-relevant skills (Amabile 1983).</p>
<p>We propose an explicit self-assessment step that leverages the evaluative capabilities of recent generative models (Lee et al. 2023;Yuan et al. 2024).This involves asking the model to choose among the top K candidates generated by DBS, according to their score.This allows the system to output the solution the model finds to be the best for the task, rather than simply returning the one with the highest combined likelihood and diversity.While (Amabile 1983) suggests evaluating a single response and repeating the entire process if the test is not passed, our method simplifies this by evaluating multiple candidates in a single step.This trade-off allows CBS to maintain short compute times, making it effective for online co-creative purposes.</p>
<p>In practice, CBS uses LLM-as-a-Judge prompting (Zheng et al. 2023) to make the model decide among the generated candidates.To address positional bias, we use the balanced position calibration scheme (Wang et al. 2023).We create K different prompts by rotating the top K candidates, ensuring each candidate is considered in all possible positions.We then aggregate the votes and the candidate with the most preferences is selected.In the event of a tie, the initial order Figure 2: The interface presented to the end-users during our experiment.After inserting a prompt with a creative request, two options are shown in a random order: the CBS output and the standard sampling output.The user is then asked to indicate which is the most creative in their opinion (or if the two options are too similar to decide). of the candidates (i.e., the DBS score) is taken into account.</p>
<p>Experiments</p>
<p>We conducted a qualitative evaluation of Creative Beam Search to assess its potential for co-creativity.Figure 2 shows a screenshot of the interface we used, which was created with Gradio (Abid et al. 2019).</p>
<p>Setup</p>
<p>We chose Llama 2 (Touvron et al. 2023) as our pre-trained language model.Due to resource constraints, we selected the 7B variant and used the RLHF-tuned version, which provides more accurate and coherent responses.We set the beam budget B to 8, divided into single-item groups (i.e., G = 8).The diversity penalty was scaled by a factor of 10 to counterbalance the likelihood score.We then retained the top K = 4 solutions for the evaluation step.For the DBS step, we used the prompt from Algorithm 1; the prompt for self-assessment is detailed in Algorithm 2.</p>
<p>Algorithm 1 Prompt for response generation.</p>
<p>{'role': 'user', 'content': '$INPUT.Provide only one answer without any explanation.'}As mentioned above, we repeated the latter step K = 4 times, each time altering the positions of the candidates.</p>
<p>We limited the model outputs to 256 new tokens.Although this is a significant constraint, we believe it does not impact the final result as differences in creativity should be noticeable even in shorter texts.Lastly, we used a greedy Algorithm 2 Prompt for response validation.</p>
<p>{'role': 'user', 'context': 'Which of the following is the most creative answer to "$INPUT"? 1) $CANDIDATE1 2) $CANDIDATE2 3) $CANDIDATE3 4) $CANDIDATE4 Provide only the number of the most creative answer without any explanation.'}decoding strategy (i.e., always selecting the most probable token) for the self-assessment to prevent the best candidate from being chosen randomly.</p>
<p>Qualitative Results</p>
<p>We carried out a qualitative evaluation involving 31 graduate students in Computer Science.They were given the freedom to input their prompts and were asked to choose between the CBS and the standard output (generated with a temperature of 1.0 and nucleus sampling (Holtzman et al. 2020) with top-p of 0.9).The presentation order of the two solutions was randomized, and the user could also indicate the outputs were too similar to differentiate.</p>
<p>We gathered a total of 217 answers.As reported in Table 1, CBS was preferred 45% of the time, with a significant margin over the standard output.However, in about onefourth of the cases, the responses were too similar to make a choice.This suggests that despite the diversity penalty and self-evaluation step, CBS output does not deviate significantly from standard sampling.We also tracked whether the candidate selected during self-evaluation was the same as the one selected by DBS.The overlap was 29%, which is less than the 35.3% that a random selection would have led to.This indicates that the self-evaluation step was not merely random and has subverted more than confirmed the DBS scoring.</p>
<p>Finally, we also analyze whether there was a difference in user preference for CBS outputs that matched or did not match the DBS outputs.Figure 3 shows the preference proportions for both scenarios.While the differences are not substantial, the standard output was preferred more when compared with the DBS output.This suggests that the final self-evaluation step can further improve Diverse Beam Search.</p>
<p>Discussion</p>
<p>This paper has introduced a new sampling scheme, Creative Beam Search, to tackle the misalignment between the human creative process and how generative models produce their outputs.It leverages recent techniques such as Diverse Beam Search and LLM-as-a-Judge to simulate aspects of response generation and validation.However, it does not address other key aspects as outlined by (Amabile 1983), such as task motivation from internal stimuli and the possibility of iteratively adjusting the responses.Moreover, both Diverse Beam Search and LLM-as-a-Judge have limitations.For instance, Diverse Beam Search uses Hamming diversity, which only considers differences at the same time step.This can lead to overly similar sequences due to minor misalignments such as initial spacing.In addition, it is only applicable to sequence generation tasks and is more expensive than classic decoding strategies.As for LLM-as-a-Judge, it is important to remark that LLMs are not conscious or intentional.Therefore, self-evaluation does not reflect any personal belief but merely returns what the model has learned to be more likely.Consequently, our approach can be considered as an artificial simulation of certain aspects of creativity.Finally, there is a need to extend the experimental evaluation, considering the impact of the prompt structure on the overall results.</p>
<p>Despite these limitations, our qualitative experiment shows that, on average, Creative Beam Search is viewed as a more creative sampling scheme than traditional methods by potential end-users.Furthermore, our results suggest that the self-evaluation step improves the output choice even when considering a small number of candidate solutions from DBS. Future work could explore whether considering a broader and more diverse set of candidates could lead to even better results.Thanks to its simplicity, our method can be easily extended to other, potentially more powerful, LLMs or to models trained with more creativity-oriented strategies.In conclusion, we believe our paper contributes to the growing field of generative learning for computational creativity (Franceschelli and Musolesi 2024).</p>
<p>Figure 3 :
3
Figure 3: Percentage of end-users' preferences comparing when CBS output is equal to DBS output and when it is not.</p>
<p>Table 1 :
1
Aggregate results from our qualitative assessment.
Preference CBS != DBS CBS == DBS TotalCBS.34.11.45STD.18.11.29Same.19.7.26.71.291.00The three possible preferences (CBS for Creative BeamSearch, STD for standard sampling, and Same for whenCBS and STD were too similar to choose) are divided con-sidering whether CBS output is the same as Diverse BeamSearch (DBS) output or not, and in total.1.00.8Proportion0.4 0.6Preference STD SAME CBS0.20.0CBS!=DBSCBS==DBS
AcknowledgmentsThe participation and presentation at ICCC'24 was supported by the ISA Doctoral Prize (ISA DP), offered by Istituto di Studi Avanzati, Alma Mater Studiorum Università di Bologna.Hawkins, W.; Stepleton, T.;Birhane, A.;Hendricks, L. A.;Rimell, L.;Isaac, W.;...;and Gabriel, I. 2022
Abid, arXiv:1906.02569[cs.LG]Gradio: Hassle-free sharing and testing of ML models in the wild. 2019. 2019</p>
<p>The social psychology of creativity: A componential conceptualization. T M Amabile, Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, C Chen, C Olsson, C Olah, D Hernandez, D Drain, D Ganguli, D Li, E Tran-Johnson, E ; . Perez, J Kaplan, arXiv:2212.08073[cs.CL]Constitutional AI: Harmlessness from AI Feedback. 1983. 1983. 202245Amabile. Bai et al. 2022</p>
<p>Bridging generative deep learning and computational creativity. Colton ; Berns, S Berns, S Colton, R Bommasani, D Hudson, E Adeli, R Altman, S Arora, S Arx, M Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N Chatterji, A Chen, K Creel, J Davis, D ; . Demszky, P Liang, arXiv:2108.07258[cs.LG]Proceedings of the 11th International Conference on Computational Creativity (ICCC'20). the 11th International Conference on Computational Creativity (ICCC'20)2020. 2020. 2021On the opportunities and risks of foundation models</p>
<p>Bradley, arXiv:2310.13032[cs.CL]Quality-Diversity through AI feedback. 2023. 2023</p>
<p>Language models are few-shot learners. Brown, Advances in Neural Information Processing Systems (NIPS'20). 2020. 2020</p>
<p>Language GANs falling short. Caccia, Proceedings of the 8th International Conference on Learning Representations (ICLR'20). the 8th International Conference on Learning Representations (ICLR'20)2020. 2020</p>
<p>Chen , arXiv:2401.01335[cs.LG]Self-play fine-tuning converts weak language models to strong language models. 2024. 2024</p>
<p>Can large language models be an alternative to human evaluations?. Lee ; Chiang, C.-H Chiang, H Lee, P F -Y. ; Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL'23). the 61st Annual Meeting of the Association for Computational Linguistics (ACL'23)2023. 2023. 201717Advances in Neural Information Processing Systems</p>
<p>Quality diversity through human feedback. Ding, arXiv:2304.00008[cs.AI]Proceedings of the NeurIPS'23 ALOE Workshop. the NeurIPS'23 ALOE Workshop2023. 2023. 2023On the creativity of large language models</p>
<p>Lexically constrained decoding for sequence generation using grid beam search. Musolesi ; Franceschelli, G Franceschelli, M Musolesi, F Goes, M Volpe, P Sawicki, M Grzés, J Watson, C Hokamp, Q Liu, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL'17). the 55th Annual Meeting of the Association for Computational Linguistics (ACL'17)2024. 2024. 2023. 2017Proceedings of the 14th International Conference on Computational Creativity (ICCC'23)</p>
<p>The curious case of neural text degeneration. Holtzman, Proceedings of the 8th International Conference on Learning Representations (ICLR'20). the 8th International Conference on Learning Representations (ICLR'20)2020. 2020</p>
<p>RLAIF: Scaling reinforcement learning from human feedback with AI feedback. Lee, arXiv:2309.00267[cs.CL]2023. 2023</p>
<p>Is AI art another industrial revolution in the making?. Newton , Dhole ; Newton, A Dhole, K , arXiv:2303.08774[cs.CL]Proceedings of the AAAI'23 Creative AI Across Modalities Workshop. the AAAI'23 Creative AI Across Modalities Workshop2023. 2023Technical ReportOpenAI 2023] OpenAI. 2023. GPT-4</p>
<p>Analyzing uncertainty in neural machine translation. Ott, Proceedings of the 35th International Conference on Machine Learning (ICML'18). the 35th International Conference on Machine Learning (ICML'18)2018. 2018</p>
<p>Bits of Grass: Does GPT already know how to write like Whitman?. Pardinas, Proceedings of the 14th International Conference on Computational Creativity. the 14th International Conference on Computational Creativity2023. 2023. 2023aProceedings of the AAAI'23 Workshop on Creative AI Across Modalities. ICCC'23</p>
<p>On the power of specialpurpose GPT models to create and evaluate new poetry in old styles. Sawicki, Proc. of the 14th International Conference on Computational Creativity. of the 14th International Conference on Computational Creativity2023b. 2023bICCC'23</p>
<p>Putting GPT-3's creativity to the (Alternative Uses) Test. M Shanahan ; Shanahan, C Stevenson, I Smal, M Baas, R Grasman, H Van Der Maas, Proceedings of the 13th International Conference on Computational Creativity. the 13th International Conference on Computational Creativity2024. 202267Talking about large language models. ICCC'22</p>
<p>Brainstorm, then select: a generative language model improves its creativity score. Summers-Stay, Lukin ; Voss, D Summers-Stay, C R Voss, S M Lukin, H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W ; Fu, arXiv:2307.09288[cs.CL]Proceedings of the AAAI'23 Workshop on Creative AI Across Modalities. the AAAI'23 Workshop on Creative AI Across Modalities2023. 2023Touvron et al. 2023. and Scialom, T. 2023. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Diverse beam search for improved description of complex scenes. Vijayakumar, arXiv:2305.17926[cs.CL]Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI'18). the 32nd AAAI Conference on Artificial Intelligence (AAAI'18)2018. 2018. 2023Large language models are not fair evaluators</p>
<p>. Weidinger, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5681 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5681</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5681</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270199477</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.20441v4.pdf" target="_blank">SECURE: Benchmarking Large Language Models for Cybersecurity</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated potential in cybersecurity applications but have also caused lower confidence due to problems like hallucinations and a lack of truthfulness. Existing benchmarks provide general evaluations but do not sufficiently address the practical and applied aspects of LLM performance in cybersecurity-specific tasks. To address this gap, we introduce the SECURE (Security Extraction, Understanding & Reasoning Evaluation), a benchmark designed to assess LLMs performance in realistic cybersecurity scenarios. SECURE includes six datasets focused on the Industrial Control System sector to evaluate knowledge extraction, understanding, and reasoning based on industry-standard sources. Our study evaluates seven state-of-the-art models on these tasks, providing insights into their strengths and weaknesses in cybersecurity contexts. We also offer recommendations for improving LLMs reliability as cyber advisory tools and release our benchmark datasets and framework for community use at https://github.com/aiforsec/SECURE.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5681.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5681.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCQ-prompt-format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple-choice question prompt format (single-letter answer with 'X' for unknown)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized MCQ prompt used for MAET and CWET that instructs the model to return only the letter (A/B/C/D) or X if it does not know the answer; used to enforce deterministic output format across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (ChatGPT-4, ChatGPT-3.5, Gemini-Pro, Llama3-70B, Llama3-8B, Mistral-7B, Mixtral-8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MAET & CWET (MCQ extraction tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge extraction tasks framed as multiple-choice questions derived from MITRE ATT&CK and CWE for ICS cybersecurity; models must pick the correct option or 'X' if unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single consistent prompt per MCQ: provide question and four options then: 'pick the best option and return as either A, B, C or D. If you do not know the answer, return X.' Return only the letter without any additional text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported per-model accuracies under the MCQ format (Table V): ChatGPT-4 MAET 88.6%, CWET 89.6%; ChatGPT-3.5 MAET 82.8%, CWET 84.2%; Gemini-Pro MAET 86.2%, CWET 87.8%; Llama3-70B MAET 86.3%, CWET 90.4%; Llama3-8B MAET 82.1%, CWET 83.9%; Mistral-7B MAET 77.9%, CWET 80.1%; Mixtral-8x7B MAET 80.9%, CWET 83.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>baseline (no alternative format comparison in primary MCQ eval)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Using a strict output format reduces variability in post-processing and enforces comparability across models; however some models occasionally deviated from the required output and outputs were manually corrected for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SECURE: Benchmarking Large Language Models for Cybersecurity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5681.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5681.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context-vs-NoContext</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context inclusion in prompts (KCV with JSON context) versus no-context prompts (VOOD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Directly embedding relevant CVE JSON context into prompts (KCV) versus presenting the same statements without context (VOOD) to test how context/presentation affects comprehension, hallucination, and out-of-distribution behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (ChatGPT-4, ChatGPT-3.5, Gemini-Pro, Llama3-70B, Llama3-8B, Mistral-7B, Mixtral-8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KCV (contextual True/False) vs VOOD (no-context True/False)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>KCV: Boolean (T/F/X) questions where the CVE JSON is embedded in the prompt; VOOD: the same statements without any supporting context to evaluate out-of-distribution recognition and tendency to hallucinate.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>KCV: prompt includes the CVE JSON as context (embedded within input); ask model to return T, F, or X. VOOD: prompt contains only the statement and asks for T/F/X with no supporting context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>KCV (context provided) vs VOOD (no context; out-of-distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per Table V: ChatGPT-4 KCV 87.6% vs VOOD 87.9% (negligible change); ChatGPT-3.5 KCV 78.4% vs VOOD 8.4% (large drop); Gemini-Pro KCV 83.5% vs VOOD 6.7% (large drop); Llama3-70B KCV 85.2% vs VOOD 7.1% (large drop); Llama3-8B KCV 82.8% vs VOOD 6.4%; Mistral-7B KCV 64.2% vs VOOD 7.1%; Mixtral-8x7B KCV 83.4% vs VOOD 9.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Many models show large accuracy decreases when context is removed (example deltas: ChatGPT-3.5 −70.0 pp, Llama3-70B −78.1 pp, Gemini-Pro −76.8 pp), while ChatGPT-4 shows almost no change.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Model-dependent; examples: ChatGPT-3.5 −70.0 percentage points (KCV→VOOD), Llama3-70B −78.1 pp, Gemini-Pro −76.8 pp, ChatGPT-4 −0.3 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>in general, removing context reduced performance (reduced); exceptions exist (ChatGPT-4 stable).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>VOOD was intentionally designed to be unanswerable without context; many models tend to 'agree' with plausible-sounding statements when missing context (hallucination). The presence of explicit context helps models verify or refute claims, but some models still fail to leverage context effectively, producing different error modes (VOOD: agreement/hallucination; KCV: increased disagree errors). Closed models with later training cutoffs or stronger safeguards (e.g., ChatGPT-4) better recognize lack of context and abstain or avoid hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>ChatGPT-4 showed nearly identical KCV and VOOD accuracy (87.6% vs 87.9%), demonstrating that removing embedded context did not degrade performance for that model on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SECURE: Benchmarking Large Language Models for Cybersecurity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5681.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5681.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-by-step-explanations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit request for detailed step-by-step explanations (chain-of-thought style prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Asking models to provide a detailed explanation of how they arrived at an answer (explicit chain-of-thought) to evaluate whether forcing reasoning steps improves correctness on reasoning and comprehension tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4 and Llama3-70B (evaluated explicitly)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>ChatGPT-4: unspecified, Llama3-70B: 70.6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KCV (understanding) and CPST (CVSS problem-solving)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>KCV: determine T/F based on CVE context; CPST: compute a numerical CVSS v3 score from a vector string (0–10).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Baseline prompt vs modified prompt that includes: 'Provide a detailed explanation of how you arrived at the answer.' (forces step-by-step or chain-of-thought reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline (no explanation) vs With detailed explanation (step-by-step).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Improvements observed (Figure 5 / Section V-D): KCV accuracy improvement — ChatGPT-4 +2.52 percentage points, Llama3-70B +4.43 pp. CPST (MAD metric) improvement — ChatGPT-4 MAD improved by 20.99% (lower MAD), Llama3-70B MAD improved by 14.29%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Baseline KCV vs reasoning KCV: ChatGPT-4 +2.52% accuracy; Llama3-70B +4.43% accuracy. Baseline CPST vs reasoning CPST: ChatGPT-4 MAD reduced by 20.99%; Llama3-70B MAD reduced by 14.29%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>KCV: +2.52 pp (ChatGPT-4), +4.43 pp (Llama3-70B). CPST: MAD improvement 20.99% (ChatGPT-4), 14.29% (Llama3-70B).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicitly requesting explanations elicits internal chain-of-thought-like reasoning that helps models perform more accurate analytic steps for comprehension and numeric problem-solving; the trade-off is substantially increased inference time and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>N/A — improvement observed for the two evaluated models/tasks; however, the paper notes a computational trade-off (inference time up to ~35×) that may make the format impractical in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SECURE: Benchmarking Large Language Models for Cybersecurity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5681.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5681.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confidence-elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asking models for answer probability and varying temperature to assess confidence-accuracy relationship</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting models to return their answer along with a numeric probability (0%–100%) and averaging across multiple temperature settings to study calibration and the relation between self-reported confidence and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4 and Llama3-70B (explicitly analyzed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>ChatGPT-4: unspecified, Llama3-70B: 70.6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CWET (extraction / MCQ) with confidence reporting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CWET MCQ prompts where the model is asked to provide its predicted answer and a probability that the answer is correct; temperature varied across runs (0.6, 0.7, 0.8, 0.9, 1.0) and confidences averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt appended with: 'Provide your answer and the probability that the answer is correct (0% to 100%) separated by a space.' Evaluate accuracy by binning predicted confidences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple temperature settings (0.6–1.0) to obtain robust confidence estimates; compare confidence bins to observed accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative finding: as reported in Section V-B / Figure 3, accuracy decreases with lower self-reported confidence — especially in the lowest confidence bins. ChatGPT-4 consistently reported higher confidence and higher accuracy than Llama3-70B in these experiments (specific bin numbers not tabulated).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>elicited confidence correlates with accuracy (useful for calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Self-reported probabilities correlate with empirical correctness: lower reported confidence corresponds to lower accuracy. The paper suggests model-specific calibration methods can exploit this to reduce incorrect responses (e.g., abstain or escalate when confidence low).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SECURE: Benchmarking Large Language Models for Cybersecurity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5681.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5681.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-vs-Finetune</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) vs Fine-tuning vs Base model formatting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of three presentation/architecture formats for Llama3-8B: (A) base model (no retrieval or fine-tuning), (B) RAG where external documents are retrieved and embedded in prompts, and (C) instruction fine-tuning on ICS-specific data; evaluated on MAET, CWET, and KCV.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B (base), RAG-Llama3-8B, Fine-tuned Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MAET, CWET (MCQ) and KCV (contextual True/False)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how augmenting prompts via retrieval or domain fine-tuning changes performance relative to base weights when answering MCQs and contextual boolean questions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Base: standard prompt without external retrieval; RAG: conversational retrieval chain returns retrieved document chunks to generator and these are included as context; Fine-tuned: model weights updated via instruction fine-tuning on domain-specific ICS instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base vs RAG vs Fine-tuned (Table VII)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table VII (Llama3-8B variants): Base MAET 82.1%, CWET 83.9%, KCV 82.8%. RAG MAET 86.6% (+4.5 pp), CWET 77.3% (−6.6 pp), KCV 60.5% (−22.3 pp). Fine-tuned MAET 84.0% (+1.9 pp), CWET 85.0% (+1.1 pp), KCV 76.0% (−6.8 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>RAG improved MAET but degraded CWET and dramatically degraded KCV; fine-tuning yielded modest gains on MAET/CWET and smaller drop on KCV compared to base.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>RAG vs Base: MAET +4.5 pp, CWET −6.6 pp, KCV −22.3 pp. Fine-tuned vs Base: MAET +1.9 pp, CWET +1.1 pp, KCV −6.8 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed — RAG can improve or reduce performance depending on task; fine-tuning gave moderate improvements on extraction tasks but decreased KCV accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>RAG's retrieval stage can return irrelevant chunks and miss crucial context, leading to hallucinations when the retrieved context does not support correct answers; fine-tuning improves domain knowledge for some extraction tasks but may still harm understanding of fresh CVE-contexts (KCV). The authors hypothesize retrieval noise as a key cause of RAG's poor KCV performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SECURE: Benchmarking Large Language Models for Cybersecurity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5681.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5681.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-length-constraint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context embedding length constraint (max embedded context ~5000 words)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practical formatting constraint: when embedding JSON CVE documents as prompt context (for KCV), only those examples whose combined prompt+context length did not exceed 5000 words were included to avoid token limits and ensure model compatibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KCV (contextual True/False)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Boolean questions derived from CVE JSONs embedded into prompts; only CVE items short enough to keep prompt <= 5000 words were used.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Embedding raw JSON context into model prompt subject to a length cap (selected subset with prompt length <= 5000 words).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Constraint reported qualitatively; no direct per-model performance delta caused by inclusion/exclusion of longer-context examples is quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>constraining (limits which examples can be evaluated in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Token/input length limits force selection of shorter contexts which may bias which CVEs are included and can affect external validity; long contextual documents could not be provided directly in a single prompt without truncation or advanced retrieval approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SECURE: Benchmarking Large Language Models for Cybersecurity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Text and patterns: For effective chain of thought, it takes two to tango <em>(Rating: 2)</em></li>
                <li>Prometheus: Inducing evaluation capability in language models <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Does fine-tuning llms on new knowledge encourage hallucinations? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5681",
    "paper_id": "paper-270199477",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "MCQ-prompt-format",
            "name_full": "Multiple-choice question prompt format (single-letter answer with 'X' for unknown)",
            "brief_description": "A standardized MCQ prompt used for MAET and CWET that instructs the model to return only the letter (A/B/C/D) or X if it does not know the answer; used to enforce deterministic output format across models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various (ChatGPT-4, ChatGPT-3.5, Gemini-Pro, Llama3-70B, Llama3-8B, Mistral-7B, Mixtral-8x7B)",
            "model_size": "various",
            "task_name": "MAET & CWET (MCQ extraction tasks)",
            "task_description": "Knowledge extraction tasks framed as multiple-choice questions derived from MITRE ATT&CK and CWE for ICS cybersecurity; models must pick the correct option or 'X' if unknown.",
            "problem_format": "Single consistent prompt per MCQ: provide question and four options then: 'pick the best option and return as either A, B, C or D. If you do not know the answer, return X.' Return only the letter without any additional text.",
            "comparison_format": null,
            "performance": "Reported per-model accuracies under the MCQ format (Table V): ChatGPT-4 MAET 88.6%, CWET 89.6%; ChatGPT-3.5 MAET 82.8%, CWET 84.2%; Gemini-Pro MAET 86.2%, CWET 87.8%; Llama3-70B MAET 86.3%, CWET 90.4%; Llama3-8B MAET 82.1%, CWET 83.9%; Mistral-7B MAET 77.9%, CWET 80.1%; Mixtral-8x7B MAET 80.9%, CWET 83.4%.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "baseline (no alternative format comparison in primary MCQ eval)",
            "explanation_or_hypothesis": "Using a strict output format reduces variability in post-processing and enforces comparability across models; however some models occasionally deviated from the required output and outputs were manually corrected for evaluation.",
            "counterexample_or_null_result": null,
            "uuid": "e5681.0",
            "source_info": {
                "paper_title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Context-vs-NoContext",
            "name_full": "Context inclusion in prompts (KCV with JSON context) versus no-context prompts (VOOD)",
            "brief_description": "Directly embedding relevant CVE JSON context into prompts (KCV) versus presenting the same statements without context (VOOD) to test how context/presentation affects comprehension, hallucination, and out-of-distribution behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various (ChatGPT-4, ChatGPT-3.5, Gemini-Pro, Llama3-70B, Llama3-8B, Mistral-7B, Mixtral-8x7B)",
            "model_size": "various",
            "task_name": "KCV (contextual True/False) vs VOOD (no-context True/False)",
            "task_description": "KCV: Boolean (T/F/X) questions where the CVE JSON is embedded in the prompt; VOOD: the same statements without any supporting context to evaluate out-of-distribution recognition and tendency to hallucinate.",
            "problem_format": "KCV: prompt includes the CVE JSON as context (embedded within input); ask model to return T, F, or X. VOOD: prompt contains only the statement and asks for T/F/X with no supporting context.",
            "comparison_format": "KCV (context provided) vs VOOD (no context; out-of-distribution)",
            "performance": "Per Table V: ChatGPT-4 KCV 87.6% vs VOOD 87.9% (negligible change); ChatGPT-3.5 KCV 78.4% vs VOOD 8.4% (large drop); Gemini-Pro KCV 83.5% vs VOOD 6.7% (large drop); Llama3-70B KCV 85.2% vs VOOD 7.1% (large drop); Llama3-8B KCV 82.8% vs VOOD 6.4%; Mistral-7B KCV 64.2% vs VOOD 7.1%; Mixtral-8x7B KCV 83.4% vs VOOD 9.6%.",
            "performance_comparison": "Many models show large accuracy decreases when context is removed (example deltas: ChatGPT-3.5 −70.0 pp, Llama3-70B −78.1 pp, Gemini-Pro −76.8 pp), while ChatGPT-4 shows almost no change.",
            "format_effect_size": "Model-dependent; examples: ChatGPT-3.5 −70.0 percentage points (KCV→VOOD), Llama3-70B −78.1 pp, Gemini-Pro −76.8 pp, ChatGPT-4 −0.3 pp.",
            "format_effect_direction": "in general, removing context reduced performance (reduced); exceptions exist (ChatGPT-4 stable).",
            "explanation_or_hypothesis": "VOOD was intentionally designed to be unanswerable without context; many models tend to 'agree' with plausible-sounding statements when missing context (hallucination). The presence of explicit context helps models verify or refute claims, but some models still fail to leverage context effectively, producing different error modes (VOOD: agreement/hallucination; KCV: increased disagree errors). Closed models with later training cutoffs or stronger safeguards (e.g., ChatGPT-4) better recognize lack of context and abstain or avoid hallucination.",
            "counterexample_or_null_result": "ChatGPT-4 showed nearly identical KCV and VOOD accuracy (87.6% vs 87.9%), demonstrating that removing embedded context did not degrade performance for that model on this dataset.",
            "uuid": "e5681.1",
            "source_info": {
                "paper_title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Step-by-step-explanations",
            "name_full": "Explicit request for detailed step-by-step explanations (chain-of-thought style prompting)",
            "brief_description": "Asking models to provide a detailed explanation of how they arrived at an answer (explicit chain-of-thought) to evaluate whether forcing reasoning steps improves correctness on reasoning and comprehension tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4 and Llama3-70B (evaluated explicitly)",
            "model_size": "ChatGPT-4: unspecified, Llama3-70B: 70.6B",
            "task_name": "KCV (understanding) and CPST (CVSS problem-solving)",
            "task_description": "KCV: determine T/F based on CVE context; CPST: compute a numerical CVSS v3 score from a vector string (0–10).",
            "problem_format": "Baseline prompt vs modified prompt that includes: 'Provide a detailed explanation of how you arrived at the answer.' (forces step-by-step or chain-of-thought reasoning).",
            "comparison_format": "Baseline (no explanation) vs With detailed explanation (step-by-step).",
            "performance": "Improvements observed (Figure 5 / Section V-D): KCV accuracy improvement — ChatGPT-4 +2.52 percentage points, Llama3-70B +4.43 pp. CPST (MAD metric) improvement — ChatGPT-4 MAD improved by 20.99% (lower MAD), Llama3-70B MAD improved by 14.29%.",
            "performance_comparison": "Baseline KCV vs reasoning KCV: ChatGPT-4 +2.52% accuracy; Llama3-70B +4.43% accuracy. Baseline CPST vs reasoning CPST: ChatGPT-4 MAD reduced by 20.99%; Llama3-70B MAD reduced by 14.29%.",
            "format_effect_size": "KCV: +2.52 pp (ChatGPT-4), +4.43 pp (Llama3-70B). CPST: MAD improvement 20.99% (ChatGPT-4), 14.29% (Llama3-70B).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Explicitly requesting explanations elicits internal chain-of-thought-like reasoning that helps models perform more accurate analytic steps for comprehension and numeric problem-solving; the trade-off is substantially increased inference time and computational cost.",
            "counterexample_or_null_result": "N/A — improvement observed for the two evaluated models/tasks; however, the paper notes a computational trade-off (inference time up to ~35×) that may make the format impractical in some settings.",
            "uuid": "e5681.2",
            "source_info": {
                "paper_title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Confidence-elicitation",
            "name_full": "Asking models for answer probability and varying temperature to assess confidence-accuracy relationship",
            "brief_description": "Prompting models to return their answer along with a numeric probability (0%–100%) and averaging across multiple temperature settings to study calibration and the relation between self-reported confidence and accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4 and Llama3-70B (explicitly analyzed)",
            "model_size": "ChatGPT-4: unspecified, Llama3-70B: 70.6B",
            "task_name": "CWET (extraction / MCQ) with confidence reporting",
            "task_description": "CWET MCQ prompts where the model is asked to provide its predicted answer and a probability that the answer is correct; temperature varied across runs (0.6, 0.7, 0.8, 0.9, 1.0) and confidences averaged.",
            "problem_format": "Prompt appended with: 'Provide your answer and the probability that the answer is correct (0% to 100%) separated by a space.' Evaluate accuracy by binning predicted confidences.",
            "comparison_format": "Multiple temperature settings (0.6–1.0) to obtain robust confidence estimates; compare confidence bins to observed accuracy.",
            "performance": "Qualitative finding: as reported in Section V-B / Figure 3, accuracy decreases with lower self-reported confidence — especially in the lowest confidence bins. ChatGPT-4 consistently reported higher confidence and higher accuracy than Llama3-70B in these experiments (specific bin numbers not tabulated).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "elicited confidence correlates with accuracy (useful for calibration)",
            "explanation_or_hypothesis": "Self-reported probabilities correlate with empirical correctness: lower reported confidence corresponds to lower accuracy. The paper suggests model-specific calibration methods can exploit this to reduce incorrect responses (e.g., abstain or escalate when confidence low).",
            "counterexample_or_null_result": null,
            "uuid": "e5681.3",
            "source_info": {
                "paper_title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "RAG-vs-Finetune",
            "name_full": "Retrieval-Augmented Generation (RAG) vs Fine-tuning vs Base model formatting",
            "brief_description": "Comparison of three presentation/architecture formats for Llama3-8B: (A) base model (no retrieval or fine-tuning), (B) RAG where external documents are retrieved and embedded in prompts, and (C) instruction fine-tuning on ICS-specific data; evaluated on MAET, CWET, and KCV.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B (base), RAG-Llama3-8B, Fine-tuned Llama3-8B",
            "model_size": "8B",
            "task_name": "MAET, CWET (MCQ) and KCV (contextual True/False)",
            "task_description": "Evaluate how augmenting prompts via retrieval or domain fine-tuning changes performance relative to base weights when answering MCQs and contextual boolean questions.",
            "problem_format": "Base: standard prompt without external retrieval; RAG: conversational retrieval chain returns retrieved document chunks to generator and these are included as context; Fine-tuned: model weights updated via instruction fine-tuning on domain-specific ICS instructions.",
            "comparison_format": "Base vs RAG vs Fine-tuned (Table VII)",
            "performance": "Table VII (Llama3-8B variants): Base MAET 82.1%, CWET 83.9%, KCV 82.8%. RAG MAET 86.6% (+4.5 pp), CWET 77.3% (−6.6 pp), KCV 60.5% (−22.3 pp). Fine-tuned MAET 84.0% (+1.9 pp), CWET 85.0% (+1.1 pp), KCV 76.0% (−6.8 pp).",
            "performance_comparison": "RAG improved MAET but degraded CWET and dramatically degraded KCV; fine-tuning yielded modest gains on MAET/CWET and smaller drop on KCV compared to base.",
            "format_effect_size": "RAG vs Base: MAET +4.5 pp, CWET −6.6 pp, KCV −22.3 pp. Fine-tuned vs Base: MAET +1.9 pp, CWET +1.1 pp, KCV −6.8 pp.",
            "format_effect_direction": "mixed — RAG can improve or reduce performance depending on task; fine-tuning gave moderate improvements on extraction tasks but decreased KCV accuracy.",
            "explanation_or_hypothesis": "RAG's retrieval stage can return irrelevant chunks and miss crucial context, leading to hallucinations when the retrieved context does not support correct answers; fine-tuning improves domain knowledge for some extraction tasks but may still harm understanding of fresh CVE-contexts (KCV). The authors hypothesize retrieval noise as a key cause of RAG's poor KCV performance.",
            "counterexample_or_null_result": null,
            "uuid": "e5681.4",
            "source_info": {
                "paper_title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Prompt-length-constraint",
            "name_full": "Context embedding length constraint (max embedded context ~5000 words)",
            "brief_description": "Practical formatting constraint: when embedding JSON CVE documents as prompt context (for KCV), only those examples whose combined prompt+context length did not exceed 5000 words were included to avoid token limits and ensure model compatibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various",
            "model_size": "various",
            "task_name": "KCV (contextual True/False)",
            "task_description": "Boolean questions derived from CVE JSONs embedded into prompts; only CVE items short enough to keep prompt &lt;= 5000 words were used.",
            "problem_format": "Embedding raw JSON context into model prompt subject to a length cap (selected subset with prompt length &lt;= 5000 words).",
            "comparison_format": null,
            "performance": "Constraint reported qualitatively; no direct per-model performance delta caused by inclusion/exclusion of longer-context examples is quantified in the paper.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "constraining (limits which examples can be evaluated in-context)",
            "explanation_or_hypothesis": "Token/input length limits force selection of shorter contexts which may bias which CVEs are included and can affect external validity; long contextual documents could not be provided directly in a single prompt without truncation or advanced retrieval approaches.",
            "counterexample_or_null_result": null,
            "uuid": "e5681.5",
            "source_info": {
                "paper_title": "SECURE: Benchmarking Large Language Models for Cybersecurity",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Text and patterns: For effective chain of thought, it takes two to tango",
            "rating": 2,
            "sanitized_title": "text_and_patterns_for_effective_chain_of_thought_it_takes_two_to_tango"
        },
        {
            "paper_title": "Prometheus: Inducing evaluation capability in language models",
            "rating": 1,
            "sanitized_title": "prometheus_inducing_evaluation_capability_in_language_models"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Does fine-tuning llms on new knowledge encourage hallucinations?",
            "rating": 2,
            "sanitized_title": "does_finetuning_llms_on_new_knowledge_encourage_hallucinations"
        }
    ],
    "cost": 0.016896750000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SECURE: Benchmarking Large Language Models for Cybersecurity</p>
<p>Dipkamal Bhusal 
MdTanvirul Alam 
Rd Le Nguyen 
Benjamin A Blakely bblakely@anl.gov 
Nidhi Rastogi </p>
<p>Rochester Institute of Technology Rochester
USA</p>
<p>Rochester Institute of Technology Rochester
USA</p>
<p>Rochester Institute of Technology Rochester
USA</p>
<p>Argonne National Lab Lemont
USA</p>
<p>Rochester Institute of Technology (RIT) Rochester
USA</p>
<p>SECURE: Benchmarking Large Language Models for Cybersecurity
5DC90FC82B177899FCE01FFB38CA71FCLarge Language ModelsCybersecurityBenchmarkingDatasetIndustrial Control System
Large Language Models (LLMs) have demonstrated potential in cybersecurity applications but have also caused lower confidence due to problems like hallucinations and a lack of truthfulness.Existing benchmarks provide general evaluations but do not sufficiently address the practical and applied aspects of LLM performance in cybersecurity-specific tasks.To address this gap, we introduce the SECURE (Security Extraction, Understanding &amp; Reasoning Evaluation), a benchmark designed to assess LLMs performance in realistic cybersecurity scenarios.SECURE includes six datasets focused on the Industrial Control System sector to evaluate knowledge extraction, understanding, and reasoning based on industry-standard sources.Our study evaluates seven state-of-the-art models on these tasks, providing insights into their strengths and weaknesses in cybersecurity contexts.We also offer recommendations for improving LLMs reliability as cyber advisory tools and release our benchmark datasets and framework for community use at https://github.com/aiforsec/SECURE.</p>
<p>I. INTRODUCTION</p>
<p>Recent breakthroughs in large language models (LLM) like OpenAI's ChatGPT [1] have opened up their applications in many domains, including security [2].These models, trained on vast datasets, can</p>
<p>The work presented in this paper was partially supported by the U.S. Department of Energy, Office of Science under DOE contract number DE-AC02-06CH11357.The submitted manuscript has been created by UChicago Argonne, LLC, operator of Argonne National Laboratory.Argonne, a DOE Office of Science laboratory, is operated under Contract No. DE-AC02-06CH11357.The U.S. Government retains for itself, and others acting on its behalf, a paidup nonexclusive, irrevocable worldwide license in said article to reproduce, prepare derivative works, distribute copies to the public, and perform publicly and display publicly, by or on behalf of the Government.</p>
<ul>
<li>Equal contribution.generate, understand, and reason across a multitude of domains [3].Users can interact with these models in a conversational style through a simple web interface and obtain answers to their questions in a short time.Despite demonstrating high potential, LLMs are plagued by issues such as hallucinations [4] and truthfulness [5].Hence, a standard evaluation benchmark is crucial to evaluate the reliability of such models.Several benchmarks like GLUE [6], MMLU [7], Helm [8] and KOLA [9], provide standard datasets and tasks to evaluate general-purpose understanding and capabilities of LLMs.GLUE assesses LLMs' performance in understanding language, while MMLU and HELM offer a holistic evaluation across various domains like mathematics, history, computer science, and law.KOLA focuses on tasks designed to measure the cognitive abilities of LLMs.Despite these advancements, there remains a significant gap in the evaluation of LLMs specifically tailored for security industries such as information security, network security, and critical infrastructure protection.</li>
</ul>
<p>Knowledge-modeling Security Texts</p>
<p>From the following document, generate a set of 5 MCQs with four options, its correct answer and explanation on the correct answer.</p>
<p>MAET KCV RERT</p>
<p>Human Validation</p>
<p>GPT-4o Example prompt</p>
<p>Given the following statement, state whether it is 'TRUE' or 'FALSE'.Traditional benchmarks often fail to capture the practical and applied aspects of cybersecurity, leading to an incomplete assessment of LLM capabilities.These benchmarks typically focus on general language tasks and do not address specific challenges such as recognizing emerging threats, handling specialized terminology, or performing tasks like vulnerability assessment and incident response.More practical, domainspecific, and comprehensive evaluations are necessary to understand LLM performance in realistic cybersecurity scenarios [10].In Figure 1, we demonstrate a conversation between ChatGPT-3.5 and a user based on our benchmark dataset (see Section III).ChatGPT-3.5 model was prompted to act as a security expert, and yet, all three responses, spanning different kinds of tasks, were incorrect, showing the unreliability of these models in cybersecurity.Especially, on the first task, we ask questions about vulnerabilities discovered in 2024.Even though the model is not trained on any such recent data, it confidently makes a decision, despite our instructions to return 'X' when it is not confident.</p>
<p>To address this gap, we introduce a comprehensive benchmarking framework encompassing realworld cybersecurity scenarios, practical tasks, and applied knowledge assessments.We introduce Security ExtraCtion, Understanding &amp; Reasoning Evaluation (SECURE) for large language models.This benchmark provides a holistic evaluation of LLMs performance in cybersecurity applications, ensuring they meet the high standards required for deployment in critical infrastructure environments, such as Industrial Control Systems (ICS).Figure 2 shows an overview of our proposed benchmark.SECURE incorporates knowledge modeling to design six different datasets (MAET: Mitre Attack Extraction Task, CWET: Common Weakness Extraction Task, KCV: Knowledge test on Common Vulnerabilities, VOOD: Vulnerability Out-of-Distribution task, RERT: Risk Evaluation Reasoning Task, and CPST: CVSS Problem Solving Task) focused on three types of knowledge evaluations: extraction, understanding, and reasoning.These datasets are sourced from standard sources, such as MITRE [11] [12], CVE [13], CWE [14], and Cybersecurity and Infrastructure Security Agency (CISA) [15].</p>
<p>Main Contributions</p>
<p>1) Assessment of Cybersecurity Knowledge: We evaluate the cybersecurity knowledge of LLMs in terms of their capacity to assist security analysts.Specifically, we design a set of benchmark tasks tailored for the evaluation of LLMs within the context of Industrial Control Systems (ICS) cyber advisories (see Section III).These tasks are grounded in comprehensive protocol specifications, which include communication standards, Common Vulnerabilities and Exposures (CVEs) with contextual details on exploitability and severity, and remediation strategies, such as patch notes and mitigation steps informed by the MITRE framework.2) Evaluation of LLMs: Recent works have shown that customized LLMs introduce a loss of safety measures [16], &amp; increased hallucinations [17].In addition, the computational and technical requirements of building customized LLMs can push researchers and industries to use commercial LLMs.However, before we can confidently claim the application of general-purpose LLMs in a specific domain, we first need to evaluate it on domain-specific tasks.To this end, we assess the performance of seven state-of-the-art, open-source, and proprietary LLMs on our cybersecurity benchmark tasks (see Section IV).These models include ChatGPT-4 [1], ChatGPT-3.5 [18], Llama3-70B [19], Llama3-8B [20], Gemini-Pro [21], Mistral-7B [22], and Mixtral-8x7B [23].Following this, we customize Llama3-8B using the Retrieval-Augmented Generation (RAG) framework and fine-tuning and reevaluate its performance on the benchmark (see Section VI). 3) Insights and Recommendations: Our evaluation reveals that, although LLMs exhibit some competence in cybersecurity tasks, their application as advisory tools demands careful consideration.We present key insights and offer recommendations to improve their practical usability (see Section V).</p>
<p>II. BACKGROUND AND RELATED WORK</p>
<p>Large Language Model (LLM) in Security: LLMs, trained on vast amounts of textual data, are capable of producing coherent and contextually relevant text.While earlier BERT-language models like SecureBERT [24] and CySecBERT [25] were used for language modeling in cybersecurity, the release of GPT (Generative Pre-trained Transformer) models has changed the nature of language models.There are two types of LLMs in the industry.Open Source LLMs like Llama [19] and Mixtral [22] make their models public so they can be fine-tuned for specific downstream tasks.Closed-source LLMs like ChatGPT [1] and Gemini [26] allow restricted access through APIs.The most notable language models in security are code-based LLMs such as CodeLlama [27], adapted to analyze and generate secure code.Opensourced models have also been fine-tuned for several cybersecurity tasks like vulnerability detection [28]- [31], program repair [32], IT operations [33], and security knowledge assistance [34].</p>
<p>Evaluation Benchmark: While there are various benchmarks like GLUE [6], MMLU [7], Helm [8] and KOLA [9] for evaluating general-purpose LLMs, comprehensive cybersecurity-specific benchmarks remain limited.Existing approaches to evaluating LLMs in security tend to focus on factual knowledge rather than applied, practical cybersecurity tasks.For example: the CyberMetric dataset [35] comprises 10,000 questions sourced from cybersecurity standards.Cy-berBench [36] comprises ten datasets from different tasks namely, named-entity recognition, summarization, multiple choice, and classification.SecEval [37] evaluates cybersecurity knowledge in LLMs with 2000 multiple-choice questions across Software Security, Application Security, System Security, Web Security, Cryptography, Memory Safety, Network Security, and PenTest.SecQA [38] consists of multiple-choice questions based on the "Computer Systems Security: Planning for Success" textbook to evaluate LLM's understanding of security principles.NetEval [39] evaluates the knowledge of large language models in IT operational tasks within a multilingual context.OpsEval [40] also contains multi-choice questions designed for fault root cause analysis, operational script generation, and alert information summarization.Ullah et al. [41] tests LLMs on identifying and reasoning about software vulnerabilities using 228 code scenarios.Their findings suggest existing LLMs are unreliable in identifying vulnerabilities in source code.CTIBench [42] is a benchmark of four tasks to evaluate the ability of LLMs in cyber threat intelligence (CTI) landscape.</p>
<p>III. PROPOSED BENCHMARK: SECURE</p>
<p>Consider an LLM as a cybersecurity advisor in an organization facing diverse threats, from malware to advanced persistent threats.To mitigate these risks, cybersecurity professionals must remain constantly informed about threat intelligence, security best practices, and incident response strategies specific to their organization.LLMs have the potential to leverage their vast knowledge base and natural language processing capabilities and assist security teams in identifying vulnerabilities, interpreting threat reports, and suggesting proactive measures to fortify an organization's defenses.</p>
<p>However, the truthfulness and reliability of the information provided by the LLMs acting as cybersecurity advisors is crucial.To achieve this, we propose Security ExtraCtion, Understanding &amp; Reasoning Evaluation (SECURE) benchmark.SECURE can comprehensively evaluate LLMs, ensuring they can be trusted as reliable advisors in the high-stakes field of cybersecurity.Below, we illustrate the main components in the design of SECURE.</p>
<p>A. Modeling</p>
<p>We emphasize knowledge modeling in designing our evaluation benchmark.Knowledge, which includes both facts and skills, is a core indicator of intelligence [43].Prior research shows that knowledge-intensive tasks can reliably evaluate the capabilities of LLMs [9], [44].For cyber-advisory LLM, we aim to assess not only its ability to retrieve known facts but also gauge its proficiency in a) using context to answer questions and b) performing reasoning based on given knowledge source.To create a robust evaluation benchmark, we focus on the following critical abilities of LLMs in handling knowledge-intensive tasks:</p>
<p>Extraction: Knowledge extraction tasks are critical for assessing a language model's ability to access and accurately retrieve specific information from its extensive knowledge base [44].In the context of cybersecurity advisory, an LLM may be required to provide information on various security frameworks, incidents, best practices, and historical data on known vulnerabilities.Accurate extraction is essential to ensure the LLM can deliver reliable and precise information promptly, which is crucial for addressing security concerns, preventing breaches, and assisting security professionals in their decision-making processes.</p>
<p>Understanding: While knowledge extraction tasks focus on recalling information, knowledge understanding tasks are designed to assess the cognitive abilities of a model [45].This involves evaluating the model's capability to discern the truthfulness of statements and comprehend underlying knowledge within a given context.For instance, an LLM might be tested on its ability to interpret the accuracy of security issues described in a report, thereby demonstrating its grasp of complex cybersecurity concepts and scenarios.Effective understanding ensures that the LLM can accurately interpret and respond to nuanced security challenges.</p>
<p>Reasoning: Knowledge reasoning tasks aim to evaluate the problem-solving capabilities of LLMs [46].This is particularly important in the cyber-advisory role of LLMs, as they need to assist security professionals in reading, analyzing, and summarizing extensive and detailed threat reports.Effective reasoning enables the LLM to make informed recommendations, identify potential security risks, and suggest mitigation strategies based on the comprehensive analysis of the available data.</p>
<p>B. Data Sources and Tasks</p>
<p>Employing a proprietary LLM like ChatGPT-4 [1] is now a research standard in generating evaluation benchmarks in long-form responses [9], [47], [48].We utilize the more recent OpenAI's ChatGPT-4o [49] in extracting our benchmark datasets, using suitable prompts.We also evaluate the quality of ChatGPT-4o output with human annotators to discard incorrect responses (annotation process and results discussed in Section III-C).Below we explain the three different tasks of SECURE and various data sources:</p>
<p>1) Extraction Task: We frame the knowledge extraction task as a multiple-choice question answering (MCQ) task.The model is expected to answer questions without any given context, relying solely on its memory or training data.For this purpose, we focus on the MITRE ATT&amp;CK [50] and CWE (Common Weakness Enumeration) [14] websites to create two datasets: MAET (Mitre Attack Extraction Task) and CWET (Common Weakness Extraction Task).Specifically, we utilize the attack patterns for ICS [11] and mitigation plans for ICS [12] to generate the questions.For CWE, we utilize the weaknesses belonging to the class CWE-1358 "CWE VIEW: Weaknesses in SEI ETF Categories of Security Vulnerabilities in ICS" [14], which enumerates all ICS-related security vulnerabilities.</p>
<p>We selected these resources due to their high relevance and quality in the domain of ICS cybersecurity.The MITRE ATT&amp;CK framework is a globally recognized repository of tactics and techniques based on real-world observations, making it an authoritative source for constructing a benchmark dataset [50].Similarly, the CWE provides a community-developed list of software and hardware weaknesses that can become vulnerabilities, which is crucial for a comprehensive understanding of ICS security.The detailed and structured information from both MITRE ATT&amp;CK and CWE ensures that the generated questions cover fundamental concepts and advanced technical details pertinent to ICS security.Below is an example prompt used in generating the questions for the MAET and CWET tasks in Prompt A (shortened).Using such prompts, we extract a total of 2036 questions.These questions test both basic and advanced understanding of ICS cybersecurity, ensuring the robustness of SE-CURE in evaluating the knowledge-intensive tasks by the LLM.</p>
<p>Prompt A: From the following URL '${URL}', generate a set of MCQs (zero to five) for 'novices' and similarly for 'experts' with four possible answers each.... Return the output in CSV format (tab separated) for the responses with the following nine columns: URL, Level (Novice or Expert), Question, Option A, Option B, Option C, Option D, Correct Answer (A, B, C or D), Explanation... Use-case: Knowledge extraction tasks measure the ability of a language model to access its vast knowledge base and accurately recall specific facts.This is useful when a cybersecurity professional has to find answers to a threat, system weakness, mitigation plans, attack patterns, and vulnerability given a specific attack scenario or just simply to gain information to assist the other tasks they are performing.LLMs are considered to be repositories of world knowledge with few-shot learning [51] and zero-shot reasoning [52] skills and hence are frequently inquired to obtain answers for different questions.MAET and CWET, both extraction tasks, consist of such questions that replicate this situation and evaluate if LLMs have sufficient knowledge in the ICS domain.Since state-ofthe-art LLMs are trained on world knowledge and have seen documents on MITRE and CWE, such questions measure whether they can accurately recall answers to these kinds of questions.</p>
<p>2) Understanding Task: Our second task is designed to evaluate the ability of LLM to comprehend and understand the security-related text.Given the continuously evolving landscape of cybersecurity, it is crucial that LLMs can assimilate new information and use it to generate accurate responses.For this purpose, we utilize the CVEs (Common Vulnerabilities and Exposures) published in 2024, available at CVE project repository [13].CVE is better for this task since they are always updated but CWEs are more appropriate for the extraction task as they are updated less frequently.</p>
<p>To ensure the integrity of our evaluation, we verified that none of the pretrained models we selected for evaluation (ChatGPT-4 [1], ChatGPT-3.5 [18], Llama3-70b [19], Llama3-8b [20], Gemini-Pro [21], Mistral-7B [22] , Mixtral-8x7b [23]) had access to these CVEs during their training phase by confirming their training-cut-off date (See Table VIII).This precaution guarantees that the models have not been exposed to this specific information and thus must rely on their comprehension abilities, and therefore, maintain the validity and reliability of SECURE.</p>
<p>We generate the KCV (Knowledge test on Common Vulnerabilities ) dataset, a series of boolean questions that require the LLMs to read the CVE descriptions provided in JSON format and determine whether the given statements are True or False based on the available information.This setup tests the models' ability to accurately process and understand newly introduced data.</p>
<p>In addition, we created a supplementary dataset named VOOD (Vulnerability Out-of-Distribution task), which contains questions without relevant context to assess the model's ability to recognize when it lacks sufficient information to answer a question.None of these questions could be answered truthfully without access to the discussed vulnerability.Ideally, the model should indicate its inability to answer in such cases.For both KCV and VOOD, we generated 466 boolean questions.These datasets serve as a robust benchmark for evaluating the comprehension capabilities of LLMs in cybersecurity, mainly focusing on their ability to adapt to and reason about newly encountered vulnerabilities.</p>
<p>Use-case: Knowledge understanding tasks are designed to assess the model's capability to discern the truthfulness of statements with and without a context.KCV evaluates whether existing LLMs can comprehend security documents, and answer questions based on a given context.VOOD inspects how the models perform when the context is not provided.Both of these tasks measure the reliability of LLMs.An effective understanding of cybersecurity can only ensure that the LLM can accurately interpret and respond to nuanced security challenges.</p>
<p>3) Reasoning Task: Our third task evaluates the LLM's reasoning capability in the context of ICS security through the Risk Evaluation Reasoning Task (RERT).To create RERT, we compile a risk assessment dataset using cybersecurity advisories from the Cybersecurity and Infrastructure Security Agency (CISA) [15].We focus on all significant ICS-related advisory reports, each containing an Executive Summary, Risk Evaluation, Technical Details (including Vulnerability Overview, Affected Products, Background Researcher), and Mitigations.Reports lacking essential information are discarded to ensure data quality, and the remaining comprehensive reports are processed for inclusion.</p>
<p>The Risk Evaluation section of these advisories consistently summarizes the key risks associated with the identified vulnerabilities.This summary is presented in a specific format that can be inferred from the detailed vulnerability information, assuming a sufficient background in cybersecurity.As these reports are meticulously curated by security professionals, we treat their risk evaluations as gold standards, allowing us to accurately assess the performance of LLMs against the high benchmarks set by human experts.</p>
<p>Therefore, the task involves predicting the Risk Evaluation based on the provided vulnerability details.This setup allows us to gauge the LLM's ability to understand and reason about complex ICS security scenarios.We compiled 1,000 samples from the most recent ICS advisories to construct this dataset, ensuring a robust and up-to-date benchmark for evaluation.</p>
<p>In addition, we create another dataset CPST (CVSS Problem Solving Task) to measure the problem-solving skills of LLMs in security.Specifically, we use the CVSS (Common Vulnerability Scoring System) framework and manually collect 100 unique CVSS3.1 vector strings from the Cybersecurity and Infrastructure Security Agency (CISA) [15].These scores, in the range of 0-10, can be computed using the CVSS calculator which uses the Base, Temporal, and Environmental scores to determine the overall severity of a vulnerability [53].This task involves using an existing formula from CVSS3.1 standard and computing a value.This computation allows the evaluation of the problemsolving skills of LLMs in practical security settings.</p>
<p>Use-case: LLMs are now integrated into emails and document software to summarize conversations and texts.Such LLMs work quite well in handling the general English language but summarizing a threat report is significantly different as the LLMs need to understand the technical details that consist of vulnerability, affected products, and risks.RERT dataset is designed to evaluate the ability of LLMs in summarizing an extensive threat report.The CVSS Problem Solving Task checks if LLMs can understand and use the CVSS formula without being explicitly programmed (zeroshot evaluation).While a simple program can do this deterministically, this task shows if LLMs can help in real-world scenarios where users might not know the formula.It shows LLMs' ability to reason and solve problems, not just follow set rules.</p>
<p>C. Dataset Validation</p>
<p>To ensure the quality and validity of our evaluation datasets, we conducted a rigorous manual verification process for all generated questions.This process involved independent assessment by human annotators with expertise in computer security, specifically Masters or Ph.D. students in the field.Discrepancies between the human annotators' labels and the original ground truth provided by ChatGPT-4o were resolved through adjudication by a second, more experienced annotator.</p>
<p>Our analysis revealed several categories of issues within the generated questions.Some questions were deemed unanswerable from the provided context, particularly within the True/False format.In the Multiple Choice Question (MCQ) format, we encountered instances where multiple answer choices were deemed correct.To maintain the integrity of our evaluation, we removed questions deemed unanswerable or exhibiting multiple correct answers from the dataset.Questions with identifiable issues that could be rectified were corrected accordingly.We fixed 13 questions in MAET, 0 in CWE and 22 questions in KCV.This manual</p>
<p>D. Benchmark Dataset and Evaluation</p>
<p>Based on the modeling and task descriptions of Section III-A and III-B, we have created the following benchmark datasets:</p>
<p>Knowledge extraction dataset: As detailed in Section III-B1, these tasks are framed as MCQs derived from MITRE ATT&amp;CK and CWE websites.We have released two datasets: MAET and CWET, comprising a total of 2036 MCQs.We show a sample in Table I.Different LLMs are evaluated based on their accuracy in predicting the correct answers from the provided options, allowing us to measure their effectiveness in knowledge extraction.</p>
<p>Knowledge understanding dataset: As explained in Section III-B2, we utilize CVE published in 2024 to create this boolean dataset.There are two variants: KCV, which includes the context from CVE JSON files, and VOOD, which lacks this context to assess the out-of-distribution performance of LLMs.Each dataset contains 466 Boolean questions.Along with the questions and answers, we also provide the JSON files used as context for evaluating the LLM's comprehension abilities.A sample is shown in Table III.Different LLM models are evaluated based on their accuracy in predicting the truthfulness of statements, both with and without context.</p>
<p>Knowledge reasoning dataset: We have released RERT, a dataset consisting of 1000 questions based on security advisories from CISA, as detailed in Section III-B3.We show a sample in Table II.Different LLM models are evaluated using the ROGUE-L metric [54] between the LLM-generated response and the ground truth.Additionally, we have released CPST, which includes 100 manually crafted CVSS3.1 vector strings along with their associated vulnerability scores.A sample of these is provided in Table IV.LLM models are evaluated using the mean average deviation (MAD) against the ground-truth scores, assessing their accuracy in generating precise vulnerability assessments.</p>
<p>IV. EXPERIMENTS &amp; RESULTS</p>
<p>We evaluate 7 state-of-the-art LLMs varying in parameter size, organization, and access (See Appendix A and Table VIII for more detail).We pick both open-source and closed models based on LLM leaderboards (ChatArena [55]).Open-source models allow the download of full model weights whereas closed models provide an API for restricted access.We evaluate our benchmark against the following models:</p>
<p>Open-source models: Llama3-70B [19], Llama3-8B [20], Mistral-7B [22], Mixtral-8x7b [23] Closed-source models: ChatGPT-3.5 [18], ChatGPT-4 [1], Gemini-Pro (v1.5) [21]</p>
<p>A. Prompting Strategy for Evaluation</p>
<p>We use consistent prompt engineering approach to ensure uniformity across different LLMs.Instead of customizing prompts for each model, we employed a single prompt structure per task type that produced responses in the required format.</p>
<p>Example Prompt (MCQ Tasks: MAET and CWET):</p>
<p>For the given question: Which protocol function can be disabled to prevent unauthorized device shutdowns?, and four options: A) DNP3 0x0D, B) HTTP GET, C) SMTP HELO, or D) FTP LIST, pick the best option as the answer, and return as either A, B, C or D. If you do not know the answer, return X. Choose the appropriate letter from A, B, C, D, or X as your answer.Please provide only the letter corresponding to your choice without any additional text or explanations.</p>
<p>We embedded relevant information from the JSON files directly within the prompt for the KCV task, which requires external knowledge.To maintain compatibility with model input constraints and avoid exceeding maximum token limits, we selectively included questions whose prompt length, including the embedded knowledge, did not exceed 5000 words.Although the models generally followed the specified Temperature Setting: LLM temperature influences the output of language models by adjusting the randomness or predictability of generated text.A higher temperature results in more creative but potentially less coherent outputs, as the model is more likely to choose less probable words.Conversely, a lower temperature makes the output more deterministic and predictable, often resulting in repetitive and conservative responses.Typically set between 0 and 1, the temperature modifies the probability distribution of the next word in a sequence.We use the default temperature parameter (set at 0.7) for all evaluation prompts.</p>
<p>B. Evaluation Metrics</p>
<p>We employ a range of evaluation metrics tailored to the specific nature of each task within our benchmarking framework: accuracy for MAET, CWET, VOOD and KCV, ROGUE-L [54] for RERT and mean absolute deviation (MAD) for CPST.We explain the metrics in detail in Appendix B.</p>
<p>C. Results Summary</p>
<p>Table V presents a comparative summary of the performance of various language models (LLMs) evaluated across the six benchmark tasks of SECURE.Our analysis reveals a consistent trend of closed-source models, particularly ChatGPT-4 and Gemini-Pro, exhibiting superior performance across most tasks.</p>
<p>ChatGPT-4 emerges as the top performer, achieving the highest scores on four key metrics: accuracy in MAET (88.6%),KCV (87.6%), and VOOD (87.9%), as well as the lowest mean absolute deviation on CPST (0.81).These results underscore ChatGPT-4's robust capabilities in handling a variety of tasks, including out-of-distribution detection and complex problemsolving scenarios.Gemini-Pro closely trails ChatGPT-4, achieving comparable results on most tasks and slightly outperforming it on the summarization task based on the ROUGE-L metric.While ChatGPT-3.5 shows decent overall performance, it struggles significantly on the VOOD task, with an accuracy of only 8.4%.</p>
<p>Among the open-source models evaluated, Llama3-70B stands out due to its higher model capacity, consistently demonstrating superior performance.It achieves results comparable to ChatGPT-4 and even surpasses it on the CWET task.However, its subpar performance on the VOOD task indicates a potential weakness in handling out-of-domain scenarios.In contrast, models such as Llama3-8B, Mistral-7B, and Mixtral-8x7B exhibit moderate performance levels across the evaluated tasks.</p>
<p>V. DISCUSSION &amp; ANALYSIS</p>
<p>A. Error Analysis</p>
<p>In this section, we analyze the incorrect responses of LLMs on some representative examples to understand their shortcomings.</p>
<p>Q1.</p>
<p>What is the primary purpose of setting up a rogue master in an ICS environment?A) Sending legitimate control messages to devices, B) Intercepting internal communications, C) Encrypting network traffic, and D) Creating network segmentation.</p>
<p>The correct answer for this question is A) Sending legitimate control messages to devices.However, among all the LLMs evaluated, only ChatGPT-4 provided the correct response.This discrepancy might be due to the LLMs difficulty in recognizing the malicious intent embedded in the term "legitimate control messages".The positive sentiment associated with the word "legitimate" could have misled the models, causing them to overlook the actual malicious purpose.</p>
<p>Q2.</p>
<p>What is the main vulnerability exploited in CAPEC-477?A) Incorrect hashing algorithm, B) Incorrect data storage structure, C) Mixing signed and unsigned content, D) Weak encryption</p>
<p>The correct answer for this question is C) Mixing signed and unsigned content.Only ChatGPT-4 and Llama3-70B managed to answer correctly.This might be because options A, B, and D explicitly mention common vulnerability types, which could have led the models to favor these options over the correct one.The subtlety of mixing signed and unsigned content as a specific vulnerability might have been less recognizable to the other LLMs.</p>
<p>Q3.</p>
<p>State whether this statement is True or False given the JSON file as context (Source 1 ): The CVE-2024-36039 vulnerability is caused by improper escaping of JSON values when using PyMySQL.Answer: F All LLMs incorrectly answered this statement as True.The JSON file clearly states that the vulnerability is due to improper escaping of JSON keys, not values.This indicates that the models failed to differentiate between JSON values and keys within the given context.Such nuanced distinctions are crucial for accurate comprehension and response, highlighting a significant area for improvement in the models' contextual understanding and attention to detail.</p>
<p>B. Impact of Confidence on LLM Accuracy</p>
<p>Prior studies have demonstrated that large language models (LLMs) can be calibrated through selfreflection or confidence analysis [56].In this study, we evaluate the relationship between model confidence and performance across different confidence levels using the CWET task.Specifically, we measure the confidence of two representative LLMs, ChatGPT-4 and Llama3-70B, across five temperature settings: 0.6, 0.7, 0.8, 0.9, and 1.0, to obtain a more robust confidence estimate.</p>
<p>The LLMs were prompted to provide both their answers and the probability that their answers were correct (ranging from 0% to 100%), formatted as follows: ''Provide your answer and the probability that the answer is correct (0% to 100%) separated by a space.''We then averaged the confidence scores provided by the LLMs across the different temperatures.The accuracy of the models was plotted against five different confidence bins, as shown in Figure 3.</p>
<p>The results indicate a clear trend: as confidence decreases, so does accuracy, particularly within the lowest confidence bins.Furthermore, ChatGPT-4 consistently exhibits higher confidence scores than Llama3.These findings suggest that different LLMs may necessitate tailored calibration techniques to effectively mitigate incorrect responses.Specifically, while both models show a correlation between confidence and accuracy, the disparity in their confidence levels highlights the need for model-specific calibration.Future research could explore more sophisticated self-reflection and confidence estimation techniques to further improve the reliability and accuracy of LLMs in the cyber advisory tasks.</p>
<p>Finding: Responses from various LLMs with lower confidence levels tend to be less accurate, and different LLMs exhibit varying degrees of confidence, impacting their overall reliability.</p>
<p>C. Open vs. Closed Model Performance</p>
<p>This section analyzes the performance disparity between open-source and closed-source LLMs utilized in our study.Figure 4 presents a comparative analysis showcasing the peak performance achieved by the three closed LLMs (ChatGPT-3.5, ChatGPT-4, and Gemini-Pro) and the remaining four open-source LLMs.</p>
<p>Our analysis reveals a consistent trend of closedsource models outperforming their open-source counterparts across most tasks.ChatGPT-4 demonstrates superior performance across all tasks except CWET, where Llama3-70B achieves marginally better results.While the performance difference remains relatively small for MAET, CWET, KCV, and RERT tasks, it becomes more pronounced in VOOD and CPST.</p>
<p>The superior performance of ChatGPT-4 and Gemini-Pro on VOOD (Out-of-Distribution Detection) suggests the implementation of more robust safeguarding mechanisms within these models.This observation is particularly evident in the substantial performance difference between ChatGPT-4 and ChatGPT-3.5,highlighting significant advancements in outof-distribution detection capabilities.Closed models also exhibit significantly better performance on the CPST task, indicating a higher proficiency in problemsolving related to CVSS score calculation.</p>
<p>However, these safeguarding features, while crucial for enhancing security, may inadvertently hinder analysis by restricting access to crucial cyber threat intelligence.This limitation was particularly pronounced when working with the Gemini model.Specifically, during the RERT summarization task, the API refused 319 out of 1000 queries, deeming them potentially harmful due to their inclusion of vulnerability infor-Fig.5: Performance of LLMs when asked to perform step-by-step analysis Fig. 6: Inference time on reasoning tasks when asked to perform step-by-step analysis mation.This finding raises a critical question: how can we strike a balance between robust security measures and the necessary access to sensitive information for effective cybersecurity research and practice?</p>
<p>Finding: The performance difference between open and closed models is negligible except for problem-solving or out-of-distribution tasks, where closed LLMs yield better results due to more strict safeguarding.</p>
<p>D. Evaluating Reasoning Abilities</p>
<p>We further evaluate LLMs' reasoning capabilities on KCV and CPST by asking the model to perform step-by-step analysis to arrive at the result.For both tasks, we modify the prompt to include the sentence: Provide a detailed explanation of how you arrived at the answer.</p>
<p>We assess the performance of the ChatGPT-4 and Llama3-70B models on these tasks.The performance results are illustrated in Figure 5.As shown, explicit instructions for providing explanations enhance performance on both tasks for these models.For the KCV task, the relative improvement in accuracy with reasoning is 2.52% for ChatGPT-4 and 4.43% for Llama3-70B.For the CPST task, the improvement in the Mean Absolute Deviation (MAD) score with reasoning is 20.99% for ChatGPT-4 and 14.29% for Llama3-70B compared to the baseline.</p>
<p>However, this advantage comes with a caveat.Requiring models to provide detailed reasoning steps can increase inference time, as illustrated in Figure 6.The figure demonstrates that the time increase can be substantial depending on the specific task, as much as 35 times compared to the base task for CPST.This potential for elevated inference time translates to additional computational costs, highlighting a crucial trade-off between enhanced reasoning capabilities and computational expenses.Finding: Explicitly asking LLMs for explanations or details about their reasoning steps to solve a task can yield significant performance gains but usually comes at a higher computational budget.</p>
<p>E. Variance in Prediction</p>
<p>Figure 7 presents a comparative analysis of response variance across different LLMs on the CPST task.This task, requiring a real-number output, is wellsuited for distribution analysis.We evaluated each model's responses over five runs, utilizing their default temperature settings.The standard deviation of scores across these runs was calculated for each model, and the resulting distributions are visualized in the violin plot.The visualization reveals a trend: smaller models, such as Llama3-8b and Mistral-7b, tend towards wider variance, suggesting more significant variability in their responses.An exception is Mixtral-8x7b, which exhibits more deterministic behavior despite its smaller size.Larger models generally exhibit minor variance, indicating more stable outputs.However, ChatGPT-4 displays a slightly higher variance than Gemini-Pro and Llama3-70b on this specific task.</p>
<p>F. Model Agreement Bias</p>
<p>This section investigates the tendency of LLMs to exhibit agreement bias when presented with factual statements within the context of cyber advisory.Specifically, we analyze the KCV and VOOD tasks, both of which assess the model's ability to correctly classify a given statement as true or false.The distinction between KCV and VOOD lies in the provision of contextual information.While both tasks utilize the same set of statements, KCV provides additional context relevant to the statement, whereas VOOD Fig. 9: Spearman correlation between different tasks in SECURE benchmark presents the statements in isolation.This design allows us to examine how the presence or absence of context influences the model's susceptibility to agreement bias.</p>
<p>Figure 8 illustrates the distribution of incorrect predictions for both tasks, categorized by whether the LLM agreed (predicted True) or disagreed (predicted False) with the statement.In VOOD, most errors stem from the LLMs agreeing with the statement, even though the statements related to vulnerabilities were not in their training data.This tendency to affirm novel information suggests a potential for hallucination, where the model generates plausible-sounding but unsubstantiated claims [57].It is also possible that the models are incorrectly associating information from known CVEs to newer, unseen vulnerabilities.</p>
<p>Conversely, the KCV task, where context is provided, exhibits a higher proportion of errors arising from the LLMs disagreeing with the statement.This pattern indicates a potential limitation in the models' ability to effectively leverage the provided context for accurate information retrieval and association.</p>
<p>Finding: LLMs can hallucinate information when they lack up-to-date information and may fail to infer correct responses even when context is included.</p>
<p>G. Task Correlation Analysis</p>
<p>To understand the relationships between the different tasks in the SECURE benchmark, we conducted a Spearman rank correlation analysis.The results, visualized in Figure 9, reveal interesting insights into the inter-task dependencies.The heatmap shows that most tasks exhibit a moderate to strong positive correlation revealing some inherent relationship between knowledge required for different tasks.For example, the knowledge extraction tasks (MAET) shows notable correlations with other reasoning tasks (CPST, RERT) and comprehension tasks (KCV).This indicates that high-level tasks like reasoning and comprehension also rely on memorized knowledge acquired during model training.As expected, the lowest correlation is between VOOD and other tasks as VOOD task is crafted to evaluate out-of-distribution capabilities of LLMs.Finding: Different tasks in SECURE exhibit strong intertask correlations, suggesting that performance gains in one task may also beneficially impact others.</p>
<p>H. LLM Performance Across Expertise Levels</p>
<p>During the question generation phase for the Multiple Choice Question (MCQ) and Boolean tasks, we developed distinct question sets reflecting two levels of expertise: expert (5+ years of experience in security) and novice (1-2 years of experience in security).The focus of the questions were on 'fundamental concepts' for 'novices' and 'advanced technical details, problemsolving skills, and implications' for 'experts'.This section analyzes the performance variations observed across these expertise levels.</p>
<p>Figure 10 presents the average accuracy of all evaluated LLMs on the CWET and KCV tasks.Notably, LLMs demonstrate marginally superior performance on the "expert" questions for the CWET task, showing an improvement of 2.31% compared to the "novice" security questions.This suggests that LLMs can leverage their extensive training data to excel even in specialized cybersecurity domains for tasks primarily reliant on existing knowledge and pattern recognition.However, as discussed in Section V-A, they often fail to retrieve the correct source for more factual questions.</p>
<p>In contrast, the KCV task, which requires integrating new information and context-aware reasoning, reveals a different trend.Here, LLMs experience a significant decrease in accuracy, with a 5.44% drop for "expert" tasks compared to "novice" tasks.This finding underscores a potential limitation of LLMs: while proficient at utilizing pre-existing knowledge, their capability to effectively incorporate and reason with novel information, particularly in complex and dynamic fields like cybersecurity, necessitates further investigation and refinement.</p>
<p>I. Human Benchmark</p>
<p>We conducted an experiment involving human subjects where participants were asked to answer a subset of the SECURE benchmark questions.7 PhD students and 7 industry professionals working in cybersecurity and threat intelligence with years of experience ranging from 2-10 years were participants of this study.We provided an MCQ form to each participant consisting of 75 questions from MAET, CWET, and KCV datasets divided into three sets, and were asked to provide their answers.We also collected their confidence level (high or low) when providing the answers and the total time taken for each set.Table VI shows the average performance of the human subjects on the dataset.Comparing the results with Table V, we can observe that large language models outperform human subjects on the three tasks.Most participants considered KCV a difficult task as it involves CVE document analysis before validating or refuting the given statement.On average, PhD students scored higher accuracy than industry professionals.While analysts scored an average accuracy of 72.2% accuracy on all tasks, PhD students scored 77.3% accuracy.</p>
<p>J. Recommendations</p>
<p>Based on our findings from the analysis of LLMs on the SECURE benchmark, we provide the following targeted recommendations specifically designed to enhance ICS security: 1) Confidence Calibration and Monitoring: Implement robust mechanisms to monitor and adjust the confidence levels of LLMs when responding to sector-related queries.In ICS environments, inaccurate responses can have severe consequences.2) Model Selection: Unless open-source models demonstrate improved performance, prioritize the use of closed-source LLMs for problem-solving or out-of-distribution tasks (unless cost is a concern).These models have demonstrated superior performance in handling complex and unfamiliar scenarios.3) Encourage Detailed Explanations: Require LLMs to provide detailed explanations or reasoning steps for their responses to security issues.This transparency not only improves trust but also allows human operators to understand the model's decisionmaking process.Implement this strategy where detailed insights are critical for accurate and informed decision-making.4) Addressing Hallucinations: Integrate a human-inthe-loop process to review and validate LLM responses before implementation.This step is crucial to mitigate the risk of hallucinations, ensuring that the information and recommendations provided are reliable and actionable.This approach is especially important in environments where incorrect data can lead to significant operational disruptions or safety hazards.</p>
<p>5) Improving Contextual Understanding: Enhance</p>
<p>LLMs' ability to interpret and respond accurately by improving their understanding of sector-specific terminology and scenarios.This can be achieved through better context framing and providing comprehensive background information relevant to the sector.</p>
<p>K. Ethical concerns</p>
<p>All of the evaluation tasks in our proposed benchmark SECURE use publicly available threat information from credible sources like MITRE [11] [12], CVE [13], CWE [14], and Cybersecurity and Infrastructure Security Agency (CISA) [15].None of the datasets contain any personal information, and they do not make sensitive judgments on social issues for bias, deception, or discrimination.</p>
<p>VI. CUSTOMIZED LLMS A. Retrieval Augmented Generation</p>
<p>Retrieval Augmented Generation (RAG) is a method that combines the strengths of retrieval based models and generation based models [58].It consists of two major components: retriever and generator.The retriever searches for relevant information from a large database or knowledge base, using retrieval techniques to find the most relevant documents, paragraphs, or data points.Once relevant information is retrieved, the generator component (typically a language model like GPT) uses this information as context to generate a more accurate and contextually relevant response.We use LLAMA3-8B as our generator model and design retriever to search for relevant information from sources like MITRE, CWE, and CVEs.We utilize the LangChain framework [59] for data-handling.The documents are segmented into chunks of 512 tokens with an overlap of 20 tokens using LangChain's Re-cursiveCharacterTextSplitter.This chunk size ensures that the model retains enough contextual information.The chunks are then transformed into vector embeddings using the "mixedbread-ai/mxbai-embedlarge-v1" model [60], and stored these embeddings in the FAISS database [61].</p>
<p>When a user queries the RAG model, it is first converted into vector embeddings using the same "mixedbread-ai/mxbai-embed-large-v1" model.These embeddings are then processed through LangChain's ConversationalRetrievalChain, which identifies the closest matches to the query from the stored documents.The matched documents, along with the query, are then fed into Llama3 to generate a coherent response.Figure 11 shows the block diagram of our RAG model.</p>
<p>B. Fine-tuning</p>
<p>Fine-tuning is the process of taking a pretrained model and training it further on a specific dataset to adapt it to particular tasks [62].The goal of this 'tuning' is to help the model learn the nuances of the new data.We follow the following steps:</p>
<p>Data preparation: We collect data using the webgraph data from Common Crawl [63], and filter nodes of representative domains, based on their eigenvector centrality.We select the top 1 million nodes, ranked by highest centrality, for community detection, and scrap web pages from the identified domains within the community using Trafilatura [64].We use this dataset for continual pre-training but exclude from the instruction fine-tuning stage.</p>
<p>We also utilize a a set of human-collected URLs to ensure the dataset is specific to Industrial Control Systems (ICS) by sorting and filtering the data extracted with Trafilatura.This step ensures the dataset focuses on ICS-specific content before proceeding to create the instruction dataset.</p>
<p>Instruction generation and fine-tuning: We apply the Self-Instruct method to generate instructions from the parsed text, using the default system prompt from the Self-Instruct paper [65].This step yielded 10,000 instructions.We generate responses using the Llama3.1 70B Instruct model, with 8-bit quantization, and a custom system prompt.We then apply instruction fine-tuning on the Llama3-8B Instruct model with 4-bit training.</p>
<p>C. Results:</p>
<p>Table VII shows the result of the benchmark.MAET, CWET, and KCV do not have source URLs provided as a context, and hence can accurately evaluate customized models performance against general LLMs.We can observe that both of the customized models perform better on MAET.Fine-tuned model outperform RAG on CWET.While fine-tuned model performs comparable to base model on KCV, RAG model has a significant drop.One key weakness of "Retrieve-Read" framework for RAG is that during the retrieval phase, the model can select irrelevant chunks and miss crucial information for accurate response.Hence, if the content is not supported by the retrieved context, the model faces hallucinations [66].This could be one of the reasons behind lower performance in CWET and KCV.We will explore advanced RAGs framework in our future works.</p>
<p>VII. LIMITATIONS &amp; FUTURE WORK</p>
<p>The reliance on standardized sources such as MITRE ATT&amp;CK, CWE, CVE, and CVSS introduces potential biases and coverage gaps in our benchmark datasets.While these sources are integral to the cybersecurity community and are frequently used in research, they are not entirely without limitations.In particular, issues such as incorrect versioning in CVE entries or incomplete coverage of attack techniques in MITRE ATT&amp;CK could impact the accuracy of SECURE's evaluation tasks.</p>
<p>To address this, we have validated the datasets through expert review and cross-referencing with additional sources.However, these measures cannot eliminate the inherent biases of the original databases.In future work, we will continue refining these datasets and exploring alternative data sources.</p>
<p>We focused on three distinct knowledge tasks (extraction, understanding, and reasoning) to assess the knowledge acquired by LLMs in the cybersecurity context, especially for ICS.While our current work involved designing six datasets centered on ICS security, we plan to extend this framework to encompass other areas of the cybersecurity industry.By adding more datasets, we aim to increase the evaluation "breadth" and provide a more comprehensive assessment.In future work, we also intend to explore various dimensions of trustworthiness, including toxicity, bias, adversarial robustness, out-of-distribution robustness, privacy, and fairness.These investigations are expected to reveal unidentified vulnerabilities and threats to the reliability of using LLMs in cybersecurity.</p>
<p>VIII. CONCLUSION</p>
<p>LLMs are rapidly transforming the landscape of artificial intelligence, offering vast potential applications in cybersecurity.However, concerns surrounding their reliability and understanding remain paramount, particularly within the sensitive and high-stakes cybersecurity domain.The SECURE benchmark, introduced in this paper, addresses these concerns by providing a comprehensive framework for assessing the capabilities of LLMs in a cybersecurity advisory context.Our benchmark encompasses diverse tasks, evaluating knowledge extraction and reasoning abilities to ensure a thorough assessment.Through our experiments and analysis, we demonstrate the potential of LLMs as valuable tools for cybersecurity professionals.However, our findings also highlight the need for caution, especially in handling complex reasoning tasks.By open-sourcing our benchmark datasets, we invite the broader research community to contribute to their refinement, thereby enhancing the reliability of LLMs and paving the way for their responsible and effective use in cybersecurity applications.</p>
<p>Large Language Models (LLMs) are pretrained autoregressive transformers [67] capable of generating one token at a time given a set of generated tokens as input.These are trained to maximize the probability of the next token given a history of tokens.We use the following LLMs in our evaluation:</p>
<p>B. Evaluation Metrics</p>
<p>1) MAET, CWET &amp; KCV: For these three tasks, we utilize accuracy as the primary evaluation metric.Accuracy represents the percentage of questions answered correctly by the LLM.2) VOOD: This out-of-distribution task presents models with True, False, and "Don't Know (X)" options.We consider a model's prediction accurate if it selects "Don't Know (X)," demonstrating its ability to recognize when it lacks sufficient information to answer.3) RERT: We treat the RERT task as a summarization task, evaluating generated summaries against reference summaries using the ROGUE-L metric [54].ROGUE score is used in natural language processing (NLP) tasks to measure the similarity between a generated text and a reference text using overlapping units (such as n-grams, word sequences, or word pairs).In RERT, the LLMs are tasked with predicting the Risk Evaluation based on the provided vulnerability details that has a ground truth.We compute ROGUE-L by measuring the longest common subsequence (LCS) between the generated risk evaluation and reference ground truth.Given a ground truth summary X of length m and generated summary Y of length n, to compute ROGUE-L, we first compute R LCS = LCS(X,Y ) m and P LCS = LCS(X,Y ) n , where LCS(X, Y ) is the length of the longest common subsequence of X and Y.Then, ROGUE score can be computed as ROGU E L = (1+β 2 )R LCS * P LCS R LCS +β2 * P LCS , where, β = P LCS R LCS .4) CPST: The CPST task requires models to predict a real number within the range of 0 to 10.Given a ground truth of true CVSS scores, we compute its difference with the LLM predicted CVSS score and finally compute the average.This is measure of mean absolute deviation (MAD), that quantifies the average distance between each data point in a set and the mean of that set.</p>
<p>C. Prompt templates for evaluation 1) Extraction Task: Example Prompt (Task: MAET)</p>
<p>For the given question: Which protocol function can be disabled to prevent unauthorized device shutdowns?, and four options: A) DNP3 0x0D, B) HTTP GET, C) SMTP HELO, or D) FTP LIST, pick the best option as the answer, and return as either A, B, C or D. If you do not know the answer, return X. Choose the appropriate letter from A, B, C, D, or X as your answer.Please provide only the letter corresponding to your choice without any additional text or explanations.</p>
<p>Example Prompt (Task: CWET)</p>
<p>For the given question: What is a common method used by attackers to bypass ATA password security?, and four options: A) Using a BIOS exploit, B) Hot swapping the drive, C) Encrypting the drive, or D) Using default passwords, pick the best option as the answer, and return as either A, B, C or D. If you do not know the answer, return X. Choose the appropriate letter from A, B, C, D, or X as your answer.Please provide only the letter corresponding to your choice without any additional text or explanations.</p>
<p>2) Understanding Task: Example Prompt (Task: KCV) You are given the following JSON data as context: JSON F ILE. Based on the context, you have to analyze the following statement: PAN-OS versions 9.0.17 and later are unaffected by the vulnerability.and indicate whether the statement is True or False.Return your answer as either T (for True) or F (for False).If you do not know the answer, return X. Provide only the letter corresponding to your choice (T, F, or X) without any additional text or explanations.</p>
<p>For this example we pass the JSON f ile downloaded from this CVE-Link 2 .</p>
<p>Example Prompt (Task: VOOD) In VOOD, we do not pass any context to the LLMs and inspect how the models perform on information that has not been seen by them during training.</p>
<p>For the given statement: The vulnerability described in CVE-2024-0011 allows for the execution of arbitrary code on the affected system., indicate whether the statement is True or False.Return your answer as either T (for True) or F (for False).If you do not know the answer, return X. Provide only the letter corresponding to your choice (T, F, or X) without any additional text or explanations.</p>
<p>3) Reasoning Task: Example Prompt (Task: RERT) You are given the following descriptions of a cyber incident Vulnerability Overview.Please perform an analysis and deduce the risk evaluation.Return your answer in one sentence in a format such as "Successful exploitation of this vulnerability could allow an attacker to inject arbitrary JavaScript into a user's web browser" for a single vulnerability, or "Successful exploitation of these vulnerabilities could cause a denial of service, disclosure of sensitive information, communication loss, and modification of settings or ladder logic" for multiple vulnerabilities.The key points to note are: 1.The vulnerability is a reflected XSS issue, which means it allows an attacker to inject malicious JavaScript code into a user's browser.2. The vulnerability is limited to the context of an authenticated Captive Portal user's browser.3. The goal of the attack is to perform phishing attacks that could lead to credential theft.Nowhere in the description or the provided data is there any indication that the vulnerability allows for the execution of arbitrary code on the affected system.The vulnerability is primarily a client-side issue, affecting the user's browser, rather than a server-side issue that could lead to arbitrary code execution on the system.Therefore, based on the analysis of the provided data, I conclude that the statement is False.</p>
<p>Fig. 1 :
1
Fig. 1: Conversation between a user and a ChatGPT-3.5 on ICS security questions.</p>
<p>Fig. 2 :
2
Fig. 2: Overview of the SECURE benchmark.</p>
<p>4 )
4
Benchmark Dataset and Framework: We present a comprehensive set of benchmark datasets-MAET (Mitre Attack Extraction Task), CWET (Common Weakness Extraction Task), KCV (Knowledge Test on Common Vulnerabilities), VOOD (Vulnerability Out-of-Distribution), RERT (Risk Evaluation Reasoning Task), and CPST (CVSS Problem Solving Task)-along with an evaluation framework.These resources are made available to the security community to facilitate the open evaluation of future LLMs across various security-related tasks.</p>
<p>Fig. 3 :
3
Fig. 3: Confidence vs. Accuracy of LLMs</p>
<p>Fig. 7 :Fig. 8 :
78
Fig. 7: Distribution of variance among predictions for different LLMs</p>
<p>Fig. 10 :
10
Fig. 10: Performance comparison of LLMs across two different levels of question difficulty.</p>
<p>Fig. 11 :
11
Fig. 11: RAG Architecture</p>
<p>3.2.1 Path Traversal CWE-22There are multiple ways in LAquis SCADA for an attacker to access locations outside of their own directory.CVE-2024-5040 has been assigned to this vulnerability.A CVSS v3.1 base score of 7.8 has been calculated; the CVSS vector string is (AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H).A CVSS v4 score has also been calculated for CVE-2024-5040.A base score of 8.5 has been calculated; the CVSS vector string is (CVSS4.0/AV:L/AC:L/AT:N/PR:N/UI:P/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N).Example Prompt (Task: CPST)Calculate the CVSS v3 score for the following CVSS v3 Vector String: AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H.Return your answer as a single number between 0.0 and 10.0.Provide only the number corresponding to your answer without any additional text or explanations.D. LLM Reasoning ExampleExample Llama3-70b reasoning for one question in the KCV taskTo determine the accuracy of the statement, I analyzed the provided JSON data, which describes a CVE record for CVE-2024-0011.The vulnerability is a reflected cross-site scripting (XSS) issue in the Captive Portal feature of Palo Alto Networks PAN-OS software.The description of the vulnerability states: "A reflected cross-site scripting (XSS) vulnerability in the Captive Portal feature of Palo Alto Networks PAN-OS software enables execution of malicious JavaScript (in the context of an authenticated Captive Portal user's browser) if a user clicks on a malicious link, allowing phishing attacks that could lead to credential theft."</p>
<p>TABLE I :
I
Sample MCQs and Answers from MAET and CWET
URLQuestionOption AOption BOption COption DAnswerLink1What is a primary purpose of setting up a rogue master in an ICS environment?Sending legitimate control messages to devicesIntercepting internal communicationsEncrypting network trafficCreating network segmentationALink2Which technique is addressed by restricting registry permissions in ICS?PhishingDenial of ServiceUnauthorized Registry ModificationsSQL InjectionC</p>
<p>TABLE II :
II
An example row from RERT dataset.Overview &amp; vulnerability description shortened for readability.Path Traversal CWE-22 There are multiple ways in LAquis SCADA for an attacker to access locations outside of their own directory.CVE-2024-5040 has been assigned to this vulnerability.A CVSS v3.1 base score of 7.8 has been calculated; Successful exploitation of this vulnerability could allow an attacker to read and write files outside of their own directory.
URL OverviewVulnerabilityRisk EvaluationSuccessful exploitation of this vulnerability could allow an attackerto inject arbitrary JavaScript into a user's web browser for a single3.2.1Linkvulnerability, successful exploitation of these vulnerabilities could cause a denial of service, disclosure of sensitive information,communication loss, and modification of settings or ladder logicfor multiple vulnerabilities.</p>
<p>TABLE III :
III
Sample from KCV and VOOD dataset
CVE-IDQuestionAnswerCVE-2024-0011The vulnerability described in CVE-2024-0011 allows for the execution of arbitrary code on the affected system.FALSECVE-2024-0017User interaction is required for the exploitation of the CVE-2024-0017 vulnerability.TRUETABLE IV: Sample from CPST datasetCVSS v3 Vector StringCorrect AnswerAV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H7.8AV:N/AC:H/PR:N/UI:R/S:C/C:L/I:L/A:N4.7AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H9.8verification and refinement process removed a smallpercentage of questions originally collected for eachdataset. Specifically, we removed 2.9% of questionsfrom the MAET dataset, 3.5% from the CWET dataset,and 6.9% from the KCV dataset, totaling 29, 36, and32 questions respectively.</p>
<p>TABLE V :
V
Result of different models on our benchmark tasks.↑-↓ indicate higher-lower values are better.
ModelMAET (Acc ↑) CWET (Acc ↑) KCV (Acc↑) VOOD (OOD-Acc↑) RERT (ROGUE-L↑) CPST (MAD↓)ChatGPT-488.689.687.687.90.530.81ChatGPT-3.582.884.278.38.40.481.26Gemini-Pro86.287.883.586.70.541.0Llama3-70B86.390.485.227.10.511.54Llama3-8B82.183.982.856.40.481.77Mistral-7B77.980.164.257.10.421.82Mixtral-8x7B80.983.479.669.30.391.63output format, there were instances where responsesdeviated slightly from the instructions. In such cases,we manually corrected the outputs for accurate eval-uation. We include all prompts we used to generatethe responses in our dataset for reproducibility, anddemonstrate more examples in Appendix C.</p>
<p>TABLE VI :
VI
Average performance of human subjects on the subset of SECURE benchmark
Dataset Accuracy (Average) Confidence (Average) Average time (mins)MAET77%76%25CWET78%81%23KCV68%72%30</p>
<p>TABLE VII :
VII
Benchmark results on customized LLMs
ModelMAET CWETKCVBase-Llama3-8B82.1%83.9%82.8%RAG model86.6%77.3%60.5%Fine-tuned model84%85%76%</p>
<p>TABLE VIII :
VIII
Comparison between different LLMs
TypeModelProviderTraining data# of parameters Context-windowChatGPT-3.5OpenaAIUpto Sept. 2021 -16385ClosedChatGPT-4OpenaAIUpto Dec. 2023-128000Gemini-ProGoogleUpto Nov. 2023-128000Llama3-70bMetaUpto Dec. 202370.6B8000OpenLlama3-8b Mistral-7BMeta MistralUpto Mar. 2023 Upto Sept. 2021 7B 8B8000 8192Mixtral-8x7BMistralUpto Sept. 2021 46.7B32000
https://github.com/CVEProject/cvelistV5/blob/main/cves/2024/ 0xxx/CVE-2024-0008.json</p>
<p>Gpt-4-turbo and gpt-4 model openai. 2024</p>
<p>Chatgpt and other large language models for cybersecurity of smart grid applications. A Zaboli, S L Choi, T.-J Song, J Hong, arXiv:2311.054622023arXiv preprint</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Ethical and social risks of harm from language models. L Weidinger, J Mellor, M Rauh, C Griffin, J Uesato, P.-S Huang, M Cheng, M Glaese, B Balle, A Kasirzadeh, arXiv:2112.043592021arXiv preprint</p>
<p>Text and patterns: For effective chain of thought, it takes two to tango. A Madaan, A Yazdanbakhsh, arXiv:2209.076862022arXiv preprint</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, arXiv:1804.074612018arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.033002020arXiv preprint</p>
<p>Holistic evaluation of language models. P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Kola: Carefully benchmarking world knowledge of large language models. J Yu, X Wang, S Tu, S Cao, D Zhang-Li, X Lv, H Peng, Z Yao, X Zhang, H Li, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Considerations for evaluating large language models for cybersecurity tasks. J Gennari, S -H. Lau, S Perl, J Parish, G Sastry, 2024</p>
<p>Techniques -ics -mitre att&amp;ck®. 2024</p>
<p>Mitigations -ics -mitre att&amp;ck®. 2024</p>
<p>Cves published in 2024. C Project, 2024</p>
<p>Cwe-1358: Weaknesses in sei etf categories of security vulnerabilities in ics. 2024</p>
<p>Cybersecurity and infrastructure security agency. C A Advisories, 2024</p>
<p>Fine-tuning aligned language models compromises safety, even when users do not intend to. X Qi, Y Zeng, T Xie, P.-Y Chen, R Jia, P Mittal, P Henderson, arXiv:2310.036932023arXiv preprint</p>
<p>Z Gekhman, G Yona, R Aharoni, M Eyal, A Feder, R Reichart, J Herzig, arXiv:2405.05904Does fine-tuning llms on new knowledge encourage hallucinations?. 2024arXiv preprint</p>
<p>Gpt-3.5 turbo model openai. 2024</p>
<p>Meta llama3-70b. 2024</p>
<p>Meta llama3-8b. 2024</p>
<p>Gemini models. 2024</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>2024) mixtral-8x7b-instruct-v0. 1</p>
<p>Securebert: A domain-specific language model for cybersecurity. E Aghaei, X Niu, W Shadid, E Al-Shaer, International Conference on Security and Privacy in Communication Systems. Springer2022</p>
<p>Cysecbert: A domain-adapted language model for the cybersecurity domain. M Bayer, P Kuehn, R Shanehsaz, C Reuter, ACM Transactions on Privacy and Security. 2722024</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Code llama: Open foundation models for code. B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, T Remez, J Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>Finetuning large language models for vulnerability detection. A Shestov, A Cheshkov, R Levichev, R Mussabayev, P Zadorozhny, E Maslov, C Vadim, E Bulychev, arXiv:2401.170102024arXiv preprint</p>
<p>M A Ferrag, A Battah, N Tihanyi, M Debbah, T Lestable, L C Cordeiro, arXiv:2307.06616Securefalcon: The next cyber reasoning system for cyber security. 2023arXiv preprint</p>
<p>Llm agents can autonomously exploit one-day vulnerabilities. R Fang, R Bindu, A Gupta, D Kang, arXiv:2404.081442024arXiv preprint</p>
<p>Llm-assisted static analysis for detecting security vulnerabilities. Z Li, S Dutta, M Naik, arXiv:2405.172382024arXiv preprint</p>
<p>Repairllama: Efficient representations and fine-tuned adapters for program repair. A Silva, S Fang, M Monperrus, arXiv:2312.156982023arXiv preprint</p>
<p>H Guo, J Yang, J Liu, L Yang, L Chai, J Bai, J Peng, X Hu, C Chen, D Zhang, arXiv:2309.09298Owl: A large language model for it operations. 2023arXiv preprint</p>
<p>Towards evaluation and understanding of large language models for cyber operation automation. M Sultana, A Taylor, L Li, S Majumdar, 2023 IEEE Conference on Communications and Network Security (CNS). IEEE2023</p>
<p>Cybermetric: A benchmark dataset for evaluating large language models knowledge in cybersecurity. N Tihanyi, M A Ferrag, R Jain, M Debbah, arXiv:2402.076882024arXiv preprint</p>
<p>Cyberbench: A multi-task benchmark for evaluating large language models in cybersecurity. Z Liu, J Shi, J F Buford, </p>
<p>Seceval: A comprehensive benchmark for evaluating cybersecurity knowledge of foundation models. G Li, Y Li, W Guannan, H Yang, Y Yu, 2023</p>
<p>Secqa: A concise question-answering dataset for evaluating large language models in computer security. Z Liu, arXiv:2312.158382023arXiv preprint</p>
<p>An empirical study of netops capability of pre-trained large language models. Y Miao, Y Bai, L Chen, D Li, H Sun, X Wang, Z Luo, D Sun, X Xu, arXiv:2309.055572023arXiv preprint</p>
<p>Y Liu, C Pei, L Xu, B Chen, M Sun, Z Zhang, Y Sun, S Zhang, K Wang, H Zhang, arXiv:2310.07637Opseval: A comprehensive task-oriented aiops benchmark for large language models. 2023arXiv preprint</p>
<p>Llms cannot reliably identify and reason about security vulnerabilities (yet?): A comprehensive evaluation, framework, and benchmarks. S Ullah, M Han, S Pujar, H Pearce, A Coskun, G Stringhini, IEEE Symposium on Security and Privacy. 2024</p>
<p>Ctibench: A benchmark for evaluating llms in cyber threat intelligence. M T Alam, D Bhushl, L Nguyen, N Rastogi, arXiv:2406.075992024arXiv preprint</p>
<p>The art of artificial intelligence: Themes and case studies of knowledge engineering. E A Feigenbaum, 1977</p>
<p>Language models as knowledge bases. F Petroni, T Rocktäschel, P Lewis, A Bakhtin, Y Wu, A H Miller, S Riedel, arXiv:1909.010662019arXiv preprint</p>
<p>Knowledge is power: How conceptual knowledge transforms visual cognition. J A Collins, I R Olson, Psychonomic bulletin &amp; review. 212014</p>
<p>Kqa pro: A dataset with explicit compositional programs for complex question answering over knowledge base. S Cao, J Shi, L Pan, L Nie, Y Xiang, L Hou, J Li, B He, H Zhang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Prometheus: Inducing evaluation capability in language models. S Kim, J Shin, Y Cho, J Jang, S Longpre, H Lee, S Yun, S Shin, S Kim, J Thorne, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202436</p>
<p>Gpt-4o model openai. 2024</p>
<p>Mitre att&amp;ck. 2024</p>
<p>Language models are few-shot learners. T B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Cvss 3.1 calculator. 2024</p>
<p>ROUGE: A package for automatic evaluation of summaries. C.-Y Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJul. 2004</p>
<p>Chatarena: Multi-agent language game environments for large language models. 2023</p>
<p>Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. J Chen, J Mueller, 2023</p>
<p>Evaluating correctness and faithfulness of instruction-following models for question answering. V Adlakha, P Behnamghader, X H Lu, N Meade, S Reddy, 2024Transactions of the Association for Computational Linguistics12</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Open source strikes bread -new fluffy embeddings model. S Lee, A Shakir, D Koenig, J Lipp, 2024</p>
<p>Faiss: A library for efficient similarity search by meta. 2024</p>
<p>When scaling meets llm finetuning: The effect of data, model and finetuning method. B Zhang, Z Liu, C Cherry, O Firat, arXiv:2402.171932024arXiv preprint</p>
<p>Common crawl. 2024</p>
<p>Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction. A Barbaresi, Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System DemonstrationsAssociation for Computational Linguistics2021</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, H Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Large Language Models. A , </p>            </div>
        </div>

    </div>
</body>
</html>
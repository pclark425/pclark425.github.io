<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-403 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-403</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-403</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-247084226</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2202.12205v2.pdf" target="_blank">Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review</a></p>
                <p><strong>Paper Abstract:</strong> Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e403.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e403.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Tensor Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tensorization approach that grounds first-order logic predicates and formulas into real-valued vector representations so that logical constraints can be enforced or reasoned about within a neural (differentiable) learning framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logic Tensor Networks (LTN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LTNs map symbols, terms, predicates and formulas of a first-order logic language into continuous vector spaces (groundings) and real-valued functions (tensorized predicate/function implementations). Logical formula satisfaction is turned into differentiable loss terms (often via fuzzy/real-valued logic t-norms), allowing a neural network to be trained that simultaneously fits data and respects logical axioms; LTNs also permit post-training (after-training) logical inference over combinations of axioms not seen during training.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order logic (FOL) represented as formulas/axioms; logical predicates and quantifiers are grounded into real-valued semantics (fuzzy/real-valued logic using t-norms). Knowledge is expressed as axioms and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks / differentiable tensor functions that implement grounded predicates and functions; gradient-based optimization (backpropagation) is used to train embeddings and predicate/function parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tensorization of logic into differentiable components: logical formulas are converted into continuous loss/constraint terms (using fuzzy semantics/t-norms) and composed with standard neural training objectives so that logic acts as a differentiable constraint or module inside end-to-end training; also supports after-training symbolic-style reasoning using the learned groundings.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables post-training logical inference over axioms not present at training time; combines statistical pattern learning (from neural grounding) with symbolic constraint satisfaction for more interpretable predicate semantics; yields models that can both learn from data and be queried/checked with logical formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General reasoning tasks and knowledge+data integration tasks described in the review (logic-informed learning and small-scale reasoning benchmarks); used where axioms + data must be combined.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Authors report LTNs can perform after-training reasoning and can generalize using logical axioms; the review notes LTNs are promising for small-data regimes and for enabling some OOD inference via explicit axioms, but also that expressivity (e.g., quantifiers) increases computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: because symbols and predicates remain explicit and grounded, LTNs afford explainability (logical formulas and grounded predicates can be inspected and used to justify inferences).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computationally heavy as logic expressivity increases (quantifiers and richer axioms raise cost); scaling and the cost of expressive logics are noted limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Tensorization of first-order logic into real-valued semantics (fuzzy/t-norm semantics) combined with gradient-based learning; complementary strengths: symbolic constraints guide neural learning, neural groundings provide statistical robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e403.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTN-EE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LTN + Embeddings extension (LTN-EE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of LTNs that feeds pre-trained distributional embeddings (commonsense/text-based entity embeddings) into an LTN to combine sub-symbolic commonsense knowledge with explicit first-order fuzzy logic axioms in one model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Complementing logical reasoning with sub-symbolic commonsense</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LTN-EE (Logic Tensor Network with Embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A combined model where pre-trained text/entity embeddings are used as groundings for constants/terms and then fed into an LTN; the model integrates distributional (sub-symbolic) commonsense representations with explicit logical axioms so a single model performs both learning and (after-training) logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order fuzzy logic axioms (LTN-style) used as constraints and for explicit reasoning over predicates and formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Pre-trained distributional embeddings (Word2Vec-style) and neural network components that parametrize grounded predicates; standard gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pre-trained embeddings are injected as groundings into the LTN architecture (tensorized logic), yielding a single differentiable model that jointly leverages symbolic axioms and sub-symbolic embeddings; logic can be applied post-training for inferencing.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines commonsense captured in embeddings with explicit logical reasoning; permits after-training inferences over combinations of axioms and commonsense embeddings, improving reasoning capability beyond either alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Reasoning tasks combining commonsense knowledge and formal axioms; small-scale reasoning experiments reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Reported as promising for small datasets and for performing logical inference involving commonsense concepts, enabling some extrapolation via explicit axioms combined with distributional knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Interpretability arises from explicit logical axioms and grounding of predicates, while embeddings provide rich semantic content; review highlights explainability due to grounding in FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same scalability concerns as LTNs (computationally demanding as logic expressivity grows); reliance on quality of pre-trained embeddings and handcrafted axioms.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Hybridization by grounding logic into vector space and injecting distributional embeddings into that grounding; uses fuzzy/real-valued logic as differentiable bridge between symbolic and sub-symbolic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e403.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NS-CL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Symbolic Concept Learner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cooperative neuro-symbolic system that jointly learns visual concepts, word meanings, and a semantic parser to map sentences into executable symbolic programs which are evaluated over neural scene representations for interpretable question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Symbolic Concept Learner (NS-CL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NS-CL has a neural visual perception module that detects objects and produces latent object representations, and a semantic parsing module that converts natural language questions into hierarchical symbolic programs in a domain-specific language (DSL). The programs are executed (symbolically) over the scene representation to produce answers; modules are trained jointly (cooperative loop) so neural perception and symbolic program generation improve each other.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic programs in a domain-specific language (DSL) representing compositional operations (filters, relations, queries); explicit symbolic program execution semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception networks (object detectors/encoders) and neural semantic parser components (sequence models/encoders) trained with gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular cooperation: neural modules produce structured representations (objects/embeddings) consumed by symbolic program executors; semantic parsing yields symbolic programs executed on neural outputs; training couples neural perception and symbolic parsing so they inform each other (end-to-end signals via program supervision or REINFORCE-like training).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Compositional generalization (can execute novel program compositions), interpretability (the generated programs are human-readable explanations), and strong few-shot performance (authors report comparable performance using a fraction of the training data), enabling better generalization and interpretability than neural-only counterparts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visual question answering / scene understanding (e.g., compositional VQA datasets such as those used in the NS-CL work).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Reported strong compositional generalization and few-shot capabilities (review cites NS-CL achieving comparable results using only 10% of training images compared to baselines trained on full data); symbolic compositionality contributes to extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: generated symbolic programs are explicit explanations of model reasoning and operations, supporting transparency and inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on hand-crafted DSL and synthetic/limited domains; requires domain engineering (DSL and program primitives) which limits scalability to broad, real-world datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division of labor (cooperative modular architecture): perception (neural) + symbolic program synthesis/execution (symbolic) with joint training to exploit compositionality and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e403.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Learning for Mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Learning for Symbolic Mathematics (Lample & Charton)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A seq2seq neural approach that treats symbolic mathematical expressions (e.g., algebraic expressions) as sequences/trees so that neural models can learn operations like symbolic integration and differentiation by exploiting expression tree structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Learning for Symbolic Mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural seq2seq for symbolic mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A sequence-to-sequence architecture that ingests symbolic mathematical expressions (structured trees) and outputs transformed symbolic expressions (e.g., derivatives or integrals). The model leverages structural encodings of mathematical expressions, training purely on examples of input-output symbolic transformations so neural sequence models perform symbolic manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic mathematical expressions and the implicit algebraic rules encoded in the training pairs (not an explicit logic module in the original formulation, but symbolic structure is represented in the input/output trees).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Sequence-to-sequence neural networks (transformer/RNN-based) that learn to map input symbolic expressions to output expressions via gradient-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Compiling symbolic structure into the neural training process by feeding structured symbolic inputs (expression trees) to the neural seq2seq model so the network internalizes symbolic manipulation rules implicitly via supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>The neural model can perform symbolic transformations (e.g., differentiation) on expressions in a manner resembling symbolic engines, producing correct symbolic outputs without an explicit symbolic theorem-prover component.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Symbolic mathematics tasks (symbolic differentiation, integration, simplification) as described in the review and original work.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Exploits the structural (tree) nature of expressions to generalize to new expressions; review notes this paradigm is analogous to leveraging parse trees in NLP for structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Limited: model produces symbolic outputs that can be inspected, but its internal reasoning is implicit; interpretability relies on inspecting generated symbolic results rather than internal symbolic traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires substantial synthetic or curated training data for diverse symbolic transformations; may not provide explicit, human-readable derivations or proofs of transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Compiles symbolic manipulation tasks into a neural sequence learning problem by encoding symbolic structure as neural inputs/outputs; demonstrates that neural models can learn procedural symbolic transformations from examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e403.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LNN / LNN-EL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Neural Networks (LNN) / LNN-EL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Logical Neural Networks are neuro-symbolic models that represent logical connectives and formulas via differentiable modules enabling logical rules to be embedded as trainable network structures; LNN-EL applies LNNs to entity linking as a neuro-symbolic approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logical Neural Networks (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LNNs implement logical operators and formulas as differentiable network components so that logical rules can be activated, weighted, and learned; LNN-EL uses this framework to link short-text mentions to entities by combining textual neural features with logical constraints encoded in LNN modules.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logical rules and formulae encoded in a differentiable logical formalism (explicit logical connectives and rules represented as network modules).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural feature extractors (text encoders) and gradient-based learning that tune both neural parameters and rule weights/activations inside the LNN framework.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Embedded differentiable logic: logical formulas are represented as differentiable network structures that consume neural features; logic can be compiled into the network or used to regularize the loss, and rule activations/weights are learned from data (including approaches combining LNN with RL for rule induction).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables learning of rule activations/weights from data (rule induction), joint optimization of neural features and logical constraints, and improved interpretability since learned logical components correspond to human-readable rules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Short-text entity linking and other NLP tasks where logical constraints over candidate entities are useful (per the LNN-EL work cited).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Reported to improve performance in low-data or constrained scenarios by using logical structure; review highlights one LNN-based work ([155]) that reportedly meets multiple NeSy goals and outperforms previous SOTA on its tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: because logical components correspond to explicit rules or weighted logical formulas that can be inspected, providing explanations for decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Learning/activation of many rules can be computationally expensive; explicit rule specification remains a bottleneck unless rule induction is provided; domain-specific engineering may be needed.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Differentiable encoding of logical connectives and formulas inside a neural architecture; treats logic as a trainable component (weights/activations) enabling both symbolic constraints and statistical learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e403.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepProbLog: Neural Probabilistic Logic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic framework that embeds neural predicates inside the ProbLog probabilistic logic programming language so that neural perception subsystems provide probabilistic facts to a symbolic probabilistic logic reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepProbLog: Neural Probabilistic Logic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DeepProbLog extends the ProbLog probabilistic logic programming language by allowing neural networks to be used as probabilistic predicates (neural predicates). Neural modules process raw/unstructured inputs and output probability distributions that feed into the symbolic probabilistic inference engine (ProbLog), combining neural perception with probabilistic logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ProbLog probabilistic logic programs (logical rules with probabilistic facts and probabilistic inference).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks used as neural predicates producing probabilistic facts from raw inputs; trained by backpropagation through the probabilistic logic learning machinery where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neural predicates supply probabilities to the probabilistic logic program; learning can be hybrid where neural modules are trained in conjunction with symbolic probabilistic inference (end-to-end gradients propagated where supported) or in iterative/cooperative regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines perceptual pattern recognition (neural nets) with symbolic probabilistic inference, enabling uncertain reasoning over structured knowledge and improved handling of noisy/unstructured inputs while retaining symbolic interpretability of reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Probabilistic logic reasoning tasks augmented with perception (image-to-symbol tasks etc.); general knowledge + perception problems discussed in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Enables reasoning under uncertainty and the combination of learned perceptual features with symbolic probabilistic reasoning; authors of the technique argue this supports transfer and handling noisy inputs, as noted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate to high: the ProbLog component yields explicit logical/probabilistic traces of inference; neural predicates obscure internal perception but their outputs are explicit probabilities used by the symbolic reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Integration complexity (training end-to-end with probabilistic logic can be difficult), potential scalability issues for large logic programs, and reliance on correctly specifying logic program structure.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Neural predicates inside a probabilistic logic programming framework; complementary strengths: neural perception for raw data, symbolic probabilistic logic for structured inference and uncertainty management.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e403.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NTP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Theorem Prover / Neural Theorem Provers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable approach to theorem proving and knowledge base inference which uses soft unification of embeddings and differentiable backward-chaining to learn inference patterns and prove facts in a neuralized manner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning knowledge base inference with neural theorem provers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Theorem Prover (NTP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NTPs implement a differentiable form of symbolic backward-chaining theorem proving by embedding symbols and applying soft unification between goal and rule heads/bodies; multi-step differentiable proof traces are constructed and scored, enabling the system to learn to prove or infer new facts in knowledge bases using gradient-based learning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logical rules / clauses and structured proof traces (backward chaining style) represented in a form amenable to soft/differentiable unification.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural embeddings for symbols and differentiable modules implementing soft unification, recursive proof construction, and scoring; trained with gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Theorem-proving procedure is made differentiable by replacing hard unification with soft (embedding-based) similarity and by implementing recursive proof construction as differentiable operations, allowing learning of rule embeddings and proof strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Learns inference patterns from data, can generalize inference via embedding-based unification, and performs KB completion/inference in ways that blend symbolic proof structure with neural generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Knowledge base inference / logical reasoning / KB completion tasks described in the review and original papers.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Embedding-based soft unification yields some ability to generalize inference to similar/non-identical symbols; review groups NTP among differentiable ILP-like systems that aim to learn rules from data.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Partial interpretability: proof-like structures exist but are soft/continuous; extracted proofs can be examined though unification is fuzzy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scalability and computational cost of differentiable recursive proof construction; difficulty handling large, highly expressive logical theories.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Differentiable theorem proving: make classical symbolic proof procedures continuous by embedding symbols and using soft unification so learning via gradient descent is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e403.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Logic Machines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Logic Machines (NLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture designed to perform multi-step relational reasoning by simulating logical operators and variable binding across layers, aiming to emulate logical computation with neural modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Logic Machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Logic Machines (NLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NLMs are layered neural architectures that compute relational predicates over objects by aggregating and transforming relational tensors across depths; they are intended to implement logic-like operators and support multi-step relational inferences in a fully differentiable neural setting.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Implicitly encoded logical relations and relational predicate structures (the architecture is designed to emulate relational logic computations rather than accepting explicit symbolic rules).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Deep neural layers implementing tensor transformations, aggregation and permutation-invariant operations; trained with gradient-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>The network architecture itself is crafted to emulate relational/logical operators (e.g., variable binding and multi-ary relations), thereby embedding logical computation into a neural (differentiable) pipeline  an implicit integration where logic is compiled into the network.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Ability to perform multi-step relational reasoning and generalize relational computations to different numbers of objects (some extrapolation to larger instance sizes), capturing certain combinatorial reasoning patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Relational reasoning tasks and synthetic benchmarks that require multi-step inference over relations (as cited in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Reported ability to extrapolate relational computations to larger instance sizes and perform combinatorial reasoning more efficiently using self-attention-like mechanisms; review notes GNN-like and attention mechanisms are promising for such reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Limited: computations are implemented neurally (implicit), so interpretability is lower than explicit symbolic systems though the architectural design is logic-inspired.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>May struggle with expressivity vs scalability trade-offs; reasoning is implicit so extracting human-readable rules or proofs is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Implements logic-inspired relational operators in neural layers (compile logical computation into network topology), trading explicit symbolic representations for differentiable, learnable approximations of logical behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e403.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KBANN / CILP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KBANN (Knowledge-Based Artificial Neural Networks) / CILP (Connectionist Inductive Learning and Logic Programming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early hybrid systems that embed propositional rules into neural network architectures by initializing or constraining network parameters according to symbolic knowledge, enabling the network to refine and extend rule-based knowledge from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-based artificial neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KBANN / CILP (early rule-to-network hybrids)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>These early systems translate symbolic production rules or propositional logic into neural network architectures by encoding rules as network connections and weight initializations; the networks are then trained on data (e.g., via gradient descent) to refine rule parameters and learn exceptions, effectively combining symbolic prior knowledge with sub-symbolic learning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Production rules / propositional logic expressed as rule sets and encoded structurally into network topology and weight initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural network training (backpropagation) used to refine and extend the rule-derived network parameters based on data.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Symbolic rules are compiled into the neural network architecture (weights/topology) prior to training; the network is trained on data so the symbolic knowledge acts as an inductive bias and a starting point for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines rule-based priors with data-driven refinement, allowing the system to learn exceptions, improve robustness to noise, and generalize beyond strictly rule-covered cases.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Classic rule-learning and classification tasks from early neural-symbolic literature; foundational examples cited in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Provides improved learning in low-data regimes by leveraging rules as prior knowledge; acts as horizontal hybrid learning (expert knowledge + data).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Partial: the initial mapping from rules to network is interpretable, but after training the learned weights may be harder to interpret; CILP sought explanations from trained networks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limited to propositional rules or limited expressivity; scaling to richer logics and large knowledge bases is challenging; interpretability can degrade after training.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Compile symbolic rules into neural network structure as inductive bias; divide labor by initializing networks with symbolic knowledge then using gradient-based learning to refine.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e403.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e403.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tensor Product Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-symbolic mechanism for representing symbolic structures (e.g., roles and fillers) in continuous vector spaces via tensor products, enabling symbolic variable binding and compositional structure in distributed representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tensor product variable binding and the representation of symbolic structures in connectionist systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Tensor Product Representations (TPR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TPRs encode symbolic role-filler bindings by combining role and filler vectors using tensor (outer) products, producing dense continuous representations that can be manipulated by neural networks while preserving the ability to unbind and recover symbolic constituents.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic structures (roles, fillers, variable bindings) represented explicitly via tensor-based encodings corresponding to symbolic compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks that operate on tensorized representations (compress/decompress, transform) and learn to process structured symbolic content via gradient-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Compile symbolic role-filler bindings into continuous tensor encodings that neural networks can consume and produce; TPRs provide a bridge enabling neural computation over symbolic structured content.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables neural models to represent and manipulate compositional symbolic structures, supporting compositional generalization and variable binding operations that pure distributional embeddings struggle to represent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Tasks requiring structured symbolic representation and composition (e.g., structured meaning representations, sequence-to-sequence tasks with compositional outputs) as discussed in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed to support compositional generalization by preserving symbolic structure within continuous representations; review cites TPRs as part of compiled/neuro-symbolic architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: symbolic constituents can be (in principle) unbound from tensor representations, affording interpretability of composed structures though unbinding can be lossy or require learned decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Practical scaling and efficient unbinding in high-dimensional representations can be challenging; integrating TPRs into large modern architectures requires careful design.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Role-filler tensor product encoding as a principled mechanism to embed symbolic compositional structure into continuous neural representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neurosymbolic AI: The 3rd Wave <em>(Rating: 2)</em></li>
                <li>Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning <em>(Rating: 2)</em></li>
                <li>Learning and Reasoning with Logic Tensor Networks <em>(Rating: 2)</em></li>
                <li>The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision <em>(Rating: 2)</em></li>
                <li>DeepProbLog: Neural Probabilistic Logic Programming <em>(Rating: 2)</em></li>
                <li>Neural Logic Machines <em>(Rating: 2)</em></li>
                <li>Learning knowledge base inference with neural theorem provers <em>(Rating: 2)</em></li>
                <li>Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge <em>(Rating: 2)</em></li>
                <li>Neural-Symbolic Learning and Reasoning: A Survey and Interpretation <em>(Rating: 1)</em></li>
                <li>Informed Machine Learning - A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-403",
    "paper_id": "paper-247084226",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "LTN",
            "name_full": "Logic Tensor Networks",
            "brief_description": "A tensorization approach that grounds first-order logic predicates and formulas into real-valued vector representations so that logical constraints can be enforced or reasoned about within a neural (differentiable) learning framework.",
            "citation_title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge",
            "mention_or_use": "mention",
            "system_name": "Logic Tensor Networks (LTN)",
            "system_description": "LTNs map symbols, terms, predicates and formulas of a first-order logic language into continuous vector spaces (groundings) and real-valued functions (tensorized predicate/function implementations). Logical formula satisfaction is turned into differentiable loss terms (often via fuzzy/real-valued logic t-norms), allowing a neural network to be trained that simultaneously fits data and respects logical axioms; LTNs also permit post-training (after-training) logical inference over combinations of axioms not seen during training.",
            "declarative_component": "First-order logic (FOL) represented as formulas/axioms; logical predicates and quantifiers are grounded into real-valued semantics (fuzzy/real-valued logic using t-norms). Knowledge is expressed as axioms and constraints.",
            "imperative_component": "Neural networks / differentiable tensor functions that implement grounded predicates and functions; gradient-based optimization (backpropagation) is used to train embeddings and predicate/function parameters.",
            "integration_method": "Tensorization of logic into differentiable components: logical formulas are converted into continuous loss/constraint terms (using fuzzy semantics/t-norms) and composed with standard neural training objectives so that logic acts as a differentiable constraint or module inside end-to-end training; also supports after-training symbolic-style reasoning using the learned groundings.",
            "emergent_properties": "Enables post-training logical inference over axioms not present at training time; combines statistical pattern learning (from neural grounding) with symbolic constraint satisfaction for more interpretable predicate semantics; yields models that can both learn from data and be queried/checked with logical formulas.",
            "task_or_benchmark": "General reasoning tasks and knowledge+data integration tasks described in the review (logic-informed learning and small-scale reasoning benchmarks); used where axioms + data must be combined.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Authors report LTNs can perform after-training reasoning and can generalize using logical axioms; the review notes LTNs are promising for small-data regimes and for enabling some OOD inference via explicit axioms, but also that expressivity (e.g., quantifiers) increases computational cost.",
            "interpretability_properties": "High: because symbols and predicates remain explicit and grounded, LTNs afford explainability (logical formulas and grounded predicates can be inspected and used to justify inferences).",
            "limitations_or_failures": "Computationally heavy as logic expressivity increases (quantifiers and richer axioms raise cost); scaling and the cost of expressive logics are noted limitations.",
            "theoretical_framework": "Tensorization of first-order logic into real-valued semantics (fuzzy/t-norm semantics) combined with gradient-based learning; complementary strengths: symbolic constraints guide neural learning, neural groundings provide statistical robustness.",
            "uuid": "e403.0",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "LTN-EE",
            "name_full": "LTN + Embeddings extension (LTN-EE)",
            "brief_description": "An extension of LTNs that feeds pre-trained distributional embeddings (commonsense/text-based entity embeddings) into an LTN to combine sub-symbolic commonsense knowledge with explicit first-order fuzzy logic axioms in one model.",
            "citation_title": "Complementing logical reasoning with sub-symbolic commonsense",
            "mention_or_use": "mention",
            "system_name": "LTN-EE (Logic Tensor Network with Embeddings)",
            "system_description": "A combined model where pre-trained text/entity embeddings are used as groundings for constants/terms and then fed into an LTN; the model integrates distributional (sub-symbolic) commonsense representations with explicit logical axioms so a single model performs both learning and (after-training) logical reasoning.",
            "declarative_component": "First-order fuzzy logic axioms (LTN-style) used as constraints and for explicit reasoning over predicates and formulas.",
            "imperative_component": "Pre-trained distributional embeddings (Word2Vec-style) and neural network components that parametrize grounded predicates; standard gradient-based optimization.",
            "integration_method": "Pre-trained embeddings are injected as groundings into the LTN architecture (tensorized logic), yielding a single differentiable model that jointly leverages symbolic axioms and sub-symbolic embeddings; logic can be applied post-training for inferencing.",
            "emergent_properties": "Combines commonsense captured in embeddings with explicit logical reasoning; permits after-training inferences over combinations of axioms and commonsense embeddings, improving reasoning capability beyond either alone.",
            "task_or_benchmark": "Reasoning tasks combining commonsense knowledge and formal axioms; small-scale reasoning experiments reported in the review.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Reported as promising for small datasets and for performing logical inference involving commonsense concepts, enabling some extrapolation via explicit axioms combined with distributional knowledge.",
            "interpretability_properties": "Interpretability arises from explicit logical axioms and grounding of predicates, while embeddings provide rich semantic content; review highlights explainability due to grounding in FOL.",
            "limitations_or_failures": "Same scalability concerns as LTNs (computationally demanding as logic expressivity grows); reliance on quality of pre-trained embeddings and handcrafted axioms.",
            "theoretical_framework": "Hybridization by grounding logic into vector space and injecting distributional embeddings into that grounding; uses fuzzy/real-valued logic as differentiable bridge between symbolic and sub-symbolic.",
            "uuid": "e403.1",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "NS-CL",
            "name_full": "Neuro-Symbolic Concept Learner",
            "brief_description": "A cooperative neuro-symbolic system that jointly learns visual concepts, word meanings, and a semantic parser to map sentences into executable symbolic programs which are evaluated over neural scene representations for interpretable question answering.",
            "citation_title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
            "mention_or_use": "mention",
            "system_name": "Neuro-Symbolic Concept Learner (NS-CL)",
            "system_description": "NS-CL has a neural visual perception module that detects objects and produces latent object representations, and a semantic parsing module that converts natural language questions into hierarchical symbolic programs in a domain-specific language (DSL). The programs are executed (symbolically) over the scene representation to produce answers; modules are trained jointly (cooperative loop) so neural perception and symbolic program generation improve each other.",
            "declarative_component": "Symbolic programs in a domain-specific language (DSL) representing compositional operations (filters, relations, queries); explicit symbolic program execution semantics.",
            "imperative_component": "Neural perception networks (object detectors/encoders) and neural semantic parser components (sequence models/encoders) trained with gradient descent.",
            "integration_method": "Modular cooperation: neural modules produce structured representations (objects/embeddings) consumed by symbolic program executors; semantic parsing yields symbolic programs executed on neural outputs; training couples neural perception and symbolic parsing so they inform each other (end-to-end signals via program supervision or REINFORCE-like training).",
            "emergent_properties": "Compositional generalization (can execute novel program compositions), interpretability (the generated programs are human-readable explanations), and strong few-shot performance (authors report comparable performance using a fraction of the training data), enabling better generalization and interpretability than neural-only counterparts.",
            "task_or_benchmark": "Visual question answering / scene understanding (e.g., compositional VQA datasets such as those used in the NS-CL work).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Reported strong compositional generalization and few-shot capabilities (review cites NS-CL achieving comparable results using only 10% of training images compared to baselines trained on full data); symbolic compositionality contributes to extrapolation.",
            "interpretability_properties": "High: generated symbolic programs are explicit explanations of model reasoning and operations, supporting transparency and inspection.",
            "limitations_or_failures": "Relies on hand-crafted DSL and synthetic/limited domains; requires domain engineering (DSL and program primitives) which limits scalability to broad, real-world datasets.",
            "theoretical_framework": "Division of labor (cooperative modular architecture): perception (neural) + symbolic program synthesis/execution (symbolic) with joint training to exploit compositionality and interpretability.",
            "uuid": "e403.2",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Deep Learning for Mathematics",
            "name_full": "Deep Learning for Symbolic Mathematics (Lample & Charton)",
            "brief_description": "A seq2seq neural approach that treats symbolic mathematical expressions (e.g., algebraic expressions) as sequences/trees so that neural models can learn operations like symbolic integration and differentiation by exploiting expression tree structure.",
            "citation_title": "Deep Learning for Symbolic Mathematics",
            "mention_or_use": "mention",
            "system_name": "Neural seq2seq for symbolic mathematics",
            "system_description": "A sequence-to-sequence architecture that ingests symbolic mathematical expressions (structured trees) and outputs transformed symbolic expressions (e.g., derivatives or integrals). The model leverages structural encodings of mathematical expressions, training purely on examples of input-output symbolic transformations so neural sequence models perform symbolic manipulations.",
            "declarative_component": "Symbolic mathematical expressions and the implicit algebraic rules encoded in the training pairs (not an explicit logic module in the original formulation, but symbolic structure is represented in the input/output trees).",
            "imperative_component": "Sequence-to-sequence neural networks (transformer/RNN-based) that learn to map input symbolic expressions to output expressions via gradient-based training.",
            "integration_method": "Compiling symbolic structure into the neural training process by feeding structured symbolic inputs (expression trees) to the neural seq2seq model so the network internalizes symbolic manipulation rules implicitly via supervised learning.",
            "emergent_properties": "The neural model can perform symbolic transformations (e.g., differentiation) on expressions in a manner resembling symbolic engines, producing correct symbolic outputs without an explicit symbolic theorem-prover component.",
            "task_or_benchmark": "Symbolic mathematics tasks (symbolic differentiation, integration, simplification) as described in the review and original work.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Exploits the structural (tree) nature of expressions to generalize to new expressions; review notes this paradigm is analogous to leveraging parse trees in NLP for structured outputs.",
            "interpretability_properties": "Limited: model produces symbolic outputs that can be inspected, but its internal reasoning is implicit; interpretability relies on inspecting generated symbolic results rather than internal symbolic traces.",
            "limitations_or_failures": "Requires substantial synthetic or curated training data for diverse symbolic transformations; may not provide explicit, human-readable derivations or proofs of transformations.",
            "theoretical_framework": "Compiles symbolic manipulation tasks into a neural sequence learning problem by encoding symbolic structure as neural inputs/outputs; demonstrates that neural models can learn procedural symbolic transformations from examples.",
            "uuid": "e403.3",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "LNN / LNN-EL",
            "name_full": "Logical Neural Networks (LNN) / LNN-EL",
            "brief_description": "Logical Neural Networks are neuro-symbolic models that represent logical connectives and formulas via differentiable modules enabling logical rules to be embedded as trainable network structures; LNN-EL applies LNNs to entity linking as a neuro-symbolic approach.",
            "citation_title": "LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking",
            "mention_or_use": "mention",
            "system_name": "Logical Neural Networks (LNN)",
            "system_description": "LNNs implement logical operators and formulas as differentiable network components so that logical rules can be activated, weighted, and learned; LNN-EL uses this framework to link short-text mentions to entities by combining textual neural features with logical constraints encoded in LNN modules.",
            "declarative_component": "Logical rules and formulae encoded in a differentiable logical formalism (explicit logical connectives and rules represented as network modules).",
            "imperative_component": "Neural feature extractors (text encoders) and gradient-based learning that tune both neural parameters and rule weights/activations inside the LNN framework.",
            "integration_method": "Embedded differentiable logic: logical formulas are represented as differentiable network structures that consume neural features; logic can be compiled into the network or used to regularize the loss, and rule activations/weights are learned from data (including approaches combining LNN with RL for rule induction).",
            "emergent_properties": "Enables learning of rule activations/weights from data (rule induction), joint optimization of neural features and logical constraints, and improved interpretability since learned logical components correspond to human-readable rules.",
            "task_or_benchmark": "Short-text entity linking and other NLP tasks where logical constraints over candidate entities are useful (per the LNN-EL work cited).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Reported to improve performance in low-data or constrained scenarios by using logical structure; review highlights one LNN-based work ([155]) that reportedly meets multiple NeSy goals and outperforms previous SOTA on its tasks.",
            "interpretability_properties": "High: because logical components correspond to explicit rules or weighted logical formulas that can be inspected, providing explanations for decisions.",
            "limitations_or_failures": "Learning/activation of many rules can be computationally expensive; explicit rule specification remains a bottleneck unless rule induction is provided; domain-specific engineering may be needed.",
            "theoretical_framework": "Differentiable encoding of logical connectives and formulas inside a neural architecture; treats logic as a trainable component (weights/activations) enabling both symbolic constraints and statistical learning.",
            "uuid": "e403.4",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "DeepProbLog",
            "name_full": "DeepProbLog: Neural Probabilistic Logic Programming",
            "brief_description": "A neuro-symbolic framework that embeds neural predicates inside the ProbLog probabilistic logic programming language so that neural perception subsystems provide probabilistic facts to a symbolic probabilistic logic reasoner.",
            "citation_title": "DeepProbLog: Neural Probabilistic Logic Programming",
            "mention_or_use": "mention",
            "system_name": "DeepProbLog",
            "system_description": "DeepProbLog extends the ProbLog probabilistic logic programming language by allowing neural networks to be used as probabilistic predicates (neural predicates). Neural modules process raw/unstructured inputs and output probability distributions that feed into the symbolic probabilistic inference engine (ProbLog), combining neural perception with probabilistic logical inference.",
            "declarative_component": "ProbLog probabilistic logic programs (logical rules with probabilistic facts and probabilistic inference).",
            "imperative_component": "Neural networks used as neural predicates producing probabilistic facts from raw inputs; trained by backpropagation through the probabilistic logic learning machinery where possible.",
            "integration_method": "Neural predicates supply probabilities to the probabilistic logic program; learning can be hybrid where neural modules are trained in conjunction with symbolic probabilistic inference (end-to-end gradients propagated where supported) or in iterative/cooperative regimes.",
            "emergent_properties": "Combines perceptual pattern recognition (neural nets) with symbolic probabilistic inference, enabling uncertain reasoning over structured knowledge and improved handling of noisy/unstructured inputs while retaining symbolic interpretability of reasoning traces.",
            "task_or_benchmark": "Probabilistic logic reasoning tasks augmented with perception (image-to-symbol tasks etc.); general knowledge + perception problems discussed in the review.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Enables reasoning under uncertainty and the combination of learned perceptual features with symbolic probabilistic reasoning; authors of the technique argue this supports transfer and handling noisy inputs, as noted in the survey.",
            "interpretability_properties": "Moderate to high: the ProbLog component yields explicit logical/probabilistic traces of inference; neural predicates obscure internal perception but their outputs are explicit probabilities used by the symbolic reasoner.",
            "limitations_or_failures": "Integration complexity (training end-to-end with probabilistic logic can be difficult), potential scalability issues for large logic programs, and reliance on correctly specifying logic program structure.",
            "theoretical_framework": "Neural predicates inside a probabilistic logic programming framework; complementary strengths: neural perception for raw data, symbolic probabilistic logic for structured inference and uncertainty management.",
            "uuid": "e403.5",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "NTP",
            "name_full": "Neural Theorem Prover / Neural Theorem Provers",
            "brief_description": "A differentiable approach to theorem proving and knowledge base inference which uses soft unification of embeddings and differentiable backward-chaining to learn inference patterns and prove facts in a neuralized manner.",
            "citation_title": "Learning knowledge base inference with neural theorem provers",
            "mention_or_use": "mention",
            "system_name": "Neural Theorem Prover (NTP)",
            "system_description": "NTPs implement a differentiable form of symbolic backward-chaining theorem proving by embedding symbols and applying soft unification between goal and rule heads/bodies; multi-step differentiable proof traces are constructed and scored, enabling the system to learn to prove or infer new facts in knowledge bases using gradient-based learning.",
            "declarative_component": "Logical rules / clauses and structured proof traces (backward chaining style) represented in a form amenable to soft/differentiable unification.",
            "imperative_component": "Neural embeddings for symbols and differentiable modules implementing soft unification, recursive proof construction, and scoring; trained with gradient descent.",
            "integration_method": "Theorem-proving procedure is made differentiable by replacing hard unification with soft (embedding-based) similarity and by implementing recursive proof construction as differentiable operations, allowing learning of rule embeddings and proof strategies.",
            "emergent_properties": "Learns inference patterns from data, can generalize inference via embedding-based unification, and performs KB completion/inference in ways that blend symbolic proof structure with neural generalization.",
            "task_or_benchmark": "Knowledge base inference / logical reasoning / KB completion tasks described in the review and original papers.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Embedding-based soft unification yields some ability to generalize inference to similar/non-identical symbols; review groups NTP among differentiable ILP-like systems that aim to learn rules from data.",
            "interpretability_properties": "Partial interpretability: proof-like structures exist but are soft/continuous; extracted proofs can be examined though unification is fuzzy.",
            "limitations_or_failures": "Scalability and computational cost of differentiable recursive proof construction; difficulty handling large, highly expressive logical theories.",
            "theoretical_framework": "Differentiable theorem proving: make classical symbolic proof procedures continuous by embedding symbols and using soft unification so learning via gradient descent is possible.",
            "uuid": "e403.6",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Neural Logic Machines",
            "name_full": "Neural Logic Machines (NLM)",
            "brief_description": "A neural architecture designed to perform multi-step relational reasoning by simulating logical operators and variable binding across layers, aiming to emulate logical computation with neural modules.",
            "citation_title": "Neural Logic Machines",
            "mention_or_use": "mention",
            "system_name": "Neural Logic Machines (NLM)",
            "system_description": "NLMs are layered neural architectures that compute relational predicates over objects by aggregating and transforming relational tensors across depths; they are intended to implement logic-like operators and support multi-step relational inferences in a fully differentiable neural setting.",
            "declarative_component": "Implicitly encoded logical relations and relational predicate structures (the architecture is designed to emulate relational logic computations rather than accepting explicit symbolic rules).",
            "imperative_component": "Deep neural layers implementing tensor transformations, aggregation and permutation-invariant operations; trained with gradient-based methods.",
            "integration_method": "The network architecture itself is crafted to emulate relational/logical operators (e.g., variable binding and multi-ary relations), thereby embedding logical computation into a neural (differentiable) pipeline  an implicit integration where logic is compiled into the network.",
            "emergent_properties": "Ability to perform multi-step relational reasoning and generalize relational computations to different numbers of objects (some extrapolation to larger instance sizes), capturing certain combinatorial reasoning patterns.",
            "task_or_benchmark": "Relational reasoning tasks and synthetic benchmarks that require multi-step inference over relations (as cited in the review).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Reported ability to extrapolate relational computations to larger instance sizes and perform combinatorial reasoning more efficiently using self-attention-like mechanisms; review notes GNN-like and attention mechanisms are promising for such reasoning.",
            "interpretability_properties": "Limited: computations are implemented neurally (implicit), so interpretability is lower than explicit symbolic systems though the architectural design is logic-inspired.",
            "limitations_or_failures": "May struggle with expressivity vs scalability trade-offs; reasoning is implicit so extracting human-readable rules or proofs is nontrivial.",
            "theoretical_framework": "Implements logic-inspired relational operators in neural layers (compile logical computation into network topology), trading explicit symbolic representations for differentiable, learnable approximations of logical behavior.",
            "uuid": "e403.7",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "KBANN / CILP",
            "name_full": "KBANN (Knowledge-Based Artificial Neural Networks) / CILP (Connectionist Inductive Learning and Logic Programming)",
            "brief_description": "Early hybrid systems that embed propositional rules into neural network architectures by initializing or constraining network parameters according to symbolic knowledge, enabling the network to refine and extend rule-based knowledge from data.",
            "citation_title": "Knowledge-based artificial neural networks",
            "mention_or_use": "mention",
            "system_name": "KBANN / CILP (early rule-to-network hybrids)",
            "system_description": "These early systems translate symbolic production rules or propositional logic into neural network architectures by encoding rules as network connections and weight initializations; the networks are then trained on data (e.g., via gradient descent) to refine rule parameters and learn exceptions, effectively combining symbolic prior knowledge with sub-symbolic learning.",
            "declarative_component": "Production rules / propositional logic expressed as rule sets and encoded structurally into network topology and weight initializations.",
            "imperative_component": "Neural network training (backpropagation) used to refine and extend the rule-derived network parameters based on data.",
            "integration_method": "Symbolic rules are compiled into the neural network architecture (weights/topology) prior to training; the network is trained on data so the symbolic knowledge acts as an inductive bias and a starting point for learning.",
            "emergent_properties": "Combines rule-based priors with data-driven refinement, allowing the system to learn exceptions, improve robustness to noise, and generalize beyond strictly rule-covered cases.",
            "task_or_benchmark": "Classic rule-learning and classification tasks from early neural-symbolic literature; foundational examples cited in the review.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Provides improved learning in low-data regimes by leveraging rules as prior knowledge; acts as horizontal hybrid learning (expert knowledge + data).",
            "interpretability_properties": "Partial: the initial mapping from rules to network is interpretable, but after training the learned weights may be harder to interpret; CILP sought explanations from trained networks.",
            "limitations_or_failures": "Limited to propositional rules or limited expressivity; scaling to richer logics and large knowledge bases is challenging; interpretability can degrade after training.",
            "theoretical_framework": "Compile symbolic rules into neural network structure as inductive bias; divide labor by initializing networks with symbolic knowledge then using gradient-based learning to refine.",
            "uuid": "e403.8",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "TPR",
            "name_full": "Tensor Product Representations",
            "brief_description": "A neural-symbolic mechanism for representing symbolic structures (e.g., roles and fillers) in continuous vector spaces via tensor products, enabling symbolic variable binding and compositional structure in distributed representations.",
            "citation_title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
            "mention_or_use": "mention",
            "system_name": "Tensor Product Representations (TPR)",
            "system_description": "TPRs encode symbolic role-filler bindings by combining role and filler vectors using tensor (outer) products, producing dense continuous representations that can be manipulated by neural networks while preserving the ability to unbind and recover symbolic constituents.",
            "declarative_component": "Symbolic structures (roles, fillers, variable bindings) represented explicitly via tensor-based encodings corresponding to symbolic compositions.",
            "imperative_component": "Neural networks that operate on tensorized representations (compress/decompress, transform) and learn to process structured symbolic content via gradient-based training.",
            "integration_method": "Compile symbolic role-filler bindings into continuous tensor encodings that neural networks can consume and produce; TPRs provide a bridge enabling neural computation over symbolic structured content.",
            "emergent_properties": "Enables neural models to represent and manipulate compositional symbolic structures, supporting compositional generalization and variable binding operations that pure distributional embeddings struggle to represent.",
            "task_or_benchmark": "Tasks requiring structured symbolic representation and composition (e.g., structured meaning representations, sequence-to-sequence tasks with compositional outputs) as discussed in the review.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Designed to support compositional generalization by preserving symbolic structure within continuous representations; review cites TPRs as part of compiled/neuro-symbolic architectures.",
            "interpretability_properties": "Moderate: symbolic constituents can be (in principle) unbound from tensor representations, affording interpretability of composed structures though unbinding can be lossy or require learned decoders.",
            "limitations_or_failures": "Practical scaling and efficient unbinding in high-dimensional representations can be challenging; integrating TPRs into large modern architectures requires careful design.",
            "theoretical_framework": "Role-filler tensor product encoding as a principled mechanism to embed symbolic compositional structure into continuous neural representations.",
            "uuid": "e403.9",
            "source_info": {
                "paper_title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neurosymbolic AI: The 3rd Wave",
            "rating": 2,
            "sanitized_title": "neurosymbolic_ai_the_3rd_wave"
        },
        {
            "paper_title": "Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",
            "rating": 2,
            "sanitized_title": "neuralsymbolic_computing_an_effective_methodology_for_principled_integration_of_machine_learning_and_reasoning"
        },
        {
            "paper_title": "Learning and Reasoning with Logic Tensor Networks",
            "rating": 2,
            "sanitized_title": "learning_and_reasoning_with_logic_tensor_networks"
        },
        {
            "paper_title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
            "rating": 2,
            "sanitized_title": "the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision"
        },
        {
            "paper_title": "DeepProbLog: Neural Probabilistic Logic Programming",
            "rating": 2,
            "sanitized_title": "deepproblog_neural_probabilistic_logic_programming"
        },
        {
            "paper_title": "Neural Logic Machines",
            "rating": 2,
            "sanitized_title": "neural_logic_machines"
        },
        {
            "paper_title": "Learning knowledge base inference with neural theorem provers",
            "rating": 2,
            "sanitized_title": "learning_knowledge_base_inference_with_neural_theorem_provers"
        },
        {
            "paper_title": "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge",
            "rating": 2,
            "sanitized_title": "logic_tensor_networks_deep_learning_and_logical_reasoning_from_data_and_knowledge"
        },
        {
            "paper_title": "Neural-Symbolic Learning and Reasoning: A Survey and Interpretation",
            "rating": 1,
            "sanitized_title": "neuralsymbolic_learning_and_reasoning_a_survey_and_interpretation"
        },
        {
            "paper_title": "Informed Machine Learning - A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems",
            "rating": 1,
            "sanitized_title": "informed_machine_learning_a_taxonomy_and_survey_of_integrating_prior_knowledge_into_learning_systems"
        }
    ],
    "cost": 0.027865499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review
Jun 2022</p>
<p>Kyle Hamilton s:kyle.i.hamilton@mytudublin.ie 
School of Computer Science
SFI Centre for Research Training in Machine Learning
Technological University Dublin
Republic of Ireland</p>
<p>Aparna Nayak aparna.nayak@tudublin.ie 
School of Computer Science
SFI Centre for Research Training in Machine Learning
Technological University Dublin
Republic of Ireland</p>
<p>Bojan Boi bojan.bozic@tudublin.ie 
School of Computer Science
SFI Centre for Research Training in Machine Learning
Technological University Dublin
Republic of Ireland</p>
<p>Luca Longo luca.longo@tudublin.ie 
School of Computer Science
SFI Centre for Research Training in Machine Learning
Technological University Dublin
Republic of Ireland</p>
<p>Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review
Jun 2022Neuro-Symbolic Artificial IntelligenceNatural Language ProcessingDeep LearningKnowledge Representation &amp; ReasoningStructured Review
Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis. 1</p>
<p>Introduction</p>
<p>At its core, Neuro-Symbolic AI (NeSy) is "the combination of deep learning and symbolic reasoning" [1]. The goal of NeSy is to address the weaknesses of each of symbolic and sub-symbolic (neural, connectionist) approaches while preserving their strengths (see figure 1). Thus NeSy promises to deliver a best-of-both-worlds approach which embodies the "two most fundamental aspects of intelligent cognitive behavior: the ability to learn from experience, and the ability to reason from what has been learned" [1,2]. is that compositionality requires the existence of a homomorphism between the expressions of a language and the meanings of those expressions." 4 In other words, there is a direct relationship between syntax and semantics (meaning). This is in line with Noam Chomsky's Universal grammar 5 which states that there is a structure to natural language which is innate and universal to all humans, and is governed by precise mathematical rules. While an analysis of the study of linguistics is beyond the scope of this paper, the key takeaway is this: what makes such theories so attractive to computational linguists is that meaning can be derived from syntactic structures which can be translated into computer programs. Today, industrial strength tools for extracting these structures (i.e., part-ofspeech tagging, constituency parsing, dependency parsing) are readily available, such as for example NLTK 6 or SpaCy 7 . The challenge lies in representing and utilizing these structures in a way that both captures the semantics and is computationally efficient.</p>
<p>On the one hand, distributed representations are desirable because they can be efficiently processed by gradient descent (the backbone of deep learning). The downside is that the meaning embedded in a distributed representation is difficult if not impossible to decompose. So while a Large Language Model (LLM), a deep learning language model based on the principle of distributional semantics, may be very good at making certain types of predictions, it cannot be queried for answers not present in the training data by way of analogy or logic. We have also seen that even as these models get infeasibly large -the larger the model, the better the predictions [19] -they still fail on tasks requiring basic commonsense. The example in Figure 3, given by Marcus and Davis in [20] is a case in point.</p>
<p>You are having a small dinner party. You want to serve dinner in the living room. The dining room table is wider than the doorway, so to get it into the living room, you will have to remove the door. You have a table saw, so you cut the door in half and remove the top half.  [21] text completion example. The prompt is rendered in regular font, while the GPT3 response is shown in bold. It is clear that GPT3 is incapable of commonsense.</p>
<p>On the other hand, traditional symbolic approaches have also failed to capture the essence of human reasoning. While we may not yet understand exactly how people reason, it is generally accepted that human reasoning is nothing like the rigorous mathematical logic where the goal is validity. Though not for lack of ambition -Socrates got himself killed trying to get people to reason with logic [22]. In the Dictionary of Cognitive Science [23], Pascal</p>
<p>A Brief History of NLP</p>
<p>The study of language and reasoning goes back thousands of years, but it was not until the 1960's that the first computational models were realized. The Association for Computational Linguistics (ACL) 9 was founded in 1962 for people working on computational problems involving human language, a field often referred to as either computational linguistics or Natural Language Processing (NLP). Common NLP tasks are illustrated in Figure 4. One of the first NLP projects was a chat-bot named ELIZA [27], written by Joseph Weizenbaum around 1965. Given a small hand crafted set of rules, ELIZA was able to hold an, albeit superficial, conversation, gaining tremendous popularity. Curiously, despite the program's simplicity those who interacted with it, attributed to it human-like emotions. These early systems were based on pattern matching and small rule-sets, and were very limited for obvious reasons. In the 1970s and 80s linguistically rich, logic-driven, grounded systems, largely influenced by Noam Chomsky's Universal Grammar 10 were developed. The 1990s and early 2000s saw the 'statistical revolution' and the rise of machine learning, and work on NLP tasks focused on semantics, such as Natural Language Understanding (NLU), diminished for the next decade or so 11 . NLU returns to center stage, mixing techniques from previous years sometime around 2010. As a case in point, in 2011 IBM's Watson DeepQA computer system won first place on Jeopardy! for a prize of $1 million, competing against champions Brad Rutter and Ken Jennings. 12 DeepQA is a large ensemble of techniques and models, the vast majority of which was focused on general Information Retrieval (IR), NLP/NLU, Knowledge Representation &amp; Reasoning (KRR), and Machine Learning (ML) [28]. Broadly speaking, DeepQA is a large neuro-symbolic question answering software pipeline. In the last decade, and especially in the last few years, the emphasis on deep learning has somewhat overshadowed traditional NLP approaches. The Long Short Term Memory (LSTM) [29] architecture paved the way for the Transformer, which has generated a huge amount of optimism leading some people to believe that "deep learning is going to be able to do everything." 13 However, as already mentioned, the success of the Transformer and Large Language Models (LLMs) has also served to highlight their inherent shortcomings. This brings us to the present, or the "3rd Wave" [1], which seeks to overcome those shortcomings by combining deep learning with symbolic reasoning and knowledge, and by integrating and expanding on the work of previous decades.</p>
<p>Areas of NLP which are said to benefit from this approach are ones which require some form of reasoning or logic. In particular, Natural Language Understanding (NLU), Natural Language Inference (NLI), and Natural Language Generation (NLG).</p>
<p>Natural Language Understanding (NLU) is a large subset of NLP containing topics particularly focused on semantics and meaning. The boundaries between NLP and NLU are not always clear and open to debate, and even when they are agreed upon, they're somewhat arbitrary, as it's a matter of convention and a reflection of history [26].</p>
<p>Natural Language Inference (NLI) enables tasks like semantic search, information retrieval, information extraction, machine translation, paraphrase acquisition, reading comprehension, and question answering. It is the problem of determining whether a natural language hypothesis h can reasonably be inferred from a given premise p [30]. For example, the premise "Hazel is an Australian Cattle Dog", entails the hypothesis "Hazel is a dog", and can be expressed in First Order Logic (FOL) by: p |= h.</p>
<p>Natural Language Generation (NLG) is the task of generating text or speech from non-linguistic (structured) input [31]. It can be seen as orthogonal to NLU, where the input is natural language. An end-to-end system can be made up of both NLU and NLG components. When that is the case, what happens in the middle is not always that clear-cut. A neural language model such as GPT3 [21] has no structured component, however, whether it performs "understanding" is subject to debate - Figure 5.</p>
<p>Related Work</p>
<p>Several recent surveys [1,7,8,11,[15][16][17][18]32] cover neuro-symbolic architectures in detail. Our aim is not to produce another NeSy survey, but rather to examine whether the promises of NeSy in NLP are materializing. However, for completeness, and by way of introduction to the subject, we briefly summarize each of these surveys and provide references for the architectures under review.</p>
<p>In response to recent discussions in the AI community and the resurgence of interest in NeSy AI, Garcez et al. [1] synthesize the last 20 years of research in the field in the context of the aforementioned debate. The authors highlight the need for trustworthiness, interpretability, and accountability in AI systems, which ostensibly, NeSy is most suited to, in particular when it comes to natural language understanding. The authors also emphasize the distinction between commonsense knowledge and expert knowledge, and suggest that these two goals may ultimately lead to two distinct research directions: "those who seek to understand and model the brain, and those who seek to achieve or improve AI." Garcez at al. conclude that "Neurosymbolic AI is in need of standard benchmarks and associated comprehensibility tests which could in a principled way offer a fair comparative evaluation with other approaches" with a focus on the following goals: learning from fewer data, reasoning about extrapolation, reducing computational complexity, and reducing energy consumption 14 - Figure 6.  Sarker et al. [11] survey recent work in the proceedings of leading AI conferences. The authors review a total of 43 papers and classify them according to Henry Kautz's categories 15 , as well as an earlier categorisation scheme from 2005 [33]. Comparing the earlier research to the current trends, the authors confirm advancements on both the neural side, as well as the logic side, with a tendency towards more expressive logics being explored today than was thought tractable in the past, and the influence of the success of neural networks on the rise in interest in NeSy in general. Sarker et al. identify four areas of AI that can benefit from NeSy approaches: Learning from small data, Out of distribution handling, Intepretability, and Error recovery - Figure 7.</p>
<p>Out of distribution handling</p>
<p>Learning from small data</p>
<p>Interpretability</p>
<p>Error recovery ? Fig. 7. Neuro-Symbolic Artificial Intelligence promise areas [11] The authors conclude that "more emphasis is needed, in the immediate future, on deepening the logical aspects in NeSy research even further, and to work towards a systematic understanding and toolbox for utilizing complex logics in this context." Based on the studies in our review, we come to a similar conclusion.</p>
<p>Garcez et al. [8] survey recent accomplishments for integrated machine learning and reasoning motivated by the need for interpretability and accountability in AI systems. According to [8], there are three main important features of a NeSy system: Representation, Extraction, and Reasoning &amp; Learning. Symbolic knowledge can also be categorized into three groups: rule-based, formula-based, and embedding-based. The authors categorize and describe the following neuro-symbolic architectures.</p>
<p>Early systems such as KBANN [34] and CILP [35] embed propositional logic in a neural network by constraining the model parameters - Figure 8. Tensorization is a process that embeds first order logic (FOL) symbols into real-valued tensors. Reasoning is performed through matrix computation. Examples include Logic Tensor Networks (LTNs) [36] and Neural Tensor Networks (NTLs) [37] - Figure 9. . Logic Tensor Network (LTN) for P(x, y)  A(y) with G(x) = v and G(y) = u; G are grounding (vector representation) for symbols in first-order language. [8] In Neural-Symbolic Learning the primary goal is learning, with the assistance of rules and logic. Different architectures are characterized by how the logic is incorporated into the network, and how it is translated into differentiable form.</p>
<p>-Inductive Logic Programming (ILP) [38] is a set of techniques for learning logic programs from examples: * Neural Logic Programming (NLP) [39] * Differentiable Inductive Logic Programming (ILP) [40] * Neural Theorem Prover (NTP) [41] * Neural Logic Machines (NLMs) [42] -Horizontal Hybrid Learning combines expert knowledge in the form of rules/logic with data, thus are suitable to knowledge transfer learning (horizontally across domains). -Vertical Hybrid Learning combines symbolic and sub-symbolic modules which take inspiration from neuroscience in that certain areas of the brain are responsible for processing input signals, while other areas perform logical thinking and reasoning (vertically for a single domain).</p>
<p>Neural-Symbolic Reasoning concerns itself with logical reasoning, as the name suggests, powered by neural computation. These consist of model-based, and theorem proving approaches. In early theorem proving systems such as SHRUTI [43] learning capability was limited. On the other hand, model-based approaches inside neural networks have been shown to demonstrate nonmonotonic, intuitionistic, abductive, and other forms of human reasoning capability. Hence, rather than attempting to perform both learning and reasoning in a single architecture, more recent designs tend to contain separate learning and reasoning modules which communicate with each other. The authors conclude that combining symbolic and sub-symbolic modules, in other words, the compositionality of neuro-symbolic systems, contributes to the development of explainable and accountable AI [44].</p>
<p>Yu et al. [32] divide neuro-symbolic systems into two types: heavy-reasoning light-learning and heavy-learning light-reasoning ( Figure 10). These are similar to the Neural-Symbolic Reasoning and Neural-Symbolic Learning categorization in [8] above. Heavy-reasoning light-learning mainly adopts the methods of the symbolic system heavy reasoning light learning heavy learning light reasoning Fig. 10. Two types of neuro-symbolic systems: heavy reasoning light learning, and heavy learning light reasoning [32] to solve the problem in machine reasoning, and introduces neural networks to assist in solving those problems, while heavy-learning light-reasoning mainly applies methods of the neural system to solve the problem in machine learning, and introduces symbolic knowledge in the training process.</p>
<p>-Heavy reasoning light learning (based on Statistical Relational Learning (SRL) [45]) * Probabilistic Logic Programming (ProbLog) [46] * Markov Logic Network (MLN) [47] * Inductive Logic Programming (ILP) [38] -Heavy learning light reasoning * Regularization models add symbols in the form of regular terms to the objective function as a kind of prior knowledge to guide training. * Knowledge transfer models integrate the knowledge graph that represents semantic information into the neural network model, making up for the lack of data by transferring semantic knowledge. Knowledge transfer models are mainly used to solve zero-shot learning and few-shot learning [48] tasks.</p>
<p>Besold et al. [7] examine neuro-symbolic learning and reasoning through the lens of cognitive science, cognitive neuro-science, and human-level artificial intelligence. This is a much more theoretical approach. The authors first describe some early systems such as CILP [35] and fibring, introduced by Garcez &amp; Gabby [49]. Fibred networks work on the principle of recursion, where multiple neural networks are connected together, such that a fibring function in a network A, determines which neurons should be activated in a network B. A key characteristic of neuro-symbolic systems is modularity, where each network in the ensemble is responsible for a specific logic or task, increasing expressivity and allowing for non-classical logics to be represented such as connectionist modal, intuitionistic, temporal, nonmonotonic, epistemic and relational logic. Neuro-symbolic computation encompasses the integration of cognitive abilities -induction, deduction, abduction -and the study of mental models. The study of mental models has a long history, and the authors reference research from the field of neuro science and cognitive science, including the "binding" problem, dual process theory (System 1/System 2), and theories of affect; with the goal of formulating these in a neuro-symbolic system. Of particular interest to our work are the two sections on Syntactic Structures, and Compositionality, as they both deal with modeling language. Psycho-linguists have different theories of language morphology (study of the internal construction of words 16 ), with some arguing for association based explanations (McClelland [50]), while others argue for a rule-based one (Pinker [51]) -the question being whether it is better to model language through a connectionist approach, per McClelland, or a symbolic one, as per Pinker. Whether to model language in a connectionist or symbolic manner hinges also on its inherent compositionality 17 .</p>
<p>Von Rueden et al. [17] propose a taxonomy for integrating prior knowledge into learning systems. This is an extensive work covering types of knowledge and knowledge representations, neuro-symbolic integration approaches, motivations for each approach, challenges and future directions. The authors categorize knowledge into three types: scientific knowledge, world knowledge, and expert knowledge. Furthermore, knowledge representations are classified into eight types - Figure 11. Fig. 11. Types of knowledge representation [17]. Given that our work deals with natural language as input, we are only concerned with Logic Rules (which we subdivide into rules and logic) and Knowledge Graphs (which we subdivide into frames and semantic networks) -see section 6.2.2 Zhang et al. [15] survey the area of neuro-symbolic reasoning on Knowledge Graphs (KGs). The authors contribute a unified reasoning framework for Knowledge Graph Completion (KGC) and Knowledge Graph Question Answering (KGQA). Among future directions, the authors advocate for taking inspiration from human cognition for neural-symbolic reasoning in KGs, alluding to the dual model of human reasoning (System 1/System 2). Additional future directions include:   The key distinctions are in how different approaches aggregate information across the layers [53]. 18 is used as a foundation for describing NeSy systems. A high level overview of state of the art neural architectures (convolutional layers, recurrent layers, and attention) is given, followed by a discussion of each of the following:</p>
<p>-Logic Tensor Networks (LTNs) [36] (Figure 9). -Pointer Networks [54]. Pointer networks are based on the encoder/decoder with attention (ie. transformer) architecture, with the modification that the input length can vary. This architecture lends itself to combinatorial optimization problems such as the Traveling Salesperson Problem (TSP). -Graph Convolutional Networks (GCNs) [55] can be thought of as a generalization of Convolutional Neural Networks (CNNs) for non-grid topologies. -Graph Neural Network Model [56] -early GNN architecture similar to GCN.</p>
<p>-Message-passing Neural Networks -similar to GNN with a slightly modified update function [16].</p>
<p>-Graph Attention Networks (GATs) [57] -implement an attention mechanism enabling vertices to weigh neighbor representations during their aggregation. GATs are known to outperform typical GCN architectures for graph classification tasks.</p>
<p>According to the authors, GNNs endowed with attention mechanisms "are a promising direction of research towards the provision of rich reasoning and learning in [Kautz's] type 6 neuralsymbolic systems." In NLP, GATs have enabled substantial improvements in several tasks through transfer learning over pretrained transformer language models, 19 while GCNs have been shown to improve upon the state-of-the-art for seq2seq models [58]. GNN models have also been successfully applied to relational tasks over knowledge bases, such as link prediction [59]. 20 The authors posit that the application of GNNs in NeSy will bring the following benefits:</p>
<p>-Extrapolation of a learned classification of graphs as Hamiltonian, to graphs of arbitrary size.</p>
<p>-Reasoning about a learned graph structure to generalise beyond the distribution of the training data.</p>
<p>-Reasoning about the partO f (X; Y) relation (e.g., to make sense of handwritten MNIST digits and non-digits).</p>
<p>-Using an adequate self-attention mechanism to make combinatorial reasoning computationally efficient.</p>
<p>Belle [18] aims to disabuse the reader of the "common misconception that logic is for discrete properties, whereas probability theory and machine learning, more generally, is for continuous properties." The author advocates for tackling problems that symbolic logic and machine learning might struggle to address individually such as time, space, abstraction, causality, quantified generalizations, relational abstractions, unknown domains, and unforeseen examples.</p>
<p>Harmelen &amp; Teije [60] present a conceptual framework to categorize the techniques for combining learning and reasoning via a set of design patterns. "Broadly recognized advantages of such design patterns are they distill previous experience in a reusable form for future design activities, they encourage re-use of code, they allow composition of such patterns into more complex systems, and they provide a common language in a community." A graphical notation is introduced where boxes with labels represent symbolic, and sub-symbolic modules, connected with arrows. Harmelen &amp; Teije's boxology representation of AlphaGo is given in figure 13. 13. Schematic diagram using the boxology graphical notation of the AlphaGo system. Ovals denote algorithmic components (i.e. objects that perform some computation), and boxes denote their input and output (i.e. data structures) [60].
data ML sym KR sym Fig.
Earlier surveys [33,[61][62][63][64] tend to focus more on logic and logic programming, and less on learning, which is not surprising given that the ground breaking successes in deep learning are relatively recent. Several themes run through the above listed works, namely, the inherent strengths and weaknesses of symbolic and sub-symbolic techniques when taken in isolation, the types of problems which NeSy promises to solve, and the development of approaches over time.</p>
<p>Two future directions of particular interest to our work emerge: building systems which take inspiration from human cognition and reasoning, and the integration of unstructured data. To our knowledge there is no survey specifically covering the application of NeSy computing for Natural Language Processing (NLP) where the input data is both unstructured and replete with the ambiguities and inconsistencies of human reasoning.</p>
<p>Contributions</p>
<p>Our aim is to analyze recent work implementing NeSy in the language domain, to verify if the goals of NeSy are being realized, and to identify the challenges and future directions. We briefly describe each of the goals illustrated in figure 14, which we have identified based on our synthesis of the related work outlined above. </p>
<p>Out-of-distribution (OOD) Generalization</p>
<p>OOD generalization [65] refers to the ability of a model to extrapolate to phenomena not previously seen in the training data. The lack of OOD generalization in LLMs is often demonstrated by their inability perform commonsense reasoning, as in the example in Figure 3.</p>
<p>Interpretability</p>
<p>As Machine Learning (ML) and AI become increasingly embedded in daily life, the need to hold ML/AI accountable is also growing. This is particularly true in sensitive domains such as healthcare, legal, and some business applications such as lending, where bias mitigation and fairness are critical. "An interpretable model is constrained, following a domain-specific set of constraints that make reasoning processes understandable" [66].</p>
<p>Reduced size of training data</p>
<p>State-of-the-Art (SOTA) language models utilize massive amounts of data for training. This can cost in the thousands or even millions of dollars [19], take a very long time, and is neither environmentally friendly nor accessible to most researchers or businesses. The ability to learn from less data brings obvious benefits. But apart from the practical implications, there is something innately disappointing in LLMs' 'bigger hammer' approach. Science rewards parsimony and elegance, and NeSy promises to deliver results without the need for such massive scale. While this issue can be partially solved by fine tuning a pre-trained LLM using only a small amount labeled data, these techniques come with their own limitations. For example, Jiang et al. [67] discuss issues such as overfitting the data of downstream tasks and forgetting the knowledge of the pre-trained model.</p>
<p>Transferability</p>
<p>Transferability is the ability of a model which was trained on one domain, to perform similarly well in a different domain. This can be particularly valuable, when the new domain has very few examples available for training. In such cases we might rely on knowledge transfer similar to the way a person might rely on abstract reasoning when faced with an unfamiliar situation [68].</p>
<p>Reasoning</p>
<p>According to Encyclopedia Britannica, "To reason is to draw inferences appropriate to the situation" [69]. Reasoning is not only a goal in its own right, but also the means by which the other above mentioned goals can be achieved. Not only is it one of the most difficult problems in AI 21 , it is one of the most contested. Also, a distinction must be made between human-level reasoning, or what is sometimes referred to as commonsense reasoning, and formal reasoning. While human-level reasoning can be ambiguous, error-prone, and difficult to specify, formal reasoning, or logic, follows strict rules and aims to be as precise as possible. The challenge lies in determining when it is appropriate to deploy one or the other or both, and how. In section 7.1 we examine the uses of the term reasoning in more depth.</p>
<p>Methods</p>
<p>Our review methodology is guided by the principles described in [70][71][72]. The data, queries, code, and additional details can be found in our github repository. 22 </p>
<p>Research Questions</p>
<p>-Is Neuro-symbolic AI meeting its promises in NLP?</p>
<ol>
<li>What are the existing studies on neurosymbolic AI (NeSy) in natural language processing (NLP)? 2. What are the current applications of NeSy in NLP? 3. How are symbolic and sub-symbolic techniques integrated and what are the advantages/disadvantages?</li>
</ol>
<p>Search Process</p>
<p>We chose Scopus to perform our initial search, as Scopus indexes most of the top journals and conferences we were interested in. In addition to Scopus, we searched the ACL Anthology database and the proceedings from conferences specific to Neuro-symbolic AI. It is possible we missed some relevant studies, but as our aim is to shed light on the field generally, our assumption is that these journals and proceedings are a good representation of the area as a whole. The included sources are listed in Appendix C. Since we were looking for studies which combine neural and symbolic approaches, our query consists of combinations of neural and symbolic terms as well as variations thereof, listed in table 1. The keywords are deliberately broad, as it would be impossible to come up with a complete list of all possible keywords relevant to NeSy in NLP. More importantly, the focus of the work is not on specific subfields, each of which may warrant a review of its own, but rather on the explicit use of neurosymbolic approaches regardless of subfield. Strictly speaking the only keywords that would cover this would be neuro-symbolic and its syntactic variants, but we relaxed this slightly on the basis that works which explore both symbolic reasoning and deep learning in combination (as per the definition in section 1) may not necessarily have used the term neuro-symbolic. The initial query was restricted to peer-reviewed English language journal articles and conference papers from the last 3 years, which produced a total of 21,462 results.</p>
<p>Study selection process</p>
<p>We further limit the Scopus articles to those published by the top 20 publishers as ranked by Scopus's CiteScore, which is based on number of citations normalized by the document count over a 4 year window 23 , and SJR (SCImago Journal Rank), a measure of prestige inspired by the PageRank algorithm over the citation network 24 , the union of which resulted in 29 publishers, and eliminated 19,560 studies, for a total of 1,519 journal articles and 383 conference papers for screening. Two researchers independently screened a sample of each of the 1,902 studies (articles and conference papers), based on the inclusion/exclusion criteria in Table 2. The selection process is illustrated in Figure 15.</p>
<p>The inclusion criteria at this stage was intentionally broad, as the process itself was meant to be exploratory, and to inform the researchers of relevant topics within NeSy. As per best practices, this first round is also designed to understand and address inter-annotator disagreement. This unsurprisingly led to some researcher disagreement on inclusion, especially since studies need not have been explicitly labeled as neuro-symbolic to be classified as such. Agreement between researchers can be measured using the Cohen Kappa statistic, with values ranging from [-1,1], where 0 represents the expected kappa score had the labels been assigned randomly, -1 indicates complete disagreement, and 1 indicates perfect agreement. Our score at this stage came to a modest 0.33. We observed that it was not always clear from the abstract alone whether the sub-symbolic and symbolic methods were integrated in a way that meets the inclusion criteria.</p>
<p>To attain inter-annotator agreement and facilitate the next round of review, we kept a shared glossary of symbolic and sub-symbolic concepts as they presented themselves in the literature. We each reviewed all of the 1,902 studies, this time by way of a shallow reading of the full text of each study. Any disagreement at this stage was discussed in person with respect to the shared glossary. This process led to 75 journal articles and 106 conference papers marked for the final round of inclusion/exclusion.  </p>
<p>Quality Assessment</p>
<p>During the final round of inclusion/exclusion, the quality of each study was determined through the use of a nine-item questionnaire. Each of the following questions was answered with a binary value, and the study's quality was determined by calculating the ratio of positive answers. Less than a handful of studies were excluded due to a quality score of less than 50%.</p>
<ol>
<li>Is there a clear and measurable research question? 2. Is the study put into context of other studies and research, and design decisions justified accordingly (number of references in the literature review/ introduction)? 3. Is it clearly stated in the study which other algorithms the study's algorithm(s) have been compared with? 4. Are the performance metrics used in the study explained and justified? 5. Is the analysis of the results relevant to the research question? 6. Does the test evidence support the findings presented? 7. Is the study algorithm sufficiently documented to be reproducible (independent researchers arriving at the same results using their own data and methods)? 8. Is code provided? 9. Are performance metrics provided (hardware, training time, inference time)? More than 85% of the studies satisfy the requirements listed from Q1 to Q6. However, over 80% of the studies fail to provide source code or details related to the computing environment which makes the system difficult to reproduce. This leads to an overall reduction of the average quality score to 76.5% - Figure 16.</li>
</ol>
<p>Finally, a deep reading of each of the eligible studies led to 59 studies selected for inclusion. Data extraction was performed for each of the features outlined in Table 3. For acceptable values of individual features see Appendix B. The lists of neural and symbolic terms referenced in the table constitute the glossary items learned from conducting the selection process. Figure 17 </p>
<p>Results, Data Analysis, Taxonomies</p>
<p>We perform quantitative data analysis based on the extracted features in Table 3. Each study was labeled with terms from the aforementioned glossary, and each term in the glossary was classified as either symbolic, or neural. A bi-product of this process are two taxonomies built bottom-up of concepts relevant to the set of studies under review. The two taxonomies are a reflection of the definition of NeSy provided earlier: "the combination of deep learning and symbolic reasoning." To make this definition more precise, we limit the type of combination that qualifies as neuro-symbolic. Specifically, the sub-symbolic and symbolic components must be integrated in a way such that one informs the other. By way of counter example, a system which is made up of two independent symbolic and sub-symbolic components would not be considered NeSy if there is no interaction between them. For example, while a system where one component is used to process one type of data, and the other is used to process another type of data may be an effective software pipeline design, we do not consider this type of solution neuro-symbolic as the two components do not interact in any way. Thus the definition becomes "the integration of deep learning and symbolic reasoning." It should be noted, that these terms are not always consistently defined in the literature. For example, in a much earlier survey, [33] split the interrelation (type of combination) of neuro-symbolic systems into hybrid and integrated, whereas we use the term integrated to cover both. On the learning side, we have neural architectures (described in Section 6.2.1), and on the symbolic reasoning side we have knowledge representation (described in Section 6.2.2). These results are rendered in Table 4, with the addition of color representing a simple metric, or promise score, for each study. The promise score is simply the number of goals reported to have been satisfied by the solution in the study.</p>
<p>Exploratory Data Analysis</p>
<p>We plot the relationships between the features extracted from the studies, and the goals from section 4 in an effort to identify any correlations between them, and ultimately to identify patterns leading to higher promise scores.</p>
<p>Business and Technical Applications</p>
<p>The business application is the stated application, or objective, of a given study. It is often but not always an NLP task, such as text classification, or sentiment analysis. It should be noted that in this example, sentiment analysis is a type of text classification, but while one author's stated objective is specific to sentiment, another author may be interested in solving for text classification in general. As such there is no particular hierarchy or taxonomy associated with business applications. The relationship between all tasks, or business applications, and NeSy goals is shown in Figure 18.  Fig. 18. Relationship between Business Applications and NeSy Goals. Question answering is the most frequently occurring task, and is associated mainly with reasoning, reduced data, and to a lesser degree, interpretability.</p>
<p>The business application largely determines the type of model output, or what we term technical application. Most business applications are associated with a single (or at most two) technical applications. The exceptions being question answering and reading comprehension, which have been tackled as both inference and classification problems, or with the goal of information extraction or text generation. Question answering is the most frequently occurring task, and is associated mainly with reasoning, reduced data, and to a lesser degree, interpretability. On a philosophical level this seems somewhat disappointing, as one would hope that in receiving an answer, one could expect to understand why such an answer was given.</p>
<p>For completeness, the number of studies representing the technical applications and most frequently occurring business application is given in Figure 19, while Figure 20 illustrates the relationship between business applications, technical applications, and goals. </p>
<p>. Type of learning</p>
<p>Machine learning algorithms are classified as supervised, unsupervised, semi-supervised, curriculum or reinforcement learning, depending on the amount and type of supervision required during training [73][74][75]. Figure  21 demonstrates that the supervised method outnumbers all other approaches. </p>
<p>Implicit vs Explicit Reasoning</p>
<p>The subset of tasks belonging to Natural Language Understanding (NLU) and Natural Language Generation (NLG) are often regarded as more difficult, and presumed to require reasoning. Given that reasoning was one of the keywords used for search, it is not surprising that many studies report reasoning as a characteristic of their model(s).</p>
<p>How reasoning is performed often depends on the underlying representation and what it facilitates. Sometimes the representations are obtained via explicit rules or logic, but are subsequently transformed into non-decomposable embeddings for learning. As such, we can say that any reasoning during the learning process is done implicitly. Studies utilizing Graph Neural Networks (GNNs) [76][77][78][79][80][81][82] would also be considered to be doing reasoning implicitly. The majority of the studies doing implicit reasoning leverage linguistic and/or relational structure to generate those internal representations. These studies meet 53 out of a possible 180 NeSy goals, where 180 = #goals * #studies, or 29.4%. For reasoning to be considered explicit, rules or logic must be applied during or after training. Studies which implement explicit reasoning perform slightly better, meeting 51 out of 135 goals, or 37.8% and generally require less training data. Additionally, 4 studies implement both implicit and explicit reasoning, at a NeSy promise rate of 40%. Of particular interest in this grouping is Bianchi et al. [83]'s implementation of Logic Tensor Networks (LTNs), originally proposed by Serafini and Garcez in [84]. "LTNs can be be used to do after-training reasoning over combinations of axioms which it was not trained on. Since LTNs are based on Neural Networks, they reach similar results while also achieving high explainability due to the fact that they ground first-order logic" [83]. Also in this grouping, Jiang et al. [85] propose a model where embeddings are learned by following the logic expressions encoded in huffman trees to represent deep first-order logic knowledge. Each node of the tree is a logic expression, thus hidden layers are interpretable. Figure 22 shows the relationship between implicit &amp; explicit reasoning and goals, while the relationship between knowledge representation, type of reasoning, and goals is shown in Figure 23.  Fig. 23. Knowledge Representation, Type of Reasoning, and Goals. What is noteworthy, is that when Semantic Networks are utilized, reasoning is almost always done implicitly. The two exception are [83], and [77]. However, [83] utilizes FOL for explicit reasoning rather than its network component. On the other hand, [77] generate a novel interpretable reasoning graph as the output of their model.</p>
<p>Linguistic and Relational Structure</p>
<p>In the previous section we described how linguistic and relational structures can be leveraged to generate internal representations for the purpose of implicit reasoning. Here we plot the relationships between these structures and other extracted features and their interactions - Figure 24. Perhaps the most telling chart is the mapping between structures and goals, where many the studies leveraging linguistic structure do not meet any of the goals. This runs counter to the intuition that language is a natural fit for NeSy.   </p>
<p>. Datasets and Benchmarks</p>
<p>Each study in our survey is based on a unique dataset, and a variety of metrics. Given that there are nearly as many business applications, or tasks, as there are studies, this is not surprising. As such it is not possible to compare the performance of the models reviewed. However, this brings up an interesting question, and that is how one might design a benchmark for NeSy in the first place. A discussion about benchmarks at the IBM Neuro-Symbolic AI Workshop 2022 25 resulted in general agreement that the most important characteristic of a good benchmark for NeSy is in the diversity of tasks tackled. Gary Marcus pointed out that current benchmarks can be solved extensionally, meaning they can be "gamed". 26 In other words, with enough attempts, a model can become very good at a specific task without solving the fundamental reasoning challenge. In essence, this akin to over-fitting on the test set. The phenomenon can be exposed when adversarial examples are introduced such as described in [86], or through the observation that spurious correlations can be introduced in the annotation process as per [87]. This leads to models which are not able to generalize out of the training distribution. In contrast, to solve a task intensionally is to demonstrate "understanding" which is transferable to different tasks. This view is controversial with advocates of purely connectionist approaches arguing that "understanding" is not only ill defined, but also a moving target [1]every time we solve for the current definition of understanding, the definition is revised to have to meet a higher bar. So instead of worrying about the semantics of "understanding", the panelists agreed that to make the benchmarks robust to gaming is to build in enormous variance in the types of tasks they tackle. Taking this a step further, Luis Lamb 27 proposed that instead of designing benchmarks for testing models, we should be designing challenges which encourage people to work on important real world problems. For a deeper dive, see the ACL-2021 Workshop on Benchmarking: Past, Present and Future (BPPF) 28 , where some of the same issues pertaining specifically to NLP and NLU were discussed, as well as the challenges in interpreting performances across datasets, models, and with the evolution of language and context over time.</p>
<p>Taxonomies: Neural, Symbolic, &amp; Neuro-Symbolic</p>
<p>Neural</p>
<p>In the main, the extracted neural terms refer to the neural architecture implemented in a given study. We group these into higher level categories such as Linear models, Early generation (which includes CNNs), Graphical models, Sequence-to-Sequence - Figure 25. We have included Transformers in the Sequence-to-Sequence category as the original architecture was an encoder/decoder with attention. It should be noted that not all Transformers since then employ both an encoder and decoder, or generate sequences. What they have in common is the attention mechanism described in the seminal paper Attention Is All You Need, by Vaswami et al. [3] which dramatically advanced NLP research. We also include here Neuro-Symbolic architectures such as Logic Tensor Networks (LTN), Recursive Neural Knowledge Networks (RNKN), Tensor Product Representations (TPRs), and Logical Neural Networks (LNN) because they are suitable to optimization via gradient descent - Figure 26. We include one study [88] which does not implement gradient descent, but rather Neuroevolution (NE). Neuroevolution involves genetic algorithms for learning neural network weights, topologies, or ensembles of networks by taking inspiration from biological nervous systems [89,90]. Neuroevolution is often employed in the service of Reinforcement Learning (RL). Studies which do not specify a particular architecture are categorised as Multilayer Perceptron (MLP).</p>
<p>Symbolic</p>
<p>The definition we adopted states that NeSy is the integration of deep learning and symbolic reasoning. Our neural taxonomy described above reflects the deep learning component. For the symbolic reasoning component we utilize four common Knowledge Representation (KR) categories: 1) production rules, 2) logical representation, 3) frames, and 4) semantic networks [91][92][93][94][95][96]. The following definitions are merely a glimpse at each of these topics, in order to provide a basic intuition.   Fig. 26. Neuro-symbolic architectures represented in Table 4 1. Production rules -A production rule is a two-part structure comprising an antecedent set of conditions and a consequent set of actions [94]. We usually write a rule in this form:
+ + = + W 1 A W 2 A V 1 A V 2 A B 1 A B 2 A uA max p (3) 1 p (2) 2 &amp; p (2) 1 p (1) 3 &amp; p (3) 1 p (2)</p>
<p>IF conditions T HEN actions ex) IF Bird T HEN f ly</p>
<p>2.</p>
<p>Logical representation -Logic is the study of entailment relations-languages, truth conditions, and rules of inference. [94,97]. A logic includes:</p>
<p>-Syntax: specifies the symbols in the language and how they can be combined to form sentences. Hence facts about the world are represented as sentences in logic. -Semantics: specifies what facts in the world a sentence refers to. Hence, also specifies how you assign a truth value to a sentence based on its meaning in the world. A fact is a claim about the world, and may be true or false. -Inference Procedure (reasoning): mechanical method for computing (deriving) new (true) sentences from existing sentences.</p>
<p>The sentence "Not all birds can fly" in First Order Logic (FOL) looks like:
(xBird(x)  Fly(x))
FOL is by no means the only choice, but as per [94] it is a simple and convenient one for the sake of illustration. Natural Logic (NL) for example, is a formal proof theory built on the syntax of human language, which can be traced to the syllogisms of Aristotle [98]. "For better or worse, most of the reasoning that is done in the world is done in natural language. And correspondingly, most uses of natural language involve reasoning of some sort. Thus it should not be too surprising to find that the logical structure that is necessary for natural language to be used as a tool for reasoning should correspond in some deep way to the grammatical structure of natural language" [99]. Implementations and extensions include [30,[100][101][102]. Real-valued logics are often utilized in machine learning because they can be made differentiable and/or probabilistic [36] -first introduced by ukasiewicz at the turn of the 20th century [103,104]). Other, logic-based cognitive modelling approaches such as non-monotonic logic, attempt to deal with the complexities of human reasoning, epistemology, and defeasible inference [105]. 3. Frames -Frames are objects which hold entities, their properties and methods. An individual frame schema looks like this:
(Frame  name &lt; slot  name1 f iller1 &gt; &lt; slot  name2 f iller2 &gt; ...) (Penguin canFly : 0 isA :  Bird  ...)
The frame and slot names are atomic symbols; the fillers are either atomic values (like numbers or strings) or the names of other individual frames [94]. This is similar to Object Oriented Programming (OOP), where the frame is analogous to the object, and slots and fillers are properties and values respectively. 4. Semantic networks -A semantic network is a structure for representing knowledge as a pattern of interconnected nodes and edges [96]. A Frame network is a kind of semantic network where nodes are frames, and edges are the relationships between nodes. An example of a semantic network often used in NLU systems is WordNet 29 -a lexical database of English - Figure 27. Today semantic networks are more often referred to as Knowledge Graphs (KGs). 30 Table 4 shows which studies combine which of the above neural (6.2.1) and symbolic (6.2.2) categories as well as the number of NeSy goals satisfied.</p>
<p>Neuro-Symbolic</p>
<p>NeSy systems can be categorized according to the nature of the combination of neural and symbolic techniques. At AAAI-20, Henry Kautz presented a taxonomy of 6 types of Neuro-Symbolic architectures with a brief example of each [10]. While Kautz has not provided any additional information beyond his talk at AAAI-20, several researchers 29 https://wordnet.princeton.edu/ 30 This term was popularized after Google introduced contextual information to search results from their semantic network under the brand name Knowledge Graph https://blog.google/products/search/introducing-knowledge-graph-things-not/. . English WordNet subgraph [106] have formed their own interpretations [1,11,16]. We have categorized all the reviewed studies according to Kautz's taxonomy as well as our proposed nomenclature - Figure 28. Table 7 in Appendix A lists all the studies by category. Type 1 symbolic Neuro symbolic is a special case where symbolic knowledge (such as words) is transformed into continuous vector space and thus encoded in the feature embeddings of an otherwise "standard" ML model. We opted to include these studies if the derived input features belong to the set of symbolic knowledge representations described in Section 6.2 - Figure 29. One could still argue that this is simply a case of good old fashioned feature engineering, and not particularly special, but we want to explore the idea that deep learning can perform reasoning, albeit implicitly, if provided with a rich knowledge representation in the pre-processing phase. We classify these studies as Sequential. Evaluating these studies as a group was particularly challenging as they have very little in common including different datasets, benchmarks and business applications. Half of the studies do not mention reasoning at all, and the ones that do are mainly executing rules on candidate solutions output by the neural models post hoc. In aggregate, only 26 out of a total of 115 (23 studies * 5 goals), or 22.6%, possible NeSy goals were met. Type 2 Symbolic[Neuro] is what we describe as a Nested architecture, where a symbolic reasoning system is the primary system with neural components driving certain internal decisions. AlphaGo is the example given by Kautz,  Fig. 29. Type 1 Sequential. A symbolic knowledge representation module is used to generate rich embeddings for downstream machine learning [138]. where the symbolic system is a Monte Carlo Tree Search with neural state estimators nominating next states. We found four studies that fit this architecture. We use [115] for the purposes of illustration - Figure 30. Type 3 Neuro; Symbolic is what we call Cooperative. Here, a neural network focuses on one task (e.g. object detection) and interacts via input/output with a symbolic reasoner specializing in a complementary task (e.g. query answering). Unstructured input is converted into symbolic representations which can be solved by a symbolic reasoner, which in turn informs the neural component which learns from the errors of the symbolic component. This process is iterated until convergence or a satisfactory output is produced. There are nine studies in this category, all but one of which utilize rules and/or logic for knowledge representation. A common theme among the cooperative architectures is the business application of question answering. The Neuro-Symbolic Concept Learner (NS-CL) [137] - Figure 31 -is an example of Type 3, meeting 4 out of the 5 NeSy goals. Its ability to perform well with reduced data is particularly impressive: "Using only 10% of the training images, our model is able to achieve comparable results with the baselines trained on the full dataset." Similarly, [116] report perfect performance on small datasets which they also attribute to the use of explicit and precise reasoning. Both studies display similar limitations, the use of synthetic datasets, and the need for handcrafted logic, a DSL (Domain Specific Language) in the case of [137], and Image Schemas in [116]. Six out of the nine studies leverage linguistic structures in some fashion, and in particular, [146] utilize natural logic, for a model which is both interpretable, and achieves state-of-the-art performance on two QA datasets. This work builds on [30,101].</p>
<p>Types 4 and 5, Neuro: Symbolic  Neuro and Neuro_Symbolic respectively, were originally presented by Kautz under one heading. After his presentation, Kautz modified the slide deck 31 separating these two types into systems where knowledge is compiled into the network weights, and where knowledge is compiled into the loss function. In Types 4 and 5, reasoning can be performed both implicitly and explicitly, in that it is calculated via gradient descent, but can also be performed post hoc. We have grouped studies belonging to these two categories under the moniker of Compiled systems, of which there are sixteen and seven respectively.</p>
<p>Deep Learning For Mathematics [156] is the canonical example of Type 4, where the input and output to the model are mathematical expressions. The model performs symbolic differentiation or integration, for example, given x 2 as input, the model outputs 2x. The model exploits the tree structure of mathematical expressions, which are fed into a sequence-to-sequence architecture. This seems like a particularly fitting paradigm for natural language applications on the basis that structures such as parse trees can be similarly leveraged to output other meaningful structures such as for example: cause and effect relationships as exemplified in [134] and [150], or the generation of argument schemes as per [76]. The downside of many of these types of systems is the need for hand-crafted 31 Filter(Sphere)))) Query(Shape, Filter(Sphere, Relate(Left, Filter(Red)))) Exist(AERelate(Shape, Filter(Red, Relate(Left, Filter(Sphere))))) Q: What is the shape of the red object left of the sphere?</p>
<p> v s Fig. 31. Type 3 Cooperative. The Neuro-Symbolic Concept Learner (NS-CL) jointly learns visual concepts, words, and semantic parsing of sentences without any explicit annotations. Given an input image, the visual perception module detects objects in the scene and extracts a deep, latent representation for each of them. The semantic parsing module translates an input question in natural language into an executable program given a domain specific language (DSL). The generated programs have a hierarchical structure of symbolic, functional modules, each fulfilling a specific operation over the scene representation. The explicit program semantics enjoys compositionality, interpretability, and generalizability [137]. rules and logic [125,133,150,152]. In contrast, [155] learn rules from data (rule induction) by combining Logical Neural Networks (LNN) with text-based Reinforcement Learning (RL). One could argue that this is a combination of Type 4, compiled (logic embedded in the network), and Type 3, cooperative (symbolic and sub-symbolic modules learning from each other in an iterative fashion). [155] is the only work we found which meets all five promises, and, it outperforms previous SOTA approaches - Figure 32. Another example of a Type 4 system in our set of studies is Observation: You find yourself in a bedroom. An usual one. I guess you better just go and list everything you see here. There is an exit to the north. Don't worry, it is unguarded. There is an exit to the south. Don't worry, it is unblocked. You don't like doors? Why not try going west, that entranceway is unguarded. proposed by [85]. Here, knowledge is encoded in the form of huffman trees made of triples and logic expressions, in order to jointly learn embeddings and model weights - Figure 33. The model is intended for medical diagnosis decision support, where a requisite characteristic is interpretability, and this model meets that goal. Type 5 comprises Tensor Product Representations (TPRs) [157], Logic Tensor Networks (LTNs) [36], Neural Tensor Networks (NTN) [37] and more broadly is referred to as tensorization, where logic acts as a constraint. LT N EE [83] is an example of a compiled Type 5 system - Figure 34.</p>
<p>Type 6 Neuro[Symbolic] is the most tightly integrated but perhaps the most elusive as there do not appear to be any recent implementations in existence. According to Kautz, this is the ultimate NeSy system which should be capable of efficient combinatorial reasoning at the level of super-intelligence, if not human intelligence. (2)   The major contribution of this work is to show that combining commonsense knowledge under the form of text-based entity embeddings with LTNs is not only simple, but it is also promising. LTNs can also be used to do after-training reasoning over combinations of axioms on which it was not trained [83]. Figure 35 shows the number of studies per category, and Figure 36 illustrates the relationship between categories and goals. Table 5 shows the number of studies in each category per goal.
p (3) 1 p (2) 2 &amp; p (2) 1 p (1) 3 &amp; p (3) 1 p (2) 1 p (1) 1 p (1) layer p</p>
<p>Discussion</p>
<p>All studies report performance either on par or above benchmarks, but we cannot compare studies based on performance as nearly every study uses a different dataset and benchmark as discussed in Section 6.1.5. Our focus is instead on whether the goals of NeSy are being met. Our Promise Score metric is not necessarily what the studies' authors were optimizing for or even reporting, especially studies which have not labeled themselves as NeSy per se. So we want to make it very clear that our analysis is not a judgement of the success of any particular study, but rather we seek to understand if the hypotheses about NeSy are materializing, namely that the combination of symbolic and sub-symbolic techniques will fulfill the goals described in Section 4: Out-of-distribution (OOD) Generalization, interpretability, tranferability, reduced data, and reasoning. And the short answer is we are not there yet, as can be seen in Figure 37. For a detailed breakdown of each goal and study see Table 6.</p>
<p>In Section 4.5 we put forward the hypothesis that reasoning is the means by which the other goals can be achieved. This is not evidenced in the studies we reviewed. Some possible explanations for this finding are: 1) The kind of (a) NeSy category (b) Kautz   reasoning required to fulfill the other goals is not the kind being implemented; 2) The approaches are theoretically promising, but the technical solutions need further development. Next we look at each of these possibilities.</p>
<p>Reasoning Challenges</p>
<p>Thirty four out of the fifty nine studies mention reasoning as a characteristic of their solution. But there is a lot of variation in how reasoning is described and implemented. Given the overwhelming evidence of the fallibility of human reasoning, to understand language, AI researchers have sought guidance from disciplines such as psychology, cognitive linguistics, neuroscience, and philosophy. The challenge is that there are multiple competing theories of human reasoning and logic both across and within these disciplines. What we have discovered in our review, is a blurring of the lines between various types of logic, human reasoning, and mathematical reasoning, as well as counter-productive assumptions about which theory to adopt. For example, drawing inspiration from "how people think", accepting that how people think is flawed, and subsequently attempting to build a model with a logical component, which by definition, is rooted in validity, seems counter productive to us. Although this does depend somewhat on the business application. For problems like MWP (Math Word Problems) [77,123,135], where answers are precise and unambiguous, less assumptions are needed. Additionally, the justification of "because that's how people think" is inconsistent. Some examples from the studies we reviewed include:</p>
<ul>
<li>[83] describe human reasoning in terms of a dual process of "subsymbolic commonsense" (strongly correlated with associative learning), and "axiomatic" knowledge (predicates and logic formulas) for structured inference. -In [108] humans reason by way of analogy, and commonsense knowledge is represented in ConceptNet, a graphical representation of common concepts and their relationships. -For [116] human reasoning can be modeled by Image Schemas (IS). Schemas are made up of logical rules on (Entity1,Relation,Entity2) tuples, such as transitivity, or inversion. - [113] explain their choice of fuzzy logic for "its resemblance to human reasoning and natural language." This is a probabilistic approach which attempts to deal with uncertainty. - [119] propose that human thought constructs can be modelled as cause-effect pairs. Commonsense is often described as the ability to draw causal conclusions from basic knowledge, for example: If I drop the glass, it will break. -And [123] state that "when people perform explicit reasoning, they can typically describe the way to the conclusion step by step via relational descriptions."</li>
</ul>
<p>But the most plausible hypothesis in our view is that of Schon et al. [128]: in order to emulate human reasoning, systems need to be flexible, be able to deal with contradicting evidence, evolving evidence, have access to enormous amounts of background knowledge, and include a combination of different techniques and logics. Most notably, no particular theory of reasoning is given. The argument put forward by Leslie Kaelbling at IBM Neuro-Symbolic AI Workshop 2022 32 is similarly appealing. Kaelbling points to the over-reliance on the System1/System2 analogy, and advocates for a much more diverse and dynamic approach. We posit that the type of reasoning employed should not be based solely on how we think people think, but on the attendant objective. This is in line with the "goal oriented" theory from neuroscience, in that reasoning involves many sub-systems: perception, information retrieval, decision making, planning, controlling, and executing, utilizing working memory, calculation, and pragmatics. But here the irony is not lost on us, and we acknowledge that by resorting to neuroscience for inspiration, we have just committed the same mischief for which we have been decrying our peers! But if we must resort to analogies with human reasoning then it is imperative to be as rigorous as possible. In their recent book, A Formal Theory of Commonsense Psychology, How People Think People Think [158], Gordon and Hobbs present a "large-scale logical formalization of commonsense psychology in support of humanlike artificial intelligence" to act as a baseline for researchers building intelligent AI systems. Santos et al. [159] take this a step in the direction we are advocating, by testing whether there is human annotator agreement when categorizing texts into Gordon and Hobbs' theories. "Our end-goal is to advocate for better design of commonsense benchmarks [and to] support the development of a formal logic for commonsense reasoning" [159]. It is difficult to imagine a single formal logic which would afford all of Gordon and Hobbs' 48 categories of reasoning tasks. Besold et al. [7] dedicate several pages to this topic under the heading of Neural-Symbolic Integration in and for Cognitive Science: Building Mental Models. In short, computational modelling of cognitive tasks and especially language processing is still considered a hard challenge.</p>
<p>Technical challenges</p>
<p>There is strong agreement that a successful NeSy system will be characterized by compositionality [1,7,8,18,[160][161][162][163]. Compositionality allows for the construction of new meaning from learned building blocks thus enabling extrapolation beyond the training data distribution. To paraphrase Garcez et al., one should be able to query the trained network using a rich description language at an adequate level of abstraction [1]. The challenge is to come up with dense/compact differentialble representations while preserving the ability to decompose, or unbind, the learned representations for downstream reasoning tasks.</p>
<p>One such system, proposed by Bianchi et al. [83] is the LT N EE - Figure 34 -an extention of Logic Tensor Networks (LTNs), in which pre-trained embeddings are fed into the LTN. They show promising results on small datasets which have the important characteristic of being capable of after-training logical inferences. However, LT N EE is limited by heavy computational requirements as the logic becomes more expressive, for example by the use of quantifiers.</p>
<p>Other studies [116,137] introduce logical inference within their solutions, but all require manually designed rules, and are limited by the domain expertise of the designer. Learning rules from data, or structure learning [164] is an ongoing research topic as pointed out by [17]. In [118] Chaturvedi et al. use fuzzy logic for emotion classification where explicit membership functions are learned. However, as stated by the authors, the classifier becomes very slow with the number of functions.</p>
<p>Other (compiled) approaches involve translating logic into differentialble functions, which are either directly included as network nodes as in [85], or added as a constraint to the loss function, as in [165]. To achieve this, First Order Logic (FOL) can be operationalized using t-norms for example. To address the many types of reasoning as discussed in the previous section, we need to be able to incorporate other types of logic, such as temporal, modal, epistemic, non-monotonic, probabilistic, and more, which, presumably, are better able to model human reasoning.</p>
<p>In summary, formulating logic, or more broadly reasoning, in a differentiable fashion remains challenging.</p>
<p>Limitations &amp; Future Work</p>
<p>We organized our analysis according to the characteristics extracted from the studies to test whether there were any patterns leading to NeSy goals. Another approach would be to reverse this perspective, and look at each goal separately to understand the characteristics leading to its fulfillment. However, each goal is really an entire field of study in and of itself, and we do not think we could have done justice to any of them by taking this approach. We spent a lot of time looking for signal in a very noisy environment where the studies we reviewed had very little in common. More can be said about what we did not find, than what we did. Another approach might be to narrow the criteria for the type of NLP task, while expanding the technical domain. In particular, a subset of tasks from the NLU domain could be a good starting point, as these tasks are often said to require reasoning.</p>
<p>We tried to be comprehensive in respect to the selected studies which led to the trade-off of less space dedicated to technical details or additional context from the neuro-symbolic discussion. There are a lot of ideas and concepts which we did not cover, such as, and in no particular order, Relational Statistical Learning (RSL), Inductive Logic Programming (ILP), DeepProbLog [166], Connectionist Modal Logics (CML), Extreme Learning Machines (ELM), Genetic Programming, grounding and proposinalization, Case Based Reasoning (CBR), Abstract Meaning Representation (AMR), to name but a few, some of which are covered in detail in other surveys [7,8].</p>
<p>Furthermore, we argued that we need differentiable forms of different types of logic, but we did not discuss how they might be implemented. A comprehensive point of reference such as this would be a very valuable contribution to the NeSy community, especially if the implementations were anchored in cognitive science and linguistics as discussed in 7.1.</p>
<p>Finally, the need for common datasets and benchmarks cannot be overstated.</p>
<p>Conclusion</p>
<p>We analyzed recent studies implementing NeSy for NLP in order to test whether the promises of NeSy are materializing in NLP. We attempted to find a pattern in a small and widely variable set of studies, and ultimately we do not believe there are enough results to draw definitive conclusions. Only 59 studies met the criteria for our review, and many of them (in the Sequential category) we would not consider truly integrated NeSy systems. The one thing studies which meet the most goals [77,88,137,143,144,152,155] have in common is that they all belong to the tightly integrated set of NeSy categories, Cooperative and Compiled which is good news for NeSy. Two out of these seven report lower computational cost than baselines, and performance on par or slightly above baselines, though we must reiterate that performance comparisons are not possible as discussed in Section 6.1.5. On the down side, we have seen that some studies suffer from high computational cost, and that explicit reasoning still often requires hand crafted domain specific rules and logic which makes them difficult to scale or generalize to other applications. Indeed, of the five goals, transferability to new domains was the least frequently satisfied.</p>
<p>Our view is that the lack of consensus around theories of reasoning and appropriate benchmarks is hindering our ability to evaluate progress. Hence we advocate for the development of robust reasoning theories and formal logics as well as the development of challenging benchmarks which not only measure the performance of specific implementations, but have the potential to address real world problems. Systems capable of capturing the nuances of natural language (ie., ones that "understand" human reasoning) while returning sound conclusions (ie., perform logical reasoning) could help combat some of the most consequential issues of our times such as mis-and disinformation, corporate propaganda such as climate change denialism, divisive political speech, and other harmful rhetoric in the social discourse.</p>
<p>Fig. 2 .
2Number of Neuro Symbolic articles published since 2010, normalized by the total number of all Computer Science articles published each year. The figure represents the unfiltered results from Scopus given the search keywords described in section 5.2.</p>
<p>Fig. 3 .
3Third Generation Generative Pre-trained Transformer (GPT3)</p>
<p>Fig. 4 .
4Common Natural Language Processing tasks[26].</p>
<p>Fig. 6 .
6Neuro-Symbolic Artificial Intelligence promise areas[1] </p>
<p>Fig. 8 .
8Knowledge representation of  = {A  B  C, B  C  D  E, D  E} using KBANN and CILP.[8] </p>
<p>Fig. 9
9Fig. 9. Logic Tensor Network (LTN) for P(x, y)  A(y) with G(x) = v and G(y) = u; G are grounding (vector representation) for symbols in first-order language. [8]</p>
<p>-
Few-shot Reasoning which addresses the issue of few labeled examples. -Reasoning upon Multi-sources which incorporates additional information from unstructured text. -Dynamic Reasoning which deals with inferring new facts evolving over time. -Analogical Reasoning (AR) which involves the use of past experiences to solve problems that are similar to problems solved before. Case Based Reasoning (CBR) is an example of AR [52]. -Knowledge Graph Pre-training which enables transfer learning for domain adaptation.</p>
<p>Fig. 12 .
12Graph Neural Network (GNN) intuition: generate node embeddings based on local neighborhoods, where nodes aggregate information from their neighbors using neural networks (a). The network neighborhood defines a computation graph such that every node corresponds to a unique computation graph (b).</p>
<p>Fig. 14 .
14Neuro-Symbolic Artificial Intelligence Goals</p>
<p>Fig. 15 .
15Selection Process Diagram</p>
<p>Fig. 16. Study quality</p>
<p>(a) shows the breakdown of conference papers vs journal articles, and Figure 17(b) shows the number of studies published each year.</p>
<p>Fig. 19 .
19Number of studies in each application category 6.1.2</p>
<p>Fig. 21 .
21Relationship between Learning Type, Technical Application, and NeSy Goals. It is clear that supervised approaches dominate the field, are applied across a variety of technical applications, and there is no clear winner when it comes to goals.</p>
<p>Fig. 22 .
22Type of Reasoning and Goals. Around half, 48%, of studies where reasoning is performed explicitly mention interpretability as a feature. While nearly a third of studies performing reasoning implicitly do not meet any of the NeSy promises identified for this review.</p>
<p>Fig. 24 .
24Relationships between leveraged structures and extracted features. As can be seen in a), e), and f), studies leveraging linguistic structures often do not meet any NeSy goals, which runs counter to our original hypothesis. Further investigation into this phenomenon may be warranted. Note: studies which do no leverage either structure are not shown 6.1.5</p>
<p>Fig. 27. English WordNet subgraph [106]</p>
<p>Fig. 30 .
30Type 2 Nested. Given a natural language query and a set of web pages, the system outputs answers for each page. A symbolic reasoner, which uses a custom Domain Specific Language (DSL) to traverse the HTML, interacts with internal neural modules such as BERT which perform a number of Natural Language Processing tasks. What is learned is a DSL program, using only a few labeled examples, which can generalize to a large number of heterogeneous web pages. The authors report large improvements in precision and recall scores over state-of-the art, in some cases over 50 points[115].</p>
<p>Fig. 34 .
34Type 5  Compiled. LT N EE -Using Logic Tensor Networks (LTNs) it is possible to integrate axioms and facts (using first-order fuzzy logic to represent terms, functions, and predicates in a vector space) with commonsense knowledge represented in a sub-symbolic form (based on the principle of distributional semantics and implemented with Word2Vec) in one single model performing well in reasoning tasks.</p>
<p>Fig. 37 .
37Proportion of studies which have met one or more of the 5 goals</p>
<p>Fig. 5. NLU takes as input unstructured text and produces output which can be reasoned over. NLG takes as input structured data and outputs a response in natural language.Natural Language 
Understanding </p>
<p>Natural Language 
Generation </p>
<p>(a) Symbolic view -reasoning is performed 
explicitly via rules and logic </p>
<p>Natural Language 
Understanding </p>
<p>Natural Language 
Generation </p>
<p>(b) Connectionist view -reasoning is performed 
implicitly inside the neural network </p>
<p>Table 1
1Search KeywordsNeural Terms 
Symbolic Terms 
Neuro-Symbolic Terms </p>
<p>sub-symbolic 
symbolic 
neuro-symbolic 
machine learning 
reasoning 
neural-symbolic </p>
<p>deep learning 
logic 
neuro symbolic </p>
<p>neural symbolic 
neurosymbolic </p>
<p>Table 2
2Inclusion/Exclusion CriteriaInclusion ExclusionInput format: unstructured or semi structured text Input format: structured query, images, speech, tabular data, categorical data, or any other data type which is not natural language text.Output format: Any 
Application: Theoretical Papers, Position Papers, 
Surveys, implementations of software pipelines from 
existing models </p>
<p>Application: Implementation of a novel architecture 
The search keywords match, but the actual content does 
not </p>
<p>Language: English 
Full text not available (Authors were contacted in these 
cases) </p>
<p>Total studies identified by 
Scopus, ACL, NeSy 
searches: 
Journal articles = 2,456 
Conference Papers = 
19,006 </p>
<p>Studies screened by title 
and abstract 
Journal articles = 1,519 
Conference papers = 383 </p>
<p>Studies screened for 
accessibility 
Journal articles = 80 
Conference papers = 119 </p>
<p>Studies assessed for 
eligibility based on full 
reading 
Journal articles = 75 
Conference papers = 106 </p>
<p>Studies included in 
analysis 
Journal articles = 15 
Conference papers = 44 </p>
<p>Excluded based on 
automation tools: 
Journal articles = 937 
Conference papers = 
18,623 </p>
<p>Excluded based on 
title and abstract 
Journal articles = 1,439 
Conference papers = 264 </p>
<p>Excluded based on 
inaccessibility 
Journal articles = 5 
Conference papers = 13 </p>
<p>Excluded based on 
inclusion/exclusion criteria 
and study quality 
Journal articles = 60 
Conference papers = 62 </p>
<p>Table 3
3Data extraction featuresFeature 
Description </p>
<p>Business application 
The stated objective or application of the proposed study. Often 
this is an NLP task, but this is not a requirement (i.e., "Medical 
decision support") </p>
<p>Technical application 
Type of model output </p>
<p>Type of learning 
Indicates learning method (supervised, unsupervised, etc.) </p>
<p>Knowledge representation 
One of four categories: Rules, Logic, Frames, and Semantic 
networks </p>
<p>Type of reasoning 
Indicates whether knowledge is represented implicitly (embedded) 
or explicitly (symbolic) </p>
<p>Language structure 
Indicates whether linguistic structure is leveraged to facilitate 
reasoning </p>
<p>Relational structure 
Indicates whether relational structure is leveraged to facilitate 
reasoning (e.g., part-of-speech tags, named entities, etc.) </p>
<p>Symbolic terms 
List of symbolic techniques used by the models </p>
<p>Neural terms 
List of neural architectures used by the models </p>
<p>Datasets 
List of all datasets used for evaluation </p>
<p>Model description 
Describes model architecture schematically </p>
<p>Evaluation Metrics 
Evaluation metrics reported by the authors </p>
<p>Reported score 
Model performance reported by the authors </p>
<p>Contribution 
Novel contribution reported by the authors </p>
<p>Key-intake 
Short description of the study </p>
<p>isNeSy 
Indicates whether the authors label their study as Neuro-Symbolic </p>
<p>NeSy goals 
For each of the goals listed in Section 1, indicates whether the goal 
is met as reported by the authors </p>
<p>Kautz category 
List of categories from Kautz's taxonomy </p>
<p>NeSy category 
List of categories from the proposed nomenclature </p>
<p>Study quality 
Percentage of positive answers in the quality assessment 
questionnaire </p>
<p>Convolutional Neural Network (CNN)input </p>
<p>convolution 
pooling </p>
<p>fully 
connected output </p>
<p>Multilayer Perceptron (MLP) </p>
<p>input 
output 
hidden layers </p>
<p>Neuroevolution (NE) </p>
<p>evaluation 
selection </p>
<p>crossover 
mutation </p>
<p>Graph Neural Network (GNN) </p>
<p>neural network 
input graph </p>
<p>neighborhood aggregation </p>
<p>Sequence-to-Sequence (Seq2Seq) </p>
<p>encoder </p>
<p>decoder 
input </p>
<p>output </p>
<p>x 1 
x 2 
x n-1 
x n </p>
<p>encoder state 
RNN 
RNN 
RNN 
RNN 
RNN 
RNN 
RNN 
RNN </p>
<p>Transformer (e.g. BERT) </p>
<p>encoder </p>
<p>decoder </p>
<p>output </p>
<p>y 1 
y 2 
y n-1 
y n </p>
<p>y 1 
y 2 
y n-1 </p>
<p>input </p>
<p>attention </p>
<p>x 1 
x 2 
x n-1 
x n </p>
<p>h 1 </p>
<p>h 0 </p>
<p>h 2 
h n-1 
h n 
s 1 
s 2 
s n-1 
s n </p>
<p>RNN 
RNN 
RNN 
RNN 
Z </p>
<p>W </p>
<p>RNN 
RNN 
RNN 
RNN </p>
<p>WX+b 
WX+b 
WX+b 
WX+b </p>
<p>Fig. 25. Neural architectures represented in Table 4 </p>
<p>Logic Tensor Networks(LTN) 
Recursive Neural Knowledge Networks (RNKN) </p>
<p>Tensor Product Representation (TPR) 
Logic Neural Networks (LNN) </p>
<p>u = u 1 , . . . .,u n  </p>
<p>G(P(v,u)  A(u)) </p>
<p>v = v 1 , . . . .,v n  </p>
<p>1-</p>
<p>uP </p>
<p>th 
th </p>
<ul>
<li></li>
<li></li>
</ul>
<p>W 1 </p>
<p>P </p>
<p>W 2 </p>
<p>P </p>
<p>V 1 </p>
<p>P </p>
<p>V 2 </p>
<p>P </p>
<p>B 1 </p>
<p>P </p>
<p>B 2 </p>
<p>P </p>
<p>1-</p>
<p>th 
th </p>
<p>Fig. 28. Proposed Neuro-Symbolic Artificial Intelligence categories. Adapted from Henry Kautz.Neuro-Symbolic Categories </p>
<p>Sequential 
Ensemble </p>
<p>Fibring </p>
<p>Integrated </p>
<p>Nested </p>
<ol>
<li>
<p>symbolic Neuro symbolic </p>
</li>
<li>
<p>Symbolic[Neuro] </p>
</li>
<li>
<p>Neuro[Symbolic] </p>
</li>
<li>
<p>Neuro; Symbolic </p>
</li>
<li>
<p>Neuro: Symbolic  Neuro </p>
</li>
<li>Neuro_Symbolic </li>
</ol>
<p>Cooperative </p>
<p>Compiled </p>
<p>Table 4 Neural
4&amp; Symbolic Combinations 
1 2 3 4 5 Number of NeSy goals satisfied out of the 5 described in Section 4. 
Note: some studies use multiple techniques. </p>
<p>Knowledge Representation </p>
<p>Frames 
Logic 
Rules 
Semantic </p>
<p>network </p>
<p>Linear Models 
SVM 
[107] 
[108] [88] </p>
<p>Early 
Generation </p>
<p>MLP 
[109] </p>
<p>[110, 111] </p>
<p>[112, 113] </p>
<p>[114] </p>
<p>[115] </p>
<p>[116] 
[81] </p>
<p>CNN 
[117] 
[113] [118] 
[119] 
[120] </p>
<p>Graphical </p>
<p>Models </p>
<p>DBN 
[118] </p>
<p>GNN 
[80] 
[76] [82] 
[78, 81] </p>
<p>[79] [77] </p>
<p>Sequence-</p>
<p>to-Sequence </p>
<p>RNN 
[117, 121] 
[122] [123] </p>
<p>[124], [125], [126] </p>
<p>[127] [118, 128] </p>
<p>[129] </p>
<p>[130] [131] [132] </p>
<p>[133] [134] [135] </p>
<p>[136] [137] </p>
<p>[120, 138] </p>
<p>[139] [140] </p>
<p>[141] </p>
<p>RcNN 
[85] 
[142] </p>
<p>Transformer 
[143, 144] </p>
<p>[145] [146] </p>
<p>[147, 148] </p>
<p>[129, 149] </p>
<p>[150], [134] </p>
<p>[151] [152] </p>
<p>[138] [153] </p>
<p>[78, 154] </p>
<p>[81] </p>
<p>Neuro-</p>
<p>Symbolic </p>
<p>LTN 
[83] </p>
<p>RNKN 
[85] </p>
<p>LNN 
[155] 
[152] </p>
<p>TPR 
[123] 
[142] </p>
<p>Neuroevolution 
[88] </p>
<p>S </p>
<p>N </p>
<p>V </p>
<p>D 
N </p>
<p>NP </p>
<p>VP </p>
<ul>
<li></li>
</ul>
<p>WSD, POS, NER, Generalization </p>
<p>Symbolic knowledge extraction 
with WordNet and linguistic structures </p>
<p>Skipgram 
model 
(Word2Vec, 
Fasttext) </p>
<p>Seq2Seq 
generative 
Neural 
Network </p>
<p>Real valued 
vector 
representation </p>
<p>OUTPUT: 
summary </p>
<p>INPUT: 
Unstructured 
text document </p>
<p>https://henrykautz.com/talks/index.htmlVisual Representation </p>
<p>Obj 1 
Obj 2 
Obj 3 
Obj 4 </p>
<p>Concept Embeddings </p>
<p>Semantic Parsing (Candidate Interpretations) </p>
<p>Back-propagation </p>
<p>Symbolic Reasoning </p>
<p>Answer: Cylinder 
Groundtruth: Box </p>
<p>REINFORCE </p>
<p>Back-propagation </p>
<p>Sphere </p>
<p>Query(Shape, Filter(Red, Relate(Left, </p>
<p>category Fig. 35. Number of studies per category 4. neuro: symbolic  neuroFig. 36. NeSy categories to NeSy Goals. There is no obvious pattern with respect to what types of goals are met within each of the NeSy categories.5. neuro_symbolic </p>
<ol>
<li>
<p>neuro; symbolic </p>
</li>
<li>
<p>symbolic[neuro] </p>
</li>
<li>
<p>symbolic neuro symbolic </p>
</li>
</ol>
<p>Compiled </p>
<p>Cooperative </p>
<p>Nested </p>
<p>Sequential 
Reasoning </p>
<p>OOD </p>
<p>Interpretability </p>
<p>Reduced data </p>
<p>Transferability </p>
<p>None </p>
<p>Table 5
5Number of studies meeting each goal. The Promise Ratio represents the percentage of goals reported to have been met out of the total number of possible goals (# of studies * 5 goals) in each category.Compiled 
Cooperative 
Nested 
Sequential </p>
<p>Reasoning 
12 
5 
3 
14 </p>
<p>OOD 
9 
3 
1 
2 
Interpretability 
8 
4 
2 
6 </p>
<p>Reduced data 
6 
4 
2 
3 </p>
<p>Transferability 
7 
2 
1 
2 </p>
<p>Promise Ratio 
29.5% 
40% 
45% 
21.6% </p>
<p>Table 6
6NeSy Promises reported as having been met ( y = yes, n = no)Ref. 
Score 
Reasoning 
OOD 
Generalization 
Interpretability 
Reduced 
Data 
Transferability 
isNeSy </p>
<p>[155] 
5 
y 
y 
y 
y 
y 
y </p>
<p>[137, 143] 
4 
y 
y 
y 
y 
n 
y 
[144] 
4 
y 
y 
y 
n 
y 
y </p>
<p>[77] 
4 
y 
n 
y 
y 
y 
n </p>
<p>[88, 152] 
4 
n 
y 
y 
y 
y 
y 
[83, 136] 
3 
y 
y 
y 
n 
n 
y </p>
<p>[116] 
3 
y 
n 
y 
y 
n 
n 
[123] 
3 
y 
n 
y 
n 
y 
n </p>
<p>[82] 
3 
n 
y 
n 
y 
y 
n </p>
<p>[133] 
2 
y 
y 
n 
n 
n 
y 
[129, 132] 
2 
y 
n 
y 
n 
n 
y </p>
<p>[79, 85, 114, 128] 
2 
y 
n 
y 
n 
n 
n </p>
<p>[130, 141] 
2 
y 
n 
n 
y 
n 
n 
[134, 135] 
2 
y 
n 
n 
n 
y 
y </p>
<p>[118] 
2 
y 
n 
n 
n 
y 
n 
[131] 
2 
n 
y 
n 
y 
n 
y </p>
<p>[149, 151] 
2 
n 
y 
n 
y 
n 
n </p>
<p>[111] 
2 
n 
y 
n 
n 
y 
n 
[110] 
2 
n 
n 
y 
n 
y 
n </p>
<p>[80, 147] 
1 
y 
n 
n 
n 
n 
y </p>
<p>[76, 78, 108, 122] 
[112, 113, 119, 140] </p>
<p>[81, 107, 127] </p>
<p>1 
y 
n 
n 
n 
n 
n </p>
<p>[154] 
1 
n 
y 
n 
n 
n 
y </p>
<p>[146] 
1 
n 
n 
y 
n 
n 
y </p>
<p>[153] 
1 
n 
n 
y 
n 
n 
n 
[115, 148] 
1 
n 
n 
n 
y 
n 
y </p>
<p>[125, 139, 145, 150] 
0 
n 
n 
n 
n 
n 
y </p>
<p>[121, 124, 126, 138] 
[109, 117, 120, 142] 
0 
n 
n 
n 
n 
n 
n </p>
<p>https://plato.stanford.edu/entries/compositionality/#FormStat 5 https://www.britannica.com/topic/universal-grammar 6 https://www.nltk.org/ 7 https://spacy.io/
https://www.w3.org/2007/OWL/wiki/Direct_Semantics 9 https://www.aclweb.org/portal/
Energy consumption is particularly significant when training Large Language Models which can cost in the thousands if not millions of dollars in electricity[19].15 Henry Kautz introduced a taxonomy of NeSy types at the Third AAAI Conference on AI[10]. We rely on this taxonomy to classify the studies under review, and discuss each type in detail in section 6.2.3
References to relevant works are not provided.20 While a detailed review of GNNs in NLP is beyond the scope of this work, we point the interested reader to an online resource dedicated to this topic: https://github.com/naganandy/graph-based-deep-learning-literature#computational-linguistics-conferences.
As expressed by Luis Lamb at https://video.ibm.com/recorded/131288165 22 https://github.com/kyleiwaniec/neuro-symbolic-ai-systematic-review
https://service.elsevier.com/app/answers/detail/a_id/14880/kw/citescore/supporthub/scopus/ 24 https://service.elsevier.com/app/answers/detail/a_id/14883/supporthub/scopus/related/1/
https://video.ibm.com/recorded/131288165 26 https://video.ibm.com/recorded/131288165 time-marker 43:00 27 https://video.ibm.com/recorded/131288165 time-marker 50:00 28 https://github.com/kwchurch/Benchmarking_past_present_future#S1
https://researcher.watson.ibm.com/researcher/view_group.php?id=10897
AcknowledgementsThis publication has emanated from research supported in part by a grant from Science Foundation Ireland under Grant number 18/CRT/6183. For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.Appendix A. NeSy and Kautz CategoriesTable 7NeSy and Kautz Categories NeSy (Ours)KautzRefs. Appendix C. VenuesTable 9Venues referred in the studyAppendix B. Allowed Values
A D Garcez, L C Lamb, 10.48550/ARXIV.2012.05876Neurosymbolic AI: The 3rd Wave, arXiv. 2020A.d. Garcez and L.C. Lamb, Neurosymbolic AI: The 3rd Wave, arXiv, 2020. doi:10.48550/ARXIV.2012.05876.</p>
<p>Three Problems in Computer Science. L G Valiant, 10.1145/602382.602410Journal of the ACM. 501L.G. Valiant, Three Problems in Computer Science, Journal of the ACM 50(1) (2003), 96-99. doi:10.1145/602382.602410.</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez,  Kaiser, I Polosukhin, Advances in neural information processing systems. 30A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, . Kaiser and I. Polosukhin, Attention is all you need, in: Advances in neural information processing systems, Vol. 30, 2017, pp. 5998-6008.</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota, 2019, pp. 4171-4186. doi:10.18653/v1/N19-1423.</p>
<p>Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution, arXiv. J Pearl, 10.48550/ARXIV.1801.04016J. Pearl, Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution, arXiv, 2018. doi:10.48550/ARXIV.1801.04016.</p>
<p>G Marcus, 10.48550/ARXIV.1801.00631Deep Learning: A Critical Appraisal, arXiv. G. Marcus, Deep Learning: A Critical Appraisal, arXiv, 2018. doi:10.48550/ARXIV.1801.00631.</p>
<p>T R Besold, A D Garcez, S Bader, H Bowman, P Domingos, P Hitzler, K.-U Kuehnberger, L C Lamb, D Lowd, P M V Lima, L De Penning, G Pinkas, H Poon, G Zaverucha, 10.48550/ARXIV.1711.03902Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. 2017T.R. Besold, A.d. Garcez, S. Bader, H. Bowman, P. Domingos, P. Hitzler, K.-U. Kuehnberger, L.C. Lamb, D. Lowd, P.M.V. Lima, L. de Penning, G. Pinkas, H. Poon and G. Zaverucha, Neural-Symbolic Learning and Reasoning: A Survey and Interpretation, arXiv, 2017. doi:10.48550/ARXIV.1711.03902.</p>
<p>A D Garcez, M Gori, L C Lamb, L Serafini, M Spranger, S N Tran, 10.48550/ARXIV.1905.06088Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning. arXivA.d. Garcez, M. Gori, L.C. Lamb, L. Serafini, M. Spranger and S.N. Tran, Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning, arXiv, 2019. doi:10.48550/ARXIV.1905.06088. https://arxiv.org/abs/1905.06088.</p>
<p>. Y Bengio, G Marcus, V Boucher, A I Debate! Yoshua Bengio Vs Gary, Montreal Ai Marcus, Y. Bengio, G. Marcus and V. Boucher, AI DEBATE! Yoshua Bengio vs Gary Marcus, Montreal.AI. https://montrealartificialintelligence.com/aidebate/.</p>
<p>The Third AI Summer. H Kautz, Thirty-fourth AAAI Conference on Artificial Intelligence. New York, NYH. Kautz, The Third AI Summer, AAAI Robert S. Engelmore Memorial Lecture, Thirty-fourth AAAI Conference on Artificial Intelligence, New York, NY. https://henrykautz.com/talks/index.html.</p>
<p>M K Sarker, L Zhou, A Eberhart, P Hitzler, 10.48550/ARXIV.2105.05330Neuro-Symbolic Artificial Intelligence: Current Trends. 2021arXivM.K. Sarker, L. Zhou, A. Eberhart and P. Hitzler, Neuro-Symbolic Artificial Intelligence: Current Trends, arXiv, 2021. doi:10.48550/ARXIV.2105.05330.</p>
<p>System 2 Deep Learning: Higher-Level Cognition, Agency, Out-of-Distribution Generalization and Causality. Y Bengio, 30th International Joint Conference on Artificial Intelligence. Y. Bengio, System 2 Deep Learning: Higher-Level Cognition, Agency, Out-of-Distribution Generalization and Causality, 30th International Joint Conference on Artificial Intelligence. https://ijcai-21.org/invited-talks/.</p>
<p>. D Kahneman, Thinking, Farrar, Giroux Straus, New YorkD. Kahneman, Thinking, fast and slow, Farrar, Straus and Giroux, New York, 2011. ISBN 9780374275631 0374275637.</p>
<p>Z Liu, Z Wang, Y Lin, H Li, arXiv:2203.10557A Neural-Symbolic Approach to Natural Language Understanding (2022). csZ. Liu, Z. Wang, Y. Lin and H. Li, A Neural-Symbolic Approach to Natural Language Understanding (2022), arXiv:2203.10557 [cs].</p>
<p>J Zhang, B Chen, L Zhang, X Ke, H Ding, 10.1016/j.aiopen.2021.03.001Neural, symbolic and neural-symbolic reasoning on knowledge graphs. 2J. Zhang, B. Chen, L. Zhang, X. Ke and H. Ding, Neural, symbolic and neural-symbolic reasoning on knowledge graphs, AI Open 2 (2021), 14-35. doi:10.1016/j.aiopen.2021.03.001.</p>
<p>L C Lamb, A D Garcez, M Gori, M O R Prates, P H C Avelar, M Y Vardi, 10.24963/ijcai.2020/679Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, International Joint Conferences on Artificial Intelligence Organization. the Twenty-Ninth International Joint Conference on Artificial Intelligence, International Joint Conferences on Artificial Intelligence OrganizationGraph Neural Networks Meet Neural-Symbolic Computing: A Survey and PerspectiveL.C. Lamb, A.d. Garcez, M. Gori, M.O.R. Prates, P.H.C. Avelar and M.Y. Vardi, Graph Neural Networks Meet Neural- Symbolic Computing: A Survey and Perspective, in: Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, International Joint Conferences on Artificial Intelligence Organization, 2020, pp. 4877-4884. ISBN 978-0-9992411-6-5. doi:10.24963/ijcai.2020/679.</p>
<p>Informed Machine Learning -A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems. L Von Rueden, S Mayer, K Beckh, B Georgiev, S Giesselbach, R Heese, B Kirsch, M Walczak, J Pfrommer, A Pick, R Ramamurthy, J Garcke, C Bauckhage, J Schuecker, 10.1109/TKDE.2021.3079836IEEE Transactions on Knowledge and Data Engineering. L. von Rueden, S. Mayer, K. Beckh, B. Georgiev, S. Giesselbach, R. Heese, B. Kirsch, M. Walczak, J. Pfrommer, A. Pick, R. Ramamurthy, J. Garcke, C. Bauckhage and J. Schuecker, Informed Machine Learning -A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems, IEEE Transactions on Knowledge and Data Engineering (2021), 1-1. doi:10.1109/TKDE.2021.3079836.</p>
<p>V Belle, 978-3-030-58449-8Symbolic Logic Meets Machine Learning: A Brief Survey in Infinite Domains. Springer International PublishingScalable Uncertainty ManagementV. Belle, Symbolic Logic Meets Machine Learning: A Brief Survey in Infinite Domains, in: Scalable Uncertainty Management, Springer International Publishing, 2020, pp. 3-16. ISBN 978-3-030-58449-8.</p>
<p>O Sharir, B Peleg, Y Shoham, 10.48550/arXiv.2004.08900The Cost of Training NLP Models: A Concise Overview. O. Sharir, B. Peleg and Y. Shoham, The Cost of Training NLP Models: A Concise Overview, ArXiv (2020). doi:10.48550/arXiv.2004.08900.</p>
<p>GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about | MIT Technology Review. G Marcus, E Davis, G. Marcus and E. Davis, GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about | MIT Technology Review. https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/.</p>
<p>. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, 10.48550/ARXIV.2005.14165Language Models are Few-Shot Learners. T.B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D.M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever and D. Amodei, Language Models are Few-Shot Learners (2020). doi:10.48550/ARXIV.2005.14165.</p>
<p>The socratic method: A practitioner's handbook, David R. W Farnsworth, Godine Publisher IncW. Farnsworth, The socratic method: A practitioner's handbook, David R. Godine Publisher Inc, 2021.</p>
<p>P Engel, Rationality Reasoning, 10.4324/9780203486030Dictionary of cognitive science neuroscience, psychology, Artificial Intelligence, linguistics, and philosophy. Taylor and FrancisP. Engel, Reasoning and Rationality, in: Dictionary of cognitive science neuroscience, psychology, Artificial Intelligence, linguistics, and philosophy, Taylor and Francis, 2003, pp. 315-316. doi:https://doi.org/10.4324/9780203486030.</p>
<p>D Kahneman, O Sibony, C R Sunstein, 978-0- 00-830900-8Noise: A Flaw in Human Judgment. HarperCollins Publishers LimitedD. Kahneman, O. Sibony and C.R. Sunstein, Noise: A Flaw in Human Judgment, HarperCollins Publishers Limited, 2021. ISBN 978-0- 00-830900-8.</p>
<p>Neural-symbolic integration and the Semantic Web. P Hitzler, F Bianchi, M Ebrahimi, M K Sarker, 10.3233/SW-190368Semantic Web. 111P. Hitzler, F. Bianchi, M. Ebrahimi and M.K. Sarker, Neural-symbolic integration and the Semantic Web, Semantic Web 11(1) (2020), 3-11. doi:10.3233/SW-190368.</p>
<p>Understanding Natural Language Understanding. B Maccartney, ACM SIGAI Bay Area Chapter Inaugural Meeting. San Mateo, CAB. MacCartney, Understanding Natural Language Understanding, ACM SIGAI Bay Area Chapter Inaugural Meeting, San Mateo, CA. https://www.youtube.com/watch?v=vcPd0V4VSNU.</p>
<p>ELIZA-a computer program for the study of natural language communication between man and machine. J Weizenbaum, Communications of the ACM. 91J. Weizenbaum, ELIZA-a computer program for the study of natural language communication between man and machine, Communications of the ACM 9(1) (1966), 36-45.</p>
<p>Introduction to. D A Ferrucci, 10.1147/JRD.2012.2184356IBM Journal of Research and Development. 56This is WatsonD.A. Ferrucci, Introduction to "This is Watson", IBM Journal of Research and Development 56(3.4) (2012), 1:1-1:15. doi:10.1147/JRD.2012.2184356.</p>
<p>Long Short-Term Memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Computation. 98S. Hochreiter and J. Schmidhuber, Long Short-Term Memory, Neural Computation 9(8) (1997), 1735-1780. doi:10.1162/neco.1997.9.8.1735.</p>
<p>An extended model of natural logic. B Maccartney, C D Manning, Proceedings of the Eight International Conference on Computational Semantics. the Eight International Conference on Computational SemanticsTilburg, The NetherlandsAssociation for Computational LinguisticsB. MacCartney and C.D. Manning, An extended model of natural logic, in: Proceedings of the Eight International Conference on Computational Semantics, Association for Computational Linguistics, Tilburg, The Netherlands, 2009, pp. 140-156.</p>
<p>Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation. A Gatt, E Krahmer, 10.1613/jair.5477Journal of Artificial Intelligence Research. 61A. Gatt and E. Krahmer, Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation, Journal of Artificial Intelligence Research 61 (2018), 65-170. doi:10.1613/jair.5477.</p>
<p>D Yu, B Yang, D Liu, H Wang, 10.48550/ARXIV.2111.08164A Survey on Neural-symbolic Systems. 2021arXivD. Yu, B. Yang, D. Liu and H. Wang, A Survey on Neural-symbolic Systems, arXiv, 2021. doi:10.48550/ARXIV.2111.08164.</p>
<p>S Bader, P Hitzler, Dimensions of Neural-symbolic Integration -A Structured Survey. S.N, College PublicationsOneWe Will Show Them! Essays in Honour of Dov GabbayS. Bader and P. Hitzler, Dimensions of Neural-symbolic Integration -A Structured Survey, in: We Will Show Them! Essays in Honour of Dov Gabbay, Volume One, S.N, College Publications, 2005.</p>
<p>Knowledge-based artificial neural networks. G G Towell, J W Shavlik, Artificial intelligence. 701-2G.G. Towell and J.W. Shavlik, Knowledge-based artificial neural networks, Artificial intelligence 70(1-2) (1994), 119-165.</p>
<p>Neural-symbolic learning systems: foundations and applications. A S Garcez, K Broda, D M Gabbay, Springer Science &amp; Business MediaA.S.d. Garcez, K. Broda, D.M. Gabbay et al., Neural-symbolic learning systems: foundations and applications, Springer Science &amp; Business Media, 2002.</p>
<p>Learning and Reasoning with Logic Tensor Networks, in: AI<em>IA 2016 Advances in Artificial Intelligence. L Serafini, A S Garcez, 10.1007/978-3-319-49130-1_25Lecture Notes in Computer Science. G. Adorni, S. Cagnoni, M. Gori and M. MarateaSpringer International PublishingL. Serafini and A.S. d'Avila Garcez, Learning and Reasoning with Logic Tensor Networks, in: AI</em>IA 2016 Advances in Artificial Intelligence, G. Adorni, S. Cagnoni, M. Gori and M. Maratea, eds, Lecture Notes in Computer Science, Springer International Publishing, 2016, pp. 334-348. ISBN 978-3-319-49130-1. doi:10.1007/978-3-319-49130-1_25.</p>
<p>Reasoning With Neural Tensor Networks for Knowledge Base Completion. R Socher, D Chen, C D Manning, A Ng, Advances in Neural Information Processing Systems. Curran Associates, Inc26R. Socher, D. Chen, C.D. Manning and A. Ng, Reasoning With Neural Tensor Networks for Knowledge Base Completion, in: Advances in Neural Information Processing Systems, Vol. 26, Curran Associates, Inc., 2013.</p>
<p>Inductive logic programming. S Muggleton, 10.1007/BF03037089New Generation Computing. 84S. Muggleton, Inductive logic programming, New Generation Computing 8(4) (1991), 295-318. doi:10.1007/BF03037089.</p>
<p>Differentiable learning of logical rules for knowledge base reasoning, Advances in neural information processing systems. F Yang, Z Yang, W W Cohen, 30F. Yang, Z. Yang and W.W. Cohen, Differentiable learning of logical rules for knowledge base reasoning, Advances in neural information processing systems 30 (2017).</p>
<p>Learning explanatory rules from noisy data. R Evans, E Grefenstette, Journal of Artificial Intelligence Research. 61R. Evans and E. Grefenstette, Learning explanatory rules from noisy data, Journal of Artificial Intelligence Research 61 (2018), 1-64.</p>
<p>Learning knowledge base inference with neural theorem provers. T Rocktschel, S Riedel, Proceedings of the 5th workshop on automated knowledge base construction. the 5th workshop on automated knowledge base constructionT. Rocktschel and S. Riedel, Learning knowledge base inference with neural theorem provers, in: Proceedings of the 5th workshop on automated knowledge base construction, 2016, pp. 45-50.</p>
<p>H Dong, J Mao, T Lin, C Wang, L Li, D Zhou, 10.48550/ARXIV.1904.11694Neural Logic Machines. arXivH. Dong, J. Mao, T. Lin, C. Wang, L. Li and D. Zhou, Neural Logic Machines, arXiv, 2019. doi:10.48550/ARXIV.1904.11694.</p>
<p>Multiple instantiation and rule mediation in SHRUTI. C Wendelken, L Shastri, Connection Science. 163C. Wendelken and L. Shastri, Multiple instantiation and rule mediation in SHRUTI, Connection Science 16(3) (2004), 211-217.</p>
<p>Notions of explainability and evaluation approaches for explainable artificial intelligence. G Vilone, L Longo, Information Fusion. 76G. Vilone and L. Longo, Notions of explainability and evaluation approaches for explainable artificial intelligence, Information Fusion 76 (2021), 89-106.</p>
<p>Introduction to statistical relational learning. D Koller, N Friedman, S Deroski, C Sutton, A Mccallum, A Pfeffer, P Abbeel, M.-F Wong, C Meek, J Neville, MIT pressD. Koller, N. Friedman, S. Deroski, C. Sutton, A. McCallum, A. Pfeffer, P. Abbeel, M.-F. Wong, C. Meek, J. Neville et al., Introduction to statistical relational learning, MIT press, 2007.</p>
<p>L De Raedt, A Kimmig, H Toivonen, ProbLog: A Probabilistic Prolog and Its Application in Link Discovery., in: IJCAI. 7L. De Raedt, A. Kimmig and H. Toivonen, ProbLog: A Probabilistic Prolog and Its Application in Link Discovery., in: IJCAI, Vol. 7, Hyderabad, 2007, pp. 2462-2467.</p>
<p>Markov logic networks. M Richardson, P Domingos, 10.1007/s10994-006-5833-1Machine Learning. 621M. Richardson and P. Domingos, Markov logic networks, Machine Learning 62(1) (2006), 107-136. doi:10.1007/s10994-006-5833-1.</p>
<p>Generalizing from a few examples: A survey on few-shot learning. Y Wang, Q Yao, J T Kwok, L M Ni, ACM computing surveys (csur). 533Y. Wang, Q. Yao, J.T. Kwok and L.M. Ni, Generalizing from a few examples: A survey on few-shot learning, ACM computing surveys (csur) 53(3) (2020), 1-34.</p>
<p>A S D Garcez, D M Gabbay, Proceedings of 19th National Conference on Artificial Intelligence -AAAI-2004. 19th National Conference on Artificial Intelligence -AAAI-2004AAAI PressFibring neural networksA.S.d. Garcez and D.M. Gabbay, Fibring neural networks, in: Proceedings of 19th National Conference on Artificial Intelligence -AAAI- 2004, AAAI Press, 2004, pp. 342-347.</p>
<p>Connectionist perspectives on language learning, representation and processing. M F Joanisse, J L Mcclelland, Wiley Interdisciplinary Reviews: Cognitive Science. 63M.F. Joanisse and J.L. McClelland, Connectionist perspectives on language learning, representation and processing, Wiley Interdisciplinary Reviews: Cognitive Science 6(3) (2015), 235-247.</p>
<p>Words and rules. S Pinker, Lingua. 1061-4S. Pinker, Words and rules, Lingua 106(1-4) (1998), 219-242.</p>
<p>Analogical and Case-Based Reasoning. R D Sriram, 10.1007/978-1-4471-0631-9_6Intelligent Systems for Engineering: A Knowledge-based Approach. London, LondonSpringerR.D. Sriram, Analogical and Case-Based Reasoning, in: Intelligent Systems for Engineering: A Knowledge-based Approach, Springer London, London, 1997, pp. 285-334. ISBN 978-1-4471-0631-9. doi:10.1007/978-1-4471-0631-9_6.</p>
<p>W L Hamilton, R Ying, J Leskovec, 10.48550/ARXIV.1709.05584Representation Learning on Graphs: Methods and Applications. W.L. Hamilton, R. Ying and J. Leskovec, Representation Learning on Graphs: Methods and Applications (2017). doi:10.48550/ARXIV.1709.05584.</p>
<p>Pointer networks. O Vinyals, M Fortunato, N Jaitly, Advances in neural information processing systems. 28O. Vinyals, M. Fortunato and N. Jaitly, Pointer networks, Advances in neural information processing systems 28 (2015).</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, ArXiv abs/1609.02907T.N. Kipf and M. Welling, Semi-supervised classification with graph convolutional networks. 2017, ArXiv abs/1609.02907 (2017).</p>
<p>The graph neural network model. F Scarselli, M Gori, A C Tsoi, M Hagenbuchner, G Monfardini, IEEE transactions on neural networks. 201F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner and G. Monfardini, The graph neural network model, IEEE transactions on neural networks 20(1) (2008), 61-80.</p>
<p>P Velickovic, G Cucurull, A Casanova, A Romero, P Li, Y Bengio, CoRR abs/1710.10903Graph Attention Networks. P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li and Y. Bengio, Graph Attention Networks, CoRR abs/1710.10903 (2017).</p>
<p>Graph convolutional networks for text classification. L Yao, C Mao, Y Luo, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence33L. Yao, C. Mao and Y. Luo, Graph convolutional networks for text classification, in: Proceedings of the AAAI conference on artificial intelligence, Vol. 33, 2019, pp. 7370-7377.</p>
<p>Modeling relational data with graph convolutional networks, in: European semantic web conference. M Schlichtkrull, T N Kipf, P Bloem, R Berg, I Titov, M Welling, SpringerM. Schlichtkrull, T.N. Kipf, P. Bloem, R.v.d. Berg, I. Titov and M. Welling, Modeling relational data with graph convolutional networks, in: European semantic web conference, Springer, 2018, pp. 593-607.</p>
<p>F Van Harmelen, A T Teije, arXiv:1905.12389A boxology of design patterns for hybrid learning and reasoning systems. arXiv preprintF. Van Harmelen and A.t. Teije, A boxology of design patterns for hybrid learning and reasoning systems, arXiv preprint arXiv:1905.12389 (2019).</p>
<p>978-3-540-73953-1Perspectives of Neural-Symbolic Integration. B. Hammer and P. HitzlerSpringer77B. Hammer and P. Hitzler (eds), Perspectives of Neural-Symbolic Integration, Vol. 77, Springer, 2007. ISBN 978-3-540-73953-1.</p>
<p>A S Garcez, L C Lamb, D M Gabbay, 10.1007/978-3-540-73246-4.Neural-Symbolic Cognitive Reasoning, Cognitive Technologies. SpringerA.S. Garcez, L.C. Lamb and D.M. Gabbay, Neural-Symbolic Cognitive Reasoning, Cognitive Technologies, Springer, 2009. ISBN 978-3- 540-73245-7. doi:10.1007/978-3-540-73246-4..</p>
<p>E Gabrilovich, R Guha, A Mccallum, K Murphy, 978-1-57735-707-0Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches. Palo Alto, CaliforniaThe AAAI PressE. Gabrilovich, R. Guha, A. McCallum and K. Murphy, Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, The AAAI Press, Palo Alto, California., 2015. ISBN 978-1-57735-707-0.</p>
<p>Towards integrated neural-symbolic systems for human-level AI: Two research programs helping to bridge the gaps. T R Besold, K.-U Khnberger, 10.1016/j.bica.2015.09.003Biologically Inspired Cognitive Architectures. 14T.R. Besold and K.-U. Khnberger, Towards integrated neural-symbolic systems for human-level AI: Two research programs helping to bridge the gaps, Biologically Inspired Cognitive Architectures 14 (2015), 97-110. doi:10.1016/j.bica.2015.09.003.</p>
<p>Z Shen, J Liu, Y He, X Zhang, R Xu, H Yu, P Cui, 10.48550/ARXIV.2108.13624Towards Out-Of-Distribution Generalization: A Survey. Z. Shen, J. Liu, Y. He, X. Zhang, R. Xu, H. Yu and P. Cui, Towards Out-Of-Distribution Generalization: A Survey (2021). doi:10.48550/ARXIV.2108.13624.</p>
<p>Interpretable machine learning: Fundamental principles and 10 grand challenges. C Rudin, C Chen, Z Chen, H Huang, L Semenova, C Zhong, Statistics Surveys. 16C. Rudin, C. Chen, Z. Chen, H. Huang, L. Semenova and C. Zhong, Interpretable machine learning: Fundamental principles and 10 grand challenges, Statistics Surveys 16 (2022), 1-85.</p>
<p>SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. H Jiang, P He, W Chen, X Liu, J Gao, T Zhao, 10.18653/v1/2020.acl-main.197Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsH. Jiang, P. He, W. Chen, X. Liu, J. Gao and T. Zhao, SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Online, 2020, pp. 2177-2190. doi:10.18653/v1/2020.acl-main.197.</p>
<p>F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, H Zhu, H Xiong, Q He, 10.1109/JPROC.2020.3004555A Comprehensive Survey on Transfer Learning. 109F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong and Q. He, A Comprehensive Survey on Transfer Learning, Proceedings of the IEEE 109(1) (2021), 43-76. doi:10.1109/JPROC.2020.3004555.</p>
<p>. Encyclopaedia Reasoning, Britannica, Reasoning, Encyclopaedia Britannica, inc. https://www.britannica.com/technology/artificial-intelligence/Reasoning.</p>
<p>Procedures for performing systematic reviews. B Kitchenham, 33Keele, UK, Keele UniversityB. Kitchenham, Procedures for performing systematic reviews, Keele, UK, Keele University 33(2004) (2004), 1-26.</p>
<p>Synthesizing information systems knowledge: A typology of literature reviews. G Par, M.-C Trudel, M Jaana, S Kitsiou, 10.1016/j.im.2014.08.008Information &amp; Management. 522G. Par, M.-C. Trudel, M. Jaana and S. Kitsiou, Synthesizing information systems knowledge: A typology of literature reviews, Information &amp; Management 52(2) (2015), 183-199. doi:https://doi.org/10.1016/j.im.2014.08.008.</p>
<p>The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, I Boutron, T C Hoffmann, C D Mulrow, L Shamseer, J M Tetzlaff, E A Akl, S E Brennan, R Chou, J Glanville, J M Grimshaw, A Hrbjartsson, M M Lalu, T Li, E W Loder, E Mayo-Wilson, S Mcdonald, L A Mcguinness, L A Stewart, J Thomas, A C Tricco, V A Welch, P Whiting, D Moher, 10.1186/s13643-021-01626-4Systematic Reviews. 10189M.J. Page, J.E. McKenzie, P.M. Bossuyt, I. Boutron, T.C. Hoffmann, C.D. Mulrow, L. Shamseer, J.M. Tetzlaff, E.A. Akl, S.E. Brennan, R. Chou, J. Glanville, J.M. Grimshaw, A. Hrbjartsson, M.M. Lalu, T. Li, E.W. Loder, E. Mayo-Wilson, S. McDonald, L.A. McGuinness, L.A. Stewart, J. Thomas, A.C. Tricco, V.A. Welch, P. Whiting and D. Moher, The PRISMA 2020 statement: an updated guideline for reporting systematic reviews, Systematic Reviews 10(1) (2021), 89. doi:10.1186/s13643-021-01626-4.</p>
<p>M Kang, N J Jameson, Machine Learning: Fundamentals, Prognostics and Health Management of Electronics: Fundamentals, Machine Learning, and the Internet of Things. M. Kang and N.J. Jameson, Machine Learning: Fundamentals, Prognostics and Health Management of Electronics: Fundamentals, Machine Learning, and the Internet of Things (2018), 85-109.</p>
<p>Machine learning algorithms. G Bonaccorso, Packt Publishing LtdG. Bonaccorso, Machine learning algorithms, Packt Publishing Ltd, 2017.</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningY. Bengio, J. Louradour, R. Collobert and J. Weston, Curriculum learning, in: Proceedings of the 26th annual international conference on machine learning, 2009, pp. 41-48.</p>
<p>Graph-based Argument Quality Assessment. E Saveleva, V Petukhova, M Mosbach, D Klakow, Proceedings of the International Conference on Recent Advances in Natural Language Processing. the International Conference on Recent Advances in Natural Language ProcessingRANLP 2021E. Saveleva, V. Petukhova, M. Mosbach and D. Klakow, Graph-based Argument Quality Assessment, in: Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), INCOMA Ltd., Held Online, 2021, pp. 1268-1280.</p>
<p>NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. Q Zhang, L Wang, S Yu, S Wang, Y Wang, J Jiang, E.-P Lim, 10.18653/v1/2021.findings-emnlp.350Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsQ. Zhang, L. Wang, S. Yu, S. Wang, Y. Wang, J. Jiang and E.-P. Lim, NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset, in: Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics, Punta Cana, Dominican Republic, 2021, pp. 4147-4161. doi:10.18653/v1/2021.findings-emnlp.350.</p>
<p>Question Directed Graph Attention Network for Numerical Reasoning over Text. K Chen, W Xu, X Cheng, Z Xiaochuan, Y Zhang, L Song, T Wang, Y Qi, W Chu, 10.18653/v1/2020.emnlp-main.549Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsK. Chen, W. Xu, X. Cheng, Z. Xiaochuan, Y. Zhang, L. Song, T. Wang, Y. Qi and W. Chu, Question Directed Graph Attention Network for Numerical Reasoning over Text, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Online, 2020, pp. 6759-6768. doi:10.18653/v1/2020.emnlp-main.549.</p>
<p>Local ABox consistency prediction with transparent TBoxes using gated graph neural networks. Y Gu, J Z Pan, G Cheng, H Paulheim, G Stoilos, Proc. 14th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy). 14th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy)Y. Gu, J.Z. Pan, G. Cheng, H. Paulheim and G. Stoilos, Local ABox consistency prediction with transparent TBoxes using gated graph neural networks, in: Proc. 14th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy), 2019.</p>
<p>H Lemos, P Avelar, M Prates, A Garcez, L Lamb, 10.1007/978-3-030-61609-0_51Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases. LNCSH. Lemos, P. Avelar, M. Prates, A. Garcez and L. Lamb, Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases, Lecture Notes in Computer Science 12396 LNCS (2020), 647-659. doi:10.1007/978- 3-030-61609-0_51.</p>
<p>Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text. M Zhou, D Ji, F Li, 10.1109/TASLP.2021.3082295IEEE/ACM Transactions on Audio Speech and Language Processing. 29M. Zhou, D. Ji and F. Li, Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text, IEEE/ACM Transactions on Audio Speech and Language Processing 29 (2021), 2015-2026. doi:10.1109/TASLP.2021.3082295.</p>
<p>Graph Enhanced Cross-Domain Text-to-SQL Generation. S Huo, T Ma, J Chen, M Chang, L Wu, M Witbrock, 10.18653/v1/D19-5319Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing. the Thirteenth Workshop on Graph-Based Methods for Natural Language ProcessingHong KongAssociation for Computational LinguisticsS. Huo, T. Ma, J. Chen, M. Chang, L. Wu and M. Witbrock, Graph Enhanced Cross-Domain Text-to-SQL Generation, in: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs@EMNLP 2019, Hong Kong, November 4, 2019, Association for Computational Linguistics, 2019, pp. 159-163. doi:10.18653/v1/D19-5319.</p>
<p>Complementing logical reasoning with sub-symbolic commonsense. F Bianchi, M Palmonari, P Hitzler, L Serafini, 10.1007/978-3-030-31095-0_11Lecture Notes in Computer Science. LNCSF. Bianchi, M. Palmonari, P. Hitzler and L. Serafini, Complementing logical reasoning with sub-symbolic commonsense, Lecture Notes in Computer Science 11784 LNCS (2019), 161-170. doi:10.1007/978-3-030-31095-0_11.</p>
<p>L Serafini, A D Garcez, arXiv:1606.04422Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge. L. Serafini and A.d. Garcez, Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge, arXiv:1606.04422 [cs] (2016).</p>
<p>Medical knowledge embedding based on recursive neural network for multi-disease diagnosis. J Jiang, H Wang, J Xie, X Guo, Y Guan, Q Yu, 10.1016/j.artmed.2019.101772Artificial Intelligence in Medicine. 103J. Jiang, H. Wang, J. Xie, X. Guo, Y. Guan and Q. Yu, Medical knowledge embedding based on recursive neural network for multi-disease diagnosis, Artificial Intelligence in Medicine 103 (2020). doi:10.1016/j.artmed.2019.101772.</p>
<p>Adversarial Examples for Evaluating Reading Comprehension Systems. R Jia, P Liang, 10.18653/v1/D17-1215Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsR. Jia and P. Liang, Adversarial Examples for Evaluating Reading Comprehension Systems, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Copenhagen, Denmark, 2017, pp. 2021- 2031. doi:10.18653/v1/D17-1215.</p>
<p>Annotation Artifacts in Natural Language Inference Data. S Gururangan, S Swayamdipta, O Levy, R Schwartz, S Bowman, N A Smith, 10.18653/v1/N18-2017Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics2Short PapersS. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. Bowman and N.A. Smith, Annotation Artifacts in Natural Language Inference Data, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), Association for Computational Linguistics, New Orleans, Louisiana, 2018, pp. 107-112. doi:10.18653/v1/N18-2017.</p>
<p>autoBOT: evolving neuro-symbolic representations for explainable low resource text classification. B krlj, M Martinc, N Lavra, S Pollak, 10.1007/s10994-021-05968-xMachine Learning. 1105B. krlj, M. Martinc, N. Lavra and S. Pollak, autoBOT: evolving neuro-symbolic representations for explainable low resource text classification, Machine Learning 110(5) (2021), 989-1028. doi:10.1007/s10994-021-05968-x.</p>
<p>R Miikkulainen, Encyclopedia of Machine Learning. New YorkSpringerNeuroevolutionR. Miikkulainen, Neuroevolution, in: Encyclopedia of Machine Learning, Springer, New York, 2010.</p>
<p>. J Lehman, R Miikkulainen, 10.4249/scholarpedia.30977Neuroevolution, Scholarpedia. 8630977J. Lehman and R. Miikkulainen, Neuroevolution, Scholarpedia 8(6) (2013), 30977. doi:10.4249/scholarpedia.30977.</p>
<p>What is a knowledge representation?. R Davis, H Shrobe, P Szolovits, AI magazine. 14R. Davis, H. Shrobe and P. Szolovits, What is a knowledge representation?, AI magazine 14(1) (1993), 17-17.</p>
<p>Knowledge representation: An approach to artificial intelligence. T J Bench-Capon, Elsevier32T.J. Bench-Capon, Knowledge representation: An approach to artificial intelligence, Vol. 32, Elsevier, 2014.</p>
<p>Knowledge representation and reasoning. H J Levesque, Annual review of computer science. 11H.J. Levesque, Knowledge representation and reasoning, Annual review of computer science 1(1) (1986), 255-287.</p>
<p>Knowledge representation and reasoning. R Brachman, H Levesque, ElsevierR. Brachman and H. Levesque, Knowledge representation and reasoning, Elsevier, 2004.</p>
<p>Knowledge Representation in Artificial Intelligence. I L Travis, Clinic on Library Applications of Data Processing. 27th: 1990I.L. Travis, Knowledge Representation in Artificial Intelligence, Clinic on Library Applications of Data Processing (27th: 1990) (1990).</p>
<p>Principles of Semantic Networks. J F Sowa, 10.1016/C2013-0-08297-7Morgan KaufmannJ.F. Sowa, Principles of Semantic Networks, Morgan Kaufmann, 1991. ISBN 978-1-4832-0771-1. doi:10.1016/C2013-0-08297-7.</p>
<p>C R Dyer, CS 540 Lecture Notes: Logic. University of Wisconsin -MadisonC.R. Dyer, CS 540 Lecture Notes: Logic, University of Wisconsin -Madison. https://pages.cs.wisc.edu/~dyer/cs540/notes/logic.html.</p>
<p>Detecting Direct Speech in Multilingual Collection of 19th-century Novels. J Byszuk, M Woniak, M Kestemont, A Leniak, W Lukasik, A el, M Eder, 979-10-95546-53-5Proceedings of LT4HALA 2020 -1st Workshop on Language Technologies for Historical and Ancient Languages. LT4HALA 2020 -1st Workshop on Language Technologies for Historical and Ancient LanguagesMarseille, France, 2020J. Byszuk, M. Woniak, M. Kestemont, A. Leniak, W. Lukasik, A. el , a and M. Eder, Detecting Direct Speech in Multilingual Collection of 19th-century Novels, in: Proceedings of LT4HALA 2020 -1st Workshop on Language Technologies for Historical and Ancient Languages, European Language Resources Association (ELRA), Marseille, France, 2020, pp. 100-104. ISBN 979-10-95546-53-5.</p>
<p>Linguistics and natural logic. G Lakoff, 10.1007/BF00413602Synthese. 221G. Lakoff, Linguistics and natural logic, Synthese 22(1) (1970), 151-271. doi:10.1007/BF00413602.</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and ParaphrasingB. MacCartney and C.D. Manning, Natural logic for textual inference, in: Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, 2007, pp. 193-200.</p>
<p>NaturalLI: Natural Logic Inference for Common Sense Reasoning. G Angeli, C D Manning, 10.3115/v1/D14-1059Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsG. Angeli and C.D. Manning, NaturalLI: Natural Logic Inference for Common Sense Reasoning, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Doha, Qatar, 2014, pp. 534-545. doi:10.3115/v1/D14-1059.</p>
<p>C Manning, M Surdeanu, J Bauer, J Finkel, S Bethard, D Mcclosky, 10.3115/v1/P14-5010Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 52nd Annual Meeting of the Association for Computational Linguistics: System DemonstrationsBaltimore, MarylandAssociation for Computational LinguisticsThe Stanford CoreNLP Natural Language Processing ToolkitC. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard and D. McClosky, The Stanford CoreNLP Natural Language Processing Toolkit, in: Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Association for Computational Linguistics, Baltimore, Maryland, 2014, pp. 55-60. doi:10.3115/v1/P14-5010. https://aclanthology.org/P14-5010.</p>
<p>Review of Selected Works. S Mccall, Synthese. 261S. McCall, Review of Selected Works, Synthese 26(1) (1973), 165-171.</p>
<p>Learning ukasiewicz logic. F Harder, T R Besold, 10.1016/j.cogsys.2017.07.004Cognitive Systems Research. 47F. Harder and T.R. Besold, Learning ukasiewicz logic, Cognitive Systems Research 47 (2018), 42-67. doi:10.1016/j.cogsys.2017.07.004.</p>
<p>Non-monotonic Logic. C Strasser, G A Antonelli, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford UniversitySummer 2019 ednC. Strasser and G.A. Antonelli, Non-monotonic Logic, in: The Stanford Encyclopedia of Philosophy, Summer 2019 edn, Metaphysics Research Lab, Stanford University, 2019.</p>
<p>J P Mccrae, E Rudnicka, F Bond, English Wordnet, A new open-source wordnet for English. J.P. McCrae, E. Rudnicka and F. Bond, English WordNet: A new open-source wordnet for English, 2021. https://lexicala.com/review/2020/mccrae-rudnicka-bond-english-wordnet/.</p>
<p>Team SVMrank: Leveraging Feature-rich Support Vector Machines for Ranking Explanations to Elementary Science Questions. J Souza, I O Mulang, &apos; , S Auer, 10.18653/v1/D19-5312Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing. the Thirteenth Workshop on Graph-Based Methods for Natural Language ProcessingHong KongAssociation for Computational LinguisticsJ. D'Souza, I.O. Mulang' and S. Auer, Team SVMrank: Leveraging Feature-rich Support Vector Machines for Ranking Explanations to Elementary Science Questions, in: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs@EMNLP 2019, Hong Kong, November 4, 2019, Association for Computational Linguistics, 2019, pp. 90-100. doi:10.18653/v1/D19-5312.</p>
<p>Semi-supervised learning for big social data analysis. A Hussain, E Cambria, 10.1016/j.neucom.2017.10.010Neurocomputing. 275A. Hussain and E. Cambria, Semi-supervised learning for big social data analysis, Neurocomputing 275 (2018), 1662-1673. doi:10.1016/j.neucom.2017.10.010.</p>
<p>Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information. Q Cui, Y Zhou, M Zheng, 10.1007/978-3-030-82147-0_34Lecture Notes in Computer Science 12816 LNAI. Q. Cui, Y. Zhou and M. Zheng, Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information, Lecture Notes in Computer Science 12816 LNAI (2021), 419-426. doi:10.1007/978-3-030-82147-0_34.</p>
<p>Relation Embedding with Dihedral Group in Knowledge Graph. C Xu, R Li, 10.18653/v1/P19-1026Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsC. Xu and R. Li, Relation Embedding with Dihedral Group in Knowledge Graph, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Florence, Italy, 2019, pp. 263-272. doi:10.18653/v1/P19-1026.</p>
<p>A I Cowen-Rivers, P Minervini, T Rocktaschel, M Bosnjak, S Riedel, J Wang, Neural Variational Inference For Estimating Uncertainty in Knowledge Graph Embeddings. A.I. Cowen-Rivers, P. Minervini, T. Rocktaschel, M. Bosnjak, S. Riedel and J. Wang, Neural Variational Inference For Estimating Uncertainty in Knowledge Graph Embeddings (2019).</p>
<p>A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case. M Bounabi, K Elmoutaouakil, K Satori, 10.1108/IJWIS-11-2020-0067International Journal of Web Information Systems. 173M. Bounabi, K. Elmoutaouakil and K. Satori, A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case, International Journal of Web Information Systems 17(3) (2021), 229-249. doi:10.1108/IJWIS-11-2020-0067.</p>
<p>F Es-Sabery, A Hair, J Qadir, B Sainz-De-Abajo, B Garcia-Zapirain, I Torre-Diez, 10.1109/ACCESS.2021.3053917Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier. 9F. Es-Sabery, A. Hair, J. Qadir, B. Sainz-De-Abajo, B. Garcia-Zapirain and I. Torre-DIez, Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier, IEEE Access 9 (2021), 17943-17985. doi:10.1109/ACCESS.2021.3053917.</p>
<p>The Impact of Semantic Linguistic Features in Relation Extraction: A Logical Relational Learning Approach. R Lima, B Espinasse, F Freitas, 10.26615/978-954-452-056-4_076Proceedings of the International Conference on Recent Advances in Natural Language Processing. the International Conference on Recent Advances in Natural Language ProcessingVarna, BulgariaRANLP 2019R. Lima, B. Espinasse and F. Freitas, The Impact of Semantic Linguistic Features in Relation Extraction: A Logical Relational Learning Approach, in: Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), INCOMA Ltd., Varna, Bulgaria, 2019, pp. 648-654. doi:10.26615/978-954-452-056-4_076.</p>
<p>Web Question Answering with Neurosymbolic Program Synthesis. Q Chen, A Lamoreaux, X Wang, G Durrett, O Bastani, I Dillig, 10.1145/3453483.3454047Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation. the 42nd ACM SIGPLAN International Conference on Programming Language Design and ImplementationNew York, NY, USAAssociation for Computing MachineryQ. Chen, A. Lamoreaux, X. Wang, G. Durrett, O. Bastani and I. Dillig, Web Question Answering with Neurosymbolic Program Synthesis, in: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, Association for Computing Machinery, New York, NY, USA, 2021, pp. 328-343-. ISBN 9781450383912. https://doi.org/10.1145/3453483.3454047 .</p>
<p>Learning to activate logic rules for textual reasoning. Y Yao, J Xu, J Shi, B Xu, 10.1016/j.neunet.2018.06.012Neural Networks. 106Y. Yao, J. Xu, J. Shi and B. Xu, Learning to activate logic rules for textual reasoning, Neural Networks 106 (2018), 42-49. doi:10.1016/j.neunet.2018.06.012.</p>
<p>Hybrid Deep Neural Networks to Predict Socio-Moral Reasoning Skills. A A N Tato, R Nkambou, A Dufresne, Proceedings of the 12th International Conference on Educational Data Mining, EDM 2019. the 12th International Conference on Educational Data Mining, EDM 2019Montral, CanadaInternational Educational Data Mining Society (IEDMS)A.A.N. Tato, R. Nkambou and A. Dufresne, Hybrid Deep Neural Networks to Predict Socio-Moral Reasoning Skills, in: Proceedings of the 12th International Conference on Educational Data Mining, EDM 2019, Montral, Canada, July 2-5, 2019, International Educational Data Mining Society (IEDMS), 2019. https://drive.google.com/file/d/1aCXyukLqVeuShQSGATRzEeDAk_Al7bVz.</p>
<p>Fuzzy commonsense reasoning for multimodal sentiment analysis. I Chaturvedi, R Satapathy, S Cavallari, E Cambria, 10.1016/j.patrec.2019.04.024Pattern Recognition Letters. 125I. Chaturvedi, R. Satapathy, S. Cavallari and E. Cambria, Fuzzy commonsense reasoning for multimodal sentiment analysis, Pattern Recognition Letters 125 (2019), 264-270. doi:10.1016/j.patrec.2019.04.024.</p>
<p>Causal relation classification using convolutional neural networks and grammar tags. R Ayyanar, G Koomullil, H Ramasangu, 10.1109/INDICON47234.2019.9028985R. Ayyanar, G. Koomullil and H. Ramasangu, Causal relation classification using convolutional neural networks and grammar tags, 2019. doi:10.1109/INDICON47234.2019.9028985.</p>
<p>Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification. J Gong, H Ma, Z Teng, Q Teng, H Zhang, L Du, S Chen, M Z A Bhuiyan, J Li, M Liu, 10.1109/ACCESS.2020.2972751IEEE Access. 8J. Gong, H. Ma, Z. Teng, Q. Teng, H. Zhang, L. Du, S. Chen, M.Z.A. Bhuiyan, J. Li and M. Liu, Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification, IEEE Access 8 (2020), 30885-30896. doi:10.1109/ACCESS.2020.2972751.</p>
<p>Semantic Fake News Detection: A Machine Learning Perspective. A M P Braoveanu, R Andonie, 10.1007/978-3-030-20521-8_54Lecture Notes in Computer Science. LNCSA.M.P. Braoveanu and R. Andonie, Semantic Fake News Detection: A Machine Learning Perspective, Lecture Notes in Computer Science 11506 LNCS (2019), 656-667. doi:10.1007/978-3-030-20521-8_54.</p>
<p>DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations. D Hu, L Wei, X Huai, 10.18653/v1/2021.acl-long.547Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)D. Hu, L. Wei and X. Huai, DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, Online, 2021, pp. 7042-7052. doi:10.18653/v1/2021.acl-long.547.</p>
<p>Mapping Natural-Language Problems to Formal-Language Solutions Using Structured Neural Representations. K Chen, Q Huang, H Palangi, P Smolensky, K D Forbus, J Gao, Proceedings of the 37th International Conference on Machine Learning, JMLR.org. the 37th International Conference on Machine Learning, JMLR.orgK. Chen, Q. Huang, H. Palangi, P. Smolensky, K.D. Forbus and J. Gao, Mapping Natural-Language Problems to Formal-Language Solutions Using Structured Neural Representations, in: Proceedings of the 37th International Conference on Machine Learning, JMLR.org, 2020.</p>
<p>Jointly Learning to Detect Emotions and Predict Facebook Reactions. L Graziani, S Melacci, M Gori, 10.1007/978-3-030-30490-4_16Lecture Notes in Computer Science. LNCSL. Graziani, S. Melacci and M. Gori, Jointly Learning to Detect Emotions and Predict Facebook Reactions, Lecture Notes in Computer Science 11730 LNCS (2019), 185-197. doi:10.1007/978-3-030-30490-4_16.</p>
<p>K Gupta, T Ghosal, A , Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation, Association for Computational Lingustics. the 35th Pacific Asia Conference on Language, Information and Computation, Association for Computational LingusticsShanghai, ChinaA Neuro-Symbolic Approach for Question Answering on Research ArticlesK. Gupta, T. Ghosal and A. Ekbal, A Neuro-Symbolic Approach for Question Answering on Research Articles, in: Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation, Association for Computational Lingustics, Shanghai, China, 2021, pp. 40-49.</p>
<p>Applied Medical Code Mapping with Character-based Deep Learning Models and Word-based Logic. J Langton, K Srihasam, Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA). the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)Groningen, the NetherlandsAssociation for Computational LinguisticsJ. Langton and K. Srihasam, Applied Medical Code Mapping with Character-based Deep Learning Models and Word-based Logic, in: Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA), Association for Computational Linguistics, Groningen, the Netherlands (online), 2021, pp. 7-11.</p>
<p>A Novel NLP-FUZZY System Prototype for Information Extraction from Medical Guidelines. L B Fazlic, A Hallawa, A Schmeink, A Peine, L Martin, G Dartmann, 10.23919/MIPRO.2019.87569292019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO). L.B. Fazlic, A. Hallawa, A. Schmeink, A. Peine, L. Martin and G. Dartmann, A Novel NLP-FUZZY System Prototype for Information Extraction from Medical Guidelines, in: 2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), 2019, pp. 1025-1030. doi:10.23919/MIPRO.2019.8756929.</p>
<p>C Schon, S Siebert, F Stolzenburg, 10.1007/s13218-019-00601-5The CoRg Project: Cognitive Reasoning. 33C. Schon, S. Siebert and F. Stolzenburg, The CoRg Project: Cognitive Reasoning, KI -Kunstliche Intelligenz 33(3) (2019), 293-299. doi:10.1007/s13218-019-00601-5.</p>
<p>Modeling Content and Context with Deep Relational Learning. M L Pacheco, D Goldwasser, 10.1162/tacl_a_00357Transactions of the Association for Computational Linguistics. 9M.L. Pacheco and D. Goldwasser, Modeling Content and Context with Deep Relational Learning, Transactions of the Association for Computational Linguistics 9 (2021), 100-119. doi:10.1162/tacl_a_00357.</p>
<p>Cases without Borders: Automating Knowledge Acquisition Approach using Deep Autoencoders and Siamese Networks in Case-Based Reasoning. K Amin, 10.1109/ICTAI.2019.000272019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI). K. Amin, Cases without Borders: Automating Knowledge Acquisition Approach using Deep Autoencoders and Siamese Networks in Case-Based Reasoning, in: 2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI), 2019, pp. 133-140. doi:10.1109/ICTAI.2019.00027.</p>
<p>Zero-shot Multi-Domain Dialog State Tracking Using Prescriptive Rules. E Altszyler, P Brusco, N Basiou, J Byrnes, D Vergyri, Proceedings of the 15th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 1st International Joint Conference on Learning &amp; Reasoning (IJCLR 2021), Virtual conference. the 15th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 1st International Joint Conference on Learning &amp; Reasoning (IJCLR 2021), Virtual conference2986CEUR-WS.orgCEUR Workshop ProceedingsE. Altszyler, P. Brusco, N. Basiou, J. Byrnes and D. Vergyri, Zero-shot Multi-Domain Dialog State Tracking Using Prescriptive Rules, in: Proceedings of the 15th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 1st International Joint Conference on Learning &amp; Reasoning (IJCLR 2021), Virtual conference, October 25-27, 2021, CEUR Workshop Proceedings, Vol. 2986, CEUR-WS.org, 2021, pp. 57-66.</p>
<p>A Sutherland, S Magg, S Wermter, 10.1109/IJCNN.2019.8851875Leveraging Recursive Processing for Neural-Symbolic Affect-Target Associations. 2019 International Joint Conference on Neural Networks (IJCNN)A. Sutherland, S. Magg and S. Wermter, Leveraging Recursive Processing for Neural-Symbolic Affect-Target Associations, in: 2019 International Joint Conference on Neural Networks (IJCNN), 2019, pp. 1-6. doi:10.1109/IJCNN.2019.8851875.</p>
<p>D Demeter, D Downey, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceD. Demeter and D. Downey, Just Add Functions: A Neural-Symbolic Language Model, in: The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, AAAI Press, 2020, pp. 7634-7642.</p>
<p>Temporal Reasoning on Implicit Events from Distant Supervision. B Zhou, K Richardson, Q Ning, T Khot, A Sabharwal, D Roth, 10.18653/v1/2021.naacl-main.107Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsB. Zhou, K. Richardson, Q. Ning, T. Khot, A. Sabharwal and D. Roth, Temporal Reasoning on Implicit Events from Distant Supervision, in: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Online, 2021, pp. 1361-1371. doi:10.18653/v1/2021.naacl- main.107.</p>
<p>Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks. J Qin, X Liang, Y Hong, J Tang, L Lin, 10.18653/v1/2021.acl-long.456Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)J. Qin, X. Liang, Y. Hong, J. Tang and L. Lin, Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, Online, 2021, pp. 5870-5881. doi:10.18653/v1/2021.acl-long.456.</p>
<p>Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification. P Sen, M Danilevsky, Y Li, S Brahma, M Boehm, L Chiticariu, R Krishnamurthy, 10.18653/v1/2020.emnlp-main.345Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsP. Sen, M. Danilevsky, Y. Li, S. Brahma, M. Boehm, L. Chiticariu and R. Krishnamurthy, Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Online, 2020, pp. 4211-4221. doi:10.18653/v1/2020.emnlp-main.345.</p>
<p>J Mao, C Gan, P Kohli, J B Tenenbaum, J Wu, The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. New Orleans, LA, USA7th International Conference on Learning Representations, ICLR 2019. OpenReview.netJ. Mao, C. Gan, P. Kohli, J.B. Tenenbaum and J. Wu, The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision, in: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019.</p>
<p>Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization. P Kouris, G Alexandridis, A Stafylopatis, 10.1162/coli_a_00417Computational Linguistics. 474P. Kouris, G. Alexandridis and A. Stafylopatis, Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization, Computational Linguistics 47(4) (2021), 813-859. doi:10.1162/coli_a_00417.</p>
<p>Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems. C S Pinhanez, P R Cavalin, V H A Ribeiro, A P Appel, H Candello, J Nogima, M Pichiliani, M A Guerra, M Bayser, G L Malfatti, H Ferreira, 10.18653/v1/2021.acl-long.545Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. C. Zong, F. Xia, W. Li and R. Naviglithe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics1Virtual EventC.S. Pinhanez, P.R. Cavalin, V.H.A. Ribeiro, A.P. Appel, H. Candello, J. Nogima, M. Pichiliani, M.A. Guerra, M. de Bayser, G.L. Malfatti and H. Ferreira, Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, C. Zong, F. Xia, W. Li and R. Navigli, eds, Association for Computational Linguistics, 2021, pp. 7014-7027. doi:10.18653/v1/2021.acl-long.545.</p>
<p>Heterogeneous graph reasoning for knowledge-grounded medical dialogue system. W Liu, J Tang, X Liang, Q Cai, 10.1016/j.neucom.2021.02.021Neurocomputing. 442W. Liu, J. Tang, X. Liang and Q. Cai, Heterogeneous graph reasoning for knowledge-grounded medical dialogue system, Neurocomputing 442 (2021), 260-268. doi:10.1016/j.neucom.2021.02.021.</p>
<p>Automated Ontology-Based Annotation of Scientific Literature Using Deep Learning. P Manda, S Sayedahmed, S D Mohanty, 10.1145/3391274.3393636Proceedings of The International Workshop on Semantic Big Data, SBD '20. The International Workshop on Semantic Big Data, SBD '20New York, NY, USAAssociation for Computing Machinery2020P. Manda, S. SayedAhmed and S.D. Mohanty, Automated Ontology-Based Annotation of Scientific Literature Using Deep Learning, in: Proceedings of The International Workshop on Semantic Big Data, SBD '20, Association for Computing Machinery, New York, NY, USA, 2020. ISBN 9781450379748. doi:10.1145/3391274.3393636.</p>
<p>Attentive Tensor Product Learning. Q Huang, L Deng, D Wu, C Liu, X He, 10.1609/aaai.v33i01.33011344Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Q. Huang, L. Deng, D. Wu, C. Liu and X. He, Attentive Tensor Product Learning, Proceedings of the AAAI Conference on Artificial Intelligence 33(01) (2019), 1344-1351. doi:10.1609/aaai.v33i01.33011344.</p>
<p>Z Chen, Q Gao, L S Moss, 10.18653/v1/2021.starsem-1.7Proceedings of <em>SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics. </em>SEM 2021: The Tenth Joint Conference on Lexical and Computational SemanticsAssociation for Computational LinguisticsNeuralLog: Natural Language Inference with Joint Neural and Logical ReasoningZ. Chen, Q. Gao and L.S. Moss, NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning, in: Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, Association for Computational Linguistics, Online, 2021, pp. 78-88. doi:10.18653/v1/2021.starsem-1.7.</p>
<p>K Kogkalidis, M Moortgat, R Moot, 10.18653/v1/2020.conll-1.3Proceedings of the 24th Conference on Computational Natural Language Learning. the 24th Conference on Computational Natural Language LearningAssociation for Computational LinguisticsNeural Proof NetsK. Kogkalidis, M. Moortgat and R. Moot, Neural Proof Nets, in: Proceedings of the 24th Conference on Computational Natural Language Learning, Association for Computational Linguistics, Online, 2020, pp. 26-40. doi:10.18653/v1/2020.conll-1.3.</p>
<p>Deep Weighted MaxSAT for Aspect-based Opinion Extraction. M Wu, W Wang, S J Pan, 10.18653/v1/2020.emnlp-main.453Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsM. Wu, W. Wang and S.J. Pan, Deep Weighted MaxSAT for Aspect-based Opinion Extraction, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Online, 2020, pp. 5618-5628. doi:10.18653/v1/2020.emnlp-main.453.</p>
<p>Neural Natural Logic Inference for Interpretable Question Answering. J Shi, X Ding, L Du, T Liu, B Qin, 10.18653/v1/2021.emnlp-main.298Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaJ. Shi, X. Ding, L. Du, T. Liu and B. Qin, Neural Natural Logic Inference for Interpretable Question Answering, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021, pp. 3673-3684. doi:10.18653/v1/2021.emnlp-main.298.</p>
<p>Variational Deep Logic Network for Joint Inference of Entities and Relations. W Wang, S J Pan, 10.1162/coli_a_00415Computational Linguistics. 474W. Wang and S.J. Pan, Variational Deep Logic Network for Joint Inference of Entities and Relations, Computational Linguistics 47(4) (2021), 775-812. doi:10.1162/coli_a_00415.</p>
<p>Augmenting Neural Networks with First-order Logic. T Li, V Srikumar, 10.18653/v1/P19-1028Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsT. Li and V. Srikumar, Augmenting Neural Networks with First-order Logic, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Florence, Italy, 2019, pp. 292-302. doi:10.18653/v1/P19- 1028.</p>
<p>Question Answering Systems with Deep Learning-Based Symbolic Processing. H Honda, M Hagiwara, 10.1109/ACCESS.2019.2948081IEEE Access. 7H. Honda and M. Hagiwara, Question Answering Systems with Deep Learning-Based Symbolic Processing, IEEE Access 7 (2019), 152368-152378. doi:10.1109/ACCESS.2019.2948081.</p>
<p>ETHAN at SemEval-2020 Task 5: Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing. L Yabloko, 10.18653/v1/2020.semeval-1.83Proceedings of the Fourteenth Workshop on Semantic Evaluation, International Committee for Computational Linguistics. the Fourteenth Workshop on Semantic Evaluation, International Committee for Computational LinguisticsBarcelonaL. Yabloko, ETHAN at SemEval-2020 Task 5: Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing, in: Proceedings of the Fourteenth Workshop on Semantic Evaluation, International Committee for Computational Linguistics, Barcelona (online), 2020, pp. 645-652. doi:10.18653/v1/2020.semeval-1.83.</p>
<p>Case-based Reasoning for Natural Language Queries over Knowledge Bases. R Das, M Zaheer, D Thai, A Godbole, E Perez, J Y Lee, L Tan, L Polymenakos, A Mccallum, 10.18653/v1/2021.emnlp-main.755Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaR. Das, M. Zaheer, D. Thai, A. Godbole, E. Perez, J.Y. Lee, L. Tan, L. Polymenakos and A. McCallum, Case-based Reasoning for Natural Language Queries over Knowledge Bases, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021, pp. 9594-9611. doi:10.18653/v1/2021.emnlp-main.755.</p>
<p>H Jiang, S Gurajada, Q Lu, S Neelam, L Popa, P Sen, Y Li, A Gray, Lnn-El, 10.18653/v1/2021.acl-long.64Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)H. Jiang, S. Gurajada, Q. Lu, S. Neelam, L. Popa, P. Sen, Y. Li and A. Gray, LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, Online, 2021, pp. 775-787. doi:10.18653/v1/2021.acl-long.64.</p>
<p>BDCN: Semantic Embedding Self-explanatory Breast Diagnostic Capsules Network. C Dehua, Z Keting, H Jianrong, Proceedings of the 20th Chinese National Conference on Computational Linguistics. the 20th Chinese National Conference on Computational LinguisticsChinese Information Processing Society of ChinaC. Dehua, Z. Keting and H. Jianrong, BDCN: Semantic Embedding Self-explanatory Breast Diagnostic Capsules Network, in: Proceedings of the 20th Chinese National Conference on Computational Linguistics, Chinese Information Processing Society of China, Huhhot, China, 2021, pp. 1178-1189.</p>
<p>P Verga, H Sun, L Soares, W Cohen, 10.18653/v1/2021.naacl-main.288Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsAdaptable and Interpretable Neural MemoryOver Symbolic KnowledgeP. Verga, H. Sun, L. Baldini Soares and W. Cohen, Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge, in: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Online, 2021, pp. 3678-3691. doi:10.18653/v1/2021.naacl-main.288.</p>
<p>Neuro-Symbolic Approaches for Text-Based Policy Learning. S Chaudhury, P Sen, M Ono, D Kimura, M Tatsubori, A Munawar, 10.18653/v1/2021.emnlp-main.245Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaS. Chaudhury, P. Sen, M. Ono, D. Kimura, M. Tatsubori and A. Munawar, Neuro-Symbolic Approaches for Text-Based Policy Learning, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021, pp. 3073-3078. doi:10.18653/v1/2021.emnlp-main.245.</p>
<p>G Lample, F Charton, 10.48550/ARXIV.1912.01412arXiv:1912.01412Deep Learning for Symbolic Mathematics. arXiv preprintG. Lample and F. Charton, Deep Learning for Symbolic Mathematics, arXiv preprint arXiv:1912.01412 (2019). doi:10.48550/ARXIV.1912.01412.</p>
<p>Tensor product variable binding and the representation of symbolic structures in connectionist systems. P Smolensky, 10.1016/0004-3702(90)90007-MArtificial Intelligence. 461P. Smolensky, Tensor product variable binding and the representation of symbolic structures in connectionist systems, Artificial Intelligence 46(1) (1990), 159-216. doi:https://doi.org/10.1016/0004-3702(90)90007-M.</p>
<p>A S Gordon, J R Hobbs, 10.1017/9781316584705A Formal Theory of Commonsense Psychology: How People Think People Think. Cambridge University PressA.S. Gordon and J.R. Hobbs, A Formal Theory of Commonsense Psychology: How People Think People Think, Cambridge University Press, 2017. doi:10.1017/9781316584705.</p>
<p>An experimental study measuring human annotator categorization agreement on commonsense sentences. H Santos, M Kejriwal, A M Mulvehill, G Forbush, D L Mcguinness, 10.1017/exp.2021.9Experimental Results. 219H. Santos, M. Kejriwal, A.M. Mulvehill, G. Forbush and D.L. McGuinness, An experimental study measuring human annotator categorization agreement on commonsense sentences, Experimental Results 2 (2021), e19. doi:10.1017/exp.2021.9.</p>
<p>Discrete and continuous representations and processing in deep learning: Looking forward. R Cartuyvels, G Spinks, M.-F Moens, 10.1016/j.aiopen.2021.07.002AI Open. 2R. Cartuyvels, G. Spinks and M.-F. Moens, Discrete and continuous representations and processing in deep learning: Looking forward, AI Open 2 (2021), 143-159. doi:10.1016/j.aiopen.2021.07.002.</p>
<p>E Tsamoura, T Hospedales, L Michael, Neural-Symbolic Integration: A Compositional Perspective, Proceedings of the AAAI Conference on Artificial Intelligence. 35E. Tsamoura, T. Hospedales and L. Michael, Neural-Symbolic Integration: A Compositional Perspective, Proceedings of the AAAI Conference on Artificial Intelligence 35(66) (2021), 5051-5060.</p>
<p>G Boleda, 10.1146/annurev-linguistics-011619-030303Distributional Semantics and Linguistic Theory. 6G. Boleda, Distributional Semantics and Linguistic Theory, Annual Review of Linguistics 6(1) (2020), 213-234. doi:10.1146/annurev- linguistics-011619-030303.</p>
<p>X Chen, C Liang, A W Yu, D Song, D Zhou, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Compositional Generalization via Neural-Symbolic Stack MachinesX. Chen, C. Liang, A.W. Yu, D. Song and D. Zhou, Compositional Generalization via Neural-Symbolic Stack Machines, in: Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, 2020. ISBN 9781713829546.</p>
<p>V Embar, D Sridhar, G Farnadi, L Getoor, arXiv:1807.00973[cs,stat]Scalable Structure Learning for Probabilistic Soft Logic. V. Embar, D. Sridhar, G. Farnadi and L. Getoor, Scalable Structure Learning for Probabilistic Soft Logic, arXiv:1807.00973 [cs, stat] (2018).</p>
<p>Semantic-based regularization for learning and inference. M Diligenti, M Gori, C Sacc, 10.1016/j.artint.2015.08.011Artificial Intelligence. 244M. Diligenti, M. Gori and C. Sacc, Semantic-based regularization for learning and inference, Artificial Intelligence 244 (2017), 143-165. doi:10.1016/j.artint.2015.08.011.</p>
<p>R Manhaeve, S Dumancic, A Kimmig, T Demeester, L De Raedt, DeepProbLog: Neural Probabilistic Logic Programming. 31R. Manhaeve, S. Dumancic, A. Kimmig, T. Demeester and L. De Raedt, DeepProbLog: Neural Probabilistic Logic Programming, Advances in Neural Information Processing Systems 31 (2018).</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4719 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4719</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4719</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-97994e4526ef7eeea59190aa466fbab05fad9187</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/97994e4526ef7eeea59190aa466fbab05fad9187" target="_blank">How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view, and demonstrates that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning.</p>
                <p><strong>Paper Abstract:</strong> Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of Llama-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context prior taking over in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token appear in the later half, attention heads that move information along ontological relationships appear in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4719.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4719.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer arithmetic depth result</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer circuit complexity limits for arithmetic (Feng et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical result that standard (constant-depth) Transformers cannot solve certain arithmetic problems (direct-answer form) unless model depth grows super-polynomially with input size, and that Chain-of-Thought (CoT) style intermediate output can overcome this barrier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards revealing the mystery behind chain of thought: a theoretical perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (theoretical model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Abstract Transformer models as studied in circuit-complexity theoretic analysis; not an empirical single pretrained LM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>General arithmetic / algorithmic numeric problems (theoretical, direct-answer tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Direct-answer constant-depth Transformers lack sufficient computational depth to implement the required arithmetic algorithm; generating intermediate computation (CoT / scratchpad) changes the effective computation allowing constant-depth models to simulate more complex algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>The paper cites Feng et al. (2023), which provides circuit-complexity style proofs showing impossibility results for direct-answer constant-depth Transformers and constructive arguments that allowing intermediate outputs (CoT) enables overcoming the depth bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>This paper does not provide empirical counterexamples; the claim is theoretical and depends on formal assumptions about input encoding and the class of arithmetic problems considered.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Not applicable in this paper; the claim is cited as a theoretical result rather than an empirical probing/intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Result applies under formal model/complexity assumptions; real pretrained LMs may use shortcuts, memorization, or other mechanisms not captured by the theoretical model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Framed as a general limitation of constant-depth Transformers vs. Transformer variants that can emit intermediate computation (CoT); no empirical model-to-model numeric comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4719.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4719.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpads / intermediate computation (Nye et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The idea of letting autoregressive Transformers produce and condition on intermediate computation steps (scratchpads) which empirically improves solving complex tasks including numeric reasoning by bypassing depth/attention bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show your work: Scratchpads for intermediate computation with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive Transformer models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer architectures that are trained or prompted to output intermediate computation tokens (scratchpad/CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step numeric reasoning / arithmetic where intermediate steps help decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>By explicitly emitting intermediate steps, the model offloads multi-step computation into the token stream, effectively increasing available computation and enabling solutions that would otherwise require greater model depth.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited empirical work (Nye et al.) demonstrated improved generalization and performance when models generate scratchpads; the present paper references this as motivation for CoT-style methods that enable arithmetic/complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Related work notes brittle shortcut solutions can exist and benefits depend on training and prompting; this paper does not present direct counter-evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Not performed here for arithmetic specifically; paper uses activation patching and probing for ontology CoT tasks, not numeric scratchpad experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Scratchpads require the model to produce useful intermediate content; explanations may correlate with rather than causally produce better answers in some settings (see Lampinen et al. 2022 discussion cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared conceptually to direct-answer Transformers; no numeric comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4719.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4719.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (Wei et al., 2022b and related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step natural-language intermediate reasoning from LLMs, which empirically improves performance on multi-step reasoning tasks, including arithmetic-style word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Models (e.g., Llama-2 7B in this paper; GPT-family in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLMs; in this paper, analyses are performed on Llama-2 7B in a few-shot CoT prompting regime.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step reasoning and word-problem style arithmetic (cited in related work; this paper focuses on fictional ontology reasoning rather than numeric arithmetic experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>CoT works by making intermediate computation explicit in the token stream, enabling model circuits (e.g., induction heads, attention pathways) to operate sequentially over subtasks; CoT may change which pathways are used and allow constant-depth networks to implement multi-step algorithms effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>This paper provides mechanistic evidence (activation patching, probing, logit-lens) showing that Llama-2 7B uses multiple parallel pathways to collect and write answers and that some heads rely on generated context (CoT) while others pull directly from question/few-shot context — supporting partial causal use of CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Related observations (Lampinen et al., 2022) show explanation-generation can improve accuracy even when not causally used; Tan (2023) shows partial causal dependence. This paper shows CoT usage varies by subtask — sometimes answers come directly from question context rather than generated CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This paper reports CoT mechanistic analyses but does not present numeric arithmetic task metrics; Llama-2 7B retained >90% relative accuracy on the fictional ontology CoT subtasks when pruned to ~400 heads (not arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Activation patching, probing classifiers, logit-lens and targeted head knockouts reveal: (a) multiple answer-writing heads appear after a mid-layer 'rift'; (b) some answer pathways read from generated CoT, some from question/few-shot context; (c) knocking out many heads leaves >90% performance on the ontology tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>CoT usage is non-uniform — some subtasks do not rely on generated CoT; parallel backup circuits and hydra-effect (self-repair) make causal intervention difficult; this paper studies fictional ontologies, not arithmetic directly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Paper references that CoT efficacy depends on model scale (Wei et al., 2022a; Saparov & He, 2023) but does not present empirical cross-model arithmetic comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4719.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4719.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-of-Thought / program generation for numerical reasoning (Chen et al., 2023; Bi et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that prompt LMs to generate executable program-like intermediate representations (instead of natural-language CoT) to disentangle computation from reasoning for numerical problems, often improving numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LMs using program-generation prompts (general references to transformers/LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer LLMs prompted to output program-like structures (e.g., pseudo-code or actual code) as intermediate steps for arithmetic/numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numerical reasoning, arithmetic word problems, and program-style multi-step computation</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Emitting program-like intermediate steps separates algorithmic computation (which can be executed or more structured) from linguistic reasoning, enabling more reliable numeric computation than free-form natural-language CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited work (Chen et al., 2023; Bi et al., 2024) empirically shows program-like CoT can improve numerical reasoning. This paper cites those works as part of related literature but does not empirically test program-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>This paper does not present counter-evidence; it notes that program generation tends to favor certain complexities and that reasoning approaches may be brittle or dependent on model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No program-of-thought probing performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Program outputs still depend on model capabilities and may be brittle; not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Referenced relative to free-form CoT and direct-answer approaches; no direct comparisons reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4719.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4719.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Induction heads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Induction heads / pattern-copying attention heads (Olsson et al., 2022; Elhage et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific attention-head composition (often a query and key/value pairing across layers) that enables pattern copying from earlier context tokens to later ones and is implicated in in-context learning and pattern matching behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In-context learning and induction heads.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based LLMs (toy models and Llama-2 7B analyzed here)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Attention-only (or attention-dominant) Transformer architectures where particular heads implement copy/induction-like behavior via attention patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Induction and pattern-based reasoning; may underlie multi-step reasoning that includes arithmetic when arithmetic can be cast as pattern copying or algorithmic induction.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Induction heads perform token-to-token information copying and pattern matching across the context; compositions of such heads can implement in-context algorithms (including the subtask operations needed for CoT) and thus support arithmetic when arithmetic is decomposed into token-level steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>This paper finds that heads identified via inductive-reasoning scoring perform consistently across CoT subtasks in the fictional ontology tasks, require fewer heads, and appear to implement information mixing among ontologically related tokens; activation patching and probing show these heads move information between residual streams.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Induction heads alone do not fully separate decision-making, copying, and induction roles — many heads are shared across those subtasks; arithmetic not directly tested here so applicability to numeric tasks is inferred but not proven.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In ontology CoT tasks, pruning to inductive-head-based subsets retained >90% accuracy with ~40% of heads active; no numeric arithmetic metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Activation patching, probing classifiers, and logit-lens analyses indicate (a) depthwise token-mixing peaks around middle layers (10-15) where induction-like mixing is strongest; (b) inductive-head sets generalize across subtasks; (c) mean-ablation/knockouts show performance sensitivity patterns tied to head importance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Induction-head mechanisms can be polyfunctional and overlap with other behaviors; backup circuits/hydra effect allow other components to compensate when induction heads are ablated, complicating causal claims.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Referenced induction head evidence from toy models and observed in Llama-2 7B here; no broad quantitative cross-model arithmetic comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4719.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4719.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hydra effect / backup circuits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hydra effect and backup circuits (McGrath et al., 2023; Wang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Phenomenon where multiple, loosely-coupled circuits in large models can compensate for interventions (ablation), producing functional redundancy and self-repair that obscures single-circuit causal attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The hydra effect: Emergent self-repair in language model computations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Transformer LMs (general; observed empirically in large models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep, wide Transformer models with many attention heads and MLP blocks, exhibiting redundant/overlapping circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Relevant to any complex task including arithmetic where multiple pathways could implement similar computations</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Large models contain multiple parallel pathways that can implement similar functionality; ablating one pathway often activates compensatory pathways, making it difficult to assign single-circuit responsibility for behavior (including arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>This paper documents multiple parallel answer-writing heads and pathways in Llama-2 7B for CoT fictional-ontology tasks, consistent with hydra/backup-circuit phenomena reported in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not contradicted in this paper; rather the paper provides empirical support for parallelism. However, degree of redundancy may vary with task/model and is not precisely quantified for arithmetic tasks here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Head knockouts and activation patching show that many heads can be removed without catastrophic failure (>90% relative accuracy retained when pruning to ~400/1024 heads for ontology tasks), demonstrating redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Redundancy/hydra makes causal interventions (targeted editing or circuit removal) harder; interventions can have unpredictable effects because other circuits compensate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Smaller models (e.g., GPT-2 small, IOI circuits) often show more unique, single-path algorithms, whereas larger models like Llama-2 show multiple parallel pathways per this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4719.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4719.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Functional rift (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional rift / phase shift between pretraining prior and in-context prior (Dutta et al., this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical observation in Llama-2 7B that around the middle decoder block (layer ~16) there is a phase shift: early layers' residual representations are biased toward pretraining priors, while later layers increasingly follow in-context priors; answer-writing heads appear after this rift.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7-billion-parameter decoder-only Transformer (Llama-2 family) analyzed in a few-shot Chain-of-Thought prompting regime on PrOntoQA fictional ontology tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Not explicitly evaluated for arithmetic in this paper; mechanism likely relevant to multi-step computations that rely on in-context information (e.g., arithmetic CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Model computation is partitioned: initial layers perform token mixing and align representations to contextual priors (moving information between tokens), while later layers use context-abiding attention and write final answers; arithmetic that depends on in-context intermediate steps likely requires signals to pass through the early mixing then be written by later answer-writing mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Measured 'context-abidance' of attention heads (fraction of head outputs that produce context bigrams) increases after layer ~16; token-mixing probing (classifiers on concatenated residuals) peaks around layers 10–15 and declines thereafter; answer-writing heads are observed only after the 16th decoder block via unembedding analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper studies fictional-ontology tasks, so direct transfer to arithmetic tasks is not empirically shown here; the precise generality of the rift across architectures and tasks is not established.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical metrics reported are task-conditional (ontology CoT): e.g., distinguishability peaks and head counts; no arithmetic accuracies reported.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Activation patching, logit-lens unembedding on head outputs, and probing classifiers were used to show depth-wise changes in representations and identify answer-writing heads located after the rift.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Observation may depend on model architecture/scale and on the artificial task used; role of MLPs and pretraining memorized factual associations across the rift is not fully explored here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Paper does not present cross-model comparisons of the rift; suggests the rift is at roughly the halfway point in Llama-2 7B but does not claim universality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4719.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4719.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal abstraction for arithmetic CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal abstraction for chain-of-thought reasoning in arithmetic word problems (Tan, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work showing that LLMs partially use CoT steps causally when solving arithmetic word problems; causal interventions can reveal usage of intermediate steps in producing final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal abstraction for chain-of-thought reasoning in arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LMs used in causal-abstraction studies (paper cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Studies use causal interventions/ablation to test whether intermediate CoT tokens causally affect final answers on arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic word problems (multi-step arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>LLMs sometimes causally condition on their own generated CoT steps when producing final answers; CoT can be part of the causal computation chain rather than merely correlated output.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited experimental evidence from Tan (2023) showing partial causal dependence of answers on generated CoT via interventions/ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Other studies (Lampinen et al., 2022) found improvements when models generate explanations even when not causally used; this paper's own analyses show that use of generated CoT varies by subtask and multiple parallel pathways may bypass CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>This paper references causal-tracing/activation-patching methods (used here for ontology tasks) as relevant; Tan (2023) specifically applies causal abstraction to arithmetic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Causal dependence on CoT is partial and variable across subtasks and models; backup circuits can confound causal attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Referenced as part of broader literature; no direct empirical comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards revealing the mystery behind chain of thought: a theoretical perspective. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. <em>(Rating: 2)</em></li>
                <li>Causal abstraction for chain-of-thought reasoning in arithmetic word problems. <em>(Rating: 2)</em></li>
                <li>In-context learning and induction heads. <em>(Rating: 2)</em></li>
                <li>The hydra effect: Emergent self-repair in language model computations. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4719",
    "paper_id": "paper-97994e4526ef7eeea59190aa466fbab05fad9187",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "Transformer arithmetic depth result",
            "name_full": "Transformer circuit complexity limits for arithmetic (Feng et al., 2023)",
            "brief_description": "A theoretical result that standard (constant-depth) Transformers cannot solve certain arithmetic problems (direct-answer form) unless model depth grows super-polynomially with input size, and that Chain-of-Thought (CoT) style intermediate output can overcome this barrier.",
            "citation_title": "Towards revealing the mystery behind chain of thought: a theoretical perspective.",
            "mention_or_use": "mention",
            "model_name": "Transformer (theoretical model)",
            "model_description": "Abstract Transformer models as studied in circuit-complexity theoretic analysis; not an empirical single pretrained LM evaluation.",
            "arithmetic_task_type": "General arithmetic / algorithmic numeric problems (theoretical, direct-answer tasks)",
            "mechanism_hypothesis": "Direct-answer constant-depth Transformers lack sufficient computational depth to implement the required arithmetic algorithm; generating intermediate computation (CoT / scratchpad) changes the effective computation allowing constant-depth models to simulate more complex algorithms.",
            "evidence_for_mechanism": "The paper cites Feng et al. (2023), which provides circuit-complexity style proofs showing impossibility results for direct-answer constant-depth Transformers and constructive arguments that allowing intermediate outputs (CoT) enables overcoming the depth bottleneck.",
            "evidence_against_mechanism": "This paper does not provide empirical counterexamples; the claim is theoretical and depends on formal assumptions about input encoding and the class of arithmetic problems considered.",
            "performance_metrics": null,
            "probing_or_intervention_results": "Not applicable in this paper; the claim is cited as a theoretical result rather than an empirical probing/intervention.",
            "limitations_and_failure_modes": "Result applies under formal model/complexity assumptions; real pretrained LMs may use shortcuts, memorization, or other mechanisms not captured by the theoretical model.",
            "comparison_to_other_models": "Framed as a general limitation of constant-depth Transformers vs. Transformer variants that can emit intermediate computation (CoT); no empirical model-to-model numeric comparisons provided here.",
            "uuid": "e4719.0",
            "source_info": {
                "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Scratchpads",
            "name_full": "Scratchpads / intermediate computation (Nye et al., 2021)",
            "brief_description": "The idea of letting autoregressive Transformers produce and condition on intermediate computation steps (scratchpads) which empirically improves solving complex tasks including numeric reasoning by bypassing depth/attention bottlenecks.",
            "citation_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "mention_or_use": "mention",
            "model_name": "Autoregressive Transformer models (general)",
            "model_description": "Autoregressive Transformer architectures that are trained or prompted to output intermediate computation tokens (scratchpad/CoT).",
            "arithmetic_task_type": "Multi-step numeric reasoning / arithmetic where intermediate steps help decomposition",
            "mechanism_hypothesis": "By explicitly emitting intermediate steps, the model offloads multi-step computation into the token stream, effectively increasing available computation and enabling solutions that would otherwise require greater model depth.",
            "evidence_for_mechanism": "Cited empirical work (Nye et al.) demonstrated improved generalization and performance when models generate scratchpads; the present paper references this as motivation for CoT-style methods that enable arithmetic/complex reasoning.",
            "evidence_against_mechanism": "Related work notes brittle shortcut solutions can exist and benefits depend on training and prompting; this paper does not present direct counter-evidence.",
            "performance_metrics": null,
            "probing_or_intervention_results": "Not performed here for arithmetic specifically; paper uses activation patching and probing for ontology CoT tasks, not numeric scratchpad experiments.",
            "limitations_and_failure_modes": "Scratchpads require the model to produce useful intermediate content; explanations may correlate with rather than causally produce better answers in some settings (see Lampinen et al. 2022 discussion cited).",
            "comparison_to_other_models": "Compared conceptually to direct-answer Transformers; no numeric comparisons in this paper.",
            "uuid": "e4719.1",
            "source_info": {
                "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting (Wei et al., 2022b and related work)",
            "brief_description": "A prompting technique that elicits step-by-step natural-language intermediate reasoning from LLMs, which empirically improves performance on multi-step reasoning tasks, including arithmetic-style word problems.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "Large Language Models (e.g., Llama-2 7B in this paper; GPT-family in related work)",
            "model_description": "Autoregressive transformer LLMs; in this paper, analyses are performed on Llama-2 7B in a few-shot CoT prompting regime.",
            "arithmetic_task_type": "Multi-step reasoning and word-problem style arithmetic (cited in related work; this paper focuses on fictional ontology reasoning rather than numeric arithmetic experiments).",
            "mechanism_hypothesis": "CoT works by making intermediate computation explicit in the token stream, enabling model circuits (e.g., induction heads, attention pathways) to operate sequentially over subtasks; CoT may change which pathways are used and allow constant-depth networks to implement multi-step algorithms effectively.",
            "evidence_for_mechanism": "This paper provides mechanistic evidence (activation patching, probing, logit-lens) showing that Llama-2 7B uses multiple parallel pathways to collect and write answers and that some heads rely on generated context (CoT) while others pull directly from question/few-shot context — supporting partial causal use of CoT.",
            "evidence_against_mechanism": "Related observations (Lampinen et al., 2022) show explanation-generation can improve accuracy even when not causally used; Tan (2023) shows partial causal dependence. This paper shows CoT usage varies by subtask — sometimes answers come directly from question context rather than generated CoT.",
            "performance_metrics": "This paper reports CoT mechanistic analyses but does not present numeric arithmetic task metrics; Llama-2 7B retained &gt;90% relative accuracy on the fictional ontology CoT subtasks when pruned to ~400 heads (not arithmetic).",
            "probing_or_intervention_results": "Activation patching, probing classifiers, logit-lens and targeted head knockouts reveal: (a) multiple answer-writing heads appear after a mid-layer 'rift'; (b) some answer pathways read from generated CoT, some from question/few-shot context; (c) knocking out many heads leaves &gt;90% performance on the ontology tasks.",
            "limitations_and_failure_modes": "CoT usage is non-uniform — some subtasks do not rely on generated CoT; parallel backup circuits and hydra-effect (self-repair) make causal intervention difficult; this paper studies fictional ontologies, not arithmetic directly.",
            "comparison_to_other_models": "Paper references that CoT efficacy depends on model scale (Wei et al., 2022a; Saparov & He, 2023) but does not present empirical cross-model arithmetic comparisons.",
            "uuid": "e4719.2",
            "source_info": {
                "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Program-of-Thought",
            "name_full": "Program-of-Thought / program generation for numerical reasoning (Chen et al., 2023; Bi et al., 2024)",
            "brief_description": "Approaches that prompt LMs to generate executable program-like intermediate representations (instead of natural-language CoT) to disentangle computation from reasoning for numerical problems, often improving numerical reasoning.",
            "citation_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "mention_or_use": "mention",
            "model_name": "Large LMs using program-generation prompts (general references to transformers/LLMs)",
            "model_description": "Autoregressive Transformer LLMs prompted to output program-like structures (e.g., pseudo-code or actual code) as intermediate steps for arithmetic/numeric tasks.",
            "arithmetic_task_type": "Numerical reasoning, arithmetic word problems, and program-style multi-step computation",
            "mechanism_hypothesis": "Emitting program-like intermediate steps separates algorithmic computation (which can be executed or more structured) from linguistic reasoning, enabling more reliable numeric computation than free-form natural-language CoT.",
            "evidence_for_mechanism": "Cited work (Chen et al., 2023; Bi et al., 2024) empirically shows program-like CoT can improve numerical reasoning. This paper cites those works as part of related literature but does not empirically test program-of-thought.",
            "evidence_against_mechanism": "This paper does not present counter-evidence; it notes that program generation tends to favor certain complexities and that reasoning approaches may be brittle or dependent on model scale.",
            "performance_metrics": null,
            "probing_or_intervention_results": "No program-of-thought probing performed in this paper.",
            "limitations_and_failure_modes": "Program outputs still depend on model capabilities and may be brittle; not analyzed here.",
            "comparison_to_other_models": "Referenced relative to free-form CoT and direct-answer approaches; no direct comparisons reported in this paper.",
            "uuid": "e4719.3",
            "source_info": {
                "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Induction heads",
            "name_full": "Induction heads / pattern-copying attention heads (Olsson et al., 2022; Elhage et al., 2021)",
            "brief_description": "A specific attention-head composition (often a query and key/value pairing across layers) that enables pattern copying from earlier context tokens to later ones and is implicated in in-context learning and pattern matching behaviors.",
            "citation_title": "In-context learning and induction heads.",
            "mention_or_use": "use",
            "model_name": "Transformer-based LLMs (toy models and Llama-2 7B analyzed here)",
            "model_description": "Attention-only (or attention-dominant) Transformer architectures where particular heads implement copy/induction-like behavior via attention patterns.",
            "arithmetic_task_type": "Induction and pattern-based reasoning; may underlie multi-step reasoning that includes arithmetic when arithmetic can be cast as pattern copying or algorithmic induction.",
            "mechanism_hypothesis": "Induction heads perform token-to-token information copying and pattern matching across the context; compositions of such heads can implement in-context algorithms (including the subtask operations needed for CoT) and thus support arithmetic when arithmetic is decomposed into token-level steps.",
            "evidence_for_mechanism": "This paper finds that heads identified via inductive-reasoning scoring perform consistently across CoT subtasks in the fictional ontology tasks, require fewer heads, and appear to implement information mixing among ontologically related tokens; activation patching and probing show these heads move information between residual streams.",
            "evidence_against_mechanism": "Induction heads alone do not fully separate decision-making, copying, and induction roles — many heads are shared across those subtasks; arithmetic not directly tested here so applicability to numeric tasks is inferred but not proven.",
            "performance_metrics": "In ontology CoT tasks, pruning to inductive-head-based subsets retained &gt;90% accuracy with ~40% of heads active; no numeric arithmetic metrics reported.",
            "probing_or_intervention_results": "Activation patching, probing classifiers, and logit-lens analyses indicate (a) depthwise token-mixing peaks around middle layers (10-15) where induction-like mixing is strongest; (b) inductive-head sets generalize across subtasks; (c) mean-ablation/knockouts show performance sensitivity patterns tied to head importance.",
            "limitations_and_failure_modes": "Induction-head mechanisms can be polyfunctional and overlap with other behaviors; backup circuits/hydra effect allow other components to compensate when induction heads are ablated, complicating causal claims.",
            "comparison_to_other_models": "Referenced induction head evidence from toy models and observed in Llama-2 7B here; no broad quantitative cross-model arithmetic comparisons.",
            "uuid": "e4719.4",
            "source_info": {
                "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Hydra effect / backup circuits",
            "name_full": "Hydra effect and backup circuits (McGrath et al., 2023; Wang et al., 2023)",
            "brief_description": "Phenomenon where multiple, loosely-coupled circuits in large models can compensate for interventions (ablation), producing functional redundancy and self-repair that obscures single-circuit causal attribution.",
            "citation_title": "The hydra effect: Emergent self-repair in language model computations.",
            "mention_or_use": "mention",
            "model_name": "Large Transformer LMs (general; observed empirically in large models)",
            "model_description": "Deep, wide Transformer models with many attention heads and MLP blocks, exhibiting redundant/overlapping circuits.",
            "arithmetic_task_type": "Relevant to any complex task including arithmetic where multiple pathways could implement similar computations",
            "mechanism_hypothesis": "Large models contain multiple parallel pathways that can implement similar functionality; ablating one pathway often activates compensatory pathways, making it difficult to assign single-circuit responsibility for behavior (including arithmetic).",
            "evidence_for_mechanism": "This paper documents multiple parallel answer-writing heads and pathways in Llama-2 7B for CoT fictional-ontology tasks, consistent with hydra/backup-circuit phenomena reported in cited work.",
            "evidence_against_mechanism": "Not contradicted in this paper; rather the paper provides empirical support for parallelism. However, degree of redundancy may vary with task/model and is not precisely quantified for arithmetic tasks here.",
            "performance_metrics": null,
            "probing_or_intervention_results": "Head knockouts and activation patching show that many heads can be removed without catastrophic failure (&gt;90% relative accuracy retained when pruning to ~400/1024 heads for ontology tasks), demonstrating redundancy.",
            "limitations_and_failure_modes": "Redundancy/hydra makes causal interventions (targeted editing or circuit removal) harder; interventions can have unpredictable effects because other circuits compensate.",
            "comparison_to_other_models": "Smaller models (e.g., GPT-2 small, IOI circuits) often show more unique, single-path algorithms, whereas larger models like Llama-2 show multiple parallel pathways per this paper.",
            "uuid": "e4719.5",
            "source_info": {
                "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Functional rift (this paper)",
            "name_full": "Functional rift / phase shift between pretraining prior and in-context prior (Dutta et al., this paper)",
            "brief_description": "An empirical observation in Llama-2 7B that around the middle decoder block (layer ~16) there is a phase shift: early layers' residual representations are biased toward pretraining priors, while later layers increasingly follow in-context priors; answer-writing heads appear after this rift.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 7B",
            "model_description": "7-billion-parameter decoder-only Transformer (Llama-2 family) analyzed in a few-shot Chain-of-Thought prompting regime on PrOntoQA fictional ontology tasks.",
            "arithmetic_task_type": "Not explicitly evaluated for arithmetic in this paper; mechanism likely relevant to multi-step computations that rely on in-context information (e.g., arithmetic CoT).",
            "mechanism_hypothesis": "Model computation is partitioned: initial layers perform token mixing and align representations to contextual priors (moving information between tokens), while later layers use context-abiding attention and write final answers; arithmetic that depends on in-context intermediate steps likely requires signals to pass through the early mixing then be written by later answer-writing mechanisms.",
            "evidence_for_mechanism": "Measured 'context-abidance' of attention heads (fraction of head outputs that produce context bigrams) increases after layer ~16; token-mixing probing (classifiers on concatenated residuals) peaks around layers 10–15 and declines thereafter; answer-writing heads are observed only after the 16th decoder block via unembedding analysis.",
            "evidence_against_mechanism": "Paper studies fictional-ontology tasks, so direct transfer to arithmetic tasks is not empirically shown here; the precise generality of the rift across architectures and tasks is not established.",
            "performance_metrics": "Empirical metrics reported are task-conditional (ontology CoT): e.g., distinguishability peaks and head counts; no arithmetic accuracies reported.",
            "probing_or_intervention_results": "Activation patching, logit-lens unembedding on head outputs, and probing classifiers were used to show depth-wise changes in representations and identify answer-writing heads located after the rift.",
            "limitations_and_failure_modes": "Observation may depend on model architecture/scale and on the artificial task used; role of MLPs and pretraining memorized factual associations across the rift is not fully explored here.",
            "comparison_to_other_models": "Paper does not present cross-model comparisons of the rift; suggests the rift is at roughly the halfway point in Llama-2 7B but does not claim universality.",
            "uuid": "e4719.6",
            "source_info": {
                "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Causal abstraction for arithmetic CoT",
            "name_full": "Causal abstraction for chain-of-thought reasoning in arithmetic word problems (Tan, 2023)",
            "brief_description": "Work showing that LLMs partially use CoT steps causally when solving arithmetic word problems; causal interventions can reveal usage of intermediate steps in producing final answers.",
            "citation_title": "Causal abstraction for chain-of-thought reasoning in arithmetic word problems.",
            "mention_or_use": "mention",
            "model_name": "Large LMs used in causal-abstraction studies (paper cited)",
            "model_description": "Studies use causal interventions/ablation to test whether intermediate CoT tokens causally affect final answers on arithmetic word problems.",
            "arithmetic_task_type": "Arithmetic word problems (multi-step arithmetic)",
            "mechanism_hypothesis": "LLMs sometimes causally condition on their own generated CoT steps when producing final answers; CoT can be part of the causal computation chain rather than merely correlated output.",
            "evidence_for_mechanism": "Cited experimental evidence from Tan (2023) showing partial causal dependence of answers on generated CoT via interventions/ablation.",
            "evidence_against_mechanism": "Other studies (Lampinen et al., 2022) found improvements when models generate explanations even when not causally used; this paper's own analyses show that use of generated CoT varies by subtask and multiple parallel pathways may bypass CoT.",
            "performance_metrics": null,
            "probing_or_intervention_results": "This paper references causal-tracing/activation-patching methods (used here for ontology tasks) as relevant; Tan (2023) specifically applies causal abstraction to arithmetic problems.",
            "limitations_and_failure_modes": "Causal dependence on CoT is partial and variable across subtasks and models; backup circuits can confound causal attribution.",
            "comparison_to_other_models": "Referenced as part of broader literature; no direct empirical comparisons in this paper.",
            "uuid": "e4719.7",
            "source_info": {
                "paper_title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards revealing the mystery behind chain of thought: a theoretical perspective.",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "rating": 2
        },
        {
            "paper_title": "Causal abstraction for chain-of-thought reasoning in arithmetic word problems.",
            "rating": 2
        },
        {
            "paper_title": "In-context learning and induction heads.",
            "rating": 2
        },
        {
            "paper_title": "The hydra effect: Emergent self-repair in language model computations.",
            "rating": 2
        }
    ],
    "cost": 0.01648475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning</h1>
<p>Subhabrata Dutta<em><br>iIT Delhi, India<br>Joykirat Singh</em><br>joykiratsingh18@gmail.com<br>Independent ${ }^{\dagger}$<br>Soumen Chakrabarti<br>iIT Bombay, India<br>Tanmoy Chakraborty<br>iIT Delhi, India<br>soumen@cse.iitb.ac.in<br>tanchak@ee.iitd.ac.in</p>
<h4>Abstract</h4>
<p>Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of Llama-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-bystep reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context prior taking over in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token appear in the later half, attention heads that move information along ontological relationships appear in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.</p>
<h2>1 Introduction</h2>
<p>Recent advancements with Large Language Models (LLMs) demonstrate their remarkable reasoning prowess with natural language across a diverse set of problems (OpenAI, 2024; Kojima et al., 2022; Chen et al., 2023). Yet, existing literature fails to describe the neural mechanism within the model that implements those abilities - how they emerge from the training dynamics and why they are often brittle against even unrelated changes. One of these capabilities of LLMs that has boosted their potential in complex reasoning is Chain-of-Thought (CoT) prompting (Wei et al., 2022b; Kojima et al., 2022). Instead of providing a direct answer to the question, in CoT prompting, we expect the model to generate a verbose response, adopting a step-by-step reasoning process to reach the answer. Despite the success of eliciting intermediate computation and their diverse, structured demonstration, the exact mechanism of CoT prompting remains mysterious. Prior attempts have been made to restrict the problem within structured, synthetic reasoning to observe the LLM's CoT generation behavior (Saparov \&amp; He, 2023); context perturbation towards causal modeling has also been used (Tan, 2023). A few recent approaches seek to associate the emergence of CoT reasoning ability in LLMs with localized structures in the pertaining data (Prystawski et al., 2024; Wang \&amp; Wang,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2023). However, these endeavors produce only an indirect observation of the mechanism; the underlying neural algorithm implemented by the LLM remains in the dark.</p>
<p>Recent developments in the mechanistic interpretation of Transformer-based models provide hope for uncovering the neural 'algorithms' at work inside LLMs (Elhage et al., 2021; Nanda, 2022). Typically, mechanistic methods seek to build a causal description of the model; starting from the output, they localize important components of the model (e.g., an attention head or a multilayer perceptron (MLP) block) by activation patching (Wang et al., 2023; Zhang \&amp; Nanda, 2023). However, there are some implicit challenges in reverseengineering CoT-prompting in foundational models. The capability of generating CoT is largely dependent on model scale (Wei et al., 2022a; Saparov \&amp; He, 2023). On the other hand, reverse-engineering a large model becomes a wild goose chase due to the hydra effect (McGrath et al., 2023) — neural algorithmic components within LLMs are adaptive: once we 'switch off' one functional component, others will pitch in to supply the missing functionality. Furthermore, in most real-world problems involving multi-step reasoning, there are implicit knowledge requirements. LLMs tend to memorize factual associations from pretraining as key-value caches using the MLP blocks Geva et al. (2021); Meng et al. (2022). Due to the large number of parameters in MLP blocks and their implicit polysemanticity, interpretation becomes extremely challenging.</p>
<p>In this paper, we seek to address these challenges, thus shedding light on the internal mechanism of Transformer-based LLMs while they perform CoT-based reasoning. Going beyond the typical toy model regime of mechanistic interpretation, we work with Llama-2 7B (Touvron et al., 2023). To minimize the effects of MLP blocks and focus primarily on reasoning from the provided context, we make use of the PrOntoQA dataset (Saparov \&amp; He, 2023) that employs ontology-based question answering using fictional entities (see Figure 1 for an example). Specifically, we dissect CoT-based reasoning on fictional reasoning as a composition of a fixed number of subtasks that require decision-making, copying, and inductive reasoning ( Section 4).</p>
<p>We draw on three prominent techniques for investigating neural algorithms, namely, activation patching (Nanda, 2022), probing classifiers (Belinkov, 2022), and logit lens (nostalgebraist, 2020) to disentangle different aspects of the neural algorithm implemented by Llama-2 7B. We find that despite the difference in reasoning requirement of different subtasks, the sets of attention heads that implement their respective algorithms enjoy significant intersection ( Section 4.1). Moreover, they point towards the existence of mechanisms similar to induction heads (Olsson et al., 2022) working together. In the initial layers of the model (typically, from first to 16th decoder blocks in Llama-2 7B), attention heads conduct information transfer between ontologically related fictional entities; e.g., for the input numpuses are rompuses, this mechanism copies information from numpus to rompus so that any pattern involving the former can be induced with the latter ( Section 5). Interestingly, in this same segment of the model, we find a gradual transition in the residual stream representation from pretraining before in-context prior, i.e., the contextual information replaces the bigram associations gathered via pretraining ( $\boldsymbol{\sim}$ Section 6.1).</p>
<p>Following this, we seek to identify the pathways of information that are responsible for processing the answer and write it to the output residual stream for each subtask. To deal with the abundance of backup circuits (Wang et al., 2023), we look for such parallel pathways simultaneously without switching off any of them. We find that multiple attention heads simultaneously write the answer token into the output, though all of them appear at or after the 16th decoder block ( $\boldsymbol{\sim}$ Section 6.2). These answer tokens are collected from multiple sources as well (i.e., from the few-shot context, input question, and the generated CoT context), pointing towards the coexistence of multiple neural algorithms working in parallel.</p>
<p>Our findings supply empirical answers to a pertinent open question about whether LLMs actually rely on CoT to answer questions (Tan, 2023; Lampinen et al., 2022): the usage of generated CoT varies across subtasks, and there exists parallel pathways of answer collection from CoT as well as directly from the question context ( Section 6.3). Here again, we observe the peculiar functional rift within the middle of the model: answer tokens, that are present in the few-shot examples but contextually different from the same token in the question, are used as sources by attention heads, primarily before the 16th decoder block. To the best of our knowledge, this is the first-ever in-depth analysis of CoT-mediated reasoning in LLMs in terms of the neural functional components. Code and data ${ }^{1}$ are made available publicly.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 Related work</h1>
<p>In this section, we provide an overview of literature related to this work, primarily around different forms of CoT reasoning with Transformer-based LMs, theoretical and empirical investigations into characteristics of CoT , and mechanistic interpretability literature around language models.</p>
<p>Nye et al. (2021) first showed that, instead of asking to answer directly, letting an autoregressive Transformer generate intermediate computation, which they called scratchpads, elicits superior reasoning performance. They intuitively explained such behavior based on the observation that, in a constant depth-width model with $\mathcal{O}\left(n^{2}\right)$ complexity attention, it is not possible to emulate an algorithm that requires super-polynomial computation; by writing the intermediate answer, this bottleneck is bypassed. Wei et al. (2022b) demonstrated that LLMs enabled with unstructured natural language expressions of intermediate computations, aka CoT , can demonstrate versatile reasoning capabilities.</p>
<p>Multiple recent attempts have been made toward a deeper understanding of CoT, both empirically and theoretically. Feng et al. (2023) employed circuit complexity theory to prove that - (i) Transformers cannot solve arithmetic problems with direct answers unless the model depth grows super-polynomially with respect to problem input size, and (ii) Constant depth Transformers with CoT generation are able to overcome the said challenge. Liu et al. (2022) showed that Transformers can learn shortcut solutions to simulate automata with standard training (without CoT); however, they are statistically brittle, and recency-biased scratchpad training similar to that of Nye et al. (2021) can help with better generalizability and robustness. Saparov $\&amp; \mathrm{He}(2023)$ dissected the behavior of LLMs across scale on synthetically generated multistep reasoning tasks on true, false, and fictional ontologies. In a somewhat counterintuitive finding, Lampinen et al. (2022) observed that even if an LLM is prompted to generate an answer followed by an explanation, there is a significant improvement over generating an answer without any explanation. While this might point toward a non-causal dependence between the explanation and the answer, Tan (2023) showed that LLMs do utilize, at least partially, the information generated within the intermediate steps such that the answer is causally dependent on the CoT steps. Prystawski et al. (2024) theoretically associated the emergence of CoT with localized statistical structures within the pertaining data. Wang \&amp; Wang (2023) investigated reasoning over knowledge graph to characterize the emergence of CoT. They identified that reasoning capabilities are primarily acquired in the pertaining stage and not downstream fine-tuning while empirically validating Prystawski et al. (2024)'s theoretical framework in a more grounded manner. Bi et al. (2024) found that while reasoning via program generation, LLMs tend to favor an optimal complexity of the programs for best performance.</p>
<p>The above-mentioned studies either 1) dissect the model behavior under controllable perturbations in the input problem or 2) construct theoretical frameworks under different assumptions to prove the existence of certain abilities/disabilities. Mechanistic interpretability techniques go one step deeper and seek to uncover the neural algorithm deployed by the model to perform a certain task. Elhage et al. (2021) studied one-, two, and three-layer deep, attention-only autoregressive Transformers and their training dynamics. A crucial finding of their investigation was the existence of induction heads - a composition of two attention heads at different layers that can perform pattern copying from context. Olsson et al. (2022) empirically observed the simultaneous emergence of induction heads and in-context learning in the training dynamics of similar toy Transformer models. Similar analyses on phenomena like polysemanticity, superposition, and memorization have been performed in the toy model regime (Elhage et al., 2022; Henighan et al., 2023). Beyond these toy models, Wang et al. (2023) analyzed circuits in GPT-2 responsible for Indirect Object Identification. Wu et al. (2023) proposed a causal abstraction model to explain price-tagging in the Alpaca-7B model: given a range of prices and a candidate price, the task is to classify if the candidate price falls within the given range. Extrapolating mechanistic observations from the regime of toy models to 'production' LLMs is particularly challenging; with billions of parameters and deep stacking of attention heads, identifying head compositions that instantiate an algorithm is extremely difficult. Furthermore, as McGrath et al. (2023) suggest, different components of the LLM are loosely coupled and adaptive in nature, for which they coined the term hydra effect - ablating an attention layer may get functionally compensated by another. Wang et al. (2023) also identified similar information pathways called backup circuits that can take over once the primary circuits are corrupted.</p>
<h1>3 Background</h1>
<p>In this section, for completeness, we briefly introduce the concepts and assumptions necessary for delving into the interpretation of CoT reasoning.</p>
<p>The Transformer architecture (Vaswani et al., 2017) consists of multiple, alternated attention and MLP blocks, preceded and followed by an embedding projection and a logit projection, respectively. Given the vocabulary as $V$, we denote the embedded representation of the $i$-th token $s_{i} \in V$ in an input token sequence $S$ of length $N$ (i.e., the sum of embedding projections and position embeddings in case of additive position encoding) as $\boldsymbol{x}<em j-1="j-1">{0}^{i} \in \mathbb{R}^{d}$, where $d$ is the model dimension. We denote the content of the residual stream corresponding to the $i$-th token, input to the $j$-th decoder block, as $\boldsymbol{x}</em>}^{i}$, which becomes $\tilde{\boldsymbol{x}<em 0="0">{j-1}^{i}$ after the attention layer reads and writes on the residual stream. The initial content of the residual stream is the same as the token embedding $\boldsymbol{x}</em>}^{i}$. Assuming $H$ number of heads for each attention layer, the operation of $k$-th attention head at $j$-th decoder block on the $i$-th token's residual stream is denoted as $\boldsymbol{y<em j_="j," k="k">{j, k}^{i}=h</em>}\left(\boldsymbol{x<em j_="j," k="k">{j-1}^{i}\right)$, $\boldsymbol{y}</em>}^{i} \in \mathbb{R}^{d}$. Then $\tilde{\boldsymbol{x}<em j-1="j-1">{j}^{i}=\boldsymbol{x}</em>}^{i}+\sum_{k} \boldsymbol{y<em j_="j," k="k">{j, k}^{i}$ is the content of the residual stream immediately after the attention operations. Each attention head $h</em>}$ is parameterized using four projection matrices: query, key and value projection matrices, denoted by $\boldsymbol{W<em K="K">{Q}^{j, k}, \boldsymbol{W}</em>}^{j, k}, \boldsymbol{W<em O="O">{V}^{j, k} \in \mathbb{R}^{d \times \frac{d}{H}}$, respectively, and output projection matrix $\boldsymbol{W}</em>}^{j, k} \in \mathbb{R}^{\frac{d}{H} \times d}$. Note that in the case of Llama models, the position encoding is incorporated via rotation of query and key projections before computing dot-product (Su et al., 2023); we omit this step for brevity. Similarly, the action of the MLP block can be expressed as $\boldsymbol{z<em j="j">{j}^{i}=\operatorname{MLP}</em>}\left(\tilde{\boldsymbol{x}<em j="j">{j}^{i}\right)$, with $\boldsymbol{x}</em>}^{i}=\tilde{\boldsymbol{x}<em j="j">{j}^{i}+\boldsymbol{z}</em>$ :}^{i}$ denoting the content of the residual stream after decoder block $j$. After processing the information through $L$ number of decoder blocks, a feedforward transformation $\boldsymbol{U} \in \mathbb{R}^{d \times|V|}$, commonly known as the unembedding projection, maps the content of the residual stream into the logit space (i.e., a distribution over the token space). Given a sequence of input tokens $S=\left{s_{1}, \cdots, s_{i}\right}$, the autoregressive Transformer model predicts a new output token $s_{i+1</p>
<p>$$
s_{i+1}=\arg \max <em _logit="{logit" _text="\text">{\boldsymbol{x}</em> \mid S\right)
$$}}^{i}} \operatorname{LM}\left(\boldsymbol{x}_{\text {logit }}^{i</p>
<p>Fictional ontology-based reasoning, as proposed by Saparov \&amp; He (2023) as PrOntoQA, provides a tractable approach to dissect CoT generation. The reasoning problem is framed as question-answering on a tree-based ontology of fictional entities (see Appendix A for examples of ontologies and reasoning problems framed). This eases two major challenges in our case: (i) Mechanistic interpretation requires input or activation perturbation and recording the results of such perturbations. With CoT, one then needs to repeat such perturbation process for all the constituent subtasks. Unlike free-form CoT reasoning, PrOntoQA provides a clearly demarcated sequence of successive steps that can be analyzed independently. (ii) The solution to most real-world reasoning problems heavily requires factual knowledge, so much so that a sound reasoning process might get misled by incorrect fact retrieval. A fictional ontology ensures zero interference between the entity relationships presented in the question and the world-knowledge acquired by the model in the pertaining stage. This further minimizes the involvement of the MLP layers in the neural mechanisms implemented by the model as they typically serves as the parametric fact memory of an LM (Geva et al., 2021; Meng et al., 2022). Additionally, PrOntoQA provides reasoning formulation over false ontologies (see example in Appendix A). False ontology grounds the reasoning over statements that are false in the real world. An important demarcation between fictional and false ontological reasoning is that while the former minimizes the effects of factual associations memorized as pretraining prior, the latter requires the LM to actively eclipse such memorized knowledge to solve the problem successfully.
Circuits, in mechanistic interpretability research, provide the abstractions of interpretable algorithms implemented by the model within itself. Typically, a circuit is a subgraph of the complete computational graph of the model, responsible for a specific set of tasks. We primarily follow the notation adopted by Wang et al. (2023), with nodes defined by model components like attention heads and projections and edges defined by interactions between such components in terms of attention, residual streams, etc.
Activation patching is a common method in interpretability research. Activation patching begins with two forward passes of the model, one with the actual input and another with a selectively corrupted one. The choice of input corruption depends on the task and the type of interpretation required. For example,</p>
<p>consider the following Indirect Object Identification (IOI) task (Wang et al., 2023): given an input John and Mary went to the park. John passed the bottle to, the model should predict Mary. Further, corrupting the input by replacing Mary with Anne would result in the output changing to Anne. Let $\boldsymbol{x}<em j="j">{j}^{\text {Mary }}$ and $\boldsymbol{x}</em>}^{\text {Anne }}$ represent the original and corrupted residual streams at decoder block $j$, depending on whether Mary or Anne was injected at the input. Now, in a corrupted forward pass, for a given $j$, if the replacement of $\boldsymbol{x<em j="j">{j}^{\text {Anne }}$ by $\boldsymbol{x}</em>$ results in the restoration of the output token from Anne to Mary, then one can conclude that attention mechanism at decoder block $j$ is responsible for moving the name information. This is an example of patching corrupted-to-clean activation, often called causal tracing (Meng et al., 2022). Activation patching, in general, refers to both clean-to-corrupted as well as corrupted-to-clean patching (Zhang \&amp; Nanda, 2023).}^{\text {Mary }</p>
<p>Knockout is a method to prune nodes in the full computational graph of the model to identify task-specific circuits. Complete ablation of a node is equivalent to replacing the node output with an all-zero vector. Our initial experiments suggest that such an ablation destructively interferes with the model's computation. Instead, we follow Wang et al. (2023) for mean-ablation to perform knockouts. Specifically, we construct inputs from the false ontologies provided in the PrOntoQA dataset and compute the mean activations for each layer across different inputs:</p>
<p>$$
\boldsymbol{x}<em i="i">{j}^{\text {Knock }}=\operatorname{Mean}</em>}\left(\left{\boldsymbol{x<em i="i">{j}^{i} \mid s</em>\right}\right)
$$} \in S \in \mathcal{D}_{\text {False }</p>
<p>where $\mathcal{D}<em j_="j," k="k">{\text {False }}$ denotes the false ontology dataset. Then, the language model function with head $h</em>$ knocked out for the residual stream corresponding to the $l$-th token, can be represented as,</p>
<p>$$
s_{i}^{\text {Knock }}=\arg \max <em _logit="{logit" _text="\text">{\boldsymbol{x}</em>}}^{l}} \operatorname{LM<em _logit="{logit" _text="\text">{j, k}^{l}\left(\boldsymbol{x}</em>}}^{l} \mid S, \boldsymbol{y<em j="j">{j, k}^{l}=x</em>\right)
$$}^{\text {Knock }</p>
<p>More often than not in this work, we will need to knock out a set of heads $\mathcal{H}$; we will denote the corresponding language model as $\mathrm{LM}_{\mathcal{H}}^{l}$. Also, if we perform knockout on the last residual stream (which, when projected to the token space, gives us the output token at the current step of generation), we will drop the superscript $l$.</p>
<h1>4 Task composition</h1>
<p>We seek to discover the circuits responsible in Llama-2 7B for few-shot CoT reasoning on the examples from PrOntoQA fictional ontology problems. Consider the example presented in Figure 1: we ask the model "Tumpuses are bright. Lempuses are tumpuses. Max is a lempus. True or False: Max is bright" (for brevity, we omit the few-shot examples that precede and the CoT prompt Let's think step by step that follows the question). Llama-2 7B generates a verbose CoT reasoning sequence as follows: "Max is a lempus. Lempuses are tumpuses. Max is a tumpus. Tumpuses are bright. Max is bright." We can observe that such a reasoning process constitutes three critical kinds of subtasks:</p>
<ol>
<li>Decision-making: The model decides on the path of reasoning to follow. In Figure 1, given the three possible entities to start with, namely Tumpus, Lempus, and Max, Llama-2 7B starts with the last one (i.e., Max is a lempus). One would require multiple such decision-making steps within the complete reasoning process.</li>
<li>Copying: The LM needs to copy key information given in the input to the output. Typically, decisionmaking precedes copying, as the model needs to decide which information to copy.</li>
<li>Induction: The LM uses a set of statements to infer new relations. Again, a decision-making step precedes as the model must decide on the relation to infer. In the majority of this work, we focus on 2-hop inductions where given statements of the form A is B and B is C, the model should infer A is C.</li>
</ol>
<p>In all our analyses henceforth, we follow a structure where the model needs to generate CoT responses that solve the overall task using ten steps or subtasks, each belonging to one of the three categories, as presented in Figure 1. (See Appendix E for details of few-shot examples used along with the overall performance of Llama-2 7B.) A natural hypothesis would be that distinct, well-defined circuits corresponding to each</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An working example of task decomposition of CoT generation on fictional ontology. Decision-type subtasks ( $0,2,4,6$, and 8 ) choose which reasoning path to follow; such decisions may be about the order of the information to be copied from the question to the generated output or whether inductive reasoning will be performed. Copy-type subtasks (1, 3, and 7) follow Decision subtasks and copy statements from the context to output. Induction-type subtasks (5 and 9) perform the reasoning steps of the form "if A is B and B is C , then A is C ".
category of subtasks (i.e., decision-making, copying, and inductive reasoning) exist that work in conjunction to lead the CoT generation. However, as our findings in the next section suggest, this is not the case in reality.</p>
<h1>4.1 Task-specific head identification</h1>
<p>As Wang et al. (2023) suggested, the very first step toward circuit discovery is to identify the components. Since we presume the attention heads as the nodes of a circuit, the goal then becomes to identify those heads in the language model that are most important for a given task. We define the importance of the $k$-th head at the $j$-th decoder block, $h_{j, k}$ for a particular task as follows.</p>
<p>Let the task be defined as predicting a token $s_{i+1}$ given the input $S=\left{s_{1}, s_{2}, \cdots, s_{i}\right}$. For example, in the demonstration provided in Figure 1, predicting the token Max given the context Tumpuses are bright. Lempuses are tumpuses. Max is a lempus. True or False: Max is bright. Response: Let us think step by step. Tumpuses are bright. Lempuses are tumpuses. We assign a score $\mu_{\text {Task }}\left(h_{j, k}\right)$ to each head $h_{j, k}$ proportional to their importance in performing the given task. Following the intuitive subtask demarcation presented earlier, we start with scoring the attention heads for each different subtask. We provide the detailed procedure of calculating $\mu_{\text {Task }}\left(h_{j, k}\right)$ for each category of subtasks - decision-making, copying, and induction, in Appendix C.</p>
<p>Figure 2 demonstrates how each of these three categories of attention heads performs on its respective subtasks and the rest of the subtasks (note that we perform head ablation here for each subtask index independently). We also show the fraction of the total number of heads involved in performing each subtask. A higher accuracy with a lower number of head involvement would suggest that we have found the minimal</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance of attention heads identified for each different subtask type across different subtasks. We show the performance of (a) decision-making, (b) copy, and (c) inductive reasoning heads for each subtask 0 to 9 (blue bars show accuracy when the rest of the heads are knocked out; red bars denote the fraction of heads involved, see Figure 1 for subtask annotation). (d) Task-only heads are only those that are not shared with other tasks, e.g., only those copying heads for subtask 4 that are not decision-making or inductive reasoning heads. Inductive reasoning heads are consistently functional across all the subtasks with the least number of heads involved.</p>
<p>set of heads constitute the responsible circuit. As we can observe, there are no obvious mappings between the tasks used for head identification and the subtasks that constitute the CoT. Quite surprisingly, heads that we deem responsible for inductive reasoning can perform well across all the subtasks (Figure 2 (c)) and requires the least number of active heads across all tasks. Furthermore, the three sets of heads share a significant number of heads that are essential for the subtasks. In Figure 2(d), for each subtask, we use the heads that are not shared by subtasks of other categories. For example, in subtask 2, since we assumed it to be a decision-making subtask, we took only those decision-making heads that were not present in the copying or inductive reasoning heads. It is evident that these tasks are not structurally well differentiated in the language model. A good majority of heads share the importance of all three subtasks. The existence of backup circuits justifies such phenomena; however, with large models, the "self-repairing" tendency (McGrath et al., 2023) is much higher. We hypothesize that there exists even further granularity of functional components that collectively express all three types of surface-level subtasks. We seek a solution in the earlier claims that pattern matching via induction heads serves as the progenitor of in-context learning (Olsson et al., 2022). We observe that the inductive reasoning subtasks of the form if [A] is [B] and [B] is [C] then [A] is [C] essentially requires circuits that can perform 1) representation mixing from [A] to [B] to [C], and 2) pattern matching over past context by <em>deciding</em> which token to copy and then copying them to the output residual stream. Therefore, heads responsible for inductive reasoning can perform decision-making and copying by this argument.</p>
<p>For empirical validation of this argument, we analyze the effects of knocking out heads on the accuracy of each of the subtask indices. For each subtask, we construct a histogram of head counts over µ(hj,k). It can be seen that the corresponding density distribution is Gaussian in nature. Furthermore, we observe that the actual effect of pruning a head on the accuracy is not dependent on the actual value of µ(hj,k) but varies with δ(hj,k) = (µ(hj,k) − Meanj,k (µ(hj,k))^2. From the definition of µ(hj,k) provided in Appendix C, we can explain the behavior as follows: a very low value of µ(hj,k) is possible when the KL-divergence between the noisy logits and original logits is almost same as the KL-divergence between patched and original logits. Conversely, µ(hj,k) can be very high when the KL-divergence between the patched logit and the original logit is very small. Both of these suggest a strong contribution of the particular head.</p>
<p>Figure 3 shows the accuracy of the model at each different subtask indices with heads pruned at different δ(hj,k) (we show the histogram over µ(hj,k) and start pruning heads from the central bin corresponding to the smallest values of δ(hj,k)). Among the 1,024 heads of Llama-2 7B, we can see that pruning the model to as low as ~400 heads retains &gt;90% of the original accuracy (see Table 1 in Appendix for subtask-wise statistics).</p>
<p>Further patterns in allocating attention heads for different subtasks can be observed in Figure 3. Typically, subtask indices 0 and 5 are most sensitive to pruning attention heads; the accuracy of the model quickly decreases as we knock out attention heads with µ(hj,k) deviated from the mean value. Subtask-0 is a decision-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Importance of inductive reasoning heads across subtasks. We plot the counts of attention heads binned over relative head importance $\left(\mu\left(h_{j, k}\right)\right)$ computed on inductive reasoning task across all subtask indices ( 0 -9, left to right and top to bottom). For a given subtask and a given bin of attention heads (corresponding to a range of $\mu\left(h_{j, k}\right)$ ) knocked out, we show the accuracy of the pruned model model relative to the original model (color coded in blue). A wider spread of dark-colored bins for a subtask signifies that a higher number of attention heads can be knocked out without hurting the model performance; for example, in subtask 7 , the removal of attention heads with $\mu\left(h_{j, k}\right) \in[0.295,0.325]$ retains a similar performance compared to heads removed in subtask 5 with $\mu\left(h_{j, k}\right) \in[0.305,0.315]$, signifying that more attention heads are required to remain active in subtask 5 compared to 7 .
making subtask where the model decides on the reasoning path. One can further demarcate between subtasks 0 vs 2,6 , and 8 , while we initially assumed all of these four to be decision-making stages. Referring to the example provided in Figure 1, in subtask-0, the model has very little information about which ontological statement to choose next, compared to subtask indices 2,6 , and 8 , where the model needs to search for the fact related to the second entity in the last generated fact. Subtask 5 is the actual inductive reasoning step. Compared to subtask 9 , which is also an inductive reasoning step, subtask 5 does not have any prior (the answer generated in subtask 9 is framed as a question in the input).</p>
<h1>5 Token mixing over fictional ontology</h1>
<p>In the earlier section, we observed that the set of heads that we identify as responsible for inductive reasoning performs consistently across all subtasks with the least number of heads remaining active, retaining more than $90 \%$ of the original performance with as low as $40 \%$ of the total number of heads. We continue to investigate the inductive reasoning functionality with this reduced set of heads. Recall that we first need to explore if there is indeed a mixing of information happening across tokens of the form [A] is [B]. Specifically, we pose the following question - given the three pairs of tokens $\left(\left[\mathrm{A}<em 1="1">{1}\right],\left[\mathrm{B}</em>}\right]\right),\left(\left[\mathrm{A<em 2="2">{2}\right],\left[\mathrm{B}</em>}\right]\right)$ and $\left(\left[\mathrm{A<em 3="3">{3}\right],\left[\mathrm{B}</em>}\right]\right)$, such that there exists a positive relation $\left[\mathrm{A<em 1="1">{1}\right]$ is $\left[\mathrm{B}</em>}\right]$, a negative relation $\left[\mathrm{A<em 2="2">{2}\right]$ is not $\left[\mathrm{B}</em>}\right]$, and no relations exist between $\left[\mathrm{A<em 3="3">{3}\right]$ and $\left[\mathrm{B}</em>}\right]$, is it possible to distinguish between $\left[\boldsymbol{x<em 1="1">{j}^{A</em>}}: \boldsymbol{x<em 1="1">{j}^{B</em>}}\right],\left[\boldsymbol{x<em 2="2">{j}^{A</em>}}: \boldsymbol{x<em 2="2">{j}^{B</em>}}\right]$, and $\left[\boldsymbol{x<em 3="3">{j}^{A</em>}}: \boldsymbol{x<em 3="3">{j}^{B</em>\right]$, where $[\cdot: \cdot]$ denotes the concatenation operator, for a given decoder layer $j$ ? Given that these tokens denote entities from a fictional ontology, the model could not possibly have memorized similar representations with the embedding/MLP blocks for the tokens. Instead, it needs to deploy the attention heads to move information from the residual stream of one token.}</p>
<p>We translate the posed question into learning a probing classifier (Belinkov, 2022) $\mathcal{C}: \mathcal{X} \rightarrow \mathcal{Y}$, where for each $\left[\boldsymbol{x}<em i="i">{j}^{A</em>}}: \boldsymbol{x<em i="i">{j}^{B</em>}}\right] \in \mathcal{X}$, the corresponding label $y \in \mathcal{Y}={-1,0,+1}^{|\mathcal{X}|}$ should be $-1,0$, or $+1$, if $\mathrm{A<em _mathrm_i="\mathrm{i">{\mathrm{i}}$ and $\mathrm{B}</em>)$ followed by a softmax; in the nonlinear setup, we utilize stacked linear transformations with ReLU as intermediate non-linearity. We provide the full implementation details of this experiment in Appendix D. Given that the baseline distribution of the residual stream representation might vary across layers, we employ layer-specific classifiers.}}$ are negatively related, unrelated, or positively related, respectively. There can be multiple occurrences of a token in the input context (e.g., there are two occurrences of $[\mathrm{B}]$ in the above example). As a result, there will be multiple residual streams emanating from the same token. To decide which residual streams to pair with (in positive/negative pairing), we rely on immediate occurrence. For example, given the input [A] is [B]. [A] is not [C], we take the first occurrence of $[\mathrm{A}]$ to be positively related to $[\mathrm{B}]$, while the second occurrence of $[\mathrm{A}]$ as negatively related to $[\mathrm{C}]$. We experiment with a linear and a non-linear implementation of $\mathcal{C}$ as feedforward networks. Specifically, in the linear setting, we use a single linear transformation of the form $y=(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{B</p>
<p>Figure 4 shows the 3 -way classification performance in terms of accuracy across different layers using 4-layer ReLU networks. The following conclusions can be drawn from the observations:</p>
<ul>
<li>Non-linear information movement between residual streams. Our experiments with the linear classifier fail to distinguish between ontologically related, unrelated, and negatively related entities.</li>
<li>Distinguishability of the residual stream pairs improves across depth. While the very first layer of attention provides a substantial degree of required token mixing, we observe that the classification performance goes better gradually, pointing towards the existence of multiple successive attention heads that continue moving information from $\mathrm{A}<em _mathrm_j="\mathrm{j">{\mathrm{j}}$ to $\mathrm{B}</em>$ if they are related. However, after a certain depth, the distinguishability starts diminishing, most likely due to accumulation of other information related to the tasks.}</li>
<li>Token mixing is not boosted from contextual prior. Across different numbers of in-context examples provided, we do not observe any significant difference in peak classification performance compared to zero-shot regimes. However, as the number of examples increases, we observe unstable classification performance. It is likely that with more in-context examples, task-specific information is accumulated earlier than the zero-shot regime.</li>
</ul>
<h1>6 Circuitry of step-by-step generation</h1>
<p>In Section 4.1, we observed that the head importance identified via inductive reasoning task could serve as a proxy to identify the important heads across all the subtasks. This provides us with an opportunity to reduce the total number of attention heads to analyze when investigating the step-by-step generation procedure. For each subtask, we use a threshold range for $\mu_{h, k}$ to select a subset of attention heads that are kept intact while the rest of the heads are knocked out (see Appendix C for the subtask-wise threshold range used and the corresponding subtask accuracy). This chosen subset of the model gives us an aggregate accuracy of 0.9 over the inputs for which the full model generates correctly.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: How does the LLM mix information among tokens according to ontology? We plot the performance (in terms of accuracy) of classifying whether two tokens are ontologically related, unrelated, or negatively related using their residual stream representations at different layers. We demonstrate the depthwise accuracy profile using different numbers of few-shot examples ( 0 to 4 ). Typically, mixing information between tokens according to their relation does not require in-context learning. Information mixing between related (or negatively related) token pairs results in better distinguishability: gradually increasing from the starting decoder blocks and achieving a peak between decoder blocks 10-15.</p>
<p>Next, we proceed to look into the exact information that is being read and written by the attention heads from and into the residual streams. Given a head $h_{j, k}$ that writes $\boldsymbol{y}<em j-1="j-1">{j, k}^{i}$ to the residual stream $\boldsymbol{x}</em>}^{i}$, we apply the unembedding projection $\boldsymbol{U}$ on $\boldsymbol{y<em i="i">{j, k}^{i}$ and select the token with the highest probability, $\hat{s}</em>$. The unembedding projection provides the opportunity to look into the token-space representation directly associated with the residual stream and its subspaces (nostalgebraist, 2020). However, it should be noted that applying unembedding projection typically corresponds to Bigram modeling (Elhage et al., 2021). Therefore, when we map any intermediate representation (attention output, residual stream, etc.), we essentially retrieve the token that is most likely to follow.}^{j, k</p>
<h1>6.1 In-context prior vs pretraining prior</h1>
<p>We start by exploring the depth at which the model starts following the context provided as input. Specifically, we check for a given token $s_{i}$ in the sequence $S$, if $\hat{s}<em j_="j," k="k">{i}^{j, k}$, the token projected by the attention head $h</em>}$ is such that $\left\langle s_{i}, \hat{s<em j_="j," k="k">{i}^{j, k}\right\rangle$ is bigram present in $S$. We compute a context-abidance score, $c</em>$ as the fraction of tokens for which the above condition holds true.}$ for the head $h_{j, k</p>
<p>In Figure 5, we plot the context abidance distribution for each head across different subtask indices. We can observe a visible correlation between the depth of the head and how much context abiding it is the LM starts focusing on the contextual information at deeper layers. Given that our experiment setup is based on fictional ontology, $c_{j, k}$ provides a strong demarcation between pretraining prior (i.e., language model statistics memorized from pretraining) and contextual prior (i.e., language model statistics inferred from context) since there are negligible chances of the LM to memorize any language modeling statistics containing the fictional entities from the pretraining data. Therefore, it predicts a correct bigram only when</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: When does the LLM start following the context? Distribution of context-abidance score $c_{j, k}$ of each head (darker blue shade signifies higher $c_{j, k}$ ) with layer $(j)$ on y -axis and head index $(k)$ on xaxis. Attention heads in red denote those with zero context-abidance. We show the distributions for all the subtask indices $0-9$ (left to right, top to bottom). Typically, context abidance is task-agnostic and emerges after the 16 th decoder block.
it is able to follow the context properly. However, there is no visible demarcation across different subtasks; we can claim that this depth-dependence is implicit to the model and does not depend on the prediction task.</p>
<h1>6.2 Subtask-wise answer generation</h1>
<p>We investigate the information propagation pathway through the attention heads that constitute the step-by-step answering of the subtasks. We start with the heads that directly write the answer to the particular subtask into the last residual stream by mapping their output to the token space using $\boldsymbol{U}$. Figure 6 plots these answer-writing heads along with the probability of the answer in the attention head output across different subtasks. The existence of multiple answer-writing heads suggests that LMs exploit multiple pathways to generate the same answer, with each such pathway reinforcing the generated output. Looking at the heads with the highest answer probability, we can observe that some top heads are utilized across different subtasks, e.g., subtasks 1 and 3 , subtasks 2,4 , and 8 , subtasks 5 and 9 , etc.</p>
<p>Sharp change in depth-wise functionality. Interestingly, the 16 -th layer appears as a region of functional transition in the model with all the answer-writing heads appearing after this particular layer. Our earlier</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Which heads are writing the answer? For each subtask (0-9), we show the attention heads that write the answer to the corresponding task into the last residual stream, along with the probability of the answer token in the attention head output.
experiments also associate this layer with some phase shifts; in the token-mixing analysis (Section 5), the distinguishability of ontologically related entities is observed to achieve peaks near this layer and starts falling after that; in Section 6.1 as well, we see a certain overall rise in context-abidance in the heads after the 16 -th layer (see Figure 5). All three findings suggest a functional rift in LLMs that happens to lie almost at the halfway point from embedding to unembedding. The embedding layer associates tokens with information available from pretraining. The initial half assists information movement between residual streams and aligns the representations to the contextual prior. The latter half of the model employs multiple pathways to write the answer to the last residual stream.</p>
<h1>6.3 Parallel pathways of answer processing</h1>
<p>Once we observe the existence of multiple answer-writers within the model, the natural step forward is to wonder if they all process the answer from the input using the same mechanism. We employ a recursive strategy to identify the flow of information through the attention heads (see Appendix G for a detailed description of the procedure). Specifically, we start from the answer-writing heads, follow which residual streams are being attended by these heads, identify the content of these residual streams via unembedding projection, and identify the heads in the previous layers that are writing that content into those residual streams. We continue till one of the two conditions is met: (i) we reach a head in the first decoder block, or (ii) we reach a residual stream corresponding to the first token in the input token sequence. With such a procedure, we construct trees of attention heads rooted at the answer writing heads.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Where do the answer-writing heads collect their answers from? For each answer-writing head and each subtask, we show those attention heads that attend to residual streams corresponding to answer tokens. We demarcate heads that collect the answer tokens from the generated context (green), question context (blue), and few-shot context (red).</p>
<p>Now, let us consider the following example context plus generated token sequence: Lempuses are tumpuses. Tumpuses are rompuses. Rompuses are blue. Max is lempus. True or false: Max is blue. Response: Let us think step by step. Max is lempus. Lempuses are tumpuses. Max is with the desired prediction being tumpus (note that this input follows the few shot examples that are not shown here for brevity). We segregate the input context into three parts: few-shot context, question context, and generated context. We can see that the LLM can collect the answer tumpus from either generated or question context. Also, if the same token exists in the few-shot context, it can decide to collect information from there as well (however, that should not be ideal since the contextual role of that token would be different). We proceed to identify from the information-flow trees all those heads that attend to the streams corresponding to the answer token (in this example, tumpus). Note that there can be multiple other heads that might be attending to these same tokens; but only those heads that are elements of the tree rooted at answer writing heads are actually contributing to the flow of information from the answer tokens in the context of the output Figure 7 demonstrates these attention heads that attend to the residual streams corresponding to the answer tokens for different subtasks, present in the generated context (green), question context (blue) and few-shot context (red). The following observations can be drawn from the figures:</p>
<ol>
<li>Coexitent pathways of answer generation. Smaller models like GPT-2 on simpler tasks like IOI implements a unique neural algorithm (Wang et al., 2023). On the contrary, a larger model like Llama-2 exploits different parallel pathways of answer propagation while performing reasoning There are different heads that are directly connected to the answer writers, and they collect answer information from different places in the context.</li>
<li>Different primary sources of answer for different subtasks. In the initial stages of generation (i.e., subtasks $0,1,2,3$ ), the answer tokens are not present in the generated context. For subtasks 6 and 7, again, the answers need to be collected from the question index. For subtasks 4, 5, 8, and 9, we can observe the presence of heads that collect answer tokens from the context generated via earlier subtasks (in fact, subtask 5 has more such heads).</li>
</ol>
<p>Given the fact that (i) there are multiple answer writing heads, and (ii) there are different heads that are directly connected to the answer writers, and they collect answer information from different places in the context, we can conclude that the different pathways implement different algorithms as well. In the fewshot examples, there are fictional entities that have been used in the question context as well, though in different contextual roles. The fact that there are attention heads collecting those entities as answers suggests that there are pathways prone to collecting similar information from the few-shot context. Although these pathways collect the same token as the answer, they actually deviate from the correct reasoning algorithm. However, the presence of such pathways decreases as the generation progresses from subtask 0 to 9 . Note that this is different from few-shot examples providing the necessary pattern via in-context learning. Such patterns are more position-specific, i.e., while generating the answer for a certain subtask in the question, there is an overall increased attention provided to the same subtask tokens in the few-shot examples (see Figures 9, 10, 11, and 12 in Appendix). Finally, the pattern of answer collection from question context and generated context across different subtasks clearly demarcates the inductive reasoning subtasks (5 and 9) from the decision-making and copying. Tasks in the former category require pathways that collect answers from the earlier generated context, but the latter does not. Moreover, the decision-making step right before inductive reasoning can extract answers from the preceding step. This explains an albeit small number of heads that collect the answer from the generated context in subtasks 4 and 8. See Figures 13 and 14 for examples of such information flow towards subtasks 1 and 5 , respectively.</p>
<h1>7 Conclusion</h1>
<p>Our findings. This work sought to disentangle the functional fabric of CoT reasoning in LLMs. Specifically, we explored Llama-2 7B in a few-shot CoT regime for solving multi-step reasoning problems on fictional ontologies of the PrOntoQA dataset. We observed that:</p>
<ol>
<li>Despite different reasoning requirements across different stages of CoT generation, the functional components of the model remain almost the same. Different neural algorithms are implemented as compositions of induction circuit-like mechanisms.</li>
<li>Attention heads perform information movement between ontologically related (or negatively related) tokens. This information movement results in distinctly identifiable representations for such token pairs. Typically, this distinctive information movement starts from the very first layer and continues till the middle. While this phenomenon happens zero-shot, in-context examples exert pressure to quickly mix other task-specific information among tokens.</li>
<li>Multiple different neural pathways are deployed to compute the answer, that too in parallel. Different attention heads, albeit with different probabilistic certainty, write the answer token (for each CoT subtask) to the last residual stream.</li>
<li>
<p>These parallel answer generation pathways collect answers from different segments of the input. We found that while generating CoT , the model gathers answer tokens from the generated context, the question context, as well as the few-shot context. This provides a strong empirical answer to the open problem of whether LLMs actually use the context generated via CoT while answering questions.</p>
</li>
<li>
<p>We observe a functional rift at the very middle of the LLM (16th decoder block in case of Llama-2 7B), which marks a phase shift in the content of residual streams and the functionality of the attention heads. Prior to this rift, the model primarily assigns bigram associations memorized via pretraining; it drastically starts following the in-context prior to and after the rift. It is likely that this is directly related to the token-mixing along ontological relatedness that happens only prior to the rift. Similarly, answer-writing heads appear only after the rift. Attention heads that (wrongly) collect the answer token from the few-shot examples are also bounded by the prior half of the model.</p>
</li>
</ol>
<p>Implications for future research. These findings bear important ramifications towards the ongoing research around language modeling and interpretability. A natural extension to this work would be to incorporate pretraining memorization in terms of MLP blocks - precisely, whether the functional rift across layers bears a similar mechanism when the LLM starts mixing factual associations that have been stored within the MLP neurons. The existence of a parallel answer-generation process is extremely important for causal interventions on model behavior (Li et al., 2024): changing how a model should reason via up-(or down-) scaling certain neural pathways should take all the parallel pathways into account.
Limitations. The design of this study imposes certain limitations in its scope. First and foremost, we analyze a very specific type of reasoning problem. While the fictional ontology with a restricted CoT template provides ease of analysis, free-form reasoning can introduce further complex dynamics not captured in this study. We also could not address the role of MLPs in reasoning. While existing literature points to their role as factual memory, it should be noted that the model also memorizes a diverse set of token-token associations pertaining to the structure of language and not just factual associations. Such associations are likely to play a decisive role in a grounded reasoning setup that our analysis ignores. Finally, we use a few-shot prompting regime to ensure that the model follows a specific structure of reasoning steps. With zero-shot CoT, more complex mechanisms are bound to emerge.</p>
<h1>References</h1>
<p>Yonatan Belinkov. Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics, 48(1):207-219, 04 2022. ISSN 0891-2017. doi: 10.1162/coli_a_00422. URL https://doi.org/10.1162/ coli_a_00422.</p>
<p>Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, and Huajun Chen. When do program-of-thought works for reasoning?, 2024.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=YfZ4ZPt8zd.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021.</p>
<p>Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022.</p>
<p>Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. arXiv preprint arXiv:2305.15408, 2023.</p>
<p>Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5484-5495. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main. 446.</p>
<p>Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, and Christopher Olah. Superposition, memorization, and double descent. Transformer Circuits Thread, 2023.</p>
<p>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 22199-22213. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf.</p>
<p>Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 537-563. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-emnlp.38.</p>
<p>Maximilian Li, Xander Davies, and Max Nadeau. Circuit breaking: Removing model behaviors with targeted ablation, 2024.</p>
<p>Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.</p>
<p>Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. The hydra effect: Emergent self-repair in language model computations. arXiv preprint arXiv:2307.15771, 2023.</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 17359-17372. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf.</p>
<p>Neel Nanda. A comprehensive mechanistic interpretability explainer \&amp; glossary, 2022.
nostalgebraist. interpreting GPT: the logit lens — LessWrong — lesswrong.com. https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens, 2020. [Accessed 09-02-2024].</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.</p>
<p>OpenAI. Gpt-4 technical report, 2024.
Ben Prystawski, Michael Li, and Noah Goodman. Why think step by step? reasoning emerges from the locality of experience. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=qFVVBzXxR2V.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023.</p>
<p>Juanhe TJ Tan. Causal abstraction for chain-of-thought reasoning in arithmetic word problems. In Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. $155-168,2023$.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan</p>
<p>Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.</p>
<p>Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=NpsVSN6o4ul.</p>
<p>Xinyi Wang and William Yang Wang. Reasoning ability emerges in large language models as aggregation of reasoning paths: A case study with knowledge graphs. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856. URL https://openreview.net/forum?id=yzkSU5zdwD. Survey Certification.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 24824-24837. Curran Associates, Inc., 2022b. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.</p>
<p>Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D Goodman. Interpretability at scale: Identifying causal mechanisms in alpaca. arXiv preprint arXiv:2305.08809, 2023.</p>
<p>Fred Zhang and Neel Nanda. Towards best practices of activation patching in language models: Metrics and methods. arXiv preprint arXiv:2309.16042, 2023.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: An example of fictional ontology in the PrOntoQA dataset.</p>
<h1>A PrOntoQA question example</h1>
<p>Following are two QA examples from fictional and false ontologies, respectively:</p>
<h2>Fictional ontology example:</h2>
<p>Context: Tumpus is bright. Lempus is tumpus. Max is lempus.
Query: True or false: Max is bright.
Answer: Max is lempus. Lempus is tumpus. Max is tumpus. Tumpus is bright. Max is bright. True</p>
<h2>False onotology example:</h2>
<p>Context: Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate.
Query: True or false: Rex is spicy.
Answer: Rex is vertebrate. Vertebrates are carnivores. Rex is carnivore. Carnivore is spicy. Rex is spicy. True</p>
<p>Figure 8 demonstrates an example ontology tree along with distractors. Here, zumpus $\rightarrow$ shumpus translates to zumpuses are shumpuses. From this example, one can construct the following multistep reasoning problem: Wumpuses are shumpuses. Shumpuses are impuses. Max is a wumpus. Wumpuses are not red. Impuses are happy. Jompuses are lorpuses. True or False: Max is happy. In this example, the statement Jompuses are lorpuses serves as a distractor as neither of the entities are part of the ontology relevant to the question.</p>
<h2>B Knockout</h2>
<p>To perform knockout average activation from false ontology the PrOntoQA dataset is used. For each subtask, false ontology activation is stored and mean activation knockout is performed. We provide step-by-step examples of subtask-wise generation over false ontology as follows:</p>
<p>Context: Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate.
Query: True or false: Rex is spicy.
Answer: "Rex is vertebrate. Vertebrates are carnivores. Rex is carnivore. Carnivore is spicy. Rex is spicy. True",
"prompt_0": "Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step.", "prompt_1": "Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is",
"prompt_2": "Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is vertebrate.",
"prompt_3": "Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy.</p>
<p>Let us think step by step. Rex is vertebrate. Vertebrates are",
"prompt_4": 'Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is vertebrate. Vertebrates are carnivores.",
"prompt_5": 'Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is vertebrate. Vertebrates are carnivores. Rex is",
"prompt_6": 'Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is vertebrate. Vertebrates are carnivores. Rex is carnivore.",
"prompt_7": 'Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is vertebrate. Vertebrates are carnivores. Rex is carnivore. Carnivore is",
"prompt_8": 'Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is vertebrate. Vertebrates are carnivores. Rex is carnivore. Carnivore is", "prompt_8": 'Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is vertebrate. Vertebrates are carnivores. Rex is carnivore. Carnivore is spicy.',
"prompt_9": 'Carnivore is spicy. Vertebrates are carnivores. Rex is vertebrate. True or false: Rex is spicy. Let us think step by step. Rex is vertebrate. Vertebrates are carnivores. Rex is carnivore. Carnivore is spicy. Rex is spicy.,</p>
<h1>C Task specific head identification</h1>
<h2>C. 1 Decision-making heads</h2>
<p>To identify the heads that actively participate in decision-making subtasks, we incorporate activation patching on individual heads over decision-making subtasks.</p>
<p>Let $S_{\text {Decision }}$ denote the input token sequence for a particular decision-making subtask with $s_{\text {ans }}$ being the token corresponding to the correct answer. Also, let $P_{\text {org }}$ denote the output token probability distribution of the full model corresponding to $S_{\text {Decision }}$ as input, i.e.,</p>
<p>$$
P_{\text {org }}=\operatorname{SoftMax}\left(\operatorname{LM}\left(\boldsymbol{x}<em _Decision="{Decision" _text="\text">{\text {logit }} \mid S</em>\right)\right)
$$}</p>
<p>Here we omit the token position superscript over $\boldsymbol{x}<em j_="j," k="k">{\text {logit }}$ for brevity since we are taking the output corresponding to the last input token. We store the activations $\boldsymbol{y}</em>$.
Next, we corrupt the input corresponding to the last token in $S_{\text {Decision }}$, which is equivalent to knocking off the set of all heads, $\mathcal{H}_{\text {full }}$. We record the corresponding output token probability as,}$ corresponding to each head $h_{j, k</p>
<p>$$
P_{\text {corrupt }}=\operatorname{SoftMax}\left(\operatorname{LM}<em _full="{full" _text="\text">{\mathcal{H}</em>}}}\left(\boldsymbol{x<em _Decision="{Decision" _text="\text">{\text {logit }} \mid S</em>\right)\right)
$$}</p>
<p>Finally, for each head $h_{j, k}$, we restore its corresponding original output $\boldsymbol{y}_{j, k}$ and record the output token probability distribution as</p>
<p>$$
P_{\text {patched }}^{j, k}=\operatorname{SoftMax}\left(\operatorname{LM}<em _full="{full" _text="\text">{\mathcal{H}</em>}} \backslash\left{h_{j, k}\right}}\left(\boldsymbol{x<em _Decision="{Decision" _text="\text">{\text {logit }} \mid S</em>\right)\right)
$$}</p>
<p>Then, we compute the importance score of head $h_{j, k}$ as</p>
<p>$$
\mu_{\text {Decision }}\left(h_{j, k}\right)=\frac{P_{\text {org }}\left(x=s_{\text {ans }}\right)-P_{\text {corrupt }}\left(x=s_{\text {ans }}\right)}{P_{\text {org }}\left(x=s_{\text {ans }}\right)-P_{\text {patched }}\left(x=s_{\text {ans }}\right)}
$$</p>
<h2>C. 2 Copy heads</h2>
<p>Copying subtasks follows decision-making subtasks immediately. Once the head entity of a statement is decided, copying requires moving the tail entity corresponding to that statement in the input context to the output (e.g., if there is a statement Rompus is grimpus in the input context and decision-making heads have decided to output Rompus, then copy subtask requires moving grimpus to the output). For this, we simply look into the attention probability assigned by each head $h_{j, k}$ to the token to be copied as source (i.e., key),</p>
<p>denoted as</p>
<p>$$
\mu_{\text {Copy }}\left(h_{j, k}\right)=\frac{\exp \left(\left(\boldsymbol{W}<em j="j">{Q}^{j, k} \boldsymbol{x}</em>}^{\text {end }}\right)^{\top}\left(\boldsymbol{W<em j="j">{K}^{j, k} \boldsymbol{x}</em>}^{\text {ans }}\right)\right)}{\sum_{i} \exp \left(\left(\boldsymbol{W<em j="j">{Q}^{j, k} \boldsymbol{x}</em>}^{\text {end }}\right)^{\top}\left(\boldsymbol{W<em j="j">{K}^{j, k} \boldsymbol{x}</em>
$$}^{i}\right)\right)</p>
<p>where $\boldsymbol{x}<em j="j">{j}^{\text {end }}, \boldsymbol{x}</em>$ denote the residual streams corresponding to the last token and the answer token, respectively, at $j$-th decoder block.}^{\text {ans }</p>
<h1>C. 3 Inductive reasoning heads</h1>
<p>Let the input token sequence for the inductive reasoning subtask be denoted as $S_{\text {ind }}$, which is of the form [A] is [B]. [B] is [C]. [A] is, and let $s_{\text {ans }}$ denote the answer token (which is [C] in this example). We represent the first and second occurrences of each token [A] and [B] as $A_{1}, A_{2}$ and $B_{1}, B_{2}$, respectively. For a given $S_{\text {Induction }}$, we can perform activation patching on three different residual streams, $\boldsymbol{x}<em 1="1">{j}^{B</em>}}, \boldsymbol{x<em 2="2">{j}^{B</em>$ to identify the responsible heads. Let the original token probability distribution be}}$, and $\boldsymbol{x}_{j}^{C</p>
<p>$$
P_{\text {org }}=\operatorname{SoftMax}\left(\operatorname{LM}\left(\boldsymbol{x}<em _Induction="{Induction" _text="\text">{\text {logit }} \mid S</em>\right)\right)
$$}</p>
<p>Next, for each $l \in\left{B_{1}, B_{2}, C\right}$, we compute the corrupted forward pass as follows:</p>
<p>$$
P_{\text {corrupt }, l}=\operatorname{SoftMax}\left(\operatorname{LM}<em _full="{full" _text="\text">{\mathcal{H}</em>}}}^{l}\left(\boldsymbol{x<em _Induction="{Induction" _text="\text">{\text {logit }} \mid S</em>\right)\right)
$$}</p>
<p>followed by patching original activation to the head $h_{j, k}$ as</p>
<p>$$
P_{\text {patched }, l}^{j, k}=\operatorname{SoftMax}\left(\operatorname{LM}<em _full="{full" _text="\text">{\mathcal{H}</em>} \backslash\left{h_{j, k}\right}}}^{l}\left(\boldsymbol{x<em _Induction="{Induction" _text="\text">{\text {logit }} \mid S</em>\right)\right)
$$}</p>
<p>Then, the inductive reasoning head importance is computed as:</p>
<p>$$
\mu_{\text {Induction }}\left(h_{j, k}\right)=\operatorname{Mean}<em _ref="{ref" _text="\text">{l} \frac{D</em>
$$}}^{l}-D_{\text {patched }}^{l}}{D_{\text {ref }}^{l}</p>
<p>where $D_{\text {ref }}^{l}$ is the KL divergence between $P_{\text {org }}$ and $P_{\text {corrupt }, l}$, and $D_{\text {patched }}^{l}$ is the KL divergence between $P_{\text {org }}$ and $P_{\text {corrupt }, l}$.</p>
<h2>C. 4 Performance of inductive reasoning heads for each subtask</h2>
<p>For each subtask 0-9, we group the attention heads based on their respective $\mu_{\text {Induction }}\left(h_{j, k}\right)$. We knockout heads within a certain threshold head importance range $\left(\mu_{\min }, \mu_{\max }\right)$ by knockout and record the accuracy. In Table 1, we report the attention head statistics that we used in the analysis for context abidance and answer-writing pathways. Note that we sought to keep the relative accuracy (i.e., the fraction of correct prediction by the ablated model over the correct predictions of the full model) close to 0.9 .</p>
<h2>D Probing for token mixing</h2>
<p>Towards probing ontological relatedness in residual streams (see Section 5), first, we need to extract the positive, negative and unrelated entities from the sentence. PrOntoQA provides us with a tree structure for each sentence, and with the help of the structure, we can extract the pairs. Consider the following example.</p>
<p>Sentence: Each shumpus is a zumpus. Shumpuses are wumpuses. Every shumpus is hot. Impuses are shumpuses. Each impus is a brimpus. Each impus is happy. Wumpuses are not red. Brimpuses are not bitter. Lorpuses are jompuses. Each yumpus is not hot. Lorpuses are not small. Max is an impus. Max is a lorpus.</p>
<p>Here all immediate entities will be termed as posive pairs, such as shumpus $&lt;&gt;$ impus, zumpus $&lt;&gt;$ shumpus, impus $&lt;&gt;$ happy, etc. Similarly, following are a few examples of negatively related entities: wumpus $&lt;&gt;$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subtask index</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">Heads removed</th>
<th style="text-align: left;">Threshold range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">475</td>
<td style="text-align: left;">$0.30487806-0.31463414$</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">0.93</td>
<td style="text-align: left;">554</td>
<td style="text-align: left;">$0.30243903,0.31707317$</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0.96</td>
<td style="text-align: left;">617</td>
<td style="text-align: left;">$0.3-0.3195122$</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">617</td>
<td style="text-align: left;">$0.3-0.3195122$</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.99</td>
<td style="text-align: left;">554</td>
<td style="text-align: left;">$0.30243903-0.31707317$</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">0.93</td>
<td style="text-align: left;">475</td>
<td style="text-align: left;">$0.30487806-0.31463414$</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">475</td>
<td style="text-align: left;">$0.30487806-0.31463414$</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">617</td>
<td style="text-align: left;">$0.3-0.3195122$</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">617</td>
<td style="text-align: left;">$0.3-0.3195122$</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">0.91</td>
<td style="text-align: left;">663</td>
<td style="text-align: left;">$0.297561-0.3219512$</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of subtask-wise attention head removal according to their importance in inductive reasoning.
red, brimpus $&lt;&gt;$ bitter, etc. The distractor entities (jompus, lorpus and small) serves as seed for unrelated entities; any two entities that belong two disjoint ontologies are taken as unrelated.</p>
<p>During training and testing, we also make sure that there are no overlap between entities. During training, we use entities such as "wumpus", "yumpus", "zumpus", "dumpus" and during testing : "zonkify", "quiblitz","flimjam", "zizzlewump", "snickerblat". Following are the implementation details for the probing classifier:</p>
<ul>
<li>Total training pairs: 28392; Total testing pairs: 9204. All three types of pairs (positively and negatively related and unrelated) are present in equal proportion in the training and testing data.</li>
<li>4-layer MLP model, $4096^{*} 2-&gt;128-&gt;64-&gt;32-&gt;3$. With ReLU in between each Linear layer.</li>
<li>Learning rate: 0.00005</li>
<li>Number of epochs: 120 .</li>
</ul>
<h1>E Prompts used and model performance</h1>
<p>We use 6 -shot examples of CoT for generation in all the experiments.:
### Input:
Gorpus is twimpus. Alex is rompus. Rompus is gorpus. Gorpus is small. Rompus is mean. True or false: Alex is small. Let us think step by step.
### Response:
Alex is rompus. Rompus is gorpus. Alex is gorpus. Gorpus is small. Alex is small. True
### Input:
Gorpuses are discordant. Max is zumpus. Zumpus is shampor. Zumpus is gorpus. Gorpus is earthy. True or false: Max is small. Let us think step by step.
### Response:
Max is zumpus. Zumpus is gorpus. Max is gorpus. Gorpuses are discordant. Max is discordant. False
### Input:
Borpin are wumpus. Wumpuses are angry. Wumpus is jempor. Sally is lempus. Lempus is wumpus. True or false: Sally is floral. Let us think step by step.
### Response:
Sally is lempus. Lempus is wumpus. Sally is wumpus. Wumpuses are angry. Sally is angry. False
### Input:
Gorpus is jelgit. Yumpuses are loud. Gorpus is yumpus. Yumpus is orange. Rex is gorpus. True or false:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/joykirat18/How-To-Think-Step-by-Step&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
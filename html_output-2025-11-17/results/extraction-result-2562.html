<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2562 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2562</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2562</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-53132ea6c107479d4557631299d3ed525109b464</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/53132ea6c107479d4557631299d3ed525109b464" target="_blank">AFlow: Automating Agentic Workflow Generation</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> AFlow is introduced, an automated framework that efficiently explores the generation and optimization of large language models using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code is available at https://github.com/FoundationAgents/AFlow.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2562.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2562.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AFLOW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AFLOW: Automating Agentic Workflow Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An MCTS-based framework that automatically searches a code-represented workflow space of LLM-invoking nodes and predefined operators (e.g., Ensemble, Review & Revise) to discover optimized agentic workflows via an LLM optimizer, execution feedback, and tree-structured experience backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AFLOW</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AFLOW models workflows as code-defined graphs of named nodes (each an LLM invocation with parameters: model, prompt, temperature, output format) and edges (code) that implement programmatic control flow; it augments this representation with reusable Operators (Generate, Format, Review, Revise, Ensemble, Test, Programmer). AFLOW treats workflow discovery as a search problem over this space and uses a variant of Monte Carlo Tree Search (MCTS) where each MCTS tree node represents a full workflow. An LLM (Claude-3.5-sonnet in experiments) acts as an optimizer to expand/modify selected workflows; executors (e.g., GPT-4o-mini, DeepSeek-V2.5) run generated workflows to obtain execution feedback; experience (modifications, scores, logs) is stored in the tree and backpropagated to guide future selections.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (multiple LLM-invoking nodes per workflow + 1 optimizer LLM during expansion)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Optimizer agent: an LLM used to generate/modify workflow code and prompts (Claude-3.5-sonnet in experiments); Executor agents / Node agents: LLM invocations that perform operations (Generate, CodeGenerate, Format, Review, Revise, Ensemble, Test, Programmer) using specified prompts and output formats (e.g., json/xml/markdown/raw); CostManager agent: bookkeeping in llm instance to track cost; Operators act as standardized specialized roles (e.g., 'Programmer' executes and verifies code, 'Test' runs public tests, 'Ensemble' aggregates multiple solutions, 'Review/Revise' provide critique and fixes).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Idea/approach generation (Generate/Custom nodes), implementation/code generation (CodeGenerate, Programmer), execution/testing (Test operator, program execution), evaluation (execution on validation set, evaluator function G), iterative refinement (Review & Revise, backpropagation of experience), selection of final workflow (MCTS selection).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized MCTS-based controller: selection chooses parent workflow (soft mixed-probability combining uniform and score-based distributions), LLM-optimizer expands/creates child workflows (code modifications/operators), executor runs child workflows on validation data and returns scores, experience is backpropagated in the MCTS tree to update selection probabilities. The tree preserves past workflow experiences and enables reuse/avoidance of past modifications. Special selection allows occasional generation from blank template to prevent local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Prompt-and-code based message passing: agents exchange information via code-represented workflows and natural-language prompts (prompt_custom entries), structured output formats (json, xml, markdown, raw) and explicit logs of predictions/expected outputs; optimizer receives a context that includes parent workflow code, modification history, and execution logs; optimizer outputs XML-tagged single modifications and full code fragments for workflow nodes; operators encapsulate standardized prompt templates for each operation.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Execution evaluation on a selected validation set: each generated workflow is executed 5 times on the validation set to produce mean and std scores; results (score, cost), optimizer modifications, and per-instance logs (predictions/errors) are recorded as experience; experience (including whether a modification helped) is backpropagated in the MCTS tree and used as context for future optimizer prompts; Test operator provides direct execution feedback and triggers reflection/fix cycles when tests fail.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand at MCTS iteration granularity: selection -> LLM-based expansion -> execution (5 runs per candidate) -> backpropagation each iteration; also operators (e.g., Ensemble, Review) create intra-workflow interactions between node invocations during workflow execution (i.e., nodes communicate during a single workflow run).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General reasoning tasks with numeric evaluation functions (benchmarked on code generation, mathematics, and question answering datasets: HumanEval, MBPP, MATH, GSM8K, HotpotQA, DROP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics per dataset (AFLOW workflows executed with GPT-4o-mini unless otherwise noted): HotpotQA F1=73.5, DROP F1=80.6, HumanEval pass@1=94.7, MBPP pass@1=83.4, GSM8K solve rate=93.5, MATH solve rate=56.2; average across six datasets = 80.3%. Compared to baselines, AFLOW gives +5.7 percentage points average over manually designed methods and +19.5 percentage points over existing automated workflow optimization (ADAS). Also reported: AFLOW enables smaller models to outperform GPT-4o on specific tasks at 4.55% of GPT-4o's inference dollar cost (claim in abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against manual workflows (IO, Chain-of-Thought, Self-Consistency CoT, MultiPersona Debate, Self-Refine, MedPrompt) and automated optimizer ADAS (used as a baseline); AFLOW outperforms all listed manual methods (avg. 5.7% improvement) and ADAS (avg. +19.5%). ADAS average performance reported as 67.2% vs AFLOW 80.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Preserving tree-structured experiences improves search efficiency and helps avoid information loss inherent in linear accumulation; LLM-driven expansion plus execution feedback yields higher final task accuracy across benchmarks; operator abstractions accelerate finding effective substructures (operators improved search efficiency in ablation); cost-performance benefit: AFLOW-produced workflows enable weaker/cheaper executors to reach or exceed performance of stronger models on the cost-performance Pareto front (quantified by example: matching/outperforming GPT-4o at far lower cost). Quantitative improvements: +5.7% over human workflows on average, +19.5% over ADAS, and specific per-benchmark gains listed above.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Computational cost: evaluating each generated workflow 5 times increases per-iteration expense; potential for local optima (mitigated via blank-template selection and mixed selection probabilities); inconsistencies/information loss in prior linear-history designs (not AFLOW) hinder guidance to optimizers; operator design requires human effort (though optional) to improve efficiency; some optimizer hyperparameter values reported inconsistently in paper (selection lambda/alpha values appear in two places with different numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation on operators (GSM8K) shows that predefined Operators improve search efficiency and final performance; even without operators AFLOW remains strong (reported: AFLOW without operators achieved 93.1% on GSM8K validation/test setting), and AFLOW can autonomously create ensemble-like structures without predefined operators. Details and curves provided in Appendix B / Figure 5.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper recommends using Operators to improve search efficiency; MCTS variant with soft mixed-probability selection (balances uniform and score-based selection) and LLM-based expansion; practical hyperparameters used in experiments: iteration rounds = 20 (AFLOW), selection uses top-k sampling (k=3 in algorithm), early stopping when top-k unchanged for n=5 rounds; execute each candidate 5 times on validation to compute mean/std. (Note: the paper lists specific selection hyperparameters in-text as lambda=0.2 and alpha=0.4 but presents lambda=0.4 and alpha=0.2 in Appendix; overall approach uses a mixed uniform/score selection strategy.)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AFlow: Automating Agentic Workflow Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2562.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2562.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADAS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADAS: Automated Design of Agentic Systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated workflow optimization method that represents workflows as code and stores historical workflows in a linear list; it uses a linear heuristic search algorithm and served as a baseline compared experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated design of agentic systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ADAS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ADAS represents workflows in code and stores historical workflows in a linear list structure; it performs automated prompt/workflow optimization via a linear heuristic search algorithm and uses an LLM optimizer and LLM executor in experiments (Claude-3.5-sonnet as optimizer and GPT-4o-mini as executor in ADAS baseline runs). The paper characterizes ADAS as having a relatively complete workflow representation but limited by the efficiency of its linear search process.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified / treated as workflow nodes (variable)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not explicitly detailed in this paper beyond general LLM-invoking nodes; ADAS uses an LLM optimizer and executor in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Workflow generation/optimization, evaluation (executing workflows to compute scores).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Linear heuristic search over code-represented workflows with a linear history list of past workflows (i.e., a flat accumulation of experience rather than a tree-structured experience store).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Code-represented workflows and prompts; details limited in this paper (cited as code-based representation).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Uses past workflows and their outcomes recorded in a linear list to prompt the optimizer for additional modifications; characterized as less efficient due to information accumulation and linear heuristic search.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Iterative linear search rounds (in experiments ADAS used 30 iteration rounds), specifics not deeply described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Same set of reasoning/code/math/QA benchmarks used as baseline comparisons (HumanEval, MBPP, MATH, GSM8K, HotpotQA, DROP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1 (executed with GPT-4o-mini): HotpotQA 64.5, DROP 76.6, HumanEval 82.4, MBPP 53.4, GSM8K 90.8, MATH 35.4; average = 67.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Included as an automated workflow-optimization baseline; AFLOW outperforms ADAS across datasets by a large margin (AFLOW avg 80.3% vs ADAS 67.2%, described in paper as +19.5% over existing automated approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Provides a relatively complete code-based workflow representation capable of expressing linear sequences, conditionals, and loops (claimed strength in representation).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Efficiency limitations: linear heuristic search and linear accumulation of experience reduce search efficiency and make discovery of effective workflows difficult within limited iterations; prone to information loss as input tokens increase during prompting of LLM optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not reported within this paper for ADAS beyond its comparison as a baseline; ADAS was compared with AFLOW experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper critiques ADAS for relying on linear search and linear experience storage; suggests tree-structured experience (as in AFLOW) and MCTS-based selection as preferable for better efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AFlow: Automating Agentic Workflow Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2562.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2562.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTSwarm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTSwarm: Language agents as optimizable graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior multi-agent/workflow approach that represents agentic workflows as graphs and uses reinforcement learning to optimize agent interactions, mentioned as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gptswarm: Language agents as optimizable graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPTSwarm</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in the paper as a graph-based representation of language-agent collaborations that uses reinforcement learning to optimize interactions; cited as related work with limitations in representing conditional states.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (graph nodes represent agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not detailed in this paper beyond graph nodes as LLM-invoking agents; likely includes generation and verification roles in the original work (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>General agentic workflow tasks (generation, coordination) as reported in related work context.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Graph-structured agent topology optimized with reinforcement learning (as described by the cited work); the authors note GPTSwarm 'struggles to represent workflows with conditional states due to graph structure limitations.'</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in this paper; implied graph message passing between agent nodes (original paper likely details this).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Reinforcement learning optimization over graph interactions (as summarized by the authors), specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General agentic workflows / multi-agent LLM collaboration (mentioned as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No performance numbers provided in this paper (only a limitation note regarding representation of conditionals).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as related prior work and contrasted with AFLOW's code-based edge representation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Graph representation can capture complex topologies and interactions (implied); RL-based optimization provides a mechanism to adapt topology/behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Paper states GPTSwarm struggles to represent conditional states naturally due to graph structure limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None reported in this paper for GPTSwarm (only referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AFlow: Automating Agentic Workflow Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2562.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2562.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metagpt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Metagpt: Meta programming for A multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent collaborative framework referenced as prior work for multi-agent/team LLM systems that program and coordinate multiple agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metagpt: Meta programming for A multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Metagpt</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as a prior multi-agent collaborative framework; cited in related work as an example of multi-agent approaches that differ from static agentic workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified here; original work described multiple cooperating agents for meta-programming tasks (this paper does not provide details).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not specified in this paper; cited as part of general agentic/workflow literature.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not described in this paper; referenced as a distinct paradigm from static agentic workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent collaborative programming and workflow tasks (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned in related work; not used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AFlow: Automating Agentic Workflow Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2562.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2562.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic LLM-Agent Network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic LLM-Agent Network: An LLM-agent collaboration framework with agent team optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced framework for LLM-agent collaboration that explores agent-team optimization; included in related work demonstrating alternative representations for agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic llm-agent network: An llmagent collaboration framework with agent team optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Dynamic LLM-Agent Network</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a framework that treats agent interactions as a network and aims to optimize the agent team; mentioned in related work contrasting AFLOW's code representation with other structure choices.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (network nodes representing agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>General agent collaboration tasks (not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Network/graph-based agent topology with agent-team optimization (details are in the cited work; this paper only references the approach).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-agent LLM collaboration (as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Referenced as related work; not used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AFlow: Automating Agentic Workflow Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2562.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2562.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mindstorms</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mindstorms in Natural Language-based Societies of Mind</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior conceptual/multi-agent work referenced as related literature about societies of language-based agents (agents cooperating in a 'society of mind').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mindstorms in natural language-based societies of mind</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mindstorms (societies of mind)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as prior work about natural-language-based societies of agents and as part of the literature on agentic/agent-team systems; mentioned to place AFLOW in the broader context of multi-agent language-model systems.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>general multi-agent reasoning/coordination (cited conceptually)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>conceptual/multi-agent reasoning frameworks (cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>cited in related work context; not experimentally compared</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AFlow: Automating Agentic Workflow Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gptswarm: Language agents as optimizable graphs <em>(Rating: 2)</em></li>
                <li>Metagpt: Meta programming for A multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>Dynamic llm-agent network: An llmagent collaboration framework with agent team optimization <em>(Rating: 2)</em></li>
                <li>Mindstorms in natural language-based societies of mind <em>(Rating: 2)</em></li>
                <li>Automated design of agentic systems <em>(Rating: 2)</em></li>
                <li>G-designer: Architecting multi-agent communication topologies via graph neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2562",
    "paper_id": "paper-53132ea6c107479d4557631299d3ed525109b464",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "AFLOW",
            "name_full": "AFLOW: Automating Agentic Workflow Generation",
            "brief_description": "An MCTS-based framework that automatically searches a code-represented workflow space of LLM-invoking nodes and predefined operators (e.g., Ensemble, Review & Revise) to discover optimized agentic workflows via an LLM optimizer, execution feedback, and tree-structured experience backpropagation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AFLOW",
            "system_description": "AFLOW models workflows as code-defined graphs of named nodes (each an LLM invocation with parameters: model, prompt, temperature, output format) and edges (code) that implement programmatic control flow; it augments this representation with reusable Operators (Generate, Format, Review, Revise, Ensemble, Test, Programmer). AFLOW treats workflow discovery as a search problem over this space and uses a variant of Monte Carlo Tree Search (MCTS) where each MCTS tree node represents a full workflow. An LLM (Claude-3.5-sonnet in experiments) acts as an optimizer to expand/modify selected workflows; executors (e.g., GPT-4o-mini, DeepSeek-V2.5) run generated workflows to obtain execution feedback; experience (modifications, scores, logs) is stored in the tree and backpropagated to guide future selections.",
            "number_of_agents": "variable (multiple LLM-invoking nodes per workflow + 1 optimizer LLM during expansion)",
            "agent_specializations": "Optimizer agent: an LLM used to generate/modify workflow code and prompts (Claude-3.5-sonnet in experiments); Executor agents / Node agents: LLM invocations that perform operations (Generate, CodeGenerate, Format, Review, Revise, Ensemble, Test, Programmer) using specified prompts and output formats (e.g., json/xml/markdown/raw); CostManager agent: bookkeeping in llm instance to track cost; Operators act as standardized specialized roles (e.g., 'Programmer' executes and verifies code, 'Test' runs public tests, 'Ensemble' aggregates multiple solutions, 'Review/Revise' provide critique and fixes).",
            "research_phases_covered": "Idea/approach generation (Generate/Custom nodes), implementation/code generation (CodeGenerate, Programmer), execution/testing (Test operator, program execution), evaluation (execution on validation set, evaluator function G), iterative refinement (Review & Revise, backpropagation of experience), selection of final workflow (MCTS selection).",
            "coordination_mechanism": "Centralized MCTS-based controller: selection chooses parent workflow (soft mixed-probability combining uniform and score-based distributions), LLM-optimizer expands/creates child workflows (code modifications/operators), executor runs child workflows on validation data and returns scores, experience is backpropagated in the MCTS tree to update selection probabilities. The tree preserves past workflow experiences and enables reuse/avoidance of past modifications. Special selection allows occasional generation from blank template to prevent local optima.",
            "communication_protocol": "Prompt-and-code based message passing: agents exchange information via code-represented workflows and natural-language prompts (prompt_custom entries), structured output formats (json, xml, markdown, raw) and explicit logs of predictions/expected outputs; optimizer receives a context that includes parent workflow code, modification history, and execution logs; optimizer outputs XML-tagged single modifications and full code fragments for workflow nodes; operators encapsulate standardized prompt templates for each operation.",
            "feedback_mechanism": "Execution evaluation on a selected validation set: each generated workflow is executed 5 times on the validation set to produce mean and std scores; results (score, cost), optimizer modifications, and per-instance logs (predictions/errors) are recorded as experience; experience (including whether a modification helped) is backpropagated in the MCTS tree and used as context for future optimizer prompts; Test operator provides direct execution feedback and triggers reflection/fix cycles when tests fail.",
            "communication_frequency": "On-demand at MCTS iteration granularity: selection -&gt; LLM-based expansion -&gt; execution (5 runs per candidate) -&gt; backpropagation each iteration; also operators (e.g., Ensemble, Review) create intra-workflow interactions between node invocations during workflow execution (i.e., nodes communicate during a single workflow run).",
            "task_domain": "General reasoning tasks with numeric evaluation functions (benchmarked on code generation, mathematics, and question answering datasets: HumanEval, MBPP, MATH, GSM8K, HotpotQA, DROP).",
            "performance_metrics": "Reported metrics per dataset (AFLOW workflows executed with GPT-4o-mini unless otherwise noted): HotpotQA F1=73.5, DROP F1=80.6, HumanEval pass@1=94.7, MBPP pass@1=83.4, GSM8K solve rate=93.5, MATH solve rate=56.2; average across six datasets = 80.3%. Compared to baselines, AFLOW gives +5.7 percentage points average over manually designed methods and +19.5 percentage points over existing automated workflow optimization (ADAS). Also reported: AFLOW enables smaller models to outperform GPT-4o on specific tasks at 4.55% of GPT-4o's inference dollar cost (claim in abstract).",
            "baseline_comparison": "Compared against manual workflows (IO, Chain-of-Thought, Self-Consistency CoT, MultiPersona Debate, Self-Refine, MedPrompt) and automated optimizer ADAS (used as a baseline); AFLOW outperforms all listed manual methods (avg. 5.7% improvement) and ADAS (avg. +19.5%). ADAS average performance reported as 67.2% vs AFLOW 80.3%.",
            "coordination_benefits": "Preserving tree-structured experiences improves search efficiency and helps avoid information loss inherent in linear accumulation; LLM-driven expansion plus execution feedback yields higher final task accuracy across benchmarks; operator abstractions accelerate finding effective substructures (operators improved search efficiency in ablation); cost-performance benefit: AFLOW-produced workflows enable weaker/cheaper executors to reach or exceed performance of stronger models on the cost-performance Pareto front (quantified by example: matching/outperforming GPT-4o at far lower cost). Quantitative improvements: +5.7% over human workflows on average, +19.5% over ADAS, and specific per-benchmark gains listed above.",
            "coordination_challenges": "Computational cost: evaluating each generated workflow 5 times increases per-iteration expense; potential for local optima (mitigated via blank-template selection and mixed selection probabilities); inconsistencies/information loss in prior linear-history designs (not AFLOW) hinder guidance to optimizers; operator design requires human effort (though optional) to improve efficiency; some optimizer hyperparameter values reported inconsistently in paper (selection lambda/alpha values appear in two places with different numbers).",
            "ablation_studies": "Ablation on operators (GSM8K) shows that predefined Operators improve search efficiency and final performance; even without operators AFLOW remains strong (reported: AFLOW without operators achieved 93.1% on GSM8K validation/test setting), and AFLOW can autonomously create ensemble-like structures without predefined operators. Details and curves provided in Appendix B / Figure 5.",
            "optimal_configurations": "Paper recommends using Operators to improve search efficiency; MCTS variant with soft mixed-probability selection (balances uniform and score-based selection) and LLM-based expansion; practical hyperparameters used in experiments: iteration rounds = 20 (AFLOW), selection uses top-k sampling (k=3 in algorithm), early stopping when top-k unchanged for n=5 rounds; execute each candidate 5 times on validation to compute mean/std. (Note: the paper lists specific selection hyperparameters in-text as lambda=0.2 and alpha=0.4 but presents lambda=0.4 and alpha=0.2 in Appendix; overall approach uses a mixed uniform/score selection strategy.)",
            "uuid": "e2562.0",
            "source_info": {
                "paper_title": "AFlow: Automating Agentic Workflow Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ADAS",
            "name_full": "ADAS: Automated Design of Agentic Systems",
            "brief_description": "An automated workflow optimization method that represents workflows as code and stores historical workflows in a linear list; it uses a linear heuristic search algorithm and served as a baseline compared experimentally in this paper.",
            "citation_title": "Automated design of agentic systems",
            "mention_or_use": "use",
            "system_name": "ADAS",
            "system_description": "ADAS represents workflows in code and stores historical workflows in a linear list structure; it performs automated prompt/workflow optimization via a linear heuristic search algorithm and uses an LLM optimizer and LLM executor in experiments (Claude-3.5-sonnet as optimizer and GPT-4o-mini as executor in ADAS baseline runs). The paper characterizes ADAS as having a relatively complete workflow representation but limited by the efficiency of its linear search process.",
            "number_of_agents": "not specified / treated as workflow nodes (variable)",
            "agent_specializations": "Not explicitly detailed in this paper beyond general LLM-invoking nodes; ADAS uses an LLM optimizer and executor in reported comparisons.",
            "research_phases_covered": "Workflow generation/optimization, evaluation (executing workflows to compute scores).",
            "coordination_mechanism": "Linear heuristic search over code-represented workflows with a linear history list of past workflows (i.e., a flat accumulation of experience rather than a tree-structured experience store).",
            "communication_protocol": "Code-represented workflows and prompts; details limited in this paper (cited as code-based representation).",
            "feedback_mechanism": "Uses past workflows and their outcomes recorded in a linear list to prompt the optimizer for additional modifications; characterized as less efficient due to information accumulation and linear heuristic search.",
            "communication_frequency": "Iterative linear search rounds (in experiments ADAS used 30 iteration rounds), specifics not deeply described in this paper.",
            "task_domain": "Same set of reasoning/code/math/QA benchmarks used as baseline comparisons (HumanEval, MBPP, MATH, GSM8K, HotpotQA, DROP).",
            "performance_metrics": "Reported in Table 1 (executed with GPT-4o-mini): HotpotQA 64.5, DROP 76.6, HumanEval 82.4, MBPP 53.4, GSM8K 90.8, MATH 35.4; average = 67.2%.",
            "baseline_comparison": "Included as an automated workflow-optimization baseline; AFLOW outperforms ADAS across datasets by a large margin (AFLOW avg 80.3% vs ADAS 67.2%, described in paper as +19.5% over existing automated approaches).",
            "coordination_benefits": "Provides a relatively complete code-based workflow representation capable of expressing linear sequences, conditionals, and loops (claimed strength in representation).",
            "coordination_challenges": "Efficiency limitations: linear heuristic search and linear accumulation of experience reduce search efficiency and make discovery of effective workflows difficult within limited iterations; prone to information loss as input tokens increase during prompting of LLM optimizer.",
            "ablation_studies": "Not reported within this paper for ADAS beyond its comparison as a baseline; ADAS was compared with AFLOW experimentally.",
            "optimal_configurations": "Paper critiques ADAS for relying on linear search and linear experience storage; suggests tree-structured experience (as in AFLOW) and MCTS-based selection as preferable for better efficiency.",
            "uuid": "e2562.1",
            "source_info": {
                "paper_title": "AFlow: Automating Agentic Workflow Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPTSwarm",
            "name_full": "GPTSwarm: Language agents as optimizable graphs",
            "brief_description": "A prior multi-agent/workflow approach that represents agentic workflows as graphs and uses reinforcement learning to optimize agent interactions, mentioned as related work.",
            "citation_title": "Gptswarm: Language agents as optimizable graphs",
            "mention_or_use": "mention",
            "system_name": "GPTSwarm",
            "system_description": "Described in the paper as a graph-based representation of language-agent collaborations that uses reinforcement learning to optimize interactions; cited as related work with limitations in representing conditional states.",
            "number_of_agents": "variable (graph nodes represent agents)",
            "agent_specializations": "Not detailed in this paper beyond graph nodes as LLM-invoking agents; likely includes generation and verification roles in the original work (not specified here).",
            "research_phases_covered": "General agentic workflow tasks (generation, coordination) as reported in related work context.",
            "coordination_mechanism": "Graph-structured agent topology optimized with reinforcement learning (as described by the cited work); the authors note GPTSwarm 'struggles to represent workflows with conditional states due to graph structure limitations.'",
            "communication_protocol": "Not specified in this paper; implied graph message passing between agent nodes (original paper likely details this).",
            "feedback_mechanism": "Reinforcement learning optimization over graph interactions (as summarized by the authors), specifics not provided here.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "General agentic workflows / multi-agent LLM collaboration (mentioned as related work).",
            "performance_metrics": "No performance numbers provided in this paper (only a limitation note regarding representation of conditionals).",
            "baseline_comparison": "Mentioned as related prior work and contrasted with AFLOW's code-based edge representation.",
            "coordination_benefits": "Graph representation can capture complex topologies and interactions (implied); RL-based optimization provides a mechanism to adapt topology/behaviors.",
            "coordination_challenges": "Paper states GPTSwarm struggles to represent conditional states naturally due to graph structure limitations.",
            "ablation_studies": "None reported in this paper for GPTSwarm (only referenced).",
            "optimal_configurations": "Not provided in this paper.",
            "uuid": "e2562.2",
            "source_info": {
                "paper_title": "AFlow: Automating Agentic Workflow Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Metagpt",
            "name_full": "Metagpt: Meta programming for A multi-agent collaborative framework",
            "brief_description": "A multi-agent collaborative framework referenced as prior work for multi-agent/team LLM systems that program and coordinate multiple agents.",
            "citation_title": "Metagpt: Meta programming for A multi-agent collaborative framework",
            "mention_or_use": "mention",
            "system_name": "Metagpt",
            "system_description": "Mentioned as a prior multi-agent collaborative framework; cited in related work as an example of multi-agent approaches that differ from static agentic workflows.",
            "number_of_agents": "variable (not specified in this paper)",
            "agent_specializations": "Not specified here; original work described multiple cooperating agents for meta-programming tasks (this paper does not provide details).",
            "research_phases_covered": "Not specified in this paper; cited as part of general agentic/workflow literature.",
            "coordination_mechanism": "Not described in this paper; referenced as a distinct paradigm from static agentic workflows.",
            "communication_protocol": "Not described in this paper.",
            "feedback_mechanism": "Not described in this paper.",
            "communication_frequency": "Not described in this paper.",
            "task_domain": "General multi-agent collaborative programming and workflow tasks (as cited).",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Mentioned in related work; not used as a baseline in experiments.",
            "coordination_benefits": "Not described in this paper.",
            "coordination_challenges": "Not described in this paper.",
            "ablation_studies": "Not described in this paper.",
            "optimal_configurations": "Not described in this paper.",
            "uuid": "e2562.3",
            "source_info": {
                "paper_title": "AFlow: Automating Agentic Workflow Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Dynamic LLM-Agent Network",
            "name_full": "Dynamic LLM-Agent Network: An LLM-agent collaboration framework with agent team optimization",
            "brief_description": "A referenced framework for LLM-agent collaboration that explores agent-team optimization; included in related work demonstrating alternative representations for agent interactions.",
            "citation_title": "Dynamic llm-agent network: An llmagent collaboration framework with agent team optimization",
            "mention_or_use": "mention",
            "system_name": "Dynamic LLM-Agent Network",
            "system_description": "Cited as a framework that treats agent interactions as a network and aims to optimize the agent team; mentioned in related work contrasting AFLOW's code representation with other structure choices.",
            "number_of_agents": "variable (network nodes representing agents)",
            "agent_specializations": "Not detailed in this paper.",
            "research_phases_covered": "General agent collaboration tasks (not specified).",
            "coordination_mechanism": "Network/graph-based agent topology with agent-team optimization (details are in the cited work; this paper only references the approach).",
            "communication_protocol": "Not specified in this paper.",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "General multi-agent LLM collaboration (as related work).",
            "performance_metrics": "Not provided in this paper.",
            "baseline_comparison": "Referenced as related work; not used as baseline.",
            "coordination_benefits": "Not detailed here.",
            "coordination_challenges": "Not detailed here.",
            "ablation_studies": "Not described in this paper.",
            "optimal_configurations": "Not described in this paper.",
            "uuid": "e2562.4",
            "source_info": {
                "paper_title": "AFlow: Automating Agentic Workflow Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Mindstorms",
            "name_full": "Mindstorms in Natural Language-based Societies of Mind",
            "brief_description": "A prior conceptual/multi-agent work referenced as related literature about societies of language-based agents (agents cooperating in a 'society of mind').",
            "citation_title": "Mindstorms in natural language-based societies of mind",
            "mention_or_use": "mention",
            "system_name": "Mindstorms (societies of mind)",
            "system_description": "Cited as prior work about natural-language-based societies of agents and as part of the literature on agentic/agent-team systems; mentioned to place AFLOW in the broader context of multi-agent language-model systems.",
            "number_of_agents": "not specified in this paper",
            "agent_specializations": "not specified in this paper",
            "research_phases_covered": "general multi-agent reasoning/coordination (cited conceptually)",
            "coordination_mechanism": "not specified in this paper",
            "communication_protocol": "not specified in this paper",
            "feedback_mechanism": "not specified in this paper",
            "communication_frequency": "not specified in this paper",
            "task_domain": "conceptual/multi-agent reasoning frameworks (cited literature)",
            "performance_metrics": "not provided in this paper",
            "baseline_comparison": "cited in related work context; not experimentally compared",
            "coordination_benefits": "not specified in this paper",
            "coordination_challenges": "not specified in this paper",
            "ablation_studies": "not specified in this paper",
            "optimal_configurations": "not specified in this paper",
            "uuid": "e2562.5",
            "source_info": {
                "paper_title": "AFlow: Automating Agentic Workflow Generation",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gptswarm: Language agents as optimizable graphs",
            "rating": 2
        },
        {
            "paper_title": "Metagpt: Meta programming for A multi-agent collaborative framework",
            "rating": 2
        },
        {
            "paper_title": "Dynamic llm-agent network: An llmagent collaboration framework with agent team optimization",
            "rating": 2
        },
        {
            "paper_title": "Mindstorms in natural language-based societies of mind",
            "rating": 2
        },
        {
            "paper_title": "Automated design of agentic systems",
            "rating": 2
        },
        {
            "paper_title": "G-designer: Architecting multi-agent communication topologies via graph neural networks",
            "rating": 1
        }
    ],
    "cost": 0.018202999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AFLOW: <br> AUTOMATING AGENTIC WORKFLOW GENERATION</h1>
<p>Jiayi Zhang ${ }^{1,2 *}$, Jinyu Xiang ${ }^{1}$; Zhaoyang Yu ${ }^{3}$, Fengwei Teng ${ }^{3}$, Xiong-Hui Chen ${ }^{3}$, Jiaqi Chen ${ }^{5}$, Mingchen Zhuge ${ }^{6}$, Xin Cheng ${ }^{3}$, Sirui Hong ${ }^{1}$, Jinlin Wang ${ }^{1}$, Bingnan Zheng ${ }^{5}$, Bang Liu ${ }^{7}$, Yuyu Luo ${ }^{2,8}$ ? Chenglin Wu ${ }^{1 \dagger}$<br>${ }^{1}$ DeepWisdom, ${ }^{2}$ The Hong Kong University of Science and Technology (Guangzhou),<br>${ }^{3}$ Renmin University of China, ${ }^{4}$ Nanjing University, ${ }^{5}$ Fudan University,<br>${ }^{6}$ King Abdullah University of Science and Technology, ${ }^{7}$ Universit de Montral \&amp; Mila,<br>${ }^{8}$ The Hong Kong University of Science and Technology</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFLOW, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFLOW's efficacy, yielding a $5.7 \%$ average improvement over state-of-the-art baselines. Furthermore, AFLOW enables smaller models to outperform GPT-4o on specific tasks at $4.55 \%$ of its inference cost in dollars. The code is available at https://github.com/FoundationAgents/AFlow.</p>
<h2>1 INTRODUCTION</h2>
<p>Large Language Models (LLMs) have emerged as powerful tools for solving complex tasks across various domains, including code generation, data analysis, decision-making, and question answering (Liu et al., 2024; Li et al., 2024a; Zhu et al., 2024; Xie et al., 2024b; Sun et al., 2024; Wang et al., 2024b; Song et al., 2023; Xie et al., 2024a; Zhong et al., 2024a). However, the rapid advancement of LLMs heavily relies on manually designed agentic workflows - structured sequences of LLM invocations accompanied by detailed instructions. Designing and refining these workflows requires significant human effort, which limits the scalability and adaptability of LLMs to new, complex domains and hinders their ability to transfer skills across diverse tasks (Tang et al., 2024).</p>
<p>Recent efforts have focused on automating the discovery of effective agentic workflows to reduce the reliance on human intervention (Khattab et al., 2024; Yksekgnl et al., 2024; Liu et al., 2023; Hu et al., 2024). Despite these advancements, full automation has not been achieved. For instance, Khattab et al. (2024) requires manual workflow setup before automated prompt optimization. Similarly, methods proposed by Yksekgnl et al. (2024) and Zhuge et al. (2024) fail to capture the full diversity of workflows necessary for a wide range of tasks (Yu et al., 2023; Yang et al., 2024b; Sun et al., 2023), as their optimization objectives struggle to represent the breadth of possible workflows. The inability to effectively model diverse workflow structures within these automated systems limits their utility and impact. ADAS (Hu et al., 2024) represents workflows using code, achieving a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance comparison with other methods. To assess the method's performance, we employ various metrics across different datasets: solve rate for Math and GSM8K, F1 score for HotpotQA and DROP, and pass@1 for HumanEval and MBPP. Our AFLOW (highlighted in yellow) consistently outperforms all automated workflow optimization and manually designed methods across all six benchmarks.</p>
<p>relatively complete representation. However, due to the efficiency limitations of its linear heuristic search algorithm, ADAS struggles to generate effective workflows within a limited number of iterations. This highlights the need for more effective techniques to represent and automate the generation of agentic workflows, which would accelerate the application of LLMs across domains.</p>
<p>In response to these challenges, we introduce an innovative framework for automatically generating agentic workflows. Our key idea is to model the workflow as a series of interconnected LLM-invoking nodes, where each node represents an LLM action and the edges define the logic, dependencies, and flow between these actions. This structure transforms the workflow into a vast search space, encompassing a wide variety of potential configurations. Our goal is to efficiently navigate this space, automatically generating optimized workflows that maximize task performance while minimizing human intervention.</p>
<p>However, the diversity and complexity of tasks present significant challenges. Specifically, each task can have different requirements, operations, and dependencies, which makes it difficult to represent them in a unified yet flexible manner (Chen et al., 2021; Cobbe et al., 2021; Yang et al., 2018; Luo et al., 2018). Furthermore, the search space for possible workflows, comprising an immense number of code structures and node configurations, is virtually boundless, creating an additional challenge for efficient exploration and optimization.</p>
<p>To address these challenges, we propose AFLOW, a Monte Carlo Tree Search (MCTS)-based framework designed to systematically explore and discover optimal agentic workflows. AFLOW represents workflows as flexible nodes connected by code-based edges, which encapsulate possible relationships such as logical flows, conditions, and dependencies. These edges allow the workflow to be modeled as a graph (Zhuge et al., 2024) or network (Liu et al., 2023), offering a powerful structure for capturing complex interactions between LLM invocations.</p>
<p>To enhance the search process and improve efficiency, AFLOW introduces a novel concept of operators  predefined, reusable combinations of nodes representing common agentic operations (e.g., Ensemble, Review &amp; Revise). These operators serve as foundational building blocks for constructing workflows and are integrated into the search space, ensuring that the exploration process leverages known patterns of effective agentic operations.</p>
<p>AFLOW employs the MCTS algorithm to navigate this infinite search space. The framework's workflow optimization process incorporates several key innovations: a soft mixed-probability selection mechanism for node exploration, LLM-driven node expansion to introduce new possibilities, execution evaluation to assess workflow performance, and backpropagation of experience to refine future search iterations. This combination of techniques ensures that AFLOW efficiently discovers workflows that adapt to the complexity of diverse tasks while reducing reliance on manual intervention.</p>
<p>We make the following key contributions: (1) <strong>Problem Formulation</strong>: We formalize the workflow optimization problem, generalizing prior approaches as specific cases. This provides a unified framework for future research at both the node and workflow optimization levels. (2) <strong>AFLOW</strong>:</p>
<p>We introduce AFLOW, an MCTS-based method that automatically discovers effective workflows across multiple domains with minimal human intervention. (3) Extensive Evaluation: We evaluate AFLOW on six benchmark datasets: HumanEval, MBPP, MATH, GSM8K, HotPotQA, and DROP. AFLOW outperforms manually designed methods by $5.7 \%$ and surpasses existing automated approaches by $19.5 \%$. Notably, workflows generated by AFLOW enable smaller LLMs to outperform larger models, offering better cost-performance efficiency, with significant implications for real-world applications.</p>
<h1>2 Related Work</h1>
<p>Agentic Workflow Agentic workflow and autonomous agents (Zhuge et al., 2023; Hong et al., 2024a; Zhang et al., 2024c; Wang et al., 2023) represent two distinct paradigms of LLM application. The former completes tasks statically through predefined processes with multiple LLM invocations, while the latter solves problems dynamically through flexible autonomous decision-making. Compared to autonomous agents that require specific actions and decision patterns designed for the environment, agentic workflows can be constructed based on existing human domain experience and iterative refinement, offering higher potential for automated construction.</p>
<p>Agentic workflows can be broadly categorized into general and domain-specific types. General workflows emphasize universal problem-solving approaches, such as (Wei et al., 2022; Wang et al., 2022; Madaan et al., 2023; Wang et al., 2024a). Domain-specific workflows focus on building effective processes to solve domain-specific problems, such as code generation (Hong et al., 2024b; Ridnik et al., 2024; Zhong et al., 2024a), data analysis (Xie et al., 2024b; Ye et al., 2024; Li et al., 2024a; Zhou et al., 2023), mathematics (Zhong et al., 2024b; Xu et al., 2024), question answering (Nori et al., 2023; Zhou et al., 2024a). Existing work has manually discovered numerous effective agentic workflows, but it's challenging to exhaust various tasks across different domains, further highlighting the importance of automated workflow generation and optimization.</p>
<p>Automated Agentic Optimization Recent work aims to automate the design of agentic workflows, categorized into three types: automated prompt optimization, hyperparameter optimization, and automated workflow optimization. Prompt optimization (Fernando et al., 2024; Yksekgnl et al., 2024; Yang et al., 2024a; Khattab et al., 2024) uses LLMs to optimize prompts within fixed workflows. Hyperparameter optimization (Saad-Falcon et al., 2024) focuses on optimizing predefined parameters. While these approaches improve performance, they are limited in generalization to new tasks and often require moderate human effort for task-specific designs.</p>
<p>Automated workflow optimization (Li et al., 2024b; Zhou et al., 2024b; Zhuge et al., 2024; Hu et al., 2024) aims to optimize entire workflow structures, offering more potential for fully automated generation. Recent works explore diverse representations and methods. GPTSwarm (Zhuge et al., 2024) uses graph structures with reinforcement learning, but struggles to represent workflows with conditional states due to graph structure limitations. ADAS (Hu et al., 2024) utilizes code structures to represent workflows and stores historical workflows in a linear list structure, aligning closely with our goals. However, it is constrained by the efficiency of its search algorithm as it relies on overly simplistic representations of experiences in the searching process, making it challenging to discover effective workflows.</p>
<p>AFLOW also uses code to represent workflows, but goes further by providing a more fundamental structure called named node. This structure encompasses various LLM invocation parameters, allowing for more detailed workflow representation. We also introduce operators that implement predefined node combination functions. Simultaneously, AFLOW employs a specially designed MCTS algorithm for automated workflow optimization, leveraging the tree-structured experience and execution feedback to efficiently discover effective workflows.</p>
<h2>3 Preliminary</h2>
<p>In this section, we will first formulate the automated agentic workflows generation problem in Section 3.1 and then discuss design considerations of our AFLOW in Section 3.2. For the core concept of this section, we provide an example explanation in Figure 2.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The example of node, operator, and edge. We demonstrate the optional parameters for Nodes, the structure of some Operators, and common representations of Edges.</p>
<h1>3.1 Problem Formulation</h1>
<p>Agentic Workflow We define an agentic workflow W as a series of LLM-invoking nodes connected by edges to define the exection orders, denoted as $\mathcal{N}=\left{N_{1}, N_{2}, \ldots, N_{i} \ldots\right}$. Each node $N_{i}$ represents a specific operation performed by an LLM and is characterized by the following parameters. The code abstraction of the node is shown in Appendix A.2.</p>
<ul>
<li>Model $M$ : The specific language model invoked at node $N_{i}$.</li>
<li>Prompt $P$ : The input or task description provided to the model at each node.</li>
<li>Temperature $\tau$ : A parameter controlling the randomness of the LLM's output at node $N_{i}$.</li>
<li>Output format $F$ : The format in which the model's output is structured (e.g., xml, json, markdown, raw). The node in workflow should provide different output formats, inspired by the Tam et al. (2024).</li>
</ul>
<p>Edge $E$ represent abstract structures defining node relationships, governing the sequence of execution. The edge $E$ can be represented via various structures, such as:</p>
<ul>
<li>Graph Zhuge et al. (2024): A flexible structure representing hierarchical, sequential, or parallel relationships between nodes, allowing for complex branching workflows.</li>
<li>Neural Network (Liu et al., 2023): A structure that can represent complex, non-linear relationships between nodes, allowing for adaptive and learnable workflows based on input and feedback.</li>
<li>Code (Hu et al., 2024): A comprehensive representation that can express linear sequences, conditional logic, loops, and incorporate graph or network structures, offering the most precise control over workflow execution for LLMs.</li>
</ul>
<p>While graph structures can represent workflow relationships, they require complex extensions (e.g., Petri nets, BPMN) beyond basic DAGs to naturally express parallel execution and conditional logic. Neural networks enable adaptive transitions but lack precise control over workflow execution. In contrast, code representation inherently supports all these relationships through standard programming constructs. Therefore, we adopt code as our primary edge structure to maximize expressivity.</p>
<p>Automated Workflow Optimization Given a task $T$ and an evaluation function $G$, the goal of workflow optimization is to discover a workflow $W$ that maximizes $G(W, T)$. This can be formulated as a search process where an algorithm $A$ explores the search space $\mathcal{S}$ to determine the optimal workflow configuration. The search space $\mathcal{S}$ for a workflow optimization problem encompasses all possible configurations of node parameters and edge structures:</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overall AFLOW framework: By setting a search space composed of nodes with only prompt parameters flexible, a given operator set, and a code representing edge, AFLOW performs an MCTS-based search within this space. Through a variant of MCTS designed for workflow optimization, AFLOW iteratively executes a cycle of Soft Mixed Probability Selection, LLM-Based Expansion, Execution Evaluation, and Experience Backpropagation until reaching the maximum number of iterations or meeting convergence criteria.</p>
<p>$$
\mathcal{S}={(\mathcal{N}, E) \mid E \in \mathcal{E}}
$$</p>
<p>where $\mathcal{N}={N(M, \tau, P, F) \mid M \in \mathcal{M}, \tau \in[0,1], P \in \mathcal{P}, F \in \mathcal{F}}$, with $\mathcal{M}, \mathcal{P}, \mathcal{F}, \mathcal{E}$ representing the sets of possible language models, prompts, output formats, and edge configurations, respectively.
With this formulation, the workflow optimization problem can be expressed as:</p>
<p>$$
\begin{aligned}
W &amp; =A(\mathcal{S}, G, T) \
W^{*} &amp; =\underset{W \in \mathcal{S}}{\arg \max } G(W, T)
\end{aligned}
$$</p>
<p>where $A$ is the search algorithm that explores the search space $\mathcal{S}$, and $W^{*}$ is the optimal workflow configuration that maximizes the evaluation function $G$ for the given task $T$.</p>
<h1>3.2 AFLOW OVERVIEW</h1>
<p>Limitations of Previous Methods Previous approaches Yksekgnl et al. (2024); Khattab et al. (2024); Zhuge et al. (2024) to workflow optimization have primarily been constrained by the limited scope of their search spaces, based on problem definition in Section 3.1. Another related work, ADAS (Hu et al., 2024), searches in a larger space comprising a combination of prompts $N(P, T)$ and edges $E$, but fails to discover effective workflows due to the efficiency limitations of its linear heuristic search algorithm.</p>
<p>Formulation To address the limitations of previous methods, we propose AFLOW, a novel framework that leverages Large Language Models (LLMs) as optimizers within a variant of Monte Carlo Tree Search (MCTS) to search for optimal workflows. As discussed in Section 3.1, edges can be represented in both graphs and code. To ensure AFLOW can explore the full range of possible agentic workflows, we represent nodes N and edges E through code. Specifically, as shown in Figure 3,</p>
<p>AFLOW uses a variant of MCTS to iteratively explore the workflow search space, evaluate different configurations, and backpropagate experiences to refine the workflow optimization process.</p>
<p>To enhance search efficiency in practice, we simplify the search space by fixing key parameters such as the model $M$, temperature $\tau$, and format $F$. This simplification allows AFLOW to focus its search primarily on the code-represented edges $E$ and prompts. To navigate this still vast search space effectively, we introduce the concept of Operators. These Operators encapsulate common agentic operations (e.g., Ensemble, Review, Revise) by combining $N$ and $E$ into unified interfaces, thereby enabling more efficient utilization by AFLOW. By employing these Operators, we achieve more efficient search and streamlined workflow generation.</p>
<p>Formally, given a set of Operators $\mathcal{O}$ that represents predefined node combinations, and an edge space $\mathcal{E}$ represented through code, the optimization problem can be formalized as:</p>
<p>$$
\begin{gathered}
\mathcal{S}<em 1="1">{\text {AFlow }}=\left{\left(P</em>\right} \
W^{*}=\operatorname{AFLOW}\left(\mathcal{S}_{\text {AFlow }}, G, T\right)
\end{gathered}
$$}, \ldots, P_{n}, E, O_{1}, \ldots, O_{n}\right) \mid P_{i} \in \mathcal{P}, E \in \mathcal{E}, O_{i} \in \mathcal{O</p>
<p>Tasks Scope and Operations In this paper, we focus on applying AFLOW to reasoning tasks with numerical evaluation functions. We extract common operations from existing literature and define them as part of the operator set $\mathcal{O}$. These operations include: (1) Generate, (2) Format, (3) Review and Revise Madaan et al. (2023), (4) Ensemble Wang et al. (2022), (5) Test Zhong et al. (2024a), (6) Programmer, and (7) Custom as the default operator for basic node construction. The operator set $\mathcal{O}$ can be easily expanded to enhance search efficiency for various tasks. Even without any predefined operators, AFLOW can construct different workflow nodes using the basic Custom operator. The efficiency comparison between these approaches is detailed in Section 5.2. For a comprehensive understanding of the operators, we provide their detailed structures in Appendix A.4.</p>
<h1>4 The Design Details of AFlow</h1>
<p>The core concept of AFLOW is to employ Large Language Models (LLMs) as optimizers within a Monte Carlo Tree Search (MCTS) variant to discover effective workflows. In our MCTS structure, each tree node represents a complete workflow rather than individual LLM-invoking node, enabling the discovery of universal solutions for classes of problems. The search process operates through an iterative cycle of soft mixed probability selection, LLM-based optimization expansion, execution evaluation, and experience backpropagation until reaching maximum iterations or convergence criteria. A simplified illustration is shown in Figure 3, with detailed algorithm process and theoretical analysis presented in Appendix A. 6 and Appendix G, respectively.</p>
<p>Existing workflow optimization methods iteratively use past workflow structures to prompt LLMs to discover new structures. However, due to information loss during accumulation (as input tokens increase), this approach struggles to guide LLMs towards specific performance metrics. Combined with the vast search space of code, this reduces search efficiency. Our key idea is to leverage the tree structure of MCTS to preserve workflow-based exploration experiences in $N_{\max }$ rounds workflow optimization. When a workflow is revisited, we accurately reuse past successful experiences and avoid failures, enabling effective workflow generation and improving search efficiency. To prevent local optima, we introduce a special selection mechanism allowing generation from a blank template at any round. Next, we will introduce the complete process of AFLOW, as shown in Algorithm 1.</p>
<p>Initialization AFLOW begins with a template workflow $W_{0}$, which provides a framework for invoking nodes and operators. The code template, detailed in Appendix A.3, allows the LLM optimizer to complete workflow simply by completing call functions. Prior to initiating the search process, we randomly partition the dataset into a validation set ( $20 \%$ ) and a test set ( $80 \%$ ), with the random seed fixed at 42. To optimize computational efficiency, AFLOW then executes the blank template five times on the validation dataset. From these executions, we select a subset of problems that exhibit high variance in scores, which becomes the final validation set.</p>
<p>Selection Our algorithm forms the initial workflow by evaluating an empty workflow on the validation set. And then continuously select workflows based on a soft mixed probability selection strategy. cWe propose this strategy for workflow optimization: combining uniform and score-based</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Algorithm</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">AFLOW</span><span class="p">:</span><span class="w"> </span><span class="nx">Detailed</span><span class="w"> </span><span class="nx">implementation</span>
<span class="nx">Require</span><span class="p">:</span><span class="w"> </span><span class="nx">Evaluator</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">Dataset</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">Operators</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">O</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="nx">Ensure</span><span class="p">:</span><span class="w"> </span><span class="nx">Optimized</span><span class="w"> </span><span class="nx">Workflow</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">W</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">W_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">split</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D_</span><span class="p">{</span><span class="nx">V</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D_</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">W</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">W_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">iteration</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N_</span><span class="p">{</span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">work</span><span class="w"> </span><span class="nx">flow</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Select</span><span class="p">(</span><span class="nx">tree</span><span class="p">)</span>
<span class="w">        </span><span class="nx">child</span><span class="p">.</span><span class="nx">work</span><span class="w"> </span><span class="nx">flow</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Expand</span><span class="w"> </span><span class="p">(</span><span class="nx">work</span><span class="w"> </span><span class="nx">flow</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">O</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">        </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Evaluate</span><span class="w"> </span><span class="p">(</span><span class="nx">child</span><span class="p">.</span><span class="nx">work</span><span class="w"> </span><span class="nx">flow</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">G</span><span class="p">,</span><span class="w"> </span><span class="nx">D_</span><span class="p">{</span><span class="nx">V</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">        </span><span class="nx">Backpropagate</span><span class="w"> </span><span class="p">(</span><span class="nx">child</span><span class="p">.</span><span class="nx">work</span><span class="w"> </span><span class="nx">flow</span><span class="p">,</span><span class="w"> </span><span class="nx">score</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Update</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">W</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">improved</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">ConvergenceCriteriaMet</span><span class="p">()</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="k">break</span>
<span class="w">        </span><span class="nx">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">W</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>weighted probability distributions to select from top-k workflows and the initial workflow, where including the initial workflow ensures persistent exploration capability while avoiding local optima. The formula for this selection strategy is as follows:</p>
<p>$$
P_{\text {mixed }}(i)=\lambda \cdot \frac{1}{n}+(1-\lambda) \cdot \frac{\exp \left(\alpha \cdot\left(s_{i}-s_{\max }\right)\right)}{\sum_{j=1}^{n} \exp \left(\alpha \cdot\left(s_{j}-s_{\max }\right)\right)}
$$</p>
<p>where $n$ is the number of workflows, $s_{i}$ is workflow $i$ 's score, $s_{\text {max }}$ is the maximum score, $\alpha(0.4)$ controls score influence, and $\lambda(0.2)$ balances exploration and exploitation.</p>
<p>Expansion In the expansion phase, we employ an LLM as an optimizer to create new workflows and the optimize prompt is illustrated in Appendix A.1. The optimizer leverages the selected workflow's experience to generate new prompts or modify node connections by altering code, resulting in new workflows. Specifically, to maximally uncover insights from past iterations, the experience includes all modifications and their corresponding improvements or failures on the selected workflow, along with precise logs of predictions and expected output.</p>
<p>Evaluation AFLOW directly executes workflows to get feedback due to explicit evaluation functions in reasoning tasks. We test each generated workflow 5 times on the validation set, computing mean and standard deviation. While this increases per-iteration cost, it provides more accurate feedback for the optimizer. This precision enhances search efficiency, ultimately reducing the number of iterations required to reach an effective solution.</p>
<p>Backpropagation After execution, we record: (1) the workflow's performance, (2) the optimizer's modification of its parent workflow, and (3) optimization success relative to its parent. This information is stored in experience and propagated back to the parent workflow, while the performance score is added to the global record for selection.</p>
<p>Terminal Condition We implement early stopping to reduce unnecessary execution costs: the process terminates if the top-k average score shows no improvement for $n$ consecutive rounds, or after $N$ total rounds otherwise. See Appendix A. 6 for algorithmic details.</p>
<h1>5 EXPERIMENTS</h1>
<h3>5.1 EXPERIMENTAL SETUP</h3>
<p>Datasets We utilized six public benchmarks for our experiments. Following established practices (Saad-Falcon et al., 2024; Hu et al., 2024) in workflow optimization, we divide the data into validation and test sets using a 1:4 ratio. Specifically, we use the full datasets for GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021). For HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019), we randomly select 1,000 samples each, in line with (Hu et al., 2024; Shinn et al., 2023). For the MATH (Hendrycks et al., 2021) dataset, we follow (Hong</p>
<p>et al., 2024a) in selecting 617 problems from four typical problem types (Combinatorics \&amp; Probability, Number Theory, Pre-algebra, Pre-calculus) at difficulty level 5.</p>
<p>Baselines We compare workflow discovered by AFLOW against manually designed methods for LLMs, including IO (direct LLM invocation), Chain-of-Thought (Wei et al., 2022), Self Consistency CoT (5 answers) (Wang et al., 2022), MultiPersona Debate (Wang et al., 2024a), Self-Refine (max 3 iteration rounds) (Madaan et al., 2023), and MedPrompt (3 answers and 5 votes) (Nori et al., 2023). We also compared against workflow designed by automated workflow optimization method ADAS (Hu et al., 2024).
Implementation Details AFLOW utilizes different models for optimization and execution. We employ Claude-3.5-sonnet (Anthropic, 2024) as the optimizer and use models: DeepSeekV2.5 (Deepseek, 2024), GPT-4o-mini-0718 (OpenAI, 2024b), Claude-3.5-sonnet-0620 (Anthropic, 2024), GPT-4o-0513 (OpenAI, 2024a)) as executors. All models are accessed via APIs. We set the temperature to 1 for DeepSeek-V2.5 and to 0 for the other models. We set iteration rounds to 20 for AFLOW. For ADAS, we use Claude-3.5-sonnet as the optimizer and GPT-4o-mini as the executor, with the iteration rounds set to 30 .
Metrics. For GSM8K and MATH $l_{l+5}$, we report the Solve Rate (\%) as the primary metric. For HumanEval and MBPP, we report the pass@1 metric as presented in (Chen et al., 2021) to assess code accuracy. For HotpotQA and DROP, we report the F1 Score. Additionally, for all datasets, we calculate the cost by tracking token usage to construct a pareto front, visually demonstrating the performance-cost trade-offs between different methods.</p>
<h1>5.2 EXPERIMENTAL RESULTS AND ANALYSIS</h1>
<p>Main Results The main experimental results, as shown in Table 1, demonstrate the effectiveness of AFLOW. Workflows optimized by AFLOW outperform all manually designed methods by an average of $\mathbf{5 . 7 \%}$ and surpass contemporary automatic workflow optimization work by $19.5 \%$. Across six datasets in QA, Code, and Math domains, AFLOW achieves an average performance of $80.3 \%$, marking the capability and usability of this method. Notably, compared to similar works, AFLOW performed better on more challenging tasks, improving over ADAS on MATH $l_{l+5}$ - and MBPP tasks by $57 \%$, showcasing the robustness of the model on complex datasets.
Table 1: Comparison of performance between manually designed methods and workflow generated by automated workflow optimization methods in QA, code, and Math scenarios. All methods are executed with GPT-4o-mini on divided test set, and we tested it three times and reported it on the average.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Benchmarks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">IO (GPT-4o-mini)</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">72.8</td>
</tr>
<tr>
<td style="text-align: left;">CoT (Wei et al., 2022)</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">74.7</td>
</tr>
<tr>
<td style="text-align: left;">CoT SC (5-shot) (Wang et al., 2022)</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: left;">MedPrompt (Nori et al., 2023)</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">75.3</td>
</tr>
<tr>
<td style="text-align: left;">MultiPersona (Wang et al., 2024a)</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">75.1</td>
</tr>
<tr>
<td style="text-align: left;">Self Refine (Madaan et al., 2023)</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">70.7</td>
</tr>
<tr>
<td style="text-align: left;">ADAS (Hu et al., 2024)</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">67.2</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">$\mathbf{7 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 3}$</td>
</tr>
</tbody>
</table>
<p>To explore whether the workflow searched by AFLOW is model-agnostic, we use GPT-4o-mini and DeepSeek-V2.5 as execution LLMs to search effective workflows with different structures, with the results illustrated in Table 2. When applying these workflows to other models, the vast majority demonstrate stronger performance than the baseline, showcasing the generalizability of the workflows discovered by AFLOW. Simultaneously, we observe that the workflow identified using DeepSeek-V2.5 performs notably weaker on GPT-4o-mini compared to the workflow found using GPT-4o-mini itself. This suggests that different language models require different workflows to achieve their optimal performance.</p>
<p>Cost Analysis We demonstrate the comparison of performance and cost between the baselines and the top three workflows found by AFLOW using GPT-4o-mini and DeepSeek-V2.5 as execution LLMs. The comparison is made across four models with different capabilities and price points.</p>
<p>Table 2: Comparison of performance between manually designed methods and workflows generated by AFLOW with two executor LLM: GPT-4o-mini ("Ours") and DeepSeek-V2.5 ("Ours*"). All workflows are tested thrice on the humaneval test set, with average results reported. "MP" denotes "MedPrompt" (Nori et al., 2023), and "MPD" denotes "MultiPersona Debate" (Wang et al., 2024a). The results demonstrate that workflows obtained through AFLOW exhibit strong transferability.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">CoT SC</td>
<td style="text-align: center;">MP</td>
<td style="text-align: center;">MPD</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Ours*</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o-mini</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">90.8</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-V2.5</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">94.7</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">$\mathbf{9 6 . 2}$</td>
<td style="text-align: center;">95.4</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3.5-sonnet</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">94.7</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The cost refers to the total expense of executing the divided HumanEval test set. AFLOW (execution model) refers to workflows found by AFLOW using the execution model to obtain feedback. The colors in the legend represent the LLM used to execute each workflow in test dataset. The specific numerical values for this Figure can be found in Appendix D.</p>
<p>Results demonstrate that AFLOW can identify workflows that allow weaker models to outperform stronger models on the pareto front of cost-effectiveness. This breakthrough effectively removes barriers to the widespread application of agentic workflows across various domains. By automating the design of effective agentic workflows, AFLOW eliminates the human labor costs previously required. Moreover, the ability to achieve superior performance at lower costs compared to stronger models opens up further possibilities for widespread adoption.
Ablation Study We introduce operators as human-designed effort to enhance search efficiency. An ablation study on GSM8K (Figure 5) shows that operators help AFLOW discover better workflows more efficiently, achieving incremental improvements. Notably, even without operators, AFLOW maintains strong performance ( $93.1 \%$ ), surpassing manual designs. Notably, AFLOW autonomously develops ensemble-like structures without operators, demonstrating its capability for independent workflow design and marking a significant step towards full automation. Details is shown in Appendix B.
Case Study AFLOW demonstrates a clear iteration process, as shown in Figure 6, illustrating how it evolves from a blank template (containing only a single Node without prompts) to the structure presented in Figure 5(B). In each iteration, AFLOW employs a single-step modification, meaning it either adds one operator (rounds 2,3 ) or makes a targeted modification to a prompt (rounds 8,10 ). Among the unsuccessful exploration rounds, AFLOW introduced a custom review node that directly modified answers generated through complex processes without additional reasoning (round 5), which decreased accuracy. In round 14, AFLOW attempted to rephrase the problem but overly focused on "discount" information, leading to a decrease in accuracy. This iteration process showcases how tree-based search allows AFLOW to further optimize known paths while retaining</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (A) Comparison of highest performance curves on GSM8K for both validation and test sets generated by AFLOW with and without operators. Compared to other datasets, GSM8K has a larger data volume, meaning that the same percentage improvement represents a greater increase in correctly solved samples, avoiding fluctuations in improvement due to small data size that could affect comparisons; (B): The code for the best-performing workflow discovered by AFLOW on the GSM8K dataset.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Tree-structured iteration process of AFLOW on GSM8K: We highlight the path from the initial round (round 1) to the best-performing workflow, reporting the score for each node and its modification from the previous node. The purple sections in the prompts on both sides represent the main prompt modifications in this iteration.
the ability to explore new ones. On the MBPP dataset, AFLOW discovered structures similar to current manually designed workflows, such as test generation and execution by LLMs as seen in Ridnik et al. (2024). The workflow and more discovered results are presented in Appendix B and a complete optimization process is presented in Appendix C.</p>
<h1>6 CONCLUSION</h1>
<p>This paper has introduced AFLOW, a novel framework for automated workflow optimization. We have comprehensively formulated the automated workflow optimization problem, establishing a foundational structure for future research. AFLOW has leveraged Monte Carlo Tree Search and code-represented workflows to navigate the vast search space of possible workflows efficiently. Our experiments across six benchmarks demonstrate the effectiveness of AFLOW, which has outperformed manually designed methods and existing automated optimization approaches. Ablation studies have shown that AFLOW can autonomously discover effective structures, even without predefined operators. Importantly, AFLOW has enabled weaker models to outperform stronger ones on the Pareto front of cost-effectiveness. We further discuss the potential applications of AFLOW across diverse domains in Appendix F, potentially revolutionizing the adoption of agentic workflows across various domains. These results have highlighted AFLOW's potential for enhancing LLMs' problem-solving capabilities while optimizing computational costs.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>This paper is supported by NSF of China (62402409), Guangzhou Municipality Big Data Intelligence Key Lab (2023A03J0012), Guangdong Basic and Applied Basic Research Foundation (2023A1515110545), Guangzhou Basic and Applied Basic Research Foundation (2025A04J3935), and Guangzhou-HKUST(GZ) Joint Funding Program (2025A03J3714).</p>
<h2>REFERENCES</h2>
<p>Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/ claude-3-5-sonnet, 2024.</p>
<p>Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Yanqi Dai, Huanran Hu, Lei Wang, Shengjie Jin, Xu Chen, and Zhiwu Lu. Mmrole: A comprehensive framework for developing and evaluating multimodal role-playing agents. arXiv preprint arXiv:2408.04203, 2024.</p>
<p>Deepseek. DeepSeek-V2.5. https://huggingface.co/deepseek-ai/DeepSeek-V2. 5, 2024.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL-HLT (1), pp. 2368-2378. Association for Computational Linguistics, 2019.</p>
<p>Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktschel. Promptbreeder: Self-referential self-improvement via prompt evolution. In ICML. OpenReview.net, 2024.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.</p>
<p>Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, and Chenglin Wu. Data interpreter: An LLM agent for data science. CoRR, abs/2402.18679, 2024a.</p>
<p>Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jrgen Schmidhuber. Metagpt: Meta programming for A multi-agent collaborative framework. In ICLR. OpenReview.net, 2024b.</p>
<p>Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024.</p>
<p>Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. Dspy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.</p>
<p>Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to SQL: are we fully ready? Proc. VLDB Endow., 17(11):3318-3331, 2024a.</p>
<p>Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. Autoflow: Automated workflow generation for large language model agents. CoRR, abs/2407.12821, 2024b.</p>
<p>Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuyu Luo, Yuxin Zhang, Ju Fan, Guoliang Li, and Nan Tang. A survey of NL2SQL with large language models: Where are we, and where are we going? CoRR, abs/2408.05109, 2024.</p>
<p>Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llmagent collaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170, 2023.</p>
<p>Yuyu Luo, Xuedi Qin, Nan Tang, and Guoliang Li. Deepeye: Towards automatic data visualization. In ICDE, pp. 101-112. IEEE Computer Society, 2018.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</p>
<p>Boye Niu, Yiliao Song, Kai Lian, Yifan Shen, Yu Yao, Kun Zhang, and Tongliang Liu. Flow: Modularized agentic workflow automation, 2025. URL https://arxiv.org/abs/2501. 07834 .</p>
<p>Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicol Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. CoRR, abs/2311.16452, 2023.</p>
<p>OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024a.
OpenAI. GPT-4o mini: Advancing cost-efficient intelligence. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/, 2024b.</p>
<p>Shuofei Qiao, Runnan Fang, Zhisong Qiu, Xiaobin Wang, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Benchmarking agentic workflow generation, 2025. URL https://arxiv.org/abs/2410.07869.</p>
<p>Tal Ridnik, Dedy Kredo, and Itamar Friedman. Code generation with alphacodium: From prompt engineering to flow engineering. CoRR, abs/2401.08500, 2024.</p>
<p>Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, E. Kelly Buchanan, Mayee Chen, Neel Guha, Christopher R, and Azalia Mirhoseini. Archon: An architecture search framework for inference-time techniques. arXiv preprint arXiv:2409.15254, 2024.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In NeurIPS, 2023.</p>
<p>Chan Hee Song, Brian M Sadler, Jiaman Wu, Wei-Lun Chao, Clayton Washington, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2986-2997. IEEE Computer Society, 2023.</p>
<p>Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, and Rui Yan. From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models. arXiv preprint arXiv:2310.18659, 2023.</p>
<p>Yiyou Sun, Junjie Hu, Wei Cheng, and Haifeng Chen. Chatbot meets pipeline: Augment large language model with definite finite automaton. arXiv preprint arXiv:2402.04411, 2024.</p>
<p>Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? A study on the impact of format restrictions on performance of large language models. CoRR, abs/2408.02442, 2024.</p>
<p>Nan Tang, Chenyu Yang, Ju Fan, Lei Cao, Yuyu Luo, and Alon Y. Halevy. Verifai: Verified generative AI. In CIDR. www.cidrdb.org, 2024.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 257-279, 2024a.</p>
<p>Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, et al. Chain-of-table: Evolving tables in the reasoning chain for table understanding. In The Twelfth International Conference on Learning Representations, 2024b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: A benchmark for real-world planning with language agents. In Forty-first International Conference on Machine Learning, 2024a.</p>
<p>Yupeng Xie, Yuyu Luo, Guoliang Li, and Nan Tang. Haichart: Human and AI paired visualization system. Proc. VLDB Endow., 17(11):3178-3191, 2024b.</p>
<p>Yiheng Xu, SU Hongjin, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. Lemur: Harmonizing natural language and code for language agents. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In ICLR. OpenReview.net, 2024a.</p>
<p>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. arXiv preprint arXiv:2406.04271, 2024b.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In EMNLP, pp. 2369-2380. Association for Computational Linguistics, 2018.</p>
<p>Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, and Wei Zeng. Generative AI for visualization: State of the art and future directions. Vis. Informatics, 8(1):43-66, 2024.</p>
<p>Junchi Yu, Ran He, and Zhitao Ying. Thought propagation: An analogical approach to complex reasoning with large language models. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Mert Yksekgnl, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic "differentiation" via text. CoRR, abs/2406.07496, 2024.</p>
<p>Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. Cut the crap: An economical communication pipeline for llm-based multi-agent systems. arXiv preprint arXiv:2410.02506, 2024a.</p>
<p>Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv preprint arXiv:2410.11782, 2024b.</p>
<p>Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, and Jianping Fan. Mobileexperts: A dynamic tool-enabled agent team in mobile devices. CoRR, abs/2407.03913, 2024c.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595-46623, 2023.</p>
<p>Li Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger via verifying runtime execution step by step. In ACL (Findings), pp. 851-870. Association for Computational Linguistics, 2024a.</p>
<p>Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao. Achieving; $97 \%$ on gsm8k: Deeply understanding the problems makes llms perfect reasoners. arXiv preprint arXiv:2404.14963, 2024b.</p>
<p>Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning, acting, and planning in language models. In Forty-first International Conference on Machine Learning, 2024a.</p>
<p>Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic learning enables self-evolving agents. CoRR, abs/2406.18532, 2024b.</p>
<p>Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. Llm as dba. arXiv preprint arXiv:2308.05481, 2023.
Yizhang Zhu, Shiyin Du, Boyan Li, Yuyu Luo, and Nan Tang. Are large language models good statisticians? In NeurIPS, 2024.</p>
<p>Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Rbert Csords, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023.</p>
<p>Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jrgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning, 2024.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 LLM BASEd Expansion: Prompt for LLM Optimizer</h2>
<h2>Workflow optimize prompt</h2>
<p>PROMPT = <strong><em>You are building a Graph and corresponding Prompt to jointly solve (type) $\rightarrow$ problems. Referring to the given graph and prompt, which forms a basic example of $\rightarrow$ a (type) solution approach, please reconstruct and optimize them. You can add, $\rightarrow$ modify, or delete nodes, parameters, or prompts. Include your single modification $\rightarrow$ in XML tags in your reply. Ensure they are complete and correct to avoid runtime $\rightarrow$ failures. When optimizing, you can incorporate critical thinking methods like $\rightarrow$ review, revise, ensemble (generating multiple answers through different/similar $\rightarrow$ prompts, then voting/integrating/checking the majority to obtain a final answer), $\rightarrow$ selfAsk, etc. Consider Python's loops (for, while, list comprehensions), $\rightarrow$ conditional statements (if-elif-else, ternary operators), or machine learning $\rightarrow$ techniques (e.g., linear regression, decision trees, neural networks, $\rightarrow$ clustering). The graph complexity should not exceed 10. Use logical and control $\rightarrow$ flow (IF-ELSE, loops) for a more enhanced graphical representation.Ensure that $\rightarrow$ all the prompts required by the current graph from prompt_custom are $\rightarrow$ included. Exclude any other prompts. Output the modified graph and all the $\rightarrow$ necessary Prompts in prompt_custom (if needed).The prompt you need to generate is $\rightarrow$ only the one used in 'prompt_custom.XXX' within Custom. Other methods already $\rightarrow$ have built-in prompts and are prohibited from being generated. Only generate $\rightarrow$ those needed for use in 'prompt_custom'; please remove any unused prompts in $\rightarrow$ prompt_custom. the generated prompt must not contain any placeholders. $\rightarrow$ Considering information loss, complex graphs may yield better results, but $\rightarrow$ insufficient information transmission can omit the solution. It's crucial to $\rightarrow$ include necessary context during the process.</em></strong></p>
<h2>A. 2 Basic Structure of Node</h2>
<h2>Node structure</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">ActionNode</span><span class="p">:</span>
<span class="w">    </span><span class="n">async</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">llm</span><span class="p">,</span><span class="w"> </span><span class="n">schema</span><span class="o">...</span><span class="p">):</span>
<span class="w">        </span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        :param context: Everything we should know when filling node.</span>
<span class="s2">        :param llm: Large Language Model with pre-defined system message.</span>
<span class="s2">        :param schema: json/markdown/xml, determine example and output format.</span>
<span class="s2">            - raw: free form text</span>
<span class="s2">            - json: it&#39;s easy to open source LLM with json format</span>
<span class="s2">            - markdown: when generating code, markdown is always better</span>
<span class="s2">            - xml: its structured format is advantageous for constraining LLM outputs</span>
<span class="s2">        &quot; &quot;</span>
<span class="s2">        ...</span>
<span class="s2">        return self</span>
</code></pre></div>

<h2>A. 3 Basic Structure of Workflow</h2>
<h2>Workflow structure</h2>
<div class="codehilite"><pre><span></span><code><span class="n">DatasetType</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Literal</span><span class="p">[</span><span class="s">&quot;HumanEval&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MBPP&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;GSM8R&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MATH&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;HotpotQa&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;DRGP&quot;</span><span class="p">]</span>
<span class="n">class</span><span class="w"> </span><span class="n">Workflow</span><span class="o">:</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__init__</span><span class="o">:</span>
<span class="w">        </span><span class="kr">self</span><span class="p">,</span>
<span class="w">        </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">,</span>
<span class="w">        </span><span class="n">llm_config</span><span class="p">,</span>
<span class="w">        </span><span class="nf">dataset</span><span class="o">:</span><span class="w"> </span><span class="n">DatasetType</span><span class="p">,</span>
<span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">        </span><span class="kr">self</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">name</span>
<span class="w">        </span><span class="kr">self</span><span class="p">.</span><span class="nf">dataset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">dataset</span>
<span class="w">        </span><span class="kr">self</span><span class="p">.</span><span class="n">llm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">create_llm_instance</span><span class="p">(</span><span class="n">llm_config</span><span class="p">)</span>
<span class="w">        </span><span class="kr">self</span><span class="p">.</span><span class="n">llm</span><span class="p">.</span><span class="n">cost_manager</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CostManager</span><span class="p">()</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nx">async</span><span class="w"> </span><span class="nx">def</span><span class="w"> </span><span class="nx">__call__</span><span class="p">(</span><span class="kp">self</span><span class="p">,</span><span class="w"> </span><span class="nx">problem</span><span class="p">:</span><span class="w"> </span><span class="nx">str</span><span class="p">):</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="nx">Implementation</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">workflow</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="nx">raise</span><span class="w"> </span><span class="nx">NotImplementedError</span><span class="p">(</span><span class="s">&quot;This method should be implemented by the subclass&quot;</span><span class="p">)</span>
</code></pre></div>

<h1>A. 4 Operators</h1>
<h2>Operations</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">ContextualGenerate</span><span class="p">(</span><span class="n">Operator</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">async</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CONTEXTUAL_GENERATE_PROMPT</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">problem_description</span><span class="o">=</span><span class="n">problem</span><span class="p">,</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">rightarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">thought</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
<span class="w">        </span><span class="n">fill_kwargs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="ss">&quot;context&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">prompt</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;llm&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span>
<span class="w">            </span><span class="n">fill_kwargs</span><span class="o">[</span><span class="n">&quot;mode&quot;</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode</span>
<span class="w">        </span><span class="n">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">await</span><span class="w"> </span><span class="n">ActionNode</span><span class="p">.</span><span class="n">from_pydantic</span><span class="p">(</span><span class="n">GenerateOp</span><span class="p">).</span><span class="n">fill</span><span class="p">(</span><span class="o">**</span><span class="n">fill_kwargs</span><span class="p">)</span>
<span class="w">        </span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="p">.</span><span class="n">instruct_content</span><span class="p">.</span><span class="n">model_dump</span><span class="p">()</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">response</span>
<span class="k">class</span><span class="w"> </span><span class="n">CodeGenerate</span><span class="p">(</span><span class="n">Operator</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">async</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">function_name</span><span class="p">,</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GENERATE_CODEBLOCK_PROMPT</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">problem_description</span><span class="o">=</span><span class="n">problem</span><span class="p">)</span>
<span class="w">        </span><span class="n">fill_kwargs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="ss">&quot;context&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">prompt</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;llm&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;function_name&quot;</span><span class="err">:</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">rightarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">function_name</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span>
<span class="w">            </span><span class="n">fill_kwargs</span><span class="o">[</span><span class="n">&quot;mode&quot;</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode</span>
<span class="w">        </span><span class="n">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">await</span><span class="w"> </span><span class="n">ActionNode</span><span class="p">.</span><span class="n">from_pydantic</span><span class="p">(</span><span class="n">CodeGenerateOp</span><span class="p">).</span><span class="n">fill</span><span class="p">(</span><span class="o">**</span><span class="n">fill_kwargs</span><span class="p">)</span>
<span class="w">        </span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="p">.</span><span class="n">instruct_content</span><span class="p">.</span><span class="n">model_dump</span><span class="p">()</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">response</span>
<span class="k">class</span><span class="w"> </span><span class="nf">Format</span><span class="w"> </span><span class="p">(</span><span class="n">Operator</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">async</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">solution</span><span class="p">,</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">FORMAT_PROMPT</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">problem_description</span><span class="o">=</span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">solution</span><span class="o">=</span><span class="n">solution</span><span class="p">)</span>
<span class="w">        </span><span class="n">fill_kwargs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="ss">&quot;context&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">prompt</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;llm&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span>
<span class="w">            </span><span class="n">fill_kwargs</span><span class="o">[</span><span class="n">&quot;mode&quot;</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode</span>
<span class="w">        </span><span class="n">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">await</span><span class="w"> </span><span class="n">ActionNode</span><span class="p">.</span><span class="n">from_pydantic</span><span class="p">(</span><span class="n">FormatOp</span><span class="p">).</span><span class="n">fill</span><span class="p">(</span><span class="o">**</span><span class="n">fill_kwargs</span><span class="p">)</span>
<span class="w">        </span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="p">.</span><span class="n">instruct_content</span><span class="p">.</span><span class="n">model_dump</span><span class="p">()</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">response</span>
<span class="k">class</span><span class="w"> </span><span class="n">Review</span><span class="w"> </span><span class="p">(</span><span class="n">Operator</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">async</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">solution</span><span class="p">,</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">REVIEW_PROMPT</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">problem_description</span><span class="o">=</span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">solution</span><span class="o">=</span><span class="n">solution</span><span class="p">,</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">rightarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">criteria</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">criteria</span><span class="p">)</span>
<span class="w">        </span><span class="n">fill_kwargs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="ss">&quot;context&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">prompt</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;llm&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span>
<span class="w">            </span><span class="n">fill_kwargs</span><span class="o">[</span><span class="n">&quot;mode&quot;</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode</span>
<span class="w">        </span><span class="n">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">await</span><span class="w"> </span><span class="n">ActionNode</span><span class="p">.</span><span class="n">from_pydantic</span><span class="p">(</span><span class="n">ReviewOp</span><span class="p">).</span><span class="n">fill</span><span class="p">(</span><span class="o">**</span><span class="n">fill_kwargs</span><span class="p">)</span>
<span class="w">        </span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="p">.</span><span class="n">instruct_content</span><span class="p">.</span><span class="n">model_dump</span><span class="p">()</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">response</span>
<span class="k">class</span><span class="w"> </span><span class="n">Revise</span><span class="w"> </span><span class="p">(</span><span class="n">Operator</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">async</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">solution</span><span class="p">,</span><span class="w"> </span><span class="n">feedback</span><span class="p">,</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">REVISE_PROMPT</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">problem_description</span><span class="o">=</span><span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="n">solution</span><span class="o">=</span><span class="n">solution</span><span class="p">,</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">rightarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">feedback</span><span class="o">=</span><span class="n">feedback</span><span class="p">)</span>
<span class="w">        </span><span class="n">fill_kwargs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="ss">&quot;context&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">prompt</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;llm&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span>
<span class="w">            </span><span class="n">fill_kwargs</span><span class="o">[</span><span class="n">&quot;mode&quot;</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode</span>
<span class="w">        </span><span class="n">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">await</span><span class="w"> </span><span class="n">ActionNode</span><span class="p">.</span><span class="n">from_pydantic</span><span class="p">(</span><span class="n">ReviseOp</span><span class="p">).</span><span class="n">fill</span><span class="p">(</span><span class="o">**</span><span class="n">fill_kwargs</span><span class="p">)</span>
<span class="w">        </span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="p">.</span><span class="n">instruct_content</span><span class="p">.</span><span class="n">model_dump</span><span class="p">()</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">response</span>
<span class="k">class</span><span class="w"> </span><span class="n">Ensemble</span><span class="w"> </span><span class="p">(</span><span class="n">Operator</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">async</span><span class="w"> </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">solutions</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="nl">problem</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">answer_mapping</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{}</span>
<span class="w">        </span><span class="n">solution_text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">***</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="k">index</span><span class="p">,</span><span class="w"> </span><span class="n">solution</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">solutions</span><span class="p">)</span><span class="err">:</span>
<span class="w">            </span><span class="n">answer_mapping</span><span class="o">[</span><span class="n">chr(65 + index)</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">index</span>
<span class="w">            </span><span class="n">solution_text</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">f</span><span class="o">*|</span><span class="n">chr</span><span class="p">(</span><span class="mi">65</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="k">index</span><span class="p">))</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="n">n</span><span class="err">{</span><span class="nf">str</span><span class="p">(</span><span class="n">solution</span><span class="p">))</span><span class="w"> </span><span class="err">\</span><span class="n">n</span><span class="err">\</span><span class="n">n</span><span class="err">\</span><span class="n">n</span><span class="err">&quot;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ENSEMBLE_PROMPT</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">solutions</span><span class="o">=</span><span class="n">solution_text</span><span class="p">,</span>
<span class="o">~</span><span class="w"> </span><span class="n">problem_description</span><span class="o">=</span><span class="n">problem</span><span class="p">)</span>
<span class="n">fill_kwargs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;context&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">prompt</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;llm&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="err">}</span>
<span class="k">if</span><span class="w"> </span><span class="nl">mode</span><span class="p">:</span>
<span class="w">    </span><span class="n">fill_kwargs</span><span class="o">[</span><span class="n">&quot;mode&quot;</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mode</span>
<span class="n">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">await</span><span class="w"> </span><span class="n">ActionNode</span><span class="p">.</span><span class="n">from_pydantic</span><span class="p">(</span><span class="n">EnsembleOp</span><span class="p">).</span><span class="n">fill</span><span class="p">(</span><span class="o">**</span><span class="n">fill_kwargs</span><span class="p">)</span>
<span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="p">.</span><span class="n">instruct_content</span><span class="p">.</span><span class="n">model_dump</span><span class="p">()</span>
<span class="n">answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="ss">&quot;solution_letter&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;&quot;</span><span class="p">)</span>
<span class="n">answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">answer</span><span class="p">.</span><span class="n">strip</span><span class="p">().</span><span class="nf">upper</span><span class="p">()</span>
<span class="k">return</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;solution&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">solutions</span><span class="o">[</span><span class="n">answer_mapping[answer</span><span class="o">]</span><span class="err">]}</span>
</code></pre></div>

<p>class Test(Operator):
    def exec_code(self, solution, entry_point):
        fail_cases = []
        ...
        if fail_cases != []:
            return fail_cases
        else:
            return "no error"
    async def <strong>call</strong>(self, problem, solution, entry_point, test_loop: int = 3):
        for _ in range(test_loop):
            result = self.exec_code(solution, entry_point)
            if result == "no error":
                return {"result": True, "solution": solution}
            elif "exec_fail_case" in result:
                result = result["exec_fail_case"]
                prompt = REFLECTION_ON_PUBLIC_TEST_PROMPT.format(
                    problem=problem,
                    solution=solution,
                    exec_pass=f"executed unsuccessfully, error: \n {result}",
                    test_fail="executed unsuccessfully",
                )
                node = await
                    ~ ActionNode.from_pydantic(ReflectionTestOp).fill(context=prompt,
                    ~ lllm=self.llm, mode="code_fill")
                    response = node.instruct_content.model_dump()
                    solution = response["reflection_and_solution"]
            else:
                ...
            result = self.exec_code(solution, entry_point)
            if result == "no error":
                return {"result": True, "solution": solution}
            else:
                return {"result": False, "solution": solution}
class Programmer(Operator):
    async def exec_code(code, timeout=180):
        def run_code():
            try:
                global_namespace = {}
                exec(code, global_namespace)
            except ...
            done_event = threading.Event()
            result = ["Error", "subprocess error"]
            def wrapper():
            nonlocal result
            result = run_code()
            done_event.set()
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(wrapper)
            try:
                if done_event.wait(timeout=timeout):
                    return result
                else:
                    future.cancel()
                    return "Error", "Exceed time limit"
            finally:
                executor.shutdown(wait=False)
    async def code_generate(self, problem, analysis, feedback, mode):
        prompt = PYTHON_CODE_VERIFIER_PROMPT.format(problem=problem,
        ~ analysis=analysis, feedback=feedback)</p>
<div class="codehilite"><pre><span></span><code>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>fill_kwargs = {&quot;context&quot;: prompt, &quot;llm&quot;: self.llm, &quot;function_name&quot;: &quot;solve&quot;}
if mode:
    fill_kwargs[&quot;mode&quot;] = mode
node = await ActionNode.from_pydantic(CodeGenerateOp).fill(**fill_kwargs)
response = node.instruct_content.model_dump()
return response
</code></pre></div>

<p>async def <strong>call</strong>(self, problem: str, analysis: str = "None"):
    code = None
    for i in range(3):
        code = await self.code_generate(problem, analysis, feedback,
        ~ mode="code_fill")
        code = code["code"]
        status, output = await self.exec_code(code)
        if status == "Success":
            return {"code": code, "output": output}
        else:
        ...
    return {"code": code, "output": "error"}</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Providing</span><span class="w"> </span><span class="nx">predefined</span><span class="w"> </span><span class="nx">operators</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">effectively</span><span class="w"> </span><span class="nx">enhance</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">search</span><span class="w"> </span><span class="nx">efficiency</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">AFLOW</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">implement</span><span class="w"> </span><span class="nx">six</span><span class="w"> </span><span class="nx">common</span><span class="w"> </span><span class="nx">operator</span><span class="w"> </span><span class="nx">structures</span><span class="p">,</span><span class="w"> </span><span class="nx">including</span><span class="p">:</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="p">(</span><span class="nx">Contextual</span><span class="p">,</span><span class="w"> </span><span class="nx">Code</span><span class="p">),</span><span class="w"> </span><span class="nx">Format</span><span class="p">,</span><span class="w"> </span><span class="nx">Review</span><span class="w"> </span><span class="err">\</span><span class="o">&amp;</span><span class="w"> </span><span class="nx">Revise</span><span class="p">,</span><span class="w"> </span><span class="nx">Ensemble</span><span class="p">,</span><span class="w"> </span><span class="nx">Test</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Programmer</span><span class="p">.</span><span class="w"> </span><span class="nx">For</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Test</span><span class="w"> </span><span class="nx">Operator</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">public</span><span class="w"> </span><span class="nx">test</span><span class="w"> </span><span class="nx">dataset</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">dataset</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">test</span><span class="w"> </span><span class="nx">data</span><span class="p">.</span><span class="w"> </span><span class="nx">For</span><span class="w"> </span><span class="nx">datasets</span><span class="w"> </span><span class="k">like</span><span class="w"> </span><span class="nx">MBPP</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">don</span><span class="err">&#39;</span><span class="nx">t</span><span class="w"> </span><span class="nx">provide</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">public</span><span class="w"> </span><span class="nx">test</span><span class="w"> </span><span class="nx">dataset</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">follow</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">setting</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Zhong</span><span class="w"> </span><span class="nx">et</span><span class="w"> </span><span class="nx">al</span><span class="p">.</span><span class="w"> </span><span class="p">(</span><span class="mi">2024</span><span class="nx">a</span><span class="p">)</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">first</span><span class="w"> </span><span class="nx">test</span><span class="w"> </span><span class="k">case</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">problem</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">public</span><span class="w"> </span><span class="nx">test</span><span class="w"> </span><span class="nx">data</span><span class="p">.</span>

<span class="err">#</span><span class="w"> </span><span class="nx">A</span><span class="p">.</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="nx">MAPPING</span><span class="w"> </span><span class="nx">WORKFLOW</span><span class="w"> </span><span class="nx">FROM</span><span class="w"> </span><span class="nx">FORMULATION</span><span class="w"> </span><span class="nx">TO</span><span class="w"> </span><span class="nx">CODE</span><span class="w"> </span>

<span class="err">##</span><span class="w"> </span><span class="nx">An</span><span class="w"> </span><span class="nx">example</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">Workflow</span>
</code></pre></div>

<p>async def <strong>call</strong>(self, problem: str, entry_point: str):
    ***
    Implementation of the workflow
    Custom operator to generate anything you want.
    But when you want to get standard code, you should use custom_code_generate
    ~ operator.
    ***
    solutions = []
    for _ in range(3): # Generate 3 solutions
        solution = await self.custom_code_generate(problem=problem,
        ~ entry_point=entry_point, instruction=prompt_custom.CODE_GENERATE_PROMPT)
        solutions.append(solution['response'])
    best_solution = await self.sc_ensemble(solutions=solutions, problem=problem)
    test_result = await self.test(problem=problem, solution=best_solution['response'],
    ~ entry_point=entry_point)
    if test_result['result']:
        return test_result['solution'], self.llm.cost_manager.total_cost
    else:
        # If the test fails, try to fix the solution
        fixed_solution = await self.custom(input=f"Problem: {problem}\nFailed
        ~ solution: {best_solution['response']}\nError: {test_result['solution']}",
        ~ instruction=prompt_custom.FIX_CODE_PROMPT)
        return fixed_solution['response'], self.llm.cost_manager.total_cost</p>
<div class="codehilite"><pre><span></span><code><span class="n">In</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">example</span><span class="p">,</span>

<span class="o">-</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">custom</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">interface</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">building</span><span class="w"> </span><span class="n">nodes</span><span class="p">,</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Optimizer</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">generate</span><span class="o">/</span><span class="n">modify</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">prompts</span><span class="o">.</span>
<span class="o">-</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">test</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sc</span><span class="w"> </span><span class="n">ensemble</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">interfaces</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">Operators</span><span class="w"> </span><span class="p">(</span><span class="n">In</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">workflow</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">operators</span><span class="p">)</span><span class="o">.</span>
<span class="o">-</span><span class="w"> </span><span class="n">Edge</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">AFLOW</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">represented</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="n">code</span><span class="p">,</span><span class="w"> </span><span class="n">controlling</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">flow</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">input</span><span class="o">/</span><span class="n">output</span><span class="w"> </span><span class="n">variables</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">Nodes</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Operators</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">form</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">workflow</span><span class="o">.</span><span class="w"> </span><span class="n">Given</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">definition</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">traditional</span><span class="w"> </span><span class="n">concept</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="s1">&#39;node having two outgoing edges&#39;</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">apply</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">formulation</span><span class="o">.</span>


<span class="c1">## A. 6 MCTS Algorithm of AFlow.</span>
</code></pre></div>

<p>Algorithm 1 Detailed Explanation of the AFLOW Algorithm
Require: Initial Workflow (W_{0}), Evaluator (G), Dataset (D), Number of rounds (N), Operators (\mathcal{O}), Top k
    (k), Early stopping rounds (n)
Ensure: Optimal Workflow (W^{<em>})
    Initialize results (\leftarrow \emptyset), experiences (\leftarrow \emptyset, N \leftarrow 20, k \leftarrow 3, n \leftarrow 5)
    (D_{V}, D_{T} \leftarrow) RandomSplit ((D, 0.2,0.8) \quad \triangleright) Split dataset: (20 \%) for validation, (80 \%) for training
    scores (\leftarrow) Execute (\left(W_{0}, G, D_{V}\right))
    (D_{V} \leftarrow) SelectHighVarianceInstances (\left(D_{V}\right.), scores, threshold () \quad \triangleright) Select instances
    for round (\leftarrow 1) to (N) do
        if round (=1) then
            parent (\leftarrow W_{0})
        else
            parent (\leftarrow) SelectParent(results)
        end if
        context (\leftarrow) LoadContext(parent, experiences)
        (W_{\text {round }}), modification (\leftarrow) Optimizer(context, (\mathcal{O}))
        for (i \leftarrow 1) to 5 do
            score, cost (\leftarrow) Executor (\left(W_{\text {round }}, E, D_{V}\right))
            results.append(round, score, cost)
        end for
        avgScore (\leftarrow) CalculateAverageScore(results[round])
        experience (\leftarrow) CreateExperience(parent, modification, avgScore)
        experiences.append(experience)
        if avgScore &gt; bestScore then
            (W^{</em>} \leftarrow W_{\text {round }})
            bestScore (\leftarrow) avgScore
        end if
        if The Top (k) Workflows remains unchanged in (n) rounds then (\triangleright) Early stopping
            return (W^{<em>})
        end if
    end for
    return (W^{</em>})
    procedure SELECTPARENT(results)
        sorted_results (\leftarrow) SortDescending(results, key=lambda r: r.scores)
        top_k_results (\leftarrow) sorted_results (\left[: k\right])
        scores (\leftarrow\left[\right.) result.scores for result in top_k_results (])
        probabilities (\leftarrow) CalculateMixedProbabilities(scores)
        return SampleFromCategorical(probabilities)
    end procedure
    procedure CALCULATEMIXEDPROBABILITIES(scores)
        (n \leftarrow \operatorname{length}() scores () \lambda \leftarrow 0.4, \alpha \leftarrow 0.2, s_{\max } \leftarrow \max () scores ())
        (w_{i} \leftarrow \exp \left(\alpha \cdot\left(s_{i}-s_{\max }\right)\right)) for (i \in[1, n])
        (P_{\text {score }} \leftarrow w_{i} / \sum_{j=1}^{n} w_{j}) for (i \in[1, n])
        (P_{\text {uniform }} \leftarrow 1 / n) for (i \in[1, n])
        (P_{\text {mixed }} \leftarrow \lambda \cdot P_{\text {uniform }}+(1-\lambda) \cdot P_{\text {score }})
        return (P_{\text {mixed }})
    end procedure
    procedure OPTIMIZER(context, Operators)
        // LLM as Optimizer, generate new workflow and modification.
        return newWorkflow, modification
    end procedure
    procedure EXECUTOR ((W), evaluator, dataset ())
        // LLM as Executor, execute workflow on dataset and return score and cost
        return score, cost
    end procedure</p>
<div class="codehilite"><pre><span></span><code>#<span class="w"> </span><span class="nv">B</span><span class="w"> </span><span class="nv">CASE</span><span class="w"> </span><span class="nv">STUDY</span><span class="w"> </span>

##<span class="w"> </span><span class="nv">B</span>.<span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">CASE</span><span class="w"> </span><span class="nv">Study</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">AFlow</span>

<span class="nv">Alpha</span><span class="w"> </span><span class="nv">Codison</span><span class="w"> </span><span class="nv">like</span><span class="w"> </span><span class="nv">workflow</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">MBPP</span>
</code></pre></div>

<p>CODE_GENERATE_PROMPT = """
Generate a Python function to solve the given problem. Ensure the function name
\hookrightarrow matches the one specified in the problem. Include necessary imports. Use clear
\hookrightarrow variable names and add comments for clarity.
Problem:
{problem}
Function signature:
{entry_point}
Generate the complete function below:
"""
FIX_CODE_PROMPT = """
The provided solution failed to pass the tests. Please analyze the error and fix the
\hookrightarrow code. Ensure the function name and signature remain unchanged. If necessary, add
\hookrightarrow or modify imports, correct logical errors, and improve the implementation.
Problem:
{input}
Provide the corrected function below:
"""
GENERATE_TESTS_PROMPT = """
Given the problem and a potential solution, generate additional test cases to
\hookrightarrow thoroughly evaluate the function. Include edge cases and typical scenarios. Format
\hookrightarrow the test cases as assert statements that can be directly added to a Python test
\hookrightarrow function.
Problem:
{input}
Generate 3-5 additional test cases as assert statements:
"""
async def <strong>call</strong>(self, problem: str, entry_point: str):
solutions = []
for _ in range(3): # Generate 3 solutions
solution = await self.custom_code_generate(problem=problem,
\hookrightarrow entry_point=entry_point, instruction=prompt_custom.CODE_GENERATE_PROMPT)
solutions.append(solution['response'])
best_solution = await self.sc_ensemble(solutions=solutions, problem=problem)</p>
<h1>Generate additional test cases</h1>
<p>additional_tests = await self.custom(input=f"Problem: {problem}\nSolution:
\hookrightarrow {best_solution['response']}", instruction=prompt_custom.GENERATE_TESTS_PROMPT}</p>
<h1>Combine original problem and additional tests</h1>
<p>enhanced_problem = f"{problem}\n\nAdditional test
\hookrightarrow cases:\n(additional_tests['response'])"
test_result = await self.test(problem=enhanced_problem,
\hookrightarrow solution=best_solution['response'], entry_point=entry_point)
if test_result['result']:
    return test_result['solution'], self.llm.cost_manager.total_cost
else:
    # If the test fails, try to fix the solution
    fixed_solution = await self.custom(input=f"Problem: {problem}\nFailed
    \hookrightarrow solution: {best_solution['response']}\nError: {test_result['solution']}",
    \hookrightarrow instruction=prompt_custom.FIX_CODE_PROMPT)
    return fixed_solution['response'], self.llm.cost_manager.total_cost</p>
<div class="codehilite"><pre><span></span><code><span class="nv">AFLOW</span><span class="w"> </span><span class="nv">demonstrates</span><span class="w"> </span><span class="nv">its</span><span class="w"> </span><span class="nv">ability</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">reduce</span><span class="w"> </span><span class="nv">human</span><span class="w"> </span><span class="nv">effort</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">evolving</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">empty</span><span class="w"> </span><span class="nv">workflow</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">solution</span><span class="w"> </span><span class="nv">highly</span><span class="w"> </span><span class="nv">similar</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">manually</span><span class="w"> </span><span class="nv">designed</span><span class="w"> </span><span class="nv">workflows</span><span class="w"> </span><span class="nv">like</span><span class="w"> </span><span class="nv">Ridnik</span><span class="w"> </span><span class="nv">et</span><span class="w"> </span><span class="nv">al</span>.<span class="w"> </span><span class="ss">(</span><span class="mi">2024</span><span class="ss">)</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">code</span><span class="w"> </span><span class="nv">generation</span><span class="w"> </span><span class="nv">scenario</span>.<span class="w"> </span><span class="nv">This</span><span class="w"> </span><span class="nv">showcases</span><span class="w"> </span><span class="nv">AFLOW</span><span class="err">&#39;s capability to generate efficient workflows comparable to expert designs with minimal human intervention.</span>

<span class="err"># The optimal workflow generated for MATH </span>
</code></pre></div>

<p>REFINE_ANSWER_PROMPT = """
Given the mathematical problem and the output from the code execution, please provide
\Leftrightarrow a well-formatted and detailed solution. Follow these guidelines:
1. Begin with a clear statement of the problem.
2. Explain the approach and any formulas or concepts used.
3. Show step-by-step calculations, using LaTeX notation for mathematical expressions.
4. Interpret the code output and incorporate it into your explanation.
5. Provide a final answer, enclosed in \boxed{} LaTeX notation.
6. Ensure all mathematical notation is in LaTeX format.
Your response should be comprehensive, mathematically rigorous, and easy to follow.</p>
<hr />
<p>GENERATE_SOLUTION_PROMPT = " " "
Please solve the given mathematical problem step by step. Follow these guidelines:
1. State the problem clearly.
2. Outline the approach and any relevant formulas or concepts.
3. Provide detailed calculations, using LaTeX notation for mathematical expressions.
4. Explain each step of your reasoning.
5. Present the final answer enclosed in \boxed{} LaTeX notation.
6. Ensure all mathematical notation is in LaTeX format.
Your solution should be thorough, mathematically sound, and easy to understand.</p>
<hr />
<p>DETAILED_SOLUTION_PROMPT = " " "
Provide a comprehensive, step-by-step solution to the given mathematical problem. Your
\Leftrightarrow response should include:
1. A clear restatement of the problem.
2. An explanation of the mathematical concepts and theorems involved.
3. A detailed, logical progression of steps leading to the solution.
4. Clear explanations for each step, including the reasoning behind it.
5. All mathematical expressions and equations in LaTeX format.
6. Visual aids or diagrams if applicable (described in text).
7. A final answer clearly marked and enclosed in \boxed{} LaTeX notation.
8. A brief explanation of the significance of the result, if relevant.
Ensure your solution is rigorous, easy to follow, and educational for someone learning
\Leftrightarrow the concept.</p>
<hr />
<p>async def <strong>call</strong>(self, problem: str):
    """
    Implementation of the graph
    """
    # Use Programmer to generate and execute Python code
    code_solution = await self.programmer(problem=problem)
    # Use Custom to refine and format the answer
    refined_solution = await self.custom(input=problem + f*\nCode output:
    \lrcorner |code_solution['output']|", instruction=prompt_custom.REFINE_ANSWER_PROMPT}
    # Generate a detailed step-by-step solution using Custom
    detailed_solution = await self.custom(input=problem,
    \lrcorner instruction=prompt_custom.DETAILED_SOLUTION_PROMPT)
    # Generate multiple solutions using Custom
    solutions = [
        refined_solution['response'],
        detailed_solution['response']
    ]
    for _ in range(2):
        solution = await self.custom(input=problem,
        \lrcorner instruction=prompt_custom.GENERATE_SOLUTION_PROMPT)
        solutions.append(solution['response'])
    # Use ScEnsemble to select the best solution
    final_solution = await self.sc_ensemble(solutions=solutions, problem=problem)
    return final_solution['response'], self.llm.cost_manager.total_cost
```</p>
<p>This optimal workflow generated for the MATH task showcases the model's ability to generate complex, task-specific solutions from task-agnostic initial settings. It combines programmatic solutions with various reasoning strategies, culminating in an ensemble selection process, and spontaneously formats the answer into the required form. This adaptation demonstrates the model's flexibility in tailoring workflows to different problem domains, while maintaining sophisticated problem-solving structures.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*These authors contributed equally to this work.
${ }^{\dagger}$ Corresponding authors: Yuyu Luo (E-mail:yuyuluo@hkust-gz.edu.cn), Chenglin Wu (E-mail: alexanderwu@deepwisdom.ai)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
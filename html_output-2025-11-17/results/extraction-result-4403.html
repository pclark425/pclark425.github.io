<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4403 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4403</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4403</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-273502797</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.15978v2.pdf" target="_blank">PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs</a></p>
                <p><strong>Paper Abstract:</strong> The growing volume of academic publications poses significant challenges for researchers conducting timely and accurate Systematic Literature Reviews, particularly in fast-evolving fields like artificial intelligence. This growth of academic literature also makes it increasingly difficult for lay people to access scientific knowledge effectively, meaning academic literature is often misrepresented in the popular press and, more broadly, in society. Traditional SLR methods are labor-intensive and error-prone, and they struggle to keep up with the rapid pace of new research. To address these issues, we developed \textit{PROMPTHEUS}: an AI-driven pipeline solution that automates the SLR process using Large Language Models. We aimed to enhance efficiency by reducing the manual workload while maintaining the precision and coherence required for comprehensive literature synthesis. PROMPTHEUS automates key stages of the SLR process, including systematic search, data extraction, topic modeling using BERTopic, and summarization with transformer models. Evaluations conducted across five research domains demonstrate that PROMPTHEUS reduces review time, achieves high precision, and provides coherent topic organization, offering a scalable and effective solution for conducting literature reviews in an increasingly crowded research landscape. In addition, such tools may reduce the increasing mistrust in science by making summarization more accessible to laypeople. The code for this project can be found on the GitHub repository at https://github.com/joaopftorres/PROMPTHEUS.git</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4403.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4403.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROMPTHEUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROMPTHEUS: A Human-Centered Pipeline to Streamline Systematic Literature Reviews with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end automated SLR pipeline that integrates LLMs (GPT family, T5) with embedding models (Sentence-BERT) and topic modeling (BERTopic) to perform query expansion, retrieval, relevance filtering, topic clustering, summarization, post-editing and final document compilation into LaTeX.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PROMPTHEUS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PROMPTHEUS is a three-phase automated SLR pipeline: (1) Systematic Search & Screening — the user provides a research topic; an LLM (GPT-3.5 / GPT-4o) expands the topic into richer queries which are executed against arXiv (up to 3000 hits); cleaned abstracts are embedded with Sentence-BERT and cosine similarity ranking selects the top ~200 papers. (2) Data Extraction & Topic Modeling — Sentence-BERT embeddings of documents are clustered with BERTopic; keywords per cluster are extracted and fed to an LLM (GPT-3.5 / GPT-4o) to generate concise topic titles and topic reports. (3) Synthesis & Summarization — individual abstracts are summarized with a transformer (T5), aggregated per topic, then post-edited/refined with GPT models; outputs are compiled into a structured LaTeX literature review plus BibTeX.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5, GPT-4o (for query expansion, post-editing, title generation); T5 (for abstractive summarization); Sentence-BERT (for embeddings); BERTopic (topic modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (Sentence-BERT embeddings + cosine similarity), LLM-driven query expansion, automated abstract summarization (T5), iterative filtering (top-N by cosine similarity); keyword extraction for topics followed by LLM title generation</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical summarization and aggregation: T5 for per-abstract summaries, aggregation into topic-level summaries, GPT-based post-editing for coherence and readability, and structured LaTeX document synthesis; BERTopic clusters provide the higher-level organization</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Retrieves up to 3000 per query; recommendation/optimal processing uses ~200 papers for downstream modeling and summarization</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Evaluated across five computer-science oriented topics: Explainable AI (XAI), Virtual Reality (VR), Blockchain, Large Language Models (LLMs), Neural Machine Translation (NMT)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured literature review document (LaTeX + BibTeX), topic reports, topic titles, aggregated topic summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1 (precision/recall/F1), topic coherence (Gensim coherence), cosine similarity (embedding alignment), Flesch Reading Ease (FRES), number of papers retrieved/filtered, CPU time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>High ROUGE-1 precision (~0.963–0.969 across topics), lower ROUGE-1 recall (approx. 0.376–0.462) and moderate F1; topic coherence generally 0.41–0.48 (Gensim metric); GPT-4o retrieved more papers than GPT-3.5 (e.g., VR: GPT-4o 2833 vs GPT-3.5 1986); CPU times: examples — Explainable AI: GPT-3.5 1213s vs GPT-4o 1555s; recommended optimal pipeline processes ~200 documents where ROUGE, coherence, cosine similarity and readability metrics peak/plateau</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons between LLM variants (GPT-3.5 vs GPT-4o), baseline references used are original abstracts (for ROUGE) and random document baseline (for cosine similarity); no direct human SLR baseline reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>T5+GPT pipeline achieves very high precision vs abstract references (ROUGE-1 precision ≈0.96); recall is substantially lower (≈0.38–0.46). GPT-4o shows higher recall and identifies more documents than GPT-3.5 but incurs higher CPU cost (tens of minutes more in examples). Topic coherence values are moderate (~0.4–0.48). Cosine similarity and readability improve through post-editing; all metrics plateau around 200 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating LLMs for query expansion, embedding-based filtering, BERTopic clustering, T5 summarization and GPT post-editing yields efficient, high-precision SLR automation; GPT-4o increases retrieval breadth and recall but at higher compute cost; an empirical optimal trade-off exists at ~200 documents for coherence, ROUGE, readability and compute; post-editing (GPT) improves readability and precision though it reduces recall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reliance on proprietary LLMs (GPT-3.5/GPT-4o) — no open-source models evaluated; risks of hallucination / misinformation in LLM-generated summaries; bias in literature selection from model/data biases; lower recall than abstracts (some relevant content omitted); topic coherence moderate (room for improvement); scalability and CPU cost (GPT-4o expensive); need for human oversight and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance metrics (topic coherence, ROUGE, cosine similarity, readability) improve up to ~200 documents then plateau or decline; CPU time increases substantially with document count and GPT model complexity (GPT-4o consistently requires more compute than GPT-3.5); BERTopic identifies more topics as documents increase but coherence drops beyond ~200 due to noise/overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4403.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4403.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sami Multi-Agent SLR System</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent AI system reported to automate most SLR steps by using LLMs to generate search strings and screen abstracts, reducing manual workload but facing limitations with complex queries and relevance assurance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-agent AI SLR system (Sami et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a multi-agent architecture where AI agents (including LLMs) automate SLR tasks such as search-string generation and abstract screening; the paper notes empirical evaluation but reports limitations in handling complex queries and ensuring relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (unspecified in this paper's summary) — described generically as using LLMs for query generation and screening</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-driven search-string generation and automated abstract screening (likely text classification/screening via LLM prompts), details not specified in the present paper</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in detail here; characterized as automating most SLR steps which may include aggregation/synthesis but specifics not given</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General SLR automation (no single domain specified)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated screening results and other SLR artifacts (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-agent approach can automate many SLR tasks (query generation, screening) but struggles with complex queries and selection relevance; human oversight remains important.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Difficulty managing complex queries and ensuring selected studies' relevance; implied need for improved filtering and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4403.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4403.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based Screening (Dennstädt et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploratory study that tested LLMs for title and abstract screening in biomedicine, finding high sensitivity but noting resource demands and bias concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based title/abstract screening (Dennstädt et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applied LLMs to the task of title and abstract screening for biomedical literature reviews, evaluating sensitivity and practical resource requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Large Language Models (specific model names not reported in this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-assisted screening/classification of titles and abstracts (zero-/few-shot or fine-tuned prompting approaches implied)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not focused on synthesis; task limited to screening/extraction stage</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical literature screening</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screening decisions (include/exclude) and filtered abstract sets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Sensitivity (recall), resource usage, bias analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported high sensitivity in screening but with notable resource demands and potential biases (as summarized in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional manual screening and possibly ML screening tools (not precisely specified here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>High sensitivity compared to baselines but increased compute/resource demands and bias considerations</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can achieve high sensitivity in title/abstract screening, but computational cost and bias must be managed; resource requirements may limit practical adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Resource demands, model biases, practical deployment constraints (compute, reproducibility).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4403.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4403.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Technology-Assisted Review (TAR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Technology-Assisted Review (TAR) systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of AI/ML systems applied to SLR workflows that iteratively refine models to prioritize and retrieve relevant studies, reducing manual screening workload.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Technology-Assisted Review (TAR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TAR systems apply NLP and ML (iterative active learning) to the search and screening phases of reviews, training models on labeled examples to prioritize likely-relevant documents and reduce manual screening effort.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Generally ML/NLP models (paper mentions TAR in context of AI approaches; specific LLM usage not consistently reported), could include modern LLMs when adapted</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Iterative model refinement / active learning for prioritization; relevance ranking and classification; may use embeddings or classifier models</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Primarily focused on selection/screening; synthesis typically not core to TAR but can feed downstream extraction/summarization modules</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General evidence synthesis and SLR processes</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Ranked/prioritized document sets for manual review, filtered corpora</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Precision/recall for retrieval/screening, reduction in manual effort (percentage), coverage</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported reductions in manual workload (examples cited in paper: Abstrackr reduced effort by up to ~35% in Gates et al. 2020); overall improvements vary by tool and dataset</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Manual screening/human-only workflows</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>TAR reduces manual screening time substantially in many cases but can miss relevant studies and therefore requires human oversight</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TAR yields significant workload reductions but faces recall/coverage limitations and requires carefully defined inclusion/exclusion criteria and human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential to miss relevant studies (false negatives), dependence on training labels/seed examples, variable usability for non-programmers, and need for human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4403.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4403.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT/LLM SLR evaluations (Qureshi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Are chatgpt and large language models "the answer" to bringing us closer to systematic review automation?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A critical examination of ChatGPT and similar LLMs applied to SLR tasks, highlighting strengths in specific tasks (e.g., screening) and the persistent need for human validation due to errors and hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are chatgpt and large language models "the answer" to bringing us closer to systematic review automation?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT / general LLM application to SLR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluations and commentary on using ChatGPT / LLMs as assistants in SLR workflows (screening, summarization), noting task-level strengths and important validation needs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT and other LLMs (as discussed in the cited critique)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompted LLM classification/screening and summarization; human-in-the-loop validation recommended</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-generated summaries with requirement for expert validation to avoid hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General SLR practice across domains</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screening suggestions, automated summaries, SLR assistance</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Task-specific accuracy (screening accuracy), error/hallucination analysis, practical feasibility</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert review; traditional SLR methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLMs can aid specific SLR steps but do not replace human experts; require validation to prevent inclusion of irrelevant or incorrect content.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs are useful assistants for SLR tasks but currently cannot be relied upon without expert oversight due to hallucination and errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination, factual inaccuracies, omission of relevant studies, overconfidence; need for robust validation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4403.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4403.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bolanos Review (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Artificial intelligence for literature reviews: Opportunities and challenges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive review of AI-integrated SLR tools that highlights efficiency benefits and usability challenges, and discusses mitigation strategies such as knowledge-injection to manage LLM hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Artificial intelligence for literature reviews: Opportunities and challenges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-integrated SLR tools (reviewed collectively)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Survey of AI methods applied to literature reviews, including discussion of techniques to handle LLM hallucinations (e.g., knowledge injection), and considerations around user-centered design and usability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Discusses various AI models including LLMs (not a single system); mentions LLM hallucinations and mitigation via knowledge injection</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Surveyed methods include embedding-based retrieval, prompting, and knowledge injection to ground LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Survey commentary on summarization, post-editing, and grounding techniques for synthesis; knowledge injection recommended to reduce hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Meta-analysis across AI-for-SLR literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Guidance, synthesis of challenges/opportunities (review paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI brings efficiency gains to literature reviews but usability and hallucination management are key bottlenecks; knowledge injection is highlighted as a mitigation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>User-interface/usability barriers, LLM hallucinations, need for better validation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation <em>(Rating: 2)</em></li>
                <li>Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain <em>(Rating: 2)</em></li>
                <li>Artificial intelligence for literature reviews: Opportunities and challenges <em>(Rating: 2)</em></li>
                <li>A novel application of machine learning and zeroshot classification methods for automated abstract screening in systematic reviews <em>(Rating: 1)</em></li>
                <li>Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: a comprehensive review <em>(Rating: 1)</em></li>
                <li>Are chatgpt and large language models "the answer" to bringing us closer to systematic review automation? <em>(Rating: 2)</em></li>
                <li>Guidance for using artificial intelligence for title and abstract screening while conducting knowledge syntheses <em>(Rating: 1)</em></li>
                <li>A novel AI-based framework that leverages ensemble learning techniques to improve the accuracy and efficiency of study selection and data extraction processes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4403",
    "paper_id": "paper-273502797",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "PROMPTHEUS",
            "name_full": "PROMPTHEUS: A Human-Centered Pipeline to Streamline Systematic Literature Reviews with LLMs",
            "brief_description": "An end-to-end automated SLR pipeline that integrates LLMs (GPT family, T5) with embedding models (Sentence-BERT) and topic modeling (BERTopic) to perform query expansion, retrieval, relevance filtering, topic clustering, summarization, post-editing and final document compilation into LaTeX.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PROMPTHEUS",
            "system_description": "PROMPTHEUS is a three-phase automated SLR pipeline: (1) Systematic Search & Screening — the user provides a research topic; an LLM (GPT-3.5 / GPT-4o) expands the topic into richer queries which are executed against arXiv (up to 3000 hits); cleaned abstracts are embedded with Sentence-BERT and cosine similarity ranking selects the top ~200 papers. (2) Data Extraction & Topic Modeling — Sentence-BERT embeddings of documents are clustered with BERTopic; keywords per cluster are extracted and fed to an LLM (GPT-3.5 / GPT-4o) to generate concise topic titles and topic reports. (3) Synthesis & Summarization — individual abstracts are summarized with a transformer (T5), aggregated per topic, then post-edited/refined with GPT models; outputs are compiled into a structured LaTeX literature review plus BibTeX.",
            "llm_model_used": "GPT-3.5, GPT-4o (for query expansion, post-editing, title generation); T5 (for abstractive summarization); Sentence-BERT (for embeddings); BERTopic (topic modeling)",
            "extraction_technique": "Embedding-based retrieval (Sentence-BERT embeddings + cosine similarity), LLM-driven query expansion, automated abstract summarization (T5), iterative filtering (top-N by cosine similarity); keyword extraction for topics followed by LLM title generation",
            "synthesis_technique": "Hierarchical summarization and aggregation: T5 for per-abstract summaries, aggregation into topic-level summaries, GPT-based post-editing for coherence and readability, and structured LaTeX document synthesis; BERTopic clusters provide the higher-level organization",
            "number_of_papers": "Retrieves up to 3000 per query; recommendation/optimal processing uses ~200 papers for downstream modeling and summarization",
            "domain_or_topic": "Evaluated across five computer-science oriented topics: Explainable AI (XAI), Virtual Reality (VR), Blockchain, Large Language Models (LLMs), Neural Machine Translation (NMT)",
            "output_type": "Structured literature review document (LaTeX + BibTeX), topic reports, topic titles, aggregated topic summaries",
            "evaluation_metrics": "ROUGE-1 (precision/recall/F1), topic coherence (Gensim coherence), cosine similarity (embedding alignment), Flesch Reading Ease (FRES), number of papers retrieved/filtered, CPU time",
            "performance_results": "High ROUGE-1 precision (~0.963–0.969 across topics), lower ROUGE-1 recall (approx. 0.376–0.462) and moderate F1; topic coherence generally 0.41–0.48 (Gensim metric); GPT-4o retrieved more papers than GPT-3.5 (e.g., VR: GPT-4o 2833 vs GPT-3.5 1986); CPU times: examples — Explainable AI: GPT-3.5 1213s vs GPT-4o 1555s; recommended optimal pipeline processes ~200 documents where ROUGE, coherence, cosine similarity and readability metrics peak/plateau",
            "comparison_baseline": "Comparisons between LLM variants (GPT-3.5 vs GPT-4o), baseline references used are original abstracts (for ROUGE) and random document baseline (for cosine similarity); no direct human SLR baseline reported",
            "performance_vs_baseline": "T5+GPT pipeline achieves very high precision vs abstract references (ROUGE-1 precision ≈0.96); recall is substantially lower (≈0.38–0.46). GPT-4o shows higher recall and identifies more documents than GPT-3.5 but incurs higher CPU cost (tens of minutes more in examples). Topic coherence values are moderate (~0.4–0.48). Cosine similarity and readability improve through post-editing; all metrics plateau around 200 papers.",
            "key_findings": "Integrating LLMs for query expansion, embedding-based filtering, BERTopic clustering, T5 summarization and GPT post-editing yields efficient, high-precision SLR automation; GPT-4o increases retrieval breadth and recall but at higher compute cost; an empirical optimal trade-off exists at ~200 documents for coherence, ROUGE, readability and compute; post-editing (GPT) improves readability and precision though it reduces recall.",
            "limitations_challenges": "Reliance on proprietary LLMs (GPT-3.5/GPT-4o) — no open-source models evaluated; risks of hallucination / misinformation in LLM-generated summaries; bias in literature selection from model/data biases; lower recall than abstracts (some relevant content omitted); topic coherence moderate (room for improvement); scalability and CPU cost (GPT-4o expensive); need for human oversight and validation.",
            "scaling_behavior": "Performance metrics (topic coherence, ROUGE, cosine similarity, readability) improve up to ~200 documents then plateau or decline; CPU time increases substantially with document count and GPT model complexity (GPT-4o consistently requires more compute than GPT-3.5); BERTopic identifies more topics as documents increase but coherence drops beyond ~200 due to noise/overfitting.",
            "uuid": "e4403.0",
            "source_info": {
                "paper_title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Sami Multi-Agent SLR System",
            "name_full": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
            "brief_description": "A multi-agent AI system reported to automate most SLR steps by using LLMs to generate search strings and screen abstracts, reducing manual workload but facing limitations with complex queries and relevance assurance.",
            "citation_title": "System for systematic literature review using multiple ai agents: Concept and an empirical evaluation",
            "mention_or_use": "mention",
            "system_name": "Multi-agent AI SLR system (Sami et al. 2024)",
            "system_description": "Described as a multi-agent architecture where AI agents (including LLMs) automate SLR tasks such as search-string generation and abstract screening; the paper notes empirical evaluation but reports limitations in handling complex queries and ensuring relevance.",
            "llm_model_used": "LLMs (unspecified in this paper's summary) — described generically as using LLMs for query generation and screening",
            "extraction_technique": "LLM-driven search-string generation and automated abstract screening (likely text classification/screening via LLM prompts), details not specified in the present paper",
            "synthesis_technique": "Not specified in detail here; characterized as automating most SLR steps which may include aggregation/synthesis but specifics not given",
            "number_of_papers": null,
            "domain_or_topic": "General SLR automation (no single domain specified)",
            "output_type": "Automated screening results and other SLR artifacts (unspecified)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Multi-agent approach can automate many SLR tasks (query generation, screening) but struggles with complex queries and selection relevance; human oversight remains important.",
            "limitations_challenges": "Difficulty managing complex queries and ensuring selected studies' relevance; implied need for improved filtering and validation.",
            "scaling_behavior": null,
            "uuid": "e4403.1",
            "source_info": {
                "paper_title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-based Screening (Dennstädt et al.)",
            "name_full": "Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain",
            "brief_description": "An exploratory study that tested LLMs for title and abstract screening in biomedicine, finding high sensitivity but noting resource demands and bias concerns.",
            "citation_title": "Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain",
            "mention_or_use": "mention",
            "system_name": "LLM-based title/abstract screening (Dennstädt et al. 2024)",
            "system_description": "Applied LLMs to the task of title and abstract screening for biomedical literature reviews, evaluating sensitivity and practical resource requirements.",
            "llm_model_used": "Large Language Models (specific model names not reported in this paper's summary)",
            "extraction_technique": "LLM-assisted screening/classification of titles and abstracts (zero-/few-shot or fine-tuned prompting approaches implied)",
            "synthesis_technique": "Not focused on synthesis; task limited to screening/extraction stage",
            "number_of_papers": null,
            "domain_or_topic": "Biomedical literature screening",
            "output_type": "Screening decisions (include/exclude) and filtered abstract sets",
            "evaluation_metrics": "Sensitivity (recall), resource usage, bias analysis",
            "performance_results": "Reported high sensitivity in screening but with notable resource demands and potential biases (as summarized in this paper)",
            "comparison_baseline": "Traditional manual screening and possibly ML screening tools (not precisely specified here)",
            "performance_vs_baseline": "High sensitivity compared to baselines but increased compute/resource demands and bias considerations",
            "key_findings": "LLMs can achieve high sensitivity in title/abstract screening, but computational cost and bias must be managed; resource requirements may limit practical adoption.",
            "limitations_challenges": "Resource demands, model biases, practical deployment constraints (compute, reproducibility).",
            "scaling_behavior": null,
            "uuid": "e4403.2",
            "source_info": {
                "paper_title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Technology-Assisted Review (TAR)",
            "name_full": "Technology-Assisted Review (TAR) systems",
            "brief_description": "A class of AI/ML systems applied to SLR workflows that iteratively refine models to prioritize and retrieve relevant studies, reducing manual screening workload.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Technology-Assisted Review (TAR)",
            "system_description": "TAR systems apply NLP and ML (iterative active learning) to the search and screening phases of reviews, training models on labeled examples to prioritize likely-relevant documents and reduce manual screening effort.",
            "llm_model_used": "Generally ML/NLP models (paper mentions TAR in context of AI approaches; specific LLM usage not consistently reported), could include modern LLMs when adapted",
            "extraction_technique": "Iterative model refinement / active learning for prioritization; relevance ranking and classification; may use embeddings or classifier models",
            "synthesis_technique": "Primarily focused on selection/screening; synthesis typically not core to TAR but can feed downstream extraction/summarization modules",
            "number_of_papers": null,
            "domain_or_topic": "General evidence synthesis and SLR processes",
            "output_type": "Ranked/prioritized document sets for manual review, filtered corpora",
            "evaluation_metrics": "Precision/recall for retrieval/screening, reduction in manual effort (percentage), coverage",
            "performance_results": "Reported reductions in manual workload (examples cited in paper: Abstrackr reduced effort by up to ~35% in Gates et al. 2020); overall improvements vary by tool and dataset",
            "comparison_baseline": "Manual screening/human-only workflows",
            "performance_vs_baseline": "TAR reduces manual screening time substantially in many cases but can miss relevant studies and therefore requires human oversight",
            "key_findings": "TAR yields significant workload reductions but faces recall/coverage limitations and requires carefully defined inclusion/exclusion criteria and human validation.",
            "limitations_challenges": "Potential to miss relevant studies (false negatives), dependence on training labels/seed examples, variable usability for non-programmers, and need for human oversight.",
            "scaling_behavior": null,
            "uuid": "e4403.3",
            "source_info": {
                "paper_title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ChatGPT/LLM SLR evaluations (Qureshi et al.)",
            "name_full": "Are chatgpt and large language models \"the answer\" to bringing us closer to systematic review automation?",
            "brief_description": "A critical examination of ChatGPT and similar LLMs applied to SLR tasks, highlighting strengths in specific tasks (e.g., screening) and the persistent need for human validation due to errors and hallucinations.",
            "citation_title": "Are chatgpt and large language models \"the answer\" to bringing us closer to systematic review automation?",
            "mention_or_use": "mention",
            "system_name": "ChatGPT / general LLM application to SLR",
            "system_description": "Evaluations and commentary on using ChatGPT / LLMs as assistants in SLR workflows (screening, summarization), noting task-level strengths and important validation needs.",
            "llm_model_used": "ChatGPT and other LLMs (as discussed in the cited critique)",
            "extraction_technique": "Prompted LLM classification/screening and summarization; human-in-the-loop validation recommended",
            "synthesis_technique": "LLM-generated summaries with requirement for expert validation to avoid hallucination",
            "number_of_papers": null,
            "domain_or_topic": "General SLR practice across domains",
            "output_type": "Screening suggestions, automated summaries, SLR assistance",
            "evaluation_metrics": "Task-specific accuracy (screening accuracy), error/hallucination analysis, practical feasibility",
            "performance_results": null,
            "comparison_baseline": "Human expert review; traditional SLR methods",
            "performance_vs_baseline": "LLMs can aid specific SLR steps but do not replace human experts; require validation to prevent inclusion of irrelevant or incorrect content.",
            "key_findings": "LLMs are useful assistants for SLR tasks but currently cannot be relied upon without expert oversight due to hallucination and errors.",
            "limitations_challenges": "Hallucination, factual inaccuracies, omission of relevant studies, overconfidence; need for robust validation workflows.",
            "scaling_behavior": null,
            "uuid": "e4403.4",
            "source_info": {
                "paper_title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Bolanos Review (2024)",
            "name_full": "Artificial intelligence for literature reviews: Opportunities and challenges",
            "brief_description": "A comprehensive review of AI-integrated SLR tools that highlights efficiency benefits and usability challenges, and discusses mitigation strategies such as knowledge-injection to manage LLM hallucinations.",
            "citation_title": "Artificial intelligence for literature reviews: Opportunities and challenges",
            "mention_or_use": "mention",
            "system_name": "AI-integrated SLR tools (reviewed collectively)",
            "system_description": "Survey of AI methods applied to literature reviews, including discussion of techniques to handle LLM hallucinations (e.g., knowledge injection), and considerations around user-centered design and usability.",
            "llm_model_used": "Discusses various AI models including LLMs (not a single system); mentions LLM hallucinations and mitigation via knowledge injection",
            "extraction_technique": "Surveyed methods include embedding-based retrieval, prompting, and knowledge injection to ground LLM outputs",
            "synthesis_technique": "Survey commentary on summarization, post-editing, and grounding techniques for synthesis; knowledge injection recommended to reduce hallucinations",
            "number_of_papers": null,
            "domain_or_topic": "Meta-analysis across AI-for-SLR literature",
            "output_type": "Guidance, synthesis of challenges/opportunities (review paper)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "AI brings efficiency gains to literature reviews but usability and hallucination management are key bottlenecks; knowledge injection is highlighted as a mitigation approach.",
            "limitations_challenges": "User-interface/usability barriers, LLM hallucinations, need for better validation strategies.",
            "scaling_behavior": null,
            "uuid": "e4403.5",
            "source_info": {
                "paper_title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "System for systematic literature review using multiple ai agents: Concept and an empirical evaluation",
            "rating": 2,
            "sanitized_title": "system_for_systematic_literature_review_using_multiple_ai_agents_concept_and_an_empirical_evaluation"
        },
        {
            "paper_title": "Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain",
            "rating": 2,
            "sanitized_title": "title_and_abstract_screening_for_literature_reviews_using_large_language_models_an_exploratory_study_in_the_biomedical_domain"
        },
        {
            "paper_title": "Artificial intelligence for literature reviews: Opportunities and challenges",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_for_literature_reviews_opportunities_and_challenges"
        },
        {
            "paper_title": "A novel application of machine learning and zeroshot classification methods for automated abstract screening in systematic reviews",
            "rating": 1,
            "sanitized_title": "a_novel_application_of_machine_learning_and_zeroshot_classification_methods_for_automated_abstract_screening_in_systematic_reviews"
        },
        {
            "paper_title": "Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: a comprehensive review",
            "rating": 1,
            "sanitized_title": "towards_the_automation_of_systematic_reviews_using_natural_language_processing_machine_learning_and_deep_learning_a_comprehensive_review"
        },
        {
            "paper_title": "Are chatgpt and large language models \"the answer\" to bringing us closer to systematic review automation?",
            "rating": 2,
            "sanitized_title": "are_chatgpt_and_large_language_models_the_answer_to_bringing_us_closer_to_systematic_review_automation"
        },
        {
            "paper_title": "Guidance for using artificial intelligence for title and abstract screening while conducting knowledge syntheses",
            "rating": 1,
            "sanitized_title": "guidance_for_using_artificial_intelligence_for_title_and_abstract_screening_while_conducting_knowledge_syntheses"
        },
        {
            "paper_title": "A novel AI-based framework that leverages ensemble learning techniques to improve the accuracy and efficiency of study selection and data extraction processes",
            "rating": 1,
            "sanitized_title": "a_novel_aibased_framework_that_leverages_ensemble_learning_techniques_to_improve_the_accuracy_and_efficiency_of_study_selection_and_data_extraction_processes"
        }
    ],
    "cost": 0.0180395,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs
22 Oct 2024</p>
<p>Joao Pedro Fernandes Torres joao.fernandes.torres@tecnico.ulisboa.pt 
DEI/IST
INESC-ID
UL Portugal</p>
<p>Catherine Mulligan c.mulligan@imperial.ac.uk 
ISST
Imperial College London
LondonUK</p>
<p>Joaquim Jorge jorgej@tecnico.ulisboa.pt 
DEI/IST
INESC-ID
UL Portugal</p>
<p>Catarina Moreira catarina.pintomoreira@uts.edu.au 
Data Science Institute
University of Technology Sydney
SydneyAustralia</p>
<p>PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs
22 Oct 2024860462D7976CCD14CD7FB77A430A993FarXiv:2410.15978v2[cs.AI]SLRLiterature ReviewsAILLM
The growing volume of academic publications poses significant challenges for researchers conducting timely and accurate Systematic Literature Reviews, particularly in fast-evolving fields like artificial intelligence.This growth of academic literature also makes it increasingly difficult for lay people to access scientific knowledge effectively, meaning academic literature is often misrepresented in the popular press and, more broadly, in society.Traditional SLR methods are labor-intensive and errorprone, and they struggle to keep up with the rapid pace of new research.To address these issues, we developed PROMPTHEUS : an AI-driven pipeline solution that automates the SLR process using Large Language Models.We aimed to enhance efficiency by reducing the manual workload while maintaining the precision and coherence required for comprehensive literature synthesis.PROMPTHEUS automates key stages of the SLR process, including systematic search, data extraction, topic modeling using BERTopic * , and summarization with transformer models.Evaluations conducted across five research domains demonstrate that PROMPTHEUS reduces review time, achieves high precision, and provides coherent topic organization, offering a scalable and effective solution for conducting literature reviews in an increasingly crowded research landscape.In addition, such tools may reduce the increasing mistrust in science by making summarization more accessible to laypeople.The code for this project can be found on the GitHub repository at https://github.com/joaopftorres/PROMPTHEUS.git</p>
<p>Introduction</p>
<p>The exponential growth of academic publications poses a significant challenge for researchers attempting to stay current with developments across numerous fields.Over 2.5 million papers are published annually Papers using emerging Technologies for High Efficiency in Updated Systematic Reviews.PROMPTHEUS is an automated framework that integrates Large Language Models (LLMs) to automate key phases of the SLR process: Systematic Search and Screening, Data Extraction, and Synthesis and Summarization.</p>
<p>While the critical planning phase remains in the hands of researchers, PROMPTHEUS significantly reduces manual workload, enhancing the precision, accuracy, and relevance of final outputs, thus allowing researchers to focus more on the innovative aspects of their work.</p>
<p>The contributions of this work are the following:</p>
<p>• Novel Integration of SLR Phases: We present a fully automated approach to SLRs, combining multiple stages-search, extraction, and synthesis-into an end-to-end process powered by advanced natural language processing (NLP) techniques.</p>
<p>• Precision in Literature Retrieval: We leverage state-of-the-art language models to enhance the precision of literature searches.This ensures that researchers receive high-quality and relevant studies, addressing a critical need for accurate literature filtering.</p>
<p>• Structured Topic Modeling: PROMPTHEUS employs BERTopic, a topic modeling technique that structures the extraction and organization of information, allowing for clear, well-organized reviews.</p>
<p>• Comprehensive Evaluation: We present a robust evaluation using several metrics, including ROUGE scores, Flesch readability scores, cosine similarity, and topic coherence.These evaluations demonstrate the effectiveness of PROMPTHEUS in automating the SLR process while maintaining high accuracy and improving the readability of generated content.</p>
<p>By automating the most time-consuming aspects of systematic literature reviews, PROMPTHEUS aims to make SLRs more accessible, efficient, and comprehensive.This will ultimately enable researchers to devote more time to innovative, high-impact research while ensuring they remain up-to-date with critical developments.</p>
<p>Background and Related Work</p>
<p>Systematic Literature Reviews are crucial for synthesizing research, identifying knowledge gaps, and shaping future directions across various domains.The traditional SLR process, defined by the PRISMA guidelines (Page et al, 2021,?), consists of four phases: Planning, Selection, Extraction, and Execution (Okoli, 2015).Despite its rigor, this method faces increasing challenges due to the sheer volume of academic publications.The manual nature of SLRs makes them labor-intensive, prone to error, and difficult to scale, particularly as research outputs grow exponentially.</p>
<p>Advances in Automating Systematic Literature Reviews</p>
<p>Recently, machine learning (ML) and natural language processing have emerged as powerful tools that can assist with these challenges by automating various SLR process stages.Large language models such as T5 (Raffel et al, 2023), GPT-3.5, GPT-4o, and GPT-o1 (Brown et al, 2020) have been integrated into the SLR workflow, particularly for tasks like literature search, data extraction, and summarization.These AI approaches, including Technology-Assisted Review (TAR) systems, apply NLP and ML techniques to automate the search and screening phases, significantly reducing manual effort by iteratively refining models to prioritize relevant studies.This automation extends to data extraction, where NLP techniques ensure consistency and synthesis, where models such as T5 and GPT generate coherent summaries of research findings, enhancing accuracy and readability.Moreno-Garcia et al (2023) propose a novel AI-based framework that leverages ensemble learning techniques to improve the accuracy and efficiency of study selection and data extraction processes.Their model demonstrates how ensemble techniques, when applied to AI-assisted systematic reviews, can enhance the precision and recall of study identification while reducing manual effort.This work contributes to AI-driven SLR tools by highlighting the potential for combining multiple AI models to tackle the inherent variability and challenges in automating complex tasks like data extraction and synthesis.Bolanos et al (2024) conducted a comprehensive review of AI-integrated SLR tools, highlighting the efficiency improvements AI brings while emphasizing usability-related challenges.The authors highlight the need for user-friendly tools and strategies to manage LLM hallucinations, notably through knowledge injection techniques.Similarly, Saeidmehr et al (2024) proposed a spiral approach to systematic reviews, significantly improving screening efficiency in smaller datasets while also addressing gaps in handling unbalanced datasets and improving article acquisition.</p>
<p>Other AI-based models, such as the multi-agent AI system developed by Sami et al (2024), offer a promising approach by automating most steps in the SLR process.Their system uses LLMs to automate tasks like generating search strings and screening abstracts.However, while this approach reduces the manual workload, it still faces limitations in managing complex queries and ensuring the relevance of the selected studies.</p>
<p>Automation techniques are increasingly used in systematic reviews, reducing manual workloads by up to 7%, as noted by Tóth et al (2024).However, challenges remain in recall consistency and real-world adoption.The study emphasizes the need for standardized evaluation metrics to better assess automation's impact, showing that while promising, automation's full potential is not yet realized due to technical and practical limitations.</p>
<p>Limitations of Current Automated SLR Systems</p>
<p>Despite the progress in AI-assisted SLRs, several limitations remain.Many systems struggle with handling complex queries, often relying on simple keyword searches that fail to capture the depth and specificity needed for comprehensive reviews.Additionally, the criteria for inclusion and exclusion are frequently poorly defined, leading to the retention of irrelevant or low-quality studies.Existing research highlights the need for more sophisticated search algorithms, improved Boolean logic integration, and better strategies for managing large datasets without sacrificing accuracy and relevance (Chappell et al, 2023;Guo et al, 2024;Robledo et al, 2023).</p>
<p>Machine learning tools, such as Abstrackr, have effectively automated SLRs' title and abstract screening stages.In the study by Gates et al (2020), Abstrackr reduced manual effort by up to 35%, significantly saving time while maintaining a high accuracy level in identifying relevant studies.However, the tool missed some important studies during the screening process, underscoring a critical limitation of AIassisted systems.Despite the efficiency gains, these tools still require human oversight to ensure that essential research is not inadvertently excluded.This highlights the balance between leveraging AI to reduce workload and ensuring that expert validation is in place to preserve the comprehensiveness and quality of the review process.</p>
<p>To support these findings, Cierco Jimenez et al (2022) reviewed a range of ML tools for automating the SLR process, noting that many tools lacked user-friendly interfaces for researchers without programming skills.Affengruber et al (2024a) presented this finding, showing that while tools like Abstrackr and Rayyan can enhance efficiency, there is still a need for more comprehensive evaluations of their usability and impact in real-world scenarios.In addition, Perlman-Arrow et al ( 2023) evaluated an NLP tool for abstract screening during a SARS-CoV-2 review, reducing screening time by 33.7% while still requiring human oversight to ensure accuracy.</p>
<p>While AI has improved screening efficiency, challenges remain in automating tasks like data extraction and risk of bias assessment.Ofori-Boateng et al (2024) highlight that screening is the most automated phase, but more advanced AI techniques are needed for accurate data extraction and bias assessment.This shows that while automation reduces workloads, it still falls short in handling complex tasks in systematic reviews.</p>
<p>The role of human expertise remains critical in maintaining the rigor of systematic reviews, particularly when AI tools are not yet fully capable of handling the complexities of comprehensive research synthesis.Qureshi et al (2023) andLi et al (2024) both highlighted the limitations of ChatGPT and similar LLMs, showing that while these models excel in specific tasks like abstract screening, they often require expert validation to prevent the inclusion of irrelevant or erroneous studies.Rather than replacing human researchers, these tools are, therefore, best viewed as assistants in keeping up to date with emerging new work in the field.</p>
<p>Advanced NLP and LLM Techniques</p>
<p>Recent studies demonstrate the potential of advanced NLP techniques in addressing some of the limitations of current automated SLR systems.For instance, Kharawala et al (2021) explored using zero-shot classification combined with ML algorithms to automate abstract screening, demonstrating high precision and recall.Similarly, Dennstädt et al (2024) tested LLMs for title and abstract screening in the biomedical domain, showing high sensitivity but noting challenges related to resource demands and biases.</p>
<p>To enhance AI's role in systematic reviews, Hamel et al (2021) developed a framework for integrating AI into the title and abstract screening phases of SLRs, stressing the importance of robust training sets and transparent reporting.In parallel, Masoumi et al (2024) demonstrated the effectiveness of BioBERT, a variant of BERT fine-tuned for biomedical texts, in automating the abstract review process in medical research, showing that such models can significantly reduce manual workloads while maintaining high accuracy.</p>
<p>Challenges of Rapid Reviews and Methodological Shortcuts</p>
<p>AI-based approaches have also been applied to rapid reviews (RRs), often employing methodological shortcuts to expedite the review process.Guo et al (2024) examined the impact of these shortcuts, showing that while they improve efficiency, they can introduce biases and reduce comprehensiveness.Speckemeier et al (2022) echoed these concerns, calling for more rigorous methodologies to balance the need for speed with the maintenance of review quality.Moreover, O'Connor et al (2019) examined the cultural and practical challenges of adopting automation tools in systematic reviews, particularly in healthcare.Their study emphasized better collaboration between AI systems and human experts to ensure these tools are effectively integrated into existing workflows.</p>
<p>Addressing the Gap</p>
<p>Building on the limitations identified in existing automated systems, our work presents PROMPTHEUS.</p>
<p>This fully automated SLR pipeline system enhances the review process by addressing critical limitations such as inadequate inclusion/exclusion criteria and complex query handling.PROMPTHEUS automates the Selection, Extraction, and Synthesis phases, allowing researchers to manage the Planning phase while leveraging advanced NLP techniques like BERTopic for topic modeling and Sentence-BERT for sentence similarity.By incorporating LLMs like GPT and T5 for summarization and post-editing, PROMPTHEUS ensures that the generated summaries are accurate and coherent.</p>
<p>PROMPTHEUS: A Framework for AI-Driven SLRs</p>
<p>Despite significant advancements in AI-assisted SLRs, challenges remain in ensuring automated systems' accuracy, scalability, and relevance.Our proposed framework, PROMPTHEUS, introduces an integrated and fully automated SLR framework that enhances SLRs' Selection, Extraction, and Synthesis phases while maintaining human oversight during the Planning phase.PROMPTHEUS leverages advanced NLP techniques such as BERTopic for topic modeling and Sentence-BERT for sentence similarity to improve the precision and relevance of selected studies.Our system also integrates LLMs like GPT and T5 for summarization and post-editing, ensuring the generated summaries are accurate and coherent.By introducing early-stage inclusion and exclusion criteria, PROMPTHEUS improves the rigor of study selection and reduces the likelihood of including irrelevant papers.This approach addresses the shortcomings identified by O'Connor et al (2019) and de la Torre-López et al (2023), who emphasized the importance of integrating AI tools that improve efficiency without compromising the accuracy and comprehensiveness of systematic reviews, and also the challenges highlighted by Affengruber et al (2024b), andShaheen et al (2023) who stressed the importance of balancing efficiency with comprehensive, high-quality reviews.</p>
<p>General Overview</p>
<p>Our automated SLR pipeline architecture is organized into three interconnected phases: (1) Systematic Search and Screening, which identifies and selects relevant academic papers; (2) Data Extraction and Topic Modeling, which categorizes and organizes the selected studies; and (3) Synthesis and Summarization, which generates coherent summaries and integrates the findings into a structured review document.</p>
<p>Each module employs specialized NLP techniques and LLMs, producing an efficient and scalable SLR process.Figure 1 presents the overall process.</p>
<p>Fig. 1 The PROMPTHEUS framework consists of three phases: (1) Systematic Search and Screening using GPT and Sentence-BERT for paper selection, (2) Data Extraction and Topic Modeling with BERTopic and GPT for organizing and generating section titles, and (3) Synthesis and Summarization with T5 and GPT to refine and compile the findings into an SLR LaTeX document.This framework leverages NLP techniques and LLMs for an efficient and scalable SLR process.</p>
<p>Systematic Search and Screening Module</p>
<p>The Systematic Search and Screening Module is the foundation of the automated Systematic Literature Review process, which automates retrieving and filtering academic papers based on a user-defined research question or topic.This module addresses the limitations of traditional literature search methods, which often require extensive manual effort, by using LLMs and advanced NLP techniques to enhance the efficiency and precision of the search process.</p>
<p>Research Topic Expansion.</p>
<p>The module begins with the user providing a research question or topic as input.The system leverages an LLM (GPT-3.5,GPT-4 or GPT-4o) to expand the initial input into a more detailed and semantically rich set of keywords and phrases to ensure the search captures a comprehensive range of relevant studies.This expansion is guided by a carefully crafted prompt that instructs the model to retain the core focus of the research topic while adding appropriate keywords and terms to cover variations and related concepts.</p>
<p>Part of the prompt used for this task is:</p>
<p>System: "You are a knowledgeable AI specializing in generating expanded titles for research topics.Your expanded titles should be concise and focus on capturing the core semantic meaning of a topic, suitable for creating informative embeddings for tasks like similarity comparisons."</p>
<p>User: "Task: Generate a slightly expanded title for the following research topic, keeping the core focus while potentially adding 1-2 highly relevant terms for improved semantic representation.</p>
<p>Topic: title</p>
<p>Guidelines:</p>
<p>as title and abstract to refine the search further, ensuring that the retrieved literature aligns closely with the expanded topic.The prompt used for generating the search query is: The output of this prompt might produce a query such as (ti:"AI-based literature review" OR abs:("AI-based literature review" OR "automated systematic reviews")) AND (ti:"NLP" OR abs:"NLP").</p>
<p>This structured query is then used to search the arXiv database through its API, retrieving up to 3000 academic papers that match the specified criteria.Once the search results are obtained, the module pre-processes the retrieved papers by extracting essential details such as paper ID, title, and abstract.The text is cleaned to ensure consistency and readability by removing unnecessary symbols and normalizing the format.This clean text is then used in the next stage of the module, where relevance filtering is performed.</p>
<p>Relevance Filtering Using Sentence Similarity.The module employs a similarity-based mechanism using Sentence-BERT embeddings to filter the most pertinent papers from the initial search results.</p>
<p>It computes vector embeddings for both the expanded research topic and the cleaned abstracts of the retrieved papers.The cosine similarity between these embeddings is then calculated to assess the relevance of each paper.The top 200 papers with the highest similarity scores are selected for further analysis, ensuring the final literature set is focused and comprehensive.This structured approach significantly reduces manual effort while improving the quality and relevance of the selected studies, providing a robust foundation for subsequent stages.</p>
<p>Once the relevant papers are identified through systematic search and screening, the next step is to organize these documents into coherent themes using the Data Extraction and Topic Modeling Module.</p>
<p>Data Extraction and Topic Modeling Module</p>
<p>The Data Extraction and Topic Modeling Module automates organizing and categorizing selected academic papers into meaningful topics based on semantic content.The module leverages topic modeling and language generation techniques to create a structured literature representation, making identifying key research themes and subtopics easier.The module's core components include Topic Modeling and Document Clustering, Keyword Extraction and Title Generation, and Topic Report Generation.</p>
<p>Topic Modeling and Document Clustering.</p>
<p>Once the most relevant documents are selected from the initial screening phase, this module initiates by creating embeddings for the textual content of each document using a Sentence-BERT model.These embeddings capture the semantic information of the documents, allowing for an effective clustering of papers based on their conceptual similarities.Topic modeling uses the BERTopic algorithm, which groups documents into coherent clusters reflecting the selected literature's primary themes.The number of topics and the minimum topic size are dynamically adjusted based on the size and content of the dataset to ensure that the generated topics are both meaningful and interpretable.</p>
<p>Keyword Extraction and Title Generation.</p>
<p>After clustering the documents into distinct topics, the system extracts keywords for each topic, summarising the main themes in that cluster.The keywords are input into a language model, such as GPT-3.5,GPT-4, or GPT-4o, to generate concise and descriptive titles for each topic.This process is guided by a structured prompt instructing the language model to create topic titles that accurately represent the essence of the keywords while maintaining clarity and relevance.The prompt used for this task is: For instance, if the extracted keywords for a topic are "deep learning, neural networks, image recognition," the generated title might be "Deep Learning for Image Recognition."This descriptive title provides an overview of the underlying theme of the clustered documents, making it easier for researchers to navigate through the literature.</p>
<p>Topic Report Generation.</p>
<p>After generating the titles, the system compiles a comprehensive report that includes the list of documents under each topic, the topic keywords, and the generated titles.This hierarchical organization of literature enhances the comprehensiveness and accessibility of the review, as it delineates different research themes and subtopics, making it easier for researchers to identify key trends and gaps in the literature.The module's process is further supported by a series of iterations and parameter adjustments to refine the topic modeling.If the initial number of topics is too few or too many, the system dynamically tunes the parameters, such as the number of topics or the minimum size of a topic, to achieve optimal clustering.Overall, this module significantly enhances the efficiency and effectiveness of the systematic literature review process by automating the categorization of papers and generating meaningful insights into the core themes of the literature.It automates the categorization of documents, offering researchers valuable insights into the core themes of the literature and simplifying the identification of key trends and gaps.</p>
<p>Synthesis and Summarization Module</p>
<p>The Synthesis and Summarization Module generates concise and coherent summaries for each identified topic cluster, significantly reducing the manual effort typically required in literature review processes.This module utilizes transformer-based models, such as T5, to summarize abstracts and GPT-based models for post-editing, ensuring that the resulting content is well-structured and easy to understand.</p>
<p>Abstract Summarization with T5.</p>
<p>The process begins by generating summaries for individual abstracts within each topic cluster using a transformer-based model like T5.This model is specifically configured to produce short yet comprehensive summaries that capture each document's key contributions and findings.The generated summaries retain essential details while significantly reducing the length of the original abstracts, making it easier to synthesize large volumes of research.</p>
<p>Topic-Level Summarization and Aggregation.</p>
<p>After individual summaries are generated, they are aggregated into a comprehensive summary for each identified topic.This step synthesizes the insights from multiple papers within the same topic, offering a holistic view of the research contributions, trends, and open questions.The aggregated summaries provide a structured narrative highlighting the most significant findings across multiple studies.</p>
<p>Post-Editing and Refinement with GPT.</p>
<p>To enhance the clarity, coherence, and flow of the aggregated summaries, a GPT-based model is employed for post-editing.The refinement process involves using a predefined prompt instructing GPT to improve readability and structure while preserving critical information.This step ensures that the final summaries are well-organized and suitable for inclusion in a structured literature review document.The following prompt is used for post-editing:</p>
<p>System: "You are an expert researcher specializing in literature reviews in the field of title.Your task is to meticulously refine and enhance machine-generated summaries of multiple research papers."User: Refine the following machine-generated summary for the section "section name" in a literature review titled "title"</p>
<p>The original summary is a compilation of various papers.Please focus on retaining the most relevant information for this literature review section.</p>
<p>Crucially, ensure the inclusion of in-text citations (e.g., citepkadir2024revealing) for all information directly sourced from the referenced documents.Feel free to shorten the section summary if it enhances clarity and conciseness, but prioritize keeping essential details and all relevant citations.Original Summary: summary Output format: * Provide only the revised summary.Do not include any additional explanations or commentary.</p>
<p>This refinement results in a more precise and cohesive summary that better communicates the core literature of the topic.</p>
<p>Document Compilation and Report Generation.</p>
<p>The final step is compiling the generated summaries and topics into a coherent literature review document.</p>
<p>This module integrates all the synthesized content into a structured LaTeX document, which includes an introduction, background information, detailed literature synthesis for each topic, and a conclusion.The system also generates a BibTeX file with the references for all included papers, ensuring proper citation and academic integrity.</p>
<p>The document generation process uses GPT, ensuring the final output is professionally formatted and adheres to the desired layout and style.The module supports various formats for exporting the final report, including LaTeX and PDF, providing researchers with a polished, ready-to-use literature review.</p>
<p>Experimental Setup</p>
<p>The proposed automated SLR framework was evaluated using a comprehensive experimental setup to assess its performance across different stages of the review process.We used five distinct research topics for the experiments: "Explainable Artificial Intelligence (XAI)," "Virtual Reality (VR)," "Blockchain," "Large Language Models (LLMs)," and "Neural Machine Translation (NMT)."Each experiment focused on a specific phase of the proposed SLR framework: Systematic Search and Screening, Data Extraction and Topic Modeling, and Synthesis and Summarization.</p>
<p>Datasets.We conducted experiments using five different research topics, each representing a unique area of academic research: Explainable Artificial Intelligence, Virtual Reality, Blockchain, Large Language Models, and Neural Machine Translation.We collected the papers for each research topic from the arXiv database.We retrieved papers based on search queries generated by GPT-3.5 and GPT-4o models, with a maximum limit of 3000 papers per query.</p>
<p>Experiments.We designed four experiments to assess the system's performance across different phases: Systematic Search and Screening, Data Extraction and Topic Modeling, Synthesis and Summarization, and Document Compilation and Report Generation.We reported the results using various metrics, including topic coherence, ROUGE scores, readability scores, and cosine similarity.</p>
<p>Readability Analysis.We evaluated the readability of the generated summaries and final LaTeX documents using the Flesch Reading Ease Score (FRES).The Flesch Reading Ease Score Kincaid (1975) provides insight into how easily a text can be read and understood.Higher FRES scores indicate simpler reading material, while lower scores denote more complex and challenging passages.We computed FRES at different stages of the summarization and document generation process to assess how readability changes as the content is processed through T5 summarization, GPT post-editing, and final document generation.</p>
<p>Metrics.</p>
<p>To evaluate the quality and robustness of the proposed framework, we used the following metrics:</p>
<p>• Topic coherence.Measures the semantic similarity between words in a topic, indicating how well the generated topics represent coherent and interpretable concepts.A higher coherence score suggests that the words within each topic are more closely related, making the topics more useful and understandable for further analysis (Rahimi et al, 2023).</p>
<p>• ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.It compares an automatically produced summary or translation against a set of reference summaries (typically human-produced).ROUGE evaluates various aspects, such as the overlap of n-grams, word sequences, and word pairs between the machine-generated output and the reference.</p>
<p>• ROUGE-1 measures the overlap of unigrams (single words) between the generated and reference abstracts.ROUGE-1 is particularly useful for evaluating summarization techniques because it captures the essential content and ensures that key information from the original text is retained in the summary.</p>
<p>• Precision for ROUGE-1 measures the fraction of relevant instances among the retrieved cases, indicating how much of the generated summary is present in the reference text, which is the abstract in our case.</p>
<p>• Recall for ROUGE-1 measures the fraction of instances retrieved over the total number of cases in the reference, indicating how much of the reference abstract is covered by the generated summary.</p>
<p>• F1-Score for ROUGE-1 is the harmonic mean of precision and recall, providing a balance between the two metrics.</p>
<p>• Cosine Similarity measures the similarity between two non-zero vectors of an inner product space, effectively capturing the semantic closeness between the generated text and the reference text.Cosine similarity was used to evaluate the semantic alignment of abstracts with expanded topics during the Systematic Search and Screening phase.</p>
<p>• Flesch Reading Ease Score (FRES) (Kincaid, 1975) provides insight into how easily a piece of text can be read and understood.The FRES formula considers sentence length and syllable count, with higher scores indicating simpler and more accessible text.We computed the FRES for three stages: T5-generated summaries, GPT post-edited sections, and the final LaTeX document.The formula is as follows: FRES = 206.835− 1.015 total words total sentences − 84.6 total syllables total words</p>
<p>This formula provides a measure of how easy a text is to read.Higher scores indicate easier-to-read material, while lower scores denote more difficult passages.</p>
<p>• Number of Papers Retrieved indicates the coverage of the search query and its ability to find relevant literature.</p>
<p>• Number of Papers Filtered reflects the number of papers that passed an initial relevance filter based on the research topic.</p>
<p>• Total CPU Time is the computational time required for generating queries, retrieving papers, and filtering results.</p>
<p>Hardware.Experiments were conducted on a Google Colab environment using an Intel Xeon CPU @ 2.20GHz (2 cores, 56MB cache), with 12.7 GB of RAM and 107.7 GB of disk space.</p>
<p>Experiment 1: Systematic Search and Screening</p>
<p>This experiment evaluated the effectiveness of GPT-3.5 and GPT-4o in generating queries for retrieving research papers from the arXiv repository.Given their capabilities in generating structured and contextually rich queries, we sought to compare the two models regarding their retrieval performance, efficiency, and computational cost.The experiment aimed to identify which model performs better across various research topics.</p>
<p>We selected five diverse research topics for this evaluation: Explainable Artificial Intelligence, Virtual</p>
<p>Results and Analysis.</p>
<p>Results are summarized in Table 1.GPT-4o consistently retrieved more papers than GPT-3.5 across all topics, indicating that GPT-4o generates comprehensive and relevant queries more effectively.For instance, GPT-4o retrieved 2833 papers for "Virtual Reality" compared to 1986 papers retrieved by GPT-3.5.Similarly, for "Explainable Artificial Intelligence," GPT-4o retrieved 1712 papers, surpassing the 1287 papers retrieved by GPT-3.5.</p>
<p>While GPT-4o demonstrated superior retrieval capability, it also required significantly more computational time than GPT-3.5.For instance, the "Explainable Artificial Intelligence" topic took 1555 seconds to process using GPT-4o, whereas GPT-3.5 completed the same task in 1213 seconds-a difference of nearly 6 minutes.Similarly, GPT-4o required 2115 seconds to process the "Large Language Models" topic, which is approximately 10 minutes longer than GPT-3.5.</p>
<p>These results suggest that GPT-4o is more effective at generating queries that yield a more extensive set of relevant papers, making it well-suited for scenarios where comprehensive literature coverage is a priority.However, this increased retrieval capability comes at the cost of longer computational time, making GPT-4o less ideal for scenarios where efficiency and speed are critical considerations.</p>
<p>In conclusion, GPT-4o is preferable for use cases prioritizing comprehensive retrieval over computational efficiency, while GPT-3.5 may be better for time-sensitive applications.This insight provides a basis for selecting the appropriate LLM based on the specific requirements of different phases in the systematic literature review process.</p>
<p>Experiment 2: Data Extraction and Topic Modelling</p>
<p>This experiment evaluated the quality of topics generated during the data extraction and topic modeling phase.The goal was to determine how well the BERTopic algorithm organized the retrieved literature into meaningful and coherent themes.</p>
<p>We used the topic coherence metric from Gensim to measure the quality of the generated topics.</p>
<p>Topic coherence quantifies the semantic similarity between words within a topic, indicating how well the topics represent coherent and interpretable concepts.This measure has been validated as a reliable method for assessing topic models in previous work by Röder et al (2015).Their study evaluated over 237,912 coherence measures across six benchmark datasets and demonstrated that specific combinations of coherence metrics correlate highly with human ratings, setting a standard for evaluating topic models.</p>
<p>Our experiment applied Gensim's implementation of the coherence metric to assess the topics generated from documents retrieved using GPT-3.5 and GPT-4o queries.This metric ensures that the topics produced are statistically sound and interpretable to human evaluators.</p>
<p>Results and Analysis.</p>
<p>This experiment assessed the semantic coherence of topics generated from the documents retrieved using GPT-3.5 and GPT-4o queries.As shown in Table 2, coherence scores for most topics fall between 0.4 and 0.5, indicating a moderate level of topic quality.This range suggests that the topics are generally coherent and interpretable but could be further refined.</p>
<p>For instance, the topic coherence score for "Explainable Artificial Intelligence" was 0.467 using GPT-3.5 queries and 0.422 using GPT-4o queries, indicating that both models produce moderately coherent topics.Similarly, for "Virtual Reality," GPT-4o achieved a coherence score of 0.481 compared to 0.434 by GPT-3.5, showing that GPT-4o produced slightly better-organized topics for this research area.Although these scores indicate that the generated topics are generally coherent, they are lower than previous benchmarks, such as the BERTopic model achieving scores of 0.681 and 0.432 on different datasets as reported by Rahimi et al (2023).This suggests that while our system can generate meaningful topics, there is potential for further improvements in topic coherence to match or exceed these higher benchmark scores.</p>
<p>Experiment 3: Synthesis and Summarization</p>
<p>This experiment assessed the performance of the Synthesis and Summarization phase of our automated literature review framework.We evaluated the quality of the generated summaries using ROUGE scores to determine their relevance and content retention.Additionally, the readability of each summary was analyzed using the Flesch Reading Ease metric.The primary goal was to determine how effectively the system condenses and synthesizes information from multiple research papers while maintaining coherence and relevance.</p>
<p>ROUGE Score Analysis.</p>
<p>We used the ROUGE-1 metric to compare the content overlap between the machine-generated summaries and the abstracts of the selected research papers, which served as reference texts.ROUGE-1 measures the degree of overlap in unigrams (single words) between the generated summaries and reference texts, making it suitable for evaluating content retention and relevance.</p>
<p>The evaluation was conducted in three stages:</p>
<p>Abstract Generated Summaries using T5: These serve as the baseline summaries generated by the T5 model, which captures the core content of the abstracts.</p>
<p>Post-Edited Generate Summaries using GPT: GPT-based models refine these summaries to enhance readability, coherence, and overall structure.</p>
<p>Document Compilation and Report Generation using LaTeX: Comprehensive sections formatted as LaTeX documents that integrate information from multiple summaries, providing a cohesive and structured literature overview.</p>
<p>We computed ROUGE-1 precision, recall, and F1 scores for each stage.While all three metrics provide valuable insights, we focused primarily on precision.High precision indicates that the summaries retain the most pertinent information from the reference abstracts, minimizing irrelevant details.</p>
<p>The results in Table 3 indicate that both GPT-3.5 and GPT-4o models achieved high precision (P) scores across all inputs, demonstrating that the generated summaries contain a significant proportion Despite this, the F1 scores, which balance precision and recall, show that the final documents maintain a strong balance between relevance and content coverage.GPT-4o generally achieved higher recall scores than GPT-3.5, suggesting it is more effective at incorporating additional relevant content while maintaining overall coherence.This makes GPT-4o particularly useful in scenarios where comprehensive literature coverage is essential.</p>
<p>While GPT-4o offers advantages in retaining more comprehensive content, GPT-3.5 remains a competitive option for generating concise and highly relevant summaries.Future efforts could focus on improving recall without sacrificing precision, allowing for even more comprehensive and well-rounded literature review sections.</p>
<p>Experiment 4: Readability Score</p>
<p>We used the Flesch Reading Ease Score to evaluate the readability of the generated summaries and final documents at different stages of the document generation process.This metric provides insights into how accessible the text is to a general audience, with higher scores indicating easier-to-read content.</p>
<p>Readability was evaluated for the T5-generated summaries, GPT post-edited summaries, and the final LaTeX documents.</p>
<p>Table 6 outlines the interpretation of Flesch Reading Ease scores, with lower scores indicating text that requires a higher level of education to comprehend.In our evaluation, we compared these scores across the stages of the automated SLR process to assess how the readability evolved from the initial summarization to the final document creation.</p>
<p>Table 7 presents the Flesch Reading Ease scores for each stage of the document generation process.</p>
<p>These scores provide an overview of how readability changes as the content is transformed from initial summaries to refined, structured documents.</p>
<p>T5-Generated Summaries exhibit low readability scores, indicating that the content is quite challenging to read.This outcome is expected due to the highly condensed nature of the T5-generated summaries, which prioritize brevity over readability, often lacking the narrative structure required for easier comprehension.</p>
<p>The readability of the summaries significantly improves after the GPT Post-Edited Sections.The post-editing process refines the content by enhancing clarity, improving sentence structure, and providing a more coherent flow.This results in more accessible and readable sections, reflected in the enhanced Flesch scores.</p>
<p>Final LaTeX Generated Documents show further improvements in readability.The additional structuring, formatting, and content synthesis contribute to easier-to-read documents than the earlier stages.However, while the readability has improved, it remains lower than the baseline.</p>
<p>Baseline Summaries exhibit the highest readability scores, demonstrating that maintaining readability while summarizing and synthesizing content remains a challenge.The baseline scores highlight the gap between the generated summaries and the clarity of the original abstracts.</p>
<p>The Flesch Reading Ease Scores improved progressively from T5-generated summaries to GPT postedited sections and final LaTeX documents.However, despite these improvements, the readability of the generated documents remains lower than that of the original abstracts.This outcome underscores the challenge of maintaining high readability while compressing and synthesizing content, particularly in automated systems.</p>
<p>Experiment 5: Sentence Similarity</p>
<p>To further evaluate the effectiveness of our automated systematic literature review (SLR) system, we computed cosine similarity scores at various stages of document generation.This analysis quantifies how closely the generated summaries and final documents align with the original input queries, providing a measure of content retention and relevance.For comparison, a baseline (Random) was included, representing a document generated with random words, to serve as a control.</p>
<p>Cosine similarity measures the cosine of the angle between two vectors in a multidimensional space-here, these vectors represent text embeddings derived from the documents.Higher cosine similarity scores indicate greater alignment between the generated texts and the original input queries.it is better at understanding and incorporating relevant content throughout the document generation process.</p>
<p>Random Baseline: As expected, the cosine similarity scores for the randomly generated document are very low.This serves as a control, validating the significance of the similarity scores observed for the generated summaries and final documents.</p>
<p>Overall, the system demonstrates robustness in generating documents that remain closely aligned with the original input queries, ensuring that the synthesized literature reviews preserve essential information while improving readability and structure.</p>
<p>Experiment 6: Finding the Optimal Number of Papers for SLR</p>
<p>This section explores several key performance metrics to determine the optimal number of papers to include in the SLR process.The metrics analyzed include CPU time, number of topics identified, topic coherence, ROUGE scores, readability scores, and cosine similarity scores.These metrics are used to assess the impact of different document limits on the quality and efficiency of the SLR.We recommend the most effective document limit that balances performance and computational resources based on the analysis results. Figure 2 presents the results obtained.</p>
<p>CPU Time. Figure 2-(a) shows the CPU time shows how computational requirements scale with the number of documents processed.As the document count increases, CPU time rises significantly, with GPT-4o consistently requiring more time than GPT-3.5.This indicates that although GPT-4o can potentially offer more accurate results, it demands more computational resources, which is a trade-off to consider when processing large volumes of documents.</p>
<p>Number of Topics Found.In Figure 2-(b), BERTopic identifies an increasing number of topics as more documents are processed.GPT-4o consistently identifies more topics than GPT-3.5 across all document limits.This suggests that GPT-4o is more adept at detailed clustering, potentially offering a more nuanced breakdown of the literature.However, after a certain threshold, the increase in topics may not necessarily translate to better quality but rather more fragmented groupings.</p>
<p>Topic Coherence.The Topic Coherence metric measures the semantic similarity within the topics identified, providing insight into the quality of the generated clusters.Figure 2-(c) illustrates the quality of the topics generated based on the semantic similarity of words within them.GPT-3.5 and GPT-4o maintain relatively stable topic coherence scores of up to 200 documents.Beyond this point, coherence begins to drop slightly for both models, likely due to overfitting or noise introduced by an excessive number of documents.This reinforces that 200 documents strike an optimal balance between quality and quantity regarding topic coherence.</p>
<p>ROUGE Scores for Summarization Quality.The ROUGE scores measure how well the generated summaries align with reference abstracts, focusing on content retention.Figure 2 -(d) shows that as the number of documents increases, the ROUGE scores improve, peaking around 200.This suggests that the system becomes better at generating summaries that capture the core content of the papers as more documents are processed.However, beyond the 200-document threshold, the improvement in ROUGE scores plateaus, indicating that additional documents do not contribute significantly to better summarization.This implies that while increasing the document count improves the system's ability to summarize effectively, there is little benefit to going beyond 200 documents regarding content retention and quality.</p>
<p>Cosine Similarity for Content Alignment.The Cosine Similarity scores measure how closely the generated documents align with the input queries, indicating relevance and focus.Figure 2-(e) shows that GPT-3.5 and GPT-4o achieve high similarity scores across all document limits, stabilizing around 200 documents.This indicates that 200 documents provide sufficient information to produce outputs wellaligned with the original research query without overwhelming the system with excess data.The plateau in similarity scores beyond this threshold suggests that additional documents do not significantly enhance the relevance of the generated summaries.Therefore, 200 documents appear to be the most efficient choice for maintaining high alignment with the research objectives while minimizing computational overhead.</p>
<p>Readability Scores.We use the Flesch Reading Ease (Kincaid, 1975) metric to evaluate how accessible and easy to read the generated summaries are. Figure 2-(f) indicates that the readability scores increase as more documents are processed, reaching their highest point, around 200.This suggests that the generated summaries become clearer and easier to read as the system processes more documents, possibly due to having a more comprehensive pool of content to draw from.However, readability scores decline slightly after 200 documents, indicating that the system might introduce more complex or fragmented language as the document count grows.This highlights that 200 documents offer the best balance for generating summaries that are both informative and easy to read.</p>
<p>Optimal Number of Papers Based on the analysis of the above metrics, 200 documents emerge as the optimal document limit for the SLR process.At this threshold, the system provides high-quality summaries, maintains strong topic coherence, and produces readable and relevant outputs without excessive computational resources.Using over 200 documents leads to diminishing returns, particularly regarding topic coherence, readability, and cosine similarity.Thus, we recommend 200 documents as the ideal balance between performance and efficiency for conducting automated systematic literature reviews.</p>
<p>Discussion</p>
<p>The results presented in this study demonstrate the potential of the proposed automated SLR framework to streamline and enhance the process of conducting literature reviews.By integrating advanced NLP techniques and LLMs such as GPT-3.5 and GPT-4o, the framework automates systematic search, data extraction, topic modeling, and summarization stages.However, a critical analysis of the results reveals both strengths and areas for improvement.</p>
<p>GPT-4o retrieves more papers than GPT-3.5.The experiments revealed that GPT-4o consistently outperformed GPT-3.5 in retrieving a larger number of papers across all research topics.This suggests that GPT-4o is better at generating more comprehensive and contextually rich search queries.The ability of GPT-4o to retrieve more papers is beneficial in scenarios where exhaustive literature coverage is important, such as systematic reviews and meta-analyses, as it ensures that a wider array of relevant research is considered.However, this improved retrieval capacity may also introduce more irrelevant or significantly enhanced the clarity and coherence of the summaries, making them easier to read and understand.However, despite these improvements, the readability of the generated documents remained lower than the baseline abstracts.This outcome reflects the inherent difficulty in maintaining high readability while condensing and synthesizing technical content.Future work could explore more advanced techniques to improve readability, especially in the post-editing phase, to close the gap with the original abstracts.</p>
<p>Cosine similarity confirms robust content retention.Cosine similarity scores across all stages of document generation were high, confirming that the system retained key content from the original input queries.The post-editing and final document generation stages further improved content alignment, particularly with GPT-4o, which generally outperformed GPT-3.5 in maintaining content relevance.These results suggest that both models effectively ensure the generated summaries and documents stay focused on the core topics of the input queries, making them reliable tools for systematic literature reviews.The consistently high similarity scores also validate the robustness of retrieval, summarization, and synthesis, ensuring that essential information is not lost throughout the stages.</p>
<p>The findings from this study underscore the utility of combining GPT models and NLP techniques to automate key phases of systematic literature reviews, from retrieval to summarization.While GPT-4o demonstrates superior performance in content retrieval and recall, GPT-3.5 remains competitive for tasks prioritizing efficiency and conciseness.The framework shows promise in automating extensive literature reviews with relatively high precision and robust content retention.However, challenges remain, particularly in optimizing readability and balancing recall with document complexity.Future work should focus on refining the post-editing processes, improving the coherence and accessibility of generated documents, and ensuring that the system remains adaptable to diverse academic domains.Enhancing the framework's ability to filter irrelevant or lower-quality content will strengthen its applicability in high-demand, resource-intensive reviews.</p>
<p>Ethical Considerations and Limitations</p>
<p>A key limitation of this study is that it only analyzed proprietary models, specifically OpenAI's GPT-3.5</p>
<p>and GPT-4o, and did not include open-source models like LLaMA or Falcon.This is important because open-source models are becoming increasingly popular for research and practical applications due to their accessibility and customization potential.By focusing only on proprietary models, this study misses the opportunity to evaluate the performance, bias mitigation strategies, and transparency advantages that open-source models may offer.</p>
<p>System: "You are a skilled research assistant specializing in crafting precise and effective search queries for the arXiv scientific paper repository."User: "Task: Craft an effective search query tailored for the arXiv database, specifically designed to retrieve research papers on the following topic: Topic: 'expanded title' Guidelines:1.Concise &amp; Precise: The query should be succinct yet accurately represent the core concept of the topic.2.Key Terms: Incorporate the most relevant keywords or phrases directly associated with the topic.3. Synonyms &amp; Variants (Optional): If applicable, include synonyms or alternative terms to broaden the search scope and capture nuanced variations of the topic.4. Specificity: Prioritize terms specific to the field or subfield to minimize irrelevant results. 5. arXiv Compatibility: Utilize operators like 'ti:' (title) and 'abs:' (abstract) to target specific fields within the arXiv entries.Output format: * Provide the ArXiv query only.Do not include any additional explanations or commentary."</p>
<p>System: "You are an experienced researcher specializing in literature reviews.You are adept at crafting concise, informative, and engaging topic names for subsections that accurately reflect the content and guide the reader."User: "Task: Create a clear and concise topic name for a subsection in a literature review.The subsection covers the following keywords: topic keywords:" Guidelines: * Length: Aim for 1-5 words.<em> Accuracy: Ensure the topic name precisely reflects the keywords' meaning.</em> Relevance: The name should fit within the broader context of a literature review.<em> Informativeness: Clearly indicate the subsection's focus to the reader.</em> Engagement: Make the topic name interesting and inviting to read.Optional: If the keywords are too broad or ambiguous, suggest a more specific or narrowed-down focus within the topic.Output format: * Provide the topic title only.Do not include any additional explanations or commentary.</p>
<p>Reality, Blockchain, Large Language Models, and Neural Machine Translation.For each topic, we measured three key performance indicators: Number of Papers Retrieved, Number of Papers Filtered, and CPU Time.</p>
<p>Fig. 2
2
Fig. 2 Performance metrics across different document limits for GPT-3.5 and GPT-4o in the SLR process.(a) CPU Time: GPT-4o consistently requires more time than GPT-3.5 as the number of documents increases, reflecting its computational complexity.(b) Number of Topics: GPT-4o identifies more topics, indicating a finer level of clustering.(c) Topic Coherence: Coherence is stable up to 200 documents for both models, but it declines as more documents are added, suggesting overfitting or noise.(d) ROUGE Scores: Summarization quality improves and plateaus around 200 documents.(e) Cosine Similarity: Both models show stable alignment with input queries, with diminishing returns beyond 200 documents.(f) Readability Scores: Readability peaks around 200 documents before declining, suggesting this as the optimal limit for accessible summaries.</p>
<p>Table 1
1
Comparison of GPT-3.5 and GPT-4o in finding papers for the SLR process.CPU time indicates the total time for the entire automated SLR process.
InputModelCPU Time (s) Papers FoundExplainable Artificial IntelligenceGPT-3.5 GPT-4o1213 15551287 1712Virtual RealityGPT-3.5 GPT-4o1319 14961986 2833BlockchainGPT-3.5 GPT-4o1476 15633000 3000Large Language ModelsGPT-3.5 GPT-4o1505 21151400 3000Neural Machine TranslationGPT-3.5 GPT-4o1648 16732018 2073</p>
<p>Table 2
2
Topic coherence analysis using Gensim's topic coherence metric for GPT-3.5 and GPT-4o generated queries
InputModelTopic Coherence Number of TopicsExplainable Artificial IntelligenceGPT-3.5 GPT-4o0.467 0.4225 6Virtual RealityGPT-3.5 GPT-4o0.434 0.4818 8BlockchainGPT-3.5 GPT-4o0.411 0.4758 5Large Language ModelsGPT-3.5 GPT-4o0.428 0.4735 5Neural Machine TranslationGPT-3.5 GPT-4o0.477 0.4706 7</p>
<p>Table 6
6
(Kincaid, 1975;of Flesch Reading Ease Scores(Kincaid, 1975; Wikipedia, 2024)
ScoreSchool Level (US) Notes100.00-90.005th gradeVery easy to read. Easily understood by an average 11-year-old student.90.00-80.006th gradeEasy to read. Conversational English for consumers.80.00-70.007th gradeFairly easy to read.70.00-60.008th &amp; 9th gradePlain English. Easily understood by 13-to 15-year-old students.60.00-50.0010th to 12th gradeFairly difficult to read.50.00-30.00CollegeDifficult to read.30.00-10.00College graduateVery difficult to read. Best understood by university graduates.10.00-0.00ProfessionalExtremely difficult to read. Best understood by university graduates.</p>
<p>Table 7
7
Readability Scores for T5-generated Summaries, GPT Post-Edited Sections, and GPT-generated Final LaTeX Document</p>
<p>Table3ROUGE-1 Scores for the T5-generated Summaries (P = Precision, R = Recall, F1 = F-measure), with the selected abstracts as referenceInputModel ROUGE P R F1Explainable Artificial Intelligence GPT-3.5 0.963 0.405 0.570 GPT-4o 0.964 0.387 0.552Virtual Reality GPT-3.5 0.967 0.418 0.583 GPT-4o 0.969 0.425 0.591 Blockchain GPT-3.5 0.967 0.401 0.567 GPT-4o 0.968 0.400 0.567Large Language Models GPT-3.5 0.966 0.376 0.540 GPT-4o 0.965 0.381 0.546Neural Machine Translation GPT-3.5 0.965 0.462 0.625 GPT-4o 0.965 0.460 0.623 of relevant content present in the reference abstracts.However, the recall scores are relatively lower, reflecting that not all content from the reference abstracts is captured in the summaries.This is expected since we want to capture only the relevant information in the abstracts.In this table, there is no significant benefit in choosing GPT-4o instead of GPT-3.5, as the T5 model computes the summary.GPT, at this stage, was only used to gather documents by creating the ArXiv query.Post-Editing Stage ResultsThe post-editing phase is crucial in refining the machine-generated summaries produced by the T5 model.This stage utilizes GPT-based models to enhance the initial summaries' clarity, coherence, and structure.The objective is to condense and reorganize the content while preserving the most relevant information.Post-editing is essential for transforming raw summaries into well-structured sections that align with the broader context of an SLR.Table4presents the ROUGE-1 scores for the summaries after being refined by GPT-based models.The results show a clear drop in recall after post-editing compared to the initial T5-generated summaries, reflecting the focus on refining and condensing content.This reduction is expected as the goal is to produce well-structured, concise sections for the systematic literature review (SLR).Despite the decrease in recall, precision scores remain high, ensuring the retained information is relevant and concise.GPT-4o demonstrates higher recall than GPT-3.5, indicating its ability to retain more content during post-editing.However, the F1 scores, which balance precision and recall, show only a slight advantage for GPT-4o, suggesting that both models perform similarly in maintaining a good balance between content relevance and retention.Post-editing with GPT significantly improved the precision of the summaries, ensuring that the most relevant information was retained, even though recall slightly decreased.GPT-4o showed a slight edge in content retention, making it more suitable for comprehensive literature reviewsFinal LaTeX Document EvaluationThe final LaTeX documents generated by the system were evaluated to understand their effectiveness in creating cohesive literature review sections that integrate information from multiple sources.As presented in Table5, both GPT-3.5 and GPT-4o achieved exceptionally high precision scores (e.g., 0.991 for GPT-3.5 and 0.989 for GPT-4o in the "Explainable Artificial Intelligence" topic), indicating that the final documents are highly aligned with the reference abstracts in terms of relevance.However, the recall scores for these documents were relatively low, which is expected given that the final LaTeX documents are designed to provide comprehensive literature review sections, not direct summaries of the abstracts.These documents incorporate additional background information, contextual insights, and synthesized content from various sources, which broadens the scope and naturally reduces recall scores.Abstract Filtering.The filtered abstracts generated by GPT-3.5 and GPT-4o exhibit high cosine similarity scores, demonstrating that the initial search and screening phase effectively identifies documents closely related to the input queries.Both models perform well at this stage, confirming the robustness of the search process.T5-Generated Summaries.The cosine similarity scores decrease slightly for the T5-generated summaries, which is expected.Summarization inherently condenses content and may omit some details, leading to a lower similarity with the full abstracts.However, the core information relevant to the input query remains retained, ensuring that the generated summaries focus on the main topics.GPT Post-Edited Summaries.The cosine similarity scores increase after the post-editing process by GPT.This improvement suggests that the GPT-based post-editing refines the structure and readability and enhances alignment with the original input.The post-editing process ensures that the key content is retained while improving the coherence of the generated sections.GPT-4o generally outperforms GPT-3.5 in maintaining content similarity, indicating that GPT-4o is more effective at preserving relevant information.Final LaTeX Documents.The final documents generated by GPT continue to exhibit high similarity scores, indicating that the synthesis and summarization process effectively retains the relevance of the content.The structured nature of LaTeX documents ensures that core themes from the input queries are well-represented.GPT-4o again shows slightly better performance than GPT-3.5, further suggesting that low-quality papers, necessitating more robust filtering mechanisms.High ROUGE-1 precision scores for both models.Both GPT-3.5 and GPT-4o demonstrated high ROUGE-1 precision scores during the summarization phase, indicating that the generated summaries retained a significant amount of relevant content from the reference abstracts.This suggests that the models effectively focus on the most important information when creating summaries, which is critical in systematic reviews where maintaining the relevance of summarized content is paramount.However, the relatively lower recall scores reflect that some content was omitted during summarization, which may be intentional to avoid overwhelming the reader with excessive detail.The high precision with lower recall suggests a bias toward conciseness, which can be advantageous in certain contexts but may require adjustment depending on the goals of the review.Post-editing improved precision but reduced recall.The post-editing phase significantly improved the precision of the generated summaries but reduced recall, indicating that while the content became more concise and focused, some relevant details were omitted.This aligns with the goal of post-editing, which is to refine and streamline the summaries for clarity and coherence.GPT-4o demonstrated higher recall than GPT-3.5 in this phase, suggesting it more effectively retained content during post-editing.This slight advantage highlights GPT-4o's ability to balance relevance and conciseness better, making it more suitable for generating comprehensive yet readable summaries in systematic reviews.GPT-4o achieved higher recall in final LaTeX documents.The final LaTeX documents generated by GPT-4o achieved higher recall scores than those generated by GPT-3.5, indicating that GPT-4o was more successful in incorporating additional relevant content while maintaining coherence.This makes GPT-4o particularly advantageous for use cases that require comprehensive literature coverage, as the final documents generated by GPT-4o were better at synthesizing information from multiple sources.However, this increase in recall may come at the cost of readability, as the additional content could make the final documents more complex and challenging to navigate.Future work could explore optimizing the balance between recall and readability in final document generation.Readability improved through post-editing and final document generation.The Flesch ReadingEase Scores demonstrated a clear improvement in readability from the initial T5-generated summaries to the GPT post-edited sections and final LaTeX documents.This suggests that the post-editing processBias in Literature Selection.AI models can introduce bias into the literature selection process.These models are trained on large, potentially unbalanced datasets, which can skew the selection towards more popular or well-represented topics, ignoring less-covered research areas.Future work should explore ways to address this bias, possibly through fairness-aware algorithms or more inclusive data sources.Hallucination and Misinformation.Both GPT-3.5 and GPT-4o are prone to generating content that is not directly grounded in the input data, which can lead to inaccurate summaries.This is particularly risky in a systematic literature review where factual accuracy is critical.Adding validation steps, such as human review or factual grounding mechanisms, would help mitigate this issue.Exclusion of Proprietary Models.By not including freely available and modifiable models likeLLaMA or Falcon, this study does not address how open-source solutions could improve transparency, reproducibility, and control in the SLR process.These models offer the potential for better alignment with specific research needs and ethical considerations such as data privacy and bias control.Scalability.Although the system performed well with up to 3000 papers per query, larger datasets may introduce computational challenges.Future iterations should improve scalability, possibly by adopting distributed computing methods.Future studies should incorporate open-source models to compare their effectiveness and address the broader needs of the academic community, offering more flexibility, transparency, and cost control.ConclusionsThis study proposed PROMPTHEUS, an automated SLR framework that integrates advanced NLP techniques and large language models to streamline the literature review process.By automating systematic search, data extraction, topic modeling, and synthesis, the framework effectively manages the growing volume of academic literature.Our experiments across five research topics demonstrated the system's strengths in selecting relevant papers, retaining key content, and improving readability in post-editing stages.However, areas like recall and topic coherence still require improvement.Despite achieving high precision, lower recall scores, and moderate topic coherence, these metrics suggest that some relevant content was omitted and topics could be better organized.GPT-4o outperformed GPT-3.5 in recall and content retention.However, readability, though improved in post-editing, remained below the clarity of the original abstracts.In conclusion, PROMPTHEUS contributes to automating systematic literature reviews by combining advanced NLP techniques and large language models.Our framework addresses the growing challenges of managing vast academic literature by streamlining critical processes while maintaining high precision and content relevance.However, the trade-offs between precision, recall, and readability underscore further refinement, particularly in improving topic coherence and ensuring that the most relevant content is consistently included.Looking ahead, the future potential of PROMPTHEUS is vast.We will continue to optimize these elements while incorporating open-source models to improve flexibility, transparency, and scalability, empowering the academic community to more effectively manage the ever-rapidly-growing body of research with enhanced comprehensiveness and efficiency.AcknowledgmentsCompeting interestsThe authors declare that they have no competing interests.
An exploration of available methods and tools to improve the efficiency of systematic review production: a scoping review. L Affengruber, M M Van Der Maten, I Spiero, BMC Medical Research Methodology. 2412102024a</p>
<p>Rapid review methods series: Guidance on the use of supportive software. L Affengruber, B Nussbaumer-Streit, C Hamel, BMJ evidence-based medicine. 2024b</p>
<p>F Bolanos, A Salatino, F Osborne, Artificial intelligence for literature reviews: Opportunities and challenges. 2024</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, 2020. 200514165</p>
<p>Machine learning for accelerating screening in evidence reviews. M Chappell, M Edwards, D Watkins, Cochrane Evidence Synthesis and Methods. 15e120212023</p>
<p>Slowed canonical progress in large fields of science. Jsg Chu, J A Evans, 10.1073/pnas.2021636118Proceedings of the National Academy of Sciences. 118412021</p>
<p>Machine learning computational tools to assist the performance of systematic reviews: A mapping review. Cierco Jimenez, R Lee, T Rosillo, N , BMC Medical Research Methodology. 2213222022</p>
<p>Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain. F Dennstädt, J Zink, P M Putora, Systematic Reviews. 1311582024</p>
<p>Decoding semi-automated title-abstract screening: findings from a convenience sample of reviews. A Gates, M Gates, D Darosa, Systematic reviews. 92020</p>
<p>Rapid review: A review of methods and recommendations based on current evidence. Q Guo, G Jiang, Q Zhao, Journal of Evidence-Based Medicine. 2024</p>
<p>Guidance for using artificial intelligence for title and abstract screening while conducting knowledge syntheses. C Hamel, M Hersi, S E Kelly, BMC Medical Research Methodology. 212021</p>
<p>Artificial intelligence in systematic literature reviews: a case for cautious optimism. S Kharawala, A Mahajan, P Gandhi, Journal of Clinical Epidemiology. 1382432021</p>
<p>Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Chief of Naval Technical Training Li M, Sun J, Tan X (2024) Evaluating the effectiveness of large language models in abstract screening: a comparative analysis. J Kincaid, Systematic Reviews. 1312191975</p>
<p>Natural language processing (nlp) to facilitate abstract review in medical research: the application of biobert to exploring the 20-year use of nlp in medical research. S Masoumi, H Amirkhani, N Sadeghian, Systematic Reviews. 1311072024</p>
<p>A novel application of machine learning and zeroshot classification methods for automated abstract screening in systematic reviews. C F Moreno-Garcia, Jayne C Elyan, E , Decision Analytics Journal. 61001622023</p>
<p>Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: a comprehensive review. R Ofori-Boateng, M Aceves-Martins, N Wiratunga, Artificial intelligence review. 5782002024</p>
<p>A guide to conducting a standalone systematic literature review. C Okoli, 10.17705/1CAIS.03743Communications of the Association for Information Systems. 372015</p>
<p>A question of trust: can we build an evidence base to gain trust in systematic review automation technologies?. A M O'connor, G Tsafnat, J Thomas, Systematic reviews. 82019</p>
<p>The prisma 2020 statement: an updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, 10.1136/bmj.n71BMJ. 712021</p>
<p>A real-world evaluation of the implementation of nlp technology in abstract screening of a systematic review. S Perlman-Arrow, N Loo, N Bobrovitz, Research Synthesis Methods. 1442023</p>
<p>Are chatgpt and large language models "the answer" to bringing us closer to systematic review automation?. R Qureshi, D Shaughnessy, K A Gill, Systematic Reviews. 121722023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, 2023. 191010683</p>
<p>. H Rahimi, J L Hoover, D Mimno, metrics. 2305.145872023</p>
<p>hasta la vista, baby"-will machine learning terminate human literature reviews in entrepreneurship. S Robledo, Grisales Aguirre, A M Hughes, M , Journal of Small Business Management. 6132023</p>
<p>Exploring the space of topic coherence measures. M Röder, A Both, A Hinneburg, Proceedings of the eight International Conference on Web Search and Data Mining. the eight International Conference on Web Search and Data MiningShanghai2015. February 2-6</p>
<p>Systematic review using a spiral approach with machine learning. A Saeidmehr, Pdg Steel, F F Samavati, 10.1186/s13643-023-02421-zSystematic Reviews. 131322024</p>
<p>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. A M Sami, Z Rasheed, K K Kemell, 2024</p>
<p>Appraising systematic reviews: a comprehensive guide to ensuring validity and reliability. N Shaheen, A Shaheen, A Ramadan, 10.3389/frma.2023.1268045Frontiers in Research Metrics and Analytics. 82023</p>
<p>Methodological guidance for rapid reviews in healthcare: a scoping review. C Speckemeier, A Niemann, J Wasem, Research synthesis methods. 1342022</p>
<p>Artificial intelligence to automate the systematic review of scientific literature. J De La Torre-López, A Ramírez, J R Romero, 10.1007/s00607-023-01181-xComputing. 105102023</p>
<p>Automation of systematic reviews of biomedical literature: a scoping review of studies indexed in pubmed. B Tóth, L Berek, L Gulácsi, Systematic Reviews. 1311742024</p>
<p>M Ware, M Mabe, The stm report: An overview of scientific and scholarly journal publishing Wikipedia (2024) Flesch-Kincaid readability tests -Wikipedia, the free encyclopedia. 2015</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5911 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5911</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5911</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-119.html">extraction-schema-119</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-ed3682e646acb412823d60f0b7c736398ecb9b38</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ed3682e646acb412823d60f0b7c736398ecb9b38" target="_blank">POLYIE: A Dataset of Information Extraction from Polymer Material Scientific Literature</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work evaluates state-of-the-art named entity extraction and relation extraction models on POLYIE, analyze their strengths and weaknesses, and highlight some difficult cases for these models.</p>
                <p><strong>Paper Abstract:</strong> Scientific information extraction (SciIE), which aims to automatically extract information from scientific literature, is becoming more important than ever. However, there are no existing SciIE datasets for polymer materials, which is an important class of materials used ubiquitously in our daily lives. To bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer materials. POLYIE is curated from 146 full-length polymer scholarly articles, which are annotated with different named entities (i.e., materials, properties, values, conditions) as well as their N-ary relations by domain experts. POLYIE presents several unique challenges due to diverse lexical formats of entities, ambiguity between entities, and variable-length relations. We evaluate state-of-the-art named entity extraction and relation extraction models on POLYIE, analyze their strengths and weaknesses, and highlight some difficult cases for these models. To the best of our knowledge, POLYIE is the first SciIE benchmark for polymer materials, and we hope it will lead to more research efforts from the community on this challenging task. Our code and data are available on: https://github.com/jerry3027/PolyIE.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5911",
    "paper_id": "paper-ed3682e646acb412823d60f0b7c736398ecb9b38",
    "extraction_schema_id": "extraction-schema-119",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00535675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PolyIE: A Dataset of Information Extraction from Polymer Material Scientific Literature</h1>
<p>Jerry Junyang Cheung ${ }^{1 <em>}$, Yuchen Zhuang ${ }^{1 </em>}$, Yinghao $\mathbf{L i}^{1}$, Pranav Shetty ${ }^{2}$, Wantian Zhao ${ }^{1}$, Sanjeev Grampurohit ${ }^{1}$, Rampi Ramprasad ${ }^{2}$, Chao Zhang ${ }^{1}$<br>${ }^{1}$ College of Computing, ${ }^{2}$ School of Materials Science and Engineering Georgia Institute of Technology, Atlanta, USA<br>{jzhang3027, yczhuang,yinghaoli, pranav.shetty, wzhao306,sgrampurohit3}@gatech.edu rampi.ramprasad@mse.gatech.edu, chaozhang@gatech.edu</p>
<h4>Abstract</h4>
<p>Scientific information extraction (SciIE), which aims to automatically extract information from scientific literature, is becoming more important than ever. However, there are no existing SciIE datasets for polymer materials, which is an important class of materials used ubiquitously in our daily lives. To bridge this gap, we introduce PolyIE, a new SciIE dataset for polymer materials. PolyIE is curated from 146 full-length polymer scholarly articles, which are annotated with different named entities (i.e., materials, properties, values, conditions) as well as their $N$-ary relations by domain experts. PolyIE presents several unique challenges due to diverse lexical formats of entities, ambiguity between entities, and variable-length relations. We evaluate state-of-the-art named entity extraction and relation extraction models on PolyIE, analyze their strengths and weaknesses, and highlight some difficult cases for these models. To the best of our knowledge, PolyIE is the first SciIE benchmark for polymer materials, and we hope it will lead to more research efforts from the community on this challenging task. Our code and data are available on: https://github.com/jerry3027/PolyIE.</p>
<h2>1 Introduction</h2>
<p>Material science literature is growing at an unprecedented rate. For example, a simple search on Google Scholar with the term "polymers" returns more than 5 million articles on polymer materials. Such literature reports valuable information on the latest advances in material science, ranging from experimental material properties to material synthesis recipes and procedures. As machine learning (ML) has achieved success in different applications of material science (Butler et al., 2018; Schmidt et al., 2019), Scientific Information Extraction (SciIE) from literature for supporting various</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of entity and relation annotations in PolyIE from a material science paper (Shi et al., 2011), including entity mentions as well as intrasentence and inter-sentence N -ary relations.
tasks is becoming increasingly important. Automatically extracting structured information about materials from massive unstructured literature data can be invaluable to understanding material properties and synthesis, as well as building data-driven ML tools for material discovery (Court et al., 2021; Doan Tran et al., 2020).</p>
<p>While SciIE has rapidly developed in domains such as biomedical science (Luan et al., 2018; Gábor et al., 2018; Jain et al., 2020; Hou et al., 2019; Jia et al., 2019; Shi et al., 2024) and chemistry (He et al., 2021; Fang et al., 2021; Kim et al., 2023), it has made limited progress in the material science domain. So far, there are only a handful of datasets for material information extraction. Some earlier works use ChemDataExtractor (Swain and Cole, 2016) to automatically generate datasets for battery materials (Huang and Cole, 2020) and temperatures (Court and Cole, 2018). More recent datasets are created manually for solid oxide fuel cells (Friedrich et al., 2020) and material science synthesis procedures (O'Gorman et al., 2021). However, none of these datasets cover polymer</p>
<p>materials, which are an important class of organic materials that play critical and ubiquitous roles in our daily lives. Due to their versatile properties, polymer materials are being widely used in applications such as packaging, coating, energy saving, and medical applications (Wu et al., 2021). As vast amounts of information on polymer development are being reported in literature data, there is a critical need for SciIE benchmarks and tools to harvest such information from the polymer literature.</p>
<p>To address this gap, we construct a dataset for extracting polymer property knowledge from unstructured literature data. Our dataset POLYIE is curated from 146 full-length polymer scientific articles, which are annotated by domain experts with named entities (i.e., materials, properties, values, conditions) as well as the $N$-ary relations among them (see Figure 1). PolyIE contains 41635 entity mentions and 4443 relations in total. It covers four different application domains of polymer materials: polymer solar cells (PSC), ring-opening polymerization (RP), polymer membranes (PM), and polymers in lithium-ion batteries (LB). This diversity of content enables the training of models with enhanced generalization capabilities. To the best of our knowledge, POLYIE is the first benchmark for SciIE from full-text polymer literature.</p>
<p>From the natural language processing perspective, extracting information for polymers on POLYIE introduces unique challenges for both named entity recognition and relation extraction:
Diverse Lexical Formats of Entities. Polymerrelated entities often have different schemes of nomenclature, such as IUPAC names (e.g., 'poly(3hexylthiophene)'), abbreviations ('PDPPNBr'), trade names ('Styron'), common names ('ABS plastic'), and sample labels ('PE-HDPE-01'). In addition, the identification of polymers can also be achieved through the concatenation of homopolymer names with hyphens or slashes, and the inclusion of numerical values for the component ratios and molecular weights ('PVC-PS-PC-20/30/50800000'). This diversity of nomenclature in literature poses a challenge for named entity recognition.
Variable-length and Cross-Sentence $N$-ary relations. Previous research on relation extraction has focused on either binary relations (Luan et al., 2018; Yao et al., 2019) or $N$-ary relations with a fixed number $N$ (Jia et al., 2019; Jain et al., 2020; Zhuang et al., 2022). In contrast, many relations described in polymer literature are variable-length
$N$-ary relations. This is because 1) the reported properties may be describing one or several materials; and 2) different properties can be measured under specific conditions. Furthermore, the elements in a relation tuple may span multiple sentences as shown in Figure 1.</p>
<p>We study seven mainstay NER, five $N$-ary RE models, and two LLM-based methods on POLYIE in terms of their overall performance and sample efficiency. We find that the models based on domainspecific pre-trained models (e.g., MaterialsBERT) yield better performance than other baselines. However, all the models struggle with accurately recognizing certain categories of named entities and inferring challenging varied-length $N$-ary relations. Moreover, our observations indicate that, under few-shot settings, the recently popular large language models (LLMs) demonstrate inferior performance than the other baselines on POLYIE, highlighting potential limitations in comprehending material science concepts.</p>
<p>Our main contributions are: (1) The first polymer information extraction dataset curated from 146 full-length articles for polymer named entity recognition and relation extraction. (2) Thorough evaluation of seven mainstream NER, five $N$-ary RE, and two LLM-based models on our curated dataset. (3) Analysis of the difficult cases and limitations of existing models, which we hope will enable future research on this challenging task from the NLP community.</p>
<h2>2 Related Work</h2>
<p>Material Science NLP Datasets. Earlier studies (Court and Cole, 2018) leverage tools such as ChemDataExtractor (Swain and Cole, 2016), ChemSpot (Rocktäschel et al., 2012), and ChemicalTagger (Hawizy et al., 2011) to perform NER annotation for dataset curation. For example, ChemDataExtractor is applied to generate datasets for Curie and Neel magnetic phase transition temperatures (Court and Cole, 2018) and magnetocaloric materials (Court et al., 2021). Besides, people also create expert-annotated datasets (Wang et al., 2021; Weston et al., 2019) for the extraction of non-value named entities (e.g., material and property names) and their relationships. In recent years, there has been an uptick in efforts to include numerical values in datasets for further extraction, with several studies closely related to POLYIE: Friedrich et al. (2020) annotate a corpus of 45 open-</p>
<p>access scholarly articles on solid oxide fuel cells, covering entity types of materials, values, and devices. Panapitiya et al. (2021) provide annotations of CHEM, VALUE, and UNIT on a set of papers on soluble materials. However, both only provide binary relations between pairs of entities, which is inadequate for describing more complex relations.
$N$-ary Relation Extraction. $N$-ary relations are size- $N$ tuples that describe the factual relationship between $N$ entities. In general domains, the MUC dataset (Chinchor, 1998) describes event participants in news articles. In the biomedical domain, the BioNLP Event Extraction Shared Task (Kim et al., 2009) and PubMed dataset (Jia et al., 2019) aim to extract biomedical events from biomedical text. In the machine learning domain, SciREX (Viswanathan et al., 2021; Jain et al., 2020; Zhuang et al., 2022) extracts $N$-ary relations in terms of <Task, Dataset, Method, Metric>. Different from these works' relations, the $N$-ary relations in POLYIE can have a varied number of named entities, which is more flexible in describing material knowledge but meanwhile introduces new challenges to RE. The closest work to POLYIE is drug-combo (Tiktinsky et al., 2022), which extracts variable-length combinations of different drugs. However, POLYIE and drug-combo are curated for two different domains, and the relations in POLYIE include numerical values.</p>
<h2>3 The Polyie Dataset</h2>
<p>In this section, we describe the details of the POLYIE dataset. We first formulate the two information extraction tasks for polymer material literature in § 3.1. We then describe the data preprocessing and annotation procedures in $\S 3.2$ and $\S 3.3$, and finally present the statistics and characteristics of the POLYIE dataset in § 3.4.</p>
<h3>3.1 Task Definition</h3>
<p>Polyie is curated for studying two key information extraction tasks on polymer literature data: (1) identifying relevant named entities, and (2) composing different entities to form $N$-ary relations.</p>
<p>Named Entity Recognition. Named Entity Recognition (NER) is the process of locating and classifying unstructured text phrases into predefined entity categories such as compound names, property names, etc. Given a sentence with $n$ tokens $\mathbf{S}=\left(w_{1}, \cdots, w_{n}\right)$, a named entity mention is
a span of tokens $\mathbf{e}=\left(w_{i}, \cdots, w_{j}\right)(0 \leq i \leq j \leq n)$ associated with an entity type. In POLYIE, we focus on NER for describing polymer material properties and include four important entity types: material name, property name, property value, and condition. An illustrative example can be found in Figure 1. Based on the BIO schema (Li et al., 2012), NER can be formulated as a sequence labeling task of assigning a sequence of labels $\mathbf{y}=\left(y_{1}, \cdots, y_{n}\right)$, each corresponding to a token in the input sentence.</p>
<p>Variable-Length $N$-ary Relation Extraction. Variable-length $N$-ary relation extraction (RE) refers to the process of identifying and extracting relationships between multiple entity mentions where the number of entities in the relationship can vary. Formally, given a list of $k$ context sentences $\mathcal{C}=\left(S_{1}, \cdots, S_{k}\right)$ in one paragraph, let $\mathcal{E}$ be the set of entities appearing in $\mathcal{C}$ where each entity $e \in \mathcal{E}$ belongs to one of the four entity types described in the NER task. The relation extraction task aims to extract a set of $m$ relations $\mathcal{R}=\left(r_{1}, \cdots, r_{m}\right)$ from $\mathcal{C}$. Each relation $r_{i}$ is a tuple of entities $r_{i}=$ $\left(e_{1}, \cdots, e_{N_{i}}\right),(1 \leq i \leq m)$ that describe their <material, property, value, condition> relations. Here, the number of entities $N_{i}$ can be variable in $\mathcal{R}$ because: 1) the property value may correspond to several materials instead of one; and 2) the condition entity may be absent. Figure 1 illustrates this variable-length $N$-ary RE task.</p>
<h3>3.2 Data Preparation</h3>
<p>We curate POLYIE from 146 publicly available scientific papers, covering four different material science domains: polymer solar cells, ring-opening polymerization, polymer membranes, and lithiumion batteries. These papers are sub-sampled from the corpus of 2.4 million material science articles described in Shetty et al. (2023). This corpus consists of papers published between 2000 to 2021 and is collected from 7 different material science publishers (Shetty and Ramprasad, 2021a,b). Keywordbased search was used to locate papers that span multiple application domains within polymers. The resulting dataset consists of 100 papers describing fullerene-acceptor polymer solar cells, 21 papers describing ring-opening polymerization, 20 describing lithium-ion batteries, and 5 describing polymer membranes. The text of these papers is parsed from the PDF of these papers using sciPDF ${ }^{1}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(a scientific parser based on GROBID (GRO, 20082023)) into utf-8 format. The incorrectly parsed units and symbols are corrected using regular expressions. Details about the regex rules used can be found in App. B.</p>
<h3>3.3 Data Annotation</h3>
<p>The PolyIE dataset is annotated by two polymer science domain experts as well as three computer science graduate students who are trained by the polymer scientists. Both the NER and RE annotations are performed using the Doccano (Nakayama et al., 2018) platform, which is an open-source text annotation tool that facilitates visual annotation with a Web interface. Below, we detail the annotation schemes for the NER and RE tasks.</p>
<h3>3.3.1 Annotating Named Entities</h3>
<p>In PolyIE, we annotate mentions of named entities for four categories: material names, property names, property values, and conditions. Each mention is a continuous text span that specifies the actual name of an entity or its abbreviation. This is done by marking the entity mention on the Doccano platform with the corresponding entity type.
Compound Names (Material). Compound Name entities include text spans that refer to material objects. Only chemical mentions that could be associated with a chemical structure are annotated as Compound Names. They may be specified by a particular composition formula (e.g., "4,9-di(2-octyldodecyl) aNDT"), a mention of chemical names (e.g., "trimethyltin chloride"), or just an abbreviation (e.g., "PaNDTDTFBT"). General chemical nouns (e.g., "ionic liquids") are not considered.
Property Names (Property). We annotate the properties of chemical compounds as long as they can be measured qualitatively (e.g., "toxicity" and "crystallinity") or quantitatively (e.g., "open-circuit voltage", "decomposition temperature"). Corresponding abbreviations should also be annotated (e.g., "PCE", "HOMO level").</p>
<p>Property Values (Value). We annotate the spans that can indicate the degree of qualitative properties (e.g., "soluble to water") or describe numerical values with units for quantitative properties (e.g., " $9.62 \times 10^{-5} \Omega^{-1} m^{-1}$ ", " 5.14 ppm ").
Conditions. In material science papers, the properties of materials can be constrained by quantitative modifiers, and we annotate them as conditions to distinguish them from normal property names and
property values (e.g., "room temperature", "frequency range $500 \mathrm{~Hz}-3 \mathrm{MHz}$ ").</p>
<h3>3.3.2 Variable-Length $N$-ary Relations</h3>
<p>For RE, we annotate the $N$-ary relations between the named entities to capture their <Material, Property, Value, Condition> relations.
Primary Binary Relations. As Doccano and most other existing text annotation tools only support annotations for binary relations, we decompose the $N$-ary relation annotation task into simpler binary relation annotation and later aggregate them into full $N$-ary relations. We split an $N$-ary relation into multiple binary relations for annotation: Material-Material marks the relations between material names that constitute one material system; Material-Property identifies the relation between a material and its reported property name; Property-Value annotates the corresponding property name and value; and Value-Condition marks the property values measured under a specific condition.
Transforming Binary to $N$-ary Relations. We then transform all the binary relations with common involved entities to generate $N$-ary relations in the format of <Material, (Material), Property, Value, (Condition)>. We abandon all binary relations that cannot be combined with other binary relations, only maintaining the generated $N$-ary relations with $N&gt;2$.</p>
<h3>3.3.3 Inter-Annotator Agreement</h3>
<p>All documents in POLYIE are annotated by at least two annotators independently. If annotation conflicts arise across two annotators, a third annotator is then assigned to annotate the corresponding sentences independently. The final annotation is determined by majority voting.</p>
<p>We calculate the inter-annotator agreement in terms of Fleiss' Kappa (Fleiss, 1971). The Fleiss' Kappa for individual entity types is calculated by treating other entity types as negative samples. The results are shown in Table 1. The Fleiss' Kappas for Material, Property, and Value are all in the range of almost perfect agreement, while the corresponding value for Condition lies in the range of substantial agreement. For RE, we consider all annotated relations as subjects and treat categories as binary. The Fleiss' Kappa for RE is 0.67 .</p>
<p>We also compute the average F1-score similar to Friedrich et al. (2020). The F1-score is calculated by treating one annotator as the gold standard and</p>
<p>the other annotator as predicted. For the NER, spans and entity types have to exactly match. For RE, all entity mentions within the n-ary relation have to exactly match. The averaged F1-score for the NER and RE task is 0.89 and 0.84 respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Overall</th>
<th style="text-align: center;">Material</th>
<th style="text-align: center;">Property</th>
<th style="text-align: center;">Value</th>
<th style="text-align: center;">Condition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0.86</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.71</td>
</tr>
</tbody>
</table>
<p>Table 1: Fleiss' kappa for all annotators across all mentions and each entity type respectively.</p>
<h3>3.4 Data Analysis</h3>
<p>Table 2 shows the key statistics for our corpus. PolyIE contains 41635 entity mentions and 4443 relations in all 146 fully annotated polymer material science literature. We quantitively analyze some key properties of POLYIE:
Statistics of Entities. For all the named entity mentions, the distribution of the four entity types Material, Property, Value, and Condition are $49.54 \%, 31.82 \%, 17.00 \%$, and $1.70 \%$, respectively. In total, those 41365 mentions describe 10890 distinct named entities for polymer materials.
Statistics of $N$-ary Relations. Among the 4443 relations on POLYIE, $86.38 \%$ are 3 -ary; $13.62 \%$ are 4 -ary; and $3.20 \%$ are 5 -ary. Meanwhile, $26.65 \%$ of the relations are cross-sentence relations, while the rest are intra-sentence relations.</p>
<h2>4 Modeling</h2>
<p>In this section, we describe how we model the named entity recognition and $N$-ary relation extraction tasks on POLYIE.</p>
<p>Named Entity Recognition. We model the NER task as a sequence labeling problem and learn a neural sequence tagger, as shown in Figure 2. We study both the bi-directional LSTM-CRF (BiLSTMCRF) (Ma and Hovy, 2016) model and BERTbased (Devlin et al., 2019) NER models for neural sequence tagging. We also study the performance of GPT-3.5 and GPT-4 on NER.</p>
<p>In BiLSTM-CRF, the input text is passed through an embedding layer to obtain token representations. These representations are then fed into a BiLSTM layer (Lample et al., 2016) to capture contextual information. The output of the BiLSTM layer is finally sent to a subsequence Conditional Random Field (CRF) layer (Lafferty et al., 2001) for sequence labeling. For the pre-trained language</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PSC</th>
<th style="text-align: center;">RP</th>
<th style="text-align: center;">LB</th>
<th style="text-align: center;">PM</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">documents</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">146</td>
</tr>
<tr>
<td style="text-align: left;">sentences</td>
<td style="text-align: center;">9,367</td>
<td style="text-align: center;">3,120</td>
<td style="text-align: center;">3,031</td>
<td style="text-align: center;">555</td>
<td style="text-align: center;">16,073</td>
</tr>
<tr>
<td style="text-align: left;">tokens</td>
<td style="text-align: center;">288,142</td>
<td style="text-align: center;">91,421</td>
<td style="text-align: center;">90,381</td>
<td style="text-align: center;">15,579</td>
<td style="text-align: center;">485,523</td>
</tr>
<tr>
<td style="text-align: left;">avg. tokens/doc.*</td>
<td style="text-align: center;">3,201.6</td>
<td style="text-align: center;">3,102.4</td>
<td style="text-align: center;">3,227.9</td>
<td style="text-align: center;">3,115.8</td>
<td style="text-align: center;">3,325.5</td>
</tr>
<tr>
<td style="text-align: left;">mentions</td>
<td style="text-align: center;">28,775</td>
<td style="text-align: center;">5,760</td>
<td style="text-align: center;">6,013</td>
<td style="text-align: center;">1,087</td>
<td style="text-align: center;">41,635</td>
</tr>
<tr>
<td style="text-align: left;">- Material</td>
<td style="text-align: center;">13,244</td>
<td style="text-align: center;">3,120</td>
<td style="text-align: center;">3,390</td>
<td style="text-align: center;">740</td>
<td style="text-align: center;">20,494</td>
</tr>
<tr>
<td style="text-align: left;">- Property</td>
<td style="text-align: center;">9,848</td>
<td style="text-align: center;">1,597</td>
<td style="text-align: center;">1,616</td>
<td style="text-align: center;">187</td>
<td style="text-align: center;">13,248</td>
</tr>
<tr>
<td style="text-align: left;">- Value</td>
<td style="text-align: center;">5,294</td>
<td style="text-align: center;">792</td>
<td style="text-align: center;">835</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">7,032</td>
</tr>
<tr>
<td style="text-align: left;">- Condition</td>
<td style="text-align: center;">364</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">167</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">702</td>
</tr>
<tr>
<td style="text-align: left;">entities</td>
<td style="text-align: center;">7,099</td>
<td style="text-align: center;">1,621</td>
<td style="text-align: center;">1,739</td>
<td style="text-align: center;">431</td>
<td style="text-align: center;">10,890</td>
</tr>
<tr>
<td style="text-align: left;">avg. mentions/doc.*</td>
<td style="text-align: center;">287.8</td>
<td style="text-align: center;">274.3</td>
<td style="text-align: center;">300.7</td>
<td style="text-align: center;">217.4</td>
<td style="text-align: center;">285.2</td>
</tr>
<tr>
<td style="text-align: left;">relations</td>
<td style="text-align: center;">3,084</td>
<td style="text-align: center;">592</td>
<td style="text-align: center;">615</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">4,443</td>
</tr>
<tr>
<td style="text-align: left;">- 3-ary</td>
<td style="text-align: center;">2,554</td>
<td style="text-align: center;">503</td>
<td style="text-align: center;">516</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">3,838</td>
</tr>
<tr>
<td style="text-align: left;">- 4-ary</td>
<td style="text-align: center;">388</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">605</td>
</tr>
<tr>
<td style="text-align: left;">- 5-ary</td>
<td style="text-align: center;">142</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">142</td>
</tr>
<tr>
<td style="text-align: left;">avg. relations/doc.*</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">30.4</td>
</tr>
</tbody>
</table>
<p>*Avg. indicates average and doc. refers to document.
Table 2: POLYIE corpus statistics.
models (PLM), we study both BERT $_{\text {base. }}$ (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) for NER. We also include four domain-specific BERT models: SciBERT (Beltagy et al., 2019), BioBERT (Lee et al., 2020), MatSciBERT (Gupta et al., 2022), and MaterialsBERT (Shetty et al., 2023). All the NER models in the BERT family are fine-tuned for sequence labeling, by stacking a linear layer that maps the contextual token representations into the label space. In addition, we also evaluate LLMs' abilities in marking material science concepts. Following the existing work (Tang et al., 2023), we directly prompt GPT-3.5-turbo and GPT-4 with few-shot exemplars to use special marks "@@" to annotate the boundaries and types of the named entities. Detailed explanations and examples of prompts are included in App. C.</p>
<p>Relation Extraction. For relation extraction, we evaluate the performances of the rule-based method, PLM-based models, and graph-based models. For the rule-based method, we leverage the assumption, Proximity-Rule, that relations are more likely to be formed with most proximitive entities. As illustrated in Figure 2, PLM-based models (such as BERT-RE) leverage the strong representation power of pre-trained language models on entities and employ simple aggregation techniques, such as concatenation and summation, to compose relation embeddings for further prediction. Example models in this category are state-of-the-art models PURE (Zhong and Chen, 2021), which inserts a special "entity marker" token around the entities in a candidate relation; and its variant PURE-</p>
<p>SUM (Tiktinsky et al., 2022), which uses embedding summation for variable-length $N$-ary RE. We also study graph-based methods for $N$-ary RE, DyGIE++ (Luan et al., 2019), which constructs a dynamic span graph from the input text, with entities as nodes and relations as edges to reason over multihop relations. For models based on LLMs like GPT-3.5-turbo and GPT-4, we randomly choose a subset of examples from the training set to serve as few-shot instances. These are then directly sent to the models as prompts to facilitate the relation extraction.</p>
<h2>5 Experiments</h2>
<h3>5.1 Experimental Setup</h3>
<p>Evaluation Protocol. We split the dataset into 123 training articles, 27 validation articles, and 27 test articles following a $70 \% / 15 \% / 15 \%$ ratio. The three sets do not have overlapping scientific documents. For NER, we report the entity-level precision, recall, and F-1 scores of each baseline for different entity categories, as well as the corresponding micro-average of these metrics. For RE, we report the precision, recall, and F-1 score.
Hyperparameters. For BiLSTM-CRF, we use one layer of BiLSTM layer with 256-dimensional hidden states and 128 embedding dimensionality. For the BERT-family NER models, we stack a linear layer with a hidden size of 128 on the BERT architecture for token classification. For all the NER and RE models, we use early stopping on the dev set for regularization. See App. D for details.</p>
<h3>5.2 Main Results</h3>
<p>Entity Mention Extraction. Table 3 shows the performance of different methods for the NER task on PolyIE. From the results, we make the following observations: (1) BERT-based models significantly outperform BiLSTM-CRF model with a $14.8 \%$ gain in micro average F1-score. This is because BERT-based models have been pre-trained on a large corpus of data, allowing them to possess more semantic knowledge than BiLSTM-CRF and to better understand the context. (2) Domainspecific BERT models achieve slightly better performance than the vanilla BERT due to the encoding of domain-specific knowledge. MaterialsBERT, which is fine-tuned on a corpus of materials science article abstracts, shows the best overall performance. (3) Upon comparing the performance of different entity types, we find that it is challenging
for all models to discriminate Condition entities from the other categories. We hypothesize that this is because Conditions are relatively rare in the training data, and the Condition entities could resemble property value entities.</p>
<p>Relation Extraction. Table 4 shows the performance of different methods for the RE task on POLYIE, and we make the following observations: (1) Among all the models evaluated, the PURESUM model with MatSciBERT as the encoder achieves the highest F-1 score, indicating that MatSciBERT can better understand the context, and the summation operation is an appropriate aggregation method for variable-length $N$-ary relation extraction. (2) The rule-based approach exhibits inferior performance in comparison to most deep learning models, indicating that there are many cases that do not conform to the proximity rule, such as cross-sentence relations and parallel relations. (3) Interestingly, the BERT-RE model shows even worse performance than the rule-based method. Compared to PURE-based models, BERTRE directly averages the embeddings of all tokens related to the relation. As tokens with similar types have similar representations, and N -ary relations are composed of certain entity-type elements, the averaging operation results in similar relation representations, ultimately leading to poor model performance. (4) As DyGIE++ is a model specifically designed for binary relation extraction, it can only determine the presence of N -ary relations by assessing the connectivity of arbitrary pairs of elements in the relationship. It thus has stricter judging criteria than the other methods, making its precision higher at the cost of lower recall.</p>
<p>Analysis on LLMs. LLMs such as GPT-3.5-turbo and GPT-4 exhibit worse performance compared to most baseline models on both NER and RE tasks. This discrepancy is likely due to the small proportion of polymer material science content in their pre-training corpus. When these models are directly prompted with few-shot examples, as opposed to being fine-tuned with training data, they receive less domain-specific information. This limitation hinders their ability to effectively understand and process concepts related to polymer material science. Potential updates on LLMs, like external tools (e.g., knowledge retriever) (Shi et al., 2023; Zhuang et al., 2023) or collaborations between LLMs and smaller pre-trained language models (Yu et al., 2023; Xu et al., 2023), may further</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model architecture for Named Entity Recognition (left) and $N$-ary Relation Extraction (right).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Material</th>
<th style="text-align: center;">Property</th>
<th style="text-align: center;">Value</th>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;">Micro Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BiLSTM-CRF</td>
<td style="text-align: center;">58.9 (68.4/51.7)</td>
<td style="text-align: center;">70.5 (75.4/66.2)</td>
<td style="text-align: center;">73.0 (74.6/71.5)</td>
<td style="text-align: center;">13.1 (36.4/8.0)</td>
<td style="text-align: center;">65.8 (72.4/60.4)</td>
</tr>
<tr>
<td style="text-align: center;">BERT $_{\text {biaso }}$</td>
<td style="text-align: center;">83.9 (84.0/83.8)</td>
<td style="text-align: center;">77.8 (81.1/74.7)</td>
<td style="text-align: center;">81.3 (83.9/79.0)</td>
<td style="text-align: center;">13.8 (16.2/12.0)</td>
<td style="text-align: center;">80.6 (82.4/78.8)</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa $_{\text {biaso }}$</td>
<td style="text-align: center;">85.4 (86.4/84.4)</td>
<td style="text-align: center;">76.2 (77.4/75.2)</td>
<td style="text-align: center;">81.8 (83.3/80.3)</td>
<td style="text-align: center;">12.5 (16.7/10.0)</td>
<td style="text-align: center;">80.7 (82.0/79.4)</td>
</tr>
<tr>
<td style="text-align: center;">SciBERT</td>
<td style="text-align: center;">85.6 (87.1/84.1)</td>
<td style="text-align: center;">74.6 (77.2/72.3)</td>
<td style="text-align: center;">81.9 (84.6/79.4)</td>
<td style="text-align: center;">11.3 (19.0/8.0)</td>
<td style="text-align: center;">80.3 (82.7/78.1)</td>
</tr>
<tr>
<td style="text-align: center;">BioBERT</td>
<td style="text-align: center;">85.1 (84.5/85.7)</td>
<td style="text-align: center;">76.9 (79.3/74.6)</td>
<td style="text-align: center;">82.6 (82.6/82.5)</td>
<td style="text-align: center;">15.2 (16.6/14.0)</td>
<td style="text-align: center;">81.0 (81.7/80.3)</td>
</tr>
<tr>
<td style="text-align: center;">MatSciBERT</td>
<td style="text-align: center;">85.8 (84.4/87.3)</td>
<td style="text-align: center;">77.4 (78.2/76.5)</td>
<td style="text-align: center;">82.4 (81.9/82.8)</td>
<td style="text-align: center;">11.4 (13.2/10.0)</td>
<td style="text-align: center;">81.3 (81.1/81.7)</td>
</tr>
<tr>
<td style="text-align: center;">MaterialsBERT</td>
<td style="text-align: center;">85.7 (85.2/86.3)</td>
<td style="text-align: center;">77.8 (79.8/75.9)</td>
<td style="text-align: center;">82.5 (82.6/82.4)</td>
<td style="text-align: center;">14.4 (17.0/12.7)</td>
<td style="text-align: center;">81.6 (82.2/80.9)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">63.7 (61.4/67.2)</td>
<td style="text-align: center;">49.4 (47.5/52.5)</td>
<td style="text-align: center;">59.5 (86.6/45.9)</td>
<td style="text-align: center;">2.2 (17.5/1.3)</td>
<td style="text-align: center;">56.4 (58.8/54.1)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">64.7 (57.6/75.2)</td>
<td style="text-align: center;">61.6 (52.2/76.6)</td>
<td style="text-align: center;">74.2 (67.1/84.2)</td>
<td style="text-align: center;">5.7 (8.5/4.8)</td>
<td style="text-align: center;">64.5 (56.5/75.1)</td>
</tr>
</tbody>
</table>
<p>Table 3: Main NER results on the test dataset, presented as "F-1 Score (Precision/Recall)" in \%. We offer scores under different metrics for each entity category and the overall micro-average performance. The reported score is the average of 5 distinct runs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F-1 Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Proximity-Rule</td>
<td style="text-align: center;">26.49</td>
<td style="text-align: center;">30.83</td>
<td style="text-align: center;">28.50</td>
</tr>
<tr>
<td style="text-align: center;">BERT-RE</td>
<td style="text-align: center;">12.06</td>
<td style="text-align: center;">40.28</td>
<td style="text-align: center;">18.57</td>
</tr>
<tr>
<td style="text-align: center;">DyGIE++</td>
<td style="text-align: center;">67.53</td>
<td style="text-align: center;">50.28</td>
<td style="text-align: center;">57.64</td>
</tr>
<tr>
<td style="text-align: center;">PURE</td>
<td style="text-align: center;">60.27</td>
<td style="text-align: center;">54.04</td>
<td style="text-align: center;">56.98</td>
</tr>
<tr>
<td style="text-align: center;">PURE-SUM (SciBERT)</td>
<td style="text-align: center;">42.86</td>
<td style="text-align: center;">82.50</td>
<td style="text-align: center;">56.41</td>
</tr>
<tr>
<td style="text-align: center;">PURE-SUM (MatSciBERT)</td>
<td style="text-align: center;">51.91</td>
<td style="text-align: center;">83.06</td>
<td style="text-align: center;">63.89</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">16.37</td>
<td style="text-align: center;">34.27</td>
<td style="text-align: center;">21.73</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">37.82</td>
<td style="text-align: center;">54.16</td>
<td style="text-align: center;">44.06</td>
</tr>
</tbody>
</table>
<p>Table 4: Main RE results on the test dataset, presented as Precision, Recall, and F-1 Scores in \%.</p>
<p>boost the performance via injecting more domainspecific knowledge. Due to the poor performance obtained under the few-shot prompting setting and the high cost when fine-tuning LLMs, we recommend fine-tuning smaller domain-specific pretrained language models, like MaterialsBERT in Table 3 and PURE-SUM (MatSciBERT) in Table 4, to extract polymer material science entities and relations.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Micro-Average
<img alt="img-3.jpeg" src="img-3.jpeg" />
(c) Property Name
(d) Property Value</p>
<p>Figure 3: Effect of training data size on NER task.</p>
<h1>5.3 Impact of Data Size</h1>
<p>We evaluate the NER model performance as a function of the amount of training data in Figure 3. Compared to BERT-based models, the performance of the BiLSTM-CRF model is consistently inferior, with only slight changes with varying sizes of</p>
<table>
<thead>
<tr>
<th>Noise Types</th>
<th>Input Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>Interweaving Relations</td>
<td>The corresponding HOMO and LUMO energy levels for PIDTT-TzTz and PIDTT-TzTz-TT <br> are (-5.24, -3.21 ) and (-5.34, -3.03) eV , respectively.</td>
</tr>
<tr>
<td>Partially Correct Relations</td>
<td>For example, OFETs made using a porphyrin-diacetylene polymer give mobilities of <br> $1 \times 10^{-7} \mathrm{~cm}^{2} \mathrm{~V}^{-1} \mathrm{~s}^{-1}$ at room temperature and $2 \times 10^{-6} \mathrm{~cm}^{2} \mathrm{~V}^{-1} \mathrm{~s}^{-1}$ at $175^{\circ} \mathrm{C}$.</td>
</tr>
<tr>
<td>Inverted Sentences</td>
<td>For polymer PDTG-DPP, the thermal stability is even better than the Sibridged analogue, PDTS- <br> DPP, and the $T_{d}=409^{\circ} \mathrm{C}$ of PDTG-IID is the same as the Si-bridged analogue, PDTS-IID.</td>
</tr>
</tbody>
</table>
<p>Table 5: Examples incorrectly predicted by MatSciBERT. Entities highlighted in green indicate the gold $N$-ary relation in the input text. Predicted relations made by the model are shown in bold fonts. Red fonts represent the location of errors.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Performances of PURE-SUM on RE task with limited training data and few-shot setting.
training data. This trend demonstrates the superiority of language model pre-training stage, which allows BERT-family NER models to encode relevant knowledge for the downstream task. Comparing different BERT models, MaterialsBERT consistently outperforms vanilla BERT by a slight margin, which reflects the benefit of developing domainspecific pre-trained language models.</p>
<p>Figure 4(a) shows the performance of the best RE model PURE-SUM as training data size varies. With more training data, the model's performance generally increases in all the metrics. However, after training on $60 \%$ data, the recall starts to decrease, while the other metrics still slightly increase. This is because the imbalance between positive and negative cases starts to influence the training, where models are more likely to predict relations as negative, making the false negative cases increase and the recall decrease. Additional details about the impact of data size on RE tasks can be seen in App. E.</p>
<h3>5.4 Error Analysis</h3>
<p>We analyze the key error types of the BERT $_{\text {base }}$ NER model by drawing its confusion matrix on the test set, as shown in Figure 5. The confusion matrix shows that the majority of entities are correctly predicted as their gold label, with the exception of Condition entities. The limited number</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: The confusion matrix of BERT on NER task.
of training samples containing Condition entities makes it difficult for the model to distinguish them from other irrelevant entities (labeled " 0 "). Additionally, the resemblance between Condition and Property Value entities often results in incorrect predictions between them.</p>
<p>For RE, Table 5 illustrates the major error types made by the PURE-SUM model, including: (1) Interweaving or parallel relations in the text present a significant challenge for models in understanding the alignment between multiple sets of entities; (2) The task of flexible-length $N$-ary relation extraction is challenging, and errors often occur when encountering relations that cover more entities (e.g., determining whether to include the Condition in the prediction); (3) The last type of error frequently arises when the sentence organization is atypical, including sentences written in the passive voice.</p>
<h2>6 Conclusion</h2>
<p>We have curated a new dataset POLYIE for named entity recognition and $N$-ary relation extraction from polymer scientific literature. POLYIE covers thousands of <Material, Property, Value, Condition> relations curated from 146 full polymer articles. We have evaluated mainstay NER and RE models on POLYIE and analyzed their perfor-</p>
<p>mance and error cases. In addition, we have also tested the performance of the strongest LLMs, GPT3.5 and GPT-4, on PolyIE. We found that even state-of-the-art models, either domain-specific pretrained language models or most advanced LLMs, can struggle with hard NER and RE cases. Through error analysis, we found that such difficulties arise from the diverse lexical formats and ambiguity of polymer named entities and also variable-length and cross-sentence $N$-ary relations. Our work contributes the first polymer scientific information extraction dataset as well as insights into this dataset. We hope PolyIE will serve as a useful resource that will and attract more research efforts from the NLP community to push the boundary of this task.</p>
<h2>Limitations</h2>
<p>One limitation of PolyIE is that we have annotated only the text modality of the polymer literature corpus. While tables and figures are not included in PolyIE, they are two important modalities that contain a considerable amount of information about polymer properties. It will be interesting to explore annotation schemes that can extend PolyIE to include tables and figures and enable multi-modal information extraction jointly from text, tables, and figures. In addition, PolyIE currently covers four application subdomains for polymer materials. In the future, PolyIE can benefit from including more sub-domains for polymers, as well as scientific publications for other organic materials. Such extensions will not only make POLYIE more comprehensive for studying polymer information extraction, but also allow it to be used to study cross-domain transfer of different information extraction models.</p>
<h2>Acknowledgements</h2>
<p>This work was supported in part by NSF IIS2008334, IIS-2106961, CAREER IIS-2144338, and ONR MURI N00014-17-1-2656.</p>
<h2>References</h2>
<p>2008-2023. Grobid. https://github.com/ kermitt2/grobid.</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the</p>
<p>9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 36153620, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Keith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. 2018. Machine learning for molecular and materials science. Nature, 559(7715):547-555.</p>
<p>Nancy A Chinchor. 1998. Overview of muc-7/met-2. Technical report.</p>
<p>Callum J Court and Jacqueline M Cole. 2018. Autogenerated materials database of curie and néel temperatures via semi-supervised relationship extraction. Scientific data, 5(1):1-12.</p>
<p>Callum J Court, Apoorv Jain, and Jacqueline M Cole. 2021. Inverse design of materials that exhibit the magnetocaloric effect by text-mining of the scientific literature and generative deep learning. Chemistry of Materials, 33(18):7217-7231.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Huan Doan Tran, Chiho Kim, Lihua Chen, Anand Chandrasekaran, Rohit Batra, Shruti Venkatram, Deepak Kamal, Jordan P Lightstone, Rishi Gurnani, Pranav Shetty, et al. 2020. Machine-learning predictions of polymer properties with polymer genome. Journal of Applied Physics, 128(17).</p>
<p>Biaoyan Fang, Christian Druckenbrodt, Saber A Akhondi, Jiayuan He, Timothy Baldwin, and Karin Verspoor. 2021. Chemu-ref: a corpus for modeling anaphora resolution in the chemical domain. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1362-1375.</p>
<p>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.</p>
<p>Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Marusczyk, and Lukas Lange. 2020. The SOFC-exp corpus and neural approaches to information extraction in the materials science domain. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1255-1268, Online. Association for Computational Linguistics.</p>
<p>Kata Gábor, Davide Buscaldi, Anne-Kathrin Schumann, Behrang QasemiZadeh, Haifa Zargayouna, and Thierry Charnois. 2018. Semeval-2018 task 7: Semantic relation extraction and classification in scientific papers. In SemEval, pages 679-688.</p>
<p>Tanishq Gupta, Mohd Zaki, NM Krishnan, et al. 2022. Matscibert: A materials domain language model for text mining and information extraction. $n p j$ Computational Materials, 8(1):1-11.</p>
<p>Lezan Hawizy, David M Jessop, Nico Adams, and Peter Murray-Rust. 2011. Chemicaltagger: A tool for semantic text-mining in chemistry. Journal of cheminformatics, 3(1):1-13.</p>
<p>Jiayuan He, Dat Quoc Nguyen, Saber A Akhondi, Christian Druckenbrodt, Camilo Thorne, Ralph Hoessel, Zubair Afzal, Zenan Zhai, Biaoyan Fang, Hiyori Yoshikawa, et al. 2021. Chemu 2020: Natural language processing methods are effective for information extraction from chemical patents. Frontiers in Research Metrics and Analytics, 6:654438.</p>
<p>Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, and Debasis Ganguly. 2019. Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 52035213, Florence, Italy. Association for Computational Linguistics.</p>
<p>Shu Huang and Jacqueline M Cole. 2020. A database of battery materials auto-generated using chemdataextractor. Scientific Data, 7(1):1-13.</p>
<p>Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. 2020. SciREX: A challenge dataset for document-level information extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 75067516, Online. Association for Computational Linguistics.</p>
<p>Robin Jia, Cliff Wong, and Hoifung Poon. 2019. Document-level n-ary relation extraction with multiscale representation learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3693-3704, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun'ichi Tsujii. 2009. Overview of BioNLP'09 shared task on event extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1-9, Boulder, Colorado. Association for Computational Linguistics.</p>
<p>Yunsoo Kim, Hyuk Ko, Jane Lee, Hyun Young Heo, Jinyoung Yang, Sungsoo Lee, and Kyu-hwang Lee. 2023. Chemical language understanding benchmark. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 404-411.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>John D Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</p>
<p>Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 260-270, San Diego, California. Association for Computational Linguistics.</p>
<p>Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240.</p>
<p>Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei Huang. 2012. Joint bilingual name tagging for parallel corpora. In 21st ACM International Conference on Information and Knowledge Management, CIKM 2012, pages 1727-1731.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. 2019. A general framework for information extraction using dynamic span graphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3036-3046, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064-1074, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines. In International Conference on Learning Representations.</p>
<p>Hiroki Nakayama, Takahiro Kubo, Junya Kamura, Yasufumi Taniguchi, and Xu Liang. 2018. doccano: Text</p>
<p>annotation tool for human. Software available from https://github.com/doccano/doccano.</p>
<p>Tim O'Gorman, Zach Jensen, Sheshera Mysore, Kevin Huang, Rubayyat Mahbub, Elsa Olivetti, and Andrew McCallum. 2021. MS-mentions: Consistently annotating entity mentions in materials science procedural text. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1337-1352, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Gihan Panapitiya, Fred Parks, Jonathan Sepulveda, and Emily Saldanha. 2021. Extracting material property measurement data from scientific articles. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 53935402, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Tim Rocktäschel, Michael Weidlich, and Ulf Leser. 2012. Chemspot: a hybrid system for chemical named entity recognition. Bioinformatics, 28(12):1633-1640.</p>
<p>Jonathan Schmidt, Mário RG Marques, Silvana Botti, and Miguel AL Marques. 2019. Recent advances and applications of machine learning in solid-state materials science. npj Computational Materials, 5(1):1-36.</p>
<p>Pranav Shetty, Arunkumar Chitteth Rajan, Chris Kuenneth, Sonakshi Gupta, Lakshmi Prerana Panchumarti, Lauren Holm, Chao Zhang, and Rampi Ramprasad. 2023. A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing. npj Computational Materials, 9(1):52.</p>
<p>Pranav Shetty and Rampi Ramprasad. 2021a. Automated knowledge extraction from polymer literature using natural language processing. Iscience, 24(1).</p>
<p>Pranav Shetty and Rampi Ramprasad. 2021b. Machineguided polymer knowledge extraction using natural language processing: The example of named entity normalization. Journal of Chemical Information and Modeling, 61(11):5377-5385.</p>
<p>Qinqin Shi, Haijun Fan, Yao Liu, Wenping Hu, Yongfang Li, and Xiaowei Zhan. 2011. A copolymer of benzodithiophene with tips side chains for enhanced photovoltaic performance. Macromolecules, 44(23):9173-9179.</p>
<p>Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce C Ho, Carl Yang, and May Dongmei Wang. 2024. Ehragent: Code empowers large language models for fewshot complex tabular reasoning on electronic health records. In ICLR 2024 Workshop on Large Language Model (LLM) Agents.</p>
<p>Wenqi Shi, Yuchen Zhuang, Yuanda Zhu, Henry Iwinski, Michael Wattenbarger, and May Dongmei Wang. 2023. Retrieval-augmented large language models
for adolescent idiopathic scoliosis patients in shared decision-making. In Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics, pages $1-10$.</p>
<p>Matthew C Swain and Jacqueline M Cole. 2016. Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature. Journal of chemical information and modeling, 56(10):1894-1904.</p>
<p>Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023. Does synthetic data generation of llms help clinical text mining? arXiv preprint arXiv:2303.04360.</p>
<p>Aryeh Tiktinsky, Vijay Viswanathan, Danna Niezni, Dana Meron Aragury, Yosi Shamay, Hillel TaubTabib, Tom Hope, and Yoav Goldberg. 2022. A dataset for n-ary relation extraction of drug combinations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3190-3203, Seattle, United States. Association for Computational Linguistics.</p>
<p>Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(11).</p>
<p>Vijay Viswanathan, Graham Neubig, and Pengfei Liu. 2021. CitationIE: Leveraging the citation graph for scientific information extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 719-731, Online. Association for Computational Linguistics.</p>
<p>Xuan Wang, Vivian Hu, Xiangchen Song, Shweta Garg, Jinfeng Xiao, and Jiawei Han. 2021. ChemNER: Fine-grained chemistry named entity recognition with ontology-guided distant supervision. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 52275240, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Leigh Weston, Vahe Tshitoyan, John Dagdelen, Olga Kononova, Amalie Trewartha, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. 2019. Named entity recognition and normalization applied to largescale information extraction from the materials science literature. Journal of chemical information and modeling, 59(9):3692-3702.</p>
<p>Chao Wu, Lihua Chen, Ajinkya Deshmukh, Deepak Kamal, Zongze Li, Pranav Shetty, Jierui Zhou, Harikrishna Sahu, Huan Tran, Gregory Sotzing, et al. 2021. Dielectric polymers tolerant to electric field and temperature extremes: Integration of phenomenology, informatics, and experimental validation. ACS Applied Materials \&amp; Interfaces, 13(45):53416-53424.</p>
<p>Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, Wei Jin, Joyce Ho, and Carl Yang. 2023. Knowledge-infused prompting: Assessing and advancing clinical text data generation with large language models. arXiv preprint arXiv:2311.00287.</p>
<p>Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. 2019. DocRED: A large-scale document-level relation extraction dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 764-777, Florence, Italy. Association for Computational Linguistics.</p>
<p>Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2023. Large language model as attributed training data generator: A tale of diversity and bias. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.</p>
<p>Zexuan Zhong and Danqi Chen. 2021. A frustratingly easy approach for entity and relation extraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 50-61, Online. Association for Computational Linguistics.</p>
<p>Yuchen Zhuang, Yinghao Li, Jerry Junyang Cheung, Yue Yu, Yingjun Mou, Xiang Chen, Le Song, and Chao Zhang. 2022. Resel: N-ary relation extraction from scientific text and tables by learning to retrieve and select. arXiv preprint arXiv:2210.14427.</p>
<p>Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. Toolqa: A dataset for llm question answering with external tools. arXiv preprint arXiv:2306.13304.</p>
<h2>A T-SNE Visualization of Entity Embeddings</h2>
<p>Figure 6 shows t-SNE (Van der Maaten and Hinton, 2008) visualization of entity embeddings generated by BERT $<em _base="{base" _text="\text">{\text {base }}$, SciBERT and MatSciBERT. Compared with all the visualization of different entity embeddings, we can observe that pre-training on a more similar domain of corpus to fine-tuning corpus will make model generate high-quality embeddings. From the figures, we can easily observe that MatSciBERT embeddings of the same entity type are more clustered than those of BERT $</em>$, which is also consistent with what we observe from the quantitive results.}</p>
<h2>B Regular Expression</h2>
<p>Regular expression is used to correct incorrectly parsed units and mathematical symbols after tokenization to ensure the quality of tokens. Details of the regular expression rules are listed below:</p>
<ul>
<li>Composite units that are broken down to multiple tokens are joined to form a single token. Missing caret symbols are added.</li>
<li>Degree Celsius symbols are merged to a single token. Missing circles are added.</li>
<li>Numbers with a following percentage sign are merged to a single token.</li>
</ul>
<h2>C Implementation Details of LLMs</h2>
<p>We conduct experiments on Azure OpenAI platform, with GPT-3.5-turbo and GPT-4 in 0613 version. We set the temperature as 0 to obtain a stable and faithful evaluation of the LLMs' results. Following the existing work (Tang et al., 2023), we have 4 components in our NER prompt: general instruction, annotation guideline, output indicator, and few-shot exemplars. (1) The general instruction part specifies the objective of the LLM to mark the polymer material science entities or relations. (2) The annotation guideline is to provide additional explanation and guidelines for the LLM to follow when annotating different types of entities and relations. (3) The output indicator specifies the output format of the LLM. (4) The few-shot exemplars allow LLM to form a more cohesive understanding of previous instructions.</p>
<p>The NER and RE prompts are presented below.
Listing 1: NER prompt.
As a proficient linguist, your objective is to identify and label specific</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: t-SNE visualization of entity embeddings generated by BERT, SciBERT, and MatSciBERT.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">entities</span><span class="w"> </span><span class="nx">within</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">provided</span><span class="w"> </span><span class="nx">paragraph</span>
<span class="p">.</span><span class="w"> </span><span class="nx">These</span><span class="w"> </span><span class="nx">entities</span><span class="w"> </span><span class="nx">include</span><span class="w"> </span><span class="nx">chemical</span>
<span class="nx">names</span><span class="w"> </span><span class="p">(</span><span class="nx">CN</span><span class="p">),</span><span class="w"> </span><span class="nx">property</span><span class="w"> </span><span class="nx">names</span><span class="w"> </span><span class="p">(</span><span class="nx">PN</span><span class="p">),</span>
<span class="nx">property</span><span class="w"> </span><span class="nx">values</span><span class="w"> </span><span class="p">(</span><span class="nx">PV</span><span class="p">),</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">conditions</span>
<span class="w">    </span><span class="p">(</span><span class="nx">Condition</span><span class="p">).</span><span class="w"> </span><span class="nx">Chemical</span><span class="w"> </span><span class="nx">names</span><span class="p">,</span>
<span class="nx">polymer</span><span class="w"> </span><span class="nx">material</span><span class="w"> </span><span class="nx">names</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">their</span>
<span class="nx">abstractions</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">entities</span><span class="p">.</span><span class="w"> </span><span class="nx">Polymer</span>
<span class="nx">material</span><span class="w"> </span><span class="nx">names</span><span class="w"> </span><span class="nx">might</span><span class="w"> </span><span class="nx">contain</span>
<span class="nx">multiple</span><span class="w"> </span><span class="nx">chemical</span><span class="w"> </span><span class="nx">names</span><span class="w"> </span><span class="nx">within</span><span class="w"> </span><span class="nx">it</span><span class="p">,</span>
<span class="nx">label</span><span class="w"> </span><span class="nx">them</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">single</span><span class="w"> </span><span class="nx">entity</span><span class="p">.</span>
<span class="nx">Abstractions</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">property</span><span class="w"> </span><span class="nx">names</span><span class="w"> </span><span class="nx">are</span>
<span class="nx">also</span><span class="w"> </span><span class="nx">considered</span><span class="w"> </span><span class="nx">entities</span><span class="p">.</span><span class="w"> </span><span class="nx">Property</span>
<span class="nx">values</span><span class="w"> </span><span class="nx">contain</span><span class="w"> </span><span class="nx">both</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="k">and</span>
<span class="nx">the</span><span class="w"> </span><span class="nx">unit</span><span class="p">.</span><span class="w"> </span><span class="nx">To</span><span class="w"> </span><span class="nx">represent</span><span class="w"> </span><span class="nx">recognized</span>
<span class="nx">named</span><span class="w"> </span><span class="nx">entities</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">text</span><span class="p">,</span>
<span class="nx">enclose</span><span class="w"> </span><span class="nx">them</span><span class="w"> </span><span class="nx">within</span><span class="w"> </span><span class="nx">special</span><span class="w"> </span><span class="nx">symbols</span>
<span class="sc">&#39;@&#39;</span><span class="p">,</span><span class="w"> </span><span class="nx">followed</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">their</span><span class="w"> </span><span class="nx">respective</span>
<span class="nx">types</span><span class="w"> </span><span class="err">&#39;</span><span class="p">(</span><span class="nx">CN</span><span class="p">)</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="p">(</span><span class="nx">PN</span><span class="p">)</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">&#39;</span><span class="p">(</span><span class="nx">PV</span><span class="p">)</span><span class="err">&#39;</span>
<span class="nx">before</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">ending</span><span class="w"> </span><span class="sc">&#39;@&#39;</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">remaining</span>
<span class="w">    </span><span class="nx">text</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">remain</span><span class="w"> </span><span class="nx">unchanged</span><span class="p">.</span>
</code></pre></div>

<p>Listing 2: RE prompt.</p>
<div class="codehilite"><pre><span></span><code><span class="n">As</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">skilled</span><span class="w"> </span><span class="n">linguist</span><span class="p">,</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">mission</span><span class="w"> </span><span class="n">is</span>
<span class="w">    </span><span class="n">to</span><span class="w"> </span><span class="n">analyze</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">paragraph</span><span class="w"> </span><span class="n">that</span>
<span class="w">        </span><span class="n">contains</span><span class="w"> </span><span class="n">four</span><span class="w"> </span><span class="n">distinct</span><span class="w"> </span><span class="n">types</span><span class="w"> </span><span class="n">of</span>
<span class="w">    </span><span class="nl">entities</span><span class="p">:</span><span class="w"> </span><span class="n">Chemical</span><span class="w"> </span><span class="n">Names</span><span class="w"> </span><span class="p">(</span><span class="n">CN</span><span class="p">),</span>
<span class="w">    </span><span class="n">Property</span><span class="w"> </span><span class="n">Names</span><span class="w"> </span><span class="p">(</span><span class="n">PN</span><span class="p">),</span><span class="w"> </span><span class="n">Property</span><span class="w"> </span><span class="n">Values</span>
<span class="w">        </span><span class="p">(</span><span class="n">PV</span><span class="p">),</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Conditions</span><span class="w"> </span><span class="p">(</span><span class="n">Condition</span><span class="p">).</span>
<span class="w">    </span><span class="n">Each</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">entities</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">enclosed</span>
<span class="w">    </span><span class="n">within</span><span class="w"> </span><span class="s">&quot;@&quot;</span><span class="w"> </span><span class="n">symbols</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">their</span>
<span class="w">    </span><span class="n">entity</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">brackets</span>
<span class="w">    </span><span class="n">before</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">closing</span><span class="w"> </span><span class="s">&quot;@&#39;. Your</span>
<span class="w">    </span><span class="n">objective</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">identify</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">extract</span>
<span class="w">        </span><span class="n">relationships</span><span class="w"> </span><span class="n">among</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">entities</span><span class="p">,</span>
<span class="w">        </span><span class="n">and</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">them</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">two</span>
<span class="w">        </span><span class="n">possible</span><span class="w"> </span><span class="n">formats</span><span class="o">:</span><span class="w"> </span><span class="p">(</span><span class="n">Chemical</span><span class="w"> </span><span class="n">Names</span><span class="p">,</span>
<span class="n">Property</span><span class="w"> </span><span class="n">Names</span><span class="p">,</span><span class="w"> </span><span class="n">Property</span><span class="w"> </span><span class="n">Values</span><span class="p">,</span>
<span class="n">Condition</span><span class="p">)</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="p">(</span><span class="n">Chemical</span><span class="w"> </span><span class="n">Names</span><span class="p">,</span>
<span class="n">Property</span><span class="w"> </span><span class="n">Names</span><span class="p">,</span><span class="w"> </span><span class="n">Property</span><span class="w"> </span><span class="n">Values</span><span class="p">).</span>
<span class="n">Please</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">establish</span><span class="w"> </span><span class="n">relationships</span>
<span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">entities</span><span class="p">,</span><span class="w"> </span><span class="n">and</span>
<span class="n">only</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">extracted</span>
<span class="w">    </span><span class="n">relations</span><span class="p">.</span><span class="w"> </span><span class="n">Below</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">examples</span><span class="o">:</span>
</code></pre></div>

<h2>D Implementation Details</h2>
<p>All the NER and RE models are trained with the Adam optimizer (Kingma and Ba, 2014), with different learning rate: The BiLSTM-CRF model is trained with a learning rate of 0.005 and batch size of 64 ; While fine-tuning the BERT-family NER
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Effect of the few-shot learning on NER task.
models, we select the learning rate of $3 e-4$; For relation extraction, instances with lengths exceeding 300 are broken into several shorter segments, without cutting off relations, and the models are trained with a learning rate of $2 e-4$ and a batch size of 8 . We run all the experiments 3 times and report the average in the tables and the average/error bar in the figures. All experiments are conducted on $C P U$ : Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz and GPU: NVIDIA GeForce RTX A5000 GPUs using python 3.8 and Pytorch 1.10.</p>
<h2>E Few-Shot Learning</h2>
<p>Figure 7 shows the performance of different NER models under few-shot settings. We can see BERT-based NER models consistently outperform BiLSTM-CRF models by large margins. However, the variances of such BERT-based NER models are also much larger. This is likely due to the different quality and representativeness of the training samples and the capacity of pre-trained language models. The MatSciBERT model, for instance, has already captured a significant amount of domain knowledge during pre-training. When it is fed with critical cases during the fine-tuning stage,</p>
<p>it can quickly adapt such knowledge to fine-tuning, resulting in high-quality decision boundaries on the corpus. However, if the training samples are of poor quality and not representative, the model's performance can be limited. Such instability of BERTbased fine-tuning is also observed on GLUE (Mosbach et al., 2021).</p>
<h2>F Impact of Negative Sampling in Training</h2>
<p>As the RE models are trained with negative sampling, we investigate the impact of negative samples during the training process. We study three ways to create negative samples from existing relations, by corrupting entities with other irrelevant entities of the same type in the context sentences. (1) Easy: all possible random corruptions; (2) Medium: single or double element corruption; and (3) Hard: only single-element corruption. Figure 8(a) shows the results when training with different negative sampling policies, with a fixed $k=10$. We find that the hard negative sampling strategy achieves superior performance, suggesting that using hard negative cases can help the model learn better decision boundaries. In Figure 8(b), we also evaluate the model performances when varying the number of negative samples $k$ from 5 to 20 . The trend shows that $k=20$ achieves the best performances with all different negative sampling strategies.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: Effect of the quality and amount of negative samples during training in $N$-ary relation extraction.</p>
<h2>G Annotation Guidance</h2>
<p>In this section, we will introduce the annotation guidance. There are 4 types of entities that should be annotated: Chemical Compound, Property Name, Property Value, and Condition.</p>
<h2>G. 1 Chemical Compound</h2>
<ul>
<li>Only chemical nouns that can be associated with a specific structure should be labeled as Chemical Compounds: e.g., "4,9-di(2-octyldodecyl) aNDT", "trimethyltin chloride";</li>
<li>Abbreviation of the chemical nouns should also be labeled as Chemical Compounds as long as it can be associated with a specific structure: e.g., "PaNDTDTFBT";</li>
<li>General chemical concepts (non-structural or nonspecific chemical nouns), adjectives, verbs, and other terms that can not be associated directly with a chemical structure should not be annotated: e.g., "polymer", "conjugated polymers" should not be annotated;</li>
<li>Spans: Spans of Chemical Compounds should not contain leading or trailing spaces. If the abbreviation of Chemical Compound appears inside brackets, the brackets should not be included in the annotation.</li>
</ul>
<h2>G. 2 Property Name</h2>
<ul>
<li>Properties of chemical compounds should be annotated as long as they can be measured qualitatively (such as toxicity and crystallinity) or quantitatively (with a unit and a value). Property Names that occur without a corresponding value should also be annotated: e.g., "Hole mobility", "Open-circuit voltage", "decomposition temperature", "conductivity", "toxicity";</li>
<li>Abbreviations of Property Names should be annotated: "PCE", "HOMO level", "LUMO level";</li>
<li>Laboratory methods should not be annotated as Property Names: "Titration", "Cyclic voltammetry" should not be annotated as Property Names;</li>
<li>Spans: Spans of Property Names should not contain leading or trailing spaces.</li>
</ul>
<h2>G. 3 Property Value</h2>
<ul>
<li>Both quantitative and qualitative Property Values should be annotated;</li>
<li>Do not annotate overly vague adjectives;</li>
<li>Spans of Property Values should not contain leading or trailing spaces. Property Value and its units should be contained as a single span. Ranges of Property Value should be contained as a single span.</li>
</ul>
<h2>G. 4 Condition</h2>
<ul>
<li>Only quantitative modifiers that constrain the numerical Property Value should be annotated as Conditions;</li>
<li>Spans of Conditions should not contain leading or trailing spaces.</li>
</ul>
<p>The screenshots of the official annotation guidance shared with all the annotators are listed in Figure 9 and Figure 10.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>To start annotating documents, simply click on the "Annotate" Button on the right of screen.</p>
<h1>Labels</h1>
<p>Figure 9: Overview of documents to annotate on the annotation platform.</p>
<h2>H Top-5 Named Entities</h2>
<p>Table 6: The top-5 entities for each category.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entity Type</th>
<th style="text-align: left;">Top-5 Entities</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chemical Name</td>
<td style="text-align: left;">PC71BM, P1, PCBM, thiophene, P3HT</td>
</tr>
<tr>
<td style="text-align: left;">Property Name</td>
<td style="text-align: left;">PCE, J sc, V oc, absorption, HOMO</td>
</tr>
<tr>
<td style="text-align: left;">Property Value</td>
<td style="text-align: left;">$\mathrm{nm}, 10 \%, 300,0.76 \mathrm{~V}, 0.82 \mathrm{~V}$</td>
</tr>
<tr>
<td style="text-align: left;">Condition</td>
<td style="text-align: left;">$5 \%$ weight loss, in solution, red-shifted, illum- <br> nation of AM1.5, at room temperature</td>
</tr>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: left;">PCE, J sc, V oc, PC71BM, HOMO</td>
</tr>
</tbody>
</table>
<h1>Annotate Data</h1>
<p>For entity annotations, you first need to select the range of words from the corpus and then select the corresponding entity label for it:
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>To remove annotations, you can click on an existing annotation and click the little cross highlighted below:</p>
<p>Figure 10: Instructions on assigning pre-defined labels to named entities.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/titipata/scipdf_parser&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2012 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2012</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2012</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-276575355</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.15801v1.pdf" target="_blank">An explainable transformer circuit for compositional generalization</a></p>
                <p><strong>Paper Abstract:</strong> Compositional generalization-the systematic combination of known components into novel structures-remains a core challenge in cognitive science and machine learning. Although transformer-based large language models can exhibit strong performance on certain compositional tasks, the underlying mechanisms driving these abilities remain opaque, calling into question their interpretability. In this work, we identify and mechanistically interpret the circuit responsible for compositional induction in a compact transformer. Using causal ablations, we validate the circuit and formalize its operation using a program-like description. We further demonstrate that this mechanistic understanding enables precise activation edits to steer the model's behavior predictably. Our findings advance the understanding of complex behaviors in transformers and highlight such insights can provide a direct pathway for model control.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2012.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2012.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CompactTransformer-FunctionComp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compact encoder-decoder Transformer on Synthetic Function Composition Task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder Transformer (2 encoder layers, 2 decoder layers, 8 heads/layer, d_model=128) trained from scratch on 10,000 synthetic episodes to perform symbolic function composition (map symbol primitives to colors and apply function definitions); achieves high compositional generalization on held-out episodes and the authors identify a causal QK/OV attention-head circuit implementing the computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard encoder-decoder Transformer with bidirectional encoder self-attention and causal decoder self-attention + cross-attention; Pre-LayerNorm, sinusoidal positional embeddings, OV and QK attention circuits analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard attention-based Transformer (encoder-decoder), cross-attention, multi-head QK and OV circuits; no symbolic modules beyond learned embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>symbolic / procedural function composition (symbol-to-color mappings and function application)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic function composition task</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Each episode contains a support set that defines primitive symbol→color mappings and symbolic function definitions (functions map argument symbols to RHS color sequences) plus a question referencing symbols; the model must generate the color sequence produced by applying the function to the question's symbols. Episodes include 2–4 function assignments and 3–4 color assignments; functions may be unary or binary and RHS sequences are sampled up to length 5. Prompts concatenate question and support (separated by '|' and '=' tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>RHS sequence length up to 5; episodes contain 2–4 functions and 3–4 color assignments (no deeper nesting reported)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition / recombination of seen primitives into novel combinations (novel combinations of primitives and functions)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Held-out test episodes where primitive assignments and function definitions are conjunctively different from training set (i.e., some primitives or functions may be seen individually but not in the same combinations) — tests novel combinations / out-of-distribution compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised cross-entropy training from scratch on 10,000 randomly generated episodes for 50 epochs (Adam optimizer, LR schedule with warmup), batch size 25</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>98% accuracy on held-out test set of 2,000 episodes (test set constructed with conjunctive non-overlap to avoid rote memorization; reported as overall accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Internal causal/ablative comparisons: path-patching and two ablation modes (keep-only-one-head and ablate-only-one-head) used to identify sufficiency/necessity of specific attention heads (e.g., Dec-cross-1.5 as Output Head, Enc-self-0.5 Question-Broadcast Head, Enc-self-1.1 Primitive-Pairing Head, Dec-cross-0.6 RHS-Scanner, Enc-self-1.0/1.2 Primitive/Function-Retrieval). Targeted activation swapping of positional embeddings provides causal evidence; no comparisons against other architectures (RNN/ConvNet) were performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A small (2×encoder, 2×decoder) Transformer trained from scratch on synthetic function-composition episodes reaches 98% accuracy on held-out compositionally novel test episodes; the authors identify a concrete, causally-validated attention-head QK/OV circuit that (1) transmits index-in-question information via an encoder K-circuit (Question-Broadcast → Primitive-Pairing → Output Head K) and (2) transmits relative-index-on-LHS via a decoder Q-circuit (Primitive/Function-Retrieval → RHS-Scanner → Output Head Q). Targeted activation swaps of positional embeddings predictably redirect attention and outputs, demonstrating causal mechanistic control.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>The authors report limitations: perturbations did not produce complete token-level control (complete predicted-token swaps) because similar functions are distributed across multiple heads (redundancy); manual circuit discovery is labor-intensive; no tests on larger models were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Success observed when (a) model trained on diverse synthetic episodes (10k), (b) QK alignment can encode positional/index information (via positional embeddings and attention circuits), and (c) multiple complementary attention heads implement a structured QK/OV circuit. The model relies on positional/index encodings being preserved and relayed through specific heads.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2012.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2012.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hupkes2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compositionality decomposed: how do neural networks generalise?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study evaluating compositional generalization across model families that reports transformers outperforming RNNs and ConvNets on systematic recombination of known elements, but still falling short of human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Compositionality decomposed: how do neural networks generalise?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer, RNN, ConvNet (comparative study)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Comparative evaluation of model families on compositional generalization benchmarks (details referenced but not reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>comparison across sequence-modeling families (Transformer, RNN, ConvNet); details not given in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>compositional generalization (linguistic/symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluates systematic recombination of known elements; specific tasks not detailed in this paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>systematic generalization / recombination of known elements</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>designed to test systematic generalization (not IID) — specific split details not provided here</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Reported in Hupkes et al. as transformers outperforming RNNs/ConvNets on systematic generalization, but numbers are not given in this paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Mentioned: Hupkes et al. evaluate different model families and report transformer superiority over RNN/ConvNet for systematic recombination, but still below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned result: transformers outperform RNNs and ConvNets on systematic recombination tasks but still lag behind humans.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2012.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2012.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LakeBaroni2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-like systematic generalization through a meta-learning neural network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work showing that small transformers (<1M parameters) pretrained on data generated by a 'metagrammar' can exhibit human-like compositional ability in novel in-context learning cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like systematic generalization through a meta-learning neural network</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Small Transformer (meta-learned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small transformer models (reported as <1M parameters) pre-trained with data from a metagrammar and evaluated for in-context compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>less than 1M</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer architecture trained via meta-learning on structured generative grammar data ('metagrammar')</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic / symbolic compositionality, in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Meta-trained on metagrammar-generated data to evaluate in-context compositional generalization; specifics not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>systematic generalization / novel in-context combinations</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>pretraining / meta-learning on synthetic grammar-generated data</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Reported to show human-like compositional ability in novel in-context cases (no numeric metrics provided in this paper's text).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Implicitly suggests small models (<1M) can achieve strong compositional behavior under the metagrammar pretraining condition</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Meta-learning on structured generative data (metagrammar) can produce human-like systematic generalization in small transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Pretraining/meta-learning on metagrammar-generated data; small model size (<1M) reported to be sufficient under these training conditions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2012.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2012.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DZhang2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based models are not yet perfect at learning to emulate structural recursion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work noting that transformers struggle with composing recursive structures, highlighting a limitation in compositional generalization for recursive phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer-based models are not yet perfect at learning to emulate structural recursion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observation/evaluation that transformer architectures have difficulty with recursive composition; details not replicated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>structural recursion / hierarchical composition</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Composing recursive structures is challenging; detailed experimental setup not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>recursive composition</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reported difficulty of transformers in composing recursive structures (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2012.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2012.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang2024-initScale</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Understanding the expressive power and mechanisms of transformer for sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work (M. Wang & E., 2024) identifying initialization scales as a critical factor determining whether models rely on memorization versus rule-based reasoning for compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding the expressive power and mechanisms of transformer for sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Theoretical/empirical analysis linking initialization scale and learning dynamics to whether transformers adopt memorization or rule-like solutions for compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer; analysis of initialization effects</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>sequence modeling / compositional reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>compositional rule learning vs memorization</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Initialization scale can determine whether learned solutions are memorization-based or rule-based, affecting compositional generalization (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Compositionality decomposed: how do neural networks generalise? <em>(Rating: 2)</em></li>
                <li>Human-like systematic generalization through a meta-learning neural network <em>(Rating: 2)</em></li>
                <li>Transformer-based models are not yet perfect at learning to emulate structural recursion <em>(Rating: 2)</em></li>
                <li>Understanding the expressive power and mechanisms of transformer for sequence modeling <em>(Rating: 1)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with GPT-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2012",
    "paper_id": "paper-276575355",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "CompactTransformer-FunctionComp",
            "name_full": "Compact encoder-decoder Transformer on Synthetic Function Composition Task",
            "brief_description": "An encoder-decoder Transformer (2 encoder layers, 2 decoder layers, 8 heads/layer, d_model=128) trained from scratch on 10,000 synthetic episodes to perform symbolic function composition (map symbol primitives to colors and apply function definitions); achieves high compositional generalization on held-out episodes and the authors identify a causal QK/OV attention-head circuit implementing the computation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (encoder-decoder)",
            "model_description": "Standard encoder-decoder Transformer with bidirectional encoder self-attention and causal decoder self-attention + cross-attention; Pre-LayerNorm, sinusoidal positional embeddings, OV and QK attention circuits analyzed.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard attention-based Transformer (encoder-decoder), cross-attention, multi-head QK and OV circuits; no symbolic modules beyond learned embeddings",
            "task_domain": "symbolic / procedural function composition (symbol-to-color mappings and function application)",
            "task_name": "Synthetic function composition task",
            "task_description": "Each episode contains a support set that defines primitive symbol→color mappings and symbolic function definitions (functions map argument symbols to RHS color sequences) plus a question referencing symbols; the model must generate the color sequence produced by applying the function to the question's symbols. Episodes include 2–4 function assignments and 3–4 color assignments; functions may be unary or binary and RHS sequences are sampled up to length 5. Prompts concatenate question and support (separated by '|' and '=' tokens).",
            "compositional_depth": "RHS sequence length up to 5; episodes contain 2–4 functions and 3–4 color assignments (no deeper nesting reported)",
            "composition_type": "function composition / recombination of seen primitives into novel combinations (novel combinations of primitives and functions)",
            "split_type": "Held-out test episodes where primitive assignments and function definitions are conjunctively different from training set (i.e., some primitives or functions may be seen individually but not in the same combinations) — tests novel combinations / out-of-distribution compositional generalization",
            "training_strategy": "Supervised cross-entropy training from scratch on 10,000 randomly generated episodes for 50 epochs (Adam optimizer, LR schedule with warmup), batch size 25",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "98% accuracy on held-out test set of 2,000 episodes (test set constructed with conjunctive non-overlap to avoid rote memorization; reported as overall accuracy)",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Internal causal/ablative comparisons: path-patching and two ablation modes (keep-only-one-head and ablate-only-one-head) used to identify sufficiency/necessity of specific attention heads (e.g., Dec-cross-1.5 as Output Head, Enc-self-0.5 Question-Broadcast Head, Enc-self-1.1 Primitive-Pairing Head, Dec-cross-0.6 RHS-Scanner, Enc-self-1.0/1.2 Primitive/Function-Retrieval). Targeted activation swapping of positional embeddings provides causal evidence; no comparisons against other architectures (RNN/ConvNet) were performed in this paper.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "A small (2×encoder, 2×decoder) Transformer trained from scratch on synthetic function-composition episodes reaches 98% accuracy on held-out compositionally novel test episodes; the authors identify a concrete, causally-validated attention-head QK/OV circuit that (1) transmits index-in-question information via an encoder K-circuit (Question-Broadcast → Primitive-Pairing → Output Head K) and (2) transmits relative-index-on-LHS via a decoder Q-circuit (Primitive/Function-Retrieval → RHS-Scanner → Output Head Q). Targeted activation swaps of positional embeddings predictably redirect attention and outputs, demonstrating causal mechanistic control.",
            "failure_analysis": "The authors report limitations: perturbations did not produce complete token-level control (complete predicted-token swaps) because similar functions are distributed across multiple heads (redundancy); manual circuit discovery is labor-intensive; no tests on larger models were performed.",
            "success_conditions": "Success observed when (a) model trained on diverse synthetic episodes (10k), (b) QK alignment can encode positional/index information (via positional embeddings and attention circuits), and (c) multiple complementary attention heads implement a structured QK/OV circuit. The model relies on positional/index encodings being preserved and relayed through specific heads.",
            "uuid": "e2012.0"
        },
        {
            "name_short": "Hupkes2019",
            "name_full": "Compositionality decomposed: how do neural networks generalise?",
            "brief_description": "A cited study evaluating compositional generalization across model families that reports transformers outperforming RNNs and ConvNets on systematic recombination of known elements, but still falling short of human performance.",
            "citation_title": "Compositionality decomposed: how do neural networks generalise?",
            "mention_or_use": "mention",
            "model_name": "Transformer, RNN, ConvNet (comparative study)",
            "model_description": "Comparative evaluation of model families on compositional generalization benchmarks (details referenced but not reproduced in this paper).",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": "comparison across sequence-modeling families (Transformer, RNN, ConvNet); details not given in this paper",
            "task_domain": "compositional generalization (linguistic/symbolic)",
            "task_name": null,
            "task_description": "Evaluates systematic recombination of known elements; specific tasks not detailed in this paper's text.",
            "compositional_depth": null,
            "composition_type": "systematic generalization / recombination of known elements",
            "split_type": "designed to test systematic generalization (not IID) — specific split details not provided here",
            "training_strategy": null,
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "Reported in Hupkes et al. as transformers outperforming RNNs/ConvNets on systematic generalization, but numbers are not given in this paper's text.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": null,
            "baseline_comparisons": null,
            "architectural_comparison": "Mentioned: Hupkes et al. evaluate different model families and report transformer superiority over RNN/ConvNet for systematic recombination, but still below human performance.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Mentioned result: transformers outperform RNNs and ConvNets on systematic recombination tasks but still lag behind humans.",
            "failure_analysis": null,
            "success_conditions": null,
            "uuid": "e2012.1"
        },
        {
            "name_short": "LakeBaroni2023",
            "name_full": "Human-like systematic generalization through a meta-learning neural network",
            "brief_description": "Cited work showing that small transformers (&lt;1M parameters) pretrained on data generated by a 'metagrammar' can exhibit human-like compositional ability in novel in-context learning cases.",
            "citation_title": "Human-like systematic generalization through a meta-learning neural network",
            "mention_or_use": "mention",
            "model_name": "Small Transformer (meta-learned)",
            "model_description": "Small transformer models (reported as &lt;1M parameters) pre-trained with data from a metagrammar and evaluated for in-context compositional generalization.",
            "model_size": "less than 1M",
            "is_pretrained": true,
            "architectural_features": "standard transformer architecture trained via meta-learning on structured generative grammar data ('metagrammar')",
            "task_domain": "linguistic / symbolic compositionality, in-context learning",
            "task_name": null,
            "task_description": "Meta-trained on metagrammar-generated data to evaluate in-context compositional generalization; specifics not reproduced here.",
            "compositional_depth": null,
            "composition_type": "systematic generalization / novel in-context combinations",
            "split_type": null,
            "training_strategy": "pretraining / meta-learning on synthetic grammar-generated data",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "Reported to show human-like compositional ability in novel in-context cases (no numeric metrics provided in this paper's text).",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": null,
            "baseline_comparisons": null,
            "architectural_comparison": null,
            "scale_effects": "Implicitly suggests small models (&lt;1M) can achieve strong compositional behavior under the metagrammar pretraining condition",
            "transfer_results": null,
            "key_findings": "Meta-learning on structured generative data (metagrammar) can produce human-like systematic generalization in small transformers.",
            "failure_analysis": null,
            "success_conditions": "Pretraining/meta-learning on metagrammar-generated data; small model size (&lt;1M) reported to be sufficient under these training conditions.",
            "uuid": "e2012.2"
        },
        {
            "name_short": "DZhang2024",
            "name_full": "Transformer-based models are not yet perfect at learning to emulate structural recursion",
            "brief_description": "Cited work noting that transformers struggle with composing recursive structures, highlighting a limitation in compositional generalization for recursive phenomena.",
            "citation_title": "Transformer-based models are not yet perfect at learning to emulate structural recursion",
            "mention_or_use": "mention",
            "model_name": "Transformer (general mention)",
            "model_description": "Observation/evaluation that transformer architectures have difficulty with recursive composition; details not replicated in this paper.",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": null,
            "task_domain": "structural recursion / hierarchical composition",
            "task_name": null,
            "task_description": "Composing recursive structures is challenging; detailed experimental setup not provided here.",
            "compositional_depth": null,
            "composition_type": "recursive composition",
            "split_type": null,
            "training_strategy": null,
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": null,
            "baseline_comparisons": null,
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Reported difficulty of transformers in composing recursive structures (citation only).",
            "failure_analysis": null,
            "success_conditions": null,
            "uuid": "e2012.3"
        },
        {
            "name_short": "Wang2024-initScale",
            "name_full": "Understanding the expressive power and mechanisms of transformer for sequence modeling",
            "brief_description": "Cited work (M. Wang & E., 2024) identifying initialization scales as a critical factor determining whether models rely on memorization versus rule-based reasoning for compositional tasks.",
            "citation_title": "Understanding the expressive power and mechanisms of transformer for sequence modeling",
            "mention_or_use": "mention",
            "model_name": "Transformer (analysis)",
            "model_description": "Theoretical/empirical analysis linking initialization scale and learning dynamics to whether transformers adopt memorization or rule-like solutions for compositional tasks.",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": "standard transformer; analysis of initialization effects",
            "task_domain": "sequence modeling / compositional reasoning",
            "task_name": null,
            "task_description": null,
            "compositional_depth": null,
            "composition_type": "compositional rule learning vs memorization",
            "split_type": null,
            "training_strategy": null,
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": null,
            "baseline_comparisons": null,
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Initialization scale can determine whether learned solutions are memorization-based or rule-based, affecting compositional generalization (citation only).",
            "failure_analysis": null,
            "success_conditions": null,
            "uuid": "e2012.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Compositionality decomposed: how do neural networks generalise?",
            "rating": 2
        },
        {
            "paper_title": "Human-like systematic generalization through a meta-learning neural network",
            "rating": 2
        },
        {
            "paper_title": "Transformer-based models are not yet perfect at learning to emulate structural recursion",
            "rating": 2
        },
        {
            "paper_title": "Understanding the expressive power and mechanisms of transformer for sequence modeling",
            "rating": 1
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "rating": 1
        }
    ],
    "cost": 0.013739749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An explainable transformer circuit for compositional generalization
19 Feb 2025</p>
<p>Cheng Tang 
Department of Brain and Cognitive Sciences
Massachusetts In-stitute of Technology
MAUSA</p>
<p>McGovern Institute
Masachusetts Institute of Technology
MAUSA</p>
<p>Brenden Lake 
Center for Data Science
Department of Psychology
New York University
NYUSA</p>
<p>Mehrdad Jazayeri 
Department of Brain and Cognitive Sciences
Massachusetts In-stitute of Technology
MAUSA</p>
<p>McGovern Institute
Masachusetts Institute of Technology
MAUSA</p>
<p>Howard Hughes Medical Institute
MAUSA</p>
<p>An explainable transformer circuit for compositional generalization
19 Feb 202571D473F3A60C355DCDE8BCED8C111731arXiv:2502.15801v1[cs.LG]TransformerMechanistic InterpretabilityCompositionality
Compositional generalization-the systematic combination of known components into novel structures-remains a core challenge in cognitive science and machine learning.Although transformer-based large language models can exhibit strong performance on certain compositional tasks, the underlying mechanisms driving these abilities remain opaque, calling into question their interpretability.In this work, we identify and mechanistically interpret the circuit responsible for compositional induction in a compact transformer.Using causal ablations, we validate the circuit and formalize its operation using a program-like description.We further demonstrate that this mechanistic understanding enables precise activation edits to steer the model's behavior predictably.Our findings advance the understanding of complex behaviors in transformers and highlight such insights can provide a direct pathway for model control.</p>
<p>Introduction</p>
<p>Transformers, first introduced by Vaswani et al. (2017), excel at tasks requiring complex reasoning such as code synthesis (Chen et al., 2021) and mathematical problemsolving (Hendrycks et al., 2020).This capability stems not merely from memorization, but from their ability to perform compositional generalization-systematically combining learned primitives into novel structures via in-context learning (ICL) (Brown et al., 2020;Lake &amp; Baroni, 2023).While humans inherently excel at such abstraction (Fodor, 1979), traditional neural architectures struggle with out-of-distribution (OOD) compositional tasks (Hupkes et al., 2019;Lake et al., 2016).Understanding how neural systems accomplish compositionality has become a focus of both machine learning and cognitive science research.</p>
<p>Mechanistic interpretability-a field dedicated to reverseengineering neural networks into human-understandable algorithms-has begun unraveling these dynamics.Seminal work identified induction heads as a critical component for ICL (Elhage et al., 2021;Olsson et al., 2022), enabling transformers to dynamically bind and retrieve contextual patterns rather Compositional generalization in transformers.In their study, Hupkes et al. (2019) evaluated compositional generalization ability on different families of models, and found that transformer outperformed RNN and ConvNet in systematic generalization, i.e., recombination of known elements, but still uncomparable to human performance.D. Zhang et al. (2024) pointed out that transformers struggle with composing recursive structures.Recently, Lake &amp; Baroni (2023) showed that after being pre-trained with data generated by a 'metagrammar', small transformers (less than 1 million parameters) can exhibit human-like compositional ability in novel in-context learning cases.This is in line with the success of commercial large language models (LLM) in solving complex out-ofdistribution reasoning tasks (Bubeck et al., 2023;DeepSeek-AI et al., 2024), where compositional genralization is necessary.</p>
<p>Several studies highlighted factors that facilitate transformer's compositional ability.M. Wang &amp; E (2024) identified initialization scales as a critical factor in determining whether models rely on memorization or rule-based reasoning for compositional tasks.Z. Zhang et al. (2025) revealed that lowcomplexity circuits enable out-of-distribution generalization by condensing primitive-level rules.(Sanford et al., 2024) identified logarithmic depth as a key constraint for transformers to emulate computations within a sequence.Here, we offer a complementary mechanistic understanding of how trasnformers perform compositional computations.</p>
<p>Experimental Setup</p>
<p>Our experimental setup involves a synthetic function composition task (Figure 1) designed to probe compositional induction in a compact Transformer.We outline the task structure, the Transformer basics (including attention mechanisms), and the training protocol.</p>
<p>Task Structure</p>
<p>Each episode consists of a support set and a question (Figure 1b):</p>
<p>• Support Set: Specifies (i) Primitives as symbol-to-color mappings (e.g., A = red, D = pink), and (ii) Functions as symbolic operations over these primitives (e.g., A S D = pink red, where S indicates swapping adjacent symbols).</p>
<p>•</p>
<p>Model</p>
<p>Transformer Basics Our transformer uses an encoderdecoder architecture that involves two types of attentions:</p>
<p>• Self-Attention: Captures within-sequence interactions.</p>
<p>The token embedding matrix X ∈ R n input ×d model is projected into Queries, Keys, and Values:
Q = XW Q , K = XW K , V = XW V , where W Q ,W K ,W V ∈ R d model ×d head are learnable weight ma- trices.
• Cross-Attention: Enables the decoder to attend to encoder outputs.Here, the Queries (Q) come from the decoder tokens, while the Keys (K) and Values (V ) come from the encoder tokens.</p>
<p>The attention mechanism operates through two separate circuits on embedding X ∈ R n input ×d model for each attention head:</p>
<p>• QK Circuit (W Q W ⊤ K ): Determines from where information flows to each token by computing attention scores between token pairs, with higher scores indicate stronger token-totoken relationships:
Attention(Q, K) = softmax X Q W Q (X K W K ) ⊤ √ d head ∈ R n query ×n key ,
where softmax is applied along the dimension for Key and independently for each head.</p>
<p>• OV Circuit (W V W O ): Controls what information gets written to each token position.Combined with the QK Circuit, this produces the output of the attention head:
Z = Attention(Q, K)X V W V W O ∈ R n query ×d model , where W O ∈ R d head ×d model is learnable weight.
Our analysis focuses on how these circuits in attention heads together implement the compositional induction algorithm.</p>
<p>Model Training</p>
<p>We adopt an encoder-decoder Transformer with 2 layers in the encoder and 2 layers in the decoder (Figure 1a) with each layer containing 8 attention heads.Further model details appear in the Appendix.</p>
<p>For each episode, we randomly generate:</p>
<p>• Primitive Assignments: A mapping from symbol tokens (e.g., A, B) to color tokens (e.g., red, pink).</p>
<p>• Function Definitions: Symbolic transformations by ran- We train on 10,000 such episodes for 50 epochs and evaluate on 2,000 test (held-out) episodes.The model achieves 98% accuracy on this test set, indicating strong compositional induction capabilities.In the test set, primitive assignments and function definitions are conjunctively different from those in the training set (i.e., some primitives or some functions might be in the training set, but not the whole combination of them), preventing a memorization strategy.Please refer to the Appendix for additional details.</p>
<p>Results</p>
<p>First, we give an intuitive overview of the effective algorithm the model appears to implement.Next, we describe our circuit discovery procedure, where we use causal methods to pinpoint the exact attention heads responsible for compositional induction.Finally, we validate this mechanism by applying targeted perturbations that predictably alter the model's behavior.</p>
<p>The Effective Algorithm</p>
<p>General Solution.We first provide a general solution to this type of compositional problem in a python-like pseudocode for intuitive understanding (Algorithm 1).We use 1-indexing (count from 1) for tokens throughout.Transformer Solution.Next, we describe the actual implementation of the algorithm with attention operations in Figure 2 through a guidance episode.color ← symbol to color [symbol]   output.append(color)return output</p>
<p>Step 1 (Figure 2a; Question-Broadcast Head).Primitive input tokens in the support (e.g., A) attend to the same primitive tokens in the question (A), inheriting the latter's index-inquestion (3rd).The step is detailed in Figure 5b.</p>
<p>Step 2 (Figure 2b; Primitive-Pairing Head).Color tokens (red) attend to their associated primitive tokens (A), inheriting the latter's index-in-question (3rd).The step is detailed in Figure 5a.</p>
<p>Step 3 (Figure 2c; Primitive-and Function-Retrieval Heads).Color tokens on the function RHS (pink) attend to their associated primitive tokens on the function Left Hand Side (LHS) (D), inheriting the latter's relative-index-on-LHS (3rd).The step is detailed in Figure 9.</p>
<p>Step 4 (Figure 2d; RHS-Scanner Head).The 1st token in the Decoder (SOS) attend to the 1st tokens on the function Right Hand Side (RHS) (pink), inheriting the latter's formerinherited relative-index-on-LHS (3rd).The step is detailed in Figure 8.</p>
<p>Step 5 (Figure 2e; Output Head).SOS token (with inherited relative-index-on-LHS=3rd) attends to color tokens (red) with the same index-in-question (3rd), inheriting the latter's token identity (red), and generate the next prediction (red).The .We unfold all relevant information superimposed in tokens' embeddings and highlight their roles in attention operations.</p>
<p>[1] * , the QK alignment discussed in Primitive-Pairing Head section.</p>
<p>[2] * , the QK alignment discussed in Primitive-Retrieval Head section.</p>
<p>Circuit Discovery</p>
<p>Nomenclature: for attention heads, Enc-self-0.5 stands for Encoder, self-attention, layer 0, head 5; similarly, Dec-cross-1.5 stands for Decoder, cross-attention, layer 1, head 5.</p>
<p>Output Head (Dec-cross-1.5;Figure 3b) We discovered the model's circuit backwards from the unembedding layer using logit attribution (nostalgebraist, 2020), which measures each decoder attention head's linear contribution to the final token logits (adjusted by the decoder's output LayerNorm).</p>
<p>We identified Dec-cross-1.5 (decoder cross attention layer 1 head 5) as the primary contributor (Figure 3a).</p>
<p>Dec-cross-1.5'sQ tokens always attend to the K tokens from the Encoder that are the next predicted ones.For example, in Figure 3b, the SOS token attends to instances of red in the support set, which is indeed the correct next output prediction.This attention accuracy (i.e., max-attended token being the next-emitted token) of Dec-cross-1.5 remains above 90% for the first three tokens in the responses across all test episodes (Figure 3c), with Dec-cross-1.1 and -1.3 partially compensating beyond that point.</p>
<p>These observations suggest that Dec-cross-1.5'sOV circuit feeds token identities directly to the decoder unembedding layer (output layer).Specifically, we observe that the output of the OV circuit, XW v W o , align closely (strong inner product) with the unembedding vectors of the corresponding tokens (Figure 3d).Hence, we designate Dec-cross-1.5 as the Output Head (while Dec-cross-1.1 and -1.3 perform similar but less dominant roles) (Algorithm Step 3).Next, we show how the Output Head identifies the correct token through QK interactions.</p>
<p>The K-Circuit to the Output Head We first determine which encoder heads critically feed into the Output Head's K. To do this, we performed path-patching (K.R. Wang et al., 2022) by ablating all but one single encoder head and then measuring how much of Output Head's QK behavior (i.e., attention accuracy) remained.During these experiments, Output Head's Q were frozen using clean-run activations.Here we report patching results with mean-ablation (qualitative similar to random-sample ablation) (details in Appendix).Through this process, we identified Enc-self-1.1 and Encself-0.5 as the primary contributors to Output Head's K, acting in a sequential chain (Figure 4).Next, we show how they sequentially encode symbols' index-in-question critical for the QK alignment.</p>
<p>Primitive-Pairing Head (Enc-self-1.1;Figure 5a) This head exhibits a distinct attention pattern that pairs each color token with its associated primitive symbol token (e.g., in the support set, all instances of red attend to C).In other words, Enc-self-1.1 relays information (described below, as computed by e.g., Enc-self-0.5) from the primitive symbols to their corresponding color tokens via its QK circuit.Hence, we call Enc-self-1.1 the Primitive-Pairing Head.</p>
<p>To investigate which upstream heads feed into the OV cir- Output Head (K), while mean-ablating all other direct paths to Output Head's K.</p>
<p>We identified Enc-self-0.5 as an important node (Figure 5b).</p>
<p>Question-Broadcast Head (Enc-self-0.5;Figure 5b) All input symbol in the support set attend to their copies in the input question.In other words, Enc-self-0.5 broadcasts question-related information (including token identity and position) across symbols in the support-set (henceforth the Question-Broadcast Head).We hypothesize that the primitive symbols' index-in-question is the critical information passed from the Question-Broadcast Head's Z through the Primitive-Pairing Head's Z and lastly into the Output Head's K.</p>
<p>Index-In-Question Tracing</p>
<p>To validate this hypothesis, we examined the Question-Broadcast Head's Z for each primitivesymbol token.We reduced these outputs to two principal components and colored each point by its index-in-question.</p>
<p>As illustrated in Figure 6a, the Question-Broadcast Head's Z exhibit clear clustering, indicating that the index-in-question is robustly encoded at this stage (quantified by the R 2 score, i.e., the amount of variance explained by index identity, details in Appendix).We further confirmed that the Primitive-Pairing Head's Z preserves index-in-question (Figure 6b) and that the resulting Output Head's K also reflect the same clustering (Figure 6c).</p>
<p>Causal Ablation Finally, we verified that this circuit indeed causally propagates index-in-question.Ablating the Question-Broadcast Head's Z (together with the similarly functioning Enc-self-0.7) obliterates the clustering in the Primitive-Pairing Head's Z; ablating the Primitive-Pairing Head's Z (together with similarly functioning Enc-self-1.0) disrupts the clustering in the Output Head's K (Figure 6).We therefore conclude that the Question-Broadcast Head, the Primitive-Pairing Head and heads with similar functions form a crucial K-circuit pathway, passing index-in-question information from primitive tokens to their associated color tokens in the Output Head's K.</p>
<p>The Q-Circuit to the Output Head Having established the role of the K-circuit, we next investigate where its Q originates.</p>
<p>We again relied on sequential path-patching to pinpoint which decoder heads ultimately provide the Output Head's Q.We identified Dec-cross-0.6 as the main conduit for the Q values of the Output Head.Enc-self-1.0 and -1.2 supply positional embeddings that enable the decoder to track primitive symbol's relative-index-on-LHS, thereby completing the QK alignment for correct predictions (Figure 7).</p>
<p>RHS-Scanner</p>
<p>Head (Dec-cross-0.6;Figure 8b) We identify Dec-cross-0.6 as the dominant contributor to the the Output Head's Q (Figure 8a).Analyzing Dec-cross-0.6'sattention patterns reveals that each Q token (from Decoder in the cross-attention) sequentially attends to the color tokens (in the support set) on the function's RHS (Figure 8b).For example, the first Decoder token (SOS) attends to the first RHS tokens (purple, red, yellow), and the second query token (red) attends to the second RHS tokens (red, purple, red), and so on.This iterative scanning mechanism enables the decoder to reconstruct the transformation defined by the function.Hence we call Dec-cross-0.6 the RHS-Scanner Head.</p>
<p>Primitive-Retrival Head (Enc-self-1.0;Figure 9b) and Function-Retrival Head (Enc-self-1.2;Figure 9c) Next,   we looked for critical encoder heads that feeds to the RHS-Scanner Head and finally contributes to the Output Head's Q.Unlike the K-circuit discovery, where "keep-only-one-head" ablations is sufficient, multiple heads appears to contribute partial but complementary information.To isolate their roles, we measured drops in the output head's accuracy when ablating each encoder head individually while keeping the others intact (the "ablate-only-one-head" approach, more discussion in Appendix).</p>
<p>This analysis highlighted Enc-self-1.0 and Enc-self-1.2 as critical (Figure 9a).In Enc-self-1.0, within the support set, each color token on the RHS attends back to its corresponding symbol on the LHS, inheriting that symbol's token and positional embedding (henceforth the Primitive-Retrieval Head) (Fig. 9b).Meanwhile, Enc-self-1.2 is similar, such that each color token on the RHS attends back to its function symbol on the LHS, passing that token and positional embedding on to the color token (henceforth the Function-Retrieval Head) (Fig. 9c).</p>
<p>Why do the color tokens on the RHS attend back to both kinds of information on the LHS?We reason that if a color token on the RHS were to encode it's primitive symbol's relativeindex-on-LHS: for example, in '...| D=pink | A S D=pink red |...', pink were to encode 3rd inherited from D (D is 3rd in 'A S D'), the absolute position of D must be compared with the absolute position of the S to yield a relative position.Now that with the Primitive-and Function-Retrievel Heads, each RHS color token carries two positional references: (1) the associated LHS primitive, and (2) the function symbol, we hypothesize that by comparing these references, the model can infer the primitive symbols' relative-index-on-LHS for each of the associated color tokens on the RHS.</p>
<p>Relative-Index-On-LHS Tracing</p>
<p>To confirm that our discovered circuit genuinely encodes the relative-index-on-LHS in the Output Head's Q, we conducted three complementary ablation experiments summarized in Figure 10: • Retaining only the Primitive-and Function-Retrieval Heads When all other encoder heads are ablated, the RHS-Scanner Head's Z still carries relative-index-on-LHS that propagate to the Output Head's Q, indicating that these two heads alone provide sufficient index information.</p>
<p>• Ablating the Primitive-or Function-Retrieval Head individually Ablating either head disrupts the clustering by relative-index-on-LHS in the RHS-Scanner Head's Z, demonstrating that both heads are necessary to preserve the full index information.</p>
<p>• Ablating the RHS-Scanner Head (together with Deccross-0.0 and -0.3)These decoder heads share similar  attention patterns that track color tokens on the function's RHS.When all three are ablated, clusterings by relativeindex-on-LHS are eliminated from the Output Head's Q.
A F C | B = | C F B = | B F C = | C F A = | C = | A = | B D C = | EOS A F C | B = | C F B = | B F C = | C F A = | C = | A = | B D C = | EOS Query Key Function-Retrieval (Encoder-self-1.2) A F C | B = | C F B = | B F C = | C F A = | C = | A = | B D C = | EOS A F C | B = | C F B = | B F C = | C F A = | C = | A = | B D C = | EOS
Thus, we conclude that the Q-circuit depends on the RHS-Scanner Head to capture the relative-index-on-LHS information supplied by the Primitive-and Function-Retrieval Heads.</p>
<p>By aligning these Q signals with the K, the model consistently determines which token to generate next.</p>
<p>Targeted Perturbation Steers Behavior</p>
<p>So far, our circuit tracing indicates that the K-circuit of the Output Head encodes the primitive symbols' index-in-question, and that the Q-circuit encodes primitive symbols' relativeindex-on-LHS.We reason that if the QK circuit of the Output Head truly leverages on the primitive symbol index to predict the next word, then swapping those index information across different color tokens should also swap the corresponding attention patterns observed in the Output Head.</p>
<p>Swapping Index Information</p>
<p>').</p>
<p>The red token will have index-in-quesiton=3rd from A (similarly blue will have '1st') on the K-side of the Output Head.If the Q-side expects a particular index from the K-side (e.g., 'SOS' in Q may carry relativeindex-on-LHS=3rd and expects tokens carrying index-in-question=3rd from K), a swap of the index information in K should lead to a predictable shift in which tokens the head attends to.We performed this perturbation in the K-circuit of the Output Head while freezing its Q-circuit.Indeed, when we swap only the position embedding of B and A on the Question-Broadcast Head's V (the most upstream node in the K-circuit), with everything else intact, we observe that the Output Head systematically "reverts" the attention from red to blue based on their swapped positions (Figure 11).</p>
<p>This intervention thus provides causal evidence that the Output Head's QK alignment relies on the index information on both sides passed through the sub-circuits.It does not merely degrade or randomly scramble the output head's behavior; rather, the predictions shift in a way directly consistent with our interpretation of how index information is encoded and matched between Q and K.The model's predictable response to this precise manipulation underscores that we have correctly identified the sufficient pathways.Overall, by performing causal backtracking, validating information flow through ablations, and finally applying targeted activation patching, we confirm that the compositional induction mechanism we uncovered is both interpretable and causally relevant to the model's behavior.</p>
<p>Discussion</p>
<p>In this work, we investigated how a compact transformer model achieves compositional induction on a synthetic function composition task.By combining path-patching analyses with causal ablations, we uncovered a detailed QK circuit that encodes index information from both the question and the function's LHS.We further demonstrated that precisely swapping these positional embeddings in the model's activations leads to predictable changes to behavior, thereby confirming the causal relevance of the discovered circuit.These results show that, even for complex functions, transformers can implement a structured and interpretable mechanism.</p>
<p>Limitations and Future Work</p>
<p>Model Scale.Our circuit analysis focused on a relatively small transformer.Establishing whether similar interpretable circuits exist in larger models remains an important open question to follow up.Manual Circuit Discovery.The techniques employed here required substantial human effort-path-patching, ablations, and extensive interpretation of attention heads.For largescale models, such manual approaches become less feasible.We therefore see a need for automated or semi-automated methods that can discover and interpret these circuits with less human input.Partial Perturbations.Although our targeted activations swaps successfully steered the Output Head's behavior, we have not demonstrated a complete perturbation of its predicted tokens.This is due to the distributed nature of the underlying mechanism (multiple heads fulfill similar roles).Coordinating interventions across all such heads will require systematic workflows, which we aim to develop in the future.</p>
<p>Despite these constraints, our work shows that disassembling transformer circuitry can yield two key benefits.First, it illuminates how compositional functions are mechanistically instantiated at the attention-head level.Second, it enables targeted, activation-based interventions that reliably steer model behavior.We hope these contributions will encourage further research on scalable circuit discovery methods and more automated interpretability approaches for large-scale models.</p>
<p>Task Structure</p>
<p>In each episode, the support set and question are concatenated into a single prompt for the encoder, with question tokens placed at the start.Question, primitive assignments, and function assignments are separated by '|' tokens, while primitive and function assignments are identified by '='.Overall, there are 6 possible colors and 9 symbols that may serve as either color primitives or function symbols.Each episode contains 2-4 function assignments and 3-4 color assignments.</p>
<p>A function may be a single-argument (arg func) or doubleargument (arg1 func arg2) function.The function's righthand side (RHS) describes how arguments are transformed, generated by randomly sampling up to length-5 sequences of arguments and mapping them to color tokens.Each prompt ends with an 'EOS' token.During decoding, the model begins with an 'SOS' token and iteratively appends each newly generated token until it emits 'EOS'.</p>
<p>We randomly generate 10,000 episodes for training and 2,000 for testing, ensuring that the primitive and function assignments in testing episodes do not overlap with those in the training set.</p>
<p>Path Patching</p>
<p>Path patching is a method for isolating how a specific source node in the network influences a particular target node.It proceeds in three runs:</p>
<ol>
<li>
<p>Clean Run: Feed the input through the model normally and cache all intermediate activations (including those of the source and target nodes).</p>
</li>
<li>
<p>Perturbed Run: Freeze all direct paths into the target node using their cached activations from the clean run.For the source node alone, replace its cached activation with meanablated values.Record the new, perturbed activation at the target node.</p>
</li>
<li>
<p>Evaluation Run: Supply the target node with the perturbed activation from Step 2, then measure any resulting changes in the model's output.This quantifies how the source node's contribution (altered via mean-ablation) affects the target node's behavior.</p>
</li>
</ol>
<p>Chained Path Patching.When analyzing circuits that span multiple nodes in sequence, we extend path patching in a chain-like manner.For instance, to evaluate a chain A → B → C:</p>
<p>• We first perform path patching on the sub-path B → C as usual.</p>
<p>• Next, to capture how A specifically influences B, we isolate and record A's effect on B via mean-ablation on all other inputs to B.</p>
<p>• Finally, we patch that recorded activation into B and evaluate its effect on C.</p>
<p>For a chain of length N, we run N + 1 forward passes, ensuring the measured impact on the target node reflects only the chained pathway.This approach precisely attributes the model's behavior to the intended sequence of dependencies.</p>
<p>Two Modes of Ablation.To assess how individual heads or nodes contribute to the target node, we use two complementary modes:</p>
<ol>
<li>
<p>Keep-only-one-head: Mean-ablate all direct paths to the target node except for one node, which retains its clean-run activation.If the target node's performance remains stable, this single node is sufficient for driving the relevant behavior.However, this method may fail when multiple heads each provide partial information that is only collectively sufficient.</p>
</li>
<li>
<p>Ablate-only-one-head: Keep all source nodes from the clean run except one, which is mean-ablated.If performance degrades, that ablated node is necessary.However, if the node's information is redundant or duplicated across other paths, the target node's performance will not significantly change.</p>
</li>
</ol>
<p>By combining both modes, we identified the putative QKcircuit of the output head.We then validate the circuits by inspecting the information they propagates and causally erasing the information by ablating specific upstream nodes.</p>
<p>R 2 Score</p>
<p>To quantify how much an activation dataset Y encodes a particular latent variable Z, we compute a linear regression of Z (one-hot encoded) onto Y and measure the explained variance: R 2 = 1 − SS res SS total .</p>
<p>An R 2 value of 1.0 indicates that Z fully explains the variance in Y, whereas an R 2 near 0.0 implies Z provides no information about Y.</p>
<p>Figure 1 :
1
Figure 1: (a) Schematic of the transfomer model and task.(b) The prompt and output format for the compositional induction task.</p>
<p>domly sampling primitive arguments to a function to produce color sequences (e.g., A S B might be expanded into a sequence [A][B][A][A][B], maximum length=5).</p>
<p>Algorithm 1
1
Pseudocode solving the function &amp; primitive composition problem # Define the question and symbol-color pairs (by Question-Broadcast and Primitive-Pairing Heads) question ← [ s 1 , func, s 2 ] # definition symbol to color ← { s i : c i | i = 1, . . ., n} # definition color to symbol ← { c i : s i | i = 1, . . ., n} # reverse definition # Define the function; Convert the function into a relational structure between the input and output (by Primitive-and Function-Retrieval Heads) func LHS ← [s 3 func s 4 ] # define function arguments func RHS ← [c 3 c 3 c 4 c 4 c 3 ] # define function outputs symbol to idx ← { s 3 :idx 1 , s 4 :idx 3 } # convert argument symbols to their index in array idx seq ← [ ] for color in func RHS do symbol ← color to symbol[color] idx ← symbol to idx[symbol] idx seq.append(idx) # idx seq = [ idx 1 , idx 1 , idx 3 , idx 3 , idx 1 ] in this case # Compose the output following the function's relational structure (by RHS-Scanner and Output Heads) output ← [ ] for idx in idx seq do symbol ← question[idx]</p>
<p>Figure 2 :
2
Figure 2: Summary of circuit for compositional generalization.Top, the example episode's input and output.For a-e, the yellow boxes indicate self-attention heads and the blue boxes indicate cross-attention heads.Titles refer to the functional attention heads that execute the steps (discussed in detail later).We unfold all relevant information superimposed in tokens' embeddings and highlight their roles in attention oper-</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: (a) Logit contributions of each decoder head to the logits of correct tokens.(b) Attention pattern of Dec-cross-1.5.(c)For Dec-cross-1.5, the percentage of attention focused on the next predicted token.(d) For Dec-cross-1.5, alignment (inner product) between its OV output (e.g., x red W v W o ) and the corresponding unembedding vector (e.g., Unemb red ).We estimated the null distribution by randomly sampling unembedding vectors.</p>
<p>Figure 5 :
5
Figure 5: (a) Top, contributions to Output Head's performance (percentage of attention on the correct next token) via K. Bottom, attention pattern of Enc-self-1.1.(b) Top, contributions to the Output Head's performance through the Primitive-Pairing Head's V .Bottom, attention pattern of Enc-self-0.5.</p>
<p>Figure 6 :
6
Figure 6: Principal Components Analysis (PCA) of token embeddings, colored by their associated index-in-question.Concretely, for a prompt like 'B S A | A=red | B=blue | ...', in (a), points are the Z of 'A' and 'B' in the support (A labeled 3rd, B labeled 1st); in (b), points are the Z of 'red' and 'blue' in the support (red labeled 3rd, blue labeled 1st); in (c), points are the K of 'red' and 'blue' in the support (red labeled 3rd, blue labeled 1st).The distinct clusters suggest strong index information.R 2 score quantifies the percentage of total variance explained by the index identity.</p>
<p>Figure 7 :
7
Figure 7: Schematic of the Q-circuit.The Output Head inherits its Q from Dec-cross-0.6, which aggregates positional information passed from Enc-self-1.0 and Enc-self-1.2.The Q-circuit encodes primitive symbols' relative-index-on-LHS.</p>
<p>Figure 8 :
8
Figure 8: (a) Contribution to Output Head's performance via Q.(b) Attention pattern of Dec-cross-0.6.</p>
<p>Figure 9 :
9
Figure 9: (a) Contribution to Output Head's performance via Q.(b) Contribution to Output Head's performance via the RHS-Scanner's V .(c) Attention pattern of Dec-cross-0.6.(d) and (e) Attention patterns of Enc-self-1.0 and Enc-self-1.2.</p>
<p>Figure 10 :
10
Figure 10: PCA for token embeddings labeled by relativeindex-on-LHS. Concretely, for an episode with prompt 'B S A | A=red | B=blue | A S B=blue red' and prediction 'SOS red blue EOS', in (a), points are the Z of 'SOS' and 'red' in the decoder input tokens (SOS is labeled 3rd, because SOS attends to the blue on function RHS, and B is the 3rd on the LHS; similarly, red is labeled 1st); in (b), points are the Q of decoder input tokens (SOS is labeled 3rd, red is labeled 1st).R 2 score quantifies the percentage of total variance explained by the index identity.</p>
<p>Concretely, we select two primitive symbols in the question (e.g., 'B S A | A=red | B=blue |...</p>
<p>Figure 11 :
11
Figure 11: Swapping position embeddings of tokens in the question causes a predictable realignment of attention in the Output Head through its K-circuit, confirming that the discovered QK circuit indeed encodes positional indices.</p>
<p>AcknowledgementCT was supported by the Friends of McGovern Fellowship.MJ was supported by the Simons Foundation.Appendix Transformer ModelWe adopt an encoder-decoder architecture, which naturally fits the task by allowing the encoder to process the prompt (question + support) with bidirectional self-attention and the decoder to generate an output sequence with causal and cross-attention.Specific hyperparameters include:• Token embedding dimension: d model = 128• Attention embedding dimension: d head = 16• Eight attention heads per layer (both encoder and decoder)• Pre-LayerNorm (applied to attention/MLP modules) plus an additional LayerNorm at the encoder and decoder outputs• Standard sinusoidal positional embeddingsThe encoder comprises two layers of bidirectional selfattention + MLP, while the decoder comprises two layers of causal self-attention + cross-attention + MLP.We train the model by minimizing the cross-entropy loss (averaged over tokens) using the Adam optimizer.The learning rate is initialized at 0.001 with a warm-up phase over the first epoch, then linearly decays to 0.00005 over training.We apply dropout of 0.1 to both input embeddings and internal Transformer layers, and train with a batch size of 25 episodes.All experiments are performed on an NVIDIA A100 GPU.
Finding transformer circuits with edge pruning. A Bhaskar, A Wettig, D Friedman, D Chen, The thirty-eighth annual conference on neural information processing systems. 2024, November</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, . . Amodei, D , arXiv [cs.CL]2020. May</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, . . Zhang, Y , arXiv [cs.CL]Sparks of artificial general intelligence: Early experiments with GPT-4. 2023, March</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, . . Zaremba, W , arXiv [cs.LG]Evaluating large language models trained on code. 2021, July</p>
<p>Towards automated circuit discovery for mechanistic interpretability. A Conmy, A N Mavor-Parker, A Lynch, S Heimersheim, A Garriga-Alonso, arXiv [cs.LG]2023, April</p>
<p>Deepseek-Ai Liu, A Feng, B Xue, B Wang, B Wu, B , arXiv [cs.CL]DeepSeek-V3 technical report. 2024, December</p>
<p>N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, . . Olah, C , A mathematical framework for transformer circuits. 2021</p>
<p>The language of thought. J A Fodor, 1979Harvard University PressLondon, England</p>
<p>N Goldowsky-Dill, C Macleod, L Sato, A Arora, arXiv [cs.LG]Localizing model behavior with path patching. 2023, April</p>
<p>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. M Hanna, O Liu, A Variengien, Thirtyseventh conference on neural information processing systems. 2023. November</p>
<p>Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks. T He, D Doshi, A Das, A Gromov, The thirty-eighth annual conference on neural information processing systems. 2024. November</p>
<p>A circuit for python docstrings in a 4-layer attention-only transformer. S Heimersheim, J Janiak, 2023</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv [cs.CY]Measuring massive multitask language understanding. 2020, September</p>
<p>Explaining the transformer circuits framework by example. F Hofst Ätter, 2023</p>
<p>Efficient automated circuit discovery in transformers using contextual decomposition. A R Hsu, G Zhou, Y Cherapanamjeri, Y Huang, A Y Odisho, P R Carroll, B Yu, arXiv [cs.AI]2024. June</p>
<p>Compositionality decomposed: how do neural networks generalise?. D Hupkes, V Dankers, M Mul, E Bruni, arXiv [cs.CL]2019August)</p>
<p>Human-like systematic generalization through a meta-learning neural network. B M Lake, M Baroni, Nature. 2023, October</p>
<p>Building machines that learn and think like people. B M Lake, T D Ullman, J B Tenenbaum, S J Gershman, arXiv [cs.AI]2016, April</p>
<p>Causal scrubbing: a method for rigorously testing interpretability hypotheses. Garriga-Alonso Lawrencec, A Goldowsky-Dill, N Radhakrishnan, A Buck, N Thomas, 2022redwood research</p>
<p>Progress measures for grokking via mechanistic interpretability. N Nanda, L Chan, T Lieberum, J Smith, J Steinhardt, The eleventh international conference on learning representations. nostalgebraist. 2022. September. 2020interpreting GPT: the logit lens</p>
<p>. C Olsson, N Elhage, N Nanda, N Joseph, N Dassarma, T Henighan, . . Olah, C , 2022, SeptemberIn-context learning and induction heads. arXiv [cs.LG</p>
<p>A practical review of mechanistic interpretability for transformer-based language models. D Rai, Y Zhou, S Feng, A Saparov, Z Yao, arXiv [cs.AI]2024, July</p>
<p>C Sanford, D Hsu, M Telgarsky, arXiv [cs.LG]Transformers, parallel computation, and logarithmic depth. 2024. February</p>
<p>Attention is all you need. A Vaswani, N M Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Neural Inf Process Syst. 302017. JunePolosukhin, I</p>
<p>Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. K R Wang, A Variengien, A Conmy, B Shlegeris, J Steinhardt, The eleventh international conference on learning representations. 2022. September</p>
<p>Understanding the expressive power and mechanisms of transformer for sequence modeling. M Wang, E , W , arXiv [cs.LG]2024. February</p>
<p>M Wang, R Yu, E , W Wu, L , arXiv [cs.LG]How transformers get rich: Approximation and dynamics analysis. 2024, October</p>
<p>Transformer-based models are not yet perfect at learning to emulate structural recursion. D Zhang, C Tigges, Z Zhang, S Biderman, M Raginsky, T Ringer, Trans. Mach. Learn. Res. 2024. January. 2024</p>
<p>Complexity control facilitates reasoning-based compositional generalization in transformers. Z Zhang, P Lin, Z Wang, Y Zhang, Z.-Q J Xu, arXiv [cs.CL]2025. January</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1162 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1162</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1162</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-220486391</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2007.05270v1.pdf" target="_blank">Learning to plan with uncertain topological maps</a></p>
                <p><strong>Paper Abstract:</strong> We train an agent to navigate in 3D environments using a hierarchical strategy including a high-level graph based planner and a local policy. Our main contribution is a data driven learning based approach for planning under uncertainty in topological maps, requiring an estimate of shortest paths in valued graphs with a probabilistic structure. Whereas classical symbolic algorithms achieve optimal results on noise-less topologies, or optimal results in a probabilistic sense on graphs with probabilistic structure, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, for instance visual information available at each location of the map. Compared to purely learned neural white box algorithms, we structure our neural model with an inductive bias for dynamic programming based shortest path algorithms, and we show that a particular parameterization of our neural model corresponds to the Bellman-Ford algorithm. By performing an empirical analysis of our method in simulated photo-realistic 3D environments, we demonstrate that the inclusion of visual features in the learned neural planner outperforms classical symbolic solutions for graph based planning.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1162.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1162.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat/Gibson topological navigation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical navigation with uncertain topological maps (photo-realistic 3D environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical navigation setup where an agent builds an uncertain topological map (nodes with visual features, distance matrix D and edge-probability matrix E) from an exploratory rollout and uses a learned graph neural planner (with recurrent message-aggregation) to propose high-level waypoints passed to a local RL point-goal policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat simulator using the Gibson dataset</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor household/apartment navigation in photo-realistic 3D environments (Gibson) where the agent receives RGB-D observations and must reach an image-specified goal; topological maps (nodes = places, valued edges = connection probabilities and distances) are constructed from exploration rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Valued (probabilistic) fully-described N×N connectivity matrix E ∈ [0,1] (edge probabilities) plus distance matrix D; during test the graph is represented as fully connected with per-edge probabilities (i.e., dense probabilistic graph), and symbolic baselines use thresholding or a cost combining distance and -log(probability). Connectivity is noisy/uncertain (estimates from explorative policy).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Graphs are limited to a maximum node count k (paper uses graphs with N≈32 nodes as training instances are 32×32 source-target combinations); dataset: 72 training environments, up to 74,000 training graphs generated.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Hierarchical agent: Neural graph planner (GNN with recurrent GRU message aggregation) + local point-goal policy (recurrent AtariNet trained with PPO); exploratory policy (PPO) for graph construction.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>High-level: supervised Graph Neural Network that consumes node visual features, per-node connection probabilities e_i (row of E), distances d_i (row of D), target indicator, and identity; uses gated linear message computation and a serialized recurrent (GRU) aggregation over neighbors to emulate min/argmin (Bellman-Ford inductive bias) and outputs a distribution over next waypoint nodes. Low-level: RL-trained recurrent point-goal policy (AtariNet variant) trained with PPO, discrete actions (forward, turn left/right, STOP), inner-loop horizon m (default 10), dense shaping reward reducing geodesic distance and large terminal reward.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Coverage-based reward (occupancy grid cells first-observed) and average return during explorative rollout; auxiliary binary line-of-sight classification accuracy used to measure edge estimation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Exploration reward: +0.1 for each new observed occupancy cell (grid spacing 10 cm). Numeric aggregate exploration performance shown in paper figures but no single scalar reported in text for overall coverage; line-of-sight prediction accuracy plotted (Figure 4b) but numeric not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Hierarchical system success rates (low-level trajectories) from Table 2: Neural planner (deterministic argmax) 98.3% success (SPL 0.877); Neural planner (sampling) 96.6% success (SPL 0.796). Baselines: Symbolic (custom cost) 70.7%, Symbolic (threshold) 62.1%, Recurrent image-goal agent 54.8%, Random 15.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical planning-based policy: a learned graph-based planner (GNN with recurrent inductive bias and visual node features) supplying waypoints to a local RL point-goal controller; this combined approach yields best performance under uncertain connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Key reported relationships: (1) Uncertainty in edge connectivity (E noisy) degrades classical symbolic planners that use only E and D (thresholding or cost weighting), causing low accuracy; (2) Neural planner leveraging visual node features can infer likely traversability and recover GT shortest-path choices, outperforming symbolic baselines under uncertainty; (3) An inductive bias (serial GRU aggregation to approximate min/argmin like Bellman‑Ford) materially improves learning and final performance, especially with larger training data; (4) Sampling from the planner's distribution (instead of deterministic argmax) helps escape loops and local minima caused by graph errors; (5) The planner's message-passing depth/number of iterations k must be at least the largest graph span (so larger-diameter graphs require more rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>No explicit systematic comparison across different graph topologies (e.g., different diameters, clustering coefficients, or engineered dead-end frequencies) is reported; comparisons are across noise levels/inputs (probabilities vs ground-truth adjacencies) and modality mixing, not across structural topology families.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Findings linking policy structure to topology: (a) High-level planning benefits from an architecture that can represent iterative min/argmin updates (recurrent aggregation over neighbors) — this inductive bias is important for approximating shortest-path algorithms and coping with uncertain connectivity; (b) Hierarchical decomposition (graph planner + local policy) reduces required planning frequency — inner-loop horizon m can be increased (up to ~20 steps) without loss of performance, implying hierarchical policies are robust to lower planning rates; (c) Policies that incorporate visual node features can implicitly compensate for missing/erroneous connectivity information; (d) Sampling policies (non-deterministic waypoint selection) help avoid failure modes induced by topological estimation errors (loops, blocked edges).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to plan with uncertain topological maps', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Habitat: A Platform for Embodied AI Research <em>(Rating: 2)</em></li>
                <li>Learning to explore using active neural slam <em>(Rating: 2)</em></li>
                <li>Semi-parametric topological memory for navigation <em>(Rating: 2)</em></li>
                <li>Learning topological maps with weak local odometric information <em>(Rating: 2)</em></li>
                <li>Neural SLAM <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1162",
    "paper_id": "paper-220486391",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Habitat/Gibson topological navigation",
            "name_full": "Hierarchical navigation with uncertain topological maps (photo-realistic 3D environments)",
            "brief_description": "A hierarchical navigation setup where an agent builds an uncertain topological map (nodes with visual features, distance matrix D and edge-probability matrix E) from an exploratory rollout and uses a learned graph neural planner (with recurrent message-aggregation) to propose high-level waypoints passed to a local RL point-goal policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Habitat simulator using the Gibson dataset",
            "environment_description": "Indoor household/apartment navigation in photo-realistic 3D environments (Gibson) where the agent receives RGB-D observations and must reach an image-specified goal; topological maps (nodes = places, valued edges = connection probabilities and distances) are constructed from exploration rollouts.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Valued (probabilistic) fully-described N×N connectivity matrix E ∈ [0,1] (edge probabilities) plus distance matrix D; during test the graph is represented as fully connected with per-edge probabilities (i.e., dense probabilistic graph), and symbolic baselines use thresholding or a cost combining distance and -log(probability). Connectivity is noisy/uncertain (estimates from explorative policy).",
            "environment_size": "Graphs are limited to a maximum node count k (paper uses graphs with N≈32 nodes as training instances are 32×32 source-target combinations); dataset: 72 training environments, up to 74,000 training graphs generated.",
            "agent_name": "Hierarchical agent: Neural graph planner (GNN with recurrent GRU message aggregation) + local point-goal policy (recurrent AtariNet trained with PPO); exploratory policy (PPO) for graph construction.",
            "agent_description": "High-level: supervised Graph Neural Network that consumes node visual features, per-node connection probabilities e_i (row of E), distances d_i (row of D), target indicator, and identity; uses gated linear message computation and a serialized recurrent (GRU) aggregation over neighbors to emulate min/argmin (Bellman-Ford inductive bias) and outputs a distribution over next waypoint nodes. Low-level: RL-trained recurrent point-goal policy (AtariNet variant) trained with PPO, discrete actions (forward, turn left/right, STOP), inner-loop horizon m (default 10), dense shaping reward reducing geodesic distance and large terminal reward.",
            "exploration_efficiency_metric": "Coverage-based reward (occupancy grid cells first-observed) and average return during explorative rollout; auxiliary binary line-of-sight classification accuracy used to measure edge estimation quality.",
            "exploration_efficiency_value": "Exploration reward: +0.1 for each new observed occupancy cell (grid spacing 10 cm). Numeric aggregate exploration performance shown in paper figures but no single scalar reported in text for overall coverage; line-of-sight prediction accuracy plotted (Figure 4b) but numeric not provided in text.",
            "success_rate": "Hierarchical system success rates (low-level trajectories) from Table 2: Neural planner (deterministic argmax) 98.3% success (SPL 0.877); Neural planner (sampling) 96.6% success (SPL 0.796). Baselines: Symbolic (custom cost) 70.7%, Symbolic (threshold) 62.1%, Recurrent image-goal agent 54.8%, Random 15.2%.",
            "optimal_policy_type": "Hierarchical planning-based policy: a learned graph-based planner (GNN with recurrent inductive bias and visual node features) supplying waypoints to a local RL point-goal controller; this combined approach yields best performance under uncertain connectivity.",
            "topology_performance_relationship": "Key reported relationships: (1) Uncertainty in edge connectivity (E noisy) degrades classical symbolic planners that use only E and D (thresholding or cost weighting), causing low accuracy; (2) Neural planner leveraging visual node features can infer likely traversability and recover GT shortest-path choices, outperforming symbolic baselines under uncertainty; (3) An inductive bias (serial GRU aggregation to approximate min/argmin like Bellman‑Ford) materially improves learning and final performance, especially with larger training data; (4) Sampling from the planner's distribution (instead of deterministic argmax) helps escape loops and local minima caused by graph errors; (5) The planner's message-passing depth/number of iterations k must be at least the largest graph span (so larger-diameter graphs require more rounds).",
            "comparison_across_topologies": false,
            "topology_comparison_results": "No explicit systematic comparison across different graph topologies (e.g., different diameters, clustering coefficients, or engineered dead-end frequencies) is reported; comparisons are across noise levels/inputs (probabilities vs ground-truth adjacencies) and modality mixing, not across structural topology families.",
            "policy_structure_findings": "Findings linking policy structure to topology: (a) High-level planning benefits from an architecture that can represent iterative min/argmin updates (recurrent aggregation over neighbors) — this inductive bias is important for approximating shortest-path algorithms and coping with uncertain connectivity; (b) Hierarchical decomposition (graph planner + local policy) reduces required planning frequency — inner-loop horizon m can be increased (up to ~20 steps) without loss of performance, implying hierarchical policies are robust to lower planning rates; (c) Policies that incorporate visual node features can implicitly compensate for missing/erroneous connectivity information; (d) Sampling policies (non-deterministic waypoint selection) help avoid failure modes induced by topological estimation errors (loops, blocked edges).",
            "uuid": "e1162.0",
            "source_info": {
                "paper_title": "Learning to plan with uncertain topological maps",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Habitat: A Platform for Embodied AI Research",
            "rating": 2,
            "sanitized_title": "habitat_a_platform_for_embodied_ai_research"
        },
        {
            "paper_title": "Learning to explore using active neural slam",
            "rating": 2,
            "sanitized_title": "learning_to_explore_using_active_neural_slam"
        },
        {
            "paper_title": "Semi-parametric topological memory for navigation",
            "rating": 2,
            "sanitized_title": "semiparametric_topological_memory_for_navigation"
        },
        {
            "paper_title": "Learning topological maps with weak local odometric information",
            "rating": 2,
            "sanitized_title": "learning_topological_maps_with_weak_local_odometric_information"
        },
        {
            "paper_title": "Neural SLAM",
            "rating": 1,
            "sanitized_title": "neural_slam"
        }
    ],
    "cost": 0.010142499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to plan with uncertain topological maps
10 Jul 2020</p>
<p>Edward Beeching 
CITI Lab. INSA Lyon
INRIA Chroma team
France</p>
<p>Jilles Dibangoye 
CITI Lab. INSA Lyon
INRIA Chroma team
France</p>
<p>Olivier Simonin 
CITI Lab. INSA Lyon
INRIA Chroma team
France</p>
<p>Christian Wolf 
Université de Lyon
INSA-Lyon, LIRIS
CNRS
France</p>
<p>Learning to plan with uncertain topological maps
10 Jul 20203EE840EF9C92FBCC8296725444CD17D8arXiv:2007.05270v1[cs.LG]Visual navigationtopological mapsgraph neural networks
We train an agent to navigate in 3D environments using a hierarchical strategy including a high-level graph based planner and a local policy.Our main contribution is a data driven learning based approach for planning under uncertainty in topological maps, requiring an estimate of shortest paths in valued graphs with a probabilistic structure.Whereas classical symbolic algorithms achieve optimal results on noise-less topologies, or optimal results in a probabilistic sense on graphs with probabilistic structure, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, for instance visual information available at each location of the map.Compared to purely learned neural white box algorithms, we structure our neural model with an inductive bias for dynamic programming based shortest path algorithms, and we show that a particular parameterization of our neural model corresponds to the Bellman-Ford algorithm.By performing an empirical analysis of our method in simulated photo-realistic 3D environments, we demonstrate that the inclusion of visual features in the learned neural planner outperforms classical symbolic solutions for graph based planning.</p>
<p>Introduction</p>
<p>A critical part of intelligence is navigation, memory and planning.An animal that is able to store and recall pertinent information about their environment is likely to exceed the performance of an animal whose behavior is purely reactive.Many control and navigation problems in partially observed 3D environments involve long term dependencies and planning.It has been shown that humans and other animals navigate through the use of waypoints combined with a local locomotion policy [55,22].In this work, we mimic this strategy by proposing a hierarchical planner, which performs high-level long term planning using an uncertain topological map (a valued graph including visual features) combined with a local RL-based policy navigating between high-level waypoints proposed by the graph planner.Our main contribution is a way to combine symbolic planning Fig. 1: A trained agent navigates to a goal location with a hierarchical planner.A high-level planner proposes new target nodes in a topological map (a graph), which are used as an objective for a local point-goal policy.The graph is estimated from an explorative rollout and, as such, uncertain: the opacities of the edges correspond to estimations of connectivity between nodes (darker lines = higher confidence).In this example we observe a low probability of connection between the node at the agent's position (orange) and its nearest neighbor, whereas from the visual observation associated to the node we can see there is a traversable space between the two nodes.</p>
<p>with machine learning, and we look to structure a neural network architecture to incorporate landmark based planning in unseen 3D environments.</p>
<p>When solving visual navigation tasks, biological or artificial agents require an internal representation of the environment if they want to solve more complex tasks than random exploration.We target a scenario where an agent is trained on a large-scale set of 3D environments to learn to reason on planning and navigation.When faced with a previously unseen environment, the agent is given the opportunity to build a representation by doing an explorative rollout from a previously learned explorative policy.It can then exploit this internal representation in subsequent visual navigation tasks.This corresponds to many realistic situations, where robots are deployed to indoor environments and are allowed to familiarize themselves before performing their tasks [43].</p>
<p>Our agent constructs an imperfect topological map of its environment in the form of a graph, where nodes correspond to places and valued edges to connections.Edges are assigned two different values, the first one being spatial distances, the second one being probabilities indicating whether it is possible to navigate between the two nodes.Nodes are also assigned rich visual features extracted from images taken at the corresponding places in the environment.After deployment, the agent faces visual navigation tasks requiring it to find a specific location in the environment provided by a set of images corresponding to different viewpoints, extending the task proposed in [63].The objective is to identify the goal location in the internal representation, and to provide an estimate for the shortest path to it.The main difficulty we address here is the fact that this path is an estimate only, since the ground truth path is not available during testing / deployment.</p>
<p>Whilst planning in graphs with known connectivity has been solved for many decades [15,7], planning under uncertainty remains an ongoing area of research.Whereas optimal results in a probabilistic sense exist for graphs with probabilistic connectivity, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, in particular features extracted from image observations associated with specific nodes.We train a graph neural network in a fully supervised way to predict estimates of the shortest path, using vision to overcome uncertainty in the connectivity information.We present a new variant of graph neural networks imbued with specific inductive bias, and we show that this structure can be parameterized to fallback to the classical Bellman-Ford algorithm.</p>
<p>Figure 1 illustrates the hierarchical planner: a neural graph based planner runs an outer loop providing estimates for next way-point on a graph, which are used as target nodes for a local RL-based policy running an inner loop and providing feedback to high-level planner on reached locations.Both planners take into account visual features, either stored in the graph (graph based planner), or directly as observations provided by the environment (local policy).The two planners are trained separately -the graph based planner in a fully supervised way from ground truth graphs, the local policy with RL and a point-goal strategy.This work makes the following contributions:</p>
<p>-A hierarchical model combining high-level graph based planning with a local point goal policy for robot navigation; -A trainable high-level neural planner which combines an uncertain topological map (graph) with rich node features to learn to estimate shortest paths in noisy and unknown environments.-A variant of graph networks encoding inductive bias inspired by dynamic programming-based shortest path algorithms.-We evaluate the performance of the method in challenging and visually realistic 3D environments and show that it outperforms optimal symbolic planning on noisy topological maps.</p>
<p>Related work</p>
<p>Classical planning and graph search -A large body of work is available on classical planning on graphs, notable references include [31,42].In robotics, there have been a number of works applying classical planning in topological maps for indoor robot navigation, for instance [48,54].</p>
<p>Planning under imperfect information -In many realistic robotic problems, the current state of the world is unknown.Though sensor observations provide measurements about the current state of the world, these measurements are usually incomplete or noisy because of disturbances that distort their values.</p>
<p>Planning problems that face these issues are referred to as planning problems under imperfect information.Research on this topic has a long history, which can be traced back to the seminal work by [2] presenting the first non-trivial exact dynamic programming algorithm for partially observable Markov decision processes (POMDPs).While there are other models [31, chap 12], POMDPs emerged as the standard framework to formalize and solve (single-agent) sequential decision-making problems with imperfect information about the state of the world [26].Since the agent does not have access to the actual state of the world, it acts based solely on its entire history of actions and observations, or the corresponding belief state, i.e., the posterior probability distribution over the states given the history [2,50].Approaches for finding optimal solutions have been intensively investigated in the 2000s, ranging from dynamic programming [26] to heuristic search methods [51,30].Key to these approaches is the idea that one can recast the original problem into a continuous-state fully observable Markov decision process, where states are belief states or histories [2].Doing so allows theory and algorithm that applies for MDPs to also apply to POMDPs, albeit in much larger (and possibly continuous) state space.Another significant result of this literature is proof that the optimal value function is a piece-wise linear and convex function of the belief states, which allows the design of algorithms with faster rates of convergence [50].For a thorough discussion on existing solvers for POMDPs, the reader can refer to [47].</p>
<p>Deep Reinforcement Learning -The field of Deep Reinforcement Learning (RL) has gained attention with successes on board games [49] and Atari games [37].Recent works have applied Deep RL for the control of an agent in 3D environments [36] [24], exploring the use of auxiliary tasks such as depth prediction, loop detection and reward prediction to accelerate learning.Other recent work uses street-view scenes to train an agent to navigate in city environments [35].To infer long term dependencies and store pertinent information about the partially observable environment, network architectures typically incorporate recurrent memory such as Gated Recurrent Units [13] or Long Short-Term Memory [23].Extensions to memory based neural approaches began with Neural Turing Machines [19] and Differentiable Neural Computers [20], and have since been adapted to expand the capacity of Deep RL agents [56].Spatially structured memory architectures have been shown to augment an agent's performance in 3D environments and are broadly split into two categories: metric maps which discretize the environment into a grid based structure and topological maps which produce node embeddings at key points in the environment.Research in learning to use a metric map is extensive and includes spatially structured memory [40],</p>
<p>Neural SLAM based approaches [61] and approaches incorporating projective geometry and neural memory [21,8], these techniques are combined, extended and evaluated in [6].Other notable works in include that of Value Iteration Networks (VIN) [53] which approximate the value iteration algorithm with a CNN, applied planning in small fully observable state spaces (grid worlds).While VIN and our work structure planners, VINs use convolutions to approximate classical value iteration, while we use a graph representation and a novel GNN architecture with recurrent updates to approximate the Bellman-Ford algorithm.[27] plans under uncertainty in partially observable gridworld environments.Here uncertainty refers to POMPs, the classical QMDP algorithm is used as inductive bias for a neural network, whereas in our work uncertainty is over node connectivity in a graph constructed in a previously unseen environment.[52] which is applied in observable state spaces to learn a forward model in a latent space to plan appropriate actions; they are not hierarchical, are not graph-based and do not appear to plan under uncertainty.Similar to ours, they are applied to goal driven problems.</p>
<p>Research combining learning, navigation in 3D environments and topological representations has been limited in recent years with notable works being [43] who create graph a through random exploration in ViZDoom RL environment [28].[16] also performs planning in 3D environments on a graph-based structure created from randomly sampled observations, with node distances estimated with value estimates.The downside of these approaches is that in order to generalize to an unseen environment, many random samples must be taken in order to populate the graph.</p>
<p>Graph neural networks -Graph Neural Networks (GNN) are deep networks that operate on graphs directly.They have recently shown great promise in domains such as knowledge graphs [44], chemical analysis [18], protein interactions [17], physics simulations [3] and social network analysis [29].These types of architectures enable learning from both node features and graph connectivity.Several review papers have covered graph neural networks in great detail [9,4,57,62].GNNs have been applied to shortest path planning in travelling salesmen problems [33,25] and it has been reasoned that they can approximate optimal symbolic planning algorithms such as the Bellman-Ford algorithm [60].This work applies a novel variant of GNN in order to solve approximate planning problems, where classical methods may struggle to deal with uncertainty.</p>
<p>Hierarchical navigation with uncertain graphs</p>
<p>We train an agent to navigate in a 3D visual environment and to exploit an internal representation, which it is allowed to obtain from an explorative rollout before the episode.Our objective is image goal, i.e. target-driven navigation to a location which is provided through a (visual) image.We extend the task introduced in [63] by generalizing to unseen environment configurations without the need to retrain the agent for a novel environment.</p>
<p>From the explorative rollout obtained with an agent trained with RL, which is further described in section 3.3, we create an uncertain topological map covering the environment, i.e. a valued graph G={V, V , E, L, D}, where V={1, . . .N } is a set of nodes, V is a K×N matrix of rich visual node features of dimensions K, E ∈ [0, 1] N ×N is a set of edge probabilities where E i,j is the probability of having an edge between nodes i and j, L is a matrix of node locations and D is a distance matrix, where D i,j is a distance between nodes i and j.While D encodes a distance in a path planning sense, E encodes the probability of j being directly accessible from i with obstructions.The uncertainty encoded by this probability can be considered to be a combination of aleatory variability, i.e. uncertainty associated with natural randomness of the environment, as well as epistemic uncertainty, i.e. uncertainty associated with variability in computational models for estimating the graph, in our case the explorative policy trained with RL and taking into account visual observations.</p>
<p>Once the topological map is obtained, the objective of the agent at each episode is to navigate to a location given an image, which is provided as additional observation at each time step.The agent acts in 3D environments like Habitat [34] (see section 5), receiving images of the environment as observations and predicting actions from a discrete space (forward, turn left 10 • , turn right 10 • ).We propose a hierarchical planner performing actions at two different levels:</p>
<p>A high-level graph based planner that operates on longer time scale τ and iteratively proposes new point-goals nodes p τ g that are predicted, by a Graph Neural Network, to be on the shortest path from the agent to the estimated location of the target image.A local policy that has been trained to navigate to a local point-goal p τ g , which has been provided by the high-level policy.The local policy operates for a maximum of m time-steps, where m is a hyper-parameter, set to 10.The agent has been trained with an additional STOP action, so that it can learn to terminate the local policy in the case that it reaches p τ g in under m steps.</p>
<p>The two planners communicate through estimated locations, the graph planner indicating the next waypoint to the local policy as a location, and the local policy (after termination) providing an estimate of its reached location back to the high-level planner.The planner updates its current node estimate as the nearest neighboring node and planning continues.</p>
<p>High-level planning with uncertain graphs</p>
<p>The objective of the high-level planner is to estimate the shortest path from the current position S∈V in the graph to a terminal node T ∈V, whose identity is estimated as the node whose visual features are closest to the target image in cosine distance.Planning takes into account the distances between nodes encoded in D as well as estimated edge connectivity encoded in E. As an edge (i, j) may have a large connection probability E i,j but still be obstructed in reality, the goal is to learn a trainable planner parameterized by parameters θ, which takes into account visual features V to overcome the uncertainty in the graph connectivity.To this end, we assume the ground truth connectivity E * available during training only.Figure 2 illustrates the different types of solutions this problem admits: the optimal shortest path is only available on ground truth data (Figure 2a), the objective is to use the noisy uncertain graph (Figure 2b) and provide an estimate of the optimal solution taking into account visual features (Figure 2d).This is unlike the optimal solution in a probabilistic sense calculated from a symbolic algorithm (Figure 2c).We propose a trainable planner, which consists of a novel graph neural network architecture with dedicated inductive bias for planning.Akin to graph networks [5], the node embeddings are updated with messages over the edges, which propagate information over the full graph.While it has been shown that graph networks can be trained to perform planning [59], we aim to closely mimic the structure of the Bellman-Ford algorithm and we embue the planner with additional inductive bias and a supervised objective to explicitly learn to calculate shortest paths from data.To this end, each node i of the graph is assigned an embedding x i = [v i , e i , t i , d i , s i ] where v i are visual features from the memory matrix V , t i is a boolean value indicating if the node is the target, e i are the edge connection probabilities from node i to all other nodes, d i are the distances for node i to all other nodes, s i is a one hot vector identifying the node (part of the identity matrix I).</p>
<p>We motivate our proposed neural model with the following objective: the planner should be able to exploit information contained in the graph connectivity, but also in the visual features, to be able to find the shortest path from a given current node to a given target node.As with classical planning algorithms, it will thus eventually be required to keep for each node a latent representation of the bound d i on the shortest distance as well as information on the identity of the outgoing edge to the neighbor lying on the shortest path, the predecessor function Π(i).Known algorithms (Dijstra, Bellman-Ford) perform iterative updates of these variables (d i , Π i ) by comparing them with neighboring nodes and the corresponding inter-node distances, updating the bound d i and Π i when a shorter path is found than the current one.This is usually done by iterating over the successors of a given node i.</p>
<p>In our trained model, these variables are not made explicit, but they are supposed to be learned as a unique vectorial latent representation for each node i in the form of an internal state r i , which generally holds current information on the reasoning of the agent.The input to each iteration of the graph network is, for each node i, the node embedding x i , and the node state r i , which we concatenate to form a single node vector n i :
n i = [x i , r i ] = [v i , e i , t i , d i , s i , r i ]
As classically done in graph neural networks, this representation is updated iteratively by exchanging messages between nodes in the form of trainable functions.The messages and trainable functions of our model are given as follows, illustrated in Figure 3, and will be motivated in detail further below.
m i,j = W 1 [n i , n j ] σ(W 2 [n i , n j ])(1)r i = φ r←h ({m i,j } ∀j , h i )(2)
Here, is the Hadamard product, W . are weight matrices, and r i is the updated latent representation after one round of updates.The features x i do not change during these operations.Equation ( 1) is inspired from gated linear layers [14], and enables each node to identify whether it is the target, and update its representation of the bound.We use gated linear layers in order to provide the network with the capacity to update bound estimates for its neighbors.</p>
<p>Equation (2) integrates messages from all neighbors j of node i, updating its latent representation.Since planning requires this step to update internal bounds on shortest paths, akin to shortest path algorithms that rely on dynamic programming, we serialize the updates from different neighbors into a sequence of updates, which allows the network to learn to calculate minimum functions on bound estimates.In particular, we model this through a recurrent network in a Gated Recurrent Unit variant [12], using a hidden state vector h i associated to each node i.The step is structured to mimic the min operation of the Bellman-Ford algorithm (see section 3.2 for details on this equivalence).</p>
<p>Equation (2) can thus be rewritten in more detail as follows: Going sequentially over the different neighbors j of node i, the hidden state h i is updated as follows:
h [j] i = W 3 m i,j + W 4 h [j−1] i (3)
For simplicity, we omitted the gating equations of GRUs and presented a single layer GRU.In practice we include all gating operations and use a stacked GRU with two layers.The output of the recurrent unit is a non-linear function of the last hidden state, providing the new latent value r i :
r i = M LP (h N i )(4)
where M LP is a two-layer neural network with ReLU activations.The above messages are exchanged and accumulated for k steps where k is a hyper-parameter which should be at least the largest span of the graphs in the dataset.The action distribution f A (r i ) is then estimated for each node in the graph as a linear mapping of the node embeddings followed by a softmax activation function.
A i = f A (r i ) = softmax(W r i )(5)</p>
<p>Relations to optimal symbolic planners</p>
<p>As mentioned before, our neural planner could in theory be instantiated with a specific set of network parameters such that it corresponds to a known symbolic planner calculating an optimal path in a certain sense.To illustrate the relationship of the network structure, in particular the recurrent nature of the graph updates, we will layout details for the case where the planner performs the estimation of a shortest path given the distance matrix and ignoring the uncertainty information -an adaptation to an optimal planner in the probabilistic sense can be done in a straightforward manner.To avoid misunderstandings, we insist that the reasoning developed in this sub section is for illustration and general understanding of the chosen inductive network bias only, the real network parameters are fully trained with supervised learning as explained in section 4.</p>
<p>Handcrafting a parameterization requires imposing a structure on the node state r i , which otherwise is a learned representation.In our case, the node state will be composed of the bound b i on the shortest path from the given node to the target node (a scalar), and the current estimate Π i of the identity of predecessor node of node i w.r.t. the shortest path, which can be represented as a 1-in-K encoded vector indicating a distribution over nodes.</p>
<p>Standard Bellman-Ford symbolic bound updates iteratively update the bound for a given node i by examining all its neighbors j and checking whether a shorter path can be found passing through neighbor j.This can be written in a sequential form s.t. the bound gets updated iterating through the neighbors j=1. ..J i of node i: b
[0] i = b i b [j] i = min(b [j−1] i , b j + d ij ) b i = b [Ji] i (6)
where b i is the bound before the round of updates for node i, and b i is the bound after the round of updates for node i.</p>
<p>In our neural formulation, the message updates given in equation ( 2), further developed in (3), mimic the Bellman-Ford bound update given in Equations ( 6).This provided motivation for our choice of a recurrent neural network in the graph neural network, as we require the update of the recurrent state h j i in Equation ( 3) to be able to perform a minimum operation and an arg min operation (or differentiable approximations of min and arg min).</p>
<p>Graph creation from explorative rollouts</p>
<p>Graphs were generated during the initial rollout from an exploratory policy trained with Reinforcement Learning.During training, the agent interacts with training environments and receives RGB-D image observations calculated as a projection from the 3D environment.The agent is trained to explore the environment and to maximize coverage, i.e. to visit as much space as possible as quickly as possible similar to [10,11].</p>
<p>To learn to estimate the graph connectivity, we add an auxiliary loss to the agent's objective function, f link (o i , o j , h i ) which is trained to classify whether two locations are in line of sight of each other, conditioned on the visual features o i , o j from the two locations and the agent's hidden state h i .Node features were calculated with a CNN [32].Ground truth line of sight measurements were computed by 2D ray tracing on an occupancy map of each environment.In order to limit the size of the graph to a maximum number of nodes k, we aim to maximize each node's coverage of the environment using a Gaussian kernel function.At each time step a new node is observed by the agent, previous node positions are compared with a Gaussian kernel function (eq.7) in order to identify the index of the most redundant node r, which is removed from the graph and replaced with the new node, node connectivities are then recomputed with f link (.), where L i is the location of node i.
r = arg min i   j K(L i , L j )   , K(v, v ) = exp( − v − v 2 2σ 2 ),(7)</p>
<p>Training</p>
<p>The high-level graph based planneris trained in a purely supervised way.We generate ground truth labels by running a symbolic algorithm (Dijkstra [15]) on a set of valued ground truth training graphs described with the method detailed in Section 3.3.In particular, the supervised training algorithm takes as input uncertain/noisy graphs, which include visual features, and is supervised to learn to produce paths, which are calculated from known ground truth graphs unavailable during test time.During training we treat path planning as a classification problem where for a given target, each node must learn to predict the subsequent node on the optimal path to the target.Formally for each node i we predict a distribution A i and aim to match a ground-truth distribution A * i , which is a one-hot vector, minimizing cross entropy loss L(A, A * ) = − n i=1 A * i log A i .We augment training with a novel version of mod-drop [39], a training algorithm for multi-modal data, which drops modalities probabilistically during training.In our case, during training we extend the node connection probabilities in the input with the ground truth node adjacencies and mask either the probabilities or the adjacencies with a probability of 50%, during training we linearly taper the masking probability from 50% to 100% over the first 250 epochs.This ensures that the final model requires only connection probabilities, but the reasoning performed during message passing and recurrent updates can be bootstrapped from the ground truth adjacency matrix.Training curves on unseen validation data are shown in figure 5b.The local policyis a recurrent version of AtariNet [38] with two output heads for the action distribution and value estimates.The network was trained with a reinforcement learning algorithm Proximal Policy Optimization (PPO) [45] to navigate with discrete actions to a local point-goal.Point-goals were generated to be within 5m of the spawn location of agent.A dense reward was provided that corresponds to a decrease in geodesic distance to the target, a large reward (10.0) was provided when the agent reached the target and the STOP action was used.The episode was terminated when either the STOP action was used or after 500 time-steps.A small negative reward of -0.01 was given at each time-step to encourage the agent to complete the task quickly.The explorative policy for graph creationis trained with PPO [46].We aim to maximize coverage that is within the field of view of the agent.We create an occupancy grid of the environment with a grid spacing of 10cm.The first time a cell is observed the agent receives a reward of 0.1.A cell is considered to observable if it is free space, within 3m of the agent and in the field of view of the agent.Agent performance is shown in Figure 4b.</p>
<p>Experiments</p>
<p>We evaluated our method in simulated 3D environments, in particular the Habitat [34] simulator with the visually realistic Gibson dataset [58].During training, the agent interacts with 72 different training environments from Gibson, where each environment corresponds to a different apartment or house, and receives as input observation an observed RGB-D image.We evaluate our method on a set</p>
<p>Method</p>
<p>Acc H-SPL Symbolic (GT)</p>
<p>1.00 1.00 Neural planner (GT) 0.921 0.983 of 16 held out environments that were unseen during training by either the local policy, the exploratory policy or the high-level neural planner.</p>
<p>High-level graph-based planner</p>
<p>The neural planner was implemented in PyTorch [41], the hyper-parameters are given in the supplementary material.We compare two metrics, accuracy of prediction of the next way-point along the optimal path and the SPL metric [1], both for paths of length two or greater.As we evaluate SPL for both the high level planner and the hierarchical planner-controller, we refer to the high-level planner's SPL as H-SPL to avoid ambiguity.Symbolic baselines -We compare the neural planner to two symbolic baselines, both of which reason on the uncertain graph only, without taking into account rich node features.While these baselines are "optimal" with respect to their respective objective functions, they are optimal with respect to the amount of information available to them, which is uncertain: (i) Thresholding -In order to generate non-probabilistic edge connections, we threshold the connection probabilities with values ranging from 0-1 in steps of 0.1.After threshholding the graph, path planning was performed with Dijkstra's algorithm; (ii) A custom cost function for Dijkstra's algorithm weighting distances and probabilities: We vary the weighting λ in order to control the trade-off of distance and connection probability.In the limit where λ is 0, the graph is a fully connected graph, whereas high values of λ would lead to finding the most probable path.Results of both symbolic baselines for varying hyper-parameters are shown in Figure 4a, we observe that they perform poorly under uncertainty.In both cases we aim to evaluate the accuracy of their predictions with respect to the symbolic baseline on the ground truth graph, i.e.Dijkstra on the shortest path.As graphs can contain many source-target pairs that are within 1 step, we report accuracy on source-target pairs separated by at least 2 steps.
cost(i, j) = D i,j − λ log(E i,j )(8
Image driven recurrent baseline -We also compare to an end-to-end RL approach where the current observation and target image are provided to a CNN based RL agent trained from reward.The agent architecture is a siamese CNN with a recurrent GRU.We train with a dense reward of improvement in geodesic distance between the agent and the target, and provide a reward of 10 when the agent reaches the goal.We used PPO, and trained for 200 M environment frames.</p>
<p>In Table 1a, we compare the neural planner,the two symbolic baselines and the recurrent baseline.We can see, that even without visual features, the neural planner is able to outperform the "optimal" symbolic baselines.This can be explained with the fact, that the baselines optimize a fixed criterion, whereas the neural planner can learn to exploit patterns in the connection probability matrix E to infer valuable information on shortest ground truth path.The gap further increases when the neural planner can use visual features.The positive impact of modality mixing (see section 4) is shown in Figure 5b.</p>
<p>As a sanity check, table 1b compares the optimal symbolic planner against the neural planner trained with ground truth adjacencies provided as input.We observe that the results of the neural planner are close to optimum in this case.</p>
<p>We evaluated our approach with different amounts of training data, ranging from 8,000 graphs to 74,000 training graphs (Figure 5a).Note that one training  graph spawns 32×32 possible source-target combinations, leading to a maximum amount of 75,000,000 training instances.</p>
<p>Hierarchical planning and control (topological &amp; local policy)</p>
<p>We evaluated the neural graph planner coupled with the local policy.For a given episode, the graph planner estimates the next node in the path to a target image and provides its location to the local policy, which executes for m time-steps.The planner then re-plans from the nearest neighbor to the agent's current position, this back and forth process of planning and navigating continues until either the agent reaches the target or 500 low-level time-steps have been conducted.We report accuracy as percentage of runs completed successfully and SPL in table 2, albeit measured on low-level trajectories as opposed to graph space.We combine the local policy with various graph planners, and can see that the neural graph planners greatly outperform the symbolic baselines.We perform two evaluations of the neural planner; a deterministic evaluation where point-goals are chosen with the argmax of the A distribution and a non-deterministic one by sampling from A. The motivation is that by sampling, the planner can escape from local minima and loops created by errors in approximation.This is confirmed when studying rollouts from the agents, and also quantitatively through the performances shown  6, where in step 10 we can see that navigation is robust w.r.t.local errors in planning (the purple line crossing white non-traversable space).</p>
<p>Ablation: Effect of chosen inductive bias</p>
<p>As developed in sections 3.1 and 3.2, our graph based planner includes a particular inductive bias, which allows it to represent the Bellman-Ford algorithm for the calculation of shortest or best paths.This bias is implemented as a recurrent model (a GRU) running sequentially over the message passing procedures, as illustrated in Figure 3. Figure 7 ablates the effect of this additional bias as a function of data sizes ranging from 8,000 to 74,000 example graphs, each one evaluated with 32 2 =1,024 different combinations of starting and end points.</p>
<p>The differences are substantial, and we can see that our model is able to exploit increasing amounts of data and translates them into gains in performance, whereas standard graph convolutional networks don't -we conjecture that they lack in structure allowing them to pick up the required reasoning.</p>
<p>Conclusion</p>
<p>We demonstrated that path planning algorithms can be approximated with learning when structured in a manner that is akin to classical path planning algorithms.We have performed an empirical analysis of the proposed solution in photo-realistic 3D environments and have shown that in uncertain environments graph neural networks can outperform their symbolic counterparts by incorporating rich visual features as part of their planning procedure.Our method can be used to augment a vision based agent with the ability to form long term plans under uncertainty in novel environments, without a priori knowledge of the particular environment.We have analysed the empirical performance of the neural planning algorithm with a variety of dataset sizes, shown that the high-level planner can be coupled with a low-level policy and evaluated the hierarchical performance on an image-goal task.7 Supplementary material</p>
<p>Example graphs</p>
<p>Figure 8 shows example graphs extracted from three different environments extracted with the method described in section 3.3 of the main paper.The m parameter limits the maximum number of steps the local policy can take before giving control back to the high-level graph planner.We recall that the local policy can also decide to terminate the inner loop earlier through an explicit STOP action.We see that performance of the planner and policy is comparable up to 20 time-steps, which means the computationally costly planning step can performed less frequently than the low level control of the local policy, without a reduction in performance.</p>
<p>Hyper-parameters</p>
<p>Table 3 provides the hyper-parameters for the three different neural models used in this work:</p>
<p>the explorative policy used to create the graphs, trained through RL; the graph based high-level planner, trained in a supervised way, and the local policy, trained through RL.</p>
<p>Fig. 2 :
2
Fig. 2: Illustration of the different types of solutions to the high-level graph planning problem: (a) the ground truth graph (unavailable during testing) with the shortest path from node S to node T in red; (b) the uncertain graph available during test time.This graph is fully connected and for each edge a connection probability is available.For clarity we here show only edges where the connection probability is above a threshold.The edge from A→B is wrongly estimated as not connected; (c) an "optimal" path taking into account both probabilities and distances; (d) A learned shortest path, where the visual features at node A indicate passage to node B. We supervise a network to predict the GT path (a).</p>
<p>Fig. 3 :
3
Fig. 3: (a) An example graph; (b) One iteration of the neural graph planner's message passing and bound update.Incoming messages from neighbors are serialized and fed through a recurrent unit, which creates inductive bias for learning minima necessary for bound updates.</p>
<p>Fig. 4 :
4
Fig. 4: (a) Symbolic baselines Dijkstra on -left: thresholded probs., right: cost function(8).(b) Left: Average return.Right: Accuracy of line of sight predictions.</p>
<p>Fig. 5 :
5
Fig. 5: (a) Accuracy and H-SPL with increasing size of data when the GNN is trained with and without visual features.(b) Modality mixing (GT connections and probabilities), we observe a 2.3% improvement over single modality training.</p>
<p>Fig. 6 :
6
Fig. 6: Six time-steps from a rollout of the hierarchical planner (graph+local) in an unseen testing environment.For each time-step: left -RGB-D observation, right -map of the environment (unseen) with graph nodes, source node, target node, agent position(black), nearest neighbour to the agent, local point-goal provided by the high level planner and planned path.Further examples can be found in the supplementary material, including failure cases.</p>
<p>Fig. 7 :
7
Fig.7: Ablation of the addition of a GRU for the accumulation of incoming messages.This recurrent unit was added to ensure that the model could represent the Bellman-Ford algorithm.</p>
<p>Fig. 8 : 7 . 2
872
Fig. 8: Examples of top down maps and graphs from three environments.For each environment -Top-left: topological map with ground truth connectivities, top-right: topological map with connection probabilities estimated by the learned f link function with line opacity corresponding to the link probability, bottomleft: ground truth shortest path between a source an target node, bottom-right: shortest path estimated by the Graph Neural Planner.Note the very bottom-right prediction connects two nodes that are not connected in the ground truth, this example would not be counted as a valid path during evaluation of the SPL metric.</p>
<ol>
<li>3
3
Effect of the length m of the local policyIn figure11of this document we show the effect of the parameter m of the local policy, described in section 3, page 6, of the main document, when evaluated on a limited random subset of the validation data (1,200 problem instances).</li>
</ol>
<p>Fig. 9 :
9
Fig. 9: Six time-steps from a rollout of the hierarchical planner (graph+local) in an unseen testing environment.For each time-step: left -RGB-D observation, right -map of the environment (unseen) with graph nodes, source node, target node, agent position(black), nearest neighbour to the agent, local point-goal provided by the high level planner and planned path.</p>
<p>Fig. 10 :
10
Fig. 10: Failure case -Six time-steps from a rollout of the hierarchical planner (graph+local) in an unseen testing environment.For each time-step: left -RGB-D observation, right -map of the environment (unseen) with graph nodes, source node, target node, agent position(black), nearest neighbour to the agent, local point-goal provided by the high level planner and planned path.</p>
<p>Fig. 11 :
11
Fig. 11: Performance of the hierarchical agent with varied low level inner loop steps for a range of high-level path lengths.</p>
<p>Table 1 :
1
Reporting H-SPL and accuracy of the neural planner's predictions in unseen environments; neural planner trained with 72,000 graphs
(a) Results on uncertain graphs(b) Results on ground truth graphsMethodAcc H-SPLSymbolic (threshold)0.114 0.184Symbolic (custom cost) 0.115 0.269Neural (w/o visual)0.251 0.468Neural (w visual)0.262 0.501</p>
<p>Table 2 :
2
Performance of the hierarchical graph planner &amp; local policy
Method: Planner + Local policySuccess rate SPLGraph oracle (optimal point-goals, not comparable)0.9630.882Random0.1520.111Recurrent Image-goal agent0.5480.248Symbolic (threshold)0.6210.527Symbolic (custom cost)0.7070.585Neural planner (sampling)0.9660.796Neural planner (deterministic)0.9830.877in table 2. A visualization of steps from an episode is shown in Figure
Acknowledgements -This work was funded by grant Deepvision (ANR-15-CE23-0029, STPGP479356-15), a joint French/Canadian call by ANR &amp; NSERC; Compute was provided by the CNRS/IN2P3 Computing Center (Lyon, France), and by GENCI-IDRIS (Grant 2019-100964).
P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, A R Zamir, On evaluation of embodied navigation agents. 2018</p>
<p>Optimal control of markov processes with incomplete state information. K J Åström, Journal of Mathematical Analysis and Applications. 1011965</p>
<p>Interaction networks for learning about objects, relations and physics. P Battaglia, R Pascanu, M Lai, D J Rezende, Advances in neural information processing systems. 2016</p>
<p>P W Battaglia, J B Hamrick, V Bapst, A Sanchez-Gonzalez, V Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, arXiv:1806.01261Relational inductive biases, deep learning, and graph networks. 2018arXiv preprint</p>
<p>P Battaglia, J Hamrick, V Bapst, A Sanchez-Gonzalez, V Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, Ç Gülçehre, F Song, A Ballard, J Gilmer, G Dahl, A Vaswani, K Allen, C Nash, V Langston, C Dyer, N Heess, D Wierstra, P Kohli, M Botvinick, O Vinyals, Y Li, R Pascanu, 1807.09244Relational inductive biases, deep learning, and graph networks. 2018arXiv preprint</p>
<p>Egomap: Projective mapping and structured egocentric memory for deep rl. E Beeching, C Wolf, J Dibangoye, O Simonin, 2020</p>
<p>On a routing problem. R Bellman, Quarterly of applied mathematics. 1611958</p>
<p>Playing doom with slam-augmented deep reinforcement learning. S Bhatti, A Desmaison, O Miksik, N Nardelli, N Siddharth, P H S Torr, arxiv preprint 1612.003802016</p>
<p>Geometric deep learning: going beyond euclidean data. M M Bronstein, J Bruna, Y Lecun, A Szlam, P Vandergheynst, IEEE Signal Processing Magazine. 3442017</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, International Conference on Learning Representations. 2020</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, International Conference on Learning Representations. 2019</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, arXiv:1412.35552014arXiv preprint</p>
<p>Gated Feedback Recurrent Neural Networks. J Chung, C Gulcehre, K Cho, Y Bengio, ICML. 2015</p>
<p>Language modeling with gated convolutional networks. Y N Dauphin, A Fan, M Auli, D Grangier, JMLR. orgProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning201770</p>
<p>A note on two problems in connexion with graphs. E W Dijkstra, Numerische mathematik. 111959</p>
<p>Search on the replay buffer: Bridging planning and reinforcement learning. B Eysenbach, R R Salakhutdinov, S Levine, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>Protein interface prediction using graph convolutional networks. A Fout, J Byrd, B Shariat, A Ben-Hur, Advances in neural information processing systems. 2017</p>
<p>Neural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, JMLR. orgProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning201770</p>
<p>A Graves, G Wayne, I Danihelka, arXiv:1410.5401Neural turing machines. 2014arXiv preprint</p>
<p>Hybrid computing using a neural network with dynamic external memory. A Graves, G Wayne, M Reynolds, T Harley, I Danihelka, A Grabska-Barwińska, S G Colmenarejo, E Grefenstette, T Ramalho, J Agapiou, Nature. 53876264712016</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, 10.1109/CVPR.2017.7692017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). July 2017</p>
<p>Unifying map and landmark based representations for visual navigation. S Gupta, D Fouhey, S Levine, J Malik, arXiv:1712.081252017arXiv preprint</p>
<p>Long Short-Term Memory. S Hochreiter, J Schmidhuber, Neural Computation. 981997</p>
<p>Reinforcement learning with unsupervised auxiliary tasks. M Jaderberg, V Mnih, W M Czarnecki, T Schaul, J Z Leibo, D Silver, K Kavukcuoglu, 2017ICLR</p>
<p>An efficient graph convolutional network technique for the travelling salesman problem. C K Joshi, T Laurent, X Bresson, arXiv:1906.012272019arXiv preprint</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artificial intelligence. 1011-21998</p>
<p>P Karkus, D Hsu, W S Lee, Qmdp-net: Deep learning for planning under partial observability. 2017</p>
<p>ViZ-Doom: A Doom-based AI research platform for visual reinforcement learning. M Kempka, M Wydmuch, G Runc, J Toczek, W Jaskowski, 10.1109/CIG.2016.7860433IEEE Conference on Computatonal Intelligence and Games. 2017</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, International Conference on Learning Representations. 2017</p>
<p>Sarsop: Efficient point-based pomdp planning by approximating optimally reachable belief spaces. H Kurniawati, Proc. Robotics: Science and Systems. Robotics: Science and Systems2008. 2008</p>
<p>Planning algorithms. S M Lavalle, 2006Cambridge university press</p>
<p>Gradient-Based Learning Applied to Document Recognition. Y Lecun, L Eon Bottou, Y Bengio, P Haaner, Proceedings of the IEEE. 86111998</p>
<p>Combinatorial optimization with graph convolutional networks and guided tree search. Z Li, Q Chen, V Koltun, Advances in Neural Information Processing Systems. 2018</p>
<p>Habitat: A Platform for Embodied AI Research. Manolis Savva, * , Abhishek Kadian, * , Oleksandr Maksymets, * Zhao, Y Wijmans, E Jain, B Straub, J Liu, J Koltun, V Malik, J Parikh, D Batra, D , Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2019</p>
<p>Learning to Navigate in Cities Without a Map. P Mirowski, M K Grimes, M Malinowski, K M Hermann, K Anderson, D Teplyashin, K Simonyan, K Kavukcuoglu, A Zisserman, R Hadsell, arxiv pre-print 1804.00168v22018</p>
<p>Learning to Navigate in Complex Environments. P Mirowski, R Pascanu, F Viola, H Soyer, A J Ballard, A Banino, M Denil, R Goroshin, L Sifre, K Kavukcuoglu, D Kumaran, R Hadsell, 2017ICLR</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 51875402015</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, 10.1038/nature14236Nature. 5182015</p>
<p>Moddrop: adaptive multi-modal gesture recognition. N Neverova, C Wolf, G Taylor, F Nebout, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3882015</p>
<p>Neural map: Structured memory for deep reinforcement learning. E Parisotto, R Salakhutdinov, 2018ICLR</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Towards a general theory of topological maps. E Remolina, B Kuipers, Artif. Intell. 1522004</p>
<p>Semi-parametric topological memory for navigation. N Savinov, A Dosovitskiy, V Koltun, International Conference on Learning Representations. 2018</p>
<p>Modeling relational data with graph convolutional networks. M Schlichtkrull, T N Kipf, P Bloem, Van Den, R Berg, I Titov, M Welling, European Semantic Web Conference. Springer2018</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, 1707.063472017arxiv pre-print</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>A survey of point-based pomdp solvers. G Shani, J Pineau, R Kaplow, Autonomous Agents and Multi-Agent Systems. 2712013</p>
<p>Learning topological maps with weak local odometric information. H Shatkay, L P Kaelbling, IJCAI. 21997</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, T Lillicrap, K Simonyan, D Hassabis, Science. 36264192018</p>
<p>The optimal control of partially observable markov processes over a finite horizon. R D Smallwood, E J Sondik, Operations research. 2151973</p>
<p>Heuristic search value iteration for pomdps. T Smith, R Simmons, Proceedings of the 20th conference on Uncertainty in artificial intelligence. the 20th conference on Uncertainty in artificial intelligence2004</p>
<p>. A Srinivas, A Jabri, P Abbeel, S Levine, C Finn, 2018Universal planning networks</p>
<p>A Tamar, Y Wu, G Thomas, S Levine, P Abbeel, Value iteration networks. 2016</p>
<p>Learning metric-topological maps for indoor mobile robot navigation. S Thrun, Artificial Intelligence. 9911998</p>
<p>Human spatial representation: Insights from animals. R F Wang, E S Spelke, 10.1016/s1364-6613(02)01961-7Trends in Cognitive Sciences. 692002</p>
<p>Unsupervised predictive memory in a goal-directed agent. G Wayne, C C Hung, D Amos, M Mirza, A Ahuja, A Grabska-Barwinska, J W Rae, P W Mirowski, J Z Leibo, A Santoro, M Gemici, M Reynolds, T Harley, J Abramson, S Mohamed, D J Rezende, D Saxton, A Cain, C Hillier, D Silver, K Kavukcuoglu, M Botvinick, D Hassabis, T P Lillicrap, arxiv preprint 1803.107602018</p>
<p>Z Wu, S Pan, F Chen, G Long, C Zhang, P S Yu, arXiv:1901.00596A comprehensive survey on graph neural networks. 2019arXiv preprint</p>
<p>Gibson env: realworld perception for embodied agents. F Xia, R Zamir, A He, Z Y Sax, A Malik, J Savarese, S , Computer Vision and Pattern Recognition (CVPR). IEEE2018. 2018</p>
<p>What can neural networks reason about?. K Xu, J Li, M Zhang, S Du, K Kawarabayashi, S Jegelka, arxiv preprint 1905.132112019</p>
<p>K Xu, J Li, M Zhang, S S Du, K I Kawarabayashi, S Jegelka, arXiv:1905.13211What can neural networks reason about?. 2019arXiv preprint</p>
<p>J Zhang, L Tai, J Boedecker, W Burgard, M Liu, 1706.09520Neural SLAM. 2017arxiv preprint</p>
<p>J Zhou, G Cui, Z Zhang, C Yang, Z Liu, L Wang, C Li, M Sun, arXiv:1812.08434Graph neural networks: A review of methods and applications. 2018arXiv preprint</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, IEEE international conference on robotics and automation. 2017. 2017IEEEICRA</p>            </div>
        </div>

    </div>
</body>
</html>
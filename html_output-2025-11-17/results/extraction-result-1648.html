<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1648 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1648</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1648</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-198893797</p>
                <p><strong>Paper Title:</strong> TuneNet: One-Shot Residual Tuning for System Identiﬁcation and Sim-to-Real Robot Task Transfer</p>
                <p><strong>Paper Abstract:</strong> : As researchers teach robots to perform more and more complex tasks, the need for realistic simulation environments is growing. Existing techniques for closing the reality gap by approximating real-world physics often require extensive real world data and/or thousands of simulation samples. This paper presents TuneNet, a new machine learning-based method to directly tune the parameters of one model to match another using an iterative residual tuning technique. TuneNet estimates the parameter difference between two models using a single observation from the target and minimal simulation, allowing rapid, accurate and sample-efﬁcient parameter estimation. The system can be trained via supervised learning over an auto-generated simulated dataset. We show that TuneNet can perform sys-tem identiﬁcation even when the true parameter values lie well outside the distribution seen during training, and demonstrate that simulators tuned with TuneNet outperform existing techniques for predicting rigid body motion. Finally, we show that our method can estimate real-world parameter values, allowing a robot to perform sim-to-real task transfer on a dynamic manipulation task unseen during training. Code and videos are available online at http://bit.ly/2lf1bAw .</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1648.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1648.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TuneNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TuneNet: One-Shot Residual Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network-based residual tuning method that estimates parameter differences between a proposed simulator and a target (simulator or real world) from a single target observation and a small number of simulated rollouts, enabling rapid system identification and sim-to-real task transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Kinova Jaco 7-DOF robot with Robotiq 2-finger gripper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DOF Kinova Jaco manipulator equipped with a Robotiq 85 two-finger gripper, used for dynamic manipulation experiments (holding/dropping balls and performing bounce-shot trials) and for collecting torque and video observations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / dynamic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A physics-based rigid-body simulator (PyBullet) used to simulate robot kinematics/dynamics, rigid-body contacts, restitution (coefficient of restitution), object masses, and to render RGB frames for visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate rigid-body physics with simulator-rendered RGB (moderate fidelity physics; not fully capturing all real-world effects)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics (positions/velocities), contact dynamics modeled via simulator contact model (including coefficient of restitution), joint torques, robot kinematics, simulated RGB rendering for visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Unmodeled or simplified real-world effects such as exact material-dependent frictional variability, complex contact micro-physics, ball deformation/shape irregularities for some ball types, unmodeled sensor calibration and camera depth, air effects/aerodynamics, and other nuanced noise sources; simulator renderings and 2D tracker outputs are uncalibrated and lack depth information.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical lab setup with the Kinova Jaco robot and Robotiq gripper, an inclined plane and hoop mounted in robot workspace, a camera observing the scene (uncalibrated RGB), and various real balls (including racquetballs, gel balls, etc.) dropped by the robot to collect bounce videos and execute bounce-shot trials.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Dynamic bounce-shot skill: use simulator-tuned physics to select a drop height such that a ball bouncing off an inclined plane passes through a hoop (planning by sampling simulated shot heights and selecting the optimal one). Also used for general object-motion prediction (bouncing ball trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning of a residual estimator (TuneNet) in simulation on auto-generated pairs of simulated observations (one proposed model, one target model) with randomized physics parameters; after pretraining, one-shot target observation from the real world plus iterative simulated rollouts are used to tune simulator parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate for bounce-shot trials (percentage of shots that passed through hoop); additionally, parameter estimation MAE (e.g., COR MAE, mass MAE) and object motion prediction MAE/percent error were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Object motion prediction (GT simulated state): TuneNet (GT) MAE = 7 mm (trans err, cm = 0.7 cm) corresponding to trans err % = 1.08%; COR estimation MAE on GT Easy: as low as 0.005 (see Table 2) depending on number of rollouts/iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Bounce-shot success rate: overall 62% across all 10 trials × 6 ball types; when excluding trials where the ball bounced off the table/out of view during the pre-tuning observation ('off table' trials), success rate = 87%. Parameter estimation (real COR) examples: racquetball measured COR ≈ 0.789 ± 0.024 (paper reports tuned CORs used for planning).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>During TuneNet training the authors generated many simulated pairs with randomized physics parameters (e.g., mass ∼ U(0,1) kg for mass ID task; COR ∼ U(0.3,0.7) for training bouncing-ball GT Easy; camera polar coordinates randomized for Obs case), and for planning they sampled shot heights uniformly in the robot's reachable space.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Uncalibrated and noisy 2D visual observations lacking depth, camera viewpoint variability, balls rolling/bouncing off table (observation truncation), unmodeled material/friction variability, shape irregularities of some balls, simplifications in contact modeling and restitution, and parameter identifiability / coupling between parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Pretraining TuneNet entirely in simulation on many randomized parameter pairs (so residuals generalize), ability to estimate small parameter deltas via iterative residual tuning, use of a single real-world observation to tune simulator parameters (one-shot), use of tuned simulator for search-based planning (sampling candidate shot heights in tuned simulator), and clamping tuned parameters to physically meaningful ranges (e.g., COR in [0,1]).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The paper emphasizes that accurate modeling of key physical parameters (e.g., mass, coefficient of restitution, contact dynamics) is sufficient to close 'much of' the reality gap for the studied tasks; no strict numeric fidelity threshold is given, but contact/restitution accuracy and correct parameter ranges are identified as critical. The paper also notes parameter identifiability issues and coupled parameters as limiting factors.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>They compare GT (ground-truth simulated state) vs Obs (visual 2D tracker) inputs and GT Easy vs GT Hard parameter ranges: TuneNet (GT) achieves object motion prediction MAE = 7 mm (1.08%), while TuneNet (Obs, visual inputs) yields worse motion prediction (~6.0% trans err, trans err cm = 3.7 cm). For parameter estimation, TuneNet reaches best accuracy within a few simulated rollouts/iterations; performance degrades when target parameters lie outside training ranges but remains better than baselines (e.g., mass MAE on test set outside training range = 0.198 kg).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TuneNet can tune an off-the-shelf simulator to match target environments (including the real world) from a single observation plus a small number of simulated rollouts, enabling effective sim-to-real transfer for dynamic tasks; iterative residual tuning is sample-efficient compared to gradient-free optimizers (CMA-ES, entropy search) and can generalize to parameter values outside the training range; primary failure modes arise from poor or truncated real-world observations (e.g., ball leaving view), unmodeled effects, and parameter identifiability/coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TuneNet: One-Shot Residual Tuning for System Identiﬁcation and Sim-to-Real Robot Task Transfer', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-Sim: Robotic Grasping via Randomized-to-Canonical Adaptation Networks <em>(Rating: 2)</em></li>
                <li>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience <em>(Rating: 2)</em></li>
                <li>Augmenting Physical Simulators with Stochastic Neural Networks: Case Study of Planar Pushing and Bouncing <em>(Rating: 2)</em></li>
                <li>BayesSim: Adaptive Domain Randomization Via Probabilistic Inference for Robotics Simulators <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1648",
    "paper_id": "paper-198893797",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "TuneNet",
            "name_full": "TuneNet: One-Shot Residual Tuning",
            "brief_description": "A neural-network-based residual tuning method that estimates parameter differences between a proposed simulator and a target (simulator or real world) from a single target observation and a small number of simulated rollouts, enabling rapid system identification and sim-to-real task transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Kinova Jaco 7-DOF robot with Robotiq 2-finger gripper",
            "agent_system_description": "A 7-DOF Kinova Jaco manipulator equipped with a Robotiq 85 two-finger gripper, used for dynamic manipulation experiments (holding/dropping balls and performing bounce-shot trials) and for collecting torque and video observations.",
            "domain": "general robotics manipulation / dynamic manipulation",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "A physics-based rigid-body simulator (PyBullet) used to simulate robot kinematics/dynamics, rigid-body contacts, restitution (coefficient of restitution), object masses, and to render RGB frames for visual observations.",
            "simulation_fidelity_level": "approximate rigid-body physics with simulator-rendered RGB (moderate fidelity physics; not fully capturing all real-world effects)",
            "fidelity_aspects_modeled": "Rigid-body dynamics (positions/velocities), contact dynamics modeled via simulator contact model (including coefficient of restitution), joint torques, robot kinematics, simulated RGB rendering for visual observations.",
            "fidelity_aspects_simplified": "Unmodeled or simplified real-world effects such as exact material-dependent frictional variability, complex contact micro-physics, ball deformation/shape irregularities for some ball types, unmodeled sensor calibration and camera depth, air effects/aerodynamics, and other nuanced noise sources; simulator renderings and 2D tracker outputs are uncalibrated and lack depth information.",
            "real_environment_description": "Physical lab setup with the Kinova Jaco robot and Robotiq gripper, an inclined plane and hoop mounted in robot workspace, a camera observing the scene (uncalibrated RGB), and various real balls (including racquetballs, gel balls, etc.) dropped by the robot to collect bounce videos and execute bounce-shot trials.",
            "task_or_skill_transferred": "Dynamic bounce-shot skill: use simulator-tuned physics to select a drop height such that a ball bouncing off an inclined plane passes through a hoop (planning by sampling simulated shot heights and selecting the optimal one). Also used for general object-motion prediction (bouncing ball trajectories).",
            "training_method": "Supervised learning of a residual estimator (TuneNet) in simulation on auto-generated pairs of simulated observations (one proposed model, one target model) with randomized physics parameters; after pretraining, one-shot target observation from the real world plus iterative simulated rollouts are used to tune simulator parameters.",
            "transfer_success_metric": "Task success rate for bounce-shot trials (percentage of shots that passed through hoop); additionally, parameter estimation MAE (e.g., COR MAE, mass MAE) and object motion prediction MAE/percent error were reported.",
            "transfer_performance_sim": "Object motion prediction (GT simulated state): TuneNet (GT) MAE = 7 mm (trans err, cm = 0.7 cm) corresponding to trans err % = 1.08%; COR estimation MAE on GT Easy: as low as 0.005 (see Table 2) depending on number of rollouts/iterations.",
            "transfer_performance_real": "Bounce-shot success rate: overall 62% across all 10 trials × 6 ball types; when excluding trials where the ball bounced off the table/out of view during the pre-tuning observation ('off table' trials), success rate = 87%. Parameter estimation (real COR) examples: racquetball measured COR ≈ 0.789 ± 0.024 (paper reports tuned CORs used for planning).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "During TuneNet training the authors generated many simulated pairs with randomized physics parameters (e.g., mass ∼ U(0,1) kg for mass ID task; COR ∼ U(0.3,0.7) for training bouncing-ball GT Easy; camera polar coordinates randomized for Obs case), and for planning they sampled shot heights uniformly in the robot's reachable space.",
            "sim_to_real_gap_factors": "Uncalibrated and noisy 2D visual observations lacking depth, camera viewpoint variability, balls rolling/bouncing off table (observation truncation), unmodeled material/friction variability, shape irregularities of some balls, simplifications in contact modeling and restitution, and parameter identifiability / coupling between parameters.",
            "transfer_enabling_conditions": "Pretraining TuneNet entirely in simulation on many randomized parameter pairs (so residuals generalize), ability to estimate small parameter deltas via iterative residual tuning, use of a single real-world observation to tune simulator parameters (one-shot), use of tuned simulator for search-based planning (sampling candidate shot heights in tuned simulator), and clamping tuned parameters to physically meaningful ranges (e.g., COR in [0,1]).",
            "fidelity_requirements_identified": "The paper emphasizes that accurate modeling of key physical parameters (e.g., mass, coefficient of restitution, contact dynamics) is sufficient to close 'much of' the reality gap for the studied tasks; no strict numeric fidelity threshold is given, but contact/restitution accuracy and correct parameter ranges are identified as critical. The paper also notes parameter identifiability issues and coupled parameters as limiting factors.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "They compare GT (ground-truth simulated state) vs Obs (visual 2D tracker) inputs and GT Easy vs GT Hard parameter ranges: TuneNet (GT) achieves object motion prediction MAE = 7 mm (1.08%), while TuneNet (Obs, visual inputs) yields worse motion prediction (~6.0% trans err, trans err cm = 3.7 cm). For parameter estimation, TuneNet reaches best accuracy within a few simulated rollouts/iterations; performance degrades when target parameters lie outside training ranges but remains better than baselines (e.g., mass MAE on test set outside training range = 0.198 kg).",
            "key_findings": "TuneNet can tune an off-the-shelf simulator to match target environments (including the real world) from a single observation plus a small number of simulated rollouts, enabling effective sim-to-real transfer for dynamic tasks; iterative residual tuning is sample-efficient compared to gradient-free optimizers (CMA-ES, entropy search) and can generalize to parameter values outside the training range; primary failure modes arise from poor or truncated real-world observations (e.g., ball leaving view), unmodeled effects, and parameter identifiability/coupling.",
            "uuid": "e1648.0",
            "source_info": {
                "paper_title": "TuneNet: One-Shot Residual Tuning for System Identiﬁcation and Sim-to-Real Robot Task Transfer",
                "publication_date_yy_mm": "2019-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-Sim: Robotic Grasping via Randomized-to-Canonical Adaptation Networks",
            "rating": 2,
            "sanitized_title": "simtosim_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        },
        {
            "paper_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Augmenting Physical Simulators with Stochastic Neural Networks: Case Study of Planar Pushing and Bouncing",
            "rating": 2,
            "sanitized_title": "augmenting_physical_simulators_with_stochastic_neural_networks_case_study_of_planar_pushing_and_bouncing"
        },
        {
            "paper_title": "BayesSim: Adaptive Domain Randomization Via Probabilistic Inference for Robotics Simulators",
            "rating": 2,
            "sanitized_title": "bayessim_adaptive_domain_randomization_via_probabilistic_inference_for_robotics_simulators"
        }
    ],
    "cost": 0.00900275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TuneNet: One-Shot Residual Tuning for System Identification and Sim-to-Real Robot Task Transfer</p>
<p>Adam Allevato allevato@utexas.edu 
Department of Mechanical Engineering</p>
<p>Elaine Schaertl Short eshort@ece.utexas.edu 
Department of Electrical and Computer Engineering
The University of Texas at Austin</p>
<p>Mitch Pryor mpryor@utexas.edu 
Department of Mechanical Engineering</p>
<p>Andrea Thomaz athomaz@ece.utexas.edu 
Department of Electrical and Computer Engineering
The University of Texas at Austin</p>
<p>TuneNet: One-Shot Residual Tuning for System Identification and Sim-to-Real Robot Task Transfer
DD2E9D5E9C9D9F269A84A5D312F8EE39
As researchers teach robots to perform more and more complex tasks, the need for realistic simulation environments is growing.Existing techniques for closing the reality gap by approximating real-world physics often require extensive real world data and/or thousands of simulation samples.This paper presents TuneNet, a new machine learning-based method to directly tune the parameters of one model to match another using an iterative residual tuning technique.TuneNet estimates the parameter difference between two models using a single observation from the target and minimal simulation, allowing rapid, accurate and sampleefficient parameter estimation.The system can be trained via supervised learning over an auto-generated simulated dataset.We show that TuneNet can perform system identification even when the true parameter values lie well outside the distribution seen during training, and demonstrate that simulators tuned with TuneNet outperform existing techniques for predicting rigid body motion.Finally, we show that our method can estimate real-world parameter values, allowing a robot to perform sim-to-real task transfer on a dynamic manipulation task unseen during training.Code and videos are available online at http://bit.ly/2lf1bAw.</p>
<p>Introduction</p>
<p>Recent research has validated simulators for real-world robot behaviors and control policies.Several studies have shown the ability to adapt simulation-learned policies to the real world based on observations [1,2,3,4,5,6,7], but these techniques require extensive real-world data collection.Even before collecting data in the real world, however, we can improve simulations by simply selecting more realistic simulator parameter values.While some approaches train over a large set of possible parameter values to create a robust policy [8,9,10,11,2], we are interested in the problem of creating a single "canonical" simulation that behaves as realistically as possible, which can then be used for robust physics prediction or better task planning.</p>
<p>In this paper, we propose TuneNet, a residual tuning technique that uses a neural network to modify the parameters of one physical model so it approximates another.TuneNet takes as input observations from two different models (i.e. a simulator and the real world), and estimates the difference in parameters between the models.By estimating the parameter gradient landscape, a small number of iterative tuning updates enable rapid convergence on improved parameters from a single observation from the target model.TuneNet is trained using supervised learning on a dataset of pairs of auto-generated simulated observations, which allows training to proceed without real-world data collection or labeling.Our primary contribution is the concept of residual tuning, which allows fast and accurate one-shot system identification, and the development and analysis of the TuneNet neural network model for applying iterative parameter updates.We show that TuneNet is fast and effective: it tunes more efficiently than other gradient-free optimization methods, and that it outperforms state-of-the-art techniques for predicting rigid body motion.Finally, we validate that TuneNet can tune simulations to match real-world environments, and use those simulations to conduct sim-to-real task learning by teaching a robot to complete a task that requires accurate dynamics prediction.</p>
<p>3rd Conference on Robot Learning (CoRL 2019), Osaka, Japan.</p>
<p>Related Work</p>
<p>This paper focuses on tuning simulation parameters as a way to help rapidly close the reality gap.This places it at the intersection of system identification and sim-to-real learning.</p>
<p>Classical system identification [12,13] used maximum likelihood methods to estimate the parameters of a fixed model given observed behavior.For estimating parameters of complex nondifferentiable models (such as physics engines), robotics researchers have used a variety of gradientfree [2,14,15,16] or gradient-approximating [17] optimization techniques.These techniques require sampling the model (e.g.performing simulation rollouts) many times with different parameter values, which is a computationally expensive process.TuneNet seeks to minimize the number of simulation rollouts during system identification.Research has shown the ability to perform robot system identification from a few observations using a neural network [18], but cannot take advantage of previous parameter estimations, and the results have not been validated in a sim-to-real setting.</p>
<p>In contrast to system identification, recent work ( [10,2]) has explored data-driven methods for simulation improvement by leveraging a combination of real-world data and domain randomization [8].These adaptive approaches require huge numbers of randomized simulations to be repeated once the robot is in the target environment.</p>
<p>TuneNet is also similar to techniques that learn a transformation from a simulated environment to a target environment.These techniques leverage physics simulators as priors, while also accounting for simulator inaccuracies.This may involve a neural network-based transformation over states [4] or actions [3], learning a new real-world policy based on one learned in simulation [7], or using a physics model and a data-driven residual to improve physics prediction [19,20,5] or policy learning [1,5,6].All of these approaches require a substantial amount of real-world training examples from the target environment to learn a transformation or develop a policy.TuneNet learns to account for errors in parameter space rather than action or state space, and is pretrained in simulation, allowing a model to be adapted using a single observation from the target environment.</p>
<p>A recent related work is James et al. [21], which trained a network to convert from randomized simulation images to non-randomized simulations, and showed that this network could also convert real-world pictures to simulations.Zhang et al. [22] took a similar approach, using adversarial domain adaptation [23] and unlabeled data from the target environment to convert real images to simulated ones.Both of these works operate in the visual domain and focus on learning the arrangement of objects in an environment, whereas our approach focuses on learning better simulator physical parameters by observing the world state over time.Finally, Xu et al. [24] discovered physics parameters through robot interaction, but the possible parameter values are fixed and the approach does not take advantage of information that could be gleaned from an existing simulation.</p>
<p>One-Shot Residual Tuning</p>
<p>In this section, we describe our approach for tuning a simulator to match real-world observations.We present the underlying theory first, followed by our algorithm and implementation.</p>
<p>Tuning a Parameterized Model</p>
<p>Our goal is to generate a proposed model, f ζ P (s t , a t ) = s t+1 , so that it approximates another (fixed) target model, g ζ T (s t , a t ) = s t+1 , where ζ P and ζ T are physics parameters of those models.One way to develop this approximation would be to minimize the prediction error (some distance function d(•, •)) over a dataset of N examples (Eq.( 1)), where L is the length of each example.</p>
<p>arg min
ζ P N n=1 L t=1 d(f ζ P (s nt , a nt ), g ζ T (s nt , a nt ))(1)
Unfortunately, we usually do not have access to the true system dynamics g ζ T .In addition, if the proposed model is an off-the-shelf simulator, it is not differentiable, and so we cannot easily calculate a gradient for Eq. ( 1).While we can directly estimate the parameters using a neural network (as in [18]), the approach would not generalize well outside the training set, and would be unable to iteratively improve a parameter estimate without additional data since it would encode a single mapping from states to parameter values.</p>
<p>In this work, we instead assume that there exists some set of parameters ζ T that will cause the proposed model to closely match the target model,
f ζ T ≈ g ζ T ,
and seek to approximate it.This assumption takes advantage of the fact that f ζ encodes at least a reasonable approximation of of g ζ 's dynamics, and allows us to use the proposed model as a prior during optimization.</p>
<p>TuneNet</p>
<p>Our model, TuneNet, estimates the parameter residual ∆ζ = ζ T − ζ P ≈ ζ T − ζ P between a single proposed model and a single target model, and then iteratively updates the proposed model parameters accordingly to match those of the target model.By using a network to estimate parameter deltas, we transform Eq. ( 1) into a differentiable loss that can easily be used as a supervised learning objective.We can still calculate the quality of the world state estimation, but it does not appear in the loss.</p>
<p>We may be unable to directly observe the state of each model, and represent this with observation functions z P and z T applied to proposed and target models, respectively.Observations of length L are generated from each model using the current proposed parameter ζ P and the true target parameter ζ T (Eq.( 2) and Eq. ( 3)).
o P = z P (f ζ P (s t , a t )), t = 1 . . . L(2)o T = z T (g ζ T (s t , a t )), t = 1 . . . L(3)
TuneNet estimates the parameter error directly based on the two observations and approximates a function h, which is parameterized by some θ (in this work, the weights of a neural network):
h θ (o P , o T ) = ∆ζ.
The new optimization problem for training (over a dataset of length n) is given in Eq. ( 4), where λ is a weight regularization constant.
arg min θ N n=1 (ζ Pn + h θ (o Pn , o Tn )) − ζ Tn + λ θ
Algorithm 1 TuneNet's parameter tuning procedure.</p>
<p>procedure
TUNE(o T , simulator f ζ , initial guess ζ P0 , Network h θ , K) for k = 1 . . . K do o P ← z P (f ζ P k−1 (s t , a t )), t = 1 . . . L Eq. (2) o T ← RESAMPLE(o T , L) ζ P k ← ζ P k−1 + h θ (o P , o T )
Eq. ( 5)
return ζ P K
We train TuneNet using a dataset where each datapoint consists of a pair of models with a known difference in physics parameters (see Fig. 1).The datasets are auto-generated purely in simulation, which removes the need for hand-labeling or real-world data and makes training faster, safer, and easier.Also, because pairs are generated from the same dynamics model (differing only by ζ), we know that during training, ζ T = ζ T and the best-case reality gap during training is 0.</p>
<p>After training TuneNet, we apply it to a set of observations from a new target model and observations from an initial proposed model having parameters ζ P (Fig. 1).As discussed below in Section 3.3, TuneNet excels at estimating smaller differences in ζ, so we tune iteratively in K steps to achieve the final ζ P = ζ P k=K given a single target observation.The process can be described and implemented using a recursive approach, or an iterative one as listed in Algorithm 1.The RESAMPLE function interpolates the values in o T , if necessary, so that the length and sampling rate match those of o P .The two observations are then passed to TuneNet to estimate the parameter difference between them.The algorithm adds the resulting estimate to the previous best guess for ζ P , bringing it into closer agreement with ζ T :
ζ P k = ζ P k−1 + h θ (o P k−1 , o T ), k = 1 . . . K(5)
Importantly, we are able to do this without collecting any additional observations from the target model, and only sampling the proposed model once per iteration (Eq.( 5)).After tuning, the tuned simulator f ζ P is ready to be used for other tasks such as object motion prediction and robot task planning.</p>
<p>Residual Tuning</p>
<p>By learning to estimate ∆ζ instead of ζ, an estimator naturally becomes better at estimating small deltas due to the nature of the difference transformation.Given uniformly distributed training data over some interval [d, u], the probability densities of parameters ζ P and ζ T are equal to some constant c = 1 u−d over the interval.However, the difference, ∆ζ, will be distributed according to
P (∆ζ = x) = − 2 (u−d) 2 x + 2 u−d ,
making small delta values more common.As the parameter estimate becomes closer and closer to the true value, the network is able to leverage more and more training data.This behavior allows us to conduct iterative residual tuning by collecting observations from the proposed model using successively better ζ P estimates.Estimating parameter deltas also allows TuneNet to tune to values that lie outside the ranges seen in training data, as opposed to other neural network approaches that often require a transformation or fine-tuning for a new output range.</p>
<p>Residual tuning can be seen as gradient descent in parameter space, using a neural network to approximate the gradient.While gradient-based optimization techniques such as momentum could potentially be combined with this approach, in this paper we show that TuneNet performs well using standard gradient descent.</p>
<p>Experiments</p>
<p>We performed three experiments to test that TuneNet can efficiently tune an off-the-shelf simulator to match a target environment, including ones it has not encountered before (Exp. 1 and 2), that it can generate tuned simulations that can be used for object motion prediction (Exp.2), that it can be used to estimate real-world parameters, even when trained purely in simulation (Exp.3), and that we can use a simulator tuned with TuneNet as a platform for learning tasks different than the one used for data collection (Exp.3).We implement all networks using PyTorch [25].Network architecture details, layer sizes, and training hyperparmaeters are provided in the appendix.</p>
<p>Experiment 1: System Identification in Simulation</p>
<p>We first performed a simulated experiment to validate TuneNet's ability to tune one model to match another.The tuned parameter was the mass of an object held in a robot's gripper-a classic system identification objective.To generate training data for the experiment, we use pairs of PyBullet1 simulations.Each simulation consists of a Kinova Jaco 7-DOF robot arm with a Robotiq 85 2finger gripper at the end effector, moving in a circle as it holds an object (see appendix for task details).During this motion, we record the 7 raw joint torques over time, and provide this as the input observation signal.To make this task more difficult, in the test set, the target simulator has its object mass increased by an additional 1 kg, putting the all correct mass values completely outside the range of values seen during training.This experiment therefore evaluates TuneNet's ability to complete more general tuning tasks, and ones that may take multiple iterations to complete.</p>
<p>After training, we applied TuneNet with K = 9 iterations over the validation and test sets to measure the estimation accuracy (K was chosen empirically based on observed tuning behavior).We report the mean absolute error (MAE) of the estimated object mass over both the validation and test sets.We compare TuneNet's performance to several baselines: a prediction of 1 kg for every data point (which would be the best our network could do if it was unable to tune values outside those seen in the training set) and a constant prediction of the mean parameter value in the test set.</p>
<p>On this task, TuneNet achieved an MAE of ±0.011 kg over the validation set, where both target and proposed masses were were in the training range (see Table 1).When evaluating on the test set, where the target masses were all outside the range used for training, TuneNet still achieved an MAE of ±0.198 kg-higher than on the validation set, but still far better than either baseline of always predicting 1 kg (MAE=0.504kg) or predicting the average test-set value (MAE=0.255kg).</p>
<p>Experiment 2: Bouncing Ball in Simulation</p>
<p>To test TuneNet's ability to predict rigid body motion, we used it to tune the coefficient of restitution (COR) of a simulated bouncing ball.Our proposed and target dynamics models were again PyBullet simulations.For training, we recreated the simulated dataset used in Ajay et al. [19]: in each episode, a 0.5m-radius sphere is dropped onto a flat plane from a starting height in the range 4-5m, having COR ∼ U(0.3, 0.7).At each timestep, we recorded the xyz position of the ball as the world state.We used the same sampling and dataset parameters as [19]-800 simulations for training and 100 for testing, with a physics update rate of 60Hz and episode length of 400 frames.Unlike in [19], each example in our dataset consists of a pair of simulations.In this experiment, the observation functions z P and z T were identity, i.e., we had direct access to the world state ground truth.We refer to this case as GT (ground truth) in the rest of the paper.</p>
<p>We test over two datasets, GT Easy and GT Hard.GT Easy consists of simulations with proposed and target COR values in the range [0.3, 0.7], the same as in the training set (this is for a more fair comparison with [19]).In GT Hard, the target COR values are extended to the range [0, 1.0].</p>
<p>After training, we evaluated TuneNet by applying K = 5 tuning iterations to tune a unique model for each run in the test set.For each run, we report the mean absolute error (MAE) between the final proposed model's COR and the target model's COR.We compare against four baselines: predicting the mean of the test set target parameters (Mean), our implementation of greedy entropy search [14] (EntSearch), the CMA-ES optimization method [26], and a neural network that learns to directly predict parameters in one step (direct prediction).Details on baseline implementations are in the   To test our tuned model's overall accuracy in object motion prediction, we record the ball's position from a rollout of the final proposed model, compare it to the target model, and report the MAE and percent error (trans err, cm, and trans err %) over all runs and timesteps.</p>
<p>Fig. 2 compares the average tuning performance (measured by the MAE between the true and estimated parameters) as more and more simulation rollouts are used for estimation.Direct Prediction and Mean require 0 rollouts, since they are computed directly from the target observation.CMA-ES uses 10 rollouts per iteration, and the other algorithms use 1 rollout per iteration.Table 2 quantifies the values in the table for various numbers of simulation rollouts, averaged across the entire training set.TuneNet's one-step prediction accuracy is slightly lower than direct prediction, but it achieves the best accuracy of all methods in less than 5 iterations.In contrast, EntSearch has difficulty converging to a good value, and CMA-ES requires far more samples to develop a good estimate.</p>
<p>Table 3 shows the object motion prediction error for the bouncing ball dataset.We compare to Ajay et al. [19]'s VRNN hybrid network, although we note that those results are from one-step prediction models, whereas our model uses the entire state history to tune and is reported averaged over the entire dataset.TuneNet GT outperforms all other approaches for position prediction, with an MAE of 7 mm, or 1.08%.</p>
<p>Experiment 3: Tuning for Sim-to-Real Task Learning</p>
<p>Finally, we tested TuneNet for sim-to-real learning by estimating the COR of a bouncing ball again, but this time in the real world.Our task is to use a tuned simulator to bounce balls off an inclined plane into a hoop (a "bounce shot"), as seen in Fig. 3a.This is a difficult task, as striking the rim of the hoop will often cause the ball to bounce away rather than passing through the hoop.</p>
<p>Our hardware platform for evaluation consists of a Kinova Jaco 7DOF robot arm with a Robotiq 85 2-finger gripper (see Fig. 3a), controlled using ROS2 .A camera is positioned opposite the robot for perception purposes.We trained TuneNet on a visual version of the GT Easy dataset used in Experiment 2. For the target simulator in each episode, we rendered a 2D video using PyBullet's renderer, and applied an RGBonly OpenCV object tracker to calculate the position of the ball in each frame.The tuning problem remains challenging after using this object detector, since in addition to the presence of visual noise, 2D pixel coordinates do not directly correspond to z-height in 3 dimensions and our camera images are uncalibrated.The network input for each tuning task is the ground truth 3D position for the proposed model and the 2D (pixel) object position for the target model.This tests TuneNet's ability to learn transformations for different input channels.We refer to this network as TuneNet Obs.</p>
<p>We first tested TuneNet Obs's motion prediction error over the simulated dataset from Experiment 2 (Table 3).It still achieves position accuracy within 6%, and while it performs more poorly than the GT case, this is expected, the network is operating on noisy, uncalibrated observations and the other approaches in the table have access to the true simulator state.</p>
<p>For the bounce shot task, we performed 10 independent tuning trials for 6 different ball types.In each trial, the robot first dropped the ball on a flat table and recorded a video of the resulting bounces.</p>
<p>The video was resampled to 60 Hz, clipped to 400 frames, and processed using the same OpenCV object tracker as on the training set.TuneNet operated on the observation from one real-world drop and the ground truth state from an un-tuned simulator which started with a COR of 0 (no bounce).We used TuneNet (K = 5) to tune the simulated COR, clamping to the range [0, 1], as values outside this range are physically meaningless.</p>
<p>Fig. 3c shows an example of TuneNet using K = 5 iterative tuning rounds to match observations from a video of a dropped racquetball, beginning with a COR of 0. The first parameter updates are large, and successive iterations fine-tune the estimate because of the more accurate tuning behavior for small parameter deltas (see Section 3.3).</p>
<p>The tuned COR value was used to construct a new simulator, consisting of a dropped ball, inclined plane, and hoop that match our real-world task setup.The simulator was used to uniformly sample 100 shot heights from a range within the robot's reachable space, and the optimal height was chosen as the shot height that passed the ball as close to the center of the hoop as possible.The real robot then executes the shot from the optimal height.We repeat the task for all 10 independent trials and all 6 ball types (not all of which are round, see Fig. 3) and record the success or failure of the bounce shot.</p>
<p>Fig. 3 shows the results of the bounce shot task and a representative execution.Overall, the robot succeeded at 62% of the bounce shots.The primary reason for failure was that during the pre-tuning data collection step, the ball bounced or rolled off the table or out of view of the camera.This resulted in a meaningless input observation, which caused in a low estimated COR and incorrect drop height.Indeed, when these "off table" trials are removed, the success rate increases to 87%, whereas the "off table" trials were only 18% successful, as shown in Fig. 3b.Eliminating these examples where the ball bounced off the table in the observation drop, two of the six ball types were successful in every trial.Successful shots with all 6 balls can be found in the supplementary video.</p>
<p>Discussion and Conclusion</p>
<p>TuneNet is fast.Since each tuning iteration consists of a single network forward pass and a single simulation rollout, TuneNet reduces the number of parameter-space simulation samples required compared to existing gradient-free optimization, and it tunes significantly faster than techniques that alternate between domain randomization in simulation and real-world data collection [14,2].Our dataset generation and training procedure is also fast due to being trained entirely in simulation, and completes in under 7 minutes using a single NVIDIA GTX 1080Ti.</p>
<p>TuneNet is effective.Our results show that even though a simulation cannot perfectly model the real world, much of the reality gap can be closed by choosing good simulation parameters.We note that TuneNet is compatible with recent approaches that learn data-driven transformations or residuals to improve simulations [20,27,4,5], and envision a two-step approach where TuneNet first matches a real-world environment as closely as possible using parameter tuning alone, then additional unmodeled effects are accounted for using learned transformations or domain randomization.</p>
<p>That said, there are some limitations of our approach that suggest avenues for future work.For example, parameters may not be fully identifiable based on observations alone, as explored in several studies [28,29,30].As discussed in [30] and [12], the identifiable set of parameters may be coupled, which would result in multiple local minima in parameter space.This may prove to be an issue if the goal is to accurately measure system parameters, but as long as the tuned parameters enable accurate prediction and sim-to-real learning using the proposed model, which local minimum the algorithm converges to is less important.For example, in our study, the racquetball had high task success rates despite a technically inaccurate COR estimate (0.789 ± 0.024 vs. regulation values of 0.68-0.72 [31]).[32] is also concurrently studying the problem of modeling belief over coupled parameter values.</p>
<p>In this paper, we introduced TuneNet, a neural network-based technique to perform residual tuning between two models, and validated the approach for use in system identification and sim-to-real robotics tasks.TuneNet represents a new method for closing the reality gap, showing that a simulator can be matched to the real world with just one observation and minimal simulation sampling.We plan to use TuneNet to rapidly generate tuned simulators for new environments, developing strong models for object manipulation without extensive real-world practice.</p>
<p>Figure 1 :
1
Figure 1: TuneNet uses observations to narrow the reality gap by tuning the physical parameters of a simulator so that it more closely approximates the real world (left), allowing sim-to-real robot skill transfer (right).</p>
<p>Figure 2 :
2
Figure 2: Mean parameter error after N iterations over the simulated ball-bouncing test set for various estimation techniques.TuneNet achieves the lowest error using just a few simulated iterations.Left: GT Easy.Right: GT Hard.</p>
<p>Figure 3 :
3
Figure 3: Experiment 3 sim-to-real task results.(a): The hardware setup used to collect data and perform tasks.(b): The ball types tested and the success rates for each type.(c) Left: observations from uncalibrated RGB video of robot dropping the red gel ball (measured COR: 0.791).Right: bounce trajectories from a simulator using 5 TuneNet iterations to tune the ball's coefficient of restitution (COR), starting from an initial guess of COR=0 (no bouncing).</p>
<p>Table 1 :
1
Percent and absolute error values on the end effector mass system identification task.
MethodVal Set (Absolute) Test Set (Absolute) Test Set (Mean %)Max of training range (1kg)-0.50428.0%Mean of test range (1.504kg) 0.250.25517.1%TuneNet0.0110.19813.2%</p>
<p>Table 2 :
2
Mean absolute error in COR prediction for the simulated ball-bouncing test set at various numbers of simulation rollouts.Best results in bold.
GT EasyGT HardMethodK = 1 510100K = 1 510100Mean0.0968 0.0968 0.0968 0.0968 0.2353 0.2353 0.2353 0.2353EntSearch0.1433 0.0622 0.0553 0.0533 0.2876 0.1021 0.1153 0.0939CMA-ES0.0540 0.0540 0.0540 0.0097 0.2766 0.2766 0.2766 0.0605Direct Prediction 0.0120 0.0120 0.0120 0.0120 0.0493 0.0493 0.0493 0.0493TuneNet0.0145 0.0050 0.0053 0.0053 0.1103 0.0212 0.0145 0.0175appendix.</p>
<p>Table 3 :
3
Mean absolute error in object motion prediction for the GT Easy dataset (Exp.2).
Methodtrans err, % trans err, cmZero (inelastic)10097.0Un-Tuned Physics20.5713.8Neural [19]9.165.8VRNN Hybrid [19]2.421.6TuneNet (Obs)6.035.7TuneNet (GT)1.080.7RobotCameratowerHoopData CollectionAreaInclinedPlane
(4) Our network consists of one independent fully connected layer for each observation, the outputs of which are concatenated and passed to additional connected layers which estimate the parameter error. We found that concatenating the two intermediate outputs performed better that other combination methods, such as calculating the difference. In this work, we use a physics-based simulator for the proposed dynamics model f ζ , and the target model g ζ is either another simulator or the real world.
https://pybullet.org
https://ros.org
https://github.com/CMA-ES/pycma
http://github.com/SheffieldML/GPy
AcknowledgmentsWe thank Josiah Hanna and Scott Niekum for their helpful suggestions.This work was supported by the US NSF (IIS-1564080, IIS-1724157) and US ONR (N000141612835, N000141612785).Appendix TuneNet Architecture Implementation DetailsWe implement all networks using PyTorch[25], and train them using stochastic gradient descent.Our TuneNet architecture for all experiments consists of fully-connected (FC) layers with ReLU activation functions for each of the feature extractors, with each feature extractor output size set to 32.The estimator therefore has an input size of 64.We model the estimator with two fully connected layers with a hidden size of 32, ReLU activation for the first layer, and a tanh activation for the final output.Details for Experiment 1: End-effector Mass IdentificationThe object's mass is randomly chosen, m ∼ U (0 kg, 1 kg), for each training point.In each simulated episode, which runs for 5 seconds, the robot moves its end effector in a circle defined by the coordinates [−0.4,0.2cos(τ ), 0.2sin(τ )], τ = 1.2t.We collect 1000, 300, and 300 episodes for the training, validation, and test sets, respectively.We trained the model for 1000 epochs using a batch size of 50, a learning rate of 0.01, L2 regularization λ = 1e−3, and 1% learning rate decay every 50 epochs.During training, the input torques were normalized to the range [0, 1] but the outputs were not transformed.Details for Experiment 2: Bouncing Ball COR TuningFor both datasets, we trained TuneNet for 200 epochs, using a batch size of 50 and a learning rate of 1e−2, with learning rate decay of 1% per 5 epochs.The L2 normalization constant λ is set to 0.01.During training, the input and output channels were normalized to the range [0, 1].For the Obs case, the visual OpenCV tracker uses hue, saturation, and value windows to threshold a camera image and isolate an object.It then calculates the y pixel position of the ball in each frame and normalizes the image pixel positions to the range [0, 1].For each randomized simulation render, the PyBullet camera was placed at random polar coordinates r ∼ U(5 m, 10 m), θ ∼ U(0, 2π), z ∼ U(5 m, 8 m), and aimed at the point (0, 0, 2 m).Baseline Implementation DetailsFor CMA-ES, we use the open-source package pycma 3 .The inputs to CMA-ES are identical to those provided to TuneNet, with the initial proposed physics parameters being used as the initial guess for the CMA algorithm.The CMA tolerance was set to 0.1.For Greedy Entropy Search (EntSearch in this paper), we re-implemented the algorithm provided in[14].The underlying Gaussian Process (GP) sampling was implemented using the open-source Python package GPy 4 , and we used an RBF GP kernel.Again, the initial proposed physics parameters were used as the initial guess for the algorithm.We discretize the parameter space into 50 bins, and use a gaussian process sampling population size of n = 100.Because we do not calculate the value function for each policy, we cannot use the value as the stopping criterion as in Zhu et al.[14].Instead, we stop iterating when the predicted parameter value does not change for two consecutive timesteps.This stopping parameter was determined empirically for our experiment after being shown to outperform two other stopping criteria: no stopping (which performed poorly because of noise in the GP sampling), and stopping after the parameter estimate changed by less than a threshold &gt; 0.The initial guesses and simulated outcomes supplied to both CMA-ES and Greedy Entropy Search were identical to the inputs supplied to TuneNet, including all normalization transformations applied to the data.
Residual Reinforcement Learning for Robot Control. T Johannink, S Bahl, A Nair, J Luo, A Kumar, M Loskyll, J A Ojea, E Solowjow, S Levine, CoRR, abs/1812.02018</p>
<p>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, Oct. 2018CoRR</p>
<p>Grounded Action Transformation for Robot Learning in Simulation. J P Hanna, P Stone, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17). the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)Feb. 2017</p>
<p>Sim-to-Real Transfer with Neural-Augmented Robot Simulation. F Golemo, A A Taïga, P.-Y Oudeyer, A Courville, 2nd Conference on Robot Learning (CoRL18). 201887</p>
<p>TossingBot: Learning to Throw Arbitrary Objects with Residual Physics. A Zeng, S Song, J Lee, A Rodriguez, T A Funkhouser, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsFreiburg im Breisgau, GermanyJune 2019</p>
<p>T Silver, K Allen, J Tenenbaum, L P Kaelbling, CoRR, abs/1812.0Residual Policy Learning. 2018</p>
<p>Sim-to-Real Robot Learning from Pixels with Progressive Nets. A A Rusu, M Večerík, T Rothörl, N Heess, R Pascanu, R Hadsell, Conference on Robot Learning. 2017</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, IEEE International Conference on Intelligent Robots and Systems. IEEESept. 2017</p>
<p>Sim-to-Real: Learning Agile Locomotion For Quadruped Robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, Robotics: Science and Systems XIV. Robotics: Science and Systems Foundation. June 2018</p>
<p>A Rajeswaran, S Ghotra, B Ravindran, S Levine, Epopt, Learning Robust Neural Network Policies Using Model Ensembles. CoRR. 2016</p>
<p>Asymmetric Actor Critic for Image-Based Robot Learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, P Abbeel, Robotics: Science and Systems XIV. Robotics: Science and Systems Foundation. June 2017</p>
<p>Parameter identification of robot dynamics. P Khosla, T Kanade, 24th IEEE Conference on Decision and Control. IEEE1985. Dec. 1985. ISBN 9684641354</p>
<p>On the identification of the inertial parameters of robots. M Gautier, W Khalil, Proceedings of the 27th IEEE Conference on Decision and Control. the 27th IEEE Conference on Decision and ControlIEEE1988</p>
<p>Fast Model Identification via Physics Engines for Data-efficient Policy Search. S Zhu, A Kimmel, K E Bekris, A Boularias, Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI). the 27th International Joint Conference on Artificial Intelligence (IJCAI)2018</p>
<p>Simulation-based design of dynamic controllers for humanoid balancing. J Tan, Z Xie, B Boots, C K Liu, IEEE International Conference on Intelligent Robots and Systems. IEEEOct. 2016</p>
<p>Closing the Reality Gap of Robotic Simulators through Task-oriented Bayesian Optimization. S Zhu, D Surovik, K E Bekris, A Boularias, Journal of Machine Learning Research. 2019</p>
<p>Physically consistent state estimation and system identification for contacts. S Kolev, E Todorov, IEEE-RAS International Conference on Humanoid Robots, volume 2015-Decem. IEEENov. 2015. ISBN 9781479968855</p>
<p>Preparing for the Unknown: Learning a Universal Policy with Online System Identification. W Yu, J Tan, C K Liu, G Turk, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsCambridge, MassachusettsJuly 2017</p>
<p>Augmenting Physical Simulators with Stochastic Neural Networks: Case Study of Planar Pushing and Bouncing. A Ajay, J Wu, N Fazeli, M Bauza, L P Kaelbling, J B Tenenbaum, A Rodriguez, International Conference on Intelligent Robots and Systems (IROS). 2018</p>
<p>Combining learned and analytical models for predicting action effects. A Kloss, S Schaal, J Bohg, 2017CoRR</p>
<p>Sim-to-Real via Sim-to-Sim: Robotic Grasping via Randomized-to-Canonical Adaptation Networks. S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, S Levine, R Hadsell, K Bousmalis, Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Vr-goggles for robots: Real-to-sim domain adaptation for visual control. J Zhang, L Tai, P Yun, Y Xiong, M Liu, J Boedecker, W Burgard, IEEE Robotics and Automation Letters. 422019</p>
<p>CyCADA: Cycle-Consistent Adversarial Domain Adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A A Efros, T Darrell, CoRR, abs/1711.02017</p>
<p>DensePhysNet: Learning Dense Physical Object Representations Via Multi-Step Dynamic Interactions. Z Xu, J Wu, A Zeng, J Tenenbaum, S Song, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsFreiburg im Breisgau, GermanyJune 2019</p>
<p>Automatic differentiation in PyTorch. A Paszke, G Chanan, Z Lin, S Gross, E Yang, L Antiga, Z Devito, 31st Conference on Neural Information Processing Systems (NIPS). Long Beach, CA2017ISBN 9788578110796</p>
<p>The CMA Evolution Strategy: A Tutorial. N Hansen, CoRR, abs/1604.02016</p>
<p>Data-efficient policy evaluation through behavior policy search. J P Hanna, P S Thomas, P Stone, S Niekum, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning201770</p>
<p>Identifiability and identification of inertial parameters using the underactuated base-link dynamics for legged multibody systems. K Ayusawa, G Venture, Y Nakamura, The International Journal of Robotics Research. 332013</p>
<p>Friction Variability in Auto-collected Dataset of Planar Pushing Experiments and Anisotropic Friction. D Ma, A Rodriguez, IEEE Robotics and Automation Letters. 2377-3766342018</p>
<p>Identifiability Analysis of Planar Rigid-Body Frictional Contact. N Fazeli, R Tedrake, A Rodriguez, International Symposium on Robotics Research (ISRR). Springer2015page To appear. ISBN 9781424479740</p>
<p>USA Racquetball Official Rules of Racquetball. USA Racquetball. O E Dietrich, 2015</p>
<p>BayesSim: Adaptive Domain Randomization Via Probabilistic Inference for Robotics Simulators. F Ramos, R C Possas, D Fox, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsFreiburg im Breisgau, GermanyJune 2019</p>            </div>
        </div>

    </div>
</body>
</html>
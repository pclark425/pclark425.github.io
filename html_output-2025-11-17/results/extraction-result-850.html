<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-850 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-850</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-850</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-260334759</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.16789v2.pdf" target="_blank">ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</a></p>
                <p><strong>Paper Abstract:</strong> Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e850.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e850.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolLLaMA (LLaMA fine-tuned on ToolBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-2 7B model fine-tuned on the ToolBench instruction-solution dataset and equipped with a neural API retriever and DFSDT reasoning strategy to perform multi-step API-based tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-2 (base transformer) fine-tuned in multi-round conversation format on instruction→solution paths involving real API calls; extended context via positional interpolation to 8192 tokens; paired with a neural Sentence-BERT based API retriever; uses DFSDT (depth-first search decision tree) at inference to explore multi-step reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench (tool-use instructions) and APIBench (OOD tool-use benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning / multi-tool planning (sequential decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported to substantially outperform Text-Davinci-003 and Claude-2, perform almost on par with ChatGPT (teacher) and second to GPT-4 when using DFSDT; robust zero-shot generalization to unseen APIs and OOD datasets (APIBench). (Qualitative — paper gives relative rankings rather than exact universal numeric scores in the main text.)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool-use interface (concatenated API docs), extended context window (positional interpolation), neural API retriever (dense Sentence-BERT), DFSDT decision-tree reasoning strategy</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning on instruction→solution pairs (ToolBench), uses ChatGPT-generated solution paths; context-extension during fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training data (ToolBench), reasoning strategy (DFSDT), retrieval integration (neural API retriever), context-extension</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Trained on ToolBench (126k instruction→solution examples with real API calls and multi-tool multi-step solution paths), uses DFSDT at annotation and inference to explore multiple reasoning traces and retract decisions; paired with a neural dense API retriever (Sentence-BERT contrastive training) to select relevant APIs from 16k+ pool; context length extended to 8192 via positional interpolation to accommodate long API responses.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Enables open-source LLaMA-2 7B to master complex multi-step tool-use tasks: ToolLLaMA + DFSDT outperforms earlier LLMs (Text-Davinci-003, Claude-2), nearly matches ChatGPT on ToolBench and generalizes well to OOD APIBench; API retriever can even improve pass/win rates relative to using oracle API set by selecting better alternative APIs. DFSDT significantly improves pass rates over ReACT during annotation and inference (qualitative and reported statistically across scenarios; more pronounced on harder multi-tool cases).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper argues open-source LLMs were mainly instruction-tuned on basic language tasks (QA/dialogue) and thus lack exposure to tool-use trajectories; vanilla CoT/ReACT reasoning strategies suffer from limited exploration and error propagation, which hinders multi-step interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e850.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (base pretrained model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source foundation LLM used as the backbone for ToolLLaMA and other instruction-tuned variants; strong general language abilities but not specialized for tool use before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA-2 (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based foundation model; in this paper LLaMA-2 7B is the backbone that was fine-tuned to produce ToolLLaMA. Base architecture not modified beyond context extension when fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer; extended context via positional interpolation when fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>pretraining (base) then supervised fine-tuning (ToolLLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>fine-tuning on tool-use dataset (ToolBench); context-extension</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fine-tuned with ToolBench instruction→solution pairs; used positional interpolation to increase context to 8192 tokens during training.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>After fine-tuning (ToolLLaMA), the LLaMA-2 backbone achieved strong tool-use capabilities comparable to ChatGPT for the tasks evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Baseline pretrained LLaMA (without tool-use instruction tuning) lacks the specialized training signal to perform multi-step API use; the gap is closed by targeted instruction tuning on tool-use data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e850.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (LLaMA variant fine-tuned for dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLaMA-based chat model fine-tuned on conversational instruction data; demonstrates strong dialogue/instruction-following but fails at multi-step tool-use in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA variant fine-tuned for general-purpose dialogue/instruction following (conversational instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench tool-use evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Pass rate and win rate = 0 on ToolBench (paper reports Vicuna failed to pass any instruction under their evaluation), indicating inability to perform tool-use despite dialogue tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>none specific to tool use (standard dialogue fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>instruction/dialogue fine-tuning (conversational data)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The model's instruction tuning focused on basic language/dialogue tasks and did not include tool-use training, leading to poor performance on interactive multi-step API tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e850.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (instruction-tuned LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLaMA model fine-tuned via self-instruct-style instruction data; strong on generic instruction following but found unable to execute tool-use instructions in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA fine-tuned with synthetic instruction-response examples (self-instruct style) for general instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench tool-use evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Pass rate and win rate = 0 on ToolBench (the paper reports Alpaca failed to pass any instruction), indicating inability to perform API-based multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>instruction fine-tuning (self-instruct dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Instruction tuning datasets used for Alpaca emphasize language/dialogue tasks rather than multi-step tool-use, leaving a capability gap for interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e850.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (teacher model; gpt-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source SOTA conversational LLM used as a teacher for data generation and a strong baseline for tool-use; demonstrates strong tool-use abilities and was used both for data creation and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-16k in data generation; ChatGPT used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source transformer LLM with function call capability; used to generate ToolBench instruction and solution paths (function-call feature) and as a baseline evaluated with ReACT/DFSDT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench tool-use evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Strong tool-use performance; ChatGPT + DFSDT surpassed GPT-4+ReACT in pass rate and performed comparably in win rate. Serves as the 'teacher model' whose outputs were used to build ToolBench.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>function-call API interface, ability to accept API docs as functions</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used function-call capability to generate diverse instruction/solution pairs and to search for solution paths during dataset creation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Provided high-quality annotation and solution paths for ToolBench and served as a high-performing baseline for tool-use; its generated data enabled fine-tuning open models to attain tool-use abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e850.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source SOTA LLM used as a strong comparison baseline; high capability in both reasoning and tool use but in some configurations outperformed by ChatGPT+DFSDT on pass rate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capacity closed-source transformer LLM; used as an upper-baseline in tool-use evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench tool-use evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Top-tier performance: GPT-4 + DFSDT achieves the highest pass rates (ToolLLaMA ranked second to GPT-4+DFSDT); GPT-4+ReACT is outperformed by ChatGPT+DFSDT in pass rate according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e850.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFSDT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-First Search-based Decision Tree (DFSDT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-search reasoning strategy that uses a depth-first traversal of alternative action nodes to explore multiple reasoning traces and enables decision retraction, improving annotation efficiency and multi-step planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DFSDT (reasoning strategy/agent procedure)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Algorithmic prompting/decision process where LLM generates child action nodes, explores along a depth-first order (pre-order traversal variant), allows retraction/abandonment of nodes, and expands diverse child nodes by prompting for distinct alternatives; designed to reduce API-call sorting costs by skipping heavy sorting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Used for solution-path annotation and inference on ToolBench (tool-use tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step reasoning / sequential decision-making / tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Significantly outperforms ReACT and ReACT@N baselines in pass rate across I1/I2/I3 scenarios; more pronounced improvements for harder multi-tool instructions (I2/I3). Also increases annotation efficiency (fewer total API calls to find valid solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>decision-tree search strategy, retraction mechanism, multi-trace evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>reasoning strategy / prompting algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Replaces linear CoT/ReACT single-trace reasoning with a DFS decision tree that explicitly explores multiple candidate branches, can abandon poor branches, and encourages diversity in child-node generation; implemented with a pre-order DFS variant to reduce LLM evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves both annotation pass rates (more instructions successfully annotated) and model inference pass/win rates when applied to LLMs; ChatGPT+DFSDT outperforms GPT-4+ReACT; DFSDT's advantage is larger on complex, multi-tool tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Addresses CoT/ReACT limitations (error propagation and limited exploration) that contribute to poor interactive/multi-step performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e850.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReACT (reasoning+acting baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-and-acting prompting method that interleaves chain-of-thought with environment actions/observations; used as a baseline and shown to be less effective than DFSDT for multi-step tool use in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReACT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting strategy where LLM emits 'Thought' and 'Action' steps interleaved with environment responses; explores a single reasoning trace in a linear fashion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolBench tool-use evaluation (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Lower pass rates and win rates than DFSDT across all evaluated settings; even running ReACT multiple times (ReACT@N) to match cost does not reach DFSDT performance, especially on harder multi-tool tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought + action interleaving (single-trace)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Baseline; demonstrated limitations including error propagation and limited exploration which DFSDT mitigates.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Its single-trace exploration leads to error cascades and limited search of action space, explaining weaker performance on complex interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e850.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>API Retriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural API Retriever (Sentence-BERT dense retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense embedding-based retriever (Sentence-BERT / BERT-Base backbone) trained contrastively to match instructions to relevant API documentation snippets from 16k+ APIs, used to recommend candidate APIs to ToolLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Neural API Retriever (Sentence-BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dense retriever encoding instructions and API docs into embeddings and ranking by similarity; trained with positive relevant APIs (from ToolBench) and negative sampled APIs; outperforms BM25 and OpenAI text-embedding-ada-002 baselines on NDCG.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>API retrieval for ToolBench and APIBench (retrieval-aware tool use)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>retrieval component for tool use / tool selection</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>High retrieval precision (higher NDCG than BM25 and text-embedding-ada-002); when top-5 retrieved APIs are provided to ToolLLaMA, overall pass and win rates improved relative to using the ground-truth oracle set in some cases because retriever found better substitute APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>dense retrieval, contrastive training with negatives</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised contrastive training with relevant API labels from ToolBench</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method / retrieval integration</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Trained on (instruction, relevant API) pairs produced during ToolBench generation; returns top-K APIs to be included in model prompt for tool-use decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Alleviates need for oracle/manual API selection in large API pools and can increase final tool-use success by expanding candidate API choices.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e850.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolBench (Tool-use instruction tuning dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction→solution dataset automatically constructed with ChatGPT: 16,464 real-world REST APIs, 126k instruction→solution-path pairs spanning single-tool and multi-tool multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolBench (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset created by sampling APIs from RapidAPI, prompting ChatGPT to generate instructions and solution paths (with DFSDT), includes real API call/response examples, and labels for relevant APIs to train retrievers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>training and evaluation dataset for tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning / multi-tool planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Used to fine-tune ToolLLaMA, enabling the model to perform complex multi-tool instructions and generalize to unseen APIs and OOD APIBench; includes annotations that increased the model's ability to execute multi-step API sequences (qualitative improvements reported).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>dataset creation (ChatGPT-based generation and DFSDT for solution paths)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training data (dataset creation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Covers 16k+ real REST APIs across 49 categories; contains single-tool and multi-tool instructions and solution traces annotated via DFSDT; used to fine-tune LLaMA-2 to produce ToolLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Enables supervised fine-tuning that substantially improves interactive/tool-use capability of LLaMA-2; includes hard examples that elicit advanced planning and multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e850.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolEval (automatic tool-use evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation framework (ChatGPT-backed) with two metrics—pass rate and win rate—validated to have high agreement with human annotators for assessing tool-use solution paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolEval (evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT-based automatic evaluator applying defined criteria to judge whether a solution path 'Pass'/'Fail' and to compare two solution paths (win/lose/tie) using aspects like information richness, factuality, reasoning, milestones, exploration, and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>evaluation of tool-use solution paths (ToolBench/API calls)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>evaluation framework for tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Validated to have 87.1% agreement with human annotators on pass rate and 80.3% agreement on win rate; used as the principal automatic metric when comparing models on ToolBench.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provides scalable pass/win metrics for tool-use tasks and uses majority voting over multiple ChatGPT evaluator calls to stabilize decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Enables large-scale, relatively reliable automatic evaluation of multi-step tool-use that correlates strongly with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e850.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e850.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gorilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gorilla (pipeline for APIBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior pipeline specifically designed for APIBench that fine-tunes LLaMA-7B on APIBench training data; used as a comparison for OOD generalization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Gorilla</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-7B fine-tuned on APIBench with retrieval-aware and zero-shot training settings; pipeline optimized for the specific APIBench domains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (as referenced in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>APIBench (tool-use benchmark) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / API invocation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>ToolLLaMA + retriever outperforms Gorilla+BM25 in AST accuracy on several APIBench domains; with oracle retriever ToolLLaMA is consistently superior to Gorilla-ZS, and competitive with Gorilla-RS in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>retrieval-aware pipeline specific to APIBench</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>fine-tuning on APIBench (ZS/RS variants)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>dataset-specific fine-tuning and retrieval pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Gorilla is trained specifically on APIBench; used for comparison to test OOD generalization of ToolLLaMA trained on ToolBench.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Serves as a strong domain-specific baseline; ToolLLaMA trained on different API domains generalizes to APIBench and can outperform Gorilla in some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Gorilla: Large language model connected with massive apis <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>ToolQA: A dataset for LLM question answering with external tools <em>(Rating: 1)</em></li>
                <li>APIBank: A benchmark for tool-augmented LLMs <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-850",
    "paper_id": "paper-260334759",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ToolLLaMA",
            "name_full": "ToolLLaMA (LLaMA fine-tuned on ToolBench)",
            "brief_description": "A LLaMA-2 7B model fine-tuned on the ToolBench instruction-solution dataset and equipped with a neural API retriever and DFSDT reasoning strategy to perform multi-step API-based tool use.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ToolLLaMA",
            "model_description": "LLaMA-2 (base transformer) fine-tuned in multi-round conversation format on instruction→solution paths involving real API calls; extended context via positional interpolation to 8192 tokens; paired with a neural Sentence-BERT based API retriever; uses DFSDT (depth-first search decision tree) at inference to explore multi-step reasoning paths.",
            "model_size": "7B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolBench (tool-use instructions) and APIBench (OOD tool-use benchmark)",
            "interactive_task_type": "tool use / multi-step reasoning / multi-tool planning (sequential decision-making)",
            "interactive_performance": "Reported to substantially outperform Text-Davinci-003 and Claude-2, perform almost on par with ChatGPT (teacher) and second to GPT-4 when using DFSDT; robust zero-shot generalization to unseen APIs and OOD datasets (APIBench). (Qualitative — paper gives relative rankings rather than exact universal numeric scores in the main text.)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "tool-use interface (concatenated API docs), extended context window (positional interpolation), neural API retriever (dense Sentence-BERT), DFSDT decision-tree reasoning strategy",
            "training_method": "supervised fine-tuning on instruction→solution pairs (ToolBench), uses ChatGPT-generated solution paths; context-extension during fine-tuning",
            "intervention_type": "training data (ToolBench), reasoning strategy (DFSDT), retrieval integration (neural API retriever), context-extension",
            "intervention_description": "Trained on ToolBench (126k instruction→solution examples with real API calls and multi-tool multi-step solution paths), uses DFSDT at annotation and inference to explore multiple reasoning traces and retract decisions; paired with a neural dense API retriever (Sentence-BERT contrastive training) to select relevant APIs from 16k+ pool; context length extended to 8192 via positional interpolation to accommodate long API responses.",
            "intervention_effect": "Enables open-source LLaMA-2 7B to master complex multi-step tool-use tasks: ToolLLaMA + DFSDT outperforms earlier LLMs (Text-Davinci-003, Claude-2), nearly matches ChatGPT on ToolBench and generalizes well to OOD APIBench; API retriever can even improve pass/win rates relative to using oracle API set by selecting better alternative APIs. DFSDT significantly improves pass rates over ReACT during annotation and inference (qualitative and reported statistically across scenarios; more pronounced on harder multi-tool cases).",
            "hypothesized_cause_of_gap": "The paper argues open-source LLMs were mainly instruction-tuned on basic language tasks (QA/dialogue) and thus lack exposure to tool-use trajectories; vanilla CoT/ReACT reasoning strategies suffer from limited exploration and error propagation, which hinders multi-step interactive tasks.",
            "uuid": "e850.0",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLaMA-2 (base)",
            "name_full": "LLaMA-2 (base pretrained model)",
            "brief_description": "Open-source foundation LLM used as the backbone for ToolLLaMA and other instruction-tuned variants; strong general language abilities but not specialized for tool use before fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA-2 (pretrained)",
            "model_description": "Transformer-based foundation model; in this paper LLaMA-2 7B is the backbone that was fine-tuned to produce ToolLLaMA. Base architecture not modified beyond context extension when fine-tuning.",
            "model_size": "7B (as used in experiments)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "standard transformer; extended context via positional interpolation when fine-tuned",
            "training_method": "pretraining (base) then supervised fine-tuning (ToolLLaMA)",
            "intervention_type": "fine-tuning on tool-use dataset (ToolBench); context-extension",
            "intervention_description": "Fine-tuned with ToolBench instruction→solution pairs; used positional interpolation to increase context to 8192 tokens during training.",
            "intervention_effect": "After fine-tuning (ToolLLaMA), the LLaMA-2 backbone achieved strong tool-use capabilities comparable to ChatGPT for the tasks evaluated.",
            "hypothesized_cause_of_gap": "Baseline pretrained LLaMA (without tool-use instruction tuning) lacks the specialized training signal to perform multi-step API use; the gap is closed by targeted instruction tuning on tool-use data.",
            "uuid": "e850.1",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Vicuna",
            "name_full": "Vicuna (LLaMA variant fine-tuned for dialogue)",
            "brief_description": "An open-source LLaMA-based chat model fine-tuned on conversational instruction data; demonstrates strong dialogue/instruction-following but fails at multi-step tool-use in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Vicuna",
            "model_description": "LLaMA variant fine-tuned for general-purpose dialogue/instruction following (conversational instruction tuning).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolBench tool-use evaluation",
            "interactive_task_type": "tool use / multi-step reasoning",
            "interactive_performance": "Pass rate and win rate = 0 on ToolBench (paper reports Vicuna failed to pass any instruction under their evaluation), indicating inability to perform tool-use despite dialogue tuning.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "none specific to tool use (standard dialogue fine-tuning)",
            "training_method": "instruction/dialogue fine-tuning (conversational data)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "The model's instruction tuning focused on basic language/dialogue tasks and did not include tool-use training, leading to poor performance on interactive multi-step API tasks.",
            "uuid": "e850.2",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Alpaca",
            "name_full": "Alpaca (instruction-tuned LLaMA)",
            "brief_description": "An LLaMA model fine-tuned via self-instruct-style instruction data; strong on generic instruction following but found unable to execute tool-use instructions in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Alpaca",
            "model_description": "LLaMA fine-tuned with synthetic instruction-response examples (self-instruct style) for general instruction following.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolBench tool-use evaluation",
            "interactive_task_type": "tool use / multi-step reasoning",
            "interactive_performance": "Pass rate and win rate = 0 on ToolBench (the paper reports Alpaca failed to pass any instruction), indicating inability to perform API-based multi-step tasks.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "instruction fine-tuning (self-instruct dataset)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Instruction tuning datasets used for Alpaca emphasize language/dialogue tasks rather than multi-step tool-use, leaving a capability gap for interactive tasks.",
            "uuid": "e850.3",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (teacher model; gpt-3.5 family)",
            "brief_description": "Closed-source SOTA conversational LLM used as a teacher for data generation and a strong baseline for tool-use; demonstrates strong tool-use abilities and was used both for data creation and evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ChatGPT (gpt-3.5-turbo-16k in data generation; ChatGPT used as baseline)",
            "model_description": "Closed-source transformer LLM with function call capability; used to generate ToolBench instruction and solution paths (function-call feature) and as a baseline evaluated with ReACT/DFSDT.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolBench tool-use evaluation",
            "interactive_task_type": "tool use / multi-step reasoning",
            "interactive_performance": "Strong tool-use performance; ChatGPT + DFSDT surpassed GPT-4+ReACT in pass rate and performed comparably in win rate. Serves as the 'teacher model' whose outputs were used to build ToolBench.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "function-call API interface, ability to accept API docs as functions",
            "training_method": null,
            "intervention_type": null,
            "intervention_description": "Used function-call capability to generate diverse instruction/solution pairs and to search for solution paths during dataset creation.",
            "intervention_effect": "Provided high-quality annotation and solution paths for ToolBench and served as a high-performing baseline for tool-use; its generated data enabled fine-tuning open models to attain tool-use abilities.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e850.4",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "Closed-source SOTA LLM used as a strong comparison baseline; high capability in both reasoning and tool use but in some configurations outperformed by ChatGPT+DFSDT on pass rate.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4",
            "model_description": "High-capacity closed-source transformer LLM; used as an upper-baseline in tool-use evaluations.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolBench tool-use evaluation",
            "interactive_task_type": "tool use / multi-step reasoning",
            "interactive_performance": "Top-tier performance: GPT-4 + DFSDT achieves the highest pass rates (ToolLLaMA ranked second to GPT-4+DFSDT); GPT-4+ReACT is outperformed by ChatGPT+DFSDT in pass rate according to the paper.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e850.5",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DFSDT",
            "name_full": "Depth-First Search-based Decision Tree (DFSDT)",
            "brief_description": "A tree-search reasoning strategy that uses a depth-first traversal of alternative action nodes to explore multiple reasoning traces and enables decision retraction, improving annotation efficiency and multi-step planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "DFSDT (reasoning strategy/agent procedure)",
            "model_description": "Algorithmic prompting/decision process where LLM generates child action nodes, explores along a depth-first order (pre-order traversal variant), allows retraction/abandonment of nodes, and expands diverse child nodes by prompting for distinct alternatives; designed to reduce API-call sorting costs by skipping heavy sorting.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Used for solution-path annotation and inference on ToolBench (tool-use tasks)",
            "interactive_task_type": "planning / multi-step reasoning / sequential decision-making / tool use",
            "interactive_performance": "Significantly outperforms ReACT and ReACT@N baselines in pass rate across I1/I2/I3 scenarios; more pronounced improvements for harder multi-tool instructions (I2/I3). Also increases annotation efficiency (fewer total API calls to find valid solutions).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": "decision-tree search strategy, retraction mechanism, multi-trace evaluation",
            "training_method": null,
            "intervention_type": "reasoning strategy / prompting algorithm",
            "intervention_description": "Replaces linear CoT/ReACT single-trace reasoning with a DFS decision tree that explicitly explores multiple candidate branches, can abandon poor branches, and encourages diversity in child-node generation; implemented with a pre-order DFS variant to reduce LLM evaluation cost.",
            "intervention_effect": "Improves both annotation pass rates (more instructions successfully annotated) and model inference pass/win rates when applied to LLMs; ChatGPT+DFSDT outperforms GPT-4+ReACT; DFSDT's advantage is larger on complex, multi-tool tasks.",
            "hypothesized_cause_of_gap": "Addresses CoT/ReACT limitations (error propagation and limited exploration) that contribute to poor interactive/multi-step performance.",
            "uuid": "e850.6",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ReACT",
            "name_full": "ReACT (reasoning+acting baseline)",
            "brief_description": "A reasoning-and-acting prompting method that interleaves chain-of-thought with environment actions/observations; used as a baseline and shown to be less effective than DFSDT for multi-step tool use in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ReACT",
            "model_description": "Prompting strategy where LLM emits 'Thought' and 'Action' steps interleaved with environment responses; explores a single reasoning trace in a linear fashion.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolBench tool-use evaluation (baseline)",
            "interactive_task_type": "tool use / sequential decision-making",
            "interactive_performance": "Lower pass rates and win rates than DFSDT across all evaluated settings; even running ReACT multiple times (ReACT@N) to match cost does not reach DFSDT performance, especially on harder multi-tool tasks.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": "chain-of-thought + action interleaving (single-trace)",
            "training_method": null,
            "intervention_type": "prompting strategy (baseline)",
            "intervention_description": null,
            "intervention_effect": "Baseline; demonstrated limitations including error propagation and limited exploration which DFSDT mitigates.",
            "hypothesized_cause_of_gap": "Its single-trace exploration leads to error cascades and limited search of action space, explaining weaker performance on complex interactive tasks.",
            "uuid": "e850.7",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "API Retriever",
            "name_full": "Neural API Retriever (Sentence-BERT dense retriever)",
            "brief_description": "A dense embedding-based retriever (Sentence-BERT / BERT-Base backbone) trained contrastively to match instructions to relevant API documentation snippets from 16k+ APIs, used to recommend candidate APIs to ToolLLaMA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Neural API Retriever (Sentence-BERT)",
            "model_description": "Dense retriever encoding instructions and API docs into embeddings and ranking by similarity; trained with positive relevant APIs (from ToolBench) and negative sampled APIs; outperforms BM25 and OpenAI text-embedding-ada-002 baselines on NDCG.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "API retrieval for ToolBench and APIBench (retrieval-aware tool use)",
            "interactive_task_type": "retrieval component for tool use / tool selection",
            "interactive_performance": "High retrieval precision (higher NDCG than BM25 and text-embedding-ada-002); when top-5 retrieved APIs are provided to ToolLLaMA, overall pass and win rates improved relative to using the ground-truth oracle set in some cases because retriever found better substitute APIs.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": "dense retrieval, contrastive training with negatives",
            "training_method": "supervised contrastive training with relevant API labels from ToolBench",
            "intervention_type": "training method / retrieval integration",
            "intervention_description": "Trained on (instruction, relevant API) pairs produced during ToolBench generation; returns top-K APIs to be included in model prompt for tool-use decision making.",
            "intervention_effect": "Alleviates need for oracle/manual API selection in large API pools and can increase final tool-use success by expanding candidate API choices.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e850.8",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ToolBench",
            "name_full": "ToolBench (Tool-use instruction tuning dataset)",
            "brief_description": "A large instruction→solution dataset automatically constructed with ChatGPT: 16,464 real-world REST APIs, 126k instruction→solution-path pairs spanning single-tool and multi-tool multi-step reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ToolBench (dataset)",
            "model_description": "Dataset created by sampling APIs from RapidAPI, prompting ChatGPT to generate instructions and solution paths (with DFSDT), includes real API call/response examples, and labels for relevant APIs to train retrievers.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "training and evaluation dataset for tool use",
            "interactive_task_type": "tool use / multi-step reasoning / multi-tool planning",
            "interactive_performance": "Used to fine-tune ToolLLaMA, enabling the model to perform complex multi-tool instructions and generalize to unseen APIs and OOD APIBench; includes annotations that increased the model's ability to execute multi-step API sequences (qualitative improvements reported).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": "dataset creation (ChatGPT-based generation and DFSDT for solution paths)",
            "intervention_type": "training data (dataset creation)",
            "intervention_description": "Covers 16k+ real REST APIs across 49 categories; contains single-tool and multi-tool instructions and solution traces annotated via DFSDT; used to fine-tune LLaMA-2 to produce ToolLLaMA.",
            "intervention_effect": "Enables supervised fine-tuning that substantially improves interactive/tool-use capability of LLaMA-2; includes hard examples that elicit advanced planning and multi-step reasoning.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e850.9",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ToolEval",
            "name_full": "ToolEval (automatic tool-use evaluator)",
            "brief_description": "An automatic evaluation framework (ChatGPT-backed) with two metrics—pass rate and win rate—validated to have high agreement with human annotators for assessing tool-use solution paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ToolEval (evaluator)",
            "model_description": "ChatGPT-based automatic evaluator applying defined criteria to judge whether a solution path 'Pass'/'Fail' and to compare two solution paths (win/lose/tie) using aspects like information richness, factuality, reasoning, milestones, exploration, and cost.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "evaluation of tool-use solution paths (ToolBench/API calls)",
            "interactive_task_type": "evaluation framework for tool use",
            "interactive_performance": "Validated to have 87.1% agreement with human annotators on pass rate and 80.3% agreement on win rate; used as the principal automatic metric when comparing models on ToolBench.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "evaluation methodology",
            "intervention_description": "Provides scalable pass/win metrics for tool-use tasks and uses majority voting over multiple ChatGPT evaluator calls to stabilize decisions.",
            "intervention_effect": "Enables large-scale, relatively reliable automatic evaluation of multi-step tool-use that correlates strongly with human judgments.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e850.10",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Gorilla",
            "name_full": "Gorilla (pipeline for APIBench)",
            "brief_description": "A prior pipeline specifically designed for APIBench that fine-tunes LLaMA-7B on APIBench training data; used as a comparison for OOD generalization experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Gorilla",
            "model_description": "LLaMA-7B fine-tuned on APIBench with retrieval-aware and zero-shot training settings; pipeline optimized for the specific APIBench domains.",
            "model_size": "7B (as referenced in paper)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "APIBench (tool-use benchmark) evaluation",
            "interactive_task_type": "tool use / API invocation",
            "interactive_performance": "ToolLLaMA + retriever outperforms Gorilla+BM25 in AST accuracy on several APIBench domains; with oracle retriever ToolLLaMA is consistently superior to Gorilla-ZS, and competitive with Gorilla-RS in some settings.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": "retrieval-aware pipeline specific to APIBench",
            "training_method": "fine-tuning on APIBench (ZS/RS variants)",
            "intervention_type": "dataset-specific fine-tuning and retrieval pipeline",
            "intervention_description": "Gorilla is trained specifically on APIBench; used for comparison to test OOD generalization of ToolLLaMA trained on ToolBench.",
            "intervention_effect": "Serves as a strong domain-specific baseline; ToolLLaMA trained on different API domains generalizes to APIBench and can outperform Gorilla in some configurations.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e850.11",
            "source_info": {
                "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Gorilla: Large language model connected with massive apis",
            "rating": 2,
            "sanitized_title": "gorilla_large_language_model_connected_with_massive_apis"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        },
        {
            "paper_title": "ToolQA: A dataset for LLM question answering with external tools",
            "rating": 1,
            "sanitized_title": "toolqa_a_dataset_for_llm_question_answering_with_external_tools"
        },
        {
            "paper_title": "APIBank: A benchmark for tool-augmented LLMs",
            "rating": 2,
            "sanitized_title": "apibank_a_benchmark_for_toolaugmented_llms"
        }
    ],
    "cost": 0.019729999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS
3 Oct 2023</p>
<p>Yujia Qin yujiaqin16@gmail.com 
Tsinghua University</p>
<p>Shihao Liang 
Tsinghua University</p>
<p>Yining Ye 
Tsinghua University</p>
<p>Kunlun Zhu 
Tsinghua University</p>
<p>Lan Yan 
Tsinghua University</p>
<p>Yaxi Lu 
Yankai Lin 
Tsinghua University</p>
<p>Renmin University of China</p>
<p>Xin Cong 
Tsinghua University</p>
<p>Xiangru Tang 
Yale University
5 WeChat AI</p>
<p>Tencent Inc. 6 Zhihu Inc</p>
<p>Bill Qian 
Yale University
5 WeChat AI</p>
<p>Tencent Inc. 6 Zhihu Inc</p>
<p>Sihan Zhao 
Tsinghua University</p>
<p>Lauren Hong 
Tsinghua University</p>
<p>Runchu Tian 
Tsinghua University</p>
<p>Ruobing Xie 
Jie Zhou 
Mark Gerstein 
Yale University
5 WeChat AI</p>
<p>Tencent Inc. 6 Zhihu Inc</p>
<p>Dahai Li 
ModelBest Inc</p>
<p>Zhiyuan Liu 
Tsinghua University</p>
<p>Maosong Sun 
Tsinghua University</p>
<p>• • Movies </p>
<p>Tool API API Collection Instruction Generation Solution Path Annotation LLaMA ToolLLaMA RapidAPI</p>
<p>TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS
3 Oct 20234E9CA5C6C64659A015849270659FBE8DarXiv:2307.16789v2[cs.AI]API Name: Search By Title API Description: Search movies and series by title… Required Parameters: Title (stringtitle to search for)Country (string…) Optional Parameters: Show type (stringType of shows to include in the resultseither "movie""series"or "all". Default is "all")… Code Snippets: GET /v2/search/title?title=batman&amp;country=us&amp;show… Example Response: type:"movie"title:"Batman"overview:"Japanese… GET /v2/search/title?title=batman&amp;country=us&amp;show… Example Response: type:"movie"title:"Batman"overview:"Japanese…
Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions.The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain.This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT.To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation.We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT.Specifically, the construction can be divided into three stages: (i) API collection: we collect 16, 464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction.To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm.It enables LLMs to evaluate multiple reasoning traces and expand the search space.Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction.Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT.Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.The codes, trained models, and demo are publicly available at https://github.com/OpenBMB/ToolBench.</p>
<p>INTRODUCTION</p>
<p>Tool learning (Qin et al., 2023b) aims to unleash the power of large language models (LLMs) to effectively interact with various tools (APIs) to accomplish complex tasks.By integrating LLMs with APIs, we can greatly expand their utility and empower them to serve as efficient intermediaries between users and the vast ecosystem of applications.Although open-source LLMs, e.g., LLaMA (Touvron et al., 2023a), have achieved versatile capabilities through instruction tuning (Taori et al., 2023;Chiang et al., 2023), they still lack the sophistication in performing higher-level tasks, such as appropriately interacting with tools (APIs) to fulfill complex human instruction.This deficiency is because current instruction tuning largely focuses on basic language tasks, with a relative neglect of the tool-use domain.On the other hand, current state-of-the-art (SOTA) LLMs (e.g., ChatGPT (OpenAI, Figure 1: Three phases of constructing ToolBench and how we train our API retriever and ToolLLaMA.</p>
<p>During inference of an instruction, the API retriever recommends relevant APIs to ToolLLaMA, which performs multiple rounds of API calls to derive the final answer.The whole reasoning process is evaluated by ToolEval.2022) and GPT-4 (OpenAI, 2023)), which have demonstrated impressive competencies in utilizing tools (Bubeck et al., 2023), are closed-source with their inner mechanisms opaque.This limits the democratization of AI technologies and the scope of community-driven innovation and development.</p>
<p>In this regard, we deem it urgent to empower open-source LLMs to skillfully master diverse APIs.</p>
<p>Although prior works have explored building instruction tuning data for tool use (Li et al., 2023a;Patil et al., 2023;Tang et al., 2023;Xu et al., 2023b), they fail to fully stimulate the tool-use capabilities within LLMs and have inherent limitations: (1) limited APIs: they either fail to involve real-world APIs (e.g., RESTAPI) (Patil et al., 2023;Tang et al., 2023) or consider only a small scope of APIs with poor diversity (Patil et al., 2023;Xu et al., 2023b;Li et al., 2023a); For win rate, we compare each method with ChatGPT-ReACT.DFSDT is our improved reasoning strategy over ReACT.ToolLLaMA surpasses Text-Davinci-003, Claude-2, and almost performs on par with ChatGPT.</p>
<p>(2) constrained scenario: existing works are confined to instructions that only involve one single tool.In contrast, real-world scenarios may require that multiple tools are interleaved together for multi-round tool execution to solve a complex task.Besides, they often assume that users manually specify the ideal API set for a given instruction in advance, which is infeasible with a large collection of real-world APIs; (3) inferior planning and reasoning: existing works adopted either CoT (Wei et al., 2023) or ReACT (Yao et al., 2022) for model reasoning, which cannot fully elicit the capabilities stored in LLMs and thus fail to handle complex instructions.In addition, some works do not even execute APIs to obtain real responses (Patil et al., 2023;Tang et al., 2023), which serve as important information for subsequent model planning.</p>
<p>To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework including data construction, model training, and evaluation.As illustrated in Figure 1, we collect a high-quality instruction-tuning dataset ToolBench.It is constructed automatically using ChatGPT (gpt-3.5-turbo-16k),which has been upgraded with function call (link) capabilities.The comparison between ToolBench and prior works is listed in Table 1.Specifically, the construction of ToolBench entails three phases:</p>
<p>• API Collection: we gather 16,464 representational state transfer (REST) APIs from RapidAPI (link), a platform that hosts massive real-world APIs provided by developers.These APIs span 49 diverse categories such as social media, e-commerce, and weather.For each API, we crawl detailed API documents from RapidAPI, including the functionality descriptions, required parameters, code snippets for API calls, etc.By comprehending these documents to learn to execute APIs, LLMs can generalize to new APIs unseen during training;</p>
<p>• Instruction Generation: we first sample APIs from the whole set and then prompt ChatGPT to generate diverse instructions for these APIs.To cover practical scenarios, we curate instructions Resource ToolBench (this work) APIBench (Patil et al., 2023) API-Bank (Li et al., 2023a) ToolAlpaca (Tang et al., 2023) ToolBench (Xu et al., 2023b) Real-world API? that involve both single-tool and multi-tool scenarios.This ensures that our model learns not only how to interact with individual tools but also how to combine them to accomplish complex tasks; • Solution Path Annotation: each solution path may contain multiple rounds of model reasoning and real-time API calls to derive the final response.However, even the most sophisticated LLM, i.e., GPT-4, achieves a low pass rate for complex human instructions, making annotation inefficient.To this end, we develop a novel depth-first search-based decision tree (DFSDT) to bolster the planning and reasoning ability of LLMs.Compared with conventional ReACT, DFSDT enables LLMs to evaluate a multitude of reasoning paths and make deliberate decisions to either retract steps or proceed along a promising path.In experiments, DFSDT significantly improves the annotation efficiency and successfully completes those complex instructions that cannot be fulfilled using ReACT.
✓ ✗ ✓ ✗ ✓ Real API Call&amp;Response? ✓ ✗ ✓ ✗ ✓ Multi-tool Scenario? ✓ ✗ ✗ ✗ ✗ API Retrieval? ✓ ✓ ✗ ✗ ✓ Multi-step Reasoning? ✓ ✗ ✓ ✓ ✓
To assess the tool-use capabilities of LLMs, we develop an automatic evaluator, ToolEval, backed up by ChatGPT.It comprises two key metrics: (1) pass rate, which measures LLM's ability to successfully execute an instruction within limited budgets, and (2) win rate, which compares the quality and usefulness of two solution paths.We demonstrate that ToolEval achieves a high correlation with human evaluation and provides a robust, scalable, and reliable assessment for machine tool use.</p>
<p>By fine-tuning LLaMA on ToolBench, we obtain ToolLLaMA.After evaluation based on our ToolEval, we derive the following findings:</p>
<p>• ToolLLaMA demonstrates a compelling capability to handle both single-tool and complex multitool instructions.As depicted in Figure 2, ToolLLaMA outperforms Text-Davinci-003 and Claude-2, achieves comparable performance to the "teacher model" ChatGPT, and is only slightly inferior to GPT4.Besides, ToolLLaMA exhibits robust generalization to previously unseen APIs, requiring only the API documentation to adapt to new APIs effectively.This flexibility allows users to incorporate novel APIs seamlessly, thus enhancing the model's practical utility.• We show that our DFSDT serves as a general decision-making strategy to enhance the reasoning capabilities of LLMs.DFSDT broadens the search space by considering multiple reasoning traces and achieves significantly better performance than ReACT.• We train a neural API retriever, which alleviates the need for manual selection from the large API pool in practice.As shown in Figure 1, given an instruction, the API retriever recommends a set of relevant APIs, which are sent to ToolLLaMA for multi-round decision making to derive the final answer.Despite sifting through a large pool of APIs, the retriever exhibits remarkable retrieval precision, returning APIs closely aligned with the ground truth.• ToolLLaMA exhibits strong generalization performance on an out-of-distribution (OOD) dataset APIBench (Patil et al., 2023).Despite not training on any of the APIs or instructions on APIBench, ToolLLaMA performs on par with Gorilla, a pipeline specifically designed for APIBench.</p>
<p>DATASET CONSTRUCTION</p>
<p>We introduce the three-stage construction process of ToolBench: API collection ( § 2.1), instruction generation ( § 2.2), and solution path annotation ( § 2.3).All procedures are based on ChatGPT (gpt-3.5-turbo-16k),requiring minimal human supervision and can be easily extended to new APIs.</p>
<p>APIs &amp; API Documentation</p>
<p>Instructions &amp; Relevant APIs</p>
<p>Category</p>
<p>Tool API</p>
<p>API COLLECTION</p>
<p>We start by introducing RapidAPI and its hierarchy, followed by how we crawl and filter APIs.</p>
<p>RapidAPI Hub RapidAPI is a leading API marketplace that connects developers with thousands of real-world APIs, streamlining the process of integrating diverse services into applications.Developers can test and connect with various APIs by registering only a RapidAPI key.All APIs in RapidAPI can be classified into 49 coarse-grained categories (link), such as sports, finance, and weather.The categories associate an API with the most relevant topic.Additionally, the hub also provides 500+ fine-grained categorization called collections (link), e.g., Chinese APIs and database APIs.APIs in the same collection share a common characteristic and often have similar functionalities or goals.</p>
<p>Hierarchy of RapidAPI As shown in Figure 3, each tool may be composed of multiple APIs.For each tool, we crawl the following information: the name and description of the tool, the URL of the host, and all the available APIs belonging to the tool; for each API, we record its name, description, HTTP method, required parameters, optional parameters, request body, executable code snippets for API call, and an example API call response.This rich and detailed metadata serves as a valuable resource for LLMs to understand and effectively use the APIs, even in a zero-shot manner.</p>
<p>API Filtering Initially, we gathered 10, 853 tools (53, 190 APIs) from RapidAPI.However, the quality and reliability of these APIs can vary significantly.In particular, some APIs may not be well-maintained, such as returning 404 errors or other internal errors.To this end, we perform a rigorous filtering process (details in appendix A.1) to ensure that the ultimate tool set of ToolBench is reliable and functional.Finally, we only retain 3, 451 high-quality tools (16, 464 APIs).</p>
<p>INSTRUCTION GENERATION</p>
<p>Different from prior works, we specifically focus on two crucial aspects for instruction generation:</p>
<p>(1) diversity: to train LLMs to handle a wide range of API usage scenarios, thereby boosting their generalizability and robustness; and (2) multi-tool usage: to mirror real-world situations that often demand the interplay of multiple tools, improving the practical applicability and flexibility of LLMs.</p>
<p>To this end, instead of brainstorming instructions from scratch and then searching for relevant APIs, we sample different combinations of APIs and craft various instructions that involve them.</p>
<p>Generating Instructions for APIs Define the total API set as S API , at each time, we sample a few APIs:
S sub N = {API 1 , • • • , API N } from S API .
We prompt ChatGPT to understand the functionalities of these APIs and then generate (1) possible instructions (Inst * ) that involve APIs in S sub N , and (2) relevant APIs (S rel * ⊂ S sub N ) for each instruction (Inst * ), i.e., {[S rel
1 , Inst 1 ], • • • , [S rel N ′ , Inst N ′ ]},
where N ′ denotes the number of generated instances.These (instruction, relevant API) pairs will be used for training the API retriever in § 3.1.We use different sampling strategies (introduced later) to cover all APIs and most of their combinations, thus ensuring the diversity of our instructions.</p>
<p>The prompt for ChatGPT is composed of (1) a general description of the intended instruction generation task, (2) comprehensive documentation of each API in S sub N , which helps ChatGPT understand their functionality and interplay, and (3) three in-context seed examples {seed 1 , seed 2 , seed 3 }.Each seed example is an ideal instruction generation written by human experts.These seed examples are leveraged to better regulate ChatGPT's behavior through in-context learning.In total, we wrote 12 / 36 diverse seed examples (S seed ) for the single-tool / multi-tool setting, and randomly sampled three examples at each time.Detailed prompts for instruction generation are described in appendix A.7. Overall, the generation process can be formulated as follows:
ChatGPT {API 1 ,••• ,API N }∈S API ,{seed 1 ,••• ,seed 3 }∈S seed ({[S rel 1 , Inst1], • • • , [S rel N' , Inst N ′ ]}|API1, • • • , APIN, seed1, • • • , seed3).
Sampling Strategies for Different Scenarios As shown in Figure 3, for the single-tool instructions (I1), we iterate over each tool and generate instructions for its APIs.However, for the multi-tool setting, since the interconnections among different tools in RapidAPI are sparse, random sampling tool combinations from the whole tool set often leads to a series of irrelevant tools that cannot be covered by a single instruction in a natural way.To address the sparsity issue, we leverage the RapidAPI hierarchy information.Since tools belonging to the same RapidAPI category or collection are generally related to each other in the functionality and goals, we randomly select 2-5 tools from the same category / collection and sample at most 3 APIs from each tool to generate the instructions.We denote the generated instructions as intra-category multi-tool instructions (I2) and intra-collection multi-tool instructions (I3), respectively.Through rigorous human evaluation, we find that instructions generated in this way already have a high diversity that covers various practical scenarios.We also provide visualization for instructions using Atlas (link) to support our claim.</p>
<p>After generating the initial set of instructions, we further filter those with the hallucinated relevant APIs by assessing whether they exist in S sub N .Finally, we collect nearly 200k qualified (instruction, relevant API) pairs, including 87413, 84815, and 25251 instances for I1, I2, and I3, respectively.</p>
<p>SOLUTION PATH ANNOTATION</p>
<p>As shown in Figure 4, given an instruction Inst * , we prompt ChatGPT to search for a valid action sequence: {a 1 , • • • , a N }.Such a multi-step decision-making process is cast as a multi-round conversation for ChatGPT.At each round t, the model generates an action a t based on previous interactions, i.e., ChatGPT(a
t |{a 1 , r 1 , • • • , a t−1 , r t−1 }, Inst * )
, where r * denotes the real API response.For each Preprint a t , ChatGPT should specify its "thought", which API to use, and the specific parameters for this API, i.e., a t has the following format: "Thought:
• • • , API Name: • • • , Parameters: • • • ".
To leverage the function call feature of ChatGPT, we treat each API as a special function and feed its API documentation into ChatGPT's function field.In this way, the model understands how to call the API.For each instruction Inst * , we feed all the sampled APIs S sub N to ChatGPT's as available functions.To let ChatGPT finish an action sequence, we define two additional functions, i.e., "Finish with Final Answer" and "Finish by Giving Up".The former function has a parameter that corresponds to a detailed final answer to the original instruction; while the latter function is designed for cases where the provided APIs cannot complete the original instruction after multiple API call attempts.</p>
<p>Depth First Search-based Decision Tree In our pilot studies, we find that CoT (Wei et al., 2023) or ReACT (Yao et al., 2022) has inherent limitations: (1) error propagation: a mistaken action may propagate the errors further and cause the model to be trapped in a faulty loop, such as continually calling an API in a wrong way or hallucinating APIs; (2) limited exploration: CoT or ReACT only explores one possible direction, leading to limited exploration of the whole action space.Hence even GPT-4 often fails to find a valid solution path, making annotation difficult.</p>
<p>To this end, we propose to construct a decision tree to expand the search space and increase the possibility of finding a valid path.As depicted in Figure 4, our DFSDT allows the model to assess different reasoning paths and choose to either (1) proceed along a promising path or (2) abandon an existing node by calling the "Finish by Giving Up" function and expand a new node.During node expansion, to diversify the child nodes and expand the search space, we prompt ChatGPT with the information of the previously generated nodes and explicitly encourage the model to generate a distinct node.For the searching process, we prefer depth-first search (DFS) instead of breadth-first search (BFS) because the annotation can be finished as long as one valid path is found.Using BFS will cost excessive OpenAI API calls.More details are described in appendix A.8.We perform DFSDT for all the generated instructions and only retain those passed solution paths.Ultimately, we generate 126, 486 (instruction, solution path) pairs, which are used to train ToolLLaMA in § 3.2.</p>
<p>EXPERIMENTS</p>
<p>In this section, we investigate the performance of ToolLLM framework.We first introduce the evaluation metric and evaluate the efficacy of API retriever and DFSDT in § 3.1.Then we present the main experiments in § 3.2, followed by a generalization experiment in § 3.3.</p>
<p>PRELIMINARY EXPERIMENTS</p>
<p>ToolEval Considering the API's temporal variability on RapidAPI and the infinite potential solution paths for an instruction, it is infeasible to annotate a fixed ground-truth solution path for each test instruction.Moreover, when comparing different models, it is crucial to ensure they employ the same version of APIs during evaluation.Considering that human evaluation can be time-consuming, we follow AlpacaEval (Li et al., 2023b) to develop an efficient evaluator ToolEval based on ChatGPT, which incorporates two evaluation metrics (details in appendix A.5): (1) Pass Rate: it calculates the proportion of successfully completing an instruction within limited budgets.The metric measures the executability of instructions for an LLM and can be seen as a basic requirement for ideal tool use; and (2) Win Rate: we provide an instruction and two solution paths to ChatGPT evaluator and obtain its preference (i.e., which one is better).We pre-define a set of criteria for both metrics and these criteria are organized as prompts for our ChatGPT evaluator.We evaluate multiple times based on ChatGPT to improve the reliability.Then we calculate the average results from the evaluator.</p>
<p>Through rigorous testing (details in appendix A.5), we find that ToolEval demonstrates a high agreement of 87.1% in pass rate and 80.3% in win rate with human annotators.This shows that ToolEval can reflect and represent human evaluation to a large extent.</p>
<p>Efficacy of API Retriever</p>
<p>The API retriever aims to retrieve relevant APIs to an instruction.We employ Sentence-BERT (Reimers &amp; Gurevych, 2019) to train a dense retriever based on BERT-BASE (Devlin et al., 2019).The API retriever encodes the instruction and API document into two embeddings, and calculates their relevance with embedding similarity.For training, we regard the relevant APIs of each instruction generated in § 2.2 as positive examples and sample a few other Preprint Method I1 I2 I3 Average NDCG NDCG NDCG NDCG @1 @5 @1 @5 @1 @5 @1 @5 BM25</p>
<p>18 APIs as negative examples for contrastive learning.For baselines, we choose BM25 (Robertson et al., 2009) and OpenAI's text-embedding-ada-002 (link).We evaluate the retrieval performance using NDCG (Järvelin &amp; Kekäläinen, 2002).We train and evaluate our model on single-tool instructions (I1), intra-category multi-tool instructions (I2), and intra-collection multi-tool instructions (I3).</p>
<p>As shown in Table 2, our API retriever consistently outperforms baselines across all settings, indicating its feasibility in real-world scenarios with massive APIs.Also, the NDCG score of I1 is generally higher than I2 and I3, which means single-tool instruction retrieval is simpler than multi-tool setting.</p>
<p>Superiority of DFSDT over ReACT Before solution path annotation, we validate the efficacy of DFSDT.Based on ChatGPT, we compare DFSDT and ReACT using the pass rate metric.Since DFSDT consumes more OpenAI API calls than ReACT, for a fairer comparison, we also establish a "ReACT@N" baseline, which conducts multiple times of ReACT until the total costs reach the same level of DFSDT.Once a valid solution is found by ReACT@N, we deem it a pass.</p>
<p>From Table 3, it can be observed that DFSDT significantly outperforms the two baselines in all scenarios.Since we only retain those passed annotations as the training data, given the same budgets, using DFSDT could annotate more instructions.This makes DFSDT a more efficient way that saves the total annotation cost.We also find that the performance improvement of DFSDT is more evident for harder instructions (i.e., I2 and I3) than those simpler instructions (I1).This means that by expanding the search space, DFSDT can better solve those difficult, complex instructions that are unanswerable by the vanilla ReACT no matter how many times it is performed.Involving such "hard examples" in our dataset can fully elicit the tool-use capabilities for those complex scenarios.</p>
<p>MAIN EXPERIMENTS</p>
<p>ToolLLaMA We fine-tune LLaMA-2 7B model (Touvron et al., 2023b) using the instructionsolution pairs.The original LLaMA-2 model has a sequence length of 4096, which is not enough under our setting since the API response can be very long.To this end, we use positional interpolation (Chen et al., 2023) to extend the context length to 8192 (training details in appendix A.3).</p>
<p>Settings Ideally, by scaling the number and diversity of instructions and unique tools in the training data, ToolLLaMA is expected to generalize to new instructions and APIs unseen during training.This is meaningful since users can define customized APIs and expect ToolLLaMA to adapt according to the documentation.To this end, we strive to evaluate the generalization ability of ToolLLaMA at three levels: (1) Inst.: unseen instructions for the same set of tools in the training data, (2) Tool: unseen tools that belong to the same (seen) category of the tools in the training data, and (3) Cat.: unseen tools that belong to a different (unseen) category of tools in the training data.</p>
<p>We perform experiments on three scenarios: single-tool instructions (I1), intra-category multi-tool instructions (I2), and intra-collection multi-tool instructions (I3).For I1, we conduct the evaluation for the aforementioned three levels (I1-Inst.,I1-Tool, and I1-Cat.);for I2, since the training instructions already involve different tools of the same category, we only perform level 1 and level 3 for the generalization evaluation (I2-Inst.and I2-Cat.);similarly, we only perform level 1 generalization for I3 (I3-Inst.)since it already covers instructions that involve various combinations of tools from different categories (the tools in a RapidAPI collection may come from different RapidAPI categories).</p>
<p>For each test instruction, we feed the ground-truth (oracle) APIs S sub N to each model.This simulates the scenario where the user specifies the API set they prefer.</p>
<p>Baselines We choose two LLaMA variants that have been fine-tuned for general-purpose dialogue, i.e., Vicuna (Chiang et al., 2023) and Alpaca (Taori et al., 2023).We also choose the "teacher model"  ChatGPT, Text-Davinci-003, GPT-4, and Claude-2 as baselines, and apply both DFSDT and ReACT to them.When calculating the win rate, each model is compared with ChatGPT-ReACT.</p>
<p>Main Results</p>
<p>The results are placed in Table 4, from which we derive that:</p>
<p>1.Although we conduct prompt engineering extensively, both Vicuna and Alpaca fail to pass any instruction (pass rate &amp; win rate = 0), which means their instruction-following abilities do not cover the tool-use domain.This underscores the deficiency of current instruction tuning attempts, which largely focus on language skills; 2. For all LLMs, using DFSDT significantly outperforms ReACT in both pass rate and win rate.</p>
<p>Notably, ChatGPT +DFSDT surpasses GPT-4+ReACT in pass rate and performs comparably in win rate.This underscores the superiority of DFSDT over ReACT in decision-making; 3. When using DFSDT, ToolLLaMA performs much better than Text-Dainci-003 and Claude-2, and achieves a result almost on par with ChatGPT (the teacher model).In general, despite generalizing to unseen instructions and tools, ToolLLaMA +DFSDT demonstrates competitive generalization performance in all scenarios, achieving a pass rate second to GPT4+DFSDT.</p>
<p>Overall, these results demonstrate that ToolBench can sufficiently elicit the tool-use capabilities within LLMs and empower them to skillfully master even unseen APIs for various instructions.</p>
<p>Integrating API Retriever with ToolLLaMA In real-world scenarios, asking users to manually recommend APIs from a large pool may not be practical.To emulate this practical setting and test the efficiency of our API retriever, we feed the top 5 APIs (instead of the ground truth APIs S sub N ) recommended by our API retriever to ToolLLaMA.As shown in Table 4, using retrieved APIs even improves the performance (both pass rate and win rate) compared to the ground truth API set.This is because many APIs in the ground truth API set can be replaced by other similar APIs with better functionalities, which our API retriever can successfully identify.In other words, our retriever expands the search space of relevant APIs and finds more appropriate ones for the current instruction.It provides robust evidence of the excellent ability of our API retriever to retrieve relevant APIs, especially considering the vast pool (16, 000+) of APIs from which our API retriever selects.</p>
<p>OUT-OF-DISTRIBUTION (OOD) GENERALIZATION TO APIBENCH (PATIL ET AL., 2023)</p>
<p>Settings We further extend ToolLLaMA to an OOD dataset APIBench to validate its generalization ability.To assess the generalization ability of ToolLLaMA in these new domains, we equip ToolLLaMA with two retrievers: our trained API retriever and the oracle retriever.We evaluate three domains of APIBench, i.e., TorchHub, TensorHub, and HuggingFace.We compare ToolLLaMA with Gorilla, a LLaMA-7B model fine-tuned using the training data of APIBench.Following the original paper, we adopt two official settings for Gorilla: zero-shot setting (ZS) and retrieval-aware setting (RS).The latter means (RS) the retrieved APIs are sent to the model as part of the prompts; while the former (ZS) does not incorporate the APIs in the prompts when training the model.We adopt the official evaluation metric and report the AST accuracy along with the hallucination rates.</p>
<p>Results</p>
<p>The results are shown in Table 5.In general, ToolLLaMA achieves remarkable OOD generalization performance on all three datasets, despite being trained on a completely different API domain and instruction domain.Specifically, ToolLLaMA+our API retriever outperforms Gorilla+BM25 from both training settings (ZS / RS) in terms of AST accuracy on HuggingFace and TorchHub.With the same oracle retriever, ToolLLaMA is consistently superior when compared to Gorilla-ZS.It should be noted that Gorilla model cannot be generalized to our ToolBench dataset due to our more complex settings, such as the multi-tool use and multi-step reasoning.</p>
<p>RELATED WORK</p>
<p>Tool Learning Recent studies have shed light on the burgeoning capabilities of LLMs in mastering tools and making decisions within complex environments (Vemprala et al., 2023;Nakano et al., 2021;Qin et al., 2023a;Shen et al., 2023;Wu et al., 2023;Schick et al., 2023;Hao et al., 2023;Qian et al., 2023;Song et al., 2023;Zhuang et al., 2023;Gao et al., 2023).Gaining access to external tools endows LLMs with real-time factual knowledge (Yang et al., 2023), multimodal functionalities (Gupta &amp; Kembhavi, 2023), and specialized skills in vertical domains (Jin et al., 2023).However, open-source LLMs still lag far behind SOTA LLMs in tool use, and how tool-use ability is acquired by SOTA LLMs remains unclear.In this paper, we aim to bridge this gap and fathom the underlying mechanism.</p>
<p>Instruction Tuning Instruction tuning enhances LLMs in understanding human instructions and generating proper responses (Wei et al., 2021;Bach et al., 2022;Mishra et al., 2022).Since manually annotating instruction tuning data is time-consuming, self-instruct (Wang et al., 2022) proposes to generate high-quality data from SOTA LLMs, which facilitates a recent trend of data curation for multi-turn dialogue (Taori et al., 2023;Chiang et al., 2023;Xu et al., 2023a;Penedo et al., 2023;Ding et al., 2023).However, compared with the dialogue, tool learning is inherently more challenging given the vast diversity of APIs and the complexity of multi-tool instructions.As a result, even GPT-4 often fails to find a valid solution path.However, existing tool-learning dataset (Li et al., 2023a;Patil et al., 2023;Tang et al., 2023;Xu et al., 2023b) and their construction methods cannot effectively address real human needs as mentioned in § 1.Instead, our ToolBench is designed for practical scenarios and improves the previous pipeline for tool-learning data construction.</p>
<p>Prompting LLMs for Decision Making Prompting facilitates LLMs to decompose high-level tasks into sub-tasks and generate grounded plans (Ahn et al., 2022;Huang et al., 2022a;b;Ye et al., 2023).ReACT (Yao et al., 2022) integrates reasoning with acting by allowing LLMs to give a proper reason for an action and incorporating environmental feedback for reasoning.However, these studies do not incorporate a mechanism for decision retraction, which becomes problematic as an initial error can lead to a cascade of subsequent errors.Recently, Reflexion (Shinn et al., 2023) mitigates this issue by asking LLMs to reflect on previous failures.Our DFSDT extends Reflexion to a more general method by allowing LLMs to assess different reasoning paths and select the most promising one.It should be noted DFSDT shares a similar idea to a concurrent work: tree-of-thought (ToT) reasoning (Yao et al., 2023).However, our DFSDT targets general decision-making problems where the decision space is infinite, compared to ToT's relatively simple tasks that can be addressed by brute-force search, such as Game of 24 and Crosswords.The distinct target between DFSDT and ToT determines the significant difference in the implementation details.</p>
<p>CONCLUSION</p>
<p>In this work, we introduce how to elicit the tool-use capabilities within LLMs.We first present an instruction tuning dataset, ToolBench, which covers 16k+ real-world APIs and various practical usecase scenarios including both single-tool and multi-tool tasks.The construction of ToolBench purely uses ChatGPT and requires minimal human supervision.Moreover, we propose DFSDT to reinforce the planning and reasoning ability of LLMs, enabling them to navigate through reasoning paths strategically.For efficient evaluation of tool learning, we devise an automatic evaluator ToolEval.</p>
<p>By fine-tuning LLaMA on ToolBench, the obtained model ToolLLaMA matches the performance of ChatGPT and exhibits remarkable generalization ability to unseen APIs.Besides, we develop a neural API retriever to recommend relevant APIs for each instruction.The retriever can be integrated with ToolLLaMA as a more automated tool-use pipeline.In the experiments, we demonstrate the generalization ability of our pipeline to out-of-distribution domains.In general, this work paves the way for future research in the intersection of instruction tuning and tool use for LLMs.We perform a rigorous filtering process to ensure that the ultimate tool set of ToolBench is reliable and functional.The filtering process is as follows: (1) initial testing: we begin by testing the basic functionality of each API to ascertain whether they are operational.We discard any APIs that do not meet this basic criterion;</p>
<p>(2) example response evaluation: we make API calls to obtain an example response.Then we evaluate their effectiveness by response time and quality.APIs that consistently exhibit a long response time are omitted.Also, we filter out the APIs with low-quality responses, such as HTML source codes or other error messages.</p>
<p>A.2 API RESPONSE COMPRESSION</p>
<p>When examining the response returned by each API, we discover that some responses may contain redundant information and are too long to be fed into LLMs.This may lead to problems due to the limited context length of LLMs.Therefore, we perform a response compression to reduce the length of API responses while maintaining their critical information.</p>
<p>Since each API has a fixed response format, we use ChatGPT to analyze one response example and remove unimportant keys within the response to reduce its length.The prompt of ChatGPT contains the following information for each API: (1) tool documentation, which includes tool name, tool description, API name, API description, parameters, and an example API response.This gives ChatGPT a hint of the API's functionality;</p>
<p>(2) 3 in-context learning examples, each containing an original API response and a compressed response schema written by experts.In this way, we obtain the response compression strategies for all APIs.During inference, when the API response length exceeds 1024 tokens, we compress the response by removing unimportant information.If the compressed response is still longer than 1024, we only retain the first 1024 tokens.Through human evaluation, we find that this compression retains important information contained in the API response and successfully removes the noises.</p>
<p>A.3 DETAILS FOR TRAINING TOOLLLAMA</p>
<p>We train the model in a multi-round conversation mode.For the training data format, we keep the input and output the same as those of ChatGPT.Since it is unclear how ChatGPT organizes the function call field, we just concatenate this information into the input as part of the prompt for ToolLLaMA.For the training hyper parameters, we use a learning rate of 5 × 10 −5 , a warmup ratio of 4 × 10 −2 , a total batch size of 64, a maximum sequence length of 8192, and use a position interpolation ratio of 2. We train the model for two epochs and select the model checkpoint with the best performance on the development set and then evaluate it on the test set.</p>
<p>A.4 DETAILS FOR DFSDT</p>
<p>In practice, it is essential to balance effectiveness with costs (the number of OpenAI API calls).Classical DFS algorithms generate multiple child nodes at each step, then sort all the child nodes, and select the highest-scoring node for expansion.After greedily expanding to the terminal node, DFS backtracks to explore nearby nodes, expanding the search space.Throughout the algorithm, the most resource-intensive part is the sorting process of child nodes.If we use an LLM to evaluate two nodes at a time, it requires approximately O(n log n) complexity of OpenAI API calls, where n is the number of child nodes.</p>
<p>In fact, we find empirically that in most cases, the nodes ranked highest are often the node generated at first.Therefore, we skip the sorting process of child nodes and choose a pre-order traversal (a variant for DFS) for the tree search.This design has the following advantages:</p>
<p>• If the model does not retract an action (e.g., for the case of simple instructions), then DFSDT degrades to ReACT, which makes it as efficient as ReACT.</p>
<p>• After the algorithm finishes, the nodes explored by this method are almost the same as those found by a classical DFS search.Hence, it can also handle complex instructions that only DFS can solve.</p>
<p>Overall, this design achieves a similar performance as DFS while significantly reducing costs.</p>
<p>It should also be noted that ReACT can be viewed as a degraded version of DFSDT.Therefore, although ToolLLaMA is trained on data created by DFSDT, the model can be used either through ReACT or DFSDT during inference.</p>
<p>A.5 DETAILS FOR TOOLEVAL</p>
<p>We adopt two metrics for automatic tool-use capability evaluation: pass rate and win rate.</p>
<p>Details for Pass Rate To assess whether a solution path completes the tasks outlined in the original instruction and successfully passes it, we need to first consider the solvability of the instruction.In principle, an instruction can be classified as either (1) solvable: for example, at least one of the provided tools is potentially helpful in solving the original instruction; or (2) unsolvable: for example, all APIs are irrelevant to the instruction or the instruction provides invalid information such as invalid email address.</p>
<p>To determine whether a solution path is deemed passed or not, we need to consider whether the instruction is solvable or unsolvable.In our evaluation, three types of labels can be given to each solution path, i.e., Pass, Fail, and Unsure.Specifically, we define different rules as follows:</p>
<p>If the instruction is solvable:</p>
<p>1.If the model gives finish type "Finish by Giving Up", (a) After trying all the APIs extensively during and receiving no helpful information from APIs, the solution path is deemed a Pass.(b) If the model only calls a few API or receiving valid information from the APIs, the solution path is deemed a Fail.2. If the model gives finish type "Finish with Final Answer", (a) If the APIs provide no valid information, and the model has tried all the APIs to retrieve useful information, but the final answer still does not resolve the original instruction or conveys a refusal (such as "I'm sorry, but I can't provide you with this, because the tools are unavailable"), the solution path is deemed a Pass.(b) If the tools provide valid information, and the final answer does not completely resolve the instruction or is a refusal, the solution path is deemed a Fail.(c) If the final answer completely resolves the original instruction, the solution path is deemed a Pass.(d) If it is unable to determine if the instruction is resolved based on the content of the final answer, the solution path is deemed an Unsure.</p>
<p>If the instruction is unsolvable:</p>
<p>1.If the model gives finish type "Finish with Final Answer", (a) If the final answer resolves an instruction that was initially considered unresolvable, the solution path is deemed a Pass.(b) If the final answer is a refusal, the solution path is deemed a Pass.(c) If the final answer is hallucinated by the model itself and provides a false positive response (such as "I've completed the task, the final answer is *"), the solution path is deemed a Fail.2. If the model gives finish type "Finish by Giving Up", (a) Under this case, the solution path is deemed a Pass.</p>
<p>For every solution path, we instruct the ChatGPT evaluator to generate multiple (≥ 4) predictions and perform a majority vote to derive the final pass rate.</p>
<p>Details for Win Rate Since pass rate only measures whether an instruction is completed or not, instead of how well it is completed, we adopt another metric: win rate.It is measured by comparing two solution paths for a given instruction.We assume that a passed candidate is better than a failed candidate and only compare those solution paths that are both "Pass", or both "Failed" annotated by the ChatGPT evaluator.Note that compared with another solution path, one solution path will be annotated with one of the following: win, lose, or tie.We build rules for the evaluator's behavior to decide which solution path is better, and the criteria are listed as follows:</p>
<ol>
<li>
<p>Information richness: whether the final answer contains all the necessary information to answer the original instruction.A significantly richer answer is better, while a similar level of richness that is sufficient to answer the question ties.</p>
</li>
<li>
<p>Factuality: whether it accurately describes what has been done, and what failed in the end.</p>
</li>
</ol>
<p>A more accurate description in the final answer is better.</p>
<ol>
<li>
<p>Reasoning: whether a detailed and accurate reason for failure is provided if the query remains unresolved.A more detailed reason is better.</p>
</li>
<li>
<p>Milestone: calculating the number of milestones reached during execution.</p>
</li>
<li>
<p>Exploration: whether more potentially useful APIs were attempted during the execution process.The use of a greater number of APIs is better.</p>
</li>
<li>
<p>Cost: Having fewer repeated (redundant) API calls is better if the number of APIs used is the same.</p>
</li>
</ol>
<p>For every solution path, we also generate multiple (≥ 4) predictions and then perform a majority vote to derive the final win rate.In Table 4, for ease of reading, we split the ratio of tie into two pieces and add them to win and lose, respectively.In Table 6, we report the original numbers as a reference.</p>
<p>Comparing Human Evaluation and ToolEval</p>
<p>To validate the reliability of ChatGPT evaluator in both pass rate and win rate, we sample among four different methods (ChatGPT+ReACT, ChatGPT+DFSDT, ToolLLaMA+DFSDT and GPT4+DFSDT) to obtain solution pairs for 300 test instructions for each method.Then we engage humans to annotate the pass rate for ChatGPT+DFSDT, ToolLLaMA+DFSDT and GPT4+DFSDT, and the win rate among ChatGPT+ReACT and Chat-GPT+DFSDT.Our ChatGPT evaluator demonstrates a high agreement of 87.1% in pass rate and 80.3% in win rate with human annotators.This result shows that our evaluator generates highly similar evaluation results to humans and can be viewed as a credible evaluator who simulates human evaluation on pass rate and win rate.</p>
<p>It should also be noted that the evaluation for tool learning is far more intricate than traditional tasks such as dialogue.The reason is that there may exist infinite "correct" solution paths for each instruction.In our initial investigations, we surprisingly found that even human experts often disagree with each other in deciding which solution path is better, leading to a relatively low agreement.For instance, one may prefer a solution path that uses only a few APIs to derive the final answer quickly; while another may prefer a solution path that extensively tries all the APIs to cross-validate specific information.In this regard, we believe there is still a long way to go for a fair evaluation of the tool-use domain, and we believe this work has paved the way for it.We expect more future works to explore this interesting research problem.</p>
<p>A.6 DETAILS FOR EXPERIMENTS ON APIBENCH</p>
<p>When generalizing ToolLLaMA to APIBench, no training updates were made to ToolLLaMA, but instead of treating each API in the prompt as a function call.We define one function that represents selecting an API, providing the code for invoking it, and describing the generated output in natural language.We do not consider the zero-shot setting of APIBench where the prompts do not contain any API descriptions because the APIs from the three tested domains were never encountered during training.In-context Seed Examples.In the following, we show one single-tool instruction seed example and one multi-tool instruction seed example.</p>
<p>For example, with tool ASCII Art, the given api names are 'figlet', 'list figlet styles', 'cowsay', 'list cowsay styles', 'matheq'.Some sample queries and related apis would be: "Query": "Need to create an ASCII art representation of a mathematical equation.The equation is 'y = mx + c', where m and c are constants.Help me generate the ASCII art for this equation.Also please generate an ASCII art representation of the text 'Newton's Second Law of Motion'.","related apis": ['figlet', 'list figlet styles', 'matheq'] "Query": "Working on a research paper on cows and need to include ASCII art representations of various cows.Can you first retrieve available ASCII art styles for cows?Then, can you generate ASCII art for cows like the Jersey, Holstein, and Guernsey?Finally, I want the cow to say 'Moo!' in the ASCII art.", "related apis": ['figlet', 'list figlet styles', 'cowsay', 'list cowsay styles'] "Query": "I'm writing a blog post on ASCII art and need to include some examples.Can you generate ASCII art for the following strings: 'ASCII', 'art', and 'gallery'?You can first retrieve available figlet styles and then generate ASCII art for the strings using the styles.","related apis": ['figlet', 'list figlet styles'] "Query": "Greetings!I'm putting together a quirky slideshow about our furry friends and need your help to sprinkle some ASCII art goodness.Could you kindly fetch me the catalog of ASCII art styles available for animals?Also, I'm particularly keen on featuring ASCII art for creatures like pandas, cows, elephants, and penguins.And if they could say something cute like 'Hello!' or 'Hugs!' in the ASCII art, that would be purr-fect!","related apis": ['figlet', 'list figlet styles', 'cowsay', 'list cowsay styles'] For example, with tool ['Entrepreneur Mindset Collection', 'Random Words', 'thedigitalnewsfeederapi', 'Chemical Elements'], the given api names are (tool 'Entrepreneur Mindset Collection')'Random Quote in JSON format', (tool 'Random Words')'Get multiple random words', (tool 'Random Words')'Get a random word', (tool 'thedigitalnewsfeederapi')'getting specific cricket articles', (tool 'thedigitalnewsfeederapi')'Getting Cricket Articles', (tool 'thedigitalnewsfeederapi')'getting specific news articles', (tool 'thedigitalnewsfeederapi')'Getting News Articles', (tool 'thedigitalnewsfeederapi')'getting all news articles', (tool 'Chemical Elements')'Get All Chemical Elements'.Some sample queries and related apis would be: "Query": "For my best friend's surprise birthday party, I require inspiration for party games and decorations.Kindly suggest some random words that can serve as themes for the party.Furthermore, I'm interested in gathering news articles about the latest party trends to ensure a modern celebration.</p>
<p>Figure 2 :
2
Figure 2: Pass rate (↑) and win rate (↑) of different methods in tool-use evaluation.For win rate, we compare each method with ChatGPT-ReACT.DFSDT is our improved reasoning strategy over ReACT.ToolLLaMA surpasses Text-Davinci-003, Claude-2, and almost performs on par with ChatGPT.</p>
<p>Figure 3 :
3
Figure 3: The hierarchy of RapidAPI (left) and the process of instruction generation (right).</p>
<p>Figure 4 :
4
Figure 4: A comparison of our DFSDT and conventional CoT or ReACT during model reasoning (left).We show part of the solution path annotation process using ChatGPT (right).</p>
<p>64.0 62.3 64.0 59.0 60.5 55.0 81.5 68.5 68.5 60.8 65.0 73.0 67.3 63.1</p>
<p>Table 1 :
1
A comparison of our ToolBench to notable instruction tuning dataset for tool learning.
Number of tools34513534008Number of APIs16464164553400232Number of Instances1264861700227439382746Number of Real API Calls469585056803926Avg. Reasoning Traces4.01.02.11.05.9</p>
<p>•</p>
<p>Table 4 :
4
Main experiments of ToolBench.Win rate is calculated by comparing each model with ChatGPT-ReACT.A win rate higher than 50% means the model performs better than ChatGPT-ReACT.Apart from ToolLLaMA-DFSDT-Retriever, all methods use the oracle API retriever (i.e., ground truth API).</p>
<p>Table 5 :
5
OOD generalization experiments on APIBench.For the Gorilla entries, ZS / RS means that Gorilla was trained in a zero-shot / retrieval-aware setting on APIBench.We report hallucination rate and AST accuracy.
PreprintMethodHuggingFace Hallu. (↓) AST (↑) Hallu. (↓) AST (↑) Hallu. (↓) AST (↑) TorchHub TensorHubToolLLaMA + Our Retriever10.6016.7715.7051.166.4840.59Gorilla-ZS + BM2546.9010.5117.2044.6220.5834.31Gorilla-RS + BM256.4215.715.9150.002.7741.90ToolLLaMA + Oracle8.6688.8014.1285.887.4488.62Gorilla-ZS + Oracle52.8844.3639.2559.1412.9983.21Gorilla-RS + Oracle6.9789.276.9993.012.0494.16</p>
<p>Preprint Yining Ye, Xin Cong, Yujia Qin, Yankai Lin, Zhiyuan Liu, and Maosong Sun.Large language model as autonomous decision maker.arXiv preprint arXiv:2308.12519,2023.Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang.Toolqa: A dataset for llm question answering with external tools.arXiv preprint arXiv:2306.13304,2023.
PreprintAPPENDIXA IMPLEMENTATION DETAILSA.1 DETAILS FOR FILTERING RAPIDAPI</p>
<p>Table 6 :
6
Win rate results before merging the tie label.Win rate is calculated by comparing each model with ChatGPT-ReACT.A win rate higher than 50% means the model performs better than ChatGPT-ReACT.Apart from ToolLLaMA-DFSDT-Retriever, all methods use the oracle API retriever (i.e., ground truth API).A.7 PROMPTS FOR INSTRUCTION GENERATIONBelow we list the detailed prompt for instruction generation, which consists of four parts: task description, in-context learning examples, sampled API list, and other requirements.Task Description of Single-tool Instructions: You will be provided with a tool, its description, all of the tool's available API functions, the descriptions of these API functions, and the parameters required for each API function.Your task involves creating 10 varied, innovative, and detailed user queries that employ multiple API functions of a tool.For instance, if the tool 'climate news' has three API calls -'get all climate change news', 'look up climate today', and 'historical climate', your query should articulate something akin to: first, determine today's weather, then verify how often it rains in Ohio in September, and finally, find news about climate change to help me understand whether the climate will change anytime soon.This query exemplifies how to utilize all API calls of 'climate news'.A query that only uses one API call will not be accepted.Additionally, you must incorporate the input parameters required for each API call.To achieve this, generate random information for required parameters such as IP address, location, coordinates, etc.For instance, don't merely say 'an address', provide the exact road and district names.Don't just mention 'a product', specify wearables, milk, a blue blanket, a pan, etc. Don't refer to 'my company', invent a company name instead.The first seven of the ten queries should be very specific.Each single query should combine all API call usages in different ways and include the necessary parameters.Note that you shouldn't ask 'which API to use', rather, simply state your needs that can be addressed by these APIs.You should also avoid asking for the input parameters required by the API call, but instead directly provide the parameter in your query.The final three queries should be complex and lengthy, describing a complicated scenario where all the API calls can be utilized to provide assistance within a single query.You should first think about possible related API combinations, then give your query.Related apis are apis that can be used for a give query; those related apis have to strictly come from the provided api names.For each query, there should be multiple related apis; for different queries, overlap of related apis should be as little as possible.Deliver your response in this format: [Query1: ......Preprint party to celebrate its birth.Get me some cat facts and NBA news to gather inspirations for the cat name.Also, find a proper hotel around my house in Houston Downtown for the party.'This query exemplifies how to utilize API calls of all the given tools.A query that uses API calls of only one tool will not be accepted.Additionally, you must incorporate the input parameters required for each API call.To achieve this, generate random information for required parameters such as IP address, location, coordinates, etc.For instance, don't merely say 'an address', provide the exact road and district names.Don't just mention 'a product', specify wearables, milk, a blue blanket, a pan, etc. Don't refer to 'my company', invent a company name instead.The first seven of the ten queries should be very specific.Each single query should combine API calls of different tools in various ways and include the necessary parameters.Note that you shouldn't ask 'which API to use', rather, simply state your needs that can be addressed by these APIs.You should also avoid asking for the input parameters required by the API call, but instead directly provide the parameters in your query.The final three queries should be complex and lengthy, describing a complicated scenario where all the provided API calls can be utilized to provide assistance within a single query.You should first think about possible related API combinations, then give your query.Related APIs are APIs that can be used for a given query; those related APIs have to strictly come from the provided API names.For each query, there should be multiple related APIs; for different queries, overlap of related APIs should be as little as possible.Deliver your response in this format: [Query1: ......, 'related apis':[[tool name, api name], [tool name, api name], [tool name, api name]...],Query2: ......, 'related apis':[[tool name, api name], [tool name, api name], [tool name, api name]...],Query3: ......, 'related apis':[[tool name, api name], [tool name, api name], [tool name, api name]...], ...]
, 'related apis':[api1,
task involves creating 10 varied, innovative, and detailed user queries that employ API functions of multiple tools.For instance, given three tools 'nba news', 'cat-facts', and 'hotels': 'nba news' has API functions 'Get individual NBA source news' and 'Get all NBA news', 'cat-facts' has API functions 'Get all facts about cats' and 'Get a random fact about cats', 'hotels' has API functions 'properties/get-details (Deprecated)', 'properties/list (Deprecated)' and 'locations/v3/search'.Your query should articulate something akin to: 'I want to name my newborn cat after Kobe and host a</p>
<p>PreprintAlso, I would appreciate details about the local hotels in my area for accommodation options.Your assistance is greatly appreciated.","related apis": [['Random Words', 'Get multiple random words'], ['thedigitalnewsfeederapi', 'Getting News Articles'], ['thedigitalnewsfeederapi', 'Getting all news articles']] "Query": "In the midst of organizing a team-building event for my esteemed company, I eagerly seek your valued input for invigorating activities.Might I kindly request a collection of random quotes that encapsulate the essence of teamwork and motivation?Additionally, I am keen on exploring news articles that showcase triumphant team-building events, as they serve as a wellspring of inspiration.","related apis":[['Entrepreneur Mindset Collection', 'Random Quote in JSON format'],['thedigitalnewsfeederapi', 'Getting News Articles']] "Query": "I need specific cricket articles that discuss the health benefits of sports for my research paper on exercise.I also want to know which chemical elements are associated with exercising, like increased iron (Fe) and its impact on bone marrow.","related apis": [['thedigitalnewsfeederapi', 'getting specific cricket articles'], ['Chemical Elements', 'Get All Chemical Elements']] "Query": "I'm starting a new business venture and I need to make a speech announcing the new dawn.Provide me some quotes and words for me to start with.I would like to gather news articles about successful entrepreneurs for inspiration.{"tool_description": "EntreAPI Faker is used to dynamically create mock, demo, test and sample data for your application", "name": "EntreAPI Faker", "api_list": [ { "name": "Longitute", "url": "https://entreapi-faker.p.rapidapi.com/address/longitude", "description": "Generate a random longitude.","method": "GET", "required_parameters": [], "optional_parameters": [ { "name": "max", "type": "NUMBER", "description": "Maximum value for latitude.","default": "" }, { "name": "min", "type": "NUMBER", "description": "Minimum value for latitude.","default": "" }, { "name": "precision", "type": "NUMBER", "description": "Precision for latitude.","default": "" } ], "tool_name": "EntreAPI Faker", "category_name": "Data" Preprint }, { "name": "Boolean", "url": "https://entreapi-faker.p.rapidapi.com/datatype/boolean", "description": "Randomly generate a boolean value.","method": "GET", "required_parameters": [], "optional_parameters": [], "tool_name": "EntreAPI Faker", "category_name": "Data" }, { "name": "Past", "url": "https://entreapi-faker.p.rapidapi.com/date/past", "description": "Randomly generate a date value in the past.","method": "GET", "required_parameters": [], "optional_parameters": [ { "name": "refDate", "type": "STRING", "description": "Starting reference date", "default": "" }, { "name": "years", "type": "NUMBER", "description": "Number of years for the range of dates.","default": "" } ], "tool_name": "EntreAPI Faker", "category_name": "Data" }, { "name": "Image Url", "url": "https://entreapi-faker.p.rapidapi.com/image/imageUrl", "description": "Randomly generate an image URL.", "method": "GET", "required_parameters": [], "optional_parameters": [ { "name": "width", "type": "NUMBER", "description": "Width of the image.Default is 640.","default": "" }, { "name": "height", "type": "NUMBER", "description": "Height of the image.Default is 480.","default": "" Preprint }, { "name": "useRandomize", "type": "BOOLEAN", "description": "Add a random number parameter to the returned URL.", "default": "" }, { "name": "category", "type": "STRING", "description": "The category for the image.Can be one: abstract, animal, avatar, business, cats, city, fashion, food, nature, nightlife, people, sports, technics, transport", "default": "" } ], "tool_name": "EntreAPI Faker", "category_name": "Data" }, { "name": "Sentence", "url": "https://entreapi-faker.p.rapidapi.com/lorem/sentence", "description": "Randomly generate a sentence of Lorem Ipsum.","method": "GET", "required_parameters": [], "optional_parameters": [ { "name": "wordCount", "type": "NUMBER", "description": "Number of words in the sentence.","default": "" } ], "tool_name": "EntreAPI Faker", "category_name": "Data" }, { "name": "Gender", "url": "https://entreapi-faker.p.rapidapi.com/name/gender", "description": "Randomly select a gender.","method": "GET", "required_parameters": [], "optional_parameters": [ { "name": "useBinary", "type": "BOOLEAN", "description": "Use binary genders only.","default": "" } ], "tool_name": "EntreAPI Faker", "category_name": "Data" Preprint }, { "name": "Prefix", "url": "https://entreapi-faker.p.rapidapi.com/name/prefix", "description": "Randomly generate a prefix (e.g., Mr., Mrs., etc.)", "method": "GET", "required_parameters": [], "optional_parameters": [ { "name": "gender", "type": "STRING", "description": "Optional gender.","default": "" } ], "tool_name": "EntreAPI Faker", "category_name": "Data" }, { "name": "Array Element", "url": "https://entreapi-faker.p.rapidapi.com/random/arrayElement", "description": "Randomly select an array element.","method": "GET", "required_parameters": [], "optional_parameters": [ { "name": "array", "type": "ARRAY", "description": "The list of elements to choose from.Default is [\"a\", \"b\", \"c\"].","default": "" } ], "tool_name": "EntreAPI Faker", "category_name": "Data" }, { "name": "Number Value", "url": "https://entreapi-faker.p.rapidapi.com/random/number", "description": "Randomly generate a number value.","method": "GET", "required_parameters": [], "optional_parameters": [ { "name": "min", "type": "NUMBER", "description": "Minimum value.","default": "" }, { "name": "max", "type": "NUMBER", "description": "Maximum value.","default": "" },Preprint{"name": "precision", "type": "NUMBER", "description": "Precision of the number.","default": "" } ], "tool_name": "EntreAPI Faker", "category_name": "Data" }, { "name": "URL", "url": "https://entreapi-faker.p.rapidapi.com/internet/url", "description": "Randomly generate a URL.", "method": "GET", "required_parameters": [], "optional_parameters": [], "tool_name": "EntreAPI Faker", "category_name": "Data" } ] } Other Requirements: Please produce ten queries in line with the given requirements and inputs.These ten queries should display a diverse range of sentence structures: some queries should be in the form of imperative sentences, others declarative, and yet others interrogative.Equally, they should encompass a variety of tones, with some being polite, others straightforward.Ensure they vary in length and contain a wide range of subjects: myself, my friends, family, and company.Aim to include a number of engaging queries as long as they relate to API calls.Keep in mind that for each query, invoking just one API won't suffice; each query should call upon two to five APIs.However, try to avoid explicitly specifying which API to employ in the query.Each query should consist of a minimum of thirty words.A.8 PROMPTS FOR SOLUTION PATH ANNOTATIONWe use the following prompt when searching for the solution path.When expanding the child nodes, we use diversity user prompt, showing the information of previous child nodes.------------------------------------------------------------------system_prompt:You are Tool-GPT, capable of utilizing numerous tools and functions to complete the given task.1.First, I will provide you with the task description, and your task will commence.----------------------------------------------------------------------------------------------------------------Finish_function_description: { "name": "Finish", "description": "If you believe that you have obtained a result that can answer the task, please call this function to provide the final answer.Alternatively, if you recognize that you are unable to proceed with the task in the current state, call this function to restart.Remember: you must ALWAYS call this function at the end of your attempt, and the only part that will be shown to the user is the final answer, so it should contain sufficient information.","parameters": { "type": "object", "properties": { "return_type": { "type": "string", "enum": ["give_answer","give_up_and_restart"], }, "final_answer": { "type": "string", "description": "The final answer you want to give the user.You should have this field if \" return_type\"==\"give_answer\"", } }, "required": ["return_type"], } }
Do as i can, not as i say: Grounding language in robotic affordances. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alex Hausman, Herzog, abs/2204.01691ArXiv preprint. 2022</p>
<p>Promptsource: An integrated development environment and repository for natural language prompts. Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Abheesht Nayak, Taewoon Sharma, Saiful Kim, Thibault Bari, Févry, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2022</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Extending context window of large language models via positional interpolation. Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian, arXiv:2306.155952023arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Enhancing chat language models by scaling high-quality instructional conversations. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, Bowen Zhou, arXiv:2305.142332023arXiv preprint</p>
<p>Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, Mike Zheng Shou, arXiv:2306.086402023arXiv preprint</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu, Toolkengpt, arXiv:2305.11554Augmenting frozen language models with massive tools via tool embeddings. 2023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Preprint, Pieter Huang, Deepak Abbeel, Igor Pathak, Mordatch, International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, Baltimore, Maryland, USAPMLR17-23 July 2022. 2022a162of Proceedings of Machine Learning Research</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, abs/2207.05608ArXiv preprint. 2022b</p>
<p>Cumulated gain-based evaluation of ir techniques. Kalervo Järvelin, Jaana Kekäläinen, ACM Transactions on Information Systems (TOIS). 2042002</p>
<p>Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu, ArXiv. 2023</p>
<p>Api-bank: A benchmark for tool-augmented llms. Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, arXiv:2304.082442023aarXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023b</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, abs/2112.09332ArXiv preprint. 2021</p>
<p>OpenAI: Introducing ChatGPT. OpenAI. Gpt-4 technical report. 2022. 2023OpenAI</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 2023arXiv preprint</p>
<p>Creator: Disentangling abstract and concrete reasonings of large language models through tool creation. Chi Cheng Qian, Yi R Han, Yujia Fung, Zhiyuan Qin, Heng Liu, Ji, arXiv:2305.143182023arXiv preprint</p>
<p>Webcpm: Interactive web search for chinese long-form question answering. Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, arXiv:2305.068492023aarXiv preprint</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, arXiv:2304.083542023barXiv preprint</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, abs/2302.047612023ArXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Preprint, Kaitao Shen, Xu Song, Dongsheng Tan, Weiming Li, Yueting Lu, Zhuang, 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, Sujian Li, Restgpt, arXiv:2306.06624Connecting large language models with real-world applications via restful apis. 2023arXiv preprint</p>
<p>Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Le Sun, Toolalpaca, arXiv:2306.05301Generalized tool learning for language models with 3000 simulated cases. 2023arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>Chatgpt for robotics: Design principles and model abilities. Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor, MSR-TR-2023-8February 2023MicrosoftTechnical Report</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, abs/2303.046712023ArXiv preprint</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, 2023a</p>
<p>Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang ; Linyao, Hongyang Yang, Zhao Chen, Xiao Li, Xindong Ding, Wu, arXiv:2305.16504arXiv:2306.114892023b. 2023arXiv preprintOn the tool manipulation capability of open-source large language models</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, abs/2210.03629ArXiv preprint. 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2027 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2027</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2027</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-277501774</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.01445v2.pdf" target="_blank">Compositional-ARC: Assessing Systematic Generalization in Abstract Spatial Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Systematic generalization refers to the capacity to understand and generate novel combinations from known components. Despite recent progress by large language models (LLMs) across various domains, these models often fail to extend their knowledge to novel compositional scenarios, revealing notable limitations in systematic generalization. There has been an ongoing debate about whether neural networks possess the capacity for systematic generalization, with recent studies suggesting that meta-learning approaches designed for compositionality can significantly enhance this ability. However, these insights have largely been confined to linguistic problems, leaving their applicability to other tasks an open question. In this study, we extend meta-learning for compositionality to the domain of abstract spatial reasoning. To this end, we introduce $\textit{Compositional-ARC}-$a dataset designed to evaluate the capacity of models to systematically generalize from known geometric transformations (e.g., translation, rotation) of abstract two-dimensional objects to novel combinations of these transformations (e.g., translation+rotation). Our results show that a small transformer-based encoder-decoder model, trained via meta-learning for compositionality, can systematically generalize to previously unseen transformation compositions. Notably, despite having only 5.7M parameters, this model significantly outperforms state-of-the-art LLMs$-$including o3-mini, GPT-4o, and Gemini 2.0 Flash, which fail to exhibit similar systematic behavior$-$and performs on par with the winning model of the ARC prize 2024, an 8B-parameter LLM trained via test-time training. Our findings highlight the effectiveness of meta-learning in promoting systematicity beyond linguistic tasks, suggesting a promising direction toward more robust and generalizable models.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2027.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2027.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Compositional-ARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compositional-ARC (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 10x10-grid benchmark introduced in this paper to evaluate systematic/compositional generalization in abstract spatial reasoning by composing five primitive geometric transformations (translation, rotation, reflection, extension, color-change) via three visual indicators into level-1 and level-2 compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>dataset / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Episode-based few-shot visual reasoning benchmark: each episode defines a visual interpretation grammar mapping indicators (shape, color, neighbor) to transformations; episodes contain study examples (primitive and level-1 compositions) and queries (level-2 compositions for systematicity evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>not applicable (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compositional-ARC</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict the output 10x10 grid given a set of study input-output pairs (few-shot) that define mappings from visual indicators to geometric transformations; compositionality tested by requiring generalization to unseen level-2 compositions (triples of indicators) from primitives and level-1 compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>level-1 (pairs of indicators) and level-2 (triples of indicators); level-2 is the target OOD compositional depth</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel combinations of seen primitives (composition of geometric transformations across indicators)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>out-of-distribution: training and evaluation sets contain different level-2 (triplet) compositions; only constituents (primitives and some level-1 combos) appear in training</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>designed for few-shot/meta-learning episodes (MLC); alternate static seq2seq baseline also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>No inoculation of the held-out level-2 compositions into training; training uses episodes sampled from many randomly generated visual grammars (100k episodes) so models must infer grammar per episode.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Paper compares episodic/meta-learning (MLC) training on Compositional-ARC to (a) static seq2seq training on a fixed visual grammar, (b) ablated MLC variants, and (c) multiple LLMs (general-purpose and domain-specific).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dataset enables explicit testing of systematic generalization to unseen compositions; shows that standard static seq2seq fails (see baseline) while meta-learning enables generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Requires exposure to many episodes (dynamic grammars) rather than static dataset; presence of level-1 compositions and auxiliary copy task in study examples strongly aid generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2027.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2027.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLC (ours)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer encoder-decoder trained via Meta-Learning for Compositionality (MLC) — this paper's model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small transformer encoder-decoder (3 encoder + 3 decoder layers, 8 heads, embedding 128, FFN 768; ~5.7M parameters) trained with the MLC episodic procedure on Compositional-ARC; aims to infer per-episode visual grammars and compose transformations systematically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer encoder-decoder (MLC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard Transformer encoder-decoder architecture (patch-based 2x2 patch tokenization of 10x10 grids), learnable 1D episode-order positional embeddings and decomposed 2D patch embeddings; trained episodically via meta-learning on dynamically changing visual grammars with an auxiliary copy task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>5.7M</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer encoder-decoder; attention mechanisms; 2D patch embeddings + 1D episode positional embeddings; auxiliary copy task as meta-learning objective</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compositional-ARC (3-Shot and Systematicity setups)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>3-Shot: model receives 3 example input-output grid pairs that directly illustrate the final composition; Systematicity: model receives 12 study examples (primitives + level-1 compositions) and must produce outputs for unseen level-2 compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>evaluated on level-2 compositions (triples of indicators); trained on primitives and level-1 (pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel action compositions (compositions of geometric transformations not seen together during training)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD compositional split: level-2 (triplet) compositions in test differ from training; episodes sample unique visual grammars</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>meta-learning over episodes (MLC) with auxiliary copy task; dataset of 100k episodes, episodes split into train/val/test (82,908 / 8,546 / 8,546 in main split); loss is cross-entropy over patch embeddings, background patches down-weighted</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>No direct exposure to held-out level-2 compositions; trained on many other grammars and compositions but withheld target combinations for OOD testing</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>3-Shot: exact match 99.92% (single reported split); averaged across four splits: exact match 98.78% ± 1.99; color accuracy 100.00%; shape accuracy 98.79% ± 1.98</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Systematicity (unseen level-2 compositions): single-split exact match 78.26%, color accuracy 97.88%, shape accuracy 80.49%; averaged across four splits: exact match 86.73% ± 6.03, color 99.36% ± 0.70, shape 87.55% ± 5.45</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Single-split gap (3-Shot → Systematicity): 99.92% → 78.26% = 21.66 pp decrease; averaged gap across splits: 98.78% → 86.73% = 12.05 pp decrease</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Reported aggregated for level-2 compositions (no finer per-depth breakdown beyond level-1 vs level-2 provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to: (i) standard static seq2seq trained on fixed grammar (fits train >99% but test OOD = 0.0%), (ii) general-purpose LLMs prompting, (iii) domain-specific LLMs (fine-tuned and with test-time training), and (iv) several ablations of MLC (no copy task, no primitives, no level-1 compositions).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Paper contrasts MLC-trained small transformer with large pre-trained LLMs (GPT-4o, Gemini, o3-mini) and domain-specific fine-tuned LLMs; MLC outperforms general LLMs and matches or beats domain-specific models on many metrics despite being much smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Key finding: a small 5.7M parameter MLC model significantly outperforms general-purpose LLMs and performs on par with an 8B-parameter domain-specific LLM trained with test-time training, suggesting that episodic/meta-learning training confers strong compositional generalization independent of model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Meta-learning over dynamically varying visual grammars plus an auxiliary copy task yields strong systematic generalization to unseen compositions: near-perfect performance on the 3-shot task and substantially higher systematicity performance than general LLM prompting. Ablations show the auxiliary copy task and level-1 examples materially improve systematic generalization (e.g., removing copy task drops mean systematicity exact match from 86.73% ±6.03 to 69.05% ±9.23; removing level-1 compositions drops to 21.01% ±19.07).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Remaining errors on systematicity primarily reduce exact-match by misplacing object location while often preserving color and shape; the model still shows a non-negligible gap between direct few-shot guidance and fully decomposed systematic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Success when trained episodically on many distinct grammars (MLC), given auxiliary copy task, and when study examples include level-1 compositions; fails when trained statically or when intermediate compositions are withheld.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2027.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2027.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>basic-seq2seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static seq2seq baseline (fixed visual grammar)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer encoder-decoder trained in the conventional supervised way on a dataset governed by a static visual grammar (no episodic/meta-learning), used to test whether standard training supports systematic generalization to unseen compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer seq2seq (static training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard transformer encoder-decoder trained on individual input-output grid pairs under a fixed mapping from indicators to transformations across the dataset (no episodes), using same patch encoding as MLC.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer encoder-decoder</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compositional-ARC (static dataset variant)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict outputs for level-2 compositions when trained on fixed grammar pairs (training includes primitives and compositions but mapping is static rather than episode-specific).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>level-2 compositions present in test are not observed during training (OOD evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel combinations of seen primitives (OOD)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD: test level-2 compositions withheld during training</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised learning on static dataset of 1,300 grid pairs</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Fits training data: >99% accuracy on training</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Test (OOD level-2 compositions): 0.0% exact match accuracy (fails to generalize to unseen compositions despite perfect training fit)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Training >99% → Test 0% = ~100 pp collapse</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Demonstrates that conventional static training (no episode/meta-learning) does not produce systematic generalization on Compositional-ARC.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows that standard supervised learning on a fixed grammar memorizes training mappings but fails completely (0%) on OOD compositions; motivates meta-learning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Complete failure on held-out level-2 compositions despite memorizing training set; indicates the necessity of episodic/meta-learning to enable composition.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Not applicable — baseline fails on compositional generalization in this setup.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2027.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2027.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art general-purpose multimodal large language model evaluated via textual prompts (and multimodal prompts for some experiments) on Compositional-ARC; performs poorly on systematic compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained general-purpose LLM with multimodal capabilities; evaluated via API prompting on grid-text representations and with optional image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>large transformer-based pretrained LM with multimodal capabilities (provider model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning (prompted via text or text+image)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compositional-ARC (prompting evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt GPT-4o with study examples (text arrays and optional images) and ask it to output predicted 10x10 output grid for query.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>evaluated on the same level-2 unseen compositions as other models</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel combinations of seen primitives via prompting</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD: held-out level-2 combinations in test episodes</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>zero/few-shot prompting (no fine-tuning on Compositional-ARC)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>3-Shot: exact match 22.28% (text-only prompt) ; color and shape accuracy reported in table (color ≈ 99.67%, shape ≈ 57.02) for 3-Shot</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Systematicity (unseen level-2): exact match ≈ 0.99% (very poor); color accuracy remains high while shape/location performance collapses</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Drops from ~22.3% (3-Shot) to ~1.0% (Systematicity) ≈ 21.3 pp</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared directly against MLC and domain-specific LLMs; GPT-4o fails to show systematicity while MLC generalizes well.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Despite being large, GPT-4o does not generalize compositionally in this task when only prompted.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>General-purpose LLMs like GPT-4o, when prompted (text or text+image), largely fail the systematicity evaluation despite decent 3-shot performance; they often predict correct colors/shapes but misplace objects (location errors).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Fails primarily on composing transformations to the correct final positions; multimodal (text+image) prompting sometimes reduces valid response rates and harms performance, suggesting modality-alignment issues.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Per-paper, prompting alone (without test-time training or domain-specific fine-tuning) is insufficient for systematic compositional generalization on Compositional-ARC.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2027.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2027.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.0 Flash (DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose LLM evaluated via prompting (text-only and multimodal) on Compositional-ARC; shows limited compositional generalization, especially on systematicity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.0 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Provider-specified multimodal LLM evaluated by textual and text+image prompts; default model parameters used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>large pretrained transformer with multimodal interface (provider model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning (prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compositional-ARC (prompting evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same prompting setup as other LLMs: represent grids as arrays and (optionally) provide images; ask for predicted 10x10 grid.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>evaluated on level-2 OOD compositions</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel combinations of seen primitives via prompting</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>zero/few-shot prompting (no fine-tuning on Compositional-ARC)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>3-Shot: exact match ≈ 30.08% (text-only); color accuracy ≈ 99.92%; shape ≈ 52.34% (table values)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Systematicity: exact match ≈ 2.66% (very poor); adding visual image sometimes reduced valid responses for Gemini (+image: valid rate drops to 94.09% in systematic task).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Approx drop from ~30% (3-Shot) to ~2.7% (Systematicity) ≈ 27.3 pp</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to MLC and domain-specific LLMs; Gemini fails systematic generalization while MLC/general domain-specific models succeed when specialized/fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Large pretrained model size does not guarantee compositional generalization under prompting-only evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gemini 2.0 Flash, like other general LLMs, can often recover colors/shapes but fails to correctly place transformed objects for novel compositions; multimodal prompting can hurt validity of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Most errors involve object placement (spatial reasoning) rather than color/shape prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>No success in compositional OOD settings when only prompted.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2027.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2027.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini (low)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini (OpenAI) - low reasoning effort configuration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller/fast variant of OpenAI's o3 family evaluated in a low-reasoning-effort configuration; does reasonably well on 3-shot but fails on systematic compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini (low)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A general-purpose LLM; evaluated with reduced 'reasoning effort' mode due to cost constraints (provider setting).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>large pretrained transformer (provider)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning (prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compositional-ARC (prompting evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt-based evaluation (text-only or multimodal) on 3-Shot and Systematicity setups.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>level-2 OOD compositions</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel combinations via prompting</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>zero/few-shot prompting (no fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>3-Shot: exact match ≈ 64.04% (text-only) — best among general-purpose LLMs in 3-Shot setup reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Systematicity: exact match ≈ 0.53% (very poor)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Drops from ~64.0% (3-Shot) to ~0.53% (Systematicity) ≈ 63.5 pp</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared alongside GPT-4o and Gemini; performs better on direct 3-shot guidance but fails on decomposed systematic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Despite being relatively capable in 3-shot, lacks the episodic/meta-learning training needed for systematic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Indicates that prompting strength in 3-shot conditions does not imply compositional generalization to unseen decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Severe collapse in systematicity setting; suggests reliance on direct examples of final composition rather than recomposition from primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Only performs well when the exact final composition is shown in the few-shot examples (3-Shot).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2027.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2027.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-ReARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.2-3B-ReARC (domain-specific LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B-parameter model fine-tuned on ARC-style data (re-ARC) and evaluated with ARC-specific inference augmentations; performs well on 3-Shot and can be further improved via test-time training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.2-3B-ReARC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3.2-3B fine-tuned on re-ARC with an ARC-customized tokenizer, data augmentations, candidate search, and optional test-time training (LoRA adaptation) used by Franzen et al. (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>pretrained transformer fine-tuned with ARC-specific augmentations; uses depth-first candidate generation and candidate-selection; supports LoRA test-time adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning (ARC-style)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compositional-ARC (domain-specific fine-tuned evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated on Compositional-ARC using ARC-tuned inference pipeline; optionally uses test-time training (one-epoch LoRA on study examples with augmentation) before inference.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>level-2 OOD compositions</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel compositions of geometric primitives</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>domain-specific fine-tuning on ARC-style data (re-ARC) plus optional test-time training (LoRA) on study examples</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Test-time training exposes the model to the study examples of the test episode via LoRA adaptation (authors' protocol) before inference in some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>3-Shot: exact match 85.85% (reported in paper for Compositional-ARC 3-Shot)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>When using test-time training, domain-specific models are reported to improve substantially (see Mistral TTT numbers). Exact systematicity numbers for Llama+TTT are reported in paper tables but are less clearly printed; paper reports substantial improvement with TTT.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Smaller than general LLMs when using domain-specific fine-tuning and test-time training; exact gap depends on TTT usage (paper shows domain-specific+TTT approaches MLC performance on systematicity in some runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against MLC and general LLMs; shows domain-specific fine-tuning and TTT close the gap but require large model and specialized pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>3B model with domain-specific fine-tuning gives strong 3-shot performance; requires data augmentation and TTT to approach systematicity performance of MLC.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain-specific LLMs fine-tuned on ARC-style data perform well on 3-shot tasks; test-time training (LoRA) substantially improves performance on systematicity, but this requires specialized pipeline and more compute.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeed when equipped with ARC-specific training, heavy augmentation, and test-time adaptation; otherwise performance on systematicity is limited.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2027.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2027.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-NeMO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-NeMO-Minitron-8B-Full (domain-specific LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter domain-specific LLM trained on broad ARC-style data and used with an ARC-customized pipeline and test-time training; achieves top domain-specific performance and approaches MLC on systematicity when using test-time training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-NeMO-Minitron-8B-Full</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>8B-parameter transformer-based LLM trained on multiple ARC-style datasets and used with extensive inference-time augmentations and optional LoRA test-time training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>large pretrained transformer fine-tuned on ARC-style corpora; uses augmented inference (rotations, color permutations), candidate pruning and optional LoRA test-time training</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning (ARC-style)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compositional-ARC (domain-specific fine-tuned evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated on Compositional-ARC with ARC-style augmentations and optional one-epoch LoRA test-time training on study examples.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>level-2 OOD compositions</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel compositions of geometric primitives</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>pretraining + domain-specific fine-tuning on ARC-style data and optional test-time LoRA adaptation on study examples</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Test-time training variant performs one-epoch LoRA adaptation on study examples (repeated with augmentations) before inference when enabled.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>3-Shot: exact match 95.71% (reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Systematicity with test-time training: exact match 78.20%, color accuracy 100.00%, shape accuracy 88.26% (reported in paper table for TTT condition)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>3-Shot 95.71% → Systematicity+TTT 78.20% ≈ 17.51 pp gap (note: TTT used to boost systematicity performance)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared versus MLC and general LLMs; Mistral+TTT approaches MLC's systematicity performance in single-split numbers but requires much larger model and specialized TTT pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Large 8B model with domain fine-tuning and TTT attains high performance, but the MLC 5.7M model matches or exceeds general LLMs and rivals domain-specific 8B+TTT performance in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates that domain-specific fine-tuning plus test-time training can yield strong systematicity, but at cost of scale, specialized tokenizers/augmentation, and TTT; MLC offers a small-model alternative with strong generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeed when using ARC-specific training, heavy augmentation, and test-time LoRA adaptation on each episode's study examples.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human-like systematic generalization through a metalearning neural network <em>(Rating: 2)</em></li>
                <li>Abstraction and Reasoning Corpus <em>(Rating: 2)</em></li>
                <li>The llm architect: Solving the arc challenge is a matter of perspective <em>(Rating: 2)</em></li>
                <li>COGS: A compositional generalization challenge based on semantic interpretation <em>(Rating: 1)</em></li>
                <li>Compositional generalization through meta sequence-to-sequence learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2027",
    "paper_id": "paper-277501774",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "Compositional-ARC",
            "name_full": "Compositional-ARC (dataset)",
            "brief_description": "A 10x10-grid benchmark introduced in this paper to evaluate systematic/compositional generalization in abstract spatial reasoning by composing five primitive geometric transformations (translation, rotation, reflection, extension, color-change) via three visual indicators into level-1 and level-2 compositions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "dataset / benchmark",
            "model_description": "Episode-based few-shot visual reasoning benchmark: each episode defines a visual interpretation grammar mapping indicators (shape, color, neighbor) to transformations; episodes contain study examples (primitive and level-1 compositions) and queries (level-2 compositions for systematicity evaluation).",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": "not applicable (dataset)",
            "task_domain": "visual reasoning",
            "task_name": "Compositional-ARC",
            "task_description": "Predict the output 10x10 grid given a set of study input-output pairs (few-shot) that define mappings from visual indicators to geometric transformations; compositionality tested by requiring generalization to unseen level-2 compositions (triples of indicators) from primitives and level-1 compositions.",
            "compositional_depth": "level-1 (pairs of indicators) and level-2 (triples of indicators); level-2 is the target OOD compositional depth",
            "composition_type": "novel combinations of seen primitives (composition of geometric transformations across indicators)",
            "split_type": "out-of-distribution: training and evaluation sets contain different level-2 (triplet) compositions; only constituents (primitives and some level-1 combos) appear in training",
            "training_strategy": "designed for few-shot/meta-learning episodes (MLC); alternate static seq2seq baseline also evaluated",
            "curriculum_details": null,
            "inoculation_details": "No inoculation of the held-out level-2 compositions into training; training uses episodes sampled from many randomly generated visual grammars (100k episodes) so models must infer grammar per episode.",
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Paper compares episodic/meta-learning (MLC) training on Compositional-ARC to (a) static seq2seq training on a fixed visual grammar, (b) ablated MLC variants, and (c) multiple LLMs (general-purpose and domain-specific).",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Dataset enables explicit testing of systematic generalization to unseen compositions; shows that standard static seq2seq fails (see baseline) while meta-learning enables generalization.",
            "failure_analysis": null,
            "success_conditions": "Requires exposure to many episodes (dynamic grammars) rather than static dataset; presence of level-1 compositions and auxiliary copy task in study examples strongly aid generalization.",
            "uuid": "e2027.0"
        },
        {
            "name_short": "MLC (ours)",
            "name_full": "Transformer encoder-decoder trained via Meta-Learning for Compositionality (MLC) — this paper's model",
            "brief_description": "A small transformer encoder-decoder (3 encoder + 3 decoder layers, 8 heads, embedding 128, FFN 768; ~5.7M parameters) trained with the MLC episodic procedure on Compositional-ARC; aims to infer per-episode visual grammars and compose transformations systematically.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer encoder-decoder (MLC)",
            "model_description": "Standard Transformer encoder-decoder architecture (patch-based 2x2 patch tokenization of 10x10 grids), learnable 1D episode-order positional embeddings and decomposed 2D patch embeddings; trained episodically via meta-learning on dynamically changing visual grammars with an auxiliary copy task.",
            "model_size": "5.7M",
            "is_pretrained": false,
            "architectural_features": "standard transformer encoder-decoder; attention mechanisms; 2D patch embeddings + 1D episode positional embeddings; auxiliary copy task as meta-learning objective",
            "task_domain": "visual reasoning",
            "task_name": "Compositional-ARC (3-Shot and Systematicity setups)",
            "task_description": "3-Shot: model receives 3 example input-output grid pairs that directly illustrate the final composition; Systematicity: model receives 12 study examples (primitives + level-1 compositions) and must produce outputs for unseen level-2 compositions.",
            "compositional_depth": "evaluated on level-2 compositions (triples of indicators); trained on primitives and level-1 (pairs)",
            "composition_type": "novel action compositions (compositions of geometric transformations not seen together during training)",
            "split_type": "OOD compositional split: level-2 (triplet) compositions in test differ from training; episodes sample unique visual grammars",
            "training_strategy": "meta-learning over episodes (MLC) with auxiliary copy task; dataset of 100k episodes, episodes split into train/val/test (82,908 / 8,546 / 8,546 in main split); loss is cross-entropy over patch embeddings, background patches down-weighted",
            "curriculum_details": null,
            "inoculation_details": "No direct exposure to held-out level-2 compositions; trained on many other grammars and compositions but withheld target combinations for OOD testing",
            "iid_performance": "3-Shot: exact match 99.92% (single reported split); averaged across four splits: exact match 98.78% ± 1.99; color accuracy 100.00%; shape accuracy 98.79% ± 1.98",
            "compositional_performance": "Systematicity (unseen level-2 compositions): single-split exact match 78.26%, color accuracy 97.88%, shape accuracy 80.49%; averaged across four splits: exact match 86.73% ± 6.03, color 99.36% ± 0.70, shape 87.55% ± 5.45",
            "generalization_gap": "Single-split gap (3-Shot → Systematicity): 99.92% → 78.26% = 21.66 pp decrease; averaged gap across splits: 98.78% → 86.73% = 12.05 pp decrease",
            "performance_by_depth": "Reported aggregated for level-2 compositions (no finer per-depth breakdown beyond level-1 vs level-2 provided).",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to: (i) standard static seq2seq trained on fixed grammar (fits train &gt;99% but test OOD = 0.0%), (ii) general-purpose LLMs prompting, (iii) domain-specific LLMs (fine-tuned and with test-time training), and (iv) several ablations of MLC (no copy task, no primitives, no level-1 compositions).",
            "architectural_comparison": "Paper contrasts MLC-trained small transformer with large pre-trained LLMs (GPT-4o, Gemini, o3-mini) and domain-specific fine-tuned LLMs; MLC outperforms general LLMs and matches or beats domain-specific models on many metrics despite being much smaller.",
            "scale_effects": "Key finding: a small 5.7M parameter MLC model significantly outperforms general-purpose LLMs and performs on par with an 8B-parameter domain-specific LLM trained with test-time training, suggesting that episodic/meta-learning training confers strong compositional generalization independent of model scale.",
            "transfer_results": null,
            "key_findings": "Meta-learning over dynamically varying visual grammars plus an auxiliary copy task yields strong systematic generalization to unseen compositions: near-perfect performance on the 3-shot task and substantially higher systematicity performance than general LLM prompting. Ablations show the auxiliary copy task and level-1 examples materially improve systematic generalization (e.g., removing copy task drops mean systematicity exact match from 86.73% ±6.03 to 69.05% ±9.23; removing level-1 compositions drops to 21.01% ±19.07).",
            "failure_analysis": "Remaining errors on systematicity primarily reduce exact-match by misplacing object location while often preserving color and shape; the model still shows a non-negligible gap between direct few-shot guidance and fully decomposed systematic inference.",
            "success_conditions": "Success when trained episodically on many distinct grammars (MLC), given auxiliary copy task, and when study examples include level-1 compositions; fails when trained statically or when intermediate compositions are withheld.",
            "uuid": "e2027.1"
        },
        {
            "name_short": "basic-seq2seq",
            "name_full": "Static seq2seq baseline (fixed visual grammar)",
            "brief_description": "A transformer encoder-decoder trained in the conventional supervised way on a dataset governed by a static visual grammar (no episodic/meta-learning), used to test whether standard training supports systematic generalization to unseen compositions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer seq2seq (static training)",
            "model_description": "Standard transformer encoder-decoder trained on individual input-output grid pairs under a fixed mapping from indicators to transformations across the dataset (no episodes), using same patch encoding as MLC.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard transformer encoder-decoder",
            "task_domain": "visual reasoning",
            "task_name": "Compositional-ARC (static dataset variant)",
            "task_description": "Predict outputs for level-2 compositions when trained on fixed grammar pairs (training includes primitives and compositions but mapping is static rather than episode-specific).",
            "compositional_depth": "level-2 compositions present in test are not observed during training (OOD evaluation)",
            "composition_type": "novel combinations of seen primitives (OOD)",
            "split_type": "OOD: test level-2 compositions withheld during training",
            "training_strategy": "standard supervised learning on static dataset of 1,300 grid pairs",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Fits training data: &gt;99% accuracy on training",
            "compositional_performance": "Test (OOD level-2 compositions): 0.0% exact match accuracy (fails to generalize to unseen compositions despite perfect training fit)",
            "generalization_gap": "Training &gt;99% → Test 0% = ~100 pp collapse",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Demonstrates that conventional static training (no episode/meta-learning) does not produce systematic generalization on Compositional-ARC.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Shows that standard supervised learning on a fixed grammar memorizes training mappings but fails completely (0%) on OOD compositions; motivates meta-learning approach.",
            "failure_analysis": "Complete failure on held-out level-2 compositions despite memorizing training set; indicates the necessity of episodic/meta-learning to enable composition.",
            "success_conditions": "Not applicable — baseline fails on compositional generalization in this setup.",
            "uuid": "e2027.2"
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A state-of-the-art general-purpose multimodal large language model evaluated via textual prompts (and multimodal prompts for some experiments) on Compositional-ARC; performs poorly on systematic compositional generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Large pre-trained general-purpose LLM with multimodal capabilities; evaluated via API prompting on grid-text representations and with optional image inputs.",
            "model_size": null,
            "is_pretrained": true,
            "architectural_features": "large transformer-based pretrained LM with multimodal capabilities (provider model)",
            "task_domain": "visual reasoning (prompted via text or text+image)",
            "task_name": "Compositional-ARC (prompting evaluation)",
            "task_description": "Prompt GPT-4o with study examples (text arrays and optional images) and ask it to output predicted 10x10 output grid for query.",
            "compositional_depth": "evaluated on the same level-2 unseen compositions as other models",
            "composition_type": "novel combinations of seen primitives via prompting",
            "split_type": "OOD: held-out level-2 combinations in test episodes",
            "training_strategy": "zero/few-shot prompting (no fine-tuning on Compositional-ARC)",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "3-Shot: exact match 22.28% (text-only prompt) ; color and shape accuracy reported in table (color ≈ 99.67%, shape ≈ 57.02) for 3-Shot",
            "compositional_performance": "Systematicity (unseen level-2): exact match ≈ 0.99% (very poor); color accuracy remains high while shape/location performance collapses",
            "generalization_gap": "Drops from ~22.3% (3-Shot) to ~1.0% (Systematicity) ≈ 21.3 pp",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared directly against MLC and domain-specific LLMs; GPT-4o fails to show systematicity while MLC generalizes well.",
            "architectural_comparison": null,
            "scale_effects": "Despite being large, GPT-4o does not generalize compositionally in this task when only prompted.",
            "transfer_results": null,
            "key_findings": "General-purpose LLMs like GPT-4o, when prompted (text or text+image), largely fail the systematicity evaluation despite decent 3-shot performance; they often predict correct colors/shapes but misplace objects (location errors).",
            "failure_analysis": "Fails primarily on composing transformations to the correct final positions; multimodal (text+image) prompting sometimes reduces valid response rates and harms performance, suggesting modality-alignment issues.",
            "success_conditions": "Per-paper, prompting alone (without test-time training or domain-specific fine-tuning) is insufficient for systematic compositional generalization on Compositional-ARC.",
            "uuid": "e2027.3"
        },
        {
            "name_short": "Gemini-Flash",
            "name_full": "Gemini 2.0 Flash (DeepMind)",
            "brief_description": "A general-purpose LLM evaluated via prompting (text-only and multimodal) on Compositional-ARC; shows limited compositional generalization, especially on systematicity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini 2.0 Flash",
            "model_description": "Provider-specified multimodal LLM evaluated by textual and text+image prompts; default model parameters used.",
            "model_size": null,
            "is_pretrained": true,
            "architectural_features": "large pretrained transformer with multimodal interface (provider model)",
            "task_domain": "visual reasoning (prompting)",
            "task_name": "Compositional-ARC (prompting evaluation)",
            "task_description": "Same prompting setup as other LLMs: represent grids as arrays and (optionally) provide images; ask for predicted 10x10 grid.",
            "compositional_depth": "evaluated on level-2 OOD compositions",
            "composition_type": "novel combinations of seen primitives via prompting",
            "split_type": "OOD",
            "training_strategy": "zero/few-shot prompting (no fine-tuning on Compositional-ARC)",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "3-Shot: exact match ≈ 30.08% (text-only); color accuracy ≈ 99.92%; shape ≈ 52.34% (table values)",
            "compositional_performance": "Systematicity: exact match ≈ 2.66% (very poor); adding visual image sometimes reduced valid responses for Gemini (+image: valid rate drops to 94.09% in systematic task).",
            "generalization_gap": "Approx drop from ~30% (3-Shot) to ~2.7% (Systematicity) ≈ 27.3 pp",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to MLC and domain-specific LLMs; Gemini fails systematic generalization while MLC/general domain-specific models succeed when specialized/fine-tuned.",
            "architectural_comparison": null,
            "scale_effects": "Large pretrained model size does not guarantee compositional generalization under prompting-only evaluation.",
            "transfer_results": null,
            "key_findings": "Gemini 2.0 Flash, like other general LLMs, can often recover colors/shapes but fails to correctly place transformed objects for novel compositions; multimodal prompting can hurt validity of outputs.",
            "failure_analysis": "Most errors involve object placement (spatial reasoning) rather than color/shape prediction.",
            "success_conditions": "No success in compositional OOD settings when only prompted.",
            "uuid": "e2027.4"
        },
        {
            "name_short": "o3-mini (low)",
            "name_full": "o3-mini (OpenAI) - low reasoning effort configuration",
            "brief_description": "A smaller/fast variant of OpenAI's o3 family evaluated in a low-reasoning-effort configuration; does reasonably well on 3-shot but fails on systematic compositional generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o3-mini (low)",
            "model_description": "A general-purpose LLM; evaluated with reduced 'reasoning effort' mode due to cost constraints (provider setting).",
            "model_size": null,
            "is_pretrained": true,
            "architectural_features": "large pretrained transformer (provider)",
            "task_domain": "visual reasoning (prompting)",
            "task_name": "Compositional-ARC (prompting evaluation)",
            "task_description": "Prompt-based evaluation (text-only or multimodal) on 3-Shot and Systematicity setups.",
            "compositional_depth": "level-2 OOD compositions",
            "composition_type": "novel combinations via prompting",
            "split_type": "OOD",
            "training_strategy": "zero/few-shot prompting (no fine-tuning)",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "3-Shot: exact match ≈ 64.04% (text-only) — best among general-purpose LLMs in 3-Shot setup reported in paper",
            "compositional_performance": "Systematicity: exact match ≈ 0.53% (very poor)",
            "generalization_gap": "Drops from ~64.0% (3-Shot) to ~0.53% (Systematicity) ≈ 63.5 pp",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared alongside GPT-4o and Gemini; performs better on direct 3-shot guidance but fails on decomposed systematic inference.",
            "architectural_comparison": null,
            "scale_effects": "Despite being relatively capable in 3-shot, lacks the episodic/meta-learning training needed for systematic generalization.",
            "transfer_results": null,
            "key_findings": "Indicates that prompting strength in 3-shot conditions does not imply compositional generalization to unseen decompositions.",
            "failure_analysis": "Severe collapse in systematicity setting; suggests reliance on direct examples of final composition rather than recomposition from primitives.",
            "success_conditions": "Only performs well when the exact final composition is shown in the few-shot examples (3-Shot).",
            "uuid": "e2027.5"
        },
        {
            "name_short": "Llama-ReARC",
            "name_full": "Llama-3.2-3B-ReARC (domain-specific LLM)",
            "brief_description": "A 3B-parameter model fine-tuned on ARC-style data (re-ARC) and evaluated with ARC-specific inference augmentations; performs well on 3-Shot and can be further improved via test-time training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.2-3B-ReARC",
            "model_description": "Llama-3.2-3B fine-tuned on re-ARC with an ARC-customized tokenizer, data augmentations, candidate search, and optional test-time training (LoRA adaptation) used by Franzen et al. (2024).",
            "model_size": "3B",
            "is_pretrained": true,
            "architectural_features": "pretrained transformer fine-tuned with ARC-specific augmentations; uses depth-first candidate generation and candidate-selection; supports LoRA test-time adaptation",
            "task_domain": "visual reasoning (ARC-style)",
            "task_name": "Compositional-ARC (domain-specific fine-tuned evaluation)",
            "task_description": "Evaluated on Compositional-ARC using ARC-tuned inference pipeline; optionally uses test-time training (one-epoch LoRA on study examples with augmentation) before inference.",
            "compositional_depth": "level-2 OOD compositions",
            "composition_type": "novel compositions of geometric primitives",
            "split_type": "OOD",
            "training_strategy": "domain-specific fine-tuning on ARC-style data (re-ARC) plus optional test-time training (LoRA) on study examples",
            "curriculum_details": null,
            "inoculation_details": "Test-time training exposes the model to the study examples of the test episode via LoRA adaptation (authors' protocol) before inference in some conditions.",
            "iid_performance": "3-Shot: exact match 85.85% (reported in paper for Compositional-ARC 3-Shot)",
            "compositional_performance": "When using test-time training, domain-specific models are reported to improve substantially (see Mistral TTT numbers). Exact systematicity numbers for Llama+TTT are reported in paper tables but are less clearly printed; paper reports substantial improvement with TTT.",
            "generalization_gap": "Smaller than general LLMs when using domain-specific fine-tuning and test-time training; exact gap depends on TTT usage (paper shows domain-specific+TTT approaches MLC performance on systematicity in some runs).",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against MLC and general LLMs; shows domain-specific fine-tuning and TTT close the gap but require large model and specialized pipeline.",
            "architectural_comparison": null,
            "scale_effects": "3B model with domain-specific fine-tuning gives strong 3-shot performance; requires data augmentation and TTT to approach systematicity performance of MLC.",
            "transfer_results": null,
            "key_findings": "Domain-specific LLMs fine-tuned on ARC-style data perform well on 3-shot tasks; test-time training (LoRA) substantially improves performance on systematicity, but this requires specialized pipeline and more compute.",
            "failure_analysis": null,
            "success_conditions": "Succeed when equipped with ARC-specific training, heavy augmentation, and test-time adaptation; otherwise performance on systematicity is limited.",
            "uuid": "e2027.6"
        },
        {
            "name_short": "Mistral-NeMO",
            "name_full": "Mistral-NeMO-Minitron-8B-Full (domain-specific LLM)",
            "brief_description": "An 8B-parameter domain-specific LLM trained on broad ARC-style data and used with an ARC-customized pipeline and test-time training; achieves top domain-specific performance and approaches MLC on systematicity when using test-time training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-NeMO-Minitron-8B-Full",
            "model_description": "8B-parameter transformer-based LLM trained on multiple ARC-style datasets and used with extensive inference-time augmentations and optional LoRA test-time training.",
            "model_size": "8B",
            "is_pretrained": true,
            "architectural_features": "large pretrained transformer fine-tuned on ARC-style corpora; uses augmented inference (rotations, color permutations), candidate pruning and optional LoRA test-time training",
            "task_domain": "visual reasoning (ARC-style)",
            "task_name": "Compositional-ARC (domain-specific fine-tuned evaluation)",
            "task_description": "Evaluated on Compositional-ARC with ARC-style augmentations and optional one-epoch LoRA test-time training on study examples.",
            "compositional_depth": "level-2 OOD compositions",
            "composition_type": "novel compositions of geometric primitives",
            "split_type": "OOD",
            "training_strategy": "pretraining + domain-specific fine-tuning on ARC-style data and optional test-time LoRA adaptation on study examples",
            "curriculum_details": null,
            "inoculation_details": "Test-time training variant performs one-epoch LoRA adaptation on study examples (repeated with augmentations) before inference when enabled.",
            "iid_performance": "3-Shot: exact match 95.71% (reported in paper)",
            "compositional_performance": "Systematicity with test-time training: exact match 78.20%, color accuracy 100.00%, shape accuracy 88.26% (reported in paper table for TTT condition)",
            "generalization_gap": "3-Shot 95.71% → Systematicity+TTT 78.20% ≈ 17.51 pp gap (note: TTT used to boost systematicity performance)",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared versus MLC and general LLMs; Mistral+TTT approaches MLC's systematicity performance in single-split numbers but requires much larger model and specialized TTT pipeline.",
            "architectural_comparison": null,
            "scale_effects": "Large 8B model with domain fine-tuning and TTT attains high performance, but the MLC 5.7M model matches or exceeds general LLMs and rivals domain-specific 8B+TTT performance in some settings.",
            "transfer_results": null,
            "key_findings": "Demonstrates that domain-specific fine-tuning plus test-time training can yield strong systematicity, but at cost of scale, specialized tokenizers/augmentation, and TTT; MLC offers a small-model alternative with strong generalization.",
            "failure_analysis": null,
            "success_conditions": "Succeed when using ARC-specific training, heavy augmentation, and test-time LoRA adaptation on each episode's study examples.",
            "uuid": "e2027.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human-like systematic generalization through a metalearning neural network",
            "rating": 2
        },
        {
            "paper_title": "Abstraction and Reasoning Corpus",
            "rating": 2
        },
        {
            "paper_title": "The llm architect: Solving the arc challenge is a matter of perspective",
            "rating": 2
        },
        {
            "paper_title": "COGS: A compositional generalization challenge based on semantic interpretation",
            "rating": 1
        },
        {
            "paper_title": "Compositional generalization through meta sequence-to-sequence learning",
            "rating": 1
        }
    ],
    "cost": 0.0256025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>COMPOSITIONAL-ARC: ASSESSING SYSTEMATIC GENERALIZATION IN ABSTRACT SPATIAL REASONING
25 Sep 2025</p>
<p>Philipp Mondorf p.mondorf@lmu.de 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>Shijia Zhou zhou.shijia@lmu.de 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>Monica Riedler 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Barbara Plank b.plank@lmu.de 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>COMPOSITIONAL-ARC: ASSESSING SYSTEMATIC GENERALIZATION IN ABSTRACT SPATIAL REASONING
25 Sep 2025EAF312043528FE02F14CEEE2B0CD0A09arXiv:2504.01445v2[cs.AI]
Systematic generalization refers to the capacity to understand and generate novel combinations from known components.Despite recent progress by large language models (LLMs) across various domains, these models often fail to extend their knowledge to novel compositional scenarios, revealing notable limitations in systematic generalization.There has been an ongoing debate about whether neural networks possess the capacity for systematic generalization, with recent studies suggesting that meta-learning approaches designed for compositionality can significantly enhance this ability.However, these insights have largely been confined to linguistic problems, leaving their applicability to other tasks an open question.In this study, we extend meta-learning for compositionality to the domain of abstract spatial reasoning.To this end, we introduce Compositional-ARC-a dataset designed to evaluate the capacity of models to systematically generalize from known geometric transformations (e.g., translation, rotation) of abstract two-dimensional objects to novel combinations of these transformations (e.g., translation+rotation).Our results show that a small transformer-based encoder-decoder model, trained via meta-learning for compositionality, can systematically generalize to previously unseen transformation compositions.Notably, despite having only 5.7M parameters, this model significantly outperforms state-of-the-art LLMs-including o3-mini, GPT-4o, and Gemini 2.0 Flash, which fail to exhibit similar systematic behavior-and performs on par with the winning model of the ARC prize 2024, an 8B-parameter LLM trained via test-time training.Our findings highlight the effectiveness of meta-learning in promoting systematicity beyond linguistic tasks, suggesting a promising direction toward more robust and generalizable models.</p>
<p>INTRODUCTION</p>
<p>A fundamental aspect of human cognition is the ability to systematically generalize from known components to novel combinations (Marcus, 2003;Lake et al., 2017).This capacity is particularly evident in language, where an infinite number of new sentences can be constructed and interpreted by extracting meaning from previously acquired expressions and rules (Chomsky, 2002;Szabó, 2012).Similarly, our spatial perception relies on systematic generalization, enabling individuals to compose learned spatial principles into novel configurations (Zhou et al., 2024;Dautriche &amp; Chemla, 2025).For instance, once a person understands how to translate and rotate an object, they can apply these transformations in combination-translating and rotating the object simultaneously-even if they have never encountered such a composed transformation before (Fife et al., 2019).</p>
<p>Despite its central role in human cognition, systematic generalization remains a significant challenge in artificial intelligence (Lake &amp; Baroni, 2018;Loula et al., 2018;Hupkes et al., 2020).While large language models have recently demonstrated notable progress across various domains (OpenAI, 2024;Guo et al., 2025), they often fail to combine acquired knowledge in novel scenarios, demonstrating notable difficulties with systematic generalization (Dziri et al., 2023;Ismayilzada et al., 2025;Gendron et al., 2024).The question of whether neural networks can achieve systematicity has been the subject of extensive debate (Fodor &amp; Pylyshyn, 1988;Brakel &amp; Frank, 2009;Calvo &amp; Symons, 2014, inter alia).Recent research by Lake &amp; Baroni (2023) demonstrates that a transformer- based encoder-decoder model, trained via meta-learning for compositionality (MLC), can achieve human-like systematic generalization in processing instructions expressed in a pseudolanguage.By training the model to combine basic units of pseudolanguage into novel sequences over a stream of dynamically changing grammars, Lake &amp; Baroni (2023) show that this model can effectively generalize to previously unseen compositions of language (see Section 2 for further details).While this approach presents a promising direction for addressing systematicity in neural networks, its applicability beyond linguistic contexts remains an open question.</p>
<p>In this study, we extend the MLC framework proposed by Lake &amp; Baroni (2023) to the domain of abstract spatial reasoning.Inspired by the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), we introduce Compositional-ARC-a new dataset for assessing systematic generalization in abstract spatial reasoning.Compositional-ARC presents examples of basic geometric transformations (e.g., translation, rotation) applied to abstract two-dimensional objects and tests generalization to previously unseen compositions (e.g., translation+rotation; see Figure 1).Using MLC, we train a small encoder-decoder model on samples from Compositional-ARC and demonstrate that it can systematically generalize to unseen transformation compositions.To the best of our knowledge, this is the first application of MLC to abstract spatial reasoning.In summary, our contributions are:</p>
<p>2 BACKGROUND: META-LEARNING FOR COMPOSITIONALITY When learning a new language, humans rely on their ability to recombine known words and expressions to interpret novel sentences (Chomsky et al., 1976;De Beule &amp; Bergen, 2006).For instance, someone who understands the meanings of "cats drink water" and "dogs like to play" will typically also understand the meanings of "dogs drink water" and "cats like to play" (Hinzen et al., 2012).</p>
<p>Whether language models possess a comparable degree of systematicity remains an open question, as current models, including large language models, still struggle with tests of systematic generalization (Ismayilzada et al., 2025;Dziri et al., 2023).To address these limitations, Lake &amp; Baroni (2023) propose meta-learning for compositionality (MLC), a framework designed to model human-like systematic generalization in learning pseudolanguage instructions.Through a series of experiments, the authors show that models trained via MLC can achieve levels of systematicity comparable to those of humans when interpreting previously unseen pseudolanguage inputs.</p>
<p>Task setup.In their study, Lake &amp; Baroni (2023) examine few-shot compositional tasks in which instructions, represented as sequences of pseudowords (e.g., "dax," "lug," "fep"), must be mapped to corresponding sequences of abstract symbols (see Figure 2 for an example).To understand the meaning of such instructions, an interpretation grammar needs to be deduced from a limited number of study examples.This grammar maps pseudowords to their symbolic representation through a set of compositional rewrite rules.For instance, if "dax" corresponds to a green circle, "dax fep" to three green circles, and "zup" to a red circle, then "zup fep" would denote three red circles.Importantly, the examples are designed to be highly systematic, progressing from primitive mappings to more complex compositions.The core challenge lies in the ability to generalize systematically, i.e., to reuse and combine components from the study examples (left side of Figure 2) to generate correct outputs for novel query instructions (right side of Figure 2).</p>
<p>Algorithmic approach.To achieve systematic generalization in the instruction-learning task, Lake &amp; Baroni ( 2023 After training the model over a set of 100,000 distinct interpretation grammars, it demonstrates the capacity to generalize to previously unseen instructions and grammars.For specific details regarding training procedures, we refer to the original paper (Lake &amp; Baroni, 2023).</p>
<p>While Lake &amp; Baroni (2023) also evaluate MLC on COGS (Kim &amp; Linzen, 2020) and SCAN (Lake &amp; Baroni, 2018), which test systematic lexical generalization to novel word combinations, their  experiments are confined to the linguistic domain.In the following section, we propose Compositional-ARC to show how MLC can be extended to support systematic generalization in abstract spatial reasoning, demonstrating its potential beyond linguistic tasks.</p>
<p>METHOD</p>
<p>COMPOSITIONAL-ARC</p>
<p>To test systematicity in abstract spatial reasoning, we leverage the closure property of combined geometric transformations, where the composition of two valid transformations-such as translation, rotation, and reflection-yields another valid geometric transformation (Brannan et al., 2011).</p>
<p>Drawing inspiration from the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), we design a task in which abstract objects, defined in a two-dimensional grid environment, are subjected to basic geometric transformations and their compositions (see Figure 1 for examples).We use fixed-size 10×10 grids, each of which can be represented as a two-dimensional array of integers, where different values correspond to distinct colors.We use integers from 0 to 9, with 0 denoting a black background and the remaining integers mapping to unique colors (see Appendix A.1 for more details).Objects are defined based on color connectivity; that is, each object comprises a group of connected cells sharing the same color.Connectivity is determined by the Moore neighborhood (Bays, 2010), meaning that cells are considered connected if they are directly or diagonally adjacent.Each grid contains either one or two objects.A transformation is represented as a pair of grids, with the input grid displaying the objects before, and the output grid showing them after the geometric transformation.</p>
<p>Each transformation affects only one of the objects in the grid.For example, in Figure 1a, a single L-shaped yellow object is translated one step downward.In Figure 1c, a square blue object in the bottom-right expands toward the neighboring top row.Objects never occlude one another nor extend beyond the boundaries of the 10 × 10 grids.</p>
<p>We limit our dataset to five basic geometric transformations and their compositions: i) translations, ii) rotations, iii) reflections, iv) extensions, and v) color changes.For our experiments, we further constrain the configurations of these transformations to establish a controlled setup.Translations are limited to movements of one cell to the right or one cell downward.Rotations are restricted to 90 degrees clockwise or counterclockwise around the top-left corner of the object.We consider horizontal and vertical reflections across the object's central axis.Extensions mean that the object grows in a certain direction, and are limited to neighboring cells either leftward or upward.Color changes are restricted to changing the object's color to either red or orange.For detailed definitions of each transformation, please refer to Appendix A.2.</p>
<p>To signal which objects undergo which transformations, we consider three types of indicators: i) shapebased transformations, which affect objects of a particular shape; ii) color-based transformations, which affect all objects of a specific color; and iii) neighbor-based transformations, where objects are transformed when a second, indicator object is present.For instance, in Figure 1, all L-shaped objects (similar to the object in Figure 1a) undergo a one-step downward translation.All green objects undergo a horizontal reflection, and any object sharing a grid with the gray diagonal object (e.g., as seen in Figure 1c) expands into the neighboring top row.This indicator-based approach enables the definition of transformation compositions.For example, objects that are both L-shaped and green undergo a one-step downward translation together with a horizontal reflection (see Figure 1d for an example).We also define different levels of composition: level 1 combines two indicators (e.g., when an object matches the indicated shape and color, but lacks a the proximity to a neighboring object, as illustrated in Figure 1d), while level 2 combines all three indicators, specifying the object's shape, color, and proximity to an indicator object (see Figure 1g).</p>
<p>To test systematicity, we present few-shot examples of primitive transformations and their level-1 compositions, and evaluate models on previously unseen level-2 compositions.For instance, in Figure 3, models are asked to infer the correct transformation for a previously unseen level-2 composition of indicators, given a set of 12 study examples illustrating primitive transformations and their level-1 compositions.Conceptually, our setup is similar to the few-shot compositional task introduced by Lake &amp; Baroni (2023) (see Section 2), but it replaces the lexical interpretation grammar with a visual interpretation grammar.Specifically, models need to infer which indicator maps to which transformation, and how to compose them to deduce the correct final transformation.For a detailed description of how we algorithmically generate dataset samples, please refer to Appendix A.3.  2  2 2 2  2 2   2 2 2  2 2 2  2   5  5   6  6 6   4 4  4  4 4  4   6  6 6   5  5 5 5   4  4 4 4   2  2  2  2 2 2  2 2   2 2 2  2 2 2  2   5 5  5 5   6  6 6   4 4 4  4 4  4 4 4  4</p>
<p>META-LEARNING FOR COMPOSITIONALITY IN ABSTRACT SPATIAL REASONING</p>
<p>To systematically generalize from known geometric transformations to previously unseen transformation compositions, we extend the meta-learning for compositionality (Lake &amp; Baroni, 2023) framework described in Section 2. As in the original MLC approach, we train a transformer-based encoder-decoder model on a dataset of dynamically changing interpretation grammars.However, instead of mapping pseudolinguistic instructions to sequences of abstract symbols, we consider a visual interpretation grammar that associates visual indicators (object shape, color, or proximity to an indicator object) with specific geometric transformations, as described in Section 3.1.An episode in Compositional-ARC is defined as a set of study examples that illustrate the underlying grammar, along with query inputs for which the correct outputs must be inferred.For instance, the episode in Figure 3  Encoding and positional embedding.Each episode is presented to the model as a sequence of input-output grid pairs (study examples), followed by a query input grid, for which the model must generate the corresponding output grid (see Figure 3).To encode the two-dimensional grids, we divide each 10 × 10 grid into 2 × 2 patches (left to right, top to bottom), yielding 25 patches per grid (Dosovitskiy et al., 2021).Each patch is mapped to a unique embedding vector.Since each grid cell can take integer values from 0 to 9, a 2 × 2 patch can yield up to 10,000 distinct configurations, resulting in 10,000 possible embedding vectors.Two special tokens, | and →, are introduced to mark the boundaries between study examples and the input-output grids, respectively.The decoder vocabulary comprises two additional tokens for the start and end of a sequence (SOS and EOS).To encode positional information, we use standard learnable 1D positional embeddings that capture the order of grid pairs, as well as a second set of learnable 2D positional embeddings applied to grid patches.These 2D embeddings are decomposed into separate row and column components, which are added to each patch embedding to capture two-dimensional spatial information.</p>
<p>Training procedure.The model is trained on a large set of episodes, each defined by a unique visual interpretation grammar.In each episode, the model is provided with a sequence of study examples and tasked with predicting the output grid for a given input query (see Figure 3).Following Lake &amp; Baroni (2023), we include an auxiliary copy task during training, in which the model must also reproduce the output grids of each study example.We employ a model with three layers each in the encoder and decoder, eight attention heads per layer, input and hidden embeddings of size 128, a feedforward hidden size of 768, and GELU (Hendrycks &amp; Gimpel, 2016)   We generate 100,000 episodes, each comprising three few-shot examples for the "3-Shot" task, 12 systematic study examples for the "Systematicity" setup, and ten query input-output grid pairs demonstrating the final level-2 transformation composition.Each episode is characterized by a unique visual interpretation grammar.For instance, in one episode, yellow objects are translated downward by a single cell, while in another, yellow objects are reflected horizontally.To train our encoder-decoder model via MLC, we split the data into 82,908 training, 8,546 validation and 8,546 test episodes.Importantly, the data splits are constructed such that the geometric transformations involved in the final query level-2 compositions differ between the training and evaluation sets.</p>
<p>For instance, while the model is trained on basic transformations and a series of transformation compositions (e.g., translation+rotation+reflection), it is tested out-of-distribution on compositions not seen during training (e.g., translation+rotation+extension).For comprehensive statistics of the dataset splits, please refer to Table 5 in the Appendix.</p>
<p>LARGE LANGUAGE MODELS</p>
<p>General-purpose LLMs.In addition to the model trained via MLC, we evaluate three state-of-theart general-purpose LLMs on the test set of our proposed dataset: o3-mini (low) (OpenAI, 2025), GPT-4o (Achiam et al., 2023), and Gemini 2.0 Flash (DeepMind, 2024).To textually prompt the models for a given episode, we represent grids as two-dimensional arrays, consistent with prior work (Moskvichev et al., 2023).We also test a multimodal setup in which both an image of the study examples and the input query are provided alongside the text prompt.Domain-specific LLMs.We further consider two LLMs specifically tailored to ARC-style data: (i) Llama-3.2-3B-ReARC,fine-tuned on the re-ARC dataset (Hodel, 2024)-an extension of 1,000 additional generated examples per sample in ARC-and (ii) Mistral-NeMO-Minitron-8B-Full, trained on a broad range of ARC-style data, including re-ARC, Concept-ARC (Moskvichev et al., 2023), and ARC-Heavy (Li et al., 2025).These models were proposed by Franzen et al. (2024) and placed 1st in the ARC prize 2024. 1 Note that in addition to fine-tuning, these models use an ARC-customized tokenizer, extensive data augmentation during training and inference, a generation procedure that leverages depth-first search to produce multiple solution candidates, and a refined candidate-selection step.The authors also employ test-time training (TTT), which further fine-tunes models on the fewshot input-output grid pairs from the final test set.We use both models with their default parameters.</p>
<p>For additional details, please refer to the original paper (Franzen et al., 2024) or Appendix C.2.</p>
<p>EVALUATION METRICS</p>
<p>To evaluate the quality of the generated output grids, we use three different metrics: i) exact match accuracy, ii) color accuracy, and iii) shape accuracy.Exact match accuracy requires that a prediction is accurate only if every cell matches the target grid.Color accuracy checks whether predicted objects match target colors, ignoring shape and location.Shape accuracy checks whether predicted objects match target shapes, ignoring color and location.Formal definitions are provided in Appendix C.1.</p>
<p>RESULTS</p>
<p>In Table 1, we report the performance of the model trained via MLC, alongside the LLMs we evaluate on the two task setups, as described in Section 4.1.</p>
<p>Standard few-shot learning task.We begin by examining model performance on the "3-Shot" task, where models are given three input-output examples illustrating the final transformation composition (see Figure 4 in the Appendix).Despite this guidance and the relatively simple transformations involved, general-purpose LLMs such as GPT-4o and Gemini 2.0 Flash struggle with the task: GPT-4o reaches an accuracy of only 22.28%, while Gemini 2.0 Flash performs slightly better at 30.08%.The long-chain-of-thought model o3-mini achieves a modest accuracy of 64.04%.In contrast, domain-specific models such as Llama-3.2-3B-ReARC, and Mistral-NeMO-Minitron-8B-Full perform significantly better.While Llama-3.2-3B-ReARC achieves an accuracy of 85.85%, Mistral-NeMO-Minitron-8B-Full reaches up to 95.71%.Note that we do not employ test-time training in this setup, as it would contradict the out-of-distribution test setup described in Section 4.1.Notably, the 5.7M-parameter encoder-decoder model trained via MLC outperforms both general-purpose and domain-specific LLMs, with an accuracy of 99.92%, despite having only a fraction of the parameters.</p>
<p>We further find that all models predict object color nearly perfectly.For GPT-4o and Gemini 2.0 Flash, we observe that shape accuracy is significantly higher than exact match accuracy.This discrepancy suggests that while these models are often able to predict the correct shape and color of an object, they frequently fail to accurately predict its final position.Interestingly, both models show lower accuracy when visual input is added to the textual prompt, likely due to modality alignment challenges (Masry et al., 2025) or limitations in leveraging the visual content for reasoning.</p>
<p>Systematicity task.In the "Systematicity" task, models are asked to infer the correct final transformation composition from a set of study examples that represent more basic, decomposed transformations (see Figure 3 for an example).As shown in Table 1, all general-purpose LLMs perform poorly on this task.For instance, GPT-4o achieves an accuracy of 0.99%, while Gemini 2.0 Flash reaches 2.66%.Interestingly, o3-mini, the best-performing general-purpose model on the "3-Shot" task, performs worst in this setting, with an accuracy of only 0.53%.</p>
<p>CONSISTENCY ACROSS DATA SPLITS</p>
<p>To ensure that the strong performance of MLC, as reported in Table 1, is not the result of a favorable data split, we train and evaluate the model on three additional, independently generated data splits for each task configuration-resulting in four distinct models per task setup.Detailed descriptions of these data splits are provided in Table 5 in the Appendix.Table 2 summarizes the average accuracy and corresponding standard deviation across all four splits.For the standard three-shot learning task, MLC consistently achieves high accuracy, with a mean of 98.78% and a standard deviation of 1.99%.Similarly, for the systematicity task, the model demonstrates robust generalization, achieving an even higher average accuracy than on the initial data split, with a mean of 86.73%.</p>
<p>Ablation studies.To gain deeper insights into the factors influencing model performance, we conduct a series of ablation studies.First, we evaluate the impact of removing the auxiliary copy task from the training objective-a setup in which the model is trained not only to predict the output grid for a given input query but also to reproduce the output grid of each study example (refer to Section 3.2).Removing this auxiliary task results in a notable decrease in accuracy from 86.73% ± 6.03% to 69.05% ± 9.23%.This decline underscores the importance of the copy task in promoting systematic generalization, aligning with the findings of Lake &amp; Baroni (2023).Subsequently, we assess the role of study examples in model performance.Removing primitive transformations from the study examples (see Figure 3) results in a moderate reduction in performance, with an average accuracy of 75.27% ± 12.95%.This suggests that examples involving only level-1 transformation compositions are, to some extent, sufficient for allowing the model to generalize to more complex level-2 compositions.However, removing level-1 transformation compositions leads to a severe performance degradation, reducing accuracy to 21.01% ± 19.07%.We hypothesize that this is due to the increased complexity of composing three primitive operations directly into a level-2 transformation, as opposed to building on intermediate level-1 compositions.</p>
<p>In conclusion, our experiments highlight the potential of MLC beyond linguistic tasks, demonstrating its capacity for systematic generalization in abstract spatial reasoning.</p>
<p>RELATED WORK</p>
<p>Meta-learning.Meta-learning aims to improve a model's ability to adapt to novel tasks by leveraging experience over multiple training episodes (Thrun &amp; Pratt, 1998;Hospedales et al., 2022).It has been successfully applied to various tasks, such as few-shot learning (Mishra et al., 2018), continual learning (Javed &amp; White, 2019;Lee et al., 2023;Irie et al., 2025), and reinforcement learning (Duan et al., 2016;Wang et al., 2017;Mishra et al., 2018).Related to our work, meta-learning has been used to improve systematic generalization.Lake &amp; Baroni (2018) showed that traditional sequence-tosequence models struggle with compositional skills, but incorporating meta-learning can significantly improve performance (Lake, 2019;Conklin et al., 2021).Recent work argues that giving models the opportunity to practice skills via meta-learning is crucial for addressing challenges such as systematic generalization, among others (Irie et al., 2025).Our method builds on meta-learning strategies inspired by Lake &amp; Baroni (2023), extending them to the domain of abstract spatial reasoning.</p>
<p>ARC-like puzzles.The abstraction and reasoning corpus (ARC) (Chollet, 2019) is a benchmark designed to evaluate a model's capacity to generalize to novel scenarios with limited to no prior knowledge.Based on a set of few-shot examples, models are required to infer transformations of abstract objects or patterns within two-dimensional grids.Unlike ARC, which encompasses a broad range of complex transformations, our work deliberately narrows the scope to the five fundamental geometric transformations described in Section 3.1, focusing instead on the aspect of systematicity.Several ARC variants have been proposed, including 1D-ARC (Xu et al., 2024), Mini-ARC (Kim et al., 2022), ConceptARC (Moskvichev et al., 2023) and MC-LARC (Shin et al., 2024).However, to the best of our knowledge, Compositional-ARC is the first to focus on compositional generalization.</p>
<p>CONCLUSION</p>
<p>In this work, we extend the meta-learning for compositionality framework proposed by Lake &amp; Baroni (2023) to the domain of abstract spatial reasoning.</p>
<p>A DATASET</p>
<p>In this work, we present Compositional-ARC, a dataset designed to study systematicity in abstract spatial reasoning.As outlined in Section 3.1, Compositional-ARC evaluates a model's capacity to systematically generalize learned geometric transformations (e.g., translation, rotation) of twodimensional objects to novel compositions of these transformations (e.g., translation+rotation).The subsequent sections offer a detailed description of the dataset, including formal definitions of the grid-based environment and the set of transformations it includes.</p>
<p>A.1 GRID SETUP</p>
<p>We define the structure of the 10 × 10 grid environment and the notion of objects within it.Each grid is represented as a matrix X ∈ N 10×10 , where each element corresponds to a cell with a discrete color value.Objects are defined based on color connectivity using the Moore neighborhood (Bays, 2010).</p>
<p>Definition 1 (Grid &amp; Object).Let X ∈ N 10×10 be a matrix with rows i and columns j , referred to as a grid, where each element X ij ∈ {0, . . ., 9}.The value X ij = 0 represents a background cell, and values X ij ∈ {1, . . ., 9} represent object colors.</p>
<p>An object is a set of coordinates O ⊆ {0, . . ., 9} 2 such that each (i, j) ∈ O satisfies X ij = c, and the elements in O form a single connected component.</p>
<p>Two elements X ij and X kl are considered connected if:
max(|i − k|, |j − l|) ≤ 1
We define the following color mapping: 0 → black, 1 → red, 2 → orange, 3 → yellow, 4 → green, 5 → blue, 6 → purple, 7 → pink, 8 → cyan, and 9 → gray.</p>
<p>A.2 GEOMETRIC TRANSFORMATIONS</p>
<p>We formally define the five basic geometric transformations used in our dataset: translation, rotation, reflection, extension, and color change.Each transformation operates on objects within the grid environment as defined in Appendix A.1.A transformation is considered valid if all transformed coordinates lie within the grid bounds and do not overlap with existing objects in the original grid.</p>
<p>Translation.Moves an object by one cell along a specified direction (downward or rightward).A formal definition is given in the text box below.</p>
<p>Definition 2 (Translation).Let O ⊆ {0, . . ., 9} 2 be an object in a grid X ∈ N 10×10 , and let v = (v 1 , v 2 ) ∈ {(1, 0), (0, 1)} be the translation direction (downward or rightward).</p>
<p>The translated object is:
T trans,v (O) = {(i + v 1 , j + v 2 ) | (i, j) ∈ O}
The translation is valid if:
∀(i ′ , j ′ ) ∈ T trans,v (O), 0 ≤ i ′ , j ′ &lt; 10, X i ′ j ′ = 0
Rotation.Rotates an object 90 • clockwise or counterclockwise around the top-left of its bounding box.A more formal definition is given in the text box below.</p>
<p>Definition 3 (Rotation).Let O ⊆ {0, . . ., 9} 2 be a set of grid cells with row-column coordinates (i, j).Let i 0 = min (i,j)∈O i and j 0 = min (i,j)∈O j.We set the pivot P = (i 0 , j 0 ) as the top-left of the bounding box.</p>
<p>For each (i, j) ∈ O, we specify the offset from the pivot as:
(∆i, ∆j) = (i − i min , j − j min ).
We define a rotation by ±90 • as:
R +90 • (∆i, ∆j) = (∆j, −∆i), R −90 • (∆i, ∆j) = (−∆j, ∆i),
where +90 • is clockwise and −90 • is counterclockwise under the row-down convention.</p>
<p>Given a 90 • rotation, either clockwise or counterclockwise, the rotated object is:
T rot,±90 • (O) = ( i min + ∆i, j min + ∆j ) (i, j) ∈ O .
The rotation is valid if:
∀(i ′ , j ′ ) ∈ T rot,θ (O), 0 ≤ i ′ , j ′ &lt; 10, x i ′ j ′ = 0
Reflection.Reflects an object across its vertical or horizontal axis, reversing the relative positions of its coordinates while preserving overall structure.</p>
<p>Definition 4 (Reflection).Let O ⊆ {0, . . ., 9} 2 be an object in a grid X ∈ N 10×10 , and let d ∈ {horizontal, vertical} indicate the axis of reflection.</p>
<p>Let:
i min = min{i | (i, j) ∈ O}, i max = max{i | (i, j) ∈ O} j min = min{j | (i, j) ∈ O}, j max = max{j | (i, j) ∈ O}
Then the reflected object is:
T ref,d (O) = {(i max − (i − i min ), j) | (i, j) ∈ O} if d = horizontal {(i, j max − (j − j min )) | (i, j) ∈ O} if d = vertical
The reflection is valid if:
∀(i ′ , j ′ ) ∈ T ref,d (O), 0 ≤ i ′ , j ′ &lt; 10, X i ′ j ′ = 0
Extension.Adds a new cell in the upward or leftward direction for each coordinate in the object.</p>
<p>Definition 5 (Extension).Let O ⊆ {0, . . ., 9} 2 be an object in a grid X ∈ N 10×10 , with color c &gt; 0. Let d ∈ {up, left} indicate the extension direction.</p>
<p>Let the set of new cells adjacent to the object in direction d be:
N d (O) = {(i − 1, j) / ∈ O | (i, j) ∈ O, i &gt; 0, x i−1,j = 0} if d = up {(i, j − 1) / ∈ O | (i, j) ∈ O, j &gt; 0, x i,j−1 = 0} if d = left
Then the extended object is:
T ext,d (O) = O ∪ N d (O)
The extension is valid if:
∀(i ′ , j ′ ) ∈ N d (O), 0 ≤ i ′ , j ′ &lt; 10, , X i ′ j ′ = 0 All new cells (i ′ , j ′ ) ∈ N d (O)
are assigned the color of the original object:
X ′ i ′ j ′ = c
Color change.Alters the color of an object to either red or orange, without changing its structure or position.</p>
<p>Definition 6 (Color Change).Let O ⊆ {0, . . ., 9} 2 be an object in a grid X ∈ N 10×10 , with color c &gt; 0. Let c ′ ∈ {1, 2} be the new color (representing red or orange).</p>
<p>The resulting grid X ′ is given by:
X ′ ij = c ′ if (i, j) ∈ O X ij otherwise A.3 DATASET GENERATION
To generate episodes that comprise primitive transformations, level-1 transformation compositions, and level-2 transformation compositions, we developed a script that systematically generates the corresponding input-output grid pairs for each transformation.The complete code repository for data generation is publicly available at: https://github.com/mainlp/C-ARC.In the following, we provide a brief overview of the procedure used to generate input-output grid pairs for each sample within an episode.As detailed in Section 3.1 and Appendix A.2, we consider five basic geometric transformations, along with three types of transformation indicators: shape-based, color-based, and neighbor-based.These allow us to define a total of ten distinct transformation triplets, each mapping the indicators to corresponding transformations (e.g., shape-based: translation, color-based: reflection, neighbor-based: extension).For each episode, a transformation triplet is uniformly sampled from this set to define the visual interpretation grammar of the episode.Once the transformations are determined, we randomly assign a specific shape for the shape-based transformation, a specific color for the color-based transformation, and an indicator object for the neighbor-based transformation.Importantly, the indicator object is constrained to neither share the shape associated with the shapebased transformation nor the color linked to the color-based transformation.</p>
<p>Using these specifications, we generate input-output grid pairs representing primitive, level-1, and level-2 transformations.For each transformation mapping, we randomly place an object on a 10 × 10 grid, ensuring it possesses the designated shape, color, and/or proximity to the indicator object as required.The specified transformation is then applied to this object.If the resulting transformed object remains within the grid bounds and does not overlap with any other object, the corresponding input-output grid pair is accepted as a valid sample for the episode.Otherwise, a new object location is sampled and the process is repeated until a valid pair is obtained.Finally, we make sure that each</p>
<p>B TRAINING DETAILS</p>
<p>As outlined in Section 3.2, we use a transformer-based encoder-decoder model trained using MLC to predict the correct output grid for a given input query, given a set of study examples.Specifically, we generate a dataset of 100,000 episodes and split it into train, validation and test sets (for more information see Section 4.1 and Table 5).The model is optimized using cross-entropy loss, averaged over the predicted patch embeddings, as described in Section 3.2.To place greater emphasis on non-background regions, patches corresponding exclusively to black 2 × 2 cells are down-weighted by a factor of 0.2 during loss computation.</p>
<p>Each episode includes a collection of study examples and queries.In the standard few-shot learning task (Section 4.1), the model receives three input-output grid pairs, along with the input query.For the systematicity task, 12 systematic study examples are provided.In both tasks, the model is required to predict the correct output grid for ten distinct input queries.</p>
<p>Training is conducted over 200 epochs with a batch size of 200 for the standard few-shot learning task (i.e., 200 • 10 = 2000 queries per batch), and over 300 epochs with the same batch size for the systematicity task.A learning rate of 0.01 is used in both cases.Following the approach of Lake &amp; Baroni (2023), we apply a warm-up phase during the first episode, beginning with a learning rate of 1 × 10 −4 , followed by a linear decay to 5 × 10 −4 over the course of training.Additional hyperparameter settings are provided in Section B.1 and summarized in Table 3.  [2,3,4], and number of decoder layers ∈ 2, 3, 4.</p>
<p>For the hyperparamter search, the model is trained for 40 epochs on the systematicity task and evaluated on its corresponding validation set.Across 25 independent runs, we select the configuration that achieves the highest validation accuracy.The final hyperparameter settings, presented in Table 3, are employed consistently across both task setups.</p>
<p>B.2 IMPLEMENTATION DETAILS</p>
<p>All experiments were conducted using PyTorch (Paszke et al., 2019) as the primary development framework.Comprehensive details regarding supporting software and versioning are available in our code repository.Experiments were executed on NVIDIA A100 and H200 GPUs.Training models with MLC on the standard three-shot learning task over 200 epochs required approximately 40 GPU hours on a single A100 GPU.For the systematicity experiments with 12 study examples, training over 300 epochs on the designated dataset consumed roughly 100 GPU hours on a single H200 GPU.</p>
<p>C EXPERIMENT DETAILS</p>
<p>This section provide further details regarding our experimental setup.Specifically, Section C.1 presents formal definitions of the evaluation metrics used to assess the performance of the models studied in this work, while Section C.2 outlines additional information on how we interact with API-based LLMs.</p>
<p>C.1 EVALUATION METRICS</p>
<p>As described in Section 4.3, we use three different evaluation metrics to assess model performance in this study: i) exact match accuracy, ii) color accuracy, and iii) shape accuracy.These metrics are formally defined based on the grid-based environment X and the concept of an object O, as specified in Definition 1.</p>
<p>Let X target , X pred ∈ N 10×10 denote the target and predicted grids, respectively.Each cell X target ij (or X pred ij</p>
<p>) contains an integer in 0, . . ., 9, where 0 represents the background and values from 1 to 9 correspond to cells occupied by colored objects.The set of objects-defined as maximal connected cells of a consistent color under the Moore neighborhood (see Section 3.1)-extracted from X target and X pred are denoted P(X target ) and P(X pred ), respectively.For each object in grid O ∈ P(X), we assign a color label c(O) ∈ 1, . . ., 9 and define its normalized shape as follows:
S(O) = {(i − i min , j − j min ) : (i, j) ∈ O},(1)
where i min = min{i : (i, j) ∈ O} and j min = min{j : (i, j) ∈ O}.</p>
<p>(
)2
This transformation "anchors" the object to the top-left corner by translating it to a coordinate system with its minimum row and column indices set to zero.</p>
<p>Accuracy.The exact match accuracy evaluates whether the predicted grid X pred is identical to the target grid X target on a cell-by-cell basis:
Accuracy(X pred , X target ) = 1, if X pred ij = X target ij ∀ (i, j) ∈ {0, . . . , 9} 2 , 0, otherwise.(3)
In other words, this metric yields a value of 1 if and only if the entire predicted grid matches the target grid exactly, i.e., X target = X pred .The mean accuracy over the dataset D is then defined as:
Accuracy = 1 |D| (X pred ,X target )∈D Accuracy(X pred , X target )(4)
Color accuracy.Color accuracy assesses whether the predicted grid contains the same number of objects of each color as the target grid, irrespective of their locations or shapes.For a given color c ∈ 1, . . ., 9, let m(c, X) = {O ∈ P(X) : c(O) = c} .</p>
<p>(5) denote the number of objects of color c in grid X.Then, color accuracy is defined as:
Color Accuracy(X pred , X target ) = 1 ∀ c ∈ {1, . . . , 9} : m(c, X pred ) = m(c, X target ) , (6)
where 1• is the indicator function, returning 1 if the condition is satisfied for all colors and 0 otherwise.The mean color accuracy over the dataset D is given by:
Color Accuracy = 1 |D| (X pred ,X target )∈D
Color Accuracy(X pred , X target ) (7)</p>
<p>Shape accuracy.Shape accuracy measures the agreement in object shapes between the predicted and target grids, independent of color and position.For each object in a grid O ∈ P(X), we consider its normalized shape S(O) as defined in Equation 1.The count of objects with a specific normalized shape s in grid X is given by:
n(s, X) = {O ∈ P(X) : S(O) = s} . (8)
Accordingly, shape accuracy is defined as:
Shape Accuracy(X pred , X target ) = 1 ∀ s : n(s, X pred ) = n(s, X target ) .(9)
That is, the predicted grid X pred has perfect shape accuracy if the number of objects corresponding to each normalized shape is identical to that in the target grid X target .Finally, the mean shape accuracy over the dataset D is given by:
Shape Accuracy = 1 |D| (X pred ,X target )∈D
Shape Accuracy(X pred , X target ) (10)</p>
<p>C.2 MODEL INFORMATION</p>
<p>General-purpose LLMs.As described in Section 4.2, we evaluate three different generalpurpose LLMs on Compositional-ARC.Specifically, we assess the performance of o3mini (OpenAI, 2025) (version o3-mini-2025-01-312 ), GPT-4o (Achiam et al., 2023) (version gpt-4o-2024-08-063 ), and Gemini 2.0 Flash (DeepMind, 2024) (version gemini-2.0-flash-0014).All models are accessed via their respective batch APIs, enabling us to process multiple samples per request.Unless otherwise specified, we employ the default API settings.For GPT-4o and o3-mini, this corresponds to a temperature and top p value of 1.0.5 Due to financial constraints, the o3-mini model is configured with a "low" reasoning effort.For Gemini 2.0 Flash, the provider does not disclose default parameter settings.</p>
<p>Prompts.The complete set of prompts used in our evaluations is presented in Figures 11 through 14.</p>
<p>To ensure consistency and facilitate meaningful comparisons, we apply the same prompts across all models.The standard few-shot learning prompt appears in Figure 11, while the prompt used for the systematicity task is shown in Figure 13.For Gemini 2.0 Flash, we add the instruction: "Do not generate any code to solve the task" to the output requirements, as the model otherwise does not adhere to the required output format.As outlined in Section 4.2, we additionally evaluate GPT-4o and Gemini 2.0 Flash in a multimodal configuration, in which both an image of the study examples and the input query are provided alongside the text prompt (text+image).The multimodal prompt for the few-shot learning task is shown in Figure 12, with the accompanying image illustrated in Figure 9.The corresponding multimodal prompt for the systematicity task is depicted in Figure 14, with the associated image presented in Figure 10.For the textual prompts, we represent grids as two-dimensional arrays, consistent with prior work (Moskvichev et al., 2023)).For instance, the final query input grid in Figure 4 would be represented as:</p>
<p>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 0, 0, 0, 0, 0, 0, 0, 0], [5, 5, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0]] Model responses are parsed using regular expressions to identify the expression "output:", followed by a two-dimensional array of the form "[[. ..]]", as specified in the input prompt.If a response does not contain this pattern, it is excluded from further analysis and omitted from accuracy computations.Table 4 summarizes the proportion of valid responses for each model.</p>
<p>Domain-specific LLMs.As mentioned in Section 4.2, we also evaluate two LLMs proposed by Franzen et al. (2024) that are specifically tailored to ARC-style data: (i) Llama-3.2-3B-ReARC(version Llama-3.2-3B-ARChitects-ReArc-bnb-4bit 6 ) and (ii) Mistral-NeMO-Minitron-8B-Full (version Mistral-NeMo-Minitron-8B-ARChitects-Full-bnb-4bit 7 ).We use the original code 8 provided by the authors to run their models on Compositional-ARC, with default parameters.This means that the models perform augmented inference on the test set with rotations and transpositions over all symmetries, in addition to color permutations and example shuffling.Candidate pruning is further applied with a minimum probability of 0.1.For models evaluated with test-time training, we follow the authors' one-epoch LoRA adaptation on the study examples of the test data repeated 48 times with the same augmentations described before.LoRA targets the attention and MLP modules, as well as the embeddings, with r = 64, α = 16, and dropout set to 0. The models are trained with a batch size of 16, gradient accumulation set to 1, a cosine learning rate of 1 × 10 −4 (with 1 × 10 −5 for embeddings), and a warmup ratio of 0.25.The resulting weights are then used for inference with the same default settings as described earlier.</p>
<p>D ADDITIONAL RESULTS</p>
<p>In this section, we present additional results for the experiments conducted in this study.First, we present additional qualitative results related to the model predictions on the standard few-shot learning and the systematicity task.Figures 4 through 6 illustrate representative episodes from the standard few-shot learning task.Model predictions are shown adjacent to each query, with results for GPT-4o and Gemini 2.0 Flash corresponding to text-only prompts.Across all three episodes, the model trained using MLC consistently predicts the correct output grid.In contrast, GPT-4o and Gemini 2.0 Flash frequently fail to identify the correct transformation-either misrepresenting the shape of the transformed object or incorrectly predicting its final position.Notably, o3-mini successfully predicts the correct output for the episodes in Figures 5 and 6, but fails on the example in Figure 4.</p>
<p>Figures 7 and 8 highlight episodes from the systematicity task.As shown, all general-purpose LLMs fail to produce accurate transformations, often misplacing the transformed object within the grid.In contrast, the model trained via MLC consistently predicts the correct transformation.</p>
<p>Response rates.As outlined in Section C.2, the general-purpose LLMs we evaluate are instructed to present their final output grid predictions using the keyword "output:", followed by a two-dimensional array of size 10 × 10 in the format "[[. ..]]".Responses that do not conform to this expected pattern are excluded from subsequent analyses and are not included in accuracy calculations.Table 4 provides an overview of the proportion of valid responses for each model.In the standard few-shot learning setting, all models demonstrate very high valid response rates, exceeding 99%.However, in the systematicity task, a slight decrease in valid responses is observed for Gemini 2.0 Flash when additional visual input (text+image) is introduced, with the rate falling to 94.09%.More significantly, GPT-4o exhibits a notable drop in valid response rate to 77.24% under multimodal conditions.We hypothesize that this decline may be attributed to the increased context length resulting from the additional image input.</p>
<p>Training on static data.In addition to the model trained via MLC on a stream of dynamically changing visual interpretation grammars, as described in Section 3.2, we adopt the approach of Lake (2019) and train a transformer-based encoder-decoder on a dataset governed by a fixed visual grammar (referred to as basic seq2seq).This means that the indicator-transformation mappings are static across the whole dataset.For instance, if yellow object translates one step downward, then this applies to all data samples across the dataset.Instead of episodes with few-shot examples, this dataset comprises individual input-output grid pairs, where the objective is to predict the output grid corresponding to a given input grid.This more closely resembles a standard training approach.</p>
<p>We construct a dataset of 1,300 grid pairs, partitioned into</p>
<p>E USE OF AI ASSISTANTS</p>
<p>We used GitHub Copilot for parts of the project's code, and ChatGPT for minor language revisions.### Task Description: You must solve an abstract visual reasoning task by identifying geometric transformations (e.g., rotation, translation, color changes, etc.) applied to objects within a 10x10 grid.
1 1 1 2 2 1 1 1 2 2 1 1 1 2 2 1 1 2 1 2 1 1 1 1 1 2 2 1 1 1 2 1 2 Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid 1 1 1 2 2 1 1 1 1 2 2 Input Grid Output Grid Query Target 1 1 1 1 2 2 1 1 1 2 2 1 1 1 1 1 2 2 1 1 1 1 2 2 1 1 1 1 1 2 2Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid
To infer the correct geometric transformation, you are given a series of 3 pairs of input-output examples.Each example pair consists of:</p>
<p>• An input grid: a 10x10 list of lists (2d array), where each element is an integer (0-9).</p>
<p>• A corresponding output grid: a 10x10 list of lists (2d array) that has undergone a transformation based on a specific geometric rule.</p>
<p>For the prediction you need to understand the transformations displayed in the provided examples and apply them to the final input grid.</p>
<h3>Your Task:</h3>
<ol>
<li>
<p>Analyze the example pairs to infer the transformation rules applied to each input grid.</p>
</li>
<li>
<p>Identify how these transformations are applied to generate the output grids.</p>
</li>
<li>
<p>Apply the deduced transformations to the final input grid.4. Output the correctly transformed 10x10 grid.</p>
</li>
</ol>
<h3>Output Requirements:</h3>
<p>• Return only the final output grid.</p>
<p>• Do not include any extra text, explanations, or comments.</p>
<p>• The output must be formatted exactly as: 'output: [[...]]'</p>
<p>• The output grid must be a 10x10 list of lists containing only integers between 0 and 9 (inclusive).</p>
<p>• Do not include unnecessary line breaks or additional text beyond the specified format.</p>
<h3>Input Format: You will receive the following data: ### Task Description: You must solve an abstract visual reasoning task by identifying geometric transformations (e.g., rotation, translation, color changes, etc.) applied to objects within a 10x10 grid.</h3>
<p>To infer the correct geometric transformation, you are given a series of 3 pairs of input-output examples.Each example pair consists of:</p>
<p>• An input grid: a 10x10 list of lists (2d array), where each element is an integer (0-9).</p>
<p>• A corresponding output grid: a 10x10 list of lists (2d array) that has undergone a transformation based on a specific geometric rule.</p>
<p>For the prediction you need to understand the transformations displayed in the provided examples and apply them to the final input grid.</p>
<h3>Your Task:</h3>
<ol>
<li>
<p>Analyze the example pairs to infer the transformation rules applied to each input grid.</p>
</li>
<li>
<p>Identify how these transformations are applied to generate the output grids.</p>
</li>
<li>
<p>Apply the deduced transformations to the final input grid.4. Output the correctly transformed 10x10 grid.</p>
</li>
</ol>
<h3>Output Requirements:</h3>
<p>• Return only the final output grid.</p>
<p>• Do not include any extra text, explanations, or comments.</p>
<p>• The output must be formatted exactly as: 'output: [[...]]'</p>
<p>• The output grid must be a 10x10 list of lists containing only integers between 0 and 9 (inclusive).</p>
<p>• Do not include unnecessary line breaks or additional text beyond the specified format.</p>
<h3>Input Format: You will receive the following data: ### Task Description: You must solve an abstract visual reasoning task by identifying geometric transformations (e.g., rotation, translation, color changes, etc.) applied to objects within a 10x10 grid.</h3>
<p>To infer the correct geometric transformation, you are given a series of 12 pairs of input-output examples.Each example pair consists of:</p>
<p>• An input grid: a 10x10 list of lists (2d array), where each element is an integer (0-9).</p>
<p>• A corresponding output grid: a 10x10 list of lists (2d array) that has undergone a transformation based on a specific geometric rule.</p>
<p>The first 6 example pairs demonstrate primitive transformations based on the object's color, shape, or the presence of an additional object.For instance, objects of a certain color within the 10x10 input grid might undergo a translation, while objects of a certain shape (distinct numerical pattern) are being rotated.</p>
<p>The latter 6 example pairs involve composite transformations, meaning multiple transformations are applied simultaneously.For instance, for objects that have the appropriate color and shape, both a translation and rotation are applied simultaneously.</p>
<p>For the final prediction you need to understand and further combine the transformations displayed in the provided examples and apply them to the final input grid.</p>
<h3>Your Task:</h3>
<ol>
<li>
<p>Analyze the example pairs to infer the transformation rules applied to each input grid.</p>
</li>
<li>
<p>Identify how these transformations might combine to generate the output grids.</p>
</li>
<li>
<p>Apply the deduced transformations to the final input grid.4. Output the correctly transformed 10x10 grid.</p>
</li>
</ol>
<h3>Output Requirements:</h3>
<p>• Return only the final output grid.</p>
<p>• Do not include any extra text, explanations, or comments.</p>
<p>• The output must be formatted exactly as: 'output: [[...]]'</p>
<p>• The output grid must be a 10x10 list of lists containing only integers between 0 and 9 (inclusive).</p>
<p>• Do not include unnecessary line breaks or additional text beyond the specified format.</p>
<h3>Input Format: You will receive the following data: ### Task Description: You must solve an abstract visual reasoning task by identifying geometric transformations (e.g., rotation, translation, color changes, etc.) applied to objects within a 10x10 grid.</h3>
<p>To infer the correct geometric transformation, you are given a series of 12 pairs of input-output examples.Each example pair consists of:</p>
<p>• An input grid: a 10x10 list of lists (2d array), where each element is an integer (0-9).• A corresponding output grid: a 10x10 list of lists (2d array) that has undergone a transformation based on a specific geometric rule.</p>
<p>The first 6 example pairs demonstrate primitive transformations based on the object's color, shape, or the presence of an additional object.For instance, objects of a certain color within the 10x10 input grid might undergo a translation, while objects of a certain shape (distinct numerical pattern) are being rotated.</p>
<p>The latter 6 example pairs involve composite transformations, meaning multiple transformations are applied simultaneously.For instance, for objects that have the appropriate color and shape, both a translation and rotation are applied simultaneously.</p>
<p>For the final prediction you need to understand and further combine the transformations displayed in the provided examples and apply them to the final input grid.</p>
<h3>Your Task:</h3>
<ol>
<li>
<p>Analyze the example pairs to infer the transformation rules applied to each input grid.</p>
</li>
<li>
<p>Identify how these transformations might combine to generate the output grids.</p>
</li>
<li>
<p>Apply the deduced transformations to the final input grid.4. Output the correctly transformed 10x10 grid.</p>
</li>
</ol>
<h3>Output Requirements:</h3>
<p>• Return only the final output grid.</p>
<p>• Do not include any extra text, explanations, or comments.</p>
<p>• The output must be formatted exactly as: 'output: [[...]]'</p>
<p>• The output grid must be a 10x10 list of lists containing only integers between 0 and 9 (inclusive).</p>
<p>• Do not include unnecessary line breaks or additional text beyond the specified format.</p>
<h3>Input Format: You will receive the following data:</h3>
<p>Figure 1 :
1
Figure 1: A conceptual overview of the data in Compositional-ARC.Primitive transformations refer to basic geometric transformations (e.g., translation, reflection, extension) based on an object's (a) shape, (b) color, or (c) proximity to a neighboring object.Pairs of these indicators, such as (d) shape+color, (e) shape+neighbor, or (f) color+neighbor, can be combined to form level-1 transformation compositions.Finally, all three indicators can be combined to form level-2 transformation compositions, based on the object's (g) shape+color+neighbor.</p>
<p>) train a transformer-based encoder-decoder model through meta-learning for compositionality.The key idea is to train the model on a dataset of dynamically changing interpretation grammars, where the mappings from input sequences to output symbols differ across training samples.This forces the model to rely on the information conveyed in the study examples to infer the appropriate grammar of a given sample, rather than memorizing static input-output mappings across the dataset.This flexibility enables the model to adjust to novel scenarios governed by new sets of examples and rules.Moreover, the compositional structure of both study examples and queries encourages the model to internalize mechanisms for composing elements presented in the study examples.</p>
<p>Figure 2 :
2
Figure 2: An example of the few-shot instruction learning task adapted from Lake &amp; Baroni (2023).Study instructions illustrate the mapping of pseudolanguage expressions to abstract symbols.On the right, query instructions and their target responses are shown.</p>
<p>Figure 3 :
3
Figure 3: An episode from Compositional-ARC.Given a set of study examples with primitive transformations and level-1 transformation compositions, models must predict the output grid for a previously unseen level-2 transformation composition.Model predictions are presented to the right.</p>
<p>Figure 4
4
in the Appendix.This task evaluates the model's ability to infer geometric transformations from a limited set of examples.The second setup, denoted as "Systematicity," focuses on compositional generalization and differs from the first in the type of few-shot examples presented.As mentioned in Section 3.1, the idea is to test whether models can infer novel compositions from known geometric transformations.To this end, we replace the level-2 few-shot examples with a set of primitive transformations plus level-1 transformation compositions, and query the model to predict the previously unseen level-2 transformation composition, as illustrated in Figure3.Specifically, we present six primitive transformations-two examples for each indicator (shape-based, color-based, neighbor-based)-and six level-1 transformation compositions, two examples for each level-1 indicator composition (shape+color, shape+neighbor, color+neighbor).</p>
<p>B. 1 HYPERPARAMETERS
1
To identify suitable hyperparameters for model training, we conduct Bayesian search over a predefined range of values: learning rate ∈ [1 × 10 −2 , 1 × 10 −3 , 1 × 10 −4 ], final learning rate after linear decay ∈ [1 × 10 −4 , 5 × 10 −4 ], dropout rate ∈ [0.0, 0.1, 0.2], gradient accumulation over k ∈[1,2] batches, cell color perturbation probability p noise ∈ [0.0, 0.01, 0.001], feedforward hidden dimension ∈ [512, 768], loss weighting for background (all-black) patches ∈ [0.2, 0.4, 1.0], number of encoder layers ∈</p>
<p>Figure 4 :
4
Figure 4: An example of the few-shot learning task.Models are provided with three study examples that demonstrate the transformation that needs to be inferred for the final input grid.Model predictions are displayed to the right.</p>
<p>Figure 5 :
5
Figure 5: A second example of the few-shot learning task.Models are provided with three study examples that demonstrate the transformation that needs to be inferred for the final input grid.Model predictions are displayed to the right.</p>
<p>Figure 6 :
6
Figure 6: A third example of the few-shot learning task.Models are provided with three study examples that demonstrate the transformation that needs to be inferred for the final input grid.Model predictions are displayed to the right.</p>
<p>Figure 7 :
7
Figure 7: An episode from the systematicity task.Given a set of study examples comprising primitive transformations and level-1 transformation compositions, models are asked to predict the output grid for a previously unseen level-2 transformation composition.Predictions of different models are presented to the right.</p>
<p>Figure 9 :
9
Figure 9: An exemplary visual input used in the multimodal prompt for the 3-shot learning task.</p>
<p>Figure 10 :
10
Figure 10: An exemplary visual input used in the multimodal prompt for the systematicity task.</p>
<p>1 .
1
Figure 11: The prompt used for the few-shot experiment when instructing LLMs in (text-only) mode.Text enclosed in sharp brackets &lt; . . .&gt; is replaced by the actual examples.</p>
<p>1 .
1
Figure 12: The prompt used for the few-shot experiment when instructing LLMs in (text+image) mode.Text enclosed in sharp brackets &lt; . . .&gt; is replaced by the actual examples.Additionally, the model is provided with the image in Figure 9.</p>
<p>1 .
1
Figure 13: The prompt used for the systematicity experiment when instructing LLMs in (text-only) mode.Text enclosed in sharp brackets &lt; . . .&gt; is replaced by the actual examples.</p>
<p>1 .
1
Figure 14: The prompt used for the systematicity experiment when instructing LLMs in (text+image) mode.Text enclosed in sharp brackets &lt; . . .&gt; is replaced by the actual examples.Additionally, the model is provided with the image in Figure 10.</p>
<p>Table 1 :
1
Comparison of model performance across the two different task setups.We report exact match accuracy, color accuracy, and shape accuracy as described in Section 4.3).
ModelExact Match Accuracy [%]Color Accuracy [%]Shape Accuracy [%]GPT-4o22.2899.6757.02+ image19.4299.7554.56Gemini 2.0 Flash30.0899.9252.343-Shot+ image o3-mini (low)17.19 64.0499.79 99.8935.86 68.74Llama-3.2-3B-ReARC85.8598.5786.05Mistral-NeMO-Minitron-8B-Full95.7199.8596.78MLC (ours)99.92100.0099.92GPT-4o0.9999.239.82+ image0.8697.947.50SystematicityGemini 2.0 Flash + image o3-mini (low) Llama-3.2-3B-ReARC + test-time training2.66 2.05 0.53 0.87 73.7099.68 99.28 99.10 99.94 100.0012.81 9.60 5.65 2.54 86.88Mistral-NeMO-Minitron-8B-Full0.7099.999.75+ test-time training78.20100.0088.26MLC (ours)78.2697.8880.49
Due to financial constraints, each model is evaluated on a single test query for each of the 8,546 episodes in the test set.All textual and visual prompts, specific model versions, and decoding parameters are detailed in Appendix C.2.</p>
<p>Table 2 :
2
Average accuracy and standard deviation across the four different data splits.For the systematicity task, we ablate different components of the training procedure to assess their individual contributions and overall impact.
ModelExact Match Accuracy [%]Color Accuracy [%]Shape Accuracy [%]MLC (3-Shot)98.78 ± 1.99100.00 ± 0.0098.79 ± 1.98MLC (Systematicity)86.73 ± 6.0399.36 ± 0.7087.55 ± 5.45-no copy task69.05 ± 9.2399.43 ± 0.3870.60 ± 9.23-no primitives75.27 ± 12.9599.56 ± 0.5076.92 ± 11.23-no level-1 compositions21.01 ± 19.0794.72 ± 7.4123.03 ± 19.08</p>
<p>STATEMENTTo ensure the reproducibility of our work, we make all code publicly available at: https://github.com/mainlp/C-ARC.This enables users to reproduce the data described in Section 3.1 and train models via MLC for the task, as outlined in Section 3.2.Details about the training procedures and hyperparameters are provided in Section 3.2 and Appendix B. Specifics on prompts, model versions, and decoding parameters are given in Appendix C.2.Further details about the datasets can be found in Section 3.1, Section 4.1, and Appendix A. Finally, Appendix B.2 outlines the software and computational resources used for model training.</p>
<p>To this end, we introduce Compositional-ARC-a novel dataset designed to evaluate systematicity in this field.Our experiments demonstrate that models trained via MLC can systematically generalize to novel compositions of geometric transformations.Moreover, a small MLC model outperforms state-of-the-art general-purpose LLMs on Compositional-ARC, and performs on par with domain-specific LLMs trained via test-time training.Our findings suggest that MLC presents a promising direction for enabling systematic generalization in language models across diverse domains.REPRODUCIBILITY</p>
<p>Table 3 :
3
Hyperparameter configuration for models trained via MLC.
ParameterValueParameterValuenumber layers in decoder3learning rate after training5 × 10 −4number layers in decoder3dropout0.0number of attention heads 8weight decay0.01hidden dimension128noise probability0.001feedforward hidden size768gradient accumulation over k batches 2learning rate0.01background patch loss weight0.2episode follows a unique grammar, i.e., that no two combinations of shape, color, and indicatorobjects correspond to the same set of transformations within the dataset.A.4 DATASET STATISTICS</p>
<p>Table 5
5
presents detailed statistics for the datasets used in this study.As outlined in Section 5.1, we train and evaluate models via MLC across four distinct dataset splits to mitigate the influence of randomness in the data split process.The table includes the number of training, validation, and test samples for each split.Additionally, it provides information on the query transformation compositions present in the training and test sets, along with the frequency of each basic geometric transformation within the train dataset.</p>
<p>Table 4 :
4
The proportion of valid responses generated by the different models reported for the standard three-shot learning task and the systematicity task.For general-purpose LLMs, valid responses must contain the string "output:", followed by a two-dimensional 10 × 10 array of the form "[[. ..]]".
ModelValid Responses (3-Shot)Valid Responses (Systematicity)GPT-4o99.95%99.40%+ image99.80%77.24%Gemini 2.0 Flash99.92%99.74%+ image99.51%94.09%o3-mini (low)100%100%Llama-3.2-3B-ReARC100%100%+ test-time training-100%Mistral-NeMO-Minitron-8B-Full100%100%+ test-time training-100%MLC (ours)100%100%</p>
<p>1,260 training samples, 20 validation samples, and 20 test samples.Samples represent primitive transformations, as well as level-1 and level-2 transformation compositions.As with our other experiments, the test set includes level-2 transformation compositions that were not observed during training-only their constituent components and level-1 compositions were seen during training.For instance, the test set might include transformations composed of shape-based downward translation, color-based horizontal reflection, and neighbor-based upward extension.However, only their decomposed elements have been shown during training.
The model is trained for 200 epochs on the dataset using the parameters specified in Section B.While it successfully fits the training data (with an accuracy of over 99%), it fails to generalize tothe out-of-distribution test set, achieving a test accuracy of 0.0%. This demonstrates that traditionalmodel training, sample by sample, does not encourage systematic generalization to unseen composi-tions. Instead, systematicity requires a training procedure with examples over dynamically varyinginterpretation grammars, as described in Section 3.2.</p>
<p>Table 5 :
5
Summary of dataset statistics across different dataset splits, each determined by a distinct random seed.Listed are the number of episodes in the training, validation, and test sets.Additionally, the final query transformation compositions (level 2) are reported for both the training and evaluation datasets.The rightmost column details the frequency of each basic geometric transformation present in the training dataset.
Data SplitNo. EpisodesQuery TransformationsBasic TransformationsSetNo.Type CompositionTransformationFreq.Train 82908translation+reflection+coloringred coloring35828Val8546reflection+rotation+extensionorange coloring35819Test8546translation+reflection+rotationdown translation23398seed 1860Traintranslation+rotation+coloring reflection+coloring+extension reflection+rotation+coloringright translation leftward extension upward extension27021 22140 21806translation+coloring+extensioncw. rotation19551rotation+coloring+extensionccw. rotation19394Testtranslation+rotation+extension translation+reflection+extensionhorizontal reflection 21967 vertical reflection 21800Train 83481translation+rotation+extensionred coloring27603Val8259translation+reflection+rotationorange coloring27525Test8260reflection+rotation+extensiondown translation31385seed 1870Trainreflection+coloring+extension translation+reflection+extension translation+rotation+coloringright translation leftward extension upward extension36126 26501 25913translation+reflection+coloringcw. rotation15421translation+coloring+extensionccw. rotation15283Testrotation+coloring+extension reflection+rotation+coloringhorizontal reflection 22366 vertical reflection 22320Train 80035translation+coloring+extensionred coloring25850Val9982translation+rotation+extensionorange coloring25832Test9983translation+rotation+coloringdown translation31385seed 1880Trainreflection+rotation+extension translation+reflection+coloring translation+reflection+extensionright translation leftward extension upward extension36126 24821 24147translation+reflection+rotationcw. rotation19734rotation+coloring+extensionccw. rotation19594Testreflection+rotation+coloring reflection+coloring+extensionhorizontal reflection 16331 vertical reflection 16285Train 80557translation+coloring+extensionred coloring30227Val9721translation+reflection+rotationorange coloring30255Test9722rotation+coloring+extensiondown translation23279seed 1890Traintranslation+reflection+coloring reflection+rotation+extension translation+reflection+extensionright translation leftward extension upward extension24789 26483 26277reflection+coloring+extensioncw. rotation13949reflection+rotation+coloringccw. rotation13831Testtranslation+rotation+coloring translation+rotation+extensionhorizontal reflection 26329 vertical reflection 26252
https://arcprize.org/competitions/2024
https://platform.openai.com/docs/models/o3-mini
https://platform.openai.com/docs/models/gpt-4o
https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash
https://platform.openai.com/docs/api-reference/chat/create
https://huggingface.co/da-fr/Llama-3.2-3B-ARChitects-ReArc-bnb-4bit
https://huggingface.co/da-fr/Mistral-NeMo-Minitron-8B-ARChitects-Full-bnb-4bit
https://github.com/da-fr/arc-prize-2024
ACKNOWLEDGMENTSWe express our gratitude to the members of the MaiNLP lab for their invaluable feedback.Furthermore, we thank the anonymous reviewers for their insightful comments and suggestions.We gratefully acknowledge that experiments involving API calls to GPT-4o and o3-mini were supported by a compute grant from OpenAI.The authors also acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) under the NHR project b217dd.NHR funding is provided by federal and Bavarian state authorities.NHR@FAU hardware is partially funded by the German Research Foundation (DFG) -440719683.Finally, we acknowledge the support for BP through the ERC Consolidator Grant DIALECT 101043235.Query TargetInput Grid Output GridMLC (ours) Llama
We introduce Compositional-ARC-a novel dataset, inspired by ARC (Chollet, 2019), that evaluates systematic generalization in abstract spatial reasoning. The dataset includes examples of basic geometric transformations applied to abstract two-dimensional objects and tests generalization to unseen transformation compositions. see Figure 1</p>
<p>We demonstrate that MLC enables transformer-based models to generalize to unseen compositions of geometric transformations, demonstrating its potential beyond linguistic tasks. </p>
<p>We show that a 5.7M-parameter encoder-decoder model trained via MLC significantly outperforms state-of-the-art general-purpose LLMs such as o3-mini. Achiam, GPT-4o. OpenAI, 2025. 2023), and Gemini 2.0 Flash (DeepMind, 2024which fail to exhibit comparable systematic behavior on Compositional-ARC</p>
<p>We find that the same MLC model performs on par with the winning model of the ARC Prize 2024, an 8B-parameter LLM trained via test-time training. Franzen, 2024</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Introduction to Cellular Automata and Conway's Game of Life. Carter Bays, 10.1007/978-1-84996-217-9_12010SpringerLondon, London</p>
<p>Strong systematicity in sentence processing by simple recurrent networks. Philémon Brakel, Stefan Frank, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society200931</p>
<p>The architecture of cognition: Rethinking Fodor and Pylyshyn's systematicity challenge. Matthew F David A Brannan, Jeremy J Esplen, Gray, Geometry, Paco Calvo and John Symons. MIT Press2011. 2014</p>
<p>On the measure of intelligence. Chollet Franc, 2019</p>
<p>Noam Chomsky et al. Reflections on language. Noam Chomsky, 2002. 1976Mouton de GruyterTemple Smith LondonSyntactic structures</p>
<p>Meta-learning to compositionally generalize. Henry Conklin, Bailin Wang, Kenny Smith, Ivan Titov, doi: 10.18653Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>URL. </p>
<p>Evidence for compositional abilities in one-yearold infants. Isabelle Dautriche, Emmanuel Chemla, 10.1038/s44271-025-00222-9Communications Psychology. 2731-912131372025</p>
<p>On the emergence of compositionality. Joachim De, Beule , Benjamin K Bergen, The Evolution of Language. World Scientific2006</p>
<p>Gemini 2.0 flash. Google Deepmind, 2024</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 2021</p>
<p>Rl 2 : Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, Pieter Abbeel, arXiv:1611.027792016arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, ( Xiang, ) Lorraine, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>A learning progression for geometric transformations. H James, Kofi Fife, Malcolm James, Bauer, 10.1002/ets2.12236ETS Research Report Series. 201912019</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, org/10.1016/0010-0277(88)90031-5Cognition. 0010-02772811988</p>
<p>The llm architect: Solving the arc challenge is a matter of perspective. Jan Daniel Franzen, David Disselhoff, Hartmann, 2024</p>
<p>Large language models are not strong abstract reasoners. Gaël Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie, 10.24963/ijcai.2024/693Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI '24. the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI '242024</p>
<p>Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, 10.1038/s41586-025-09422-zNature. 1476-46876458081Sep 2025</p>
<p>Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). 2016arXiv preprint</p>
<p>The Oxford Handbook of Compositionality. Wolfram Hinzen, Edouard Machery, Markus Werning, 10.1093/oxfordhb/9780199541072.001.0001Oxford University Press</p>
<p>Addressing the abstraction and reasoning corpus via procedural example generation. Michael Hodel, arXiv:2404.073532024arXiv preprint</p>
<p>Meta-Learning in Neural Networks: A Survey. Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey, 10.1109/TPAMI.2021.3079209IEEE Transactions on Pattern Analysis &amp; Machine Intelligence. 1939-35394409September 2022</p>
<p>Compositionality decomposed: How do neural networks generalise. Dieuwke Hupkes, Verna Dankers, Mathijs Mul, Elia Bruni, Journal of Artificial Intelligence Research. 672020</p>
<p>Metalearning continual learning algorithms. Kazuki Irie, Róbert Csordás, Jürgen Schmidhuber, Transactions on Machine Learning Research. 2835-88562025</p>
<p>Evaluating morphological compositional generalization in large language models. Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Duygu Ataman, Lonneke Van Der, Plas, 10.18653/v1/2025.naacl-long.59Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Luis Chiruzzo, Alan Ritter, Lu Wang, the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language TechnologiesAlbuquerque, New MexicoAssociation for Computational LinguisticsApril 20251</p>
<p>Meta-learning representations for continual learning. Khurram Javed, Martha White, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>COGS: A compositional generalization challenge based on semantic interpretation. Najoung Kim, Tal Linzen, 10.18653/v1/2020.emnlp-main.731Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>Playgrounds for abstraction and reasoning. Subin Kim, Prin Phunyaphibarn, Donghyun Ahn, Sundong Kim, NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI). 2022</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. Brenden Lake, Marco Baroni, Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy, Andreas Krause, the 35th International Conference on Machine LearningPMLR10-15 Jul 201880</p>
<p>Compositional generalization through meta sequence-to-sequence learning. M Brenden, Lake, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Human-like systematic generalization through a metalearning neural network. M Brenden, Marco Lake, Baroni, 10.1038/s41586-023-06668-3Nature. 1476-468762379852023</p>
<p>Building machines that learn and think like people. M Brenden, Lake, D Tomer, Joshua B Ullman, Samuel J Tenenbaum, Gershman, 10.1017/S0140525X16001837Behavioral and Brain Sciences. 40e2532017</p>
<p>Recasting continual learning as sequence modeling. Soochan Lee, Jaehyeon Son, Gunhee Kim, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Combining induction and transduction for abstract reasoning. Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M Dunn, Hao Tang, Wei-Long Zheng, Yewen Pu, Kevin Ellis, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Rearranging the familiar: Testing compositional generalization in recurrent networks. João Loula, Marco Baroni, Brenden Lake, 10.18653/v1/W18-5413Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Tal Linzen, Grzegorz Chrupała, Afra Alishahi, the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsNovember 2018</p>
<p>Alignvlm: Bridging vision and language latent spaces for multimodal understanding. Marcus ; Gary, Ahmed Masry, Juan A Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Kalkunte Akshay, Abhay Suresh, Xiangru Puri, Pierre-André Jian, Sathwik Noël, Marco Tejaswi Madhusudhan, Bang Pedersoli, Nicolas Liu, Yoshua Chapados, Enamul Bengio, Hoque, Pal, Issam H. Laradji, David Vazquez, Perouz Taslakian, Spandana Gella, and Sai Rajeswar2003. 2025MIT pressThe algebraic mind: Integrating connectionism and cognitive science</p>
<p>A simple neural attentive metalearner. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel, International Conference on Learning Representations. 2018</p>
<p>The conceptARC benchmark: Evaluating understanding and generalization in the ARC domain. Arsenii Kirillovich Moskvichev, Victor Vikram Odouard, Melanie Mitchell, Transactions on Machine Learning Research. 2023</p>
<p>Openai o3-mini system card. Openai, 2024. January 2025Openai o1 system card</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>From generation to selection: Findings of converting analogical problem-solving into multiple-choice questions. Donghyeon Shin, Seungpil Lee, Klea , Lena Kovacec, Sundong Kim, 10.18653/v1/2024.findings-emnlp.392Findings of the Association for Computational Linguistics: EMNLP 2024. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, Miami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>The case for compositionality. Zoltán Gendler, Szabó , The Oxford Handbook of Compositionality. Markus Werning, Wolfram Hinzen, Edouard Machery, Oxford University Press2012</p>
<p>Learning to Learn: Introduction and Overview. Sebastian Thrun, Lorien Pratt, 10.1007/978-1-4615-5529-2_11998Springer USBoston, MA</p>
<p>Learning to reinforcement learn. Jane Wang, Zeb Kurth-Nelson, Hubert Soyer, Joel Leibo, Dhruva Tirumala, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society201739</p>
<p>LLMs and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, Elias Boutros Khalil, Transactions on Machine Learning Research. 2835-88562024</p>
<p>Compositional diversity in visual concept learning. Yanli Zhou, Reuben Feinman, Brenden M Lake, 10.1016/j.cognition.2023.105711.URLhttps://www.sciencedirect.com/science/article/pii/S0010027723003451Cognition. 0010-02772441057112024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1758 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1758</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1758</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-d238a9770d24d0725656ef6cf4789afebf2126e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d238a9770d24d0725656ef6cf4789afebf2126e7" target="_blank">TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task and can even surpass the best existing reference-based metrics.</p>
                <p><strong>Paper Abstract:</strong> We present TIGERScore, a \textbf{T}rained metric that follows \textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and \textbf{R}eference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output $\rightarrow$ error analysis). We collected the `system outputs' through from a large variety of models to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that TIGERScore can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8\% accurate. Through these experimental results, we believe TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task. All the resourced are released in our project website: \url{https://tiger-ai-lab.github.io/TIGERScore/}.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1758",
    "paper_id": "paper-d238a9770d24d0725656ef6cf4789afebf2126e7",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0061589999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks</h1>
<p>${ }^{\bullet}$ Dongfu Jiang<em>, ${ }^{\bullet}$ Yishan $\mathrm{Li}^{</em>}$, ${ }^{\bullet}$ Ge Zhang, ${ }^{\circledR}$ Wenhao Huang, ${ }^{\bullet}$ Bill Yuchen Lin, ${ }^{\bullet}$ Wenhu Chen ${ }^{\bullet}$ University of Waterloo, ${ }^{\bullet}$ Tsinghua University, ${ }^{\circ} 01 . \mathrm{AI},{ }^{\bullet}$ Allen Institute for AI {dongfu.jiang, wenhuchen}@uwaterloo.ca, liyisha19@mails.tsinghua.edu.cn</p>
<p>Reviewed on OpenReview: https://openreview.net/forum?id=EE1CBKC0SZ</p>
<h4>Abstract</h4>
<p>We present TIGERSCORE ${ }^{1}$, a Trained metric that follows Instruction Guidance to perform Explainable, and Reference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42 K quadruple in the form of (instruction, input, system output $\rightarrow$ error analysis). We collected the 'system outputs' through from a large variety of models to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that TIGERSCORE can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are $70.8 \%$ accurate. Through these experimental results, we believe TIGERSCORE demonstrates the possibility of building universal explainable metrics to evaluate any text generation task.</p>
<h2>1 Introduction</h2>
<p>Evaluation for natural language generation tasks is a long-standing challenging problem. With the recent advancement of large pre-trained language models (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023), text generation models have become more widely adopted in real-world applications than ever. Newly developed text-generative models are being deployed across a wide range of downstream applications. As more and more people use these generative models, there is an increasing need to develop trustworthy evaluation metrics. Though GPT-4 has shown to achieve strong evaluation correlation with humans (Zheng et al., 2023), the open-sourced metrics are lagging significantly behind and mainly suffer from specific issues:</p>
<p>Dependency on references: Some evaluation metrics like ROUGE (Lin, 2004), BLEU (Papineni et al., 2002b), COMET (Rei et al., 2020), InstructScore (Xu et al., 2023c) would require gold references to measure the quality. These metrics compare the generated output against one or more reference texts to assign the evaluation score. However, such an assumption can be highly unrealistic in many downstream applications where the gold reference is hard to collect.
Limited to specific domains: Some evaluation metrics are limited to specific domains, lacking the ability to generalize to broader text generation tasks. For example, COMET (Rei et al., 2020), BLEURT (Sellam</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The upper part shows the input and output format of TIGERSCORE, which takes an instruction and an input context, along with the model-generated output that is to be evaluated as the inputs, and output detailed breakdown error analysis with explanations. It is finetuned on our curated dataset MetricInstruct. The lower part shows the Kendall's correlation of different metrics w.r.t human ratings, from which we can see TIGERSCORE achieves the SoTA correlation on most generation tasks.
et al., 2020), SESCORE2 (Xu et al., 2023b) and InstructScore (Xu et al., 2023b) are specifically designed for tasks like machine translation or WebNLG tasks.</p>
<p>Lack of attribution: Some evaluation metrics tend to directly output a score without any attributions, e.g., where the errors occur and why. For instance, BARTScore (Yuan et al., 2021), BERTScore (Zhang et al., 2019) and GPTScore (Fu et al., 2023) adopt the pre-trained language models' log likelihood as the evaluation metric. Such metrics do not provide the location and reason for the assigned score, which limits their trustworthiness and reliability.</p>
<p>To address these issues, we propose a novel metric, TIGERSCORE, a Trained metric that follows Instruction Guidance to perform Explainable and Reference-free evaluation. As shown in Figure 1, the input to TIGERSCORE consists of an instruction describing the task definition, the task input, and the system output. TIGERSCORE is capable of generating breakdown error analysis that can (1) locate each mistake, (2) explain the mistake and suggest revisions, and (3) provide a penalty score (between $[-5,-0.5]$ ) for each mistake. The final score for the system output can be calculated by summing up all the penalty scores.</p>
<p>TIGERSCORE is built by fine-tuning LLMs on our curated MetricInstruct dataset, which contains a total of 42 K examples of (instruction, input, system output, error analysis), obtained from 23 diverse text generation datasets. The dataset includes system outputs from more than 50 systems, covering a wide variety of errors. The error analysis is curated by prompting GPT-4 (OpenAI, 2023) and filtered through various strategies. The tuned model TIGERSCORE has shown the highest overall Kendall's correlation with human ratings on seven major text generation tasks as depicted in Figure 1. TIGERSCORE is highly convenient to use because it does not require any additional reference. We evaluate TIGERSCORE on 5 held-in datasets and 2 held-out datasets. As a reference-free metric, TIGERSCORE surpasses the best reference-free metrics (GPTScore (Fu et al., 2023)) by $15 \%$ and surpasses the best reference-based metrics (COMET-22 (Rei et al., 2022a)) by a margin of $8 \%$ in terms of Kendall's score. On the two held-out datasets, TIGERSCORE demonstrates unprecedented generalization capabilities. We further employ humans to evaluate the explanations</p>
<p>generated by TIGERSCORE and found that over $70 \%$ of the generated explanations are highly accurate and trustworthy. Our analysis shows that the success of TIGERSCORE is attributed to three key aspects in our curated MetricInstruct: (1) dataset diversity, (2) error coverage, and (3) high quality, which enable TIGERSCORE to generalize better on unseen tasks than any other metric.</p>
<h1>2 TIGERScore</h1>
<p>TIGERSCORE is built upon three design criteria: (1) It is driven by instructions, making it easily adaptable to any text generation task. (2) The evaluation process eliminates the need for a "gold standard" or perfect example for comparison. (3) It is highly explainable, as the model can generate an error analysis that helps the user understand each identified mistake and its associated penalty.</p>
<h3>2.1 Background</h3>
<p>The pursuit of improved metrics for text evaluation has been a significant focus since the inception of language models. Automatic n-gram-based metrics (Elliott \&amp; Keller, 2014; Callison-Burch et al., 2006; Isozaki et al., 2010) have always served as the default metric, computing the n-gram match F-1 score with the reference text until research highlighted their significant weaknesses in aligning with human preferences. Later, neural metrics were proposed to better capture the semantic similarity in addition to mere morphological similarity only by either computing based on neural representation (Zhang et al., 2019; Yuan et al., 2021) or directly fine-tuning with human preferences (Rei et al., 2020). It has also been demonstrated that multi-aspect scores, using the logits of large language models with well-designed instructions for various aspects as prompts (Fu et al., 2023), could achieve an even higher correlation.</p>
<p>There have been some attempts to build explainable metrics leveraging the great capacity of large language models. For instance, UniEval (Zhong et al., 2022a) constructs a multi-aspect evaluation system by individually training on aspect-specific data. PandaLM (Wang et al., 2023a) compares two responses for a given instruction and input to judge which is better, providing a short reason for its decision. InstructScore (Xu et al., 2023c) evaluates the quality of translation by training Llama-2 to compare the reference and translation, listing errors with structured information. However, none of these metrics has been able to address all the issues mentioned in section 1 concurrently.</p>
<h3>2.2 Problem Formulation</h3>
<p>Suppose $y^{\prime}$ is the system output from a given source context $x$ with a specific natural language instruction $I$ to describe the task definition. And $y$ is a corresponding reference output. If a metric uses $y$, it's referencebased, otherwise, it's reference-free. For example, when $T$ refers to "translation", an instruction $I$ for that task could be "Translate the following text from Chinese to English". For each task type $T$, we ask the evaluation metric to focus on a few pre-defined evaluation aspects $A_{T}$ like relevance, factuality, fluency, etc.
TIGERSCORE is a reference-free metric, defined by a function $f$ to take the triple $\left(I, x, y^{\prime}\right)$ as input, to produce a list of structured errors $\left{E_{1}, \ldots, E_{m}\right}$ as the otuput, called the error analysis content:</p>
<p>$$
\left{\ldots, E_{i}, \ldots\right}=\left{\ldots,\left(l_{i}, a_{i}, e_{i}, s_{i}\right), \ldots\right}=f\left(I, x, y^{\prime}\right)
$$</p>
<p>where $\left(l_{i}, a_{i}, e_{i}, s_{i}\right)$ denotes specific information of the error $E_{i}$. Specifically, $l_{i}$ points to the location of the error, and $a_{i} \in A_{T}$ is a pre-defined aspect to which this error belongs. $e_{i}$ comprises both the explanation of this error and its revision suggestion. $s_{i}$ is the penalty score reduced for this error, which lies in $[-5,-0.5]$. The final score of $y^{\prime}$ is computed as the sum of all the penalty scores: $s_{y^{\prime}}=\sum_{i} s_{i}$. The range of the final score lies within $(-00,0]$, where 0 means perfect generation and a lower score indicates worse quality and more severe errors. However, in practice, the number of detected errors is affected by both the training data and the generation length, thus leading to less than 10 errors most of the time during the evaluation.</p>
<h3>2.3 Multi-Aspect Evaluation</h3>
<p>As stated in the problem formulation, each error is considered to be attributed to a certain aspect that the system output might make mistakes on. For each task, 3 or 4 aspects are designed with the help of both</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overall pipeline of constructing MetricInstruct through the two-channel collection. The main difference is that the real-world channel collects outputs $y^{t}$ from real-world models, while the synthetic channel asks GPT-4 to synthesize an error output based on the provided reference.</p>
<p>GPT-4 prompting and human supervision, detailed in subsection A.6. These aspects are designed to be both non-overlapping (mutually exclusive) and collectively exhaustive. For instance, 4 aspects to evaluate an output for the instruction-following tasks is shown in Table 1.</p>
<h1>2.4 Training Setup</h1>
<p>TIGERSCORE is finetuned on Llama-2-7B and Llama-2-13B (Touvron et al., 2023) respectively with both's batch size being 128. The prompt templates used for both the fine-tuning and inference are given in Table 9. The model's maximum context length is set to 1024 . We use a cosine learning scheduler with the warmpup ratio being 0.1 We finetune the 7 B version on 4 A100 GPUs ( 80 GB ) for 3 epochs with a learning rate of $2 \mathrm{e}-5$. And 13 B version is run on 8 A100 GPUs ( 80 GB ) for 2 epochs with a learning rate of $1 \mathrm{e}-5$. Inference of TIGERSCORE is conducted on a single A100 GPU with the assistance of the VLLM toolkit to increase the speed. (Kwon et al., 2023)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Instruct</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Aspect</td>
<td style="text-align: left;">Definition</td>
</tr>
<tr>
<td style="text-align: left;">Comprehension</td>
<td style="text-align: left;">Evaluates how well the output understands the given instruction.</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy</td>
<td style="text-align: left;">Measures the correctness of the output in relation to the instruction and the paired input context.</td>
</tr>
<tr>
<td style="text-align: left;">Informativeness</td>
<td style="text-align: left;">Assesses the relevancy and usefulness of the information provided by the output.</td>
</tr>
<tr>
<td style="text-align: left;">Coherence</td>
<td style="text-align: left;">Evaluates how logically the output flows and connects.</td>
</tr>
</tbody>
</table>
<p>Table 1: Definitions of evaluation aspects of TIGERSCORE for Instruction-following (Instruct) as an example. See full table in Table 10 for aspect definitions of all 6 text generation tasks</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Real-World (training set)</th>
<th></th>
<th></th>
<th>Synthetic (training set)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Dataset</td>
<td>Output Source</td>
<td># Sample</td>
<td>Dataset</td>
<td>Output Source</td>
<td># Sample</td>
</tr>
<tr>
<td>Summarization (Summ)</td>
<td>SummEval, XSum, Newsroom,SAMSum</td>
<td>27 Systems</td>
<td>4339</td>
<td>CNN/DM,XSum, Gigaword,SAMSum</td>
<td>GPT-4</td>
<td>612</td>
</tr>
<tr>
<td>Translation (Trans)</td>
<td>WMT-22</td>
<td>18 Systems</td>
<td>5507</td>
<td>WMT-22</td>
<td>GPT-4</td>
<td>672</td>
</tr>
<tr>
<td>Data2Text (D2T)</td>
<td>WebNLG-2020, WikiTableText,ToTTo</td>
<td>17 Systems</td>
<td>4701</td>
<td>WikiTableText Dart,ToTTo</td>
<td>GPT-4</td>
<td>160</td>
</tr>
<tr>
<td>Long-Form QA (LF-QA)</td>
<td>ASQA,FeTaQA CosmosQA,ELI5</td>
<td>5 Systems</td>
<td>3370</td>
<td>ASQA,FeTaQA Cosmos QA,ELI5</td>
<td>GPT-4</td>
<td>2146</td>
</tr>
<tr>
<td>MathQA</td>
<td>GSM8K</td>
<td>5 Systems</td>
<td>4529</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Instruction-Following (Instruct)</td>
<td>MixInstruct</td>
<td>11 Systems</td>
<td>2248</td>
<td>AlpacaFarm,Dolly Guanaco,OASST</td>
<td>GPT-4</td>
<td>3014</td>
</tr>
</tbody>
</table>
<p>Table 2: The composition of our dataset. For synthetic data, the output is generated by asking GPT-4 to synthesize incorrect outputs that contain a few specific types of errors. For the datasets with , we take their pre-released system outputs. For the others, we collect the system outputs on our own.</p>
<h1>3 MetricInstruct</h1>
<p>We present the MetricInstruct dataset, which is employed to fine-tune TIGERSCORE. The three underlying criteria for dataset construction are: (1) dataset diversity: we choose 23 distinctive datasets as the source context to cover enough generation tasks. (2) error coverage: we take system outputs generated from $50+$ text generation systems to cover all types of errors and guarantee a balanced distribution. (3) quality ensurance: to ensure MetricInstruct is tailored to gather in-depth error analysis as detailed in subsection 2.2, we sourced it by prompting GPT-4 (OpenAI, 2023) and then filtered through different heuristics to eliminate low-quality error analysis.</p>
<h3>3.1 Diverse Dataset Source</h3>
<p>MetricInstruct incorporates samples from 23 distinctive text generation datasets, which are categorized into 6 major categories of text generation tasks. While the collection encompasses well-researched tasks such as summarization (Summ), translation (Trans), and data2text (D2T), it also introduces popular new tasks like Long-Form QA (LF-QA), MathQA, and instruction-following (Instruct). These latter tasks have witnessed limited evaluation research. Although the assessment of traditional tasks has dominated the research landscape, we posit that effectively evaluating these new tasks is crucial for constructing a comprehensive evaluator for all text generation domains.</p>
<p>In addition, we meticulously selected datasets for each task to ensure diverse coverage across the knowledge domain, as shown in Table 2. For instance, in the case of the LF-QA task, we utilize ASQA (Stelmakh et al., 2022b) to include knowledge about ambiguous factoid questions, while FeTaQA (Nan et al., 2022b) is employed to handle challenges related to tabular source question answering. We then selected samples from each dataset's training set, following the predefined constraints like maximum input and reference output lengths, topics balancing of instances, and so on.</p>
<h3>3.2 Broad Coverage of Errors</h3>
<p>As shown in Figure 2, our "system outputs" come from two channels, namely real-world system outputs and synthetic incorrect outputs, representing 2 main components of MetricInstruct.</p>
<p>We consider a wide range of systems and use their outputs as our evaluation input, as shown in Table 2. These outputs are either collected from pre-released outputs, like WMT-2021 official evaluation systems, or generated by us by prompting existing domain-specialized models, like BRIO (Liu et al., 2022), WizardMath (Luo et al., 2023), etc.</p>
<p>As illustrated in Figure 2, for those self-generated outputs, we sample 5 different outputs using top-p sampling for each instance using various output systems. After that, we use BARTScore to sort the 5</p>
<p>outputs. Outputs with lower BARTScore are heuristically considered to contain more errors and will be preferred to be chosen as the final hypothesis output $y^{\prime}$ to be evaluated.</p>
<p>Subsequently, we utilized meticulously designed prompting templates (see in A.7) to elicit standardized error analysis from GPT-4. The main idea is to provide them with the instruction $I$, input $x$, the reference output $y$, and system output $y^{\prime}$ along with the definitions of pre-defined aspects $A_{T}$, to query the OpenAI models to generate the list of errors, as described in subsection 2.2. We also report their correlation performance in Table 4, showcasing the reference-based ChatGPT results.</p>
<p>Synthetic: While real-world system outputs ensure the error distribution is aligned with practical scenarios, they might be overly tailored to the bias of these limited systems and omit certain error cases their outputs fail to cover. Therefore, we decided to synthesize more incorrect outputs that can balance under-represented errors by prompting GPT-4 with our designed templates.</p>
<p>Firstly, to complement the real-world system outputs to cover broader error cases, we prompt GPT-4 to deliberately generate designated erroneous outputs, modified from the existing reference output $y$, using the prompt template in Table 18. By supplying GPT-4 with a combination of randomly selected aspects and their definitions $A_{T}^{\prime}$, we control the aspect of errors it produces so that error aspects can be more balanced. Datasets used in this step are reported in the right column of Table 2.</p>
<p>Secondly, to improve TIGERSCORE's generalization capability in evaluation for tasks or errors it has not previously encountered, we employ a strategy that involves prompting GPT-4 to generate data with customized error aspects tailored to individual instructions. Our rationale is that while the manually designed aspects are logical, they may not be specific enough for tasks that involve following instructions where various types of instructions will be included. Besides, this can also prevent the trained model from being overfitted to the pre-defined aspects in subsection A.6. To do this, we supplement the first part of synthetic data by sampling 10k data points from the Alpaca-52k dataset and using the template in Table 19 to prompt GPT-4 to get this second part of synthetic data.</p>
<h1>3.3 Heurstic-based Filtering</h1>
<p>We refine our preliminary raw dataset drawn from both real-world and synthetic channels using heuristic filtering techniques. Initially, we remove any anomalous data in JSON format. Instances marked by hallucinated or mismatched error locations, illogical severity labels, or excessively long outputs are considered flawed and excluded. Furthermore, given the reference-free nature of TIGERSCORE, an explanation that relies on reference outputs for justification is excluded. These steps eliminate approximately $35 \%$ of the initial data. Subsequently, we employ GPT-4 to assess the reasonableness of our error analysis, utilizing the template outlined in Table 20. This process further filtered out about $15 \%$ percentage data. This phase further excludes about $15 \%$ of the data. Ultimately, we compile a high-quality dataset of 42,484 instances, designated as MetricInstruct.</p>
<h2>4 Experiments</h2>
<h3>4.1 Evaluation Datasets</h3>
<p>We have gathered both the held-in and held-out datasets to compare the performance of TIGERSCORE with the existing baseline metrics. The basic statistics of some main datasets for each task are shown in Table 3. System outputs to be evaluated of each test dataset are either from official releases, such as SummEval (Fabbri et al., 2021), WebNLG-2020 (Zhou \&amp; Lampouras, 2020), WMT-22 (Freitag et al., 2022), and OpenMEVA (Guan et al., 2021), or by generating by ourselves, such as A-F-E-C (Stelmakh et al., 2022a; Nan et al., 2022a; Fan et al., 2019; Huang et al., 2019), GSM8K (Cobbe et al., 2021), LIMA (Zhou et al., 2023) and AlpacaEval (Li et al., 2023).</p>
<p>Human preference scores are necessary to conduct the correlation analysis. To get the gold preference scores we used to conduct the evaluation experiments, we collected either existing human ratings like WMT-22</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Eval Dataset</th>
<th>Output Source</th>
<th># Inputs</th>
<th># Samples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Held-in Evaluation Dataset (test set)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Summ</td>
<td>SummEval</td>
<td>16 Systems</td>
<td>100</td>
<td>1600</td>
</tr>
<tr>
<td>Trans</td>
<td>WMT-22 (zh-en)</td>
<td>18 Systems</td>
<td>1875</td>
<td>33750</td>
</tr>
<tr>
<td>D2T</td>
<td>WebNLG-2020</td>
<td>18 Systems</td>
<td>179</td>
<td>2848</td>
</tr>
<tr>
<td>LF-QA</td>
<td>ASQA, FeTaQA, CosmosQA, ELI5</td>
<td>4 Systems</td>
<td>400</td>
<td>1600</td>
</tr>
<tr>
<td>Math QA</td>
<td>GSM8K</td>
<td>2 Systems</td>
<td>1319</td>
<td>2638</td>
</tr>
<tr>
<td>Held-out Evaluation Dataset (test set)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Instruct</td>
<td>LIMA,AlpacaEval</td>
<td>9 Systems</td>
<td>500</td>
<td>4500</td>
</tr>
<tr>
<td>Story-Gen</td>
<td>OpenMEVA (ROC)</td>
<td>5 Systems</td>
<td>200</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>Table 3: Overview of our main evaluation datasets. "# Samples" is computed by multipling "# Inputs" with the number of outputs systems, representing the total instances that a metric need to compute.</p>
<p>MQM scores for translation, or GPT-4 rated scores for Long-form QA. Details of the curation are shown in subsection A.5.</p>
<h3>4.2 Baselines</h3>
<p>We categorize the baselines into reference-based and reference-free metrics for fair comparison.</p>
<p>Reference-based: We choose popular metrics, including BLEU (Papineni et al., 2002a), ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020) and BARTScore (Yuan et al., 2021). Recent emerging metrics are also included, like COMET-22 (Rei et al., 2022a), UniEval (Zhong et al., 2022b), GPTScore (Fu et al., 2023), and InstructScore (Xu et al., 2023c). Specifically, we use BARTScore-ref to denote that we adopt the ref-hypo scoring type. For GPTScore, we use FLAN-T5-base Chung et al., 2022) as base models and use GPTScore-ref to denote that we adopt the f-1 average of the ref-hypo and hypo-ref scores. We also report the zero-shot results of GPT-3.5-turbo with the same prompting templates as those used for generating real-world channel data. Reporting this baseline will help us understand whether TIGERSCORE has surpassed its easy substitute using the cheap OpenAI model, thus proving its effectiveness.</p>
<p>Reference-free: We choose BARTScore, GPTScore and COMETKiwi (Rei et al., 2022b) as reference-free baselines to compare. Specifically, we use BARTScore-src to denote the src-hypo scoring type, thus making it a reference-free metric. For GPTScore, we still use the FLAN-T5-base model and use GPTScore-src to denote that we use the src-hypo scoring type. We also include the frequently used 0-shot results of Llama-2-13b (Touvron et al., 2023) and GPT-4 (OpenAI, 2023)for better comparison and to know more about the performance gaps with GPT-4.</p>
<h3>4.3 Main Results</h3>
<p>We present a comprehensive analysis of TIGERSCORE across all 5 held-in tasks and 2 held-out task in Table 4 reporting Kendall correlation results. Additionally, we provide supplementary results on Pearson and Spearman correlations in the Appendix (see Table 25 and Table 26). We average the performance across these tasks to gauge the general ability of the model.</p>
<p>Our results highlight the significant advantages of TIGERSCORE over other reference-free metrics. Notably, it has surpassed all other reference-free metrics in Kendall correlation. In Pearson correlation, it is the highest for 6 out of 7 tasks. This underscores the robustness and consistency of TIGERSCORE in evaluating text generation tasks.</p>
<p>Compared with reference-based baselines, TIGERSCORE generally outperforms most reference-based metrics. However, it does score lower than some task-specific metrics like UniEval (summ) in summarization,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks $\rightarrow$</th>
<th style="text-align: center;">Summ</th>
<th style="text-align: center;">Trans</th>
<th style="text-align: center;">D2T</th>
<th style="text-align: center;">LF-QA</th>
<th style="text-align: center;">MathQA</th>
<th style="text-align: center;">Instruct</th>
<th style="text-align: center;">Story-Gen</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-based Metrics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo (zero-shot)</td>
<td style="text-align: center;">30.45</td>
<td style="text-align: center;">32.30</td>
<td style="text-align: center;">30.38</td>
<td style="text-align: center;">20.91</td>
<td style="text-align: center;">58.57</td>
<td style="text-align: center;">17.73</td>
<td style="text-align: center;">3.26</td>
<td style="text-align: center;">27.65</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (zero-shot)</td>
<td style="text-align: center;">29.32</td>
<td style="text-align: center;">35.38</td>
<td style="text-align: center;">32.26</td>
<td style="text-align: center;">35.85</td>
<td style="text-align: center;">46.63</td>
<td style="text-align: center;">49.50</td>
<td style="text-align: center;">25.69</td>
<td style="text-align: center;">36.38</td>
</tr>
<tr>
<td style="text-align: center;">Reference-based Metrics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">8.71</td>
<td style="text-align: center;">14.50</td>
<td style="text-align: center;">23.13</td>
<td style="text-align: center;">7.73</td>
<td style="text-align: center;">17.25</td>
<td style="text-align: center;">35.92</td>
<td style="text-align: center;">$-0.89$</td>
<td style="text-align: center;">15.19</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2f</td>
<td style="text-align: center;">10.67</td>
<td style="text-align: center;">13.19</td>
<td style="text-align: center;">24.74</td>
<td style="text-align: center;">11.73</td>
<td style="text-align: center;">18.07</td>
<td style="text-align: center;">34.59</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">16.40</td>
</tr>
<tr>
<td style="text-align: center;">InstructScore</td>
<td style="text-align: center;">20.86</td>
<td style="text-align: center;">40.44</td>
<td style="text-align: center;">30.21</td>
<td style="text-align: center;">15.64</td>
<td style="text-align: center;">$-3.87$</td>
<td style="text-align: center;">13.87</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">18.66</td>
</tr>
<tr>
<td style="text-align: center;">GPTScore-ref</td>
<td style="text-align: center;">10.80</td>
<td style="text-align: center;">18.74</td>
<td style="text-align: center;">27.47</td>
<td style="text-align: center;">22.13</td>
<td style="text-align: center;">14.86</td>
<td style="text-align: center;">25.40</td>
<td style="text-align: center;">12.78</td>
<td style="text-align: center;">18.88</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore-cnn (hypo-ref)</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">21.06</td>
<td style="text-align: center;">27.04</td>
<td style="text-align: center;">20.67</td>
<td style="text-align: center;">19.07</td>
<td style="text-align: center;">24.70</td>
<td style="text-align: center;">18.58</td>
<td style="text-align: center;">20.16</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore-para (hypo-ref)</td>
<td style="text-align: center;">10.41</td>
<td style="text-align: center;">24.90</td>
<td style="text-align: center;">28.42</td>
<td style="text-align: center;">20.24</td>
<td style="text-align: center;">14.10</td>
<td style="text-align: center;">26.13</td>
<td style="text-align: center;">12.11</td>
<td style="text-align: center;">19.47</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">17.39</td>
<td style="text-align: center;">31.57</td>
<td style="text-align: center;">30.74</td>
<td style="text-align: center;">17.70</td>
<td style="text-align: center;">9.41</td>
<td style="text-align: center;">35.61</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">20.63</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">12.69</td>
<td style="text-align: center;">36.12</td>
<td style="text-align: center;">34.48</td>
<td style="text-align: center;">23.11</td>
<td style="text-align: center;">2.88</td>
<td style="text-align: center;">27.94</td>
<td style="text-align: center;">19.18</td>
<td style="text-align: center;">22.34</td>
</tr>
<tr>
<td style="text-align: center;">UniEval (summ)</td>
<td style="text-align: center;">35.89</td>
<td style="text-align: center;">16.08</td>
<td style="text-align: center;">28.56</td>
<td style="text-align: center;">29.32</td>
<td style="text-align: center;">16.15</td>
<td style="text-align: center;">11.93</td>
<td style="text-align: center;">31.22</td>
<td style="text-align: center;">24.17</td>
</tr>
<tr>
<td style="text-align: center;">COMET-22</td>
<td style="text-align: center;">25.01</td>
<td style="text-align: center;">42.79</td>
<td style="text-align: center;">23.43</td>
<td style="text-align: center;">24.66</td>
<td style="text-align: center;">$-4.52$</td>
<td style="text-align: center;">36.17</td>
<td style="text-align: center;">27.52</td>
<td style="text-align: center;">25.01</td>
</tr>
<tr>
<td style="text-align: center;">Reference-free Metrics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BARTScore-para (src-hypo)</td>
<td style="text-align: center;">29.12</td>
<td style="text-align: center;">7.01</td>
<td style="text-align: center;">22.32</td>
<td style="text-align: center;">18.80</td>
<td style="text-align: center;">$-2.21$</td>
<td style="text-align: center;">4.26</td>
<td style="text-align: center;">14.15</td>
<td style="text-align: center;">13.35</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore-cnn (src-hypo)</td>
<td style="text-align: center;">26.63</td>
<td style="text-align: center;">9.40</td>
<td style="text-align: center;">23.69</td>
<td style="text-align: center;">28.93</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">19.09</td>
<td style="text-align: center;">23.29</td>
<td style="text-align: center;">18.89</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-13b-chat-0-shot</td>
<td style="text-align: center;">25.22</td>
<td style="text-align: center;">11.79</td>
<td style="text-align: center;">23.45</td>
<td style="text-align: center;">15.96</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">19.50</td>
<td style="text-align: center;">21.52</td>
<td style="text-align: center;">16.93</td>
</tr>
<tr>
<td style="text-align: center;">COMETKiwi</td>
<td style="text-align: center;">11.87</td>
<td style="text-align: center;">36.37</td>
<td style="text-align: center;">19.08</td>
<td style="text-align: center;">12.23</td>
<td style="text-align: center;">$-9.38$</td>
<td style="text-align: center;">26.46</td>
<td style="text-align: center;">12.78</td>
<td style="text-align: center;">15.63</td>
</tr>
<tr>
<td style="text-align: center;">GPTScore-src</td>
<td style="text-align: center;">28.20</td>
<td style="text-align: center;">6.50</td>
<td style="text-align: center;">19.81</td>
<td style="text-align: center;">27.64</td>
<td style="text-align: center;">11.64</td>
<td style="text-align: center;">20.04</td>
<td style="text-align: center;">16.36</td>
<td style="text-align: center;">18.60</td>
</tr>
<tr>
<td style="text-align: center;">TIGERScore-7B</td>
<td style="text-align: center;">28.79</td>
<td style="text-align: center;">33.65</td>
<td style="text-align: center;">32.44</td>
<td style="text-align: center;">33.93</td>
<td style="text-align: center;">19.98</td>
<td style="text-align: center;">38.13</td>
<td style="text-align: center;">29.72</td>
<td style="text-align: center;">30.95</td>
</tr>
<tr>
<td style="text-align: center;">TIGERScore-13B</td>
<td style="text-align: center;">31.29</td>
<td style="text-align: center;">36.50</td>
<td style="text-align: center;">36.43</td>
<td style="text-align: center;">33.17</td>
<td style="text-align: center;">21.58</td>
<td style="text-align: center;">41.84</td>
<td style="text-align: center;">35.33</td>
<td style="text-align: center;">33.73</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$ (ours - best reference-free)</td>
<td style="text-align: center;">$+2$</td>
<td style="text-align: center;">$+0$</td>
<td style="text-align: center;">$+13$</td>
<td style="text-align: center;">$+4$</td>
<td style="text-align: center;">$+10$</td>
<td style="text-align: center;">$+15$</td>
<td style="text-align: center;">$+14$</td>
<td style="text-align: center;">$+15$</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$ (ours - best reference-based)</td>
<td style="text-align: center;">$-4$</td>
<td style="text-align: center;">$-6$</td>
<td style="text-align: center;">$+2$</td>
<td style="text-align: center;">$+4$</td>
<td style="text-align: center;">$+2$</td>
<td style="text-align: center;">$+5$</td>
<td style="text-align: center;">$+4$</td>
<td style="text-align: center;">$+8$</td>
</tr>
</tbody>
</table>
<p>Table 4: The Kendall correlation results of all the baseline metrics and TIGERSCORE on the evaluation datasets shown in Table 3. For each task, the metric with the highest correlation to average performance is highlighted in bold. The results of Pearson and Spearman are reported in Table 25 and Table 26 respectively, showcasing the same great performance of TIGERSCORE</p>
<p>COMET-22 in translation. We consider these discrepancies acceptable, because the compared metrics are reference-based and all specifically fine-tuned for a single task. Furthermore, TIGERSCORE achieves significantly higher overall correlation, compared with the cheap API substitute GPT-3.5-Turbo (zero-shot) and the Llama-2-13b-chat (zero-shot), proving its effectiveness. What's exciting to note is that TIGERSCORE13 b can achieve comparable correlation performance with GPT-4 (zero-shot), even higher on summarization, translation, data2text, and story generation.</p>
<h1>4.4 Human Evaluation</h1>
<p>To better understand the quality of the generated error analysis provided by TIGERSCORE, a random selection of 50 error analyses from each evaluation dataset was assessed by human experts who rated them from the following perspectives: 1) Reasonableness, 2) Comprehensiveness, 3) Effectiveness, 4) Overall Estimation, whose definitions are given in the following:</p>
<p>Reasonableness: The human experts directly pointed out which errors are problematic in error analyses, examining whether the analysis contained hallucination or illogical reasoning.</p>
<p>Comprehensiveness: The human experts carefully review the source, output, and error analyses to determine if there are any additional errors unnoticed by TIGERSCORE. Based on human experts' analysis, they give a score on a scale of 1 to 4 , specifically focused on identifying potential errors that may have been overlooked in the original analysis conducted by TIGERSCORE.</p>
<table>
<thead>
<tr>
<th>Aspects</th>
<th>Explanation Error?</th>
<th>Overlooked Errors</th>
<th>Revision Suggestions</th>
<th>Overall Rating</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rate $\rightarrow$</td>
<td>No</td>
<td>Yes</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Summ</td>
<td>$\mathbf{7 0}$</td>
<td>35</td>
<td>2</td>
<td>$\mathbf{1 7}$</td>
</tr>
<tr>
<td>Trans</td>
<td>$\mathbf{5 4}$</td>
<td>25</td>
<td>3</td>
<td>8</td>
</tr>
<tr>
<td>D2T</td>
<td>19</td>
<td>$\mathbf{2 1}$</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>LF-QA</td>
<td>$\mathbf{4 2}$</td>
<td>19</td>
<td>4</td>
<td>10</td>
</tr>
<tr>
<td>MathQA</td>
<td>$\mathbf{3 9}$</td>
<td>26</td>
<td>5</td>
<td>12</td>
</tr>
<tr>
<td>Instruct</td>
<td>5</td>
<td>$\mathbf{9}$</td>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>Story-Gen</td>
<td>$\mathbf{6 6}$</td>
<td>29</td>
<td>7</td>
<td>$\mathbf{1 6}$</td>
</tr>
<tr>
<td>Total</td>
<td>$\mathbf{2 9 5}$</td>
<td>164</td>
<td>27</td>
<td>76</td>
</tr>
</tbody>
</table>
<p>Table 5: Human evaluation results, the first question is asked per error in error analyses, and the others are per sample. Superior performance is indicated by higher numerical values. The most-voted rate of each task for each human evaluation aspect is bolded.</p>
<p>Effectiveness: The revision suggestions in error analyses are evaluated by human experts, on a scale of 1 to 5 , to determine their appropriateness and effectiveness in enhancing the output quantity.</p>
<p>Overall: The Human experts further assign an overall score on a scale of 1 to 5 based on the reasonableness, comprehensiveness, and effectiveness of the error analysis.</p>
<p>We report the detailed human evaluation results in Table 5, it is found that $64.3 \%$ of TIGERSCORE's error analyses are deemed reasonable, that is, the answer to the first question is 'no errors in interpretation'. This suggests that most error analyses accurately identified and explained errors. In $70.6 \%$ of cases, evaluators gave a positive score (3 or 4 ) for question 2 , implying no missing errors were found. This demonstrates TIGERSCORE's effectiveness in comprehensive error analysis. Overall, $70.8 \%$ of error analyses received positive ratings ( 3 to 5 ), indicating good quality and usefulness in identifying and explaining errors according to human experts.</p>
<h1>4.5 Hallucination Analysis</h1>
<p>Alignments on no-error outputs In order to understand the hallucinations generated by TIGERScore, we used TIGERScore to evaluate the gold reference, which is used in reference-based metrics, and expect TIGERScore not to hallucinate errors on these no-error instances. Results are shown in Table 6.</p>
<p>In tasks related to instruction-following and long-form QA, TIGERScore demonstrates a high level of accuracy, avoiding hallucinations in over $85 \%$ of cases. This highlights its proficiency in producing factual and error-free analysis. However, TIGERScore is less consistent in tasks like summarization, translation, and data-to-text conversion. In these areas, it often fails to achieve perfect scores (0), but still frequently identifies gold references as either flawless or only minimally flawed (with score reductions less than 2). This could be due to the subjective nature of these tasks, where minor errors, such as the substitution of similar words, maybe more open to interpretation. Additionally, TIGERScore faces challenges in tasks like MathQA and story generation. These difficulties may stem from the inherent complexity of MathQA problems and the subjective nature of story creation, as well as specific limitations of TIGERScore in these areas. Improving TIGERScore's performance in these challenging tasks remains a topic for future research.</p>
<p>Hallucinations analysis of TIGERScore outputs To better understand how TIGERScore handles hallucinations, we conducted experiments across six different tasks. For each task, we ran TIGERScore13 B on 20 samples with errors in the system output. We then used GPT-4 to determine if these samples contained hallucinations or factual inaccuracies, as outlined in the prompting templates found in Table 22. According to the results in Table 7, approximately $89.28 \%$ of TIGERSCORE's error analyses are free from hallucinations or factual errors. We acknowledge the limitations of our study, including the small sample size and the reliance on GPT-4 rather than human evaluators. Nonetheless, our findings are significant, demonstrating that TIGERSCORE is effective at avoiding hallucinations in generated content.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks $\rightarrow$</th>
<th style="text-align: center;">Summ</th>
<th style="text-align: center;">Trans</th>
<th style="text-align: center;">D2T</th>
<th style="text-align: center;">LF-QA</th>
<th style="text-align: center;">MathQA</th>
<th style="text-align: center;">Instruct</th>
<th style="text-align: center;">Story-Gen</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Gold reference's score $=0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TIGERScore-7B</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">3.57</td>
<td style="text-align: center;">45.51</td>
<td style="text-align: center;">84.75</td>
<td style="text-align: center;">34.36</td>
<td style="text-align: center;">73.98</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">41.74</td>
</tr>
<tr>
<td style="text-align: center;">TIGERScore-13B</td>
<td style="text-align: center;">48.00</td>
<td style="text-align: center;">21.01</td>
<td style="text-align: center;">23.03</td>
<td style="text-align: center;">94.50</td>
<td style="text-align: center;">25.28</td>
<td style="text-align: center;">86.38</td>
<td style="text-align: center;">46.00</td>
<td style="text-align: center;">49.17</td>
</tr>
<tr>
<td style="text-align: center;">Gold reference's score $&gt;-2$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TIGERScore-7B</td>
<td style="text-align: center;">68.00</td>
<td style="text-align: center;">72.48</td>
<td style="text-align: center;">78.65</td>
<td style="text-align: center;">84.75</td>
<td style="text-align: center;">34.36</td>
<td style="text-align: center;">92.48</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">66.39</td>
</tr>
<tr>
<td style="text-align: center;">TIGERScore-13B</td>
<td style="text-align: center;">97.00</td>
<td style="text-align: center;">83.63</td>
<td style="text-align: center;">94.94</td>
<td style="text-align: center;">94.50</td>
<td style="text-align: center;">25.28</td>
<td style="text-align: center;">96.14</td>
<td style="text-align: center;">49.00</td>
<td style="text-align: center;">77.21</td>
</tr>
</tbody>
</table>
<p>Table 6: TIGERScore's score on the gold reference of the test set. For each task, the 0 column refers to the percentage that TIGERScore reduces 0 scores for the gold references. The $&gt;-2$ column refers to the percentage that TIGERScore reduces less or equal to 1 score on the gold references.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Summ</th>
<th style="text-align: center;">Trans</th>
<th style="text-align: center;">D2T</th>
<th style="text-align: center;">LF-QA</th>
<th style="text-align: center;">MathQA</th>
<th style="text-align: center;">Instruct</th>
<th style="text-align: center;">Story-Gen</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">95.00</td>
<td style="text-align: center;">95.00</td>
<td style="text-align: center;">90.00</td>
<td style="text-align: center;">85.00</td>
<td style="text-align: center;">90.00</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">95.00</td>
<td style="text-align: center;">89.28</td>
</tr>
</tbody>
</table>
<p>Table 7: The accuracy of the error analysis from TIGERScore-13B assessed by GPT-4 that do not contain hallucinations or factual errors. if includes hallucinations or factual errors, assessed by GPT4.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Investigation of the influence of Real-World \&amp; Synthesic mix training on the 7B model.</p>
<h1>4.6 Abaltion Study on Data Source</h1>
<p>Ablation of Two Channel Data Collection To assess the effectiveness of our two-channel data in enhancing TIGERSCORE's performance, we fine-tune 3 models using different dataset setups for experiments: using only Real-World data (ReW), using only Synthetic data (Syn) and using the mixed of both (Mix). The results are detailed in the Spearman correlation in Figure 3. the Mix model outperformed both ReW and Syn in 5 of the 7 tasks. It also achieves an average correlation approximately $20 \%$ higher than ReW and $31 \%$ higher than Syn. Slight decreases in correlation for D2T and Story-Gen tasks are deemed as acceptable compromises for better overall performance.</p>
<p>Ablation of Each Single Generation Task In order to investigate the contribution of each task in the MetricInstruct, we conducted experiments to see whether multi-task training will benefit the single task. We trained a model for each task and compared its performance to that of TIGERSCORE. As depicted in the Table 8, the multi-task learning paradigm does benefit almost all tasks' performance, except for MathQA. We contend that Math QA poses a significant challenge for LLMs, and a separately trained model is more adept at handling this task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics $\downarrow$ Tasks $\rightarrow$</th>
<th style="text-align: center;">Summ</th>
<th style="text-align: center;">Trans</th>
<th style="text-align: center;">D2T</th>
<th style="text-align: center;">LF-QA</th>
<th style="text-align: center;">MathQA</th>
<th style="text-align: center;">Instruct</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single Task</td>
<td style="text-align: center;">35.55</td>
<td style="text-align: center;">41.65</td>
<td style="text-align: center;">39.75</td>
<td style="text-align: center;">41.60</td>
<td style="text-align: center;">$\mathbf{2 6 . 7 5}$</td>
<td style="text-align: center;">41.19</td>
<td style="text-align: center;">37.75</td>
</tr>
<tr>
<td style="text-align: center;">Multi Task</td>
<td style="text-align: center;">$\mathbf{3 6 . 8 1}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 9 9}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 8 8}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 2 2}$</td>
<td style="text-align: center;">23.32</td>
<td style="text-align: center;">$\mathbf{4 7 . 0 3}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 7 1}$</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta(\%)$</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">8.02</td>
<td style="text-align: center;">15.43</td>
<td style="text-align: center;">11.09</td>
<td style="text-align: center;">-12.83</td>
<td style="text-align: center;">14.18</td>
<td style="text-align: center;">7.84</td>
</tr>
</tbody>
</table>
<p>Table 8: Ablation of the influence of multiple tasks mix or single task on the 13B model.</p>
<h1>5 Related Work</h1>
<h3>5.1 Instruction-driven Large language models</h3>
<p>Instruction tuning has recently become the standard to "align" language models with more useful objectives and human preferences. The instruction tuning step is normally done to enhance certain skillset of large language models. Previously, instruction tuning has been focused on activating models' general capabilities to follow instructions to solve general tasks. Some work has been published like NaturalInstruction (Wang et al., 2022), FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) are the earliest work in the field. Later on, FLAN-v2 (Chung et al., 2022) have been proposed to understand the effect of scaling up the instruction datasets to understand its impact on model performance. These approaches mainly adopt human-annotated datasets to build the instruction following dataset. More recently, multiple works (Wang et al., 2023b; Xu et al., 2023a) propose to utilize synthetic instruction following data distilled from GPT-4 to align open-source LLMs. Our work differs from them in the sense that our method aims to activate specialized capability to generate error analysis according to instruction, which is the first of its kind.</p>
<h3>5.2 Explainable Metrics</h3>
<p>The increasing focus on model interpretability has led to a surge in research dedicated to explainable metrics. Research in these fields aims to build a metric system for a certain task that is readable to humans and is expected to help the development of better text generation systems (Leiter et al., 2022). Early endeavors in this area delved into explainability via multi-faceted evaluations, as exemplified by works such as Unieval (Zhong et al., 2022b) and GPTScore (Fu et al., 2023). As LLM blooms, researchers began to directly prompt LLMs to create interpretable metrics. One instance is PandaLM, trained on Llama to compare two responses pairwisely and provide a textual rationale for its decisions (Wang et al., 2023a). Another noteworthy approach is InstructScore, leveraging large language models as knowledge repositories to obtain premium error analysis examples (Xu et al., 2023c). Despite these commendable advancements, most existing explainable metrics still require gold references and are often limited concerning the task domain. Our contribution distinguishes itself by offering a reference-free nature and the cross-task ability brought by instruction-tuning over large language models, aiming to serve as a universal explainable metric.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we propose the novel metric TIGERSCORE, which is able to evaluate any text generation task guided by natural language instruction. We demonstrate the exceptional performance of TIGERSCORE by its high correlation with human preference. We also demonstrate the high accuracy of its generated rationale. However, TIGERSCORE does hallucinate sometimes to generate false explanations. On the other hand, we found that TIGERSCORE is not good in evaluating reasoning tasks like mathQA. In the future, we plan to devote more effort to enable more faithful explanations and unleash its potential to evaluate errors for harder tasks, like reasoning errors.</p>
<h2>Limitation</h2>
<p>Hallucinated Errors Despite substantial efforts to minimize hallucinations in TIGERScore's output, we still observe hallucinated errors, particularly in challenging tasks such as mathQA. This issue is attributed to both the quality of our training data and the limitations of our base model. A potential solution involves</p>
<p>initial fine-tuning on specific tasks for generation purposes, followed by further fine-tuning for evaluation. However, additional strategies are necessary to effectively reduce these hallucinations.</p>
<p>Evaluation Efficiency TIGERScore, fine-tuned on the 7B and 13B versions of Llama-2, faces challenges with inference speed when used as an evaluation metric. Our testing reveals that TIGERScore achieves an evaluation speed of approximately 0.2 seconds per instance on a single A800 GPU with the assistance of VLLM. While this is manageable in interactive environments, further improvements are needed for efficiency in large-scale batch evaluations, compared to faster traditional metrics like BLEU and BERTScore.</p>
<p>Discrepancy Between the Local Errors and Global Evaluation Dividing output evaluation into multiple local errors is logical, but using a simple summation of these errors as a global evaluation metric can lead to discrepancies. Longer outputs often have multiple errors, while shorter ones might be simply judged as entirely erroneous in a single error. Compared to global evaluation methods, like rating a score out of 10 , developing a structured and reasonable method to accumulate and represent these errors remains an area for further exploration.</p>
<h1>Ethics Statements</h1>
<p>Our work collected data from publicly available datasets that were ethically curated with informed consent from all participants. We ensure that all privacy data is excluded. We acknowledge the potential of our machine-learning models to generate hallucinated, biased, or unfair content. Methods have been adopted to prevent the generation of these kinds of content with our best efforts. Our research involves human evaluation experiments and we ensure that each participant's privacy is excluded and protected on our side. We make sure each participant is paid fairly according to their work amount. Our hourly rate is 12 dollars, which is above the US lowest payment rate.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In 11th Conference of the European Chapter of the Association for Computational Linguistics, pp. 249-256, Trento, Italy, April 2006. Association for Computational Linguistics. URL https://aclanthology.org/E06-1032.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL https://api.semanticscholar. org/CorpusID:239998651.</p>
<p>Desmond Elliott and Frank Keller. Comparing automatic evaluation measures for image description. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 452-457, Baltimore, Maryland, June 2014. Association for Computational Linguistics. doi: $10.3115 / \mathrm{v} 1 / \mathrm{P} 14-2074$. URL https://aclanthology.org/P14-2074.</p>
<p>Alexander R Fabbri, Wojciech Kryciski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409, 2021.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/P19-1346.</p>
<p>Markus Freitag, George F. Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474, 2021. URL https://api.semanticscholar. org/CorpusID:233444275.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and Andr F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 46-68, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.2.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire, 2023.
Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. Openmeva: A benchmark for evaluating open-ended story generation metrics. arXiv preprint arXiv:2105.08920, 2021.</p>
<p>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Conference on Empirical Methods in Natural Language Processing, 2019. URL https://api.semanticscholar.org/CorpusID:202540590.</p>
<p>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. Automatic evaluation of translation quality for distant language pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 944-952, Cambridge, MA, October 2010. Association for Computational Linguistics. URL https://aclanthology.org/010-1092.</p>
<p>Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14165-14178, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.792. URL https: //aclanthology.org/2023.acl-long. 792.</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023 .</p>
<p>Christoph Leiter, Piyawat Lertvittayakumjorn, M. Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. Towards explainable evaluation metrics for natural language generation. ArXiv, abs/2203.11131, 2022. URL https://api.semanticscholar.org/CorpusID:247594648.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https: //github.com/tatsu-lab/alpaca_eval, 2023.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.</p>
<p>Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. Brio: Bringing order to abstractive summarization. arXiv preprint arXiv:2203.16804, 2022.</p>
<p>Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng Tao. Error analysis prompting enables humanlike translation evaluation in large language models: A case study on chatgpt. ArXiv, abs/2303.13809, 2023. URL https://api.semanticscholar.org/CorpusID:257756967.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.</p>
<p>Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryciski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev, and Dragomir Radev. FeTaQA: Freeform table question answering. Transactions of the Association for Computational Linguistics, 10:35-49, 2022a. doi: 10.1162/tacl_a_00446. URL https://aclanthology.org/2022.tacl-1.3.</p>
<p>Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryciski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, et al. Fetaqa: Free-form table question answering. Transactions of the Association for Computational Linguistics, 10:35-49, 2022b.</p>
<p>OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Annual Meeting of the Association for Computational Linguistics, 2002a. URL https://api.semanticscholar.org/CorpusID:11080756.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002b.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2685-2702, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.213. URL https://aclanthology.org/2020.emnlp-main. 213.</p>
<p>Ricardo Rei, Jos G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578-585, Abu Dhabi, United Arab Emirates (Hybrid), December 2022a. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.</p>
<p>Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, Jos G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and Andr F. T. Martins. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 634-645, Abu Dhabi, United Arab Emirates (Hybrid), December 2022b. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.wmt-1.60.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2021.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. Bleurt: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, 2020 .</p>
<p>Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-form answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 8273-8288, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.566. URL https://aclanthology.org/2022.emnlp-main.566.</p>
<p>Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form answers. arXiv preprint arXiv:2204.06092, 2022b.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xingxu Xie, Wei Ye, Shi-Bo Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. ArXiv, abs/2306.05087, 2023a. URL https://api. semanticscholar.org/CorpusID:259108266.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085-5109, 2022.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023), 2023b. URL https: //aclanthology.org/2023.acl-long.754.pdf.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023a.</p>
<p>Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, and William Yang Wang. SESCORE2: Learning text generation evaluation via synthesizing realistic mistakes. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5166-5183, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.283. URL https: //aclanthology.org/2023.acl-long. 283.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. Instructscore: Towards explainable text generation evaluation with automatic feedback, 2023c.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277, 2021.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Peng Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. Towards a unified multi-dimensional evaluator for text generation. In Conference on Empirical Methods in Natural Language Processing, 2022a. URL https://api.semanticscholar.org/CorpusID:252873117.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. Towards a unified multi-dimensional evaluator for text generation, 2022b.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment, 2023.</p>
<p>Giulio Zhou and Gerasimos Lampouras. Webnlg challenge 2020: Language agnostic delexicalisation for multilingual rdf-to-text generation. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pp. 186-191, 2020.</p>
<h1>A Appendix</h1>
<h2>A. 1 Prompting Strategies</h2>
<p>In our study to extract high-quality error-analysis insights from GPT-4, we employed various intuitive prompting strategies. These strategies are detailed in the prompting templates found in subsection A.7. Key strategies are outlined below:</p>
<p>Two-Step Generation and Formatting Process One of the challenges in eliciting structured knowledge from LLMs is directing them to generate content in a specific format. Although GPT-4 often adheres to given formats, there are instances of deviations. We observed that enforcing a strict format can compromise content quality, as indicated by reduced correlation in our analysis. Our approach involves initially allowing GPT-4 to generate responses freely in the first conversation turn. In the subsequent turn, we request GPT-4 to reformat its response according to a predefined format. The initial generation templates for each task, along with a singular template for the formatting step, are listed in subsection A.7.</p>
<p>Incorporation of Task-Specific Words in Initial Queries To leverage GPT-4's task-specific knowledge, we designed varied prompting templates for different tasks with slight modifications. Keywords like 'Source,' 'Instruction,' 'Reference,' 'Output,' 'Solution,' 'Translation,' and 'Summary' are dynamically utilized in various task contexts. In tasks like instruction-following, where the context is self-explanatory, we omitted specific keywords.</p>
<p>Integration of Predefined Aspect Definitions for Focused Evaluation Directly requesting GPT-4 to evaluate task outputs often led to low-quality error identification. It either points out simple discrepancies with the reference or misses crucial evaluation aspects, thus overlooking some errors. To address this, we incorporated predefined evaluation aspects defined in Table 10 into the templates, guiding GPT-4 to produce more focused responses. Exceptionally, for data2text, we found that directly evaluating errors was good enough, and thus, we did not include our predefined aspects.</p>
<p>Classification of Errors Using Major/Minor Error Guidelines Drawing inspiration from the MQM translation human rating system and InstructScore prompting template (Freitag et al., 2021; Xu et al., 2023c), we classified translation errors as either major or minor. This classification helped GPT-4 in assigning more consistent scores to each error, countering its instability with numerical judgments (Lu et al., 2023).</p>
<p>Adopting a 0.5 to 5 Scale for Scoring Error Severity Initially, we used an integer scale from 1 to 5 for error penalty scores. While effective in translation tasks, this scale is less effective in tasks like summarization, data2text, and instruction-following. Our experiments demonstrated that a more nuanced scoring scale ranging from 0.5 to 5 yielded better correlation across all tasks</p>
<h1>A. 2 Creation of evaluation aspects</h1>
<p>With the assistance of GPT-4, we have carefully curated the evaluation aspects of each task that are mutually exclusive and collectively exhaustive. The steps include:</p>
<ul>
<li>Step 1: We prompt GPT-4 to output 20 candidate aspects for each task.</li>
<li>Step 2: We ask GPT-4 to summarize these aspects into 3 to 5 general aspects for this task.</li>
<li>Step 3: We ask GPT-4 to generate detailed definition and 5 specific error types under each aspect.</li>
</ul>
<p>In each step, we check the reasonability of GPT-4's response and make necessary modifications to the responses, including summarized error aspects, error definition, and error types, to make them clear, concise, and typical. Example prompts used in each step are show in Table 21.</p>
<h2>A. 3 TIGERScore prompt template</h2>
<div class="codehilite"><pre><span></span><code>You<span class="w"> </span>are<span class="w"> </span>evaluating<span class="w"> </span>errors<span class="w"> </span>in<span class="w"> </span>a<span class="w"> </span>model-generated<span class="w"> </span>output<span class="w"> </span>for<span class="w"> </span>a<span class="w"> </span>given<span class="w"> </span>instruction.
Instruction:
<span class="cp">${</span><span class="n">generation_instruction</span><span class="cp">}</span>
<span class="cp">${</span><span class="n">input_context</span><span class="cp">}</span>
</code></pre></div>

<p>Model-generated Output:
\${hypothesis_output}</p>
<p>For each error you give in the response, please also elaborate the following information:</p>
<ul>
<li>error location (the words that are wrong in the output)</li>
<li>error aspect it belongs to.</li>
<li>explanation why it's an error, and the correction suggestions.</li>
<li>severity of the error ("Major" or "Minor").</li>
<li>reduction of score (between 0.5 and 5 given the severity of the error)</li>
</ul>
<p>Your evaluation output:
Table 9: The prompt template used for the fine-tuning and inference of TIGERSCORE</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Distribution analysis of MetricInstruct training data for the context length, number of errors, and error severities. The left figure shows the context length distribution. The right-top plot illustrates the distribution of the per-instance number of errors. The right-bottom figure illustrates the counts of "Major" and "Minor" errors for all six tasks</p>
<h1>A. 4 MetricInstruct data statistics analysis</h1>
<p>In order to find the secret of the success of MetricInstruct, we conducted a deep analysis of its inherent statistics, as shown in Figure 4. we first count the distribution of data length through the Llama tokenizer. The results show that more than $90 \%$ of data has a length lower than 1024. Due to the lack of GPU resources for fine-tuning in the longer context length scenario, this distribution further demonstrates the reasonability of using 1024 as the context length of TIGERScore.</p>
<p>Furthermore, we examined the incidence of errors per data instance across various tasks. The figure illustrates that each task contains most instances with the number of errors being 1 or 2 and fewer instances are considered perfect and very erroneous, reflecting a naturally occurring distribution. We contend that such a balanced distribution is crucial for the model's performance - it helps in reducing fabricated errors in correct outputs and aids in the precise identification of minor and major errors in outputs that are partially correct or entirely incorrect.</p>
<p>Additionally, we categorized errors as 'Major' or 'Minor' and quantified their occurrences. Our analysis reveals that tasks of a more subjective nature, such as translation and summarization, tend to have a higher frequency of minor errors. Contrastingly, in tasks like MathQA, the predominance of major errors is in alignment with the expectation that mathematical inaccuracies are generally more critical.*</p>
<h2>A. 5 Gold scores for correlation computation</h2>
<p>Human preference scores are necessary to conduct the correlation analysis. Those datasets with official system outputs released are usually accompanied by systematic human preference scores, like the WMTMQM score for the translation task. However, these scores are not available for tasks like long-form QA, MathQA, and instruction-following, where we need to create on our own.</p>
<p>Therefore, we here introduce what human preference scores we have used to conduct the evaluation experiments. For summarization, data2text, and story generation tasks, we use their official human ratings from multiple aspects of their released outputs. For translation, we use the official WMT-22 MQM scores as the gold scores. For MathQA, we simply use the accuracy ( 1 or 0 ) as the gold preferences. For instruct-following, we use human ratings from the hugging face community of a dataset where 500 instances are sampled from LIMA and AlpacaEvla. For Long-form QA, we use the powerful GPT-4 to perform pairwise comparisons for them and count the winning times as the way to rank them, which is similar to how Jiang et al. (2023) constructs MixInstruct.</p>
<h1>A. 6 Evaluation aspects for all tasks</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Aspect</th>
<th style="text-align: center;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Summ</td>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">The degree to which the summarized output accurately reflects the key points of the input text.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fact Consistency</td>
<td style="text-align: center;">If the facts in the summary are consistent with the facts in the original text.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coherence</td>
<td style="text-align: center;">Pertains to the logical and meaningful arrangement of ideas in the summary.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fluency</td>
<td style="text-align: center;">Reviews the model-generated output's use of language, including grammar, punctuation, and vocabulary that affect the quality of the sentences.</td>
</tr>
<tr>
<td style="text-align: center;">Trans</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">The degree to which the translated text adheres to the original text, maintaining the same meaning, context and cultural nuances.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fluency</td>
<td style="text-align: center;">How naturally the translation reads in the target language.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Terminology</td>
<td style="text-align: center;">The appropriate use of specific terms and jargon related to a particular field or industry.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Style Matching</td>
<td style="text-align: center;">Translator's ability to maintain the same style, tone, and voice as the original text. Example error types include:</td>
</tr>
<tr>
<td style="text-align: center;">D2T</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Deals with the correctness of the information presented by the output.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Logical Coherence</td>
<td style="text-align: center;">How well the output transforms structured data into a comprehensible, logical, and engaging text.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fluency</td>
<td style="text-align: center;">Reviews the model-generated output's use of language, including grammar, punctuation, and vocabulary that affect the quality of the sentences.</td>
</tr>
<tr>
<td style="text-align: center;">LF-QA</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Evaluates the factual correctness of the answer.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Completeness</td>
<td style="text-align: center;">Evaluates if the answer leaves out any critical parts or details that were asked in the question.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Informativeness</td>
<td style="text-align: center;">Assesses the quality of the response in terms of how helpful it is for the user to understand the answer.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Clarity</td>
<td style="text-align: center;">Assesses the readability and understandability of the response.</td>
</tr>
<tr>
<td style="text-align: center;">MathQA</td>
<td style="text-align: center;">Problem Understanding</td>
<td style="text-align: center;">Assesses how well the output accurately comprehend the text-based description of the math problem.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Problem Formulation</td>
<td style="text-align: center;">Involves translating the problem from a textual form into a mathematical equation or set of equations that can be solved.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Computing Accuracy</td>
<td style="text-align: center;">Assesses the output's ability to perform the mathematical operations accurately to arrive at the correct solution.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Solution Interpretation</td>
<td style="text-align: center;">Involves the how well the output correctly interpret the solution of the problem in the context of the original problem statement.</td>
</tr>
<tr>
<td style="text-align: center;">Instruct</td>
<td style="text-align: center;">Comprehension</td>
<td style="text-align: center;">Evaluates how well the output understands the given instruction.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Measures the correctness of the output in relation to the instruction and the paired input context.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Informativeness</td>
<td style="text-align: center;">Assesses the relevancy and usefulness of the information provided by the output.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coherence</td>
<td style="text-align: center;">Evaluates how logically the output flows and connects.</td>
</tr>
</tbody>
</table>
<p>Table 10: Definitions of evaluation aspects of TIGERSCORE for the 6 text generation task.</p>
<h1>A. 7 Prompting templates</h1>
<div class="codehilite"><pre><span></span><code>Source:<span class="w"> </span><span class="cp">${</span><span class="n">input_context</span><span class="cp">}</span>
Reference:<span class="w"> </span><span class="cp">${</span><span class="n">reference_output</span><span class="cp">}</span>
Output:<span class="cp">${</span><span class="n">hypothesis_output</span><span class="cp">}</span>
Based<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>given<span class="w"> </span>Source<span class="w"> </span>and<span class="w"> </span>Reference,<span class="w"> </span>please<span class="w"> </span>evaluate<span class="w"> </span>the<span class="w"> </span>quality<span class="w"> </span>of<span class="w"> </span>summary(Output)<span class="w"> </span>written<span class="w"> </span>for<span class="w"> </span>the<span class="w"> </span>input<span class="w"> </span>text.
Please<span class="w"> </span>score<span class="w"> </span>the<span class="w"> </span>summarization<span class="w"> </span>with<span class="w"> </span>0.5<span class="w"> </span>to<span class="w"> </span>5<span class="w"> </span>for<span class="w"> </span>aspects<span class="w"> </span>below.<span class="w"> </span>Then,<span class="w"> </span>identify<span class="w"> </span>the<span class="w"> </span>major<span class="w"> </span>and<span class="w"> </span>minor<span class="w"> </span>errors<span class="w"> </span>in<span class="w"> </span>this
output<span class="w"> </span>for<span class="w"> </span>the<span class="w"> </span><span class="nv">$task</span><span class="w"> </span>task.<span class="w"> </span>There<span class="w"> </span>may<span class="w"> </span>be<span class="w"> </span>multiple<span class="w"> </span>errors<span class="w"> </span>or<span class="w"> </span>no<span class="w"> </span>error<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>output.<span class="w"> </span>Here<span class="w"> </span>are<span class="w"> </span>the<span class="w"> </span>aspects<span class="w"> </span>you<span class="w"> </span>need<span class="w"> </span>to
focus<span class="w"> </span>on:<span class="w"> </span><span class="cp">${</span><span class="n">aspects_descriptions</span><span class="cp">}</span>
</code></pre></div>

<p>Table 11: Prompting templates for summarization task</p>
<div class="codehilite"><pre><span></span><code>Translation<span class="w"> </span>Instruction:<span class="w"> </span><span class="cp">${</span><span class="n">generation_instruction</span><span class="cp">}</span>
Source<span class="w"> </span>Text:<span class="w"> </span><span class="cp">${</span><span class="n">input_context</span><span class="cp">}</span>
<span class="cp">${</span><span class="n">reference_output</span><span class="cp">}</span>
Model-generated<span class="w"> </span>Translation:<span class="w"> </span><span class="cp">${</span><span class="n">hypothesis_output</span><span class="cp">}</span>
Please<span class="w"> </span>identify<span class="w"> </span>and<span class="w"> </span>categorize<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>model-generated<span class="w"> </span>translation<span class="w"> </span>as<span class="w"> </span>either<span class="w"> </span>Major<span class="w"> </span>or<span class="w"> </span>Minor.<span class="w"> </span>Major<span class="w"> </span>errors
significantly<span class="w"> </span>impact<span class="w"> </span>the<span class="w"> </span>task,<span class="w"> </span>while<span class="w"> </span>Minor<span class="w"> </span>errors<span class="w"> </span>are<span class="w"> </span>subjective<span class="w"> </span>and<span class="w"> </span>represent<span class="w"> </span>minor<span class="w"> </span>imperfections.
When<span class="w"> </span>identifying<span class="w"> </span>errors,<span class="w"> </span>do<span class="w"> </span>not<span class="w"> </span>solely<span class="w"> </span>rely<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>reference<span class="w"> </span>translation<span class="w"> </span>for<span class="w"> </span>comparison.<span class="w"> </span>Provide<span class="w"> </span>explanations<span class="w"> </span>as<span class="w"> </span>an
expert<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>task<span class="w"> </span>domain,<span class="w"> </span>without<span class="w"> </span>explicitly<span class="w"> </span>mentioning<span class="w"> </span>the<span class="w"> </span>reference<span class="w"> </span>output.
</code></pre></div>

<p>Table 12: Prompting templates for translation task</p>
<div class="codehilite"><pre><span></span><code>Task<span class="w"> </span>instruction:{generation_instruction}
Source:<span class="w"> </span><span class="cp">${</span><span class="n">input_context</span><span class="cp">}</span>
<span class="cp">${</span><span class="n">reference_output</span><span class="cp">}</span>
Output:<span class="w"> </span><span class="cp">${</span> <span class="n">hypothesis_output</span><span class="cp">}</span>
Based<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>given<span class="w"> </span>source<span class="w"> </span>and<span class="w"> </span>reference,<span class="w"> </span>identify<span class="w"> </span>the<span class="w"> </span>major<span class="w"> </span>and<span class="w"> </span>minor<span class="w"> </span>errors<span class="w"> </span>in<span class="w"> </span>this<span class="w"> </span>Output<span class="w"> </span>for<span class="w"> </span>the<span class="w"> </span>data<span class="w"> </span>to<span class="w"> </span>text<span class="w"> </span>task,
which<span class="w"> </span>is<span class="w"> </span>to<span class="w"> </span><span class="nv">$generation_instruction.</span><span class="w"> </span>Note<span class="w"> </span>that<span class="w"> </span>Major<span class="w"> </span>errors<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span>actual<span class="w"> </span>errors<span class="w"> </span>that<span class="w"> </span>affects<span class="w"> </span>the<span class="w"> </span>task<span class="w"> </span>severely,<span class="w"> </span>may
change<span class="w"> </span>the<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>output,<span class="w"> </span>and<span class="w"> </span>Minor<span class="w"> </span>errors<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span>smaller<span class="w"> </span>imperfections,<span class="w"> </span>and<span class="w"> </span>purely<span class="w"> </span>subjective<span class="w"> </span>opinions
about<span class="w"> </span>the<span class="w"> </span>output.<span class="w"> </span>There<span class="w"> </span>may<span class="w"> </span>be<span class="w"> </span>multiple<span class="w"> </span>errors<span class="w"> </span>or<span class="w"> </span>no<span class="w"> </span>error<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>output.
</code></pre></div>

<p>Table 13: Prompting templates for data2text task</p>
<div class="codehilite"><pre><span></span><code><span class="cp">${</span><span class="n">generation_instruction</span><span class="cp">}</span>
<span class="cp">${</span><span class="n">input_context</span><span class="cp">}</span>
The<span class="w"> </span>correct<span class="w"> </span>solution<span class="w"> </span>is:
<span class="cp">${</span><span class="n">reference_output</span><span class="cp">}</span>
A<span class="w"> </span>model-generated<span class="w"> </span>solution<span class="w"> </span>is:<span class="w"> </span><span class="cp">${</span><span class="n">hypothesis_output</span><span class="cp">}</span>
Please<span class="w"> </span>identify<span class="w"> </span>all<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>in<span class="w"> </span>this<span class="w"> </span>output<span class="w"> </span>considering<span class="w"> </span>the<span class="w"> </span>following<span class="w"> </span>aspects:<span class="w"> </span><span class="cp">${</span><span class="n">aspects_list</span><span class="cp">}</span>
</code></pre></div>

<p>Table 14: Prompting templates for mathQA task</p>
<div class="codehilite"><pre><span></span><code>Source:<span class="w"> </span><span class="cp">${</span><span class="n">input_context</span><span class="cp">}</span>
<span class="cp">${</span><span class="n">reference_output</span><span class="cp">}</span>
Output:<span class="w"> </span><span class="cp">${</span><span class="n">hypothesis_output</span><span class="cp">}</span>
Based<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>given<span class="w"> </span>Source<span class="w"> </span>and<span class="w"> </span>reference,<span class="w"> </span>identify<span class="w"> </span>the<span class="w"> </span>major<span class="w"> </span>and<span class="w"> </span>minor<span class="w"> </span>errors<span class="w"> </span>in<span class="w"> </span>this<span class="w"> </span>Output<span class="w"> </span>for<span class="w"> </span>the<span class="w"> </span><span class="cp">${</span><span class="n">task</span><span class="cp">}</span><span class="w"> </span>task,
which<span class="w"> </span>is<span class="w"> </span>to<span class="w"> </span><span class="cp">${</span><span class="n">generation_instruction</span><span class="cp">}</span>.<span class="w"> </span>Note<span class="w"> </span>that<span class="w"> </span>Major<span class="w"> </span>errors<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span>actual<span class="w"> </span>errors<span class="w"> </span>that<span class="w"> </span>affects<span class="w"> </span>the<span class="w"> </span>task<span class="w"> </span>severely,<span class="w"> </span>may
change<span class="w"> </span>the<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>output,<span class="w"> </span>and<span class="w"> </span>Minor<span class="w"> </span>errors<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span>smaller<span class="w"> </span>imperfections,<span class="w"> </span>and<span class="w"> </span>purely<span class="w"> </span>subjective<span class="w"> </span>opinions
about<span class="w"> </span>the<span class="w"> </span>output.<span class="w"> </span>You<span class="w"> </span>should<span class="w"> </span>check<span class="w"> </span>about<span class="w"> </span><span class="cp">${</span><span class="n">aspects_descriptions</span><span class="cp">}</span>.There<span class="w"> </span>may<span class="w"> </span>be<span class="w"> </span>multiple<span class="w"> </span>errors<span class="w"> </span>or<span class="w"> </span>no<span class="w"> </span>error<span class="w"> </span>in<span class="w"> </span>the
output.
</code></pre></div>

<p>Table 15: Prompting templates for long-form QA task</p>
<div class="codehilite"><pre><span></span><code><span class="cp">${</span><span class="n">generation_instruction_and_source</span><span class="cp">}</span>
<span class="cp">${</span><span class="n">reference_output</span><span class="cp">}</span>
Output:<span class="w"> </span><span class="cp">${</span><span class="n">hypothesis_output</span><span class="cp">}</span>
Based<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>given<span class="w"> </span>Source<span class="w"> </span>and<span class="w"> </span>reference,<span class="w"> </span>identify<span class="w"> </span>the<span class="w"> </span>major<span class="w"> </span>and<span class="w"> </span>minor<span class="w"> </span>errors<span class="w"> </span>in<span class="w"> </span>this<span class="w"> </span>Output<span class="w"> </span>for<span class="w"> </span>the<span class="w"> </span><span class="cp">${</span><span class="n">task</span><span class="cp">}</span><span class="w"> </span>task.
Note<span class="w"> </span>that<span class="w"> </span>Major<span class="w"> </span>errors<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span>actual<span class="w"> </span>errors<span class="w"> </span>that<span class="w"> </span>affects<span class="w"> </span>the<span class="w"> </span>task<span class="w"> </span>severely,<span class="w"> </span>may<span class="w"> </span>change<span class="w"> </span>the<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>output,
and<span class="w"> </span>Minor<span class="w"> </span>errors<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span>smaller<span class="w"> </span>imperfections,<span class="w"> </span>and<span class="w"> </span>purely<span class="w"> </span>subjective<span class="w"> </span>opinions<span class="w"> </span>about<span class="w"> </span>the<span class="w"> </span>output.<span class="w"> </span>You<span class="w"> </span>should<span class="w"> </span>check
about<span class="w"> </span><span class="cp">${</span><span class="n">aspects_descriptions</span><span class="cp">}</span>.There<span class="w"> </span>may<span class="w"> </span>be<span class="w"> </span>multiple<span class="w"> </span>errors<span class="w"> </span>or<span class="w"> </span>no<span class="w"> </span>error<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>output.
</code></pre></div>

<p>Table 16: Prompting templates for instruction-following task</p>
<div class="codehilite"><pre><span></span><code>For<span class="w"> </span>each<span class="w"> </span>error<span class="w"> </span>identified<span class="w"> </span>in<span class="w"> </span>your<span class="w"> </span>response,<span class="w"> </span>please<span class="w"> </span>provide<span class="w"> </span>the<span class="w"> </span>following<span class="w"> </span>information<span class="w"> </span>in<span class="w"> </span>a<span class="w"> </span>specific<span class="w"> </span>JSON<span class="w"> </span>format:
-<span class="w"> </span>Error<span class="w"> </span>Location:<span class="w"> </span>The<span class="w"> </span>substring<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>Output<span class="w"> </span>that<span class="w"> </span>contains<span class="w"> </span>the<span class="w"> </span>error.
-<span class="w"> </span>Error<span class="w"> </span>Aspect:<span class="w"> </span>Choose<span class="w"> </span>only<span class="w"> </span>one<span class="w"> </span>from<span class="w"> </span><span class="cp">${</span><span class="n">aspects_list</span><span class="cp">}</span>.
-<span class="w"> </span>Explanation:<span class="w"> </span>Describe<span class="w"> </span>why<span class="w"> </span>the<span class="w"> </span>identified<span class="w"> </span>issue<span class="w"> </span>is<span class="w"> </span>an<span class="w"> </span>error,<span class="w"> </span>and<span class="w"> </span>offer<span class="w"> </span>suggestions<span class="w"> </span>for<span class="w"> </span>correction.<span class="w"> </span>Explain<span class="w"> </span>as<span class="w"> </span>an<span class="w"> </span>expert
in<span class="w"> </span>the<span class="w"> </span>task<span class="w"> </span>domain,<span class="w"> </span>without<span class="w"> </span>explicitly<span class="w"> </span>mentioning<span class="w"> </span>the<span class="w"> </span>reference<span class="w"> </span>output.
-<span class="w"> </span>Severity:<span class="w"> </span>Classify<span class="w"> </span>the<span class="w"> </span>error<span class="w"> </span>as<span class="w"> </span>&quot;Major&quot;<span class="w"> </span>or<span class="w"> </span>&quot;Minor&quot;.
-<span class="w"> </span>Score<span class="w"> </span>Reduction:<span class="w"> </span>Assign<span class="w"> </span>a<span class="w"> </span>reduction<span class="w"> </span>score<span class="w"> </span>between<span class="w"> </span>0.5<span class="w"> </span>and<span class="w"> </span>5,<span class="w"> </span>considering<span class="w"> </span>the<span class="w"> </span>severity<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>error.
JSON<span class="w"> </span>Format<span class="w"> </span>for<span class="w"> </span>Output:
-<span class="w"> </span>If<span class="w"> </span>there<span class="w"> </span>are<span class="w"> </span>no<span class="w"> </span>errors:
{&quot;errors&quot;:<span class="w"> </span>{}}
-<span class="w"> </span>If<span class="w"> </span>there<span class="w"> </span>are<span class="w"> </span>errors:
{&quot;errors&quot;:<span class="w"> </span>{
&quot;error_1&quot;:<span class="w"> </span>{
&quot;error_location&quot;:<span class="w"> </span>&quot;<span class="w"> </span>...&quot;,
&quot;error_aspect&quot;:<span class="w"> </span>&quot;<span class="w"> </span>...&quot;,
&quot;explanation&quot;:<span class="w"> </span>&quot;<span class="w"> </span>...&quot;,
&quot;severity&quot;:<span class="w"> </span>&quot;<span class="w"> </span>...&quot;,
&quot;score_reduction&quot;:<span class="w"> </span>...
},
...
}}
</code></pre></div>

<p>Table 17: Prompt template to format the response into json format</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Dongfu Jiang and Yishan Li have led the project and contributed equally.
${ }^{1}$ Project website: https://tiger-ai-lab.github.io/TIGERScore/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
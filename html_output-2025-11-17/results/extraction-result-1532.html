<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1532 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1532</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1532</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-49142e3e381c0dc7fee0049ea41d2ef02c0340d7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/49142e3e381c0dc7fee0049ea41d2ef02c0340d7" target="_blank">Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning</a></p>
                <p><strong>Paper Venue:</strong> NeurIPS Datasets and Benchmarks</p>
                <p><strong>Paper TL;DR:</strong> Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks.</p>
                <p><strong>Paper Abstract:</strong> Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at \url{https://sites.google.com/view/isaacgym-nvidia} and isaac gym can be downloaded at \url{https://developer.nvidia.com/isaac-gym}.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1532.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1532.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Isaac Gym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Isaac Gym (NVIDIA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPU-native, end-to-end robotics simulation and RL training platform that runs physics (NVIDIA PhysX backend) and policy training on the GPU with a PyTorch tensor API, enabling tens of thousands of parallel environments and very fast RL training for contact-rich robotics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Isaac Gym</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A platform that integrates a GPU-accelerated physics backend (PhysX) with a tensor API exposing physics state/control as PyTorch tensors on the device, enabling physics stepping, observation/reward computation, and policy interaction to remain on GPU without CPU-GPU copying.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (locomotion, manipulation, character animation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity for contact-rich robotic dynamics (articulated bodies, contacts, tendons, PD drives, detailed joint and contact parameterization) with configurable time-steps; some aspects may be simplified per-environment (e.g., aerodynamics simplified in certain tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes contact modeling (friction, restitution, contact forces), reduced-coordinate articulations, maximal-coordinate rigid bodies, tendons, PD joint drives, Jacobian and generalized mass matrices, configurable solver iterations and dt (examples: dt=1/120s,1/200s etc. per environment), uses the Temporal Gauss-Seidel (TGS) solver which emulates sub-stepping behavior; supports domain randomization; environment-specific simplifications noted (e.g., rotor aerodynamics for Ingenuity not simulated, forces applied directly to rotors).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Ant, Humanoid, Ingenuity, ANYmal, Franka (cube stacking, OSC), Shadow Hand (Standard & OpenAI variants), TriFinger, Allegro Hand, Humanoid (AMP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agents trained with PPO (GPU-vectorized rl_games implementation), using feed-forward and LSTM policies; some experiments use asymmetric actor-critic (privileged value inputs) to enable sim-to-real transfer; policies map observations to torques/position targets/OSC commands depending on environment.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learning control policies for locomotion, contact-rich dexterous manipulation (in-hand cube reorientation and 6-DoF reposing), stacking, and motion imitation (character animation) via model-free RL.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Multiple reported results: Ant achieves performant locomotion in ~20 seconds (on A100, 4096 envs); Humanoid in ~4 minutes (4096 envs); ANYmal in under 2 minutes; Humanoid AMP in ~6 minutes; Shadow Hand cube rotation (Standard) converged to 20 consecutive successes in under ~35 minutes; Shadow Hand OpenAI observations: 20 consecutive successes <1 hour (FF), 37 successes <6 hours (LSTM) on A100. Max parallel environment steps/sec reported up to ~700k (Ant), ~200k (Humanoid), ~150k (Shadow Hand).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world robots (sim-to-real) and remote physical robot platforms; also comparisons to CPU-based simulators/systems (MuJoCo baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Demonstrated sim-to-real: ANYmal rough-terrain locomotion policies trained in Isaac Gym were transferred and validated on the real ANYmal robot (full task trained and transferred in under ~20 minutes on RTX A6000); TriFinger policies trained in Isaac Gym + domain randomization transferred to the real TriFinger with a reported mean success rate of 55% on the real robot. Shadow Hand OpenAI-style policies were trained with sim-to-real friendly observations and domain randomization but this paper did not perform Shadow Hand sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors emphasize that high-fidelity contact modeling, actuator modeling (they train an actuator network to model series-elastic actuators for ANYmal), and domain randomization are important for successful sim-to-real transfer; they also note some environment-specific simplifications can be acceptable (e.g., direct rotor forces used for Ingenuity) and that omission of some randomizations (e.g., action delay and motor backlash) is possible but may affect transfer. No systematic lower-bound study of minimal fidelity is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Partial/limited transfer observed: TriFinger real-world mean success rate was 55% (i.e., imperfect transfer). Shadow Hand experiments using OpenAI observations were designed for transfer, but the paper did not attempt real-world transfer for Shadow Hand; training with too many parallel environments (hence very short horizons) sometimes produced sub-optimal behaviors (e.g., for Humanoid) indicating training-time limits related to episode horizon rather than simulation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1532.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1532.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhysX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NVIDIA PhysX (GPU backend)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The physics engine used as Isaac Gym's simulation backend; GPU-accelerated, supports reduced-coordinate articulations, contact dynamics, tendons, and the Temporal Gauss-Seidel solver for efficient constraint resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PhysX</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A general-purpose physics engine (here used in GPU-accelerated mode) providing rigid-body dynamics, articulations, contact handling, constraints, tendons, and configurable solver parameters; extended with a direct GPU API to expose state/control buffers to Isaac Gym.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity for multi-body/contact dynamics within the assumptions of rigid-body dynamics and the implemented contact model; supports detailed joint constraints and contact/solver parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Models contacts with friction and restitution, supports position/velocity solver iterations, max bias, friction offsets, joint armature and damping, per-body damping, max velocity limits; uses TGS solver (sub-stepping emulation); provides access to Jacobians and generalized mass matrices; tendons modeled via Fixed Tendon mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>All Isaac Gym environments (Ant, Humanoid, Shadow Hand, TriFinger, ANYmal, Franka, Allegro, Ingenuity)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Physics engine component used to simulate the dynamics of robotic agents and environments; not an ML model itself.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Physical dynamics simulation to enable training of control policies via RL (locomotion, manipulation, imitation).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper documents solver and physical parameters available to tune fidelity (dt, iterations, friction, restitution, offsets, drive stiffness/damping) and shows how these can be randomized for transfer; no single minimal-fidelity threshold is proven, but actuator-network modeling and domain randomization are emphasized for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit physics-engine failure cases reported beyond the general note that environment complexity and insufficient episode horizon (training setup) can impede learning; specific simplifications (e.g., simplified rotor forces for Ingenuity) are noted as design choices rather than failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1532.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1532.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commonly used CPU-based physics engine for simulating articulated rigid-body dynamics in robotics and control research; referenced here as the baseline simulator used in prior large-scale dexterous manipulation work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A CPU-based rigid-body dynamics simulator widely used in reinforcement learning and robotics research, supporting articulated bodies and contact.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-to-high fidelity for articulated dynamics on CPUs; historically used for dexterous manipulation benchmarks but slower at large scale compared to GPU approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides articulated body dynamics and contact handling; in practice often run on CPU clusters for large experiments; paper contrasts its CPU-bound pipeline and data-transfer overheads with Isaac Gym's GPU-native pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>OpenAI Shadow Hand experiments (referenced as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agents (OpenAI's Shadow Hand policies) trained using MuJoCo in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Dexterous in-hand manipulation (cube manipulation/rotation, complex dexterous tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Reported OpenAI baseline: roughly 30 hours (feed-forward case) and ~17 hours (LSTM case) on a large CPU cluster + GPUs for comparable tasks (as reported by the authors when comparing Isaac Gym training times to OpenAI's results using MuJoCo).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1532.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1532.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source physics simulator (Bullet-based) commonly used for robotics and character simulation; referenced as the environment used in prior CPU-based AMP (Adversarial Motion Priors) experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A CPU-based wrapper around the Bullet physics engine used for robotics and character simulation experiments (supports multi-body dynamics, contacts, and common robotics tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics / character animation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium fidelity for character and robotics simulation on CPUs; historically used in research but slower when scaled compared to GPU-native engines.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Supports articulated dynamics and contact modeling; in referenced AMP work, simulating the same sample count required ~30 hours on 16 CPU cores in PyBullet versus ~6 minutes in Isaac Gym for AMP training (demonstrating throughput differences rather than intrinsic physics fidelity differences).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Humanoid character animation using AMP (Peng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL policies trained with adversarial imitation learning (AMP) in prior work using PyBullet as the physics simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Imitation learning / motion imitation for humanoid characters.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Prior AMP implementation in PyBullet required ~30 hours on 16 CPU cores to generate a comparable number of samples that Isaac Gym produced in ~6 minutes (authors' comparison highlighting throughput differences).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1532.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1532.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FleX (limited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NVIDIA FleX (limited support)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A particle-based GPU physics library; the paper notes limited tensor API functionality with FleX in Isaac Gym although the primary backend used is PhysX.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>FleX</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A GPU-accelerated particle/cloth/soft-body physics engine from NVIDIA; in this paper it is only noted that some limited tensor-API support exists.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / soft-body / particle simulation (general)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Not characterized in this paper (limited support only).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>No detailed characteristics provided in the paper; only that limited tensor API functionality exists for FleX within Isaac Gym.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1532.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1532.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other CPU engines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo / PyBullet / DART / Drake / V-Rep (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Several popular CPU-based simulators are mentioned as commonly used in robotics research and as contrasting baselines to GPU-native simulation; they are cited as requiring large CPU resources for large-scale experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo, PyBullet, DART, Drake, V-Rep (mentioned collectively)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>CPU-based simulators commonly used in robotics; mentioned to illustrate conventional pipelines where simulation runs on CPU and policy networks run on GPU, incurring data transfer bottlenecks and requiring large CPU clusters for large-scale experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Varies by engine; generally medium-to-high for articulated rigid-body dynamics but limited in throughput when scaled on CPU clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Support articulated dynamics and contacts; details not enumerated in this paper beyond contrast with GPU-native pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Referenced prior works' agents (e.g., OpenAI Shadow Hand trained with MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agents trained in prior works using these CPU-based engines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Dexterous manipulation and locomotion benchmarks in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving Rubik's Cube with a Robot Hand <em>(Rating: 2)</em></li>
                <li>Adversarial Motion Priors for Physically Simulated Characters <em>(Rating: 2)</em></li>
                <li>TriFinger: A Remote Platform for Dexterous Manipulation Research <em>(Rating: 1)</em></li>
                <li>Sim-to-Real Transfer of Robotic Policies via Domain Randomization and Actuator Modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1532",
    "paper_id": "paper-49142e3e381c0dc7fee0049ea41d2ef02c0340d7",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Isaac Gym",
            "name_full": "Isaac Gym (NVIDIA)",
            "brief_description": "A GPU-native, end-to-end robotics simulation and RL training platform that runs physics (NVIDIA PhysX backend) and policy training on the GPU with a PyTorch tensor API, enabling tens of thousands of parallel environments and very fast RL training for contact-rich robotics tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Isaac Gym",
            "simulator_description": "A platform that integrates a GPU-accelerated physics backend (PhysX) with a tensor API exposing physics state/control as PyTorch tensors on the device, enabling physics stepping, observation/reward computation, and policy interaction to remain on GPU without CPU-GPU copying.",
            "scientific_domain": "mechanics / robotics (locomotion, manipulation, character animation)",
            "fidelity_level": "High-fidelity for contact-rich robotic dynamics (articulated bodies, contacts, tendons, PD drives, detailed joint and contact parameterization) with configurable time-steps; some aspects may be simplified per-environment (e.g., aerodynamics simplified in certain tasks).",
            "fidelity_characteristics": "Includes contact modeling (friction, restitution, contact forces), reduced-coordinate articulations, maximal-coordinate rigid bodies, tendons, PD joint drives, Jacobian and generalized mass matrices, configurable solver iterations and dt (examples: dt=1/120s,1/200s etc. per environment), uses the Temporal Gauss-Seidel (TGS) solver which emulates sub-stepping behavior; supports domain randomization; environment-specific simplifications noted (e.g., rotor aerodynamics for Ingenuity not simulated, forces applied directly to rotors).",
            "model_or_agent_name": "Ant, Humanoid, Ingenuity, ANYmal, Franka (cube stacking, OSC), Shadow Hand (Standard & OpenAI variants), TriFinger, Allegro Hand, Humanoid (AMP)",
            "model_description": "Reinforcement learning agents trained with PPO (GPU-vectorized rl_games implementation), using feed-forward and LSTM policies; some experiments use asymmetric actor-critic (privileged value inputs) to enable sim-to-real transfer; policies map observations to torques/position targets/OSC commands depending on environment.",
            "reasoning_task": "Learning control policies for locomotion, contact-rich dexterous manipulation (in-hand cube reorientation and 6-DoF reposing), stacking, and motion imitation (character animation) via model-free RL.",
            "training_performance": "Multiple reported results: Ant achieves performant locomotion in ~20 seconds (on A100, 4096 envs); Humanoid in ~4 minutes (4096 envs); ANYmal in under 2 minutes; Humanoid AMP in ~6 minutes; Shadow Hand cube rotation (Standard) converged to 20 consecutive successes in under ~35 minutes; Shadow Hand OpenAI observations: 20 consecutive successes &lt;1 hour (FF), 37 successes &lt;6 hours (LSTM) on A100. Max parallel environment steps/sec reported up to ~700k (Ant), ~200k (Humanoid), ~150k (Shadow Hand).",
            "transfer_target": "Real-world robots (sim-to-real) and remote physical robot platforms; also comparisons to CPU-based simulators/systems (MuJoCo baseline).",
            "transfer_performance": "Demonstrated sim-to-real: ANYmal rough-terrain locomotion policies trained in Isaac Gym were transferred and validated on the real ANYmal robot (full task trained and transferred in under ~20 minutes on RTX A6000); TriFinger policies trained in Isaac Gym + domain randomization transferred to the real TriFinger with a reported mean success rate of 55% on the real robot. Shadow Hand OpenAI-style policies were trained with sim-to-real friendly observations and domain randomization but this paper did not perform Shadow Hand sim-to-real transfer.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors emphasize that high-fidelity contact modeling, actuator modeling (they train an actuator network to model series-elastic actuators for ANYmal), and domain randomization are important for successful sim-to-real transfer; they also note some environment-specific simplifications can be acceptable (e.g., direct rotor forces used for Ingenuity) and that omission of some randomizations (e.g., action delay and motor backlash) is possible but may affect transfer. No systematic lower-bound study of minimal fidelity is provided.",
            "failure_cases": "Partial/limited transfer observed: TriFinger real-world mean success rate was 55% (i.e., imperfect transfer). Shadow Hand experiments using OpenAI observations were designed for transfer, but the paper did not attempt real-world transfer for Shadow Hand; training with too many parallel environments (hence very short horizons) sometimes produced sub-optimal behaviors (e.g., for Humanoid) indicating training-time limits related to episode horizon rather than simulation fidelity.",
            "uuid": "e1532.0",
            "source_info": {
                "paper_title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "PhysX",
            "name_full": "NVIDIA PhysX (GPU backend)",
            "brief_description": "The physics engine used as Isaac Gym's simulation backend; GPU-accelerated, supports reduced-coordinate articulations, contact dynamics, tendons, and the Temporal Gauss-Seidel solver for efficient constraint resolution.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "PhysX",
            "simulator_description": "A general-purpose physics engine (here used in GPU-accelerated mode) providing rigid-body dynamics, articulations, contact handling, constraints, tendons, and configurable solver parameters; extended with a direct GPU API to expose state/control buffers to Isaac Gym.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "High-fidelity for multi-body/contact dynamics within the assumptions of rigid-body dynamics and the implemented contact model; supports detailed joint constraints and contact/solver parameters.",
            "fidelity_characteristics": "Models contacts with friction and restitution, supports position/velocity solver iterations, max bias, friction offsets, joint armature and damping, per-body damping, max velocity limits; uses TGS solver (sub-stepping emulation); provides access to Jacobians and generalized mass matrices; tendons modeled via Fixed Tendon mechanics.",
            "model_or_agent_name": "All Isaac Gym environments (Ant, Humanoid, Shadow Hand, TriFinger, ANYmal, Franka, Allegro, Ingenuity)",
            "model_description": "Physics engine component used to simulate the dynamics of robotic agents and environments; not an ML model itself.",
            "reasoning_task": "Physical dynamics simulation to enable training of control policies via RL (locomotion, manipulation, imitation).",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper documents solver and physical parameters available to tune fidelity (dt, iterations, friction, restitution, offsets, drive stiffness/damping) and shows how these can be randomized for transfer; no single minimal-fidelity threshold is proven, but actuator-network modeling and domain randomization are emphasized for transfer.",
            "failure_cases": "No explicit physics-engine failure cases reported beyond the general note that environment complexity and insufficient episode horizon (training setup) can impede learning; specific simplifications (e.g., simplified rotor forces for Ingenuity) are noted as design choices rather than failures.",
            "uuid": "e1532.1",
            "source_info": {
                "paper_title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo",
            "brief_description": "A commonly used CPU-based physics engine for simulating articulated rigid-body dynamics in robotics and control research; referenced here as the baseline simulator used in prior large-scale dexterous manipulation work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo",
            "simulator_description": "A CPU-based rigid-body dynamics simulator widely used in reinforcement learning and robotics research, supporting articulated bodies and contact.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "Medium-to-high fidelity for articulated dynamics on CPUs; historically used for dexterous manipulation benchmarks but slower at large scale compared to GPU approaches.",
            "fidelity_characteristics": "Provides articulated body dynamics and contact handling; in practice often run on CPU clusters for large experiments; paper contrasts its CPU-bound pipeline and data-transfer overheads with Isaac Gym's GPU-native pipeline.",
            "model_or_agent_name": "OpenAI Shadow Hand experiments (referenced as baseline)",
            "model_description": "RL agents (OpenAI's Shadow Hand policies) trained using MuJoCo in prior work.",
            "reasoning_task": "Dexterous in-hand manipulation (cube manipulation/rotation, complex dexterous tasks).",
            "training_performance": "Reported OpenAI baseline: roughly 30 hours (feed-forward case) and ~17 hours (LSTM case) on a large CPU cluster + GPUs for comparable tasks (as reported by the authors when comparing Isaac Gym training times to OpenAI's results using MuJoCo).",
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1532.2",
            "source_info": {
                "paper_title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "PyBullet",
            "name_full": "PyBullet",
            "brief_description": "An open-source physics simulator (Bullet-based) commonly used for robotics and character simulation; referenced as the environment used in prior CPU-based AMP (Adversarial Motion Priors) experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "PyBullet",
            "simulator_description": "A CPU-based wrapper around the Bullet physics engine used for robotics and character simulation experiments (supports multi-body dynamics, contacts, and common robotics tasks).",
            "scientific_domain": "mechanics / robotics / character animation",
            "fidelity_level": "Medium fidelity for character and robotics simulation on CPUs; historically used in research but slower when scaled compared to GPU-native engines.",
            "fidelity_characteristics": "Supports articulated dynamics and contact modeling; in referenced AMP work, simulating the same sample count required ~30 hours on 16 CPU cores in PyBullet versus ~6 minutes in Isaac Gym for AMP training (demonstrating throughput differences rather than intrinsic physics fidelity differences).",
            "model_or_agent_name": "Humanoid character animation using AMP (Peng et al.)",
            "model_description": "RL policies trained with adversarial imitation learning (AMP) in prior work using PyBullet as the physics simulator.",
            "reasoning_task": "Imitation learning / motion imitation for humanoid characters.",
            "training_performance": "Prior AMP implementation in PyBullet required ~30 hours on 16 CPU cores to generate a comparable number of samples that Isaac Gym produced in ~6 minutes (authors' comparison highlighting throughput differences).",
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1532.3",
            "source_info": {
                "paper_title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "FleX (limited)",
            "name_full": "NVIDIA FleX (limited support)",
            "brief_description": "A particle-based GPU physics library; the paper notes limited tensor API functionality with FleX in Isaac Gym although the primary backend used is PhysX.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "FleX",
            "simulator_description": "A GPU-accelerated particle/cloth/soft-body physics engine from NVIDIA; in this paper it is only noted that some limited tensor-API support exists.",
            "scientific_domain": "mechanics / soft-body / particle simulation (general)",
            "fidelity_level": "Not characterized in this paper (limited support only).",
            "fidelity_characteristics": "No detailed characteristics provided in the paper; only that limited tensor API functionality exists for FleX within Isaac Gym.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1532.4",
            "source_info": {
                "paper_title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Other CPU engines",
            "name_full": "MuJoCo / PyBullet / DART / Drake / V-Rep (mentioned)",
            "brief_description": "Several popular CPU-based simulators are mentioned as commonly used in robotics research and as contrasting baselines to GPU-native simulation; they are cited as requiring large CPU resources for large-scale experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo, PyBullet, DART, Drake, V-Rep (mentioned collectively)",
            "simulator_description": "CPU-based simulators commonly used in robotics; mentioned to illustrate conventional pipelines where simulation runs on CPU and policy networks run on GPU, incurring data transfer bottlenecks and requiring large CPU clusters for large-scale experiments.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "Varies by engine; generally medium-to-high for articulated rigid-body dynamics but limited in throughput when scaled on CPU clusters.",
            "fidelity_characteristics": "Support articulated dynamics and contacts; details not enumerated in this paper beyond contrast with GPU-native pipeline.",
            "model_or_agent_name": "Referenced prior works' agents (e.g., OpenAI Shadow Hand trained with MuJoCo)",
            "model_description": "RL agents trained in prior works using these CPU-based engines.",
            "reasoning_task": "Dexterous manipulation and locomotion benchmarks in prior literature.",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1532.5",
            "source_info": {
                "paper_title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving Rubik's Cube with a Robot Hand",
            "rating": 2
        },
        {
            "paper_title": "Adversarial Motion Priors for Physically Simulated Characters",
            "rating": 2
        },
        {
            "paper_title": "TriFinger: A Remote Platform for Dexterous Manipulation Research",
            "rating": 1
        },
        {
            "paper_title": "Sim-to-Real Transfer of Robotic Policies via Domain Randomization and Actuator Modeling",
            "rating": 1
        }
    ],
    "cost": 0.01666825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning</h1>
<p>Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey,</p>
<p>Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, Gavriel State</p>
<p>NVIDIA<br>{vmakoviychuk, lwawrzyniak, kellyg, michellel, kstorey, mmacklin, dhoeller, nrudin, aallshire, ahanda, gstate}@nvidia.com</p>
<h4>Abstract</h4>
<p>Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at https://sites.google.com/view/ isaacgym-nvidia and isaac gym can be downloaded at https://developer. nvidia.com/isaac-gym.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 4
2 Background ..... 6
2.1 Parallelization Strategy ..... 6
2.1.1 CPU Simulations ..... 6
2.1.2 GPU Simulations ..... 7
2.2 Simulation Setup ..... 8
2.3 Tensor API ..... 8
2.3.1 Python Interface ..... 9
2.3.2 Physics State Tensors ..... 9
2.3.3 Physics Control Tensors ..... 10
3 Physics Simulation ..... 10
4 Environments ..... 11
5 Characterising Simulation Performance ..... 13
5.1 Ant ..... 13
5.2 Humanoid ..... 13
5.3 Shadow Hand ..... 14
6 Characterising Environment Performance ..... 15
6.1 Locomotion environments ..... 15
6.1.1 Ant ..... 15
6.1.2 Humanoid ..... 15
6.1.3 Ingenuity ..... 15
6.1.4 ANYmal Robot Locomotion ..... 15
6.2 Humanoid Character Animation ..... 16
6.3 Franka Cube Stacking ..... 17
6.4 Robotic Hands ..... 17
6.4.1 Shadow Hand ..... 18
6.4.2 TriFinger ..... 19
6.4.3 Allegro Hand ..... 20
7 Summary ..... 20
8 Acknowledgements ..... 21
A Appendix ..... 25
A. 1 Tendons ..... 25
A.1.1 Fixed Tendons ..... 25</p>
<p>A.1.2 Spatial Tendons ..... 25
A. 2 Observations \&amp; Rewards ..... 25
A.2.1 Ant and Humanoid environments ..... 25
A.2.2 Locomotion environments ..... 26
A.2.3 Robotic Hands ..... 28
A. 3 Hyperparameters for Training PPO ..... 30
A. 4 Shadow Hand Details ..... 30
A.4.1 Randomizations ..... 30
A.4.2 OpenAI Observations ..... 31</p>
<h1>1 Introduction</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Isaac Gym allows high performance training on a variety of robotics environments. We benchmark on 8 different environments that offer a wide range of complexity and show the strengths of the simulator in blazing fast policy training on a single GPU. Top: Ant, Humanoid, Franka-cube-stack, Ingenuity. Bottom: Shadow Hand, ANYmal, Allegro, TriFinger.</p>
<p>In recent years, reinforcement learning (RL) has become one of the most promising research areas in machine learning and has demonstrated great potential for solving sophisticated decision-making problems. Deep reinforcement learning (Deep RL) has achieved superhuman performance in very challenging tasks, ranging from classic strategy games such as Go and Chess [1], to real-time computer games like StarCraft [2] and DOTA [3]. It has also shown impressive results in robotic settings, including legged locomotion [4] and dexterous manipulation [5].
Simulators play a key role in training robots improving both the safety and iteration speed in the learning process. Training a humanoid robot that walks up and down stairs in the real world can lead to damage to its machinery and the environment, including humans that are working on the robot. An alternative is to train inside simulators that offer an efficient and scalable platform via trial-and-error with no safety issues as observed in the real world. To date, most researchers have relied on a combination of CPUs and GPUs to run reinforcement learning system [5]. Different parts of the computer tackle different steps of the physics simulation and rendering process. CPUs are used to simulate environment physics, calculate rewards, and run the environment, while GPUs are used to accelerate neural network models during training and inference as well as rendering if required.
However, switching back and forth between CPU cores optimized for sequential tasks and GPUs which offer large-scale parallelism is by nature inefficient, requiring data to be transferred between different parts of the system at multiple points during the training process. Therefore, scalability of deep reinforcement learning in robotics is faced with two critical bottlenecks: 1) enormous computational requirements and 2) limited simulation speed. These problems are especially challenging when learning long-horizon behaviours for robots with high degrees of freedom.
Popular physics engines like MuJoCo[6], PyBullet[7], DART[8], Drake[9], V-Rep[10] etc. need large CPU clusters to solve challenging RL tasks naturally face these bottlenecks. For instance, in [11], almost 30,000 CPU cores ( 920 worker machines with 32 cores each) were used to train a robot to solve the Rubik's Cube task using RL. In a similar task, [5] used a cluster of 384 systems with 6144 CPU cores, plus 8 NVIDIA V100 GPUs, and required 30 hours of training for RL to converge.
One way to speed-up simulation and training is to make use of hardware accelerators. GPUs have enjoyed enormous success in computer graphics are also naturally suited for highly parallel simulations. This approach was taken by [12], and showed very promising results running simulation on GPU, proving that it is possible to greatly reduce both training time as well as computational resources required to solve very challenging tasks using RL. However, some bottlenecks were still not addressed in the work - simulation was on GPU but physics state was copied back to CPU. There, observations and rewards were calculated using optimized C++ code and later copied back to GPU</p>
<p>where policy and value networks ran. Furthermore, only simplified physics-based scenarios were trained, rather than representative robotic environments, and no attempt was made to show sim2real.</p>
<p>To address these bottlenecks, we present Isaac Gym - an end-to-end high performance robotics simulation platform. It runs an end-to-end GPU accelerated training pipeline, which allows researchers to overcome the aforementioned limitations and achieves 2-3 orders of magnitude of training speed-up in continuous control tasks. Isaac Gym leverages NVIDIA PhysX [13] to provide a GPU-accelerated simulation back-end, allowing it to gather experience data required for robotics RL at rates only achievable using a high degree of parallelism. It provides a PyTorch tensor-based API to access the results of physics simulation natively on the GPU. Observation tensors can be used as inputs to a policy network and the resulting action tensors can be directly fed back into the physics system. We note that others [14] have recently begun attempting an approach similar to Isaac Gym with respect to running end-to-end training on hardware accelerators.</p>
<p>With the end-to-end approach, roll-outs of observation, reward, and action buffers can stay on the GPU for the entire learning process, eliminating the need to read data back from the CPU. This set-up permits tens of thousands of simultaneous environments on a single GPU, allowing researchers to easily run experiments locally on their desktops that previously required an entire data center and to solve previously out of reach tasks using just a small GPU server.</p>
<p>Isaac Gym provides a straightforward API for creating and populating a scene with robots and objects, supporting loading data from the common URDF and MJCF file formats. Each environment is duplicated as many times as needed, while preserving the ability for variations between copies (e.g. via Domain Randomization [15]). Environments are simulated simultaneously in parallel without interaction with other environments. Using a fully GPU-accelerated simulation and training pipeline can help lower the barrier for research, enabling solving of tasks with a single GPU that were previously only possible on massive CPU clusters. Isaac Gym also includes a basic Proximal Policy Optimization (PPO) implementation and a straightforward RL task system, but users may substitute alternative task systems or RL algorithms as desired. While the included examples use PyTorch, users should also be able to integrate with TensorFlow training libraries with further customization. An overview of the system is provided in Figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of the Isaac Gym pipeline. The Tensor API provides an interface to Python code to step the PhysX backend, as well as get and set simulator states, directly on the GPU, allowing a 100-1000x speedup in the overall RL training pipeline while providing high-fidelity simulation and the ability to interface with existing robot models.</p>
<h1>Summary of Results</h1>
<p>Our major contributions include:</p>
<ul>
<li>Development of high-fidelity GPU-accelerated robotics simulator for robot learning tasks.</li>
<li>A Tensor API in Python providing direct access to physics buffers by wrapping them into PyTorch tensors without going through any CPU bottlenecks.</li>
<li>Implementation of multiple highly complex robotic manipulation environments which can be simulated at hundreds of thousands of steps per second on a single GPU.</li>
<li>High-performance training results using Isaac Gym with Deep Reinforcement Learning on challenging robotic environments.</li>
</ul>
<p>Our major empirical results include:</p>
<ul>
<li>We achieve significant speed-ups in training various simulated environments: Ant and Humanoid environments can achieve performant locomotion in 20 seconds and 4 minutes respectively, ANYmal [16] in under 2 minutes, Humanoid character animation using AMP [17] in 6 minutes and cube rotation with Shadow Hand in 35 minutes all on a single NVIDIA A100 GPU.</li>
<li>Additionally, we reproduce OpenAI Shadow Hand cube training setup [5] with asymmetric actor-critic and domain randomization. We show that we can achieve similar performance to OpenAI results of 20 consecutive successes with feed forward and 37 consecutive successes with LSTM networks with a success tolerance of 0.4 rad in about 1 hour and 6 hours on an average respectively on A100. In contrast, OpenAI effort required 30 hours and 17 hours respectively on a combination of a CPU cluster ( 384 CPUs with 16 cores each) and 8 NVIDIA V100 GPUs with MuJoCo [6] using a conventional RL training setup. It is worth mentioning that since OpenAI show results with only 1 seed, comparing our best seed we find that we achieve 37 consecutive successes with LSTMs in just 2.5 hours.</li>
<li>We also demonstrate sim-to-real transfer results on ANYmal and TriFinger which further showcases the ability of our simulator to perform high-fidelity contact rich manipulation.</li>
</ul>
<h2>2 Background</h2>
<h3>2.1 Parallelization Strategy</h3>
<p>There are many approaches to parallelizing physics simulations. We outline these approaches here and justify our design decisions in the context of GPU-accelerated simulation tailored towards learning algorithms. Isaac Gym was developed to maximize the throughput of physics-based machine learning algorithms with particular emphasis on simulations that require large numbers of environment instances executing in parallel.</p>
<h3>2.1.1 CPU Simulations</h3>
<p>When physics simulation runs on CPU, multiple threads can be used to distribute computation among the available cores. The most straightforward strategy is simulating one environment instance per thread. In this approach, scaling is limited by the number of physical cores in the system. On a 64-core hyper-threaded CPU, we could run up to 128 environments in parallel, but CPUs with a large number of cores are typically clocked lower to prevent overheating. Running tens or hundreds of threads comes with other potential pitfalls including synchronization, context-switching overhead, and memory bandwidth limitations. To scale further, we would need to use a multi-CPU setup or build a cluster, which introduces additional communication overhead.
Running a single environment instance per thread in its own dedicated physics scene can be inefficient. There is some overhead involved in setting up, executing, and gathering the results of each physics</p>
<p>step. The simpler the environment, the more significant the overhead. To mitigate this, we can pack multiple environments into a single physics scene. For example, we could split 1024 environments into eight physics scenes with 128 environments each. Each scene can run in its own thread. Extra provisions are needed to ensure that environments in the same scene do not interact with each other physically, which can be done using contact filtering and other methods.</p>
<h1>2.1.2 GPU Simulations</h1>
<p>Running the physics simulation on GPU can result in significant speedups, especially for large scenes with thousands of individual actors. On the GPU, the physics engine can parallelize computations at the level of individual shapes, bodies, or joints. High-end GPUs require many thousands of objects to effectively utilize their streaming multiprocessor architecture. This makes them a good match for running simulations with thousands of environment instances. On GPU, we don't need to worry about splitting the environments into multiple scenes. In fact, the opposite is generally true - we want to pack everything into a single scene to take advantage of the deep fine-grained parallelism and maximize the overall throughput.
Physics simulations on a GPU is not new. In previous work, we demonstrated good results with running GPU physics simulations for reinforcement learning [12]. In this work, the GPU was used as a co-processor that accelerates the physics simulation, while the API for getting physics state and applying controls was CPU-based. There are, however, performance bottlenecks with this strategy. In a reinforcement learning pipeline, physics simulation is just one part of the system. After a physics step, we need to get the latest physics state to compute observations and rewards. If these computations are done on the CPU, we need to transfer the physics state from the GPU. While modern hardware architectures can achieve impressive data transfer speeds, large simulations can incur nontrivial overhead. Then, the raw physics state needs to be processed on the CPU to compute observations and rewards, which is subject to similar parallelization challenges as discussed above due to the limited number of CPU cores. Next, the observations and rewards need to be copied from system memory back to device memory for the reinforcement learning algorithm. After the learning step, a set of actions is generated by the policy network on the GPU. These actions need to be copied to the CPU so that they can be converted to physics simulation inputs. Those inputs end up being copied to the GPU again to run the next step of physics simulation on the device.
Isaac Gym eliminates those inefficiencies by keeping all of the computations on the GPU. Stepping physics, computing observations and rewards, and applying actions are performed on the GPU without ever copying large quantities of data between devices. Two new features were added to PhysX to facilitate this. First, PhysX GPU simulations can run without fetching the results to the CPU after every step. Second, a new direct GPU API was added to access the current state, submit state changes, and apply control inputs in GPU buffers. In Figure 3, we contrast the traditional RL experience collection pipeline with our high throughput fully GPU-based pipeline.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) Traditional RL experience collection pipelines often use CPU based physics engines which quickly become the bottleneck. (b) In contrast, Isaac Gym not only runs physics on the GPU but also directly copies the physics data to the deep neural network framework using CUDA interoperatability without ever using CPU in the process. This massively improves the performance of RL training process leading to significantly faster training times.</p>
<h1>2.2 Simulation Setup</h1>
<p>Isaac Gym provides a simple procedural API to create environments and populate them with actors. It supports loading assets from URDF and MJCF file formats. These assets can be instanced multiple times in simulation environments to create actors. In the underlying PhysX engine, single-body actors are created as rigid dynamics and multi-body actors are created as reduced coordinate articulations. During the setup phase, users can set initial actor poses, configure joint drives, and customize rigid body properties and physics materials. Most joint and rigid body properties can be changed during the simulation as well, which facilitates domain randomization without stopping and restarting the simulation. Below we provide definitions of some useful terms.
Actor: An entity composed of rigid bodies connected via joints. It can be created via direct loading of a URDF model or XML file composed of either meshes or primitive shapes.
Rigid Bodies: A primitive shape or a mesh model that comprises an actor is called a rigid body. The positions, rotations and velocities of a rigid body can be obtained via the API.
DOF States: Rigid bodies are connected by various joints. A joint can have 0 or more degrees of freedom. Fixed joints have no DOFs, revolute and prismatic joints have 1 DOF and spherical joints have 3 DOFs. The DOF states, which include joint position and velocity, can be obtained via the API.
The setup code runs on the CPU to allow flexibility in per-instance setup, but once the simulation starts Isaac Gym provides a tensor API that can be used to interact with the running simulation on either CPU or GPU. Users can specify the device to be used for the simulation and the tensor interface in the simulation parameters.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Tensors associated with the scene composed of multiple copies of the same environment simulating different variations all running in parallel. Each actor (e.g. table, box or franka) has various bodies and their corresponding positions, quaternions and velocities are stored directly in PyTorch tensors.</p>
<h3>2.3 Tensor API</h3>
<p>Isaac Gym provides a data abstraction layer over the physics engine. This allows us to support multiple physics engines with a shared front-end API. In this work, the physics engine is PhysX, although some limited tensor API functionality is available with the FleX physics engine as well.
Instead of calling physics engine functions directly, users can access all of the physics data in flat buffers. This data-oriented approach allows us to eliminate a lot of overhead caused by looping over tens of thousands of individual simulation actors in user code. Physics state is exposed to Python users as global tensors. For example, all rigid body states can be found in a single rigid body state tensor. Figure 4 shows a typical Isaac Gym scene composed of various copies of the same environment simulating different variations all running in parallel and the corresponding tensors associated with it. Control inputs can be applied using global tensors as well. For example, applying forces to all rigid bodies in the simulation can be done using a single function call that takes a tensor containing all of the forces. Users can create custom views or slices of the global tensors to suit their needs. When</p>
<table>
<thead>
<tr>
<th>Tensor</th>
<th>Description</th>
<th>Shape</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actor root state</td>
<td>State of all actor root bodies (position, orientation, <br> linear and angular velocity).</td>
<td>$\left(N_{A}, 13\right)$</td>
<td>Get/Set</td>
</tr>
<tr>
<td>DOF state</td>
<td>State of all degrees of freedom (position and veloc- <br> ity).</td>
<td>$\left(N_{D}, 2\right)$</td>
<td>Get/Set</td>
</tr>
<tr>
<td>Rigid body state</td>
<td>State of all rigid bodies (position, orientation, lin- <br> ear and angular velocity).</td>
<td>$\left(N_{B}, 13\right)$</td>
<td>Get</td>
</tr>
<tr>
<td>DOF forces</td>
<td>Net forces experienced at each degree of freedom.</td>
<td>$N_{D}$</td>
<td>Get</td>
</tr>
<tr>
<td>Rigid body forces</td>
<td>Rigid body forces and torques experienced at force <br> sensor locations.</td>
<td>$\left(N_{F}, 6\right)$</td>
<td>Get</td>
</tr>
<tr>
<td>Net contact forces</td>
<td>Net forces experienced by each rigid body.</td>
<td>$\left(N_{B}, 3\right)$</td>
<td>Get</td>
</tr>
<tr>
<td>Jacobian matrix</td>
<td>Jacobian matrices for a homogeneous group of <br> actors.</td>
<td>Variable</td>
<td>Get</td>
</tr>
<tr>
<td>Mass matrix</td>
<td>Generalized mass matrices for a homogeneous <br> group of actors.</td>
<td>Variable</td>
<td>Get</td>
</tr>
</tbody>
</table>
<p>Table 1: Physics state tensors. $N_{A}$ is the total number of actors, $N_{B}$ is the total number of rigid bodies (including articulation links), $N_{D}$ is the total number of degrees of freedom, and $N_{F}$ is the total number of rigid body force sensors.
multiple environment instances are packed into the simulation, it is possible to create custom views of the data with the environment index as one of the dimensions. This makes it easy to vectorize observation and reward computations by running GPU kernels on multiple environments in parallel.</p>
<h1>2.3.1 Python Interface</h1>
<p>The core of Isaac Gym is implemented using C++ and CUDA. It is completely independent of any Python frameworks commonly used in machine learning. To make the data easily accessible to Python users, Isaac Gym provides utilities that can "wrap" the raw data buffers as tensor objects in common machine learning frameworks like PyTorch. The tensor-wrapping utilities make it possible to share the native CPU or GPU buffers with Python without any copying overhead.</p>
<p>A powerful feature of Isaac Gym is the ability to run the same code on either CPU or GPU by simply toggling a flag. Python users do not need to write custom CUDA or C++ kernels to compute observations, rewards, or actions. When physics state and control tensors are wrapped as PyTorch tensors, users can take advantage of TorchScript JIT to compile their Python functions to lower level scripts which orchestrate the training pipeline quickly.</p>
<h3>2.3.2 Physics State Tensors</h3>
<p>Physics state tensors are used to obtain state snapshots of a running simulation. Isaac Gym allows for interacting with the simulation using maximal and reduced coordinates. Physics state includes the kinematic state of rigid bodies and degrees of freedom (DOFs). Rigid body state consists of position, orientation (quaternion), linear velocity, and angular velocity. DOF state includes position and velocity. In the code snippet below we show how to access them through the API.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Acquire tensor descriptors
<span class="gh">#</span> - Raw storage buffer independent of client framework.
<span class="gh">#</span> - Storage will be on GPU if using GPU pipeline, CPU otherwise.
<span class="gh">#</span> - Same code for CPU and GPU just different device.
root_state_desc = gym.acquire_actor_root_state_tensor(sim)
dof_state_desc = gym.acquire_dof_state_tensor(sim)
<span class="gh">#</span> PyTorch interop
<span class="gh">#</span> No data copying, just wrap the gym buffers as torch tensors.
<span class="gh">#</span> The root state tensor captures the state of the root bodies of all actors.
root_states = gymtorch.wrap_tensor(root_state_desc)
dof_states = gymtorch.wrap_tensor(dof_state_desc)
<span class="gh">#</span> obtaining physics states
<span class="gh">#</span> Physics state includes kinematic states of rigid bodies and degrees of freedom (DOFs).
</code></pre></div>

<div class="codehilite"><pre><span></span><code>root_state_vec = root_states.view(num_envs, actors_per_env, 13)
dof_state_vec = dof_states.view(num_envs, dofs_per_env, 13)
root_p = root_states[..., 0:3] # positions of rigid bodies
root_q = root_states[..., 3:7] # rotations, in quaternions, of rigid bodies
root_v = root_states[..., 7:10] # linear velocities of rigid bodies
root_a = root_states[..., 10:13] # angular velocities of rigid bodies
dof_p = dof_state_vec[..., 0] # joint positions
dof_v = dof_state_vec[..., 1] # joint velocities
</code></pre></div>

<p>Obtaining state information by wrapping physics buffers into PyTorch tensors. CUDA interoperability allows copying the data directly without ever going through the host.</p>
<p>Revolute DOFs use radians and linear DOFs use meters for units. Additional state data includes contact forces, rigid body force sensors, and DOF force sensors. To support operational space control and inverse kinematics applications, Isaac Gym also provides Jacobian and generalized mass matrices which can be obtained for articulated actors.</p>
<p>The available state tensors are listed in Table 1. Most of the state tensors are read-only, except the root state tensor and the DOF state tensor. These two tensors play a special role, because they can be used to fully set the poses and velocities of actors. This can be used during environment resets, when new poses are generated or original poses need to be restored. The root state tensor captures the state of the root bodies of all actors. For single-body actors, the root state fully captures their poses and velocities in maximal coordinates. For articulated actors, the root state can be used to "teleport" them without changing the poses of the descendant articulation links. The DOF state tensor can be used to configure the descendant articulation links using reduced coordinates. Setting new DOF states does not affect the root state. For fixed-base articulated actors, such as mounted robotic arms, the DOF state tensor fully captures the articulation poses and velocities. Users can apply new root and DOF states for all actors at once or to a limited subset using an index buffer. This allows resetting a subset of environments without affecting the rest.</p>
<h1>2.3.3 Physics Control Tensors</h1>
<p>Physics simulation inputs include forces, torques, and PD controls such as position and velocity targets. Forces and torques can be applied to rigid bodies and DOFs. PD targets are applied to DOFs that have been configured to use position or velocity drives. Users can configure the drive parameters like stiffness and damping using a separate API. Table 2 lists the available control tensors. The control tensors are typically created in a higher-level framework like PyTorch, but can be efficiently shared with Isaac Gym using the tensor-wrapping utilities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tensor</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Shape</th>
<th style="text-align: left;">Applied to</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DOF actuation forces</td>
<td style="text-align: left;">Torques or linear forces to be <br> applied to degrees of freedom.</td>
<td style="text-align: left;">$N_{D}$</td>
<td style="text-align: left;">All actors or indexed subset</td>
</tr>
<tr>
<td style="text-align: left;">DOF position targets</td>
<td style="text-align: left;">PD position targets for <br> degrees of freedom.</td>
<td style="text-align: left;">$N_{D}$</td>
<td style="text-align: left;">All actors or indexed subset</td>
</tr>
<tr>
<td style="text-align: left;">DOF velocity targets</td>
<td style="text-align: left;">PD velocity targets for <br> degrees of freedom.</td>
<td style="text-align: left;">$N_{D}$</td>
<td style="text-align: left;">All actors or indexed subset</td>
</tr>
<tr>
<td style="text-align: left;">Rigid body forces</td>
<td style="text-align: left;">Forces to be applied to <br> rigid bodies.</td>
<td style="text-align: left;">$\left(N_{B}, 3\right)$</td>
<td style="text-align: left;">All rigid bodies</td>
</tr>
<tr>
<td style="text-align: left;">Rigid body torques</td>
<td style="text-align: left;">Torques to be applied to <br> rigid bodies.</td>
<td style="text-align: left;">$\left(N_{B}, 3\right)$</td>
<td style="text-align: left;">All rigid bodies</td>
</tr>
</tbody>
</table>
<p>Table 2: Physics control tensors. $N_{B}$ is the total number of rigid bodies (including articulation links) and $N_{D}$ is the total number of degrees of freedom.</p>
<h2>3 Physics Simulation</h2>
<p>Robots are simulated using PhysX [13] reduced coordinate articulations. Any individual rigid bodies may be simulated using either maximal coordinate rigid bodies or single-link reduced coordinate articulations. Articulations with a single link and rigid bodies are equivalent and interchangeable.</p>
<p>We also support tendons to actuate degrees of freedom and they are simulated in PhysX using Fixed Tendon mechanics. The physics of tendons are described in detail in Section A.1. We tested the dynamics of tendons using the Shadow Hand simulation environment, described in Section 6.4.1.</p>
<p>We use the Temporal Gauss Seidel (TGS) [18] solver to compute the future states of objects in our physics simulation. The TGS solver uses the observation that sub-stepping a simulation with a single gauss-seidel solver iteration yields significantly faster convergence than running larger steps with more solver iterations. It folds this process efficiently into the iteration process, calculating the velocity at the end of each iteration and accumulating these velocities (scaled by $d t / N$, where $N$ is the number of iterations) into a per-body accumulated delta buffer. This delta buffer is projected onto the constraint Jacobians and added to the bias terms in the constraints. This approach adds only a few additional operations to a more traditional Gauss-Seidel solver, producing almost identical performance cost per-iteration. However, it achieves the same effect on convergence as having sub-stepped the simulation without the computational expense. With positional joint constraints, an additional rotational term is calculated for joint anchors to improve handling of non-linear motion to avoid linearization artifacts. This term is not necessary (and in fact undesirable) to add to contacts. Various parameters exposed to the user to tune the simulator are described in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Delta time (dt)</td>
<td style="text-align: left;">Controls time-step size</td>
</tr>
<tr>
<td style="text-align: left;">Gravity</td>
<td style="text-align: left;">Controls the gravity in the scene</td>
</tr>
<tr>
<td style="text-align: left;">Collision filtering</td>
<td style="text-align: left;">Filters collisions between shapes</td>
</tr>
<tr>
<td style="text-align: left;">Position iterations</td>
<td style="text-align: left;">Biased (velocity + positional error correcting) solver iterations</td>
</tr>
<tr>
<td style="text-align: left;">Velocity iterations</td>
<td style="text-align: left;">Unbiased (velocity error only correcting) solver iterations</td>
</tr>
<tr>
<td style="text-align: left;">Max bias coefficient</td>
<td style="text-align: left;">Limits the magnitude of position error bias Friction</td>
</tr>
<tr>
<td style="text-align: left;">Restitution</td>
<td style="text-align: left;">Controls bounce</td>
</tr>
<tr>
<td style="text-align: left;">Static/dynamic friction</td>
<td style="text-align: left;">Static and dynamic friction coefficients</td>
</tr>
<tr>
<td style="text-align: left;">Bounce threshold</td>
<td style="text-align: left;">Relative normal velocity limit below which restitution is ignored</td>
</tr>
<tr>
<td style="text-align: left;">Rest offset</td>
<td style="text-align: left;">Distance at which shapes are held separated. Default is 0 but can</td>
</tr>
<tr>
<td style="text-align: left;">Friction offset threshold</td>
<td style="text-align: left;">be increased to hold objects at gap. Useful for thin objects.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Distance at which friction anchors are discarded</td>
</tr>
<tr>
<td style="text-align: left;">Solver offset slop</td>
<td style="text-align: left;">(static friction depends on friction anchor caching)</td>
</tr>
<tr>
<td style="text-align: left;">Friction correlation distance</td>
<td style="text-align: left;">An epsilon value used to correct for round-off errors in contact</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">gen. Corrects small skew effects with rolling spheres or capsules.</td>
</tr>
<tr>
<td style="text-align: left;">Max force</td>
<td style="text-align: left;">Distance at which contacts are merged into a single</td>
</tr>
<tr>
<td style="text-align: left;">Drive stiffness</td>
<td style="text-align: left;">friction constraint</td>
</tr>
<tr>
<td style="text-align: left;">Drive damping</td>
<td style="text-align: left;">Per-body and per-contact force limits</td>
</tr>
<tr>
<td style="text-align: left;">Joint friction</td>
<td style="text-align: left;">Positional error correction coefficient of a PD controller</td>
</tr>
<tr>
<td style="text-align: left;">Joint armature</td>
<td style="text-align: left;">Velocity error correction coefficient of a PD controller</td>
</tr>
<tr>
<td style="text-align: left;">Body/link Damping</td>
<td style="text-align: left;">Per-joint frictional term. Simulates dry friction in a joint.</td>
</tr>
<tr>
<td style="text-align: left;">Max velocity</td>
<td style="text-align: left;">Per-joint armature term - simulates motor inertia.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">World-space linear/angular damping on each body/link</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Linear/angular velocity limits per-body</td>
</tr>
</tbody>
</table>
<p>Table 3: Parameters exposed to tune the simulator.</p>
<h1>4 Environments</h1>
<p>We implemented a diverse set of environments covering different application areas. Here we describe a subset of representative examples and key points related to the training. Benchmark results on the simulation performance and training results are presented in the subsequent sections.</p>
<p>All environments are trained using the Proximal Policy Optimization algorithm [19], using rl_games, a highly-optimized GPU end-to-end implementation from [20]. This implementation vectorizes observations and actions on GPU allowing us to take advantage of the parallelization provided by the simulator. We list the environments used in our experiments below:</p>
<h1>Environments used</h1>
<ol>
<li>
<p>Locomotion Environments</p>
</li>
<li>
<p>Ant</p>
</li>
<li>Humanoid</li>
<li>Ingenuity</li>
<li>
<p>ANYmal</p>
</li>
<li>
<p>Franka Cube Stacking</p>
</li>
<li>Humanoid Character Animation</li>
<li>
<p>Robotic Hands</p>
</li>
<li>
<p>Shadow</p>
</li>
<li>Allegro</li>
<li>Trifinger</li>
</ol>
<p>While Ant and Humanoid are relatively simple environments popularised by MuJoCo continuous control benchmarks, the strength of our simulator really shines when training on environments that are rich in complexity particularly robotic hands. Various meta-data related to simulation setup for these environments is in Table 4.</p>
<h2>Key Experimental Details</h2>
<ul>
<li>Unless stated otherwise, all experiments are done on a system with a single NVIDIA A100 GPU and a single 3.7 GHz Intel i7-8700K CPU</li>
<li>All training runs for each environment are averaged over 5 seeds. The reward curves are plotted with $\mu \pm \sigma$ regions.</li>
<li>All the environments by default follow symmetric actor-critic approach with shared observations as well as shared network for policy and value functions. Sharing the network allows faster forward passes and improves training.</li>
<li>Moreover, for Shadow Hand and TriFinger, we also use an asymmetric actor critic approach [21] with policy observations that are closest to real world settings while value function receives privileged state information from simulation as well as the observations received by the policy. This approach is naturally suited for sim-to-real transfers.</li>
<li>For all environments trained with feed forward networks we use a discount factor of $\gamma=0.99$ while LSTM networks use $\gamma=0.998$. We use a GAE discount factor, $\lambda=0.95$ and clipping $\epsilon=0.2$. Also, we use an adaptive learning rate and varying KL thresholds per environment.</li>
<li>Detailed hyper-parameters for each training task are shown in Table 17. Rewards and observations for each environment we used can be found in Appendix A.2.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Environment</th>
<th style="text-align: left;">Control Type</th>
<th style="text-align: left;">Simulation $d t$</th>
<th style="text-align: left;">Control $d t$</th>
<th style="text-align: left;">Action Dims</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ant</td>
<td style="text-align: left;">Joint Torques</td>
<td style="text-align: left;">$1 / 120 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 60 \mathrm{~s}$</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">Humanoid</td>
<td style="text-align: left;">Joint Torques</td>
<td style="text-align: left;">$1 / 120 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 60 \mathrm{~s}$</td>
<td style="text-align: left;">21</td>
</tr>
<tr>
<td style="text-align: left;">Ingenuity</td>
<td style="text-align: left;">Rigid Body Forces</td>
<td style="text-align: left;">$1 / 200 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 100 \mathrm{~s}$</td>
<td style="text-align: left;">6</td>
</tr>
<tr>
<td style="text-align: left;">ANYmal</td>
<td style="text-align: left;">Joint Position Targets</td>
<td style="text-align: left;">$1 / 200 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 50 \mathrm{~s}$</td>
<td style="text-align: left;">12</td>
</tr>
<tr>
<td style="text-align: left;">Franka Cube Stacking</td>
<td style="text-align: left;">Operation Space Control</td>
<td style="text-align: left;">$1 / 60 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 60 \mathrm{~s}$</td>
<td style="text-align: left;">7</td>
</tr>
<tr>
<td style="text-align: left;">Shadow Hand Standard</td>
<td style="text-align: left;">Joint Position Targets</td>
<td style="text-align: left;">$1 / 120 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 60 \mathrm{~s}$</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">Shadow Hand OpenAI</td>
<td style="text-align: left;">Joint Position Targets</td>
<td style="text-align: left;">$1 / 120 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 20 \mathrm{~s}$</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">Allegro Hand</td>
<td style="text-align: left;">Joint Position Targets</td>
<td style="text-align: left;">$1 / 120 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 20 \mathrm{~s}$</td>
<td style="text-align: left;">16</td>
</tr>
<tr>
<td style="text-align: left;">TriFinger</td>
<td style="text-align: left;">Joint Torques</td>
<td style="text-align: left;">$1 / 200 \mathrm{~s}$</td>
<td style="text-align: left;">$1 / 50 \mathrm{~s}$</td>
<td style="text-align: left;">9</td>
</tr>
</tbody>
</table>
<p>Table 4: Simulation setup for the environments.</p>
<h1>5 Characterising Simulation Performance</h1>
<p>We first characterise the simulation performance as a function of number of environments. As we vary this number, we aim to keep the overall experience an RL agent observes constant by decreasing the horizon length proportionally (i.e. number of steps in PPO) for a fair comparison. While we provide detailed training studies for many environments later, we characterise simulation performance only for Ant, Humanoid and Shadow Hand as they are sufficiently complex to test the limits of the simulation and also represent a gradual increase in the complexity. All three environments use feed forward networks for training.</p>
<h3>5.1 Ant</h3>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Rewards and effective FPS with respect to number of parallel environments for the Ant experiment. Best training time is achieved with 8192 environments and a horizon lengths of 16.</p>
<p>We first experiment with the standard Ant environment where the agent is trained to run on a flat ground. We find that as the number of agents is increased, the training time, as expected, is reduced i.e. changing the number of environments from 256 to 8192 - an increase by 5 orders of magnitude - leads to a reduction in training time to reach 7000 reward by an order of magnitude from 1000 seconds ( $\sim 16.6$ minutes) to 100 seconds ( $\sim 1.6$ minutes). However, note that Ant reaches performant locomotion at 3000 reward in just 20 seconds on a single GPU.
Since Ant is one of the simplest environments to simulate, the number of parallel environment steps per second as depicted in the Figure 5(b) can go as high as 700 K . We do not observe gains when increasing the number of environments from 8192 to 16384 due to reduced horizon length.</p>
<h3>5.2 Humanoid</h3>
<p>The Humanoid environment has more degrees of freedom and requires the agent to discover the gait that lets itself balance on two feet and walk on the ground. As observed in Figure 6 and Figure 7, the training times are increased by an order of magnitude compared to the Ant in Figure 5.
We also note in Figure 6 that as the number of agents is increased, in this case, from 256 to 4096, the training time needed to reach the highest reward of 7000 is reduced by an order of magnitude from $10^{4}$ seconds ( $\sim 2.7$ hours) to $10^{3}$ seconds ( $\sim 17$ minutes). However, performant locomotion starts happening at around a reward of 5000 at a training time of just 4 minutes. Going beyond 4096 environments for this set up resulted in no further gains and in fact led to both increase in training time and sub-optimal gaits. We attribute this to the complexity of the environment that makes it challenging to learn walking at such small horizon lengths.
We verified this by training on another set of environment and horizon length combinations where horizon length was increased by a factor of 2 compared to Figure 6. As shown in the Figure 7, the humanoid is able to walk even with 8192 and 16384 environments which have small horizon lengths of 32 and 16 respectively but sufficiently long to enable learning.
Also worth noting that due to the increased degrees of freedom the number of parallel environment steps per second is reduced from 700 K for Ant to 200 K for Humanoid as shown in Figures 6 and 7.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Rewards and effective FPS with respect to number of parallel environments for the Humanoid experiment. Best training time is achieved with 4096 environments and a horizon lengths of 32.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Rewards and effective FPS with respect to number of parallel environments for the Humanoid experiment. Best training time is achieved with both 4096 and 8192 environments and horizon lengths of 64 and 32 respectively.</p>
<h1>5.3 Shadow Hand</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Rewards and effective FPS with respect to number of parallel environments for the Shadow Hand experiment. Best training time is achieved with both 8192 and 16384 environments and horizon lengths of 16 and 8 respectively.</p>
<p>Lastly, we experiment with Shadow Hand [5] to learn to rotate a cube resting on the palm to a target orientation using the fingers and the wrist. This task is challenging due to the number of DoFs involved and the contacts that are made and broken during the process of rotation. Our results with Shadow Hand environment follow similar trends. As the number of agents is increased, in this case, from 256 to 16384, the training time is reduced by an order of magnitude from $5 \times 10^{4}$ seconds ( $\sim 14$ hours) to $3 \times 10^{3}$ seconds ( $\sim 1$ hour). We find that the environment reaches performant dexterity of $\mathbf{1 0}$ consecutive successes at reward of $\mathbf{3 0 0 0}$ in just $\mathbf{5}$ minutes. ${ }^{1}$ Further performance</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Locomotion environments and the corresponding reward curves.
improvements continue to happen as more experience is collected. Additionally, we find that the horizon length of 8 for 16384 agents still allows learning re-posing the cube. The maximum effective frame-rate of 150 K number of parallel environment steps per second was achieved with 16384 agents.</p>
<h1>6 Characterising Environment Performance</h1>
<p>We now provide details and performance metrics for individual environments mentioned in Section 4 trained using a PPO implementation that operates on vectorised states and actions.</p>
<h3>6.1 Locomotion environments</h3>
<h3>6.1.1 Ant</h3>
<p>The Ant model has four legs with two degrees of freedom per leg. On A100 with 4096 agents simulated in parallel we find that ant can learn to run and achieve a reward above 3000 in just 20 seconds, and fully converge in under 2 minutes. The average simulation performance achieved during training is 540 K environment steps per second. The results are shown in Figure 9(a). For details of the reward function used, we refer to Appendix A.2.1 and for the observations used, we refer to Appendix A.2.1.</p>
<h3>6.1.2 Humanoid</h3>
<p>The Humanoid environment has 21 DOFs and on a A100 with 4096 agents simulated in parallel we can train it to run - a reward threshold of 5000 - in less than 4 minutes. This is 4 x faster than our previous results in [12] obtained using the same threshold. As shown in Figures 6 and 7, we achieve peak performance for this environment at 4096 agents. Figure 9(b) shows the evolution of reward as a function of time. For details of the reward function used, we refer to Appendix A.2.1 and for the observations used, we refer to Appendix A.2.1.</p>
<h3>6.1.3 Ingenuity</h3>
<p>We train a simplified model of NASA's Ingenuity helicopter to navigate to a target that periodically teleports to different locations. The environment with trained with 4096 agents and achieves a reward of 5000 in just under 30 seconds. Forces are applied directly to the two rotors on the chassis, rather than simulating aerodynamics. We use a gravity value of $-3.721 \mathrm{~m} / \mathrm{s}^{2}$ to simulate martian gravity. In Figure 9(c) we show how the reward increases as a function of time.</p>
<h3>6.1.4 ANYmal Robot Locomotion</h3>
<p>ANYmal is a robot developed by ANYbotics for industrial maintenance. It is a four-legged dog-like robot, and has been used for experiments on navigation of rough and variable terrain. We train</p>
<p>the robot to follow target $\mathrm{X}, \mathrm{Y}$, and yaw base velocities while minimizing joint torques. The target velocities are randomized at each reset and are provided as observations alongside the positional and angular velocities of the base, the measured gravity vector, most recent actions, and DOF positions and velocities. With 4096 agents simulating in parallel, we find that the robot is able to follow the targets in under 2 minutes as shown in Figure 9(d). The reward function is defined in A.2.2</p>
<p>ANYmal Sim-to-real on Uneven Terrain In addition to the simple flat terrain environment, we have developed a rough terrain locomotion task for ANYmal and validated the approach by transferring trained policies to the real robot. The robot learns to walk on uneven surfaces, slopes, stairs and obstacles. In addition to the observations of the flat terrain environment it receives terrain height measurements around the robot's base. For sim-to-real transfer we extend the reward function, add noise to the observations, randomize the friction coefficient of the ground, randomly push the robots during the episode and add an actuator network to the simulation. Following the approach used in [22], the actuator network is trained to model the complex dynamics of the series elastic actuators of the real robot.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Trained policy for ANYmal on rough terrain tested in simulation and on the real robot.</p>
<p>We implement an automatic curriculum of increasing terrain difficulties. The robots start to learn on simple versions of the terrains, and when they are able to solve a certain level the difficulty is automatically increased. In order to avoid costly terrain generation during training, we create a single mesh with all terrain types and levels and change the robots' reset location depending on their progress. With 4096 environments, we can train the full task on NVIDIA RTX A6000 and transfer to the real robot in under 20 minutes. We refer to [23] for more details.</p>
<h1>6.2 Humanoid Character Animation</h1>
<p>We evaluate the performance of Isaac Gym on adversarial imitation learning tasks using an implementation of adversarial motion priors (AMP) [17]. This technique enables physically simulated humanoid character to imitate complex behaviors from reference motion data. Instead of a manually engineered imitation objective, as is commonly used in prior systems [24], AMP learns an imitation objective using an adversarial discriminator trained to differentiate between motion from the dataset and motions produced by the policy.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Humanoid character trained using AMP to imitate a spin-kick.</p>
<p>Our character is modelled as a 34-DOF humanoid [17], and all motion clips are recorded from human actors using motion capture. Table 11 in Appendix A.2.2 details the observation features. The adversarial training process enables the character to closely imitate a diverse corpus of motions, ranging from common locomotion behaviors, such as walking and running, to more athletic behaviors, such as spin-kicks and dancing. Effective policies can be learned with approximately 39 million samples, requiring approximately $\mathbf{6}$ minutes with 4096 environments. The implementation provided by Peng et al., 2021 [17] requires about 1 day ( 30 hours) on 16 CPU cores to simulate a similar number of samples in PyBullet. Therefore, Isaac Gym provides 300x or 2.48 orders of magnitude improvement in the training time.</p>
<h1>6.3 Franka Cube Stacking</h1>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>We use 16384 agents to train a Franka robot to stack a cube on top of an other. In this environment, we use a slightly different choice of action space, Operation Space Control (OSC), for learning. OSC [25] is a task-space compliant controller that has been shown to enable faster policy learning compared to joint-space controllers [26] and learn
Figure 12: The Franka Cube Stacking environment and the corresponding reward curves.
contact-rich tasks [27]. Our OSC implementation is fully differentiable in Isaac Gym and we obtain convergence with this controller in under 25 minutes. Figure 12 shows the training results.</p>
<h3>6.4 Robotic Hands</h3>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: The three in-hand manipulation environments implemented in Isaac Gym: Shadow Hand, Trifinger, and Allegro.</p>
<p>Large-scale simulation has the ability to solve not just individual instances but whole classes of problems in robotics, by leveraging the generality of the model-free reinforcement learning framework. Dexterous manipulations is one of the most challenging problems in robotics.</p>
<p>To show the performance of our simulator and the ability to realistically model contact we implemented 3 different hand training environments as shown in Figure13. Shadow Hand and Allegro Hand are trained to learn cube orientation while TriFinger learns to repose the cube in 6 degrees-offreedom involving rotation and translation. We now focus on the specific training details for these environments.</p>
<p>Firstly, the Shadow Dexterous Hand. We follow the standard formulation where policy and value function both receive the same input as well as OpenAI observations with asymmetric formulation and domain randomisation from [5]. Secondly, the TriFinger robot [28], which shows the ability to do 6-DoF manipulation by reposing the cube to a desired position and orientation, a task which has previously shown to be challenging for model-free reinforcement learning [29]. We use asymmetric actor-critic and domain randomisation for TriFinger and demonstrate sim-to-real transfer on a real robot. Finally, we reuse system from the Shadow Hand to the Allegro hand with minimal changes to show the generality of our approach. These three environments are depicted in Figure 13 and the corresponding reward curves in Figure 14.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Reward curves for the three in-hand manipulation environments implemented in Isaac Gym. These results are obtained with (a) Shadow Hand with OpenAI observation and LSTMs, (b) Shadow Hand with OpenAI observation and feed forward networks (c) Shadow Hand with Standard observations and (d) Allegro Hand with Standard observations. Shadow Hand OpenAI is trained with asymmetric actor-critic and domain radomisation while Shadow Hand Standard and Allegro Hand Standard are trained with standard observations and symmetric actor-critic with no domain randomisation.</p>
<h1>6.4.1 Shadow Hand</h1>
<p>As mentioned, the task with Shadow Hand is to manipulate the cube to achieve a specific target orientation and is inspired by OpenAI et al. [5]. We train with multiple variants on the Shadow Hand environment and describe them below:</p>
<p>Shadow Hand Standard In this setting, we use a standard formulation for training where the policy and the value function use feed forward networks and receive the same input observations. The default observations we used for the Shadow Hand Standard include joint position, velocities, forces, force-torque sensors reading from each fingertip, manipulated object position and orientation, linear and angular velocities, goal orientation, relative rotation between the current object and target rotations, actions applied on the previous step. For a detailed overview of observation and reward, see Appendix A.4. Also note that this variant does not use any randomisations.</p>
<p>Shadow Hand OpenAI We also reproduce results with OpenAI Shadow Hand experiments in Isaac Gym with observations used in dexterity work from OpenAI et al. [5]. A key difference between this and the Shadow Hand Standard variant is that it uses asymmetric observations. The policy receives only the input observations that are possible to obtain in the real world settings while the value function receives the same observations in addition to the other privileged information available from the simulator. This variant should make it possible to transfer the policy to the real world, mimicking the setup in [5]. The observations for the policy and value function are provided in Table 14. We experiment with both feed forward networks (SH OpenAI FF) and LSTMs (SH OpenAI LSTM). The LSTM networks are trained with a sequence length of 4.</p>
<p>It is worth noting that only networks trained with OpenAI observations use domain randomisation to closely match the results in OpenAI dexterity work [5].
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Consecutive successes per episode for (a) Shadow Hand with OpenAI observation and LSTMs, (b) Shadow Hand with OpenAI observation and feed forward networks (c) Shadow Hand with Standard observations and (d) Allegro Hand with Standard observations. Shadow Hand Standard and Allegro Hand Standard both use feed forward networks for policy and value functions.</p>
<p>Randomizations For domain randomization we closely followed the approach proposed in [5] and applied correlated and uncorrelated noise to observations, actions, as well as randomized cube size</p>
<p>and all the key physics properties - masses, inertia tensors, friction, restitution, joint limits, stiffness and damping. Full details of these are available in Appendix A.4.1.</p>
<p>We outline a few important differences between our setup and the one used in the OpenAI work below:</p>
<ul>
<li>While OpenAI used a success tolerance of $0.4 \mathrm{rad}^{2}$ [5], we use both 0.4 rad and a tighter tolerance of 0.1 rad . We focus on results with 0.4 rad in this section and provide results with 0.1 tolerance in Appendix A.4.2</li>
<li>We use a continuous as opposed to a discrete control space used in [5].</li>
<li>Our results are averaged with 5 seeds while OpenAI show results with only 1 seed $^{3}$.</li>
<li>The randomizations used in our work do not include action delay and motor backlash.</li>
<li>We use an LSTM layer of 1024 hidden units after the input followed by an MLP layer of 512 hidden units. On the other hand OpenAI et al. [5] used an MLP layer of size 1024 after the input followed by an LSTM layer of size 512 hidden units. We found our setting performs better with Isaac Gym.</li>
<li>We use a somewhat different reward function to OpenAI as shown in Appendix A.2.3.</li>
<li>Our experiments are only in simulation and unlike [5] we do not attempt any sim-to-real transfer for the Shadow Hand experiment.</li>
</ul>
<p>Figure 14(a), (b) and (c) show the reward curves for various settings we used for Shadow Hand. Shadow Hand Standard - trained with no randomization and uses symmetric actor critic setting with a feed forward network - is the fastest to reach a reward of 6000 . This setting achieves 20 consecutive successes in under 35 minutes. Important to remember that this setting is not suitable for sim-to-real transfer as it includes some observations that may not be directly available in the real world.</p>
<p>We now focus on experiments with OpenAI observations and asymmetric feed-forward actor-critic. This setting is suited for sim-to-real transfer and the policy uses only the observations that are possible to obtain in the real world. As shown in Figure 15(b), we achieved more than 20 consecutive successes in less than 1 hour. In contrast, for the same performance it takes 30 hours on the OpenAI setup consisting of CPU based simulation and training setup running MuJoCo [6] simulator on a cluster of 384 16-core CPUs with 6144 CPU cores in total and using 8 NVIDIA V100 GPUs for training. In Figure 15(a) we show that using LSTM networks, the performance increases and we can reach 37 consecutive successes in just under than 6 hours while OpenAI et al. [5] achieve same performance in $\sim 17$ hours. Since OpenAI et al. [5] show results only with 1 seed, comparing their result with our best seed we note that 37 consecutive successes with LSTM experiments can be achieved in just 2.5 hours. We provide the results for Shadow Hand OpenAI experiment with success tolerance of 0.1 in the Appendix A.4.</p>
<h1>6.4.2 TriFinger</h1>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: TriFinger reward and the corresponding success rate.</p>
<p>The TriFinger manipulation task, originating in [28], involves picking a cube lying on a flat surface and repositioning it to a desired 6-degrees-of-freedom pose. The manipulator has 3 fingers each with three degrees of freedom. In [29], it was shown that Isaac Gym training combined with Domain Randomization allows sim-to-real transfer. The environment is shown in Figure 13.</p>
<p>We use an asymmetric actor-critic formulation for this system as that allows to design a policy that uses input observations that are possible to obtain in the real world and therefore enable sim-to-real transfer. We show the reward and success</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17: Trifinger learns a variety of dexterous manipulation behaviours in order to move the cube to the correct position and orientation. These results are obtained on the real TriFinger robot hosted by [28, 30].
rate in simulation in Figure 16. We also transfer results from simulation to the real world and note that our mean success rate in the real world is $55 \%$. We refer to [29] for more detailed analysis.</p>
<p>In particular, this example shows the ability of policies learned using Isaac Gym's physics to generalize to the real world. Some of the behaviours leaned by the policy are shown in the Figure 17. It is worth noting that the robot is situated in a different location and therefore the sim-to-real transfer was done remotely.</p>
<h1>6.4.3 Allegro Hand</h1>
<p>We learn cube orientation with Allegro Hand and use the same reward as for the Shadow Hand as well similar observation scheme, with the only difference - smaller number of observations because of the different number of fingers in Allegro Hand - that it has 4 fingers instead of 5 and fewer degrees of freedom as a result, shown in Appendix A.2.3.</p>
<p>Figure 14(d) shows the reward curves for Allegro Hand and Figure 15(d) shows consecutive successes achieved. Interestingly, despite having fewer degrees of freedom this hand does not achieve as high consecutive successes as Shadow hand. This is because the wrist is fixed and fingers are slightly longer. We observed in Shadow hand experiment that having a movable wrist allows for better manipulation when reorienting the cube.</p>
<h2>7 Summary</h2>
<p>We show that Isaac Gym is a high performance and high-fidelity framework that allows blistering fast training on many challenging simulated robotic environments on a single NVIDIA A100 GPU that previously would have required large heterogeneous clusters of CPUs and GPUs using a conventional RL setup with CPU-only simulators. Moreover, the simulation backend [13] is also suited for learning contact-rich manipulations as confirmed by our sim-to-real transfer demonstrations with ANYmal locomotion and TriFinger cube reposing.</p>
<h1>8 Acknowledgements</h1>
<p>We would like to thank the following for additional hard work helping us with this work.
Jonah Alben, Rika Antonova, Ayon Bakshi, Dennis Da, Shoubhik Debnath, Clemens Eppner, Dieter Fox, Animesh Garg, Renato Gasoto, Isabella Huang, Andrew Kondrich, Rev Lebaredian, Qiyang Li, Jacky Liang, Denys Makoviichuk, Brendon Matusch, Hammad Mazhar, Mayank Mittal, Adam Moravansky, Yashraj Narang, Oyindamola Omotuyi, Fabio Ramos, Andrew Reidmeyer, Philipp Reist, Tony Scudiero, Mike Skolones, Balakumar Sundaralingam, Liila Torabi, Cameron Upright, Zhaoming Xie, Winnie Xu, Yuke Zhu, and the rest of the NVIDIA PhysX, Omniverse, and robotics research teams. We also thank Jason Peng and Josiah Wong for the help in AMP and Franka Cube Stacking experiments.
Thanks are also due to open-source community projects like Matplotib[31], Python[32], NumPy[33], PyTorch[34], Tensorboard[35], Tensorboard Aggregator[36] and SciencePlots[37] which we used heavily in this work. We are thankful to Overleaf [38] for hosting our latex project.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ page 22, section C.1, paragraph Goals in [5]
${ }^{3}$ page 11, section 6.3, Ablation of Randomizations, Figure 8 in [5]&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
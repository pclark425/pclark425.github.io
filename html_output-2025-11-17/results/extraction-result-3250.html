<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3250 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3250</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3250</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-253420805</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2211.04591v1.pdf" target="_blank">Learning to Follow Instructions in Text-Based Games</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games present a unique class of sequential decision making problem in which agents interact with a partially observable, simulated environment via actions and observations conveyed through natural language. Such observations typically include instructions that, in a reinforcement learning (RL) setting, can directly or indirectly guide a player towards completing reward-worthy tasks. In this work, we study the ability of RL agents to follow such instructions. We conduct experiments that show that the performance of state-of-the-art text-based game agents is largely unaffected by the presence or absence of such instructions, and that these agents are typically unable to execute tasks to completion. To further study and address the task of instruction following, we equip RL agents with an internal structured representation of natural language instructions in the form of Linear Temporal Logic (LTL), a formal language that is increasingly used for temporally extended reward specification in RL. Our framework both supports and highlights the benefit of understanding the temporal semantics of instructions and in measuring progress towards achievement of such a temporally extended behaviour. Experiments with 500+ games in TextWorld demonstrate the superior performance of our approach.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3250.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3250.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Aided Transformer Agent (GATA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art model-free text-based game RL agent that augments a transformer-based policy with a learned dynamic belief graph acting as long-term structured memory (triplet graph of (object, relation, object)). It was used as the baseline in TextWorld Cooking experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer-based RL agent (transformer variant of the LSTM-DQN architecture) combined with a dynamic belief graph updater and an R-GCN graph encoder; the belief graph is pre-trained (COC or GTP variants) and updated from observations to produce g_t which the policy conditions on.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>TextWorld (Cooking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Choice-based cooking tasks: examine cookbook to obtain a recipe, gather ingredients, perform preparatory actions (cut/fry/etc.), prepare and eat the meal; success = full recipe completed and eaten.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured graphical long-term memory (dynamic belief graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Belief graph represented as a list of triplets (object, relationship, object) updated by a graph updater from observations; encoded by an R-GCN into a vector GE(g_t) and concatenated with observation encodings for the action selector. The belief graph also provides predicate detections (e.g., apple-needs-cut) used as event detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: GATA uses the belief graph but still largely fails to exploit textual instructions; quantitative example reported: original GATA achieves ~0.4 normalized game points on Level 2 while its actual game success rate is near 0 (i.e., gathers reward without completing tasks). GATA rarely examined the cookbook in tests (<15% of testing games). (Exact full score tables not enumerated in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>When in-game textual instructions were stripped (removing cookbook/recipe/knife warnings), GATA's normalized game points and success rates remained largely unchanged, indicating it did not exploit those instructions; forcing GATA to examine the cookbook (so its belief state contains goal relations) also produced no meaningful performance improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>The belief graph can capture goal relations and state predicates (e.g., 'apple-needs-cut') and is a natural mechanism for long-term memory and event detection, but in practice GATA did not exploit these to follow instructions in the tested scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Despite having a long-term belief graph, GATA often ignored in-game instructions (did not examine cookbook), failed to use goal relations in its belief state to complete tasks, and achieved low task-completion (success) rates; fixes to other implementation bugs improved performance but did not resolve instruction-following failure. Navigation/exploration problems are orthogonal but remain a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Pre-train or augment the graph updater to include key triples (e.g., 'cookbook is examined') so progression/event detection is possible; simply providing belief-graph information is insufficient—agents must be architected to explicitly attend to or condition on instruction-bearing state; pair structured memory with explicit instruction representations and progression to incentivize following instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Follow Instructions in Text-Based Games', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3250.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3250.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTL-GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LTL-augmented Graph Aided Transformer Agent (LTL-GATA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An augmentation of GATA that translates textual instructions into Linear Temporal Logic (LTL), monitors instruction progress via LTL progression using the agent's belief graph as event detectors, and conditions the policy on observation, LTL state, and belief graph to improve instruction following and task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LTL-GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends GATA by (1) generating LTL formulas from textual observations (simple translator or GPT-3), (2) progressing these formulas stepwise using truth assignments extracted from the GATA belief graph (Spot engine used for progression), (3) encoding LTL as text tokens via a transformer text encoder, and (4) concatenating TE(observation), TE(LTL_formula), and GE(belief_graph) to form the policy input.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>TextWorld (Cooking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same cooking tasks as GATA: examine cookbook, gather and prepare ingredients, prepare/consume meal; the agent is additionally tasked with satisfying LTL-encoded instructions (NEXT cookbook-is-examined; conjunction of EVENTUALLY predicates for recipe items; EVENTUALLY player-at-kitchen for navigation levels).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>combined structured memory: dynamic belief graph (long-term) + temporal instruction state via progressed LTL (instruction memory / task-tracking)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Belief graph (GATA) provides labelling function L(s,a) for propositions; LTL progression prog(L(s,a), phi) is computed each step (Spot engine) to update remaining instruction formula phi_t; LTL strings are tokenized (predicates as single-token or multi-token), encoded by a transformer, and concatenated with observation and graph encodings for the Q-network; LTL-based reward bonuses and optional LTL-based episode termination can be applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Substantially improved over baselines: LTL-GATA shows consistently high normalized game points and high success rates across levels; paper reports that LTL-GATA maintains performance on Level 2 (where baselines drop) and that average success rate across both training sets is approximately 82% of the normalized points metric. Exact numeric tables are shown in the paper figures (higher than GATA/TDQN baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablations: removing LTL progression (i.e., not updating instruction state) causes a large performance drop—LTL-GATA without progression falls below baseline performance; removing both the LTL bonus reward and LTL-based termination leads the agent to not examine the cookbook and regress to baseline (GATA) performance. Thus, without the progressing instruction state or incentives, performance collapses.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Enables reliable instruction following, substantially higher task-completion (success) rates and normalized game points; monitoring instruction progression provides an explicit signal of progress and reduces agent confusion; structured temporal instructions guide long-term planning and improve completion of temporally extended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Still suffers from navigation/exploration failures in levels requiring searching for rooms/items (LTL provides the requirement but not the low-level navigation strategy); relies on a domain-aware translator to construct initial LTL from text (authors mitigate with a GPT-3 proof-of-concept), and requires progression and incentive mechanisms (LTL reward/termination) to ensure cookbook is examined; if progression is omitted or incentives removed performance drops sharply.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Represent instructions with a formal temporal language (LTL) and monitor progression at every time step using event detectors from a belief-state memory; encode LTL and include it as part of the agent's input state; use LTL progression rather than static instructions (progression is critical); provide rewards or termination signals tied to instruction satisfaction to incentivize inspection of instruction-bearing observations; pre-train the belief-graph updater to emit relevant predicates/triples (e.g., cookbook-is-examined).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Follow Instructions in Text-Based Games', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3250.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3250.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 LTL translator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (few-shot) Natural Language to LTL translator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proof-of-concept use of OpenAI's GPT-3 (Ada and Da Vinci) in few-shot mode to translate TextWorld natural-language cookbook/recipe observations into LTL formulas; Da Vinci performed well on the provided examples while Ada sometimes hallucinated predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-3 (Da Vinci / Ada) as LTL translator</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pretrained autoregressive large language model used in a few-shot prompting setup (six example NL->LTL pairs) to complete prompts translating textual recipe observations into LTL expressions; no additional fine-tuning was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>TextWorld (Cooking domain) — used only to generate NL -> LTL translation pairs, not for game-playing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate in-game natural language observations (cookbook/recipe text) into LTL formulas that enumerate EVENTUALLY predicates for recipe goals (and NEXT/EVENTUALLY navigation/cookbook instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Few-shot prompt with six example NL:LTL pairs followed by the test observation; models responded with LTL structures. No additional memory augmentation or stateful use inside the game agent was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Not applicable for game-play memory; demonstrates that pretrained LLMs can be used to automate generation of structured LTL instructions from textual observations, potentially reducing domain engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Smaller/faster GPT-3 variant (Ada) hallucinated incorrect predicates in examples; few-shot approach may be brittle and require careful prompt engineering or larger models (Da Vinci performed better). The translator approach still requires reliable downstream grounding of predicates in the belief graph.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Few-shot LLM translation is a feasible proof-of-concept—use higher-capacity variants (Da Vinci) or more examples for robustness; consider fine-tuning or hybrid neuro-symbolic parsers for production-quality NL->LTL translation; always validate translated predicates against the agent's vocabulary and belief-state schema.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Follow Instructions in Text-Based Games', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic belief graphs to generalize on text-based games. <em>(Rating: 2)</em></li>
                <li>TextWorld: A learning environment for text-based games. <em>(Rating: 2)</em></li>
                <li>LTL2action: Generalizing LTL instructions for multi-task RL <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Advice-based exploration in model-based reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3250",
    "paper_id": "paper-253420805",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "GATA",
            "name_full": "Graph Aided Transformer Agent (GATA)",
            "brief_description": "A state-of-the-art model-free text-based game RL agent that augments a transformer-based policy with a learned dynamic belief graph acting as long-term structured memory (triplet graph of (object, relation, object)). It was used as the baseline in TextWorld Cooking experiments.",
            "citation_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "mention_or_use": "use",
            "agent_name": "GATA",
            "agent_description": "Transformer-based RL agent (transformer variant of the LSTM-DQN architecture) combined with a dynamic belief graph updater and an R-GCN graph encoder; the belief graph is pre-trained (COC or GTP variants) and updated from observations to produce g_t which the policy conditions on.",
            "game_or_benchmark_name": "TextWorld (Cooking domain)",
            "task_description": "Choice-based cooking tasks: examine cookbook to obtain a recipe, gather ingredients, perform preparatory actions (cut/fry/etc.), prepare and eat the meal; success = full recipe completed and eaten.",
            "uses_memory": true,
            "memory_type": "structured graphical long-term memory (dynamic belief graph)",
            "memory_implementation_details": "Belief graph represented as a list of triplets (object, relationship, object) updated by a graph updater from observations; encoded by an R-GCN into a vector GE(g_t) and concatenated with observation encodings for the action selector. The belief graph also provides predicate detections (e.g., apple-needs-cut) used as event detectors.",
            "performance_with_memory": "Qualitative: GATA uses the belief graph but still largely fails to exploit textual instructions; quantitative example reported: original GATA achieves ~0.4 normalized game points on Level 2 while its actual game success rate is near 0 (i.e., gathers reward without completing tasks). GATA rarely examined the cookbook in tests (&lt;15% of testing games). (Exact full score tables not enumerated in text.)",
            "performance_without_memory": "When in-game textual instructions were stripped (removing cookbook/recipe/knife warnings), GATA's normalized game points and success rates remained largely unchanged, indicating it did not exploit those instructions; forcing GATA to examine the cookbook (so its belief state contains goal relations) also produced no meaningful performance improvement.",
            "has_performance_comparison": true,
            "memory_benefits": "The belief graph can capture goal relations and state predicates (e.g., 'apple-needs-cut') and is a natural mechanism for long-term memory and event detection, but in practice GATA did not exploit these to follow instructions in the tested scenarios.",
            "memory_limitations_or_failures": "Despite having a long-term belief graph, GATA often ignored in-game instructions (did not examine cookbook), failed to use goal relations in its belief state to complete tasks, and achieved low task-completion (success) rates; fixes to other implementation bugs improved performance but did not resolve instruction-following failure. Navigation/exploration problems are orthogonal but remain a limitation.",
            "best_practices_or_recommendations": "Pre-train or augment the graph updater to include key triples (e.g., 'cookbook is examined') so progression/event detection is possible; simply providing belief-graph information is insufficient—agents must be architected to explicitly attend to or condition on instruction-bearing state; pair structured memory with explicit instruction representations and progression to incentivize following instructions.",
            "uuid": "e3250.0",
            "source_info": {
                "paper_title": "Learning to Follow Instructions in Text-Based Games",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "LTL-GATA",
            "name_full": "LTL-augmented Graph Aided Transformer Agent (LTL-GATA)",
            "brief_description": "An augmentation of GATA that translates textual instructions into Linear Temporal Logic (LTL), monitors instruction progress via LTL progression using the agent's belief graph as event detectors, and conditions the policy on observation, LTL state, and belief graph to improve instruction following and task completion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LTL-GATA",
            "agent_description": "Extends GATA by (1) generating LTL formulas from textual observations (simple translator or GPT-3), (2) progressing these formulas stepwise using truth assignments extracted from the GATA belief graph (Spot engine used for progression), (3) encoding LTL as text tokens via a transformer text encoder, and (4) concatenating TE(observation), TE(LTL_formula), and GE(belief_graph) to form the policy input.",
            "game_or_benchmark_name": "TextWorld (Cooking domain)",
            "task_description": "Same cooking tasks as GATA: examine cookbook, gather and prepare ingredients, prepare/consume meal; the agent is additionally tasked with satisfying LTL-encoded instructions (NEXT cookbook-is-examined; conjunction of EVENTUALLY predicates for recipe items; EVENTUALLY player-at-kitchen for navigation levels).",
            "uses_memory": true,
            "memory_type": "combined structured memory: dynamic belief graph (long-term) + temporal instruction state via progressed LTL (instruction memory / task-tracking)",
            "memory_implementation_details": "Belief graph (GATA) provides labelling function L(s,a) for propositions; LTL progression prog(L(s,a), phi) is computed each step (Spot engine) to update remaining instruction formula phi_t; LTL strings are tokenized (predicates as single-token or multi-token), encoded by a transformer, and concatenated with observation and graph encodings for the Q-network; LTL-based reward bonuses and optional LTL-based episode termination can be applied.",
            "performance_with_memory": "Substantially improved over baselines: LTL-GATA shows consistently high normalized game points and high success rates across levels; paper reports that LTL-GATA maintains performance on Level 2 (where baselines drop) and that average success rate across both training sets is approximately 82% of the normalized points metric. Exact numeric tables are shown in the paper figures (higher than GATA/TDQN baselines).",
            "performance_without_memory": "Ablations: removing LTL progression (i.e., not updating instruction state) causes a large performance drop—LTL-GATA without progression falls below baseline performance; removing both the LTL bonus reward and LTL-based termination leads the agent to not examine the cookbook and regress to baseline (GATA) performance. Thus, without the progressing instruction state or incentives, performance collapses.",
            "has_performance_comparison": true,
            "memory_benefits": "Enables reliable instruction following, substantially higher task-completion (success) rates and normalized game points; monitoring instruction progression provides an explicit signal of progress and reduces agent confusion; structured temporal instructions guide long-term planning and improve completion of temporally extended tasks.",
            "memory_limitations_or_failures": "Still suffers from navigation/exploration failures in levels requiring searching for rooms/items (LTL provides the requirement but not the low-level navigation strategy); relies on a domain-aware translator to construct initial LTL from text (authors mitigate with a GPT-3 proof-of-concept), and requires progression and incentive mechanisms (LTL reward/termination) to ensure cookbook is examined; if progression is omitted or incentives removed performance drops sharply.",
            "best_practices_or_recommendations": "Represent instructions with a formal temporal language (LTL) and monitor progression at every time step using event detectors from a belief-state memory; encode LTL and include it as part of the agent's input state; use LTL progression rather than static instructions (progression is critical); provide rewards or termination signals tied to instruction satisfaction to incentivize inspection of instruction-bearing observations; pre-train the belief-graph updater to emit relevant predicates/triples (e.g., cookbook-is-examined).",
            "uuid": "e3250.1",
            "source_info": {
                "paper_title": "Learning to Follow Instructions in Text-Based Games",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "GPT-3 LTL translator",
            "name_full": "GPT-3 (few-shot) Natural Language to LTL translator",
            "brief_description": "A proof-of-concept use of OpenAI's GPT-3 (Ada and Da Vinci) in few-shot mode to translate TextWorld natural-language cookbook/recipe observations into LTL formulas; Da Vinci performed well on the provided examples while Ada sometimes hallucinated predicates.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "agent_name": "GPT-3 (Da Vinci / Ada) as LTL translator",
            "agent_description": "Pretrained autoregressive large language model used in a few-shot prompting setup (six example NL-&gt;LTL pairs) to complete prompts translating textual recipe observations into LTL expressions; no additional fine-tuning was performed.",
            "game_or_benchmark_name": "TextWorld (Cooking domain) — used only to generate NL -&gt; LTL translation pairs, not for game-playing",
            "task_description": "Translate in-game natural language observations (cookbook/recipe text) into LTL formulas that enumerate EVENTUALLY predicates for recipe goals (and NEXT/EVENTUALLY navigation/cookbook instructions).",
            "uses_memory": null,
            "memory_type": null,
            "memory_implementation_details": "Few-shot prompt with six example NL:LTL pairs followed by the test observation; models responded with LTL structures. No additional memory augmentation or stateful use inside the game agent was reported.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "memory_benefits": "Not applicable for game-play memory; demonstrates that pretrained LLMs can be used to automate generation of structured LTL instructions from textual observations, potentially reducing domain engineering.",
            "memory_limitations_or_failures": "Smaller/faster GPT-3 variant (Ada) hallucinated incorrect predicates in examples; few-shot approach may be brittle and require careful prompt engineering or larger models (Da Vinci performed better). The translator approach still requires reliable downstream grounding of predicates in the belief graph.",
            "best_practices_or_recommendations": "Few-shot LLM translation is a feasible proof-of-concept—use higher-capacity variants (Da Vinci) or more examples for robustness; consider fine-tuning or hybrid neuro-symbolic parsers for production-quality NL-&gt;LTL translation; always validate translated predicates against the agent's vocabulary and belief-state schema.",
            "uuid": "e3250.2",
            "source_info": {
                "paper_title": "Learning to Follow Instructions in Text-Based Games",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "rating": 2
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games.",
            "rating": 2
        },
        {
            "paper_title": "LTL2action: Generalizing LTL instructions for multi-task RL",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Advice-based exploration in model-based reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.015648000000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Follow Instructions in Text-Based Games
8 Nov 2022</p>
<p>Mathieu Tuli mathieutuli@cs.toronto.edu 
Andrew C Li andrewli@cs.toronto.edu 
Pashootan Vaezipoor pashootan@cs.toronto.edu 
Toryn Q Klassen 
Schwartz Reisman Institute for Technology and Society
TorontoCanada</p>
<p>Scott Sanner ssanner@mie.utoronto.ca 
Sheila A Mcilraith sheila@cs.toronto.edu 
Schwartz Reisman Institute for Technology and Society
TorontoCanada</p>
<p>University of Toronto
TorontoCanada</p>
<p>Vector Institute for Artificial Intelligence
TorontoCanada</p>
<p>Learning to Follow Instructions in Text-Based Games
8 Nov 2022818C5ED2B5164EF77571672AF0AD61BDarXiv:2211.04591v1[cs.LG]
Text-based games present a unique class of sequential decision making problem in which agents interact with a partially observable, simulated environment via actions and observations conveyed through natural language.Such observations typically include instructions that, in a reinforcement learning (RL) setting, can directly or indirectly guide a player towards completing reward-worthy tasks.In this work, we study the ability of RL agents to follow such instructions.We conduct experiments that show that the performance of state-of-the-art text-based game agents is largely unaffected by the presence or absence of such instructions, and that these agents are typically unable to execute tasks to completion.To further study and address the task of instruction following, we equip RL agents with an internal structured representation of natural language instructions in the form of Linear Temporal Logic (LTL), a formal language that is increasingly used for temporally extended reward specification in RL.Our framework both supports and highlights the benefit of understanding the temporal semantics of instructions and in measuring progress towards achievement of such a temporally extended behaviour.Experiments with 500+ games in TextWorld demonstrate the superior performance of our approach.</p>
<p>Introduction</p>
<p>Building AI agents that can understand natural language is an important and longstanding problem in AI.In recent years, instrumented text-based game (TBG) engines have served as compelling environments for studying a variety of tasks related to language understanding, affordance extraction, memory, and sequential decision making (e.g., Côté et al., 2018;Adhikari et al., 2020;Liu et al., 2022).They provide a simulated, partially observable environment where an agent can navigate and interact with environment objects, receiving observations and administering commands via natural language.TextWorld (Côté et al., 2018) is a TBG learning environment for training reinforcement learning (RL) agents.Successful play requires language understanding, effective navigation, memory, and an ability to follow instructions embedded within the text.Instructions may or may not be directly bound to reward but can guide an RL agent towards completing tasks and collecting reward.</p>
<p>In this paper we study instruction following in text-based games and propose an approach that advances the previous state of the art.To this end, we employ the state-of-the-art model-free TBG RL agent called GATA (Graph Aided Transformer Agent) (Adhikari et al., 2020) that operates in the TextWorld environment.GATA has made significant advances in performance by augmenting TBG agents with long-term memory -a critical component of effective game play.Despite GATA's improvement over previous baselines, our experiments (see Figure 1) show that GATA performance is largely unaffected by the presence or absence of instructions, leading us to conclude that GATA is not effectively following instructions.We also find that while GATA agents are able to garner reward, 36th Conference on Neural Information Processing Systems (NeurIPS 2022).Figure 1: Comparison of GATA performance when trained with instructions (GATA D ) versus when instructions are stripped from environment observations (GATA D -S).Agents were trained with 20 or 100 games, at increasing levels of task difficulty (level 1 vs level 2).Note that normalized game point performance (solid blocks) and rate of success (hashed blocks) are largely unchanged whether instructions are present or absent.Low success rate (i.e., task completion) rate is also seen in level 2.</p>
<p>they are not typically successful in completing tasks -an important vulnerability to the deployment of such techniques in environments where partial completion of tasks can be unsafe.</p>
<p>To further study and address the task of instruction following, we equip GATA with an internal structured representation of natural language instructions specified in Linear Temporal Logic (LTL) (Pnueli, 1977), a formal language that is increasingly used for temporally extended goals in planning and reward specification and other purposes in RL (e.g., Bacchus &amp; Kabanza, 2000;Baier &amp; McIlraith, 2006;Patrizi et al., 2011;Camacho &amp; McIlraith, 2019;Littman et al., 2017;Toro Icarte et al., 2018a,b;Camacho et al., 2019;Leon et al., 2020;Kuo et al., 2020;Vaezipoor et al., 2021).LTL also provides a mechanism to monitor progress towards completion of instructions.Our framework both supports and highlights the benefit of understanding the temporal semantics of instructions and in measuring progress towards achievement of a temporally extended behaviour.We perform experiments that illustrate the superior performance of our TBG agent and its ability to follow instructions.Contributions of this work include:</p>
<p>• Experiments that expose the lack of instruction following and low task completion rate in a state-of-the-art TBG agent.</p>
<p>• An approach to the study and deployment of instruction following in TBG environments via exploitation of a formal language: LTL.LTL provides well-defined semantics and supports a measure of progress towards satisfaction of instructions.</p>
<p>• An augmentation to an existing state-of-the-art architecture for TBGs to equip a TBG agent with instruction-following capabilities.</p>
<p>• Comprehensive experiments and insights that study our and others' approaches to instruction following, and that highlight the superior performance of our proposed approach.</p>
<p>Background</p>
<p>In this section we introduce TextWorld, the TBG engine that we use, together with the Cooking domain that we employ in our experiments.We also overview Linear Temporal Logic, which (as described in section 1) we use in our approach as an internal representation for instructions.</p>
<p>Text-Based Games: TextWorld</p>
<p>Text-based games are partially observable multi-turn games where the environment and the player's action choices are represented textually.In this work, we use TextWorld (Côté et al., 2018) as our textbased game engine.A text-based game can be viewed as a (discrete-time) partially observable Markov decision process (POMDP) S, T, A, O, Ω, R, γ (Côté et al., 2018) where S is the environment's state space, A is the action space, T (s t+1 |s t , a t ) where s t+1 , s t ∈ S and a t ∈ A is the conditional transition probability between states s t+1 and s t given action a t , O is the set of (partial) observations that the agent receives, Ω(o t |s t , a t−1 ) is the set of conditional observation probabilities, R : S × A → R is the reward function, and γ ∈ [0, 1] is the discount factor.An agent's goal is to learn some optimal policy π * (a|o) (or a policy that conditions on historical observations or on some internal memory) that maximizes the expected discounted return.In this work, we focus on the choice-based variant of games, similar to previous works (Adhikari et al., 2020;Narasimhan et al., 2015).The action space A is a list of possible commands and at each time-step t in the game, the agent must select action a t ∈ C t from the current subset of permissible actions C t ⊂ A.</p>
<p>Environment Setting</p>
<p>We focus on the TextWorld Cooking domain, popularized by Adhikari et al. (2020) and Microsoft's First TextWorld Problems: A Language and Reinforcement Learning Challenge (FTWP) (Trischler et al., 2019).The game tasks agents with gathering and preparing various cooking ingredients described by an in-game recipe that is to be found.Game points (rewards) are earned for each of (1) collecting a required ingredient, (2) performing a preparatory step (some cutting or cooking action) on an ingredient as required by the recipe, (3) preparing the meal once all of the ingredients have been prepared, and (4) eating the meal.The game's partial observations can contain instructions that guide the agent towards completion of tasks, but not all instructions correspond directly to rewards.The game first instructs the agent to examine a cookbook, which elicits a recipe to be followed.The act of examining the cookbook returns no reward, but following its recipe will return reward.See Appendix C for more details.Success is determined by whether the recipe is fully completed and eaten.Preparing ingredients can also involve collecting certain tools (e.g., a knife).The game may also involve navigation -the agent may need to navigate to the kitchen or to find certain ingredients.</p>
<p>Linear Temporal Logic (LTL)</p>
<p>Linear Temporal Logic (LTL) (Pnueli, 1977) is a formal language -a propositional logical language with temporal modalities -that can be used to describe properties of trajectories.We will use LTL to specify instructions.LTL formulas are constructed from propositional variables (e.g., player-has-carrot), connectives from propositional logic (e.g.¬), and two temporal operators: (NEXT) and U (UNTIL).Formally, we define the syntax of LTL per Baier &amp; Katoen (2008) as
ϕ ::= p | ¬ϕ | ϕ ∧ ψ | ϕ | ϕ U ψ
where p ∈ P for some finite set of propositional symbols P. Satisfaction of an LTL formula is determined by a sequence of truth assignments σ = σ 0 , σ 1 , σ 2 , . . .for P, where p ∈ σ i iff proposition p ∈ P holds at time step i.Formally, σ satisfies ϕ at time i ≥ 0, denoted as σ, i |= ϕ, under the following conditions:
• σ, i |= p iff p ∈ σ i , where p ∈ P • σ, i |= (ϕ ∧ ψ) iff σ, i |= ϕ and σ, i |= ψ • σ, i |= ¬ϕ iff σ, i |= ϕ • σ, i |= ϕ U ψ iff there exists j such that i ≤ j and • σ, i |= ϕ iff σ, i + 1 |= ϕ σ, j |= ψ, and σ, k |= ϕ for all k ∈ [i, j)
A sequence σ is then said to satisfy ϕ iff σ, 0 |= ϕ.</p>
<p>Any LTL formula can be defined in terms of p ∈ P, ¬ (negation), ∧ (and), (NEXT), and U (UNTIL).</p>
<p>From these operators, we can also define the Boolean operators ∨ (or) and → (implication), and the temporal operators (ALWAYS) and ♦ (EVENTUALLY), where σ, 0 |= ϕ if ϕ always holds in σ, and σ, 0 |= ♦ϕ if ϕ holds at some point in σ.</p>
<p>LTL Progression</p>
<p>LTL formulas can also be progressed along a sequence of truth assignments (Bacchus &amp; Kabanza, 2000;Toro Icarte et al., 2018b).In other words, as an agent acts in the environment, resulting truth assignments can be used to update the formula to reflect what has been satisfied.The updated formula would now reflect the parts of the original formula that are remaining to be satisfied or whether the formula has been violated/satisfied.The progression operator prog(σ i , ϕ) is defined as follows.</p>
<p>Definition 2.1.For LTL formula ϕ, truth assignment σ i over P, and p ∈ P, prog(σ i , ϕ) is defined as
• prog(σ i , p) = true if p ∈ σ i false otherwise • prog(σ i , ϕ 1 ∧ ϕ 2 ) = prog(σ 1 , ϕ 1 ) ∧ prog(σ 1 , ϕ 2 ) • prog(σ i , ¬ϕ) = ¬ prog(σ i , ϕ) • prog(σ i , ϕ 1 UNTIL ϕ 2 ) = • prog(σ i , NEXT ϕ) = ϕ prog(σ 1 , ϕ 2 ) ∨ (prog(σ 1 , ϕ 1 ) ∧ ϕ 1 UNTIL ϕ 2 )
In the context of TextWorld, the progression operator can be applied at every step in the episode to update the LTL instruction fed to the agent.To do so, it's necessary to have event detectors that can detect when propositions are true as the agent acts during an episode (e.g., to detect that player-has-carrot is true when the player has the carrot).We discuss how event detection occurs in section 4, and give an example of how progression works in Appendix D.</p>
<p>3 Following Instructions with GATA</p>
<p>In order to evaluate the effectiveness of state-of-the-art text-based game agents at following instructions, we conducted experiments on the Cooking domain using the state-of-the-art model-free RL agent for TextWorld, GATA (Adhikari et al., 2020).GATA uses a transformer variant of the popular LSTM-DQN (Narasimhan et al., 2015) combined with a dynamic belief graph that is updated during game-play.The aim is to use this belief graph as long-term memory to improve action selection by modelling the underlying game dynamics (Adhikari et al., 2020).Formally, given the POMDP, GATA attempts to learn some optimal policy π * (a|o, g) where g is the belief graph.</p>
<p>While GATA's belief graph can capture goal relations (e.g.apple-needs-cut), it turns out that agents trained to condition on observations and the GATA belief graph alone largely ignore in-game instructions.We tested a GATA agent on levels 1 and 2 in the Cooking domain, after training on either the 20-game or 100-game training set, and found that in none of those settings was the cookbook examined more than 15% of the time (3/20 testing games).In short, the GATA agent usually doesn't observe what the recipe is for the current game, meaning it has no way of knowing what the actual goal of the game is (except -eventually -from the rewards it gets and when the episode ends).</p>
<p>We further investigate how GATA agents fail to follow instructions by training these agents using modified game observations that have their instructions stripped (specifically, instructions directing the agent to examine the cookbook, the recipe text within the cookbook, and instructions to grab a knife if attempting to cut an ingredient without first holding the knife were removed from observations).This has two effects: (1) the agent no longer receives text-based instructions about what the goal is or what it should do; and (2) GATA's belief state will no longer capture goal relations like 'needs'.The results of this experiment are in Figure 1, and demonstrate how GATA's performance remains largely unchanged.This suggests that GATA is (here at least) (a) not exploiting text-based instructions that would lead it to success and (b) even not exploiting the goal-related relations in its own belief state.</p>
<p>The results in Figure 1 also show a drop in GATA's performance when moving from level 1 to level 2 in the Cooking domain, where the games' complexity is increased by just one added ingredient preparation step in the recipes (see Table 1 for more details on the levels).GATA has difficulty in fully completing tasks on level 2 games, where its success rate is roughly half that of its achieved normalized game points (only the latter metric was used by Adhikari et al. (2020)).</p>
<p>Given these insights, we wish to further study and address instruction following in TBGs.In the next section, we propose using LTL and demonstrate how existing work can be easily augmented.</p>
<p>An Approach to Following Instructions</p>
<p>We now investigate a mechanism for both studying and advancing the ability of an RL agent to follow instructions.We do so by translating instructions to an internal structured representation of language in the form of LTL, a formal language that is increasingly being used for reward specification in RL agents (Vaezipoor et al., 2021;Leon et al., 2020;Kuo et al., 2020;Camacho et al., 2019;Toro Icarte et al., 2018b).We describe how to augment the GATA architecture with these LTL instructions and how to monitor progress towards their completion.</p>
<p>Generating and Representing LTL Instructions for TextWorld</p>
<p>We use three types of instructions for the Cooking domain.The first instruction identifies the need to examine the cookbook: This instruction is defined as ϕ : NEXT cookbook-is-examined.This instruction simply states that the agent should examine the cookbook (i.e.cookbook-is-examined = true) in the next step of the game.The second instruction is the actual recipe that gets elicited from the cookbook.We format this instruction to be order-invariant and incomplete.Order-invariance allows the agent to complete the instructions in any order, but is still constrained by any ordering that the TextWorld engine may enforce."Incomplete" simply refers to the fact that not every single action required to complete the recipe is encoded (i.e.grabbing a knife before slicing a carrot, opening the fridge).The agent must still learn to do these things to accomplish its tasks, but is not directly instructed to.Assuming the recipe requires that predicates p 1 , p 2 , . . .p n be true, the cookbook instructions are modelled as ϕ : (EVENTUALLY p 1 ) ∧ (EVENTUALLY p 2 ) ∧ . . .(EVENTUALLY p n ).</p>
<p>For example, in the Cooking Domain, this instruction might be the conjunction
ϕ : (EVENTUALLY apple-in-player) ∧ (EVENTUALLY meal-in-player) ∧ (EVENTUALLY meal-is-consumed).
The third and final type of instruction identifies the need to navigate to the kitchen.This instruction is defined as ϕ : EVENTUALLY player-at-kitchen.This instruction will come prior to the first two described above, but is only used in games with navigation (see Table 1).</p>
<p>We build a simple LTL translator that generates these instructions from the textual observations, similar to the goal generator used in Liu et al. (2022).TextWorld's observations are easily parsed to extract the goal information already contained within them, which we then formalize and keep track of using LTL.We provide examples of these observations and more details in Appendix E. Note that these observations are only used to generate the instruction itself, and subsequently LTL progression is used with the GATA belief state as our event detector to monitor completion of instruction steps and to update instructions that remain to be addressed.</p>
<p>One possible criticism with such an LTL translator is its reliance on domain knowledge.While not the main focus of this paper, a complementary research problem that has begun to be explored is to automatically translate natural language instructions to LTL (e.g., Dzifcak et al., 2009;Finucane et al., 2010;Wang et al., 2020).Traditionally, such approaches have required large corpora of training data or hard-coded rules, and were restricted to a specific domain.However, pretrained large language models such as GPT-3 introduce the potential for a general natural-language-to-LTL translation scheme with minimal domain-specific adaptation (Hahn et al., 2022;Huang et al., 2022;Brohan et al., 2022).We explore this prospect by applying GPT-3 to TextWorld in subsection 5.5.</p>
<p>Finally, we note that in this work, GATA provides the domain-dependent vocabulary for describing properties of state (e.g.carrot-is-chopped) while our LTL augmentation provides the domainindependent temporal modalities (i.e., NEXT, EVENTUALLY, etc.) and the logical connectives for composing those properties of state into the instructions we use.In this way, our technique is very generalizable, limited only by the recognizable properties of state (which in our case are provided by GATA) and instructions that can be extracted in game-play.</p>
<p>LTL Augmented Rewards and Episode Termination</p>
<p>We can also reward our agent for completing instructions, which we model as reward R LTL (s, a, ϕ).</p>
<p>For some labelling function L : S × A → 2 P that assigns truth values to the propositions in P,
R LTL (s, a, ϕ) = R(s, a) +    1 if prog(L(s, a), ϕ) = true −1 if prog(L(s, a), ϕ) = false 0 otherwise
In other words, a bonus reward is given for every LTL instruction the agent satisfies and a penalty is given if the agent fails to complete an instruction.We perform an ablative study on the effect of this reward in subsubsection H.3.2.We henceforth refer to this modified reward function as the LTL reward.The maximum bonus reward an agent receives is either 2 if there is no navigation task, or 3.</p>
<p>Further, because we wish to satisfy instructions, we can also use the instructions to modify episode termination.That is, if our LTL instruction is violated, we have arrived in a terminal state, even if TextWorld has not indicated so.We perform an ablative study on the effect of this LTL-based termination in subsubsection H.3.2.</p>
<p>LTL-GATA Model Architecture</p>
<p>We build a similar model to GATA's original architecture, augmented to include the LTL encoding of instructions and their progression according to observed system state.We dub this model LTL-GATA, which we describe in detail below.In turn, the agent's graph updater (GATA) updates its belief graph g t in response to both o t and g t−1 .Next, g t and o t update the LTL instructions.ϕ t is generated from o t after the cookbook is examined and thereafter ϕ t−1 is progressed to ϕ t at each time step.The policy network selects action a t from C t conditioned on o t , ϕ t , and g t and the cycle repeats.</p>
<p>Graph Updater: We use the original GATA-GTP model (Adhikari et al., 2020), which generates a discrete belief graph as a list of triplets of the form (object, relationship, object) .It is composed of two sub-components: (a) the belief state updater, which generates g t from observation o t and the graph g t−1 ; and (b) the graph encoder, which encodes the current graph into a vector as GE(g t ) = g t ∈ R D for some latent dimension D. The graph encoder is a relational graph convolutional network (R-GCN) (Schlichtkrull et al., 2018) using basis regularization (Schlichtkrull et al., 2018) and highway connections (Srivastava et al., 2015).We refer the readers to Adhikari et al. ( 2020) for more details.</p>
<p>LTL Updater: The LTL updater generates and progresses LTL instructions.LTL instructions defining the need to arrive at the kitchen and examine the cookbook are generated from the initial observation o 0 .The subsequent instruction defining the recipe is generated from game observation o t , as described in subsection 4.1, when the action examine cookbook is executed at time t.For the truth assignments (i.e. the labelling function L), we leverage GATA's highly accurate belief state from the graph updater.We use the Spot engine (Duret-Lutz et al., 2016) to perform the progression.</p>
<p>Text Encoders: For encoding the action choices C t , observations o t , as well as encoding the LTL instructions ϕ t , we use a simplified version of the Transformer architecture presented by Vaswani et al. (2017).This is the same architecture used by Adhikari et al. (2020).For LTL instructions, we encode them directly as a string.For example, the LTL formula ϕ : (EVENTUALLY p 1 ) ∧ (EVENTUALLY p 2 ) where p 1 = pepper-in-player and p 2 = pepper-is-cut, has the string representation str(ϕ) : "eventually player_has_pepper and eventually pepper_is_cut"</p>
<p>We format each predicate as a single token, and we show in subsubsection H.3.1 that our method is robust to predicate format.For some input string v ∈ R of length , the text encoder outputs a single vector TE(v) = v ∈ R D of dimension D, which is the same latent dimension as the graph encoder.</p>
<p>Action Selector: The action selector is a 2-layer multi-layer perceptron (MLP).The encoded state vectors
TE(o t ) = o t ∈ R D , TE(ϕ t ) = ϕ t ∈ R D , and GE(g t ) = g t ∈ R D are concatenated to form the agent's final state representation z t = [o t ; ϕ t ; g t ] ∈ R 3D .
In contrast to Adhikari et al. (2020), we concatenate features rather than use the bi-directional attention-based aggregator.This simplified the model's complexity and worked just as well experimentally.This vector is then repeated n c times and concatenated with the encoded actino choices C t ∈ R nc×D where n c is the number of action choices.This input matrix is fed to the MLP which returns the a vector of Q-values for each action q c ∈ R nc .</p>
<p>Training.Formally, for belief state g and LTL instruction ϕ, LTL-GATA aims to learn an optimal policy π * (a|o, g, ϕ).To learn this optimal policy, we implement Double DQN (DDQN) (Van Hasselt et al., 2016) with reward function and termination criteria as discussed in subsection 4.2.We use a prioritized experience replay buffer (Schaul et al., 2016).Refer to subsection G.2 for further details.</p>
<p>Experiments</p>
<p>Our experimental assessment was designed both to understand how well GATA was exploiting observational instructions, as discussed in section 3, and to assess the instruction-following performance
0 1 1 3 {, , } 1 1 1 4 {, , } 2 1 1 5 {, , } 3 1 9 3 {, , }
of our proposed approach relative to this state of the art (not only in terms of game points but also successful completion).We additionally strove to assess features of our approach (such as monitoring instruction progress) that contributed to its performance, as well as general challenges to text-based game playing that limited its performance (such as navigation).1</p>
<p>Experimental Setup</p>
<p>Games.To have as fair a comparison with Adhikari et al. (2020) as possible, we reused the sets of games they had generated.For the training games, they had created two sets: one set that contains 20 unique games per level and another that contains 100 unique games per level.Both the validation and testing sets have 20 unique games each per level.The levels we chose to use in our assessment are shown in Table 1.Note that in our assessment we omit Levels 4 and 5. Level 4 is an augmentation to Level 3 that adds more ingredients; both GATA and LTL-GATA at this level suffer from the navigation issues we discuss later with respect to Level 3. As we wanted to focus on instruction following and not navigation, we omitted this level and chose to use Level 0 instead.Level 5 is simply a random combination of all levels, so it is omitted for similar reasons.</p>
<p>Hyper-parameters.We replicate all but three hyper-parameters from Adhikari et al. ( 2020 Baselines.We compare against (1) TDQN (Adhikari et al., 2020), the transformer variant of the LSTM-DQN (Narasimhan et al., 2015) model, (2) GATA C , and (3) GATA D .GATA C is GATA's best performing model (GATA-COC) that uses a continue graph-updater pre-trained using contrastive observation classification.GATA D is a similarly performant model (GATA-GTP) that uses a discrete graph-updater pre-trained with ground-truth graphs from the FTWP dataset.Finally, we note that we found a few issues with GATA's original code2 and have since fixed them (see subsubsection H.4.1).</p>
<p>For comparison, we include the original paper GATA models, labelled as GATA C P and GATA D P .Measuring performance.We measure performance using two metrics: normalized accumulated game points and game success rate.We report averaged results over 3 seeds for each experiment.Previous works only compared using the normalized accumulated game points; however, this may sometimes be misleading -an agent could get 3/4 = 0.75 points on all games but never actually succeed on any.In contrast, measuring the success rate alongside the normalized game points allows for a more complete analysis of the agent's ability to play and complete these games.</p>
<p>LTL-GATA Compared to Baselines</p>
<p>Consistently high performance with 20 training games.We see from Figure 4 that LTL-GATA exhibits consistently high performance across levels as compared to the baselines when trained on the 20 games set.In particular, LTL-GATA maintains its performance on level 2, where the game's slight increase in complexity causes large performance drop-offs in other methods.Our agent can easily complete the added task and maintain similar performance to the previous level 1.Large performance gains with 100 training games.We see from Figure 4 that LTL-GATA gains considerable performance when trained on 100 games.With the added games, our agent is exposed to more predicates and can now generalize better to the testing set.Future work may look at how to achieve this kind of generalization without having to expose our agent to more predicates.
T D Q N G A T A D P G A T A C P G A T A C G A T A D LT L -G A T A 0.0 0.2 0.4 0.6 0.8 1.0 Test Normalized Game Points Level 0 G A T A D P G A T A C P G A T A C G A T A D T D Q N LT L -G A T A 0.0 0.2 0.4 0.6 0.8 1.0 Level 1 G A T A D P G A T A C P G A T A D G A T A C T D Q N LT L -G A T A 0.0 0.2 0.4 0.6 0.8 1.0 Level 2 G A T A D P G A T A C P G A T A D G A T A C T D Q N LT L -G A T A 0.0 0.T D Q N G A T A D P G A T A C P G A T A C G A T A D LT L -G A T A 0.0 0.2 0.4 0.6 0.8 1.0 Test Normalized Game Points Level 0 G A T A D P G A T A C P G A T A D G A T A C T D Q N LT L -G A T A 0.0 0.2 0.4 0.6 0.8 1.0 Level 1 G A T A D P G A T A C P G A T A D G A T A C T D Q N LT L -G A T A 0.0 0.2 0.4 0.6 0.8 1.0 Level 2 G A T A D P G A T A C P G A T A D G A T A C T D Q N LT L -G A
Success rate and normalized game points.Looking at the performance of GATA on level 2, it becomes apparent why measuring the success is important.Although it achieves almost 0.4 normalized points, the actual success rate is near 0 for original GATA models, and ∼ 60% of the normalized points for the fixed models average across both training sets.In contrast, LTL-GATA exhibits high normalized points and success rate, where the average success rate across both training sets is ∼ 82% of the normalized points.</p>
<p>Competitive performance on level 3. Level 3 introduces the added challenge of navigation.LTL-GATA outperforms GATA in this level as well, but not to the degree of previous levels.Inspecting testing trajectories, it becomes evident that both LTL-GATA and GATA methods struggle with navigation in this level, and have difficulties even navigating to the kitchen in the first place.Exploring at test time to find items and rooms in an unknown environment is a major challenge built into textbased games.Hypothetically, LTL could contribute to addressing this challenge.LTL could be used to dictate strategy and/or to simply track such exploration (e.g., for remembering which rooms have been previously visited).LTL might also be used to encode learned navigation instructions (e.g."find the blue door, go through it, then turn right").We do not pursue this vector of research here, but it is an interesting direction for future work.</p>
<p>Does LTL Progression Matter?</p>
<p>We show in Figure 5b that the use of progression is critical to performance, where LTL-GATA without progression incurs a large performance drop-off, dropping below the performance of the baselines as well.Without progression, the LTL instruction will not reflect the changes incurred by the agent's actions.This appears to confuse the agent considerably, demonstrated by its performance drop-off.</p>
<p>Forcing GATA to Examine the Cookbook</p>
<p>Because LTL-GATA is always tasked with examining the cookbook, we question whether a similar tasking for GATA improves performance.We experiment with GATA D by forcing the agent to examine the cookbook on the first step of the episode.Forcing GATA to examine the cookbook will elicit goal relations like (apple,needs,cut) in the belief state.We show however in Figure 5a that GATA does not improve when being given the cookbook.This shows that GATA cannot make use of the information elicited from the cookbook, continuing to ignore important instructions.Even with the presence of goal relations in its belief state, GATA fails to properly attend to this information.This highlights the benefits of a formalized representation of instructions used by LTL-GATA.</p>
<p>On Automatic Translation: Natural Language Instructions to LTL</p>
<p>While LTL-GATA relies on a handcrafted LTL translator to provide initial instructions from text observations, we investigate the potential of automating this step using pretrained large language models.This is not a central focus of the paper.Rather, we include this exploration as a proof of concept that the use of LTL is not a barrier to broad deployment of the work presented here.To this end, we evaluate whether GPT-3 (Brown et al., 2020) can few-shot learn to translate TextWorld observations to LTL, given only six examples and without additional training.</p>
<p>We experiment with two models of GPT-3 from OpenAI: Ada (the fastest model) and Da Vinci (the most powerful model).We perform few-shot translation by constructing prompts that contain six example translations, followed by the natural language observation to translate (the test case).</p>
<p>The examples remain fixed for all test cases, and follow the form "NL: <natural language observation>.LTL: <ltl-formulas>".Our test case follows the form "NL: <natural language observation>.LTL:", where the model must complete the prompt, thereby performing a translation.We consider a response that exactly matches the ground-truth LTL formula as absolutely correct, a response that is otherwise correct except for parentheses and spaces as almost correct, and all other responses as incorrect.2022) took a model-based approach, focusing on object-oriented dynamics.However, these works do not address the role and representation of instructions that defines our work.Kimura et al. (2021) does employ a neuro-symbolic RL method using Logical Neural Networks.However, it does not focus on instructions, operates over all logical facts of the environment, and is applied to a simpler domain.</p>
<p>Instruction following and Linear Temporal Logic.Vaezipoor et al. (2021) trained an RL agent to follow various LTL instructions in both discrete and continuous action-space visual environments.They used R-GCNs to learn representations of the LTL instructions and also employed LTL progression.Their model showed good generalization performance on similar and much larger unseen instructions than those observed during training .However, in contrast to the work presented here, they relied on ground-truth event detectors and operated in fully observable settings, while we use GATA's learned belief graphs, in a partially observable setting, to evaluate the truth or falsity of propositions and to progress formulae.We further distinguish ourselves from this work by opting for training the LTL semantics end-to-end using a transformer rather than an R-GCN.Works using LTL for reward specification (Leon et al., 2020;Kuo et al., 2020;Camacho et al., 2019;Toro Icarte et al., 2018b;Littman et al., 2017) or advice (Toro Icarte et al., 2018a) in RL agents exist, however they do not focus on text-based environments nor partially observable ones.</p>
<p>Conclusion</p>
<p>We studied the ability of RL agents to follow instructions in text-based games using TextWorld.We conducted experiments to show how current state-of-the-art model-free agents largely fail to exploit instructions and do not typically complete prescribed tasks.We then showed how LTL can be used to construct internal structured representations for state augmentation that result in large performance improvements and more reliable instruction following and task completion.Experiments showed that monitoring instruction progress was critical to these gains.Our method inherits limitations in dealing with navigation and unseen games from prior work, but these concerns are somewhat orthogonal to our focus on instruction following.</p>
<p>Furthermore, we can consider the broader impact of this work by relating to the critical need for good instruction following in safety-oriented domains such as autonomous transport or health care.We would like to suggest that works towards building better language agents should also emphasize the importance of completing instructions.To illustrate, for an agent to help a person half-way across a street, or to start but not finish a medical operation, may be worse than for it to do nothing at all.To that end, we have proposed using (game) success rate as a metric for future work, and demonstrated how LTL-GATA is very successful in the games it plays, relative to the state-of-the-art.Overall, we intend this paper to highlight the importance of studying instruction following in environments like TextWorld that act as a proxies to the general class of problems dealing with language understanding and human-machine interaction.</p>
<p>Finally, in follow-on work we would like to explore more complex text-based games such as the Jericho environment (Hausknecht et al., 2020).These games involve a number of distinct challenges, including exploration, navigation, puzzle solving, language understanding, and instruction following.</p>
<p>In this vein, we'd like to see whether LTL can be exploited to capture (learned) domain-specific strategic advice, or memory to tackle both navigation and exploration challenges.We'd like to further explore seamless ways to exploit the merits of natural language together with the benefits afforded by the compositional syntax and semantics of formal languages such as LTL.To this end, further advancing our explorations translating natural language to LTL is of interest and import, for this and a diversity of other applications in and outside RL.</p>
<p>Appendices</p>
<p>A Reinforcement Learning</p>
<p>Reinforcement Learning (RL) is the problem of training machine learning models to solve sequential decision making problems.By interacting with an environment, RL agents must learn optimal behaviours given the current state of their environment.If the environment is fully observable, we can frame it as a Markov Decision Process (MDP) modelled as S, A, T, R, γ where S is the environment's state space, A is the action space, T (s t+1 |s t , a t ) where s t+1 , s t ∈ S and a t ∈ A is the conditional transition probability between states s t+1 and s t given action a t , r t = R(s, a) : S × A → R is the reward function for state action pair (s, a), and γ ∈ [0, 1] is the discount factor.The goal for an RL agent is to learn some optimal policy π * (a|s) that maximizes the expected discounted return E π ∞ k=0 γ k r t+k S t = s .A single game is an episode, and steps in an episode are indexed by t.</p>
<p>B Partially Observed Reinforcement Learning</p>
<p>In a partially observed environment, an agent does not have access to the full state space S. We can frame this environment as a Partially Observable MDP (POMDP) modelled by S, A, T, O, Ω, R, γ .</p>
<p>In this new setting, S, A, T, R, γ remain unchanged, O represents the set of (partial) observations that the agent receives and Ω(o t |s t , a t−1 ) is the set of conditional observation probabilities.An agent's goal is to learn some optimal policy π * (a|o) (or a policy that conditions on historical observations or on some internal memory) that maximizes the expected discounted return.</p>
<p>C TextWorld: Cooking Domain</p>
<p>We present two examples of observations with instructions in Table 2 and highlight where the instructions are and where the rewards come from.</p>
<p>Reward</p>
<p>There is a reward of 1 given for eating the meal.i.e. the instruction "Once done, enjoy your meal!" will result in a reward of 1 after the recipe has been completed.Note that the instruction "Check the cookbook in the kitchen for the recipe." is not bound to a reward.</p>
<p>Observation following the examine cookbook action</p>
<p>"You open the copy of "Cooking : a modern approach (3rd ed.)" and start reading: recipe #1 -----Gather all following ingredients and follow the directions to prepare this tasty meal.Ingredients: red potato: directions: chop the red potato, fry the red potato, prepare meal"</p>
<p>Reward</p>
<p>There are 4 rewards from the instruction "Gather all following ingredients and follow the directions to prepare this tasty meal.Ingredients: red potato: directions: chop the red potato, fry the red potato, prepare meal":</p>
<p>• 1 for grabbing the red potato • 1 for chopping the red potato • 1 for frying the red potato • 1 for preparing the meal</p>
<p>D An example of LTL progression</p>
<p>To illustrate how progression works, the LTL instruction (EVENTUALLY player-has-carrot) ∧ (EVENTUALLY player-has-apple) would be progressed to (EVENTUALLY player-has-apple) once the agent grabs the carrot during an episode.In other words, when the agent reaches a state where player-has-carrot is true, the LTL instruction is progressed to reflect that the agent no longer needs to get the carrot but must still grab the apple at some point.</p>
<p>E Generating LTL in TextWorld</p>
<p>We provide some examples of the LTL instructions used in this work in Table 3, Table 4, and Table 5.We build a simple translator that reads game observations and constructs these LTL instructions directly, but only once.Repeated observations will not result in the same LTL formula being generated.Once a formula has been generated, LTL progression is used with the agent's belief state to progress the instructions along the truth assignments: observations are not directly used in the progression, although they do indirectly affect the progression by affecting the belief state.</p>
<p>For levels 0, 1, and 2, the LTL instructions that an agent can receive throughout an episode are (a) the task to examine the cookbook and (b) the recipe-bound task.In other words, the set of un-progressed instructions Φ it can receive over the course of an episode (assuming the cookbook is examined) is as follows:
Φ : [ NEXT cookbook-is-examined, (EVENTUALLY p 1 ) ∧ (EVENTUALLY p 2 ) ∧ . . . (EVENTUALLY p n )]
where the recipe requires that predicates p 1 , p 2 , . . .p n be true.Note that we also consider eating the meal to be a part of recipe in this case, although it is not explicitly mentioned in the recipe.Further, we note that the "prepare meal" task is represented by the predicate meal-in-player, as this is the event that occurs when the meal is prepared in the game.</p>
<p>For levels with navigation (i.e. level 3),
Φ : [ EVENTUALLY player-at-kitchen, NEXT cookbook-is-examined, (EVENTUALLY p 1 ) ∧ (EVENTUALLY p 2 ) ∧ . . . (EVENTUALLY p n )]
where the agent has the added task of first navigating to the kitchen.This instruction provides no help for actually how to arrive at the kitchen, only that the agent must do so.As a result, LTL-GATA still suffers from the difficulties of exploration, and perhaps investigating how LTL can be used to improve in navigation could be a direction for future work.</p>
<p>In total, LTL generation occurs only twice for any level, either during the initial observation or when the cookbook is read.When multiple instructions are generated at once, the agent will process them sequentially, in the order they are given.</p>
<p>Table 3: Level 3 observation and resulting generated LTL instruction Observation "You are hungry!Let's cook a delicious meal.Check the cookbook in the kitchen for the recipe.Once done, enjoy your meal!" -= corridor =-"You've entered a corridor.There is a closed screen door leading west.You don't like doors?Why not try going north, that entranceway is not blocked by one.You need an exit without a door?You should try going south."Generated LTL This observation will generate two instructions: First, ϕ : (EVENTUALLY player-at-kitchen) and second, ϕ : (NEXT cookbook-is-examined) "You open the copy of "Cooking : a modern approach (3rd ed.)" and start reading: recipe #1 -----Gather all following ingredients and follow the directions to prepare this tasty meal.Ingredients: red potato: directions: chop the red potato, prepare meal"
Generated LTL ϕ :(EVENTUALLY red-potato-in-player) ∧ (EVENTUALLY red-potato-is-chopped)∧ (EVENTUALLY meal-in-player) ∧ (EVENTUALLY meal-is-consumed).
Table 5: Level 2 observation and resulting generated LTL instruction Observation "You open the copy of "Cooking : a modern approach (3rd ed.)" and start reading: recipe #1 -----Gather all following ingredients and follow the directions to prepare this tasty meal.Ingredients: red potato: directions: chop the red potato, fry the red potato, prepare meal"
Generated LTL ϕ :(EVENTUALLY red-potato-in-player) ∧ (EVENTUALLY red-potato-is-chopped)∧ (EVENTUALLY red-potato-is-fried) ∧ (EVENTUALLY meal-in-player)∧ (EVENTUALLY meal-is-consumed).</p>
<p>F Model F.1 Text Encoder</p>
<p>The text encoder is a simple transformer-based model, with a transformer block (Vaswani et al., 2017) and word embedding layer.We use the pre-trained 300-dimensional fastText (Mikolov et al., 2017) word embeddings, which are trained on Common Crawl (600B tokens).These word embeddings are frozen during training.Strings are tokenized by spaces.</p>
<p>The transformer block is composed of: (1) a stack of 5 convolutional layers, (2) a single-head self-attention layer, and (3) a 2-layer MLP with ReLU non-linear activation function in between.</p>
<p>The convolutional layers each have 64 filters, with kernel sizes of 5 and are each followed by a Layer Norm (Ba et al., 2016).We also use standard positional encoding (Vaswani et al., 2017).The self-attention layer uses a hidden size H of 64.The Text Encoder outputs a single feature vector v ∈ R D , where D = 64 in our experiments.</p>
<p>F.2 Encoder Independence</p>
<p>Figure 3 in the main paper visualizes each component of our model.Specifically, our model has four encoders: (1) Graph Encoder, (2) Text Encoder for observations, (3) Text Encoder for LTL instructions, and (4) Text Encoder for action choices.We note here that each of these encoders are independent models, trained concurrently.This is in contrast to the original GATA model that used the same Text Encoder for both the actions and the observations.Because these Text Encoders are relatively small transformers, there is no issues with fitting this model in memory.As shown in Table 6, the model is still quite efficient, even more than the original GATA code.We found that using independent encoders resulted in better performance than using a single Text Encoder that would have been responsible for encoding the observations, LTL instructions, and action choices.</p>
<p>F.3 Action Selector</p>
<p>The action selector is a simple two-layer MLP with a ReLU non-linear activation function in between.It takes as input, at time step t, the concatenated representation of the agent's state vector z t ∈ R 3D and the action choices C t ∈ R nc×D .Recall that in our experiments D = 64.The first layer uses an input dimension of 4D and an output dimension of D. The second layer has an input dimension of D and output dimension of 1, which after squeezing the last dimension during the forward pass, the final output vector q c ∈ R nc represents the q-values for each action choice.</p>
<p>The input to the action selector is constructed by repeating the agent's state representation, z t , n c times and then concatenating with the encoded actions choices C t .We wanted to further explain why this occurs, as it may not be immediately clear.The action selector in this work is a parameter-tied Q-value predictor.That is, for some action a i ∈ C t , i ∈ [1, . . ., n c ] and agent state representation z t , the predicted Q-value is q i = AS([a i , z t ]).Thus, the action selector (i.e.AS(•)) predicts Q-values given action a i and agent state representation z t .Thus, during a single episode step, given our encoded actions choices C t ∈ R nc×D , in order for the action selector to predict Q-values for each of these action choices, we repeat z t ∈ R 3D n c times and stack it together, which results in a state matrix Z t ∈ R nc×3D .When we concatenate this matrix with our action choices we are left with the input to our action selector: [C t ; Z t ] ∈ R nc×4D .Looking at this matrix, each row in this input matrix is effectively the concatenation of action a i with agent state representation z t , and so passing this matrix to our action selector performs the parameter-tied Q-value prediction q i = AS([a i , z t ]) for all action choices, and outputs a single vector of Q-values for each action q c ∈ R nc .We can then use these predicted Q-values to perform action selection using either a greedy approach, an -greedy approach, Boltzmann action selection, etc.</p>
<p>G Implementation Details</p>
<p>G.1 Augmenting GATA's Pre-Training Dataset</p>
<p>We note here that although possible, the vocabulary and dataset used by Adhikari et al. (2020) did not allow for the knowledge triple {cookbook, is, examined} to be extracted from observations.Without this triple being extracted and added to the agent's belief state, there would be no way for the agent to progress LTL instructions requiring the agent to examine the cookbook.In our pre-training of the GATA graph encoder, we augmented the dataset provided by Adhikari et al. (2020) to include the triplet {cookbook, is, examined} when relevant (i.e. when the agent examines the cookbook).This was a simple process of adding this triple to the ground truth belief graphs in the dataset so that during pre-training, GATA could learn how to translate these triplets from relevant observations.</p>
<p>G.2 Training</p>
<p>For training to learn our optimal policy we use the Double-DQN (DDQN) (Van Hasselt et al., 2016) framework.We use -greedy for training, which first starts with a warm-up period, using a completely random policy (i.e.= 1.0) for the first 1, 000 episodes.We then anneal from 1.0 to 0.1 over the next 3, 000 episodes after the initial warm-up (i.e.episodes 1, 000 to 4, 000).We use a prioritized experience replay buffer (α = 0.6 and β = 0.4) with capacity 500, 000.For DDQN, the target network updates occur every 500 episodes.We update network parameters every 50 game steps, and we play 50 games in parallel.</p>
<p>We train all agents for 100, 000 episodes using a discount factor of 0.9, and we use {123, 321, 666} as our random seeds.Each episode during training is limited to a maximum of 50 steps, and during testing/validation this limit is increased to 100 steps.We report results and save checkpoints every 1, 000 episodes.We also use a patience window p that reloads from the previous best checkpoint during training when validation performance has decreased for p episodes in a row.This is the same strategy used in Adhikari et al. (2020).For our experiments, we used p = 3.</p>
<p>For reporting testing results, each model is trained using the three seeds mentioned before, and fine-tuned on the validation set.That is, the checkpoint of the model that performs best on the validation set during training is saved, and each of these models (three, one for each seed) is applied to the test set.Reported test results are the average over these three models.</p>
<p>H Experiments</p>
<p>H.1 Hyper-Parameters</p>
<p>To have as fair a comparison as possible, we replicate all but three hyper-parameters from the settings used in Adhikari et al. (2020).We do this to remove any bias towards more finely tuned experimental configurations and focus only on the LTL integration.Further, we re-run the GATA experiments to confirm their original results.The three changes we implemented were (1) we use a batch size of 200 instead of 64 when training on the 100 game set, (2) for level 3, we use Boltzmann Action selection, and (3) we use Adam (Kingma &amp; Ba, 2015) with a learning rate of 0.0003 instead of RAdam (Liu et al., 2020) with a learning rate of 0.001.These changes boosted performance for all models.For the 20 training game set, we use a batch size of 64.</p>
<p>For Boltzmann action selection, we used a temperature of τ = 100.We experimented with various temperatures (τ ∈ {1, 10, 25, 50, 100, 200}) and found τ = 100 to perform the best across models.</p>
<p>H.2 Computational Requirements</p>
<p>We report the wall-clock times for our experiments in Table 6.</p>
<p>H.3.1 Ablation: Formatting LTL Predicates</p>
<p>As we saw from Figure 4, LTL-GATA when trained on the 100 games set performs significantly better than when trained on the 20 game set, which we attribute to the increased exposure to predicates during training, allowing it to generalize better during testing.To see if we can achieve the same level of generalization when training on the 20 game set, we compare LTL-GATA with LTL predicates represented as single tokens (what we did in the main paper) with using multiple tokens.That is, we compare the following two string representations: (single-token predicates) str(ϕ) : "eventually player_has_pepper and eventually pepper_is_cut" (multi-token predicates) str(ϕ) : "eventually player has pepper and eventually pepper is cut" We visualize the ablative study of these three scenarios in Figure 7.</p>
<p>From Figure 7 we can conclude that the presence of either the new reward function R LTL (s, a, ϕ) or LTL-based episode termination is important to the performance of LTL-GATA.This is because either of these methods will incentivize the agent to complete the initial NEXT cookbook-is-examined instruction, which isn't intrinsically rewarded by TextWorld.We can demonstrate the importance of this incentive by analyzing just one level (level 2 on 20 training games).Removing both methods leads to the agent not examining the cookbook, preventing it from receiving further instructions, which we can see from Figure 7(c) results in considerable performance loss, regressing to the baseline GATA.</p>
<p>H.4 Code</p>
<p>All code for this work can be found at https://github.com/MathieuTuli/LTL-GATA.</p>
<p>H.4.1 Fixing the GATA code</p>
<p>We found two primary issues in the GATA code.First, we noticed that their implementation of the double Q-learning error was wrong.For Double Q-Learning, after performing some action a t in state s t and observing the immediate reward r t and resulting state s t+1 , the Q-Learning error is defined per Van Hasselt et al. (2016) as
Y t = r t + γQ(s t+1 , arg max a Q(s t+1 , a; θ t ); θ t )(1)
where θ t and θ t are the parameters of the policy network and the target network, respectively.However, we noticed that the original code for GATA was computing the error as3 Y t = r t +r t+1 + γQ(s t+1 , arg max a Q(s t+1 , a; θ t ); θ t )</p>
<p>In other words, the reward for the stepped state was also being added to the error.</p>
<p>Second, we found that the double Q-learning error for terminal states was being incorrectly implemented.Specifically, when computing the error for the case where s t is a terminal state, and therefore the stepped state s t+1 does not exist, the stepped state was not being masked4 .Additionally, presumably because of this initial error, terminal states were very rarely returned when sampling from experience, unless certain criteria were met5 .</p>
<p>We found fixing these issues improved GATA's performance considerably, which we demonstrated in Figure 4, and all our experimental results for GATA have this correction implemented.</p>
<p>H.5 Training Curves</p>
<p>Here we present accompanying training curves for experiments reported in this work.We report averaged curves of the normalized accumulated reward with bands representing the standard deviation.</p>
<p>H.6 Automated Translation: Natural Language Instructions to LTL Details</p>
<p>For the GPT-3 experiments on automated LTL translation in subsection 5.5, we simply extracted the observations used by our simple translator and saved the observation-translation pairs.Details of that translator were described in in Appendix E and examples of these pairs can be found in Table 3 and Table 4.These observations are the sort of natural language we wish to translate into LTL, and we used six of the observation-translation pairs as the examples in our prompt to GPT-3.</p>
<p>The full prompt to GPT-3 is shown below (with colors added for readability).The six examples consist of a natural language observation (in turquoise) and a corresponding LTL formula (in red)these remain fixed for all prompts.The seventh line begins with the natural language observation to be translated (in blue).</p>
<ol>
<li>
<p>NL: you open the copy of "cooking : a modern approach ( 3rd ed .)" and start reading : recipe # 1 -----gather all following ingredients and follow the directions to prepare this tasty meal .ingredients : cilantro directions : dice the cilantro prepare meal LTL: ('and', ('eventually', 'cilantro_in_player'), ('and', ('eventually', 'cilantro_is_diced'), ('eventually', 'meal_in_player')))</p>
</li>
<li>
<p>NL: you open the copy of "cooking : a modern approach ( 3rd ed .)" and start reading : recipe # 1 -----gather all following ingredients and follow the directions to prepare this tasty meal .ingredients : pork chop directions : chop the pork chop fry the pork chop prepare meal LTL: ('and', ('eventually', 'pork_chop_in_player'), ('and', ('eventually', 'pork_chop_is_chopped'), ('and', ('eventually', 'pork_chop_is_fried'), ('eventually', 'meal_in_player'))))</p>
</li>
<li>
<p>NL: you open the copy of "cooking : a modern approach ( 3rd ed .)" and start reading : recipe # 1 -----gather all following ingredients and follow the directions to prepare this tasty meal .ingredients : black pepper directions : prepare meal LTL: ('and', ('eventually', 'black_pepper_in_player'), ('eventually', 'meal_in_player'))</p>
</li>
<li>
<p>NL: you open the copy of "cooking : a modern approach ( 3rd ed .)" and start reading : recipe # 1 -----gather all following ingredients and follow the directions to prepare this tasty meal .ingredients : purple potato red onion salt directions : dice the purple potato roast the purple potato dice the red onion fry the red onion prepare meal LTL: ('and', ('eventually', 'purple_potato_in_player'), ('and', ('eventually', 'red_onion_in_player'), ('and', ('eventually', 'salt_in_player'), ('and', ('eventually', 'purple_potato_is_diced'), ('and', ('eventually', 'purple_potato_is_roasted'), ('and', ('eventually', 'red_onion_is_diced'), ('and', ('eventually', 'red_onion_is_fried'), ('eventually', 'meal_in_player'))))))))</p>
</li>
<li>
<p>NL: you open the copy of "cooking : a modern approach ( 3rd ed .)" and start reading : recipe # 1 -----gather all following ingredients and follow the directions to prepare this tasty meal .ingredients : black pepper parsley salt directions : dice the parsley prepare meal LTL: ('and', ('eventually', 'black_pepper_in_player'), ('and', ('eventually', 'parsley_in_player'), ('and', ('eventually', 'salt_in_player'), ('and', ('eventually', 'parsley_is_diced'), ('eventually', 'meal_in_player')))))</p>
</li>
<li>
<p>NL: you open the copy of "cooking : a modern approach ( 3rd ed .)" and start reading : recipe # 1 -----gather all following ingredients and follow the directions to prepare this tasty meal .ingredients : purple potato white onion yellow bell pepper directions : roast the purple potato roast the white onion dice the yellow bell pepper prepare meal LTL: ('and', ('eventually', 'purple_potato_in_player'), ('and', ('eventually', 'white_onion_in_player'), ('and', ('eventually', 'yellow_bell_pepper_in_player'), ('and', ('eventually', 'purple_potato_is_roasted'), ('and',</p>
</li>
</ol>
<p>I Broader Impact</p>
<p>As Adhikari et al. (2020) suggested, text-based games can be a proxy for studying human-machine interaction through language.Human-machine interaction and relevant systems have many potential ethical, social, and safety concerns.Providing inaccurate policies or information or partially completing tasks in critical systems can have devastating consequences.For example, in health care, improper treatment can be fatal, or in travel planning, poor interactions can lose a client money.</p>
<p>Adhikari et al. (2020, section 7) identified several research objectives relating to language-based agents: improve the ability to make better decisions, allow for constraining decisions for safety purposes, and improve interpretability.We highlight how RL agents equipped with LTL instructions can improve in these areas.For constraining decisions, it may be desirable to do so in way that depends on the history, which LTL gives a way to keep track of.With respect to interpretability, we propose that monitoring the progression of instructions provides a mechanism for understanding where and when an agent might be making incorrect decisions, and provides the opportunity to revise instructions or attempt to fix the problem by other means.</p>
<p>However, instruction following, especially overly literal instruction following, may not always be beneficial and can even be harmful.Ammanabrolu et al. (2022) describe a good example where an agent in the Zork1 game breaks into a home and steals the items it needs.In that specific case, breaking into the home has no adverse effect on the agent's reward, and so it has no incentive not to perform this act.Violation of social norms like this are not modelled in our work, and can have negative impacts, even in less extreme cases.Furthermore, there are potential dangers of incorrect, immoral, or even misinterpreted instructions that lead to dangerous outcomes.Although we do not directly address these concerns in this work, they pose interesting directions for future work.</p>
<p>Figure 2 depicts an episode step interaction of LTL-GATA with TextWorld and Figure 3 depicts the model itself.Additional details can be found in Appendix F.</p>
<p>Figure 2 :
2
Figure 2: An example of a single step in an episode of TextWorld.The game environment returns an observation o t and action candidate set C t in response to action a t−1 .In turn, the agent's graph updater (GATA) updates its belief graph g t in response to both o t and g t−1 .Next, g t and o t update the LTL instructions.ϕ t is generated from o t after the cookbook is examined and thereafter ϕ t−1 is progressed to ϕ t at each time step.The policy network selects action a t from C t conditioned on o t , ϕ t , and g t and the cycle repeats.</p>
<p>Figure 3 :
3
Figure 3: LTL-GATA's policy model.The model chooses action a t ∈ C t conditioned on the state z t = [o t ; ϕ t ; g t ].The action selector chooses a t based on the predicted Q-values.</p>
<p>): (1) we use a batch size of 200 instead of 64 when training on the 100 game set, (2) for level 3, we use Boltzmann Action selection, and (3) we use AdamKingma &amp; Ba (2015)  with a learning rate of 0.0003 instead of RAdamLiu et al. (2020)  with a learning rate of 0.001.These changes boosted performance for all models.See subsection H.1 for more details.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Testing scores across various levels and on both the 20 (top) and 100 (bottom) game training sets.We select the top-performing models (per seed) on the validation set during training and apply those models on the test set and report the average scores.</p>
<p>LTL-GATA with the new reward function R LTL (s, a, ϕ) and without LTL-based episode termination; (b) LTL-GATA with the base TextWorld reward function R(s, a) and LTL-based episode termination; and (c) LTL-GATA with the normal TextWorld reward function R(s, a) and without LTL-based episode termination.For (a) and (b) we select level 1 on both the 20 and 100 game training set and level 2 on the 20 game training set.For (c) we select level 2 on 20 training games.</p>
<p>Table 1 :
1
Cooking Levels
LevelRecipeSizeRoomsMaxScoreNeed{Grab,Cut,Cook}</p>
<p>Table 2 :
2
TextWorld observations for the Cooking Domain game.We show the observations and highlight where the instructions are, and finally identify what the rewards would be.This is for a level 2 game, and the total possible reward is 5.Initial Game Observation "You are hungry!Let's cook a delicious meal.Check the cookbook in the kitchen for the recipe.Once done, enjoy your meal!" -= kitchen =-you find yourself in a kitchen.You start to take note of what's in the room.You can make out a closed fridge nearby.You can see an oven.You can make out a table.You wonder idly who left that here.You see a knife on the table.Something scurries by right in the corner of your eye.Probably nothing.You see a counter.The counter is vast.On the counter you see a raw red potato and a cookbook.You see a stove, but the thing is empty, unfortunately."</p>
<p>Table 4 :
4
Level 1 observation and resulting generated LTL instruction Observation</p>
<p>Table 6 :
6
Training times for each model and training set size.The times were reported using a workstation with dual RTX3090s, an AMD Ryzen 5950x 16-core CPU, and 128GB of RAM.For the graph updater, COC stands for the contrastive observation classification pre-training (the continuous belief graph model) and GTP stands for ground-truth pre-training (the discrete belief graph model).
ModelTraining Set Size Batch Size Approximate TimeTDQN206416 hoursLTL-GATA206424 hoursGATA D206424 hoursGATA C206424 hoursGATA D P GATA C P20 2064 6436 hours 36 hoursTDQN10020032 hoursLTL-GATA10020048 hoursGATA D *10020048 hoursGATA C10020048 hoursGATA D P GATA C P100 100200 20065 hours 65 hoursGraph Updater using COCN/A6448 hoursGraph Updater using GTPN/A6448 hoursH.3 Additional Results
Our code for the experiments can be found at https://github.com/MathieuTuli/LTL-GATA
https://github.com/xingdi-eric-yuan/GATA-public, released under the open-source MIT License.
https://github.com/xingdi-eric-yuan/GATA-public/blob/c1afc3c9ab38256f839b3e0ddf8243796df5bd77/ dqn_memory_priortized_replay_buffer.py#L120-L123
https://github.com/xingdi-eric-yuan/GATA-public/blob/c1afc3c9ab38256f839b3e0ddf8243796df5bd77/ agent.py#L1353-L1369
https://github.com/xingdi-eric-yuan/GATA-public/blob/c1afc3c9ab38256f839b3e0ddf8243796df5bd77/ dqn_memory_priortized_replay_buffer.py#L93-L102
AcknowledgementsWe thank the NeurIPS reviewers for their constructive feedback, and also the reviewers from the Wordplay: When Language Meets Games workshop at NAACL 2022, where a preliminary version of this paper appeared(Tuli et al., 2022).We gratefully acknowledge funding from the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada CIFAR AI Chairs Program, and Microsoft Research.Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence (www.vectorinstitute.ai/partners).Finally, we thank the Schwartz Reisman Institute for Technology and Society for providing a rich multi-disciplinary research environment.The footnotes in section 5 include links to the original assets.(c) Did you include any new assets either in the supplemental material or as a URL?[Yes] The only new asset in this work is our code, which we provide here: https: //github.com/MathieuTuli/LTL-GATA. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating?[N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?The single-token predicates are mapped in the vocabulary to a single word embedding.In our work, we compute word embedding for these single-token predicates by averaging the word embeddings of each underscore-separated word in the predicate.For example, the word embedding (WE) of the token player_has_pepper isFor multiple-token predicates, each word has its own word embedding and we treat each word as any other word in the sentence.The idea is that by separating the tokens in the predicates, the text encoder (transformer) may be able to attend to each token independently, and during testing have better generalization.We visualize the results of this study in Figure6.We can see from Figure6that this in fact does not help, and LTL-GATA performs almost equally in either scenario.This does however show how our method is robust to predicate format.H.3.2 The Effect of LTL Reward and LTL-Based TerminationIt is important to study the effect that the additional LTL bonus reward and LTL-based episode termination has on the performance of LTL-GATA.To study this, we consider three scenarios: (a)  No Termination No Reward  ('eventually', 'white_onion_is_roasted'), ('and', ('eventually', 'yellow_bell_pepper_is_diced'), ('eventually', 'meal_in_player')))))))7. NL: you open the copy of "cooking : a modern approach ( 3rd ed .)" and start reading : recipe # 1 -----gather all following ingredients and follow the directions to prepare this tasty meal .ingredients : banana red hot pepper yellow potato directions : chop the banana fry the banana chop the red hot pepper fry the red hot pepper slice the yellow potato fry the yellow potato prepare meal LTL:In Table7, we show how Da Vinci and Ada complete the prompt to translate this example of a natural language TextWorld observation.While Da Vinci produces the correct LTL translation, Ada struggles and hallucinates incorrect predicates such as 'zucchini_fry_player' and 'banana_is_frozen'.Table7: GPT-3 Natural-Language-to-LTL Translation Example Observation you open the copy of "cooking : a modern approach ( 3rd ed .)" and start reading : recipe # 1 -----gather all following ingredients and follow the directions to prepare this tasty meal .ingredients : banana red hot pepper yellow potato directions : chop the banana fry the banana chop the red hot pepper fry the red hot pepper slice the yellow potato fry the yellow potato prepare meal Da Vinci LTL Translation (Absolutely Correct)('and', ('eventually', 'banana_in_player'), ('and', ('eventually', 'red_hot_pepper_in_player'), ('and', ('eventually', 'yellow_potato_in_player'), ('and', ('eventually', 'banana_is_chopped'), ('and', ('eventually', 'banana_is_fried'), ('and', ('eventually', 'red_hot_pepper_is_chopped'), ('and', ('eventually', 'red_hot_pepper_is_fried'), ('and', ('eventually', 'yellow_potato_is_sliced'), ('and', ('eventually', 'yellow_potato_is_fried'), ('eventually', 'meal_in_player'))))))))))Ada LTL Translation (Incorrect)('and', ('eventually', 'banana_in_player'), ('and', ('eventually', 'red_hot_pepper_in_player'), ('and', ('eventually', 'yellow_potato_in_player'), ('and', ('eventually', 'zucchini_fry_player'), ('and', ('eventually', 'banana_is_frozen'), ('eventually', 'meal_in_player'))))
Learning dynamic belief graphs to generalize on text-based games. A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W Hamilton, Advances in Neural Information Processing Systems. NeurIPS2020. 202033</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M J Hausknecht, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. P Ammanabrolu, M O Riedl, 10.18653/v1/n19-1358Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational LinguisticsJune 2-7, 2019. 20191</p>
<p>Aligning to social norms and values in interactive narratives. P Ammanabrolu, L Jiang, M Sap, H Hajishirzi, Y Choi, 10.48550/arXiv.2205.019752022</p>
<p>Layer normalization. L J Ba, J R Kiros, G E Hinton, CoRR, abs/1607.064502016</p>
<p>Using temporal logics to express search control knowledge for planning. F Bacchus, F Kabanza, Artificial Intelligence. 1161-22000</p>
<p>Principles of Model Checking. C Baier, J Katoen, 2008MIT Press</p>
<p>Planning with temporally extended goals using heuristic search. J Baier, S Mcilraith, Proceedings of the 16th International Conference on Automated Planning and Scheduling (ICAPS06). the 16th International Conference on Automated Planning and Scheduling (ICAPS06)June 2006</p>
<p>Do as I can, not as I say: Grounding language in robotic affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, 6th Annual Conference on Robot Learning. 2022</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. 202033</p>
<p>Strong fully observable non-deterministic planning with LTL and LTL-f goals. A Camacho, S A Mcilraith, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI). the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)2019</p>
<p>LTL and beyond: Formal languages for reward function specification in reinforcement learning. A Camacho, R Toro Icarte, T Q Klassen, R A Valenzano, S A Mcilraith, Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI). the 28th International Joint Conference on Artificial Intelligence (IJCAI)2019</p>
<p>Textworld: A learning environment for textbased games. M Côté, Á Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M J Hausknecht, L E Asri, M Adada, W Tay, A Trischler, 10.1007/978-3-030-24337-1_3Computer Games -7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018. T Cazenave, A Saffidine, N R Sturtevant, Stockholm, SwedenSpringerJuly 13. 2018. 20181017Revised Selected Papers</p>
<p>Spot 2.0-A Framework for LTL and ω-Automata Manipulation. A Duret-Lutz, A Lewkowicz, A Fauchille, T Michaud, E Renault, L Xu, Proceedings of the 14th International Symposium on Automated Technology for Verification and Analysis (ATVA). the 14th International Symposium on Automated Technology for Verification and Analysis (ATVA)Springer2016</p>
<p>What to do and how to do it: Translating natural language directives into temporal and dynamic logic representation for goal management and action execution. J Dzifcak, M Scheutz, C Baral, P Schermerhorn, Proceedings of the 2009 IEEE International Conference on Robotics and Automation (ICRA). the 2009 IEEE International Conference on Robotics and Automation (ICRA)IEEE2009</p>
<p>Experimenting with language, temporal logic and robot control. C Finucane, G Jing, H Kress-Gazit, Ltlmop, 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2010</p>
<p>C Hahn, F Schmitt, J J Tillman, N Metzger, J Siber, B Finkbeiner, arXiv:2206.01962Formal specifications from natural language. 2022arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Deep reinforcement learning with a natural language action space. J He, J Chen, X He, J Gao, L Li, L Deng, M Ostendorf, 10.18653/v1/p16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. Long Papers. The Association for Computer Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016Berlin, GermanyAugust 7-12, 2016. 20161</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, arXiv:2201.072072022arXiv preprint</p>
<p>Algorithmic improvements for deep reinforcement learning applied to interactive fiction. V Jain, W Fedus, H Larochelle, D Precup, M G Bellemare, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Neuro-symbolic reinforcement learning with first-order logic. D Kimura, M Ono, S Chaudhury, R Kohita, A Wachi, D J Agravante, M Tatsubori, A Munawar, A Gray, 10.18653/v1/2021.emnlp-main.283Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. M Moens, X Huang, L Specia, S W Yih, the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican RepublicAssociation for Computational Linguistics7-11 November, 20212021</p>
<p>A method for stochastic optimization. D P Kingma, J Ba, Adam, 3rd International Conference on Learning Representations, ICLR 2015. Y Bengio, Y Lecun, San Diego, CA, USAMay 7-9, 2015. 2015Conference Track Proceedings</p>
<p>Encoding formulas as deep networks: Reinforcement learning for zero-shot execution of LTL formulas. Y Kuo, B Katz, A Barbu, 10.1109/IROS45743.2020.9341325IEEE/RSJ International Conference on Intelligent Robots and Systems. Las Vegas, NV, USAIEEEOctober 24, 2020 -January 24, 2021. 20202020</p>
<p>Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning. B G Leon, M Shanahan, F Belardinelli, arXiv:2006.087672020arXiv preprint</p>
<p>Environment-independent task specifications via GLTL. M L Littman, U Topcu, J Fu, C L I Jr, M Wen, J Macglashan, CoRR, abs/1704.043412017</p>
<p>Learning object-oriented dynamics for planning from text. G Liu, A Adhikari, A.-M Farahmand, P Poupart, ICLR 2022April 2022</p>
<p>On the variance of the adaptive learning rate and beyond. L Liu, H Jiang, P He, W Chen, X Liu, J Gao, J Han, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>
<p>T Mikolov, E Grave, P Bojanowski, C Puhrsch, A Joulin, arXiv:1712.09405Advances in pre-training distributed word representations. 2017arXiv preprint</p>
<p>Language understanding for text-based games using deep reinforcement learning. K Narasimhan, T D Kulkarni, R Barzilay, 10.18653/v1/d15-1001Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, D Pighin, Y Marton, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational Linguistics2015. September 17-21, 2015. 2015</p>
<p>Computing infinite plans for LTL goals using a classical planner. F Patrizi, N Lipovetzky, G D Giacomo, H Geffner, 10.5591/978-1-57735-516-8/IJCAI11-334Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI). the 22nd International Joint Conference on Artificial Intelligence (IJCAI)2011IJCAI/AAAI</p>
<p>The temporal logic of programs. A Pnueli, Proceedings of the 18th IEEE Symposium on Foundations of Computer Science (FOCS). the 18th IEEE Symposium on Foundations of Computer Science (FOCS)IEEE1977</p>
<p>Prioritized experience replay. T Schaul, J Quan, I Antonoglou, D Silver, 4th International Conference on Learning Representations, ICLR 2016. Y Bengio, Y Lecun, San Juan, Puerto RicoMay 2-4, 2016. 2016Conference Track Proceedings</p>
<p>Modeling relational data with graph convolutional networks. M Schlichtkrull, T N Kipf, P Bloem, R V D Berg, I Titov, M Welling, European Semantic Web Conference. Springer2018</p>
<p>. R K Srivastava, K Greff, J Schmidhuber, arXiv:1505.003872015Highway networks. arXiv preprint</p>
<p>Advice-based exploration in model-based reinforcement learning. R Toro Icarte, T Q Klassen, R Valenzano, S A Mcilraith, Proceedings of the 31st Canadian Conference on Artificial Intelligence (CCAI). the 31st Canadian Conference on Artificial Intelligence (CCAI)Springer2018a</p>
<p>Teaching multiple tasks to an RL agent using LTL. R Toro Icarte, T Q Klassen, R Valenzano, S A Mcilraith, Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. the 17th International Conference on Autonomous Agents and MultiAgent Systems2018b</p>
<p>First TextWorld Problems, the competition: Using text-based games to advance capabilities of AI agents. A Trischler, M.-A Côté, P Lima, 2019Microsoft Research Blog</p>
<p>Instruction following in text-based games. M Tuli, A Li, P Vaezipoor, T Q Klassen, S Sanner, S A Mcilraith, Wordplay: When Language Meets Games Workshop @ NAACL 2022. 2022</p>
<p>LTL2action: Generalizing LTL instructions for multi-task RL. P Vaezipoor, A C Li, R A Toro Icarte, S A Mcilraith, International Conference on Machine Learning. PMLR2021</p>
<p>Deep reinforcement learning with double q-learning. H Van Hasselt, A Guez, D Silver, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201630</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Learning a natural-language to LTL executable semantic parser for grounded robotics. C Wang, C Ross, Y Kuo, B Katz, A Barbu, of Proceedings of Machine Learning Research. J Kober, F Ramos, C J Tomlin, CoRL; Cambridge, MA, USAPMLR2020, 16-18 November 2020. 20201554th Conference on Robot Learning</p>
<p>Comprehensible context-driven text game playing. X Yin, J May, 2019 IEEE Conference on Games (CoG). IEEE2019a</p>
<p>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games. X Yin, J May, arXiv:1908.047772019barXiv preprint</p>
<p>Counting to explore and generalize in text-based games. X Yuan, M Côté, A Sordoni, R Laroche, R T Des Combes, M J Hausknecht, A Trischler, CoRR, abs/1806.115252018</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. T Zahavy, M Haroush, N Merlis, D J Mankowitz, S Mannor, P Jiang, L Sap, M Hajishirzi, H Choi, Y , 10.48550/arXiv.2205.01975Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. S Bengio, H M Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, Montréal, Canada2018. 2018. December 3-8, 2018. 20182022Aligning to social norms and values in interactive narratives</p>
<p>A method for stochastic optimization. D P Kingma, J Ba, Adam, ICLR 20153rd International Conference on Learning Representations. San Diego, CA, USAMay 7-9, 2015. 2015Conference Track Proceedings</p>
<p>On the variance of the adaptive learning rate and beyond. L Liu, H Jiang, P He, W Chen, X Liu, J Gao, J Han, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>            </div>
        </div>

    </div>
</body>
</html>
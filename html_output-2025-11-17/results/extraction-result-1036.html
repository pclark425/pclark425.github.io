<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1036 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1036</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1036</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-267636859</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.08212v1.pdf" target="_blank">BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents</a></p>
                <p><strong>Paper Abstract:</strong> Embodied agents capable of complex physical skills can improve productivity, elevate life quality, and reshape human-machine collaboration. We aim at autonomous training of embodied agents for various tasks involving mainly large foundation models. It is believed that these models could act as a brain for embodied agents; however, existing methods heavily rely on humans for task proposal and scene customization, limiting the learning autonomy, training efficiency, and generalization of the learned policies. In contrast, we introduce a brain-body synchronization ({\it BBSEA}) scheme to promote embodied learning in unknown environments without human involvement. The proposed combines the wisdom of foundation models (``brain'') with the physical capabilities of embodied agents (``body''). Specifically, it leverages the ``brain'' to propose learnable physical tasks and success metrics, enabling the ``body'' to automatically acquire various skills by continuously interacting with the scene. We carry out an exploration of the proposed autonomous learning scheme in a table-top setting, and we demonstrate that the proposed synchronization can generate diverse tasks and develop multi-task policies with promising adaptability to new tasks and configurations. We will release our data, code, and trained models to facilitate future studies in building autonomously learning agents with large foundation models in more complex scenarios. More visualizations are available at \href{https://bbsea-embodied-ai.github.io}{https://bbsea-embodied-ai.github.io}</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1036.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1036.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BBSEA-agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Brain-Body Synchronization Embodied Agent (BBSEA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tabletop robotic arm agent trained via a brain-body synchronization pipeline where large foundation models (LFMs) propose scene-compatible tasks and success metrics (the "brain") and the robot (the "body") collects demonstrations and learns a language-conditioned multi-task visuomotor policy via behavior cloning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BBSEA embodied agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A single-arm robotic manipulator (gripper) that learns manipulation skills through: (1) LFM-driven task proposal and success inference (GPT-4 / LLMs + scene graph), (2) automated demonstration collection using waypoint sampling, motion planning and a set of 7 primitive actions, and (3) distillation into a language-conditioned multi-task visuomotor policy trained with behavior cloning (imitation learning). No online RL was used for the main distilled policy; behavior cloning on collected successful trajectories is the primary learning algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical/robotic agent (tabletop robot arm with gripper) / simulated-tabletop environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Tabletop manipulation environment (as in Ha et al., 2023 benchmark used by BBSEA)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A tabletop scene observed by two RGBD cameras (front and top) containing multiple objects (blocks, bowls, toys, drawer, catapult, etc.). Scene state is represented as a scene graph with object class, state (e.g., open/closed), 3D positions, bounding boxes and spatial relations. Tasks vary from single-object moves to multi-step stacking, pushing, opening drawers and pressing buttons. Environment complexity arises from object count, spatial relations, object states (e.g., drawers), and long-horizon or physically ambiguous actions; variation arises from the number and diversity of tasks proposed by the LLM and differences across scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized primarily by: (1) task structure / difficulty (long-horizon vs short-horizon; presence of stateful objects like drawers/microwave), (2) number and type of objects in scene, (3) availability/precision of spatial info (bounding boxes & positions in scene graph). Task diversity quantified by a human-derived task-distance scoring (five factors: main action, shape of object, shape of location, object color, target color) and visualized using MDS area coverage; numerical proxies used: number of trained tasks (10, 30, 60), per-scene object counts and explicit scene graph entries.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (tabletop domain with multiple objects and stateful elements; limited to tabletop scenarios and a fixed set of primitive actions)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct proposed training tasks (experiments used 10, 30, 60 tasks); task diversity score (human survey-driven distance metric aggregated across five factors) and MDS area coverage; also evaluated across multiple scene instances (five scenes in feasibility/success-inference studies; nine environments visualized for policy data collection).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>variable: low (10 tasks), medium (30 tasks), high (60 tasks) — paper treats number of tasks as primary measure of variation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Primary: task success rate (percentage of successful task executions). Secondary: task proposal feasibility rate (% of proposed tasks that lead to successful demo collection), success-inference accuracy (% correct completion judgments), confusion-matrix rates (TPR/TNR). Zero-shot and fine-tuning success rates used to evaluate generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Key reported values: distilled multi-task policy average success rate 68.75% vs data-collection policy 43.44% (Table 3). Task-proposal feasibility for BBSEA: 100.00% (Scenes 0–3) and 70.00% (Scene 4) in Table 1. Success-inference accuracy for BBSEA: often 100% across scenes, with some scenes 83.34% (Table 2). Task-completion inference confusion: true positive rate 81.82%, true negative rate 84.69% (Appendix E / Fig.7). Zero-shot averages: 10-task pretrained policy 0.00% avg, 30-task 15.80% avg, 60-task 48.57% avg (Table 4). Demonstrations: 200 successful demos per training task collected for policy distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly reports that increased task variety (higher environment variation as measured by number/diversity of tasks) improves generalization and zero-shot performance: policies trained on more tasks (30 → 60) show substantially higher zero-shot success and faster/firmer fine-tuning. The paper also highlights that richer/per- object scene information (higher perceptual complexity via detailed scene graphs with bounding boxes and positions) is critical for generating feasible tasks and accurate success inference; ablations removing bbox/position degrade feasibility and inference accuracy. Trade-offs discussed: behavior cloning reduces exploration overhead (benefit) but uses fixed primitives and may limit online adaptation (cost); task-completion inference is less reliable for 'Push' primitives and long-horizon/color-spatial tasks (limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automated multi-task learning with demonstration collection and behavior cloning (imitation learning). The pipeline: LFM-driven task proposal → LLM task decomposition into primitive actions → waypoint sampling + motion planning execute trials → LLM success inference to filter successful trajectories → store successful demos → distill into language-conditioned multi-task visuomotor policy. Pretraining experiments vary number of tasks (10/30/60) and use rehearsal/fine-tuning with small demo counts (20–100) for few-shot adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Zero-shot: policies trained on larger and more diverse task sets generalize better — averaged zero-shot success: 10-task 0.00%, 30-task 15.80%, 60-task 48.57% (Table 4). Few-shot/fine-tuning: policies pre-trained on more tasks (60-task) achieve higher fine-tuning success across unseen tasks with limited demonstrations (20–100 demos) compared to 0/10/30-task baselines (Table 5). Distilled multi-task policy also shows better per-trained-task performance vs the raw data-collection policy (average 68.75% vs 43.44%).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training used 200 successful demonstrations per task for distillation (explicit). Fine-tuning experiments show measurable adaptation from as few as 20 demonstrations per unseen task; zero-shot performance improves with scale of training-task variety rather than sheer per-task samples. No per-episode interaction counts for demonstration collection are provided, but trial-and-error collectors were used until successful demos were obtained.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) LLMs + structured scene graphs can autonomously propose a diverse and feasible set of manipulation tasks; detailed spatial scene graphs (bounding boxes & positions) are essential — ablation (no bbox/positions) reduces feasibility and success-inference accuracy. 2) Behavior cloning on LLM-filtered successful demonstrations yields a distilled policy that substantially outperforms the raw data-collection policy (avg success: 68.75% vs 43.44%). 3) Training on more diverse tasks (10→30→60) increases the policy's zero-shot and few-shot generalization (zero-shot average: 0% → 15.8% → 48.57%). 4) Success inference by GPT-4 (on scene graphs) is accurate overall (often near 100% in BBSEA) but exhibits failure modes for push primitives and long-horizon/color-spatial tasks (observed false positives ~16 in 100 successful-labeled trajectories in manual check; confusion matrix TPR 81.82%, TNR 84.69%). 5) The pipeline reduces human involvement and exploration overhead but is currently limited to tabletop settings and a fixed set of primitive actions; scaling to more complex real-world scenes and expanded primitives is future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scaling up and distilling down: Language-guided robot skill acquisition <em>(Rating: 2)</em></li>
                <li>Gensim: Generating robotic simulation tasks via large language models <em>(Rating: 2)</em></li>
                <li>Text2reward: Automated dense reward function generation for reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1036",
    "paper_id": "paper-267636859",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "BBSEA-agent",
            "name_full": "Brain-Body Synchronization Embodied Agent (BBSEA)",
            "brief_description": "A tabletop robotic arm agent trained via a brain-body synchronization pipeline where large foundation models (LFMs) propose scene-compatible tasks and success metrics (the \"brain\") and the robot (the \"body\") collects demonstrations and learns a language-conditioned multi-task visuomotor policy via behavior cloning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BBSEA embodied agent",
            "agent_description": "A single-arm robotic manipulator (gripper) that learns manipulation skills through: (1) LFM-driven task proposal and success inference (GPT-4 / LLMs + scene graph), (2) automated demonstration collection using waypoint sampling, motion planning and a set of 7 primitive actions, and (3) distillation into a language-conditioned multi-task visuomotor policy trained with behavior cloning (imitation learning). No online RL was used for the main distilled policy; behavior cloning on collected successful trajectories is the primary learning algorithm.",
            "agent_type": "physical/robotic agent (tabletop robot arm with gripper) / simulated-tabletop environment",
            "environment_name": "Tabletop manipulation environment (as in Ha et al., 2023 benchmark used by BBSEA)",
            "environment_description": "A tabletop scene observed by two RGBD cameras (front and top) containing multiple objects (blocks, bowls, toys, drawer, catapult, etc.). Scene state is represented as a scene graph with object class, state (e.g., open/closed), 3D positions, bounding boxes and spatial relations. Tasks vary from single-object moves to multi-step stacking, pushing, opening drawers and pressing buttons. Environment complexity arises from object count, spatial relations, object states (e.g., drawers), and long-horizon or physically ambiguous actions; variation arises from the number and diversity of tasks proposed by the LLM and differences across scenes.",
            "complexity_measure": "Characterized primarily by: (1) task structure / difficulty (long-horizon vs short-horizon; presence of stateful objects like drawers/microwave), (2) number and type of objects in scene, (3) availability/precision of spatial info (bounding boxes & positions in scene graph). Task diversity quantified by a human-derived task-distance scoring (five factors: main action, shape of object, shape of location, object color, target color) and visualized using MDS area coverage; numerical proxies used: number of trained tasks (10, 30, 60), per-scene object counts and explicit scene graph entries.",
            "complexity_level": "medium (tabletop domain with multiple objects and stateful elements; limited to tabletop scenarios and a fixed set of primitive actions)",
            "variation_measure": "Number of distinct proposed training tasks (experiments used 10, 30, 60 tasks); task diversity score (human survey-driven distance metric aggregated across five factors) and MDS area coverage; also evaluated across multiple scene instances (five scenes in feasibility/success-inference studies; nine environments visualized for policy data collection).",
            "variation_level": "variable: low (10 tasks), medium (30 tasks), high (60 tasks) — paper treats number of tasks as primary measure of variation",
            "performance_metric": "Primary: task success rate (percentage of successful task executions). Secondary: task proposal feasibility rate (% of proposed tasks that lead to successful demo collection), success-inference accuracy (% correct completion judgments), confusion-matrix rates (TPR/TNR). Zero-shot and fine-tuning success rates used to evaluate generalization.",
            "performance_value": "Key reported values: distilled multi-task policy average success rate 68.75% vs data-collection policy 43.44% (Table 3). Task-proposal feasibility for BBSEA: 100.00% (Scenes 0–3) and 70.00% (Scene 4) in Table 1. Success-inference accuracy for BBSEA: often 100% across scenes, with some scenes 83.34% (Table 2). Task-completion inference confusion: true positive rate 81.82%, true negative rate 84.69% (Appendix E / Fig.7). Zero-shot averages: 10-task pretrained policy 0.00% avg, 30-task 15.80% avg, 60-task 48.57% avg (Table 4). Demonstrations: 200 successful demos per training task collected for policy distillation.",
            "complexity_variation_relationship": "Yes — the paper explicitly reports that increased task variety (higher environment variation as measured by number/diversity of tasks) improves generalization and zero-shot performance: policies trained on more tasks (30 → 60) show substantially higher zero-shot success and faster/firmer fine-tuning. The paper also highlights that richer/per- object scene information (higher perceptual complexity via detailed scene graphs with bounding boxes and positions) is critical for generating feasible tasks and accurate success inference; ablations removing bbox/position degrade feasibility and inference accuracy. Trade-offs discussed: behavior cloning reduces exploration overhead (benefit) but uses fixed primitives and may limit online adaptation (cost); task-completion inference is less reliable for 'Push' primitives and long-horizon/color-spatial tasks (limitation).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automated multi-task learning with demonstration collection and behavior cloning (imitation learning). The pipeline: LFM-driven task proposal → LLM task decomposition into primitive actions → waypoint sampling + motion planning execute trials → LLM success inference to filter successful trajectories → store successful demos → distill into language-conditioned multi-task visuomotor policy. Pretraining experiments vary number of tasks (10/30/60) and use rehearsal/fine-tuning with small demo counts (20–100) for few-shot adaptation.",
            "generalization_tested": true,
            "generalization_results": "Zero-shot: policies trained on larger and more diverse task sets generalize better — averaged zero-shot success: 10-task 0.00%, 30-task 15.80%, 60-task 48.57% (Table 4). Few-shot/fine-tuning: policies pre-trained on more tasks (60-task) achieve higher fine-tuning success across unseen tasks with limited demonstrations (20–100 demos) compared to 0/10/30-task baselines (Table 5). Distilled multi-task policy also shows better per-trained-task performance vs the raw data-collection policy (average 68.75% vs 43.44%).",
            "sample_efficiency": "Training used 200 successful demonstrations per task for distillation (explicit). Fine-tuning experiments show measurable adaptation from as few as 20 demonstrations per unseen task; zero-shot performance improves with scale of training-task variety rather than sheer per-task samples. No per-episode interaction counts for demonstration collection are provided, but trial-and-error collectors were used until successful demos were obtained.",
            "key_findings": "1) LLMs + structured scene graphs can autonomously propose a diverse and feasible set of manipulation tasks; detailed spatial scene graphs (bounding boxes & positions) are essential — ablation (no bbox/positions) reduces feasibility and success-inference accuracy. 2) Behavior cloning on LLM-filtered successful demonstrations yields a distilled policy that substantially outperforms the raw data-collection policy (avg success: 68.75% vs 43.44%). 3) Training on more diverse tasks (10→30→60) increases the policy's zero-shot and few-shot generalization (zero-shot average: 0% → 15.8% → 48.57%). 4) Success inference by GPT-4 (on scene graphs) is accurate overall (often near 100% in BBSEA) but exhibits failure modes for push primitives and long-horizon/color-spatial tasks (observed false positives ~16 in 100 successful-labeled trajectories in manual check; confusion matrix TPR 81.82%, TNR 84.69%). 5) The pipeline reduces human involvement and exploration overhead but is currently limited to tabletop settings and a fixed set of primitive actions; scaling to more complex real-world scenes and expanded primitives is future work.",
            "uuid": "e1036.0",
            "source_info": {
                "paper_title": "BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scaling up and distilling down: Language-guided robot skill acquisition",
            "rating": 2,
            "sanitized_title": "scaling_up_and_distilling_down_languageguided_robot_skill_acquisition"
        },
        {
            "paper_title": "Gensim: Generating robotic simulation tasks via large language models",
            "rating": 2,
            "sanitized_title": "gensim_generating_robotic_simulation_tasks_via_large_language_models"
        },
        {
            "paper_title": "Text2reward: Automated dense reward function generation for reinforcement learning",
            "rating": 1,
            "sanitized_title": "text2reward_automated_dense_reward_function_generation_for_reinforcement_learning"
        }
    ],
    "cost": 0.013039499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents
13 Feb 2024</p>
<p>Sizhe Yang 
University of Hong Kong</p>
<p>Qian Luo 
University of Hong Kong</p>
<p>Anumpam Pani 
University of Hong Kong</p>
<p>Yanchao Yang 
University of Hong Kong</p>
<p>Hong Kong 
University of Hong Kong</p>
<p>BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents
13 Feb 2024CBFF15EFE32230D9E08AB3D0C48FB546arXiv:2402.08212v1[cs.RO]
Embodied agents capable of complex physical skills can improve productivity, elevate life quality, and reshape human-machine collaboration.We aim at autonomous training of embodied agents for various tasks involving mainly large foundation models.It is believed that these models could act as a brain for embodied agents; however, existing methods heavily rely on humans for task proposal and scene customization, limiting the learning autonomy, training efficiency, and generalization of the learned policies.In contrast, we introduce a brain-body synchronization (BB-SEA) scheme to promote embodied learning in unknown environments without human involvement.The proposed combines the wisdom of foundation models ("brain") with the physical capabilities of embodied agents ("body").Specifically, it leverages the "brain" to propose learnable physical tasks and success metrics, enabling the "body" to automatically acquire various skills by continuously interacting with the scene.We carry out an exploration of the proposed autonomous learning scheme in a table-top setting, and we demonstrate that the proposed synchronization can generate diverse tasks and develop multi-task policies with promising adaptability to new tasks and configurations.We will release our data, code, and trained models to facilitate future studies in building autonomously learning agents with large foundation models in more complex scenarios.More visualizations are available at https://bbseaembodied-ai.github.io</p>
<p>Introduction</p>
<p>With age, one may lose the capacity to act, to see, and even to hear; however, it also comes with wisdom such that one can think smartly and view clearly through the mind's eye.Thus, it is important to learn from senior people, whose rich experience and structural reasoning can significantly facilitate the development of the younger ones (Fig. 1 (a)).Nowadays, given the advancement of large foundation models (LFMs), a similar complementarity emerges for the development of embodied agents.On one hand, we have models like GPT (Radford et al., 2018;2019;Brown et al., 2020;OpenAI, 2023) that can perform reasoning in the language space as well as question and answer with visual input.However, these models are not grounded in the 3D environment (embodiment) and are incapable of physical interactions.On the other, we have embodied agents that can interact with the scene but lack an understanding of the physical world to accomplish semantically meaningful tasks.</p>
<p>In this work, we treat the ensemble of the foundation models (e.g., GPTs) as a wise elderly equipped with the common sense of the physical world (the brain), and seek the possibilities of training an embodied agent (the body) from scratch with the disembodied structural knowledge of the brain towards intelligent physical interactions.More explicitly, we consider the scenarios in which the foundation models, together with an embodied agent (of a basic factory setting), are put into an environment without any prior knowledge of the specific scene.Our goal is to develop a pipeline that enables the autonomous acquisition of various physical interaction skills for the embodied agent with as little human involvement as possible.</p>
<p>We propose that the key to our goal lies in threefold.First, the brain has to propose interaction tasks or skills for the body to learn, which have to be compatible with the scene and achievable given the physical constraints of the agent.Second, to make the proposed tasks learnable by the body, the brain should also define the tasks by specifying metrics that help determine whether the task is successfully executed or not.Lastly, the body has to acquire the skills for accomplishing the proposed tasks through efficient interaction (trial and error) with solely the feedback (e.g., whether the metrics are satisfied) from the brain.Existing works leveraging foundation models for training embodied agents either rely on humans to propose tasks (Ha et al., 2023) or synthesize scenes to suit the tasks proposed by LFMs (Wang et al., 2023), which imposes a bottleneck on the learning autonomy and is subject to potential generalization issues.Left: An experienced elder instructs a toddler on playing with colorful blocks (image credit to GPT-4V).Right: Using the proposed brain-body synchronization scheme, foundation models (brain) teach an embodied agent (body) a variety of physical interaction skills.To accomplish this, the brain needs to propose interaction tasks that are compatible with the scene and the body's physical constraints, as well as define measurable success metrics for the suggested tasks.The body then synchronizes with the brain through trial and error, acquiring interaction skills solely based on feedback from the brain.</p>
<p>In contrast, our approach is uniquely positioned to minimize human input in task proposal and scene customization, aiming to enhance autonomous learning ability and generalization in various settings and providing a foundation for future research in more complex environments.</p>
<p>To achieve autonomous learning of embodied agents in unknown scenes, we develop a framework that first comes up with task proposals compatible with the scene and the agent's physical limitations.Specifically, the scene is parsed with a few sensing modules, e.g., SAM (Kirillov et al., 2023) and CLIP (Radford et al., 2021), which extract rich semantic and physical information about the scene components.A scene graph is then constructed holding this information and serving as an easy interface for an LFM to comprehend the environment and propose diverse and achievable interaction tasks.Meanwhile, a task completion metric(s) is instantiated by prompting an LFM to provide feedback on whether a proposed task is accomplished based on the scene graphs (before and after action).These two components form a verifiable definition of a task, i.e., its description and measurement of success, and also render the task learnable by an embodied agent.Consequently, an agent performs brainguided exploration with action primitives for obtaining successful execution trajectories which are further distilled into a language-conditioned policy for enhancing and expanding the agent's physical interaction capabilities.Therefore, the name brain-body synchronization for the fact that an embodied agent can align with the physical understanding of a collection of foundation models via continuous interaction and feedback.</p>
<p>The proposed synchronization diagram helps improve the efficiency of learning interactive tasks and the explainability of the learned policies, e.g., the structural knowledge in foundation models can help reduce the exploration cost while the modular instructions can help understand the behavior of the embodied agents.Moreover, it enhances the generalization capability of embodied agents (learned interactive policies) to new tasks, e.g., by reusing common skills summarized in the language guidance as well as efficiently acquiring novel skills.Furthermore, language-driven human-robot cooperation can be automatically achieved with the minimum involvement of human agents in the training loop.Experiments show that our system can automatically propose diverse and feasible tasks on par with humans and distill a multi-task policy in a continuous manner.The learned policy also shows graceful zero-shot capabilities and can be effectively fine-tuned for novel tasks, evidencing adaptability in diverse physical interactions.Even though our study is mainly performed in a table-top scenario, which is commonly used in the literature, our exploration presents useful insights and a solid foundation for scaling up to more complex environments.</p>
<p>In summary, our contributions are:</p>
<p>• A framework that integrates foundation models with embodied agents for autonomously learning physical interaction tasks in unknown environments, creating an effective brain-body synchronization and a stepping stone towards more complex real-world scenarios.</p>
<p>• A task proposal module for efficient scene comprehension and automatically proposing scene-compatible tasks.Moreover, it establishes metrics for task completion, aiding the embodied agent in skill acquisition with minimum human involvement.</p>
<p>• An extensive validation of the proposed synchronization by continuously learning an effective languageconditioned policy in both zero-shot and few-shot settings, demonstrating reasonable adaptability to novel tasks and configurations.</p>
<p>Figure 2.An overview of the proposed brain-body synchronization.The scene comprehension module constructs and passes a scene graph of the current environment to an LFM (brain).The brain then proposes interaction tasks compatible with the scene and the physical limitations of the body, which acquires the interaction skills via trial and error with solely the feedback from the brain.</p>
<p>Related Work</p>
<p>Embodied AI with Large Foundation Models.The advent of Large Foundation Models (LFMs), including Language Models (LLMs) and Vision-Language Models (VLMs), enables significant advancements in Embodied AI research, making progress across various domains such as manipulation (Zeng et al., 2022;Huang et al., 2022b;Liang et al., 2023;Lin et al., 2023;Driess et al., 2023), navigation (Khandelwal et al., 2022;Shah et al., 2023;Huang et al., 2023a), andlocomotion (Tang et al., 2023;Yu et al., 2023).</p>
<p>In addition, LLMs have been instrumental in transforming complex language instructions into robotic affordance, empowering tailored and flexible task execution (Ahn et al., 2022;Huang et al., 2022a;Song et al., 2023;Wu et al., 2023).Furthermore, advances in code policy generation (Liang et al., 2023;Singh et al., 2023;Huang et al., 2023c) highlight LFMs' adaptability to varied environments, diverse robotic functionalities, and a spectrum of task requirements.Recent studies (Brohan et al., 2023;Huang et al., 2023b;Szot et al., 2023) also reveal LFMs' role in shaping low-level policies for complex visual tasks in different scenarios.Unlike the aforementioned research, our framework leverages the common sense knowledge from LFMs to acquire a rich set of skills in unknown environments.Building on the emergent capabilities of LFMs as tools for embodied learning (Wang et al., 2023;Fang et al., 2022), reward design (Yu et al., 2023;Xie et al., 2023;Ma et al., 2023), and completion inference (Liu et al., 2023;Di Palo et al., 2023), we develop a fully automated pipeline to learn policies with minimum human intervention.</p>
<p>Policy Distillation in Robotic Manipulation.Robot learning has achieved great success in mastering manipulation skills via reinforcement learning (Gu et al., 2017;Rajeswaran et al., 2017;Yarats et al., 2021;Hansen et al., 2022) or imitation learning (Zhan et al., 2020;Lynch &amp; Sermanet, 2021;Jang et al., 2021;Mees et al., 2022).Recent research also leverages VLMs to train language-conditioned multi-task policies (Shridhar et al., 2022a;Jiang et al., 2022;Shridhar et al., 2022b;Goyal et al., 2023;Luo et al., 2023).However, in these works, both the design of tasks and the supervision needed for policy learning require a substantial amount of human effort.LFMs help relieve humans of this labor to some extent.Huy et al. (Ha et al., 2023) scale up the data collection for manipulation skills by leveraging an LLM, then distilling the robot experiences into a visuo-linguistic-motor policy that infers control sequences.However, the diversity of skills is still constrained by the quantity of tasks available, which are defined by human experts.In this paper, we propose an innovative framework that automates diverse skill acquisition for robotic manipulation.Our method uses LFMs as a central brain to guide the robot body, enabling embodied agents to acquire various manipulation tasks efficiently, bypassing the need for human-provided task descriptions, success metrics, or demonstrations.</p>
<p>Brain-Body Synchronization</p>
<p>Our goal is to convert common-sense knowledge in (disembodied) Large Foundation Models into manipulation policies supporting language-based instructions.Specifically, we aim to automate the training process for embodied agents  in various manipulation tasks with minimum human intervention through the help of LFMs.Our proposed framework consists of two components: the brain -foundation models and a perception module, and the body -robot arms that interact with the environment based on textual instructions.</p>
<p>The brain leverages a scene graph to understand the environment, proposes tasks, and determines task completion.</p>
<p>And the body learns a policy for the tasks proposed by the brain.In summary, our pipeline comprises three parts: 1) scene-compatible task proposal, 2) task completion inference, and 3) task-conditioned policy learning.An overview is shown in Fig. 2.</p>
<p>Scene-Compatible Task Proposal</p>
<p>Automatic task proposal and engagement in physical interactions are crucial for embodied agents to acquire diverse manipulation skills and adapt to new environments (Haywood &amp; Getchell, 2021;Haibach-Beach et al., 2023).A task represents a set of reachable states from the current scene configuration, considering the agent's physical limitations (e.g., a wheeled robot can not climb the wall to install a light bulb).Next, we describe how to use an LFM to continuously suggest compatible manipulation tasks within an agent's physical constraints, initiating the skill acquisition process.</p>
<p>SCENE COMPREHENSION WITH ROBUST SENSING</p>
<p>First, it is important to have an accurate perception of scene entities' physical states.Although large models show promising physical reasoning capabilities, directly querying GPT-4V (Yang et al., 2023b) may result in unreasonable tasks due to imprecise and ungrounded understanding (Yang et al., 2023a) (Fig. 3).Thus, we resort to robust sensing modules to extract meaningful scene information and enable the LFM to produce scene-compatible tasks.</p>
<p>We assume two RGBD cameras for sensing: one horizontal view observing the robot and one top-down view for a holistic scene perspective.Using RGBD images, we first employ an object detector to identify objects (Jocher, 2020), outputting 2D bounding boxes and names.Then we prompt SAM (Kirillov et al., 2023) with detected bounding boxes to derive segmentation masks, refining spatial occupancy.This enables extracting point clouds (3D position and bounding boxes) for each object, given camera parameters and depth.</p>
<p>In addition to "what" and "where" information, an LFM should know the objects' states (in text) to propose reasonable tasks.For each object, we predefine a list of semantically meaningful states depending on its category (e.g., a drawer can be {"open ′′ , "closed ′′ }).Please refer to Appendix A for a full list of the considered states.We then apply CLIP encoders (Radford et al., 2021)   the learning.More explicitly, an LLM takes in the scene graph G and outputs a set of task descriptions T (G) = {τ i }, which should comply with the following:</p>
<p>• The tasks are based on the objects in the scene, considering their states and spatial relationships.This ensures that the tasks are grounded and permitted by the scene.• The tasks should also be aware of the agent's physical capabilities in terms of primitive actions.This ensures that the tasks are practically achievable.• With the above, the LLM still needs to propose a wide range of tasks, ensuring diversity in the interactions and skills to be acquired.This is crucial for training a versatile and adaptable embodied agent.</p>
<p>To ensure these properties, we develop a prompting method shown in Fig. 4 (left).The prompt consists of a clearly defined goal (what to output), along with a few (fixed) incontext examples or rules that the LLM could refer to while formulating the response, as well as the output format.The devised prompt guarantees that the tasks proposed are feasible, diverse, and tailored to the specific environment and capabilities of the robot.We perform a study of the diversity of the proposed manipulation tasks in Sec.4.1.</p>
<p>Furthermore, to facilitate task-conditioned policy training, the LLM is asked to propose tasks that can be decomposed into sequences of predefined (factory) actions.This has two benefits.First, it prevents physically ambiguous tasks, like "Organizing the Desk," which could imply numerous goal states.Second, explicit action items using primitives allow easy assessment of task success.Examples of proposed tasks are in Fig. 3b.</p>
<p>Hereto, we introduce an automatic mechanism using robust sensing modules and LLMs to propose scene-compatible, achievable, and definitive manipulation tasks.However, textual tasks need a measure of physical state to determine completion.We address this with the inference module introduced in the next section.</p>
<p>Task Completion Inference</p>
<p>For acquiring interaction skills, the foundation models (brain) must propose diverse manipulation tasks and provide feedback on their completion.Assessing task completion is crucial for training embodied agents (body) to derive action policies.Thus, we utilize GPT-4's reasoning capability to automatically generate success criteria and determine task accomplishment.</p>
<p>Specifically, when an agent performs a sequence of actions, an updated scene graph G ′ is constructed, reflecting object states and relationships.The before-action scene graph G and after-action scene graph G ′ are encapsulated in a prompt, providing GPT-4 with the necessary information to infer task completion status.An example prompt is shown in Fig. 4 (right), applicable to other tasks by changing the task description.</p>
<p>It is worth noting that GPT-4 can generalize beyond incontext examples.For instance, Fig. 4 shows decisions based on direction, but in practice, GPT-4 can also calculate object positions to assess task accomplishment.This ensures that an embodied agent receives meaningful instructions and effective feedback during learning.</p>
<p>Task-Conditioned Policy Learning</p>
<p>Given the proposed tasks and completion metrics, training an embodied agent to efficiently achieve proposed tasks fulfills the synchronization between the foundation models (brain) and the agent (body).One approach is using reinforcement learning (Sutton &amp; Barto, 2018) with LLMgenerated task completion status as the reward.However, this is challenging due to the large exploration space and sparse rewards.Instead, we use robustly executable primitive actions to collect successful task demonstrations and train a task-conditioned policy network via behavior cloning (Pomerleau, 1988).</p>
<p>Task Decomposition with Primitive Actions</p>
<p>To collect demonstrations, we first prompt GPT-4 to decompose generated tasks into step-by-step instructions using primitive actions like "Pick," "PlaceOn," and "Push," which cover most daily physical interaction tasks (Wu et al., 2023).See Appendix A.2 for the full list of primitive actions.The task decomposition prompt is shown in Fig. 4 (middle).</p>
<p>Demonstration Collection</p>
<p>We employ a waypoint sampler and motion planner to execute primitive actions proposed by the LLM.The waypoint sampler takes object point clouds and GPT-4-generated parameters as input to sample potential waypoints, while the motion planner determines robot arm configurations to achieve these waypoints.</p>
<p>Actions are executed sequentially and assessed using the completion inference mechanism.Successful task trajectories are added to the demonstration pool and collected for various tasks and scenes.Note that the demonstration collection mechanism is not the task-conditioned policy to be learned.Its trial-and-error nature leads to less robust task performance, and fixed action primitives without trainable parameters limit learning and adaptation from interaction experiences, as described in the following.</p>
<p>Task-Conditioned Policy With successful trajectories, an embodied agent can continuously train a languageconditioned action policy to acquire diverse manipulation skills.The policy takes scene observations and task descriptions as input, outputting actions that achieve the desired goal state.Distilling physical experiences into a taskconditioned policy is crucial for an evolving automatic skill acquisition framework.</p>
<p>The conditional policy is learned using behavior cloning by training the network proposed in (Ha et al., 2023).The 10-dimensional action space policy network allocates three dimensions for position, six for rotation (represented by elements in the upper two rows of the rotation matrix {r i,j | i ∈ {0, 1}, j ∈ {0, 1, 2}}), and one for the gripper command.By learning a stream of tasks suggested by the scene-compatible proposal module, the embodied agent, equipped with the evolving conditional policy, gradually acquires skills to perform complex interaction tasks.</p>
<p>To this point, we present an autonomous method for training embodied agents to continuously acquire manipulation skills by transforming foundation models' high-level physical understanding into low-level motor commands, i.e., brain-body synchronization.We also note that online reinforcement learning can perform synchronization within a lifelong training framework.However, by reducing exploration overhead using behavior cloning and enabling continuous learning with rehearsal, we establish a minimal yet functionally equivalent pipeline to investigate essential modules' roles.Next, we evaluate the pipeline's effectiveness in acquisition efficiency and policy generalization.</p>
<p>Experiments</p>
<p>We evaluate the effectiveness of our pipeline in a tabletop manipulation environment provided by (Ha et al., 2023).This evaluation is conducted via collecting trajectories in a way that mimics the continuous skill acquisition process and by distilling a multi-task policy based on the proposed tasks.We design the experiments to answer the following questions:</p>
<p>• Can the proposed synchronization pipeline generate diverse tasks consistent with human understanding of manipulation task diversity?• Is our pipeline reliable in task generation and success inference, and how essential is the scene graph in ensuring the quality of these tasks?• How well does the distilled policy (synchronized body) perform compared to the elementary data collection policy (factory setting)?• Can the policy perform effectively and adapt efficiently to unseen tasks as it accumulates interaction experience of learned tasks?</p>
<p>Analysis of Task Proposal Diversity</p>
<p>We first assess our system's capability to generate a contextually relevant array of tasks that are diverse enough by human standards.</p>
<p>Experiment Setting.First, we propose a scoring metric to quantify the similarity between two tasks based on human rankings of a set of critical factors.The rankings are obtained through a survey from a diverse population of subjects (e.g., college students, bankers, doctors, and engineers).Please refer to Appendix B for details about the scoring metric.</p>
<p>We apply this scoring metric on a set of 100 tasks proposed by the task proposer module to obtain a distance matrix containing pairwise distances between tasks.Visualization and Result.We perform Multidimensional Scaling (MDS) on the distance matrix and cluster the tasks by K-Means, as shown in Fig. 5a.MDS is performed again using the GPT text embedding of the task descriptions and we visualize the previously obtained clusters in Fig. 5c.The overlapping clusters highlight that the GPT text embedding may not represent the diversity of tasks well regarding human cognition.A snapshot of tasks in each cluster is also shown in Fig. 5b.This study shows that the tasks proposed by our pipeline are potentially more diverse than those tasks selected according to the text-embedding-based diversity.</p>
<p>Analysis of Task Proposal Feasibility</p>
<p>We then conduct an evaluation of the feasibility of proposed tasks from the task proposer.We consider the baselines of different task proposal methods: (1) BBSEA queries GPT-4 with full information of the scene graph.</p>
<p>(2) BBSEA w/o bbox queries GPT-4 without bounding boxes of the objects in the scene graph.</p>
<p>(3) BBSEA w/o bbox&amp;positions queries GPT-4 without bounding boxes and positions of the objects in the scene graph.(4) GPT-4V queries GPT-4V with front view image, without any information of the scene graph.</p>
<p>(5) GPT-4V-SG queries GPT-4V with front view image, and is prompted to formulate its own scene graph for task proposal.See Appendix F for the full list of prompts.The evaluation is conducted across five randomly chosen scenes (the environments are visualized in Appendix C).For each baseline, we query GPT-4/4V in multiple trials and assess the feasibility based on the proportion of the tasks that can be used to collect successful demos for policy training.</p>
<p>Results in Tab. 1 reveal that tasks generated by our pipeline demonstrate significantly higher feasibility compared to those proposed by GPT-4V/4V-SG, which matches the case shown in Fig. 3.It shows the imprecise and ungrounded understanding of GPT-4V and the need for our robust sensing module.Also, the ablations of scene graph (w/o bbox and w/o bbox&amp;positions) are worse than the original pipeline, which shows that the usage of scene graph is essential in ensuring the quality of the proposed tasks.</p>
<p>Analysis of Success Inference Accuracy</p>
<p>We further investigate the accuracy of our task completion inference module.The baseline methods and evaluation scenes from the previous section 4.2 are retained.This analysis focuses on the ability of these methods to accu- rately infer whether a proposed task has been successfully completed, which is crucial for effective demonstration collection and policy training in our pipeline.</p>
<p>The results in Tab. 2 reveal that our BBSEA framework, which leverages complete scene graph information, is more accurate than other approaches when inferring task success.The consistent high scores observed for the BBSEA framework across various scenes demonstrates the importance of detailed spatial data, including bounding boxes and object positions.In contrast, GPT-4V and GPT-4V-SG which rely solely on visual inputs, show lower accuracy.The findings underscore the vital role of comprehensive scene graph information for precise success inference.</p>
<p>Evaluation of Distilled Policies</p>
<p>We evaluate the performance of the policies learned with BBSEA by comparing it against the policy we used to collect data.This assessment aims to demonstrate the necessity and effectiveness of the policy distillation.</p>
<p>Experiment Setting.We train a multi-task policy on eight tasks with 200 demonstrations per-task.The implementation details including environment and training settings are shown in Appendix C. The performance of distilled policy is then evaluated on the eight tasks, and compared with the original data collection policy.</p>
<p>Main Results.The results in Tab. 3 reveal an improvement by 25% in average success rate of the distilled policy over the data collection policy, affirming the effectiveness of our methodology in skill acquisition and policy evolution.</p>
<p>Zero-shot Capability Assessment</p>
<p>We then assess the zero-shot capability of our pipeline by evaluating tasks not seen during training.Following the training procedure in Sec.4.4, we evaluate the policies trained on 10, 30, and 60 proposed tasks on four unseen tasks respectively.The task details are shown in Appendix D.</p>
<p>We also visualize the MDS diversity of these tasks in Fig. 9, suggesting that a larger task set contributes to a broader range of task variations.The results of our assessment are presented in Tab. 4, which indicate that policies trained on a larger variety of tasks can exhibit better zero-shot generalization.This suggests that the breadth of training directly influences the system's ability to adeptly handle tasks it has not encountered before, highlighting the significance of scaling diverse skills using our automatic pipeline.</p>
<p>Adaptation Performance on Novel Tasks</p>
<p>We further evaluate the adaptation performance of our derived policies across four unseen tasks: "gather", "bin", "bus" and "drawer".We choose the distilled policies (trained on 10, 30, and 60 tasks) from Sec. 4.5 as pre-trained policies.</p>
<p>We then fine-tune the policies across the four unseen tasks with varying demonstrations per task.The outcome in Tab. 5 demonstrates that policies trained with a larger task variety exhibit better performance after fine-tuning.Also, distilled policies trained on 60 tasks show the highest success rate compared to others.Despite small variations induced by the training dynamics, our experiments confirm the importance of acquiring physical skills across diverse tasks.This emphasizes the necessity of autonomously proposing and learning interaction policies as in the proposed BBSEA pipeline.</p>
<p>Conclusion</p>
<p>We present a brain-body synchronization pipeline to enable embodied agents to autonomously acquire various interaction skills without human intervention.Leveraging large foundation models, our pipeline proposes tasks for learning and establishes success metrics for providing learning feedback.Our experiments in the widely adopted tabletop settings demonstrate that with a structured representation of scene information and a small set of in-context examples, a collection of large foundation models (representing the brain) can effectively suggest scene-compatible and physically feasible tasks for an agent (body).</p>
<p>This approach facilitates autonomous skill acquisition, enabling the training of language-conditioned policies that exhibit promising zero-shot and few-shot generalization capabilities on novel tasks and environments.</p>
<p>While our experiments provide valuable insights, they are primarily confined to tabletop environments and utilize a limited set of action primitives.These are the current limitations of our approach for dealing with complicated realworld scenarios.In future work, we aim to extend our pipeline beyond the boundaries, exploring more intricate and realistic environments and integrating a wider array of action primitives.This extension is essential for fully realizing the potential of our methods and for advancing the application of autonomous skill acquisition in more diverse and complex scenarios.</p>
<p>Acknowledgment</p>
<p>Appendix A. Implementation Details</p>
<p>In this section, we describe the implementation details of some components of our pipeline.Please visit https://bbseaembodied-ai.github.iofor more visualizations.</p>
<p>A.1. Scene Comprehension</p>
<p>Object Detection and Segmentation In our pipeline, the process of scene comprehension starts with detecting the objects in the images obtained via RGBD cameras.The object detector (Jocher, 2020)</p>
<p>A.2. Primitive Actions</p>
<p>The primitive actions consist of a waypoint sampler and a motion planner.The waypoint sampler samples potential waypoints that indicate gripper poses and open/closed states.The motion planner figures out configurations of the robot arm that could achieve the sampled waypoints.</p>
<p>Currently, we compose a feasible set of actions for the robot arm using seven primitive actions, including "Pick(obj name)", "PlaceOn(obj name)", "PlaceAt(place pos", "Push(obj name, direction, distance)", "PrismaticJointOpen(obj name)", "PrismaticJoint-Close(obj name)" and "Press(obj name)".These primitive actions are sufficient to accomplish a wide range of tasks.Further expanding the set of primitive actions will be considered for future work.</p>
<p>Listing 2 illustrates the functionality and parameters of the primitive actions, with specific implementation details referenced in the code of our pipeline, which we are committed to releasing.would be moved to) Push(obj_name, direction, distance): Close the gripper and then push the object in the specified direction by a specified distance.(Parameters: obj_name --the name of the object which would be pushed; direction --the direction which the object would be moved in, it is direction vector [x, y]; distance -the distance which the object would be moved by, the distance is in the unit of meter) PrismaticJointOpen(obj_name): Open the object with a prismatic joint.( Parameters: obj_name --the name of the handle of the object with a prismatic joint) PrismaticJointClose(obj_name): Close the object with a prismatic joint.( Parameters: obj_name --the name of the handle of the object with a prismatic joint) Press(obj_name): Close the gripper and then press the object.(Parameters: obj_name --the name of the object which should be pressed)</p>
<p>A.3. Language Conditioned Multi-Task Policy</p>
<p>We utilize the official code of visuomotor policy in (Ha et al., 2023) which is available at github.com/realstanford/scalingup as the implementation of the language conditioned multi-task policy.Further details can be found there.</p>
<p>B. Task Diversity Analysis</p>
<p>A questionnaire is designed by identifying a set of 5 factors which would impact how we quantify the difference between two tasks.These factors are identified as follows:</p>
<p>1.The main action involved in the tasks.</p>
<p>The object on which the action is implemented in the tasks</p>
<ol>
<li>The shape of the location mentioned in the tasks 4. The color of the object of focus in the tasks</li>
</ol>
<p>The color of the target location in the tasks</p>
<p>The human perception of the importance of factors is subjective but a survey can provide a starting point regarding the ranking of these factors.To identify the rankings, we asked a group of people from various backgrounds a set of questions where each question consisted of two pairs of tasks.For example, Set A consists of tasks which differ in the main actions.Set B consists of tasks which differ by only one factor -the object used.Similarly Set C has tasks with different target locations, Set D has tasks with different colors of the objects and Set E has tasks where the color of the target is different.</p>
<p>Questionnaire Sample:</p>
<ol>
<li>Given the two sets of tasks, which set do you think has a larger distance/difference between them ?Set A Push the green bin close to the drawer Pick up and place the green bin close to the drawer.</li>
</ol>
<p>Set B</p>
<p>Push the green bin close to the drawer Push the green bowl close to the drawer 2. Given the two sets of tasks, which set do you think has a larger distance/difference between them ?Set A Push all the red blocks in a triangle shape.Pick up and place all the red blocks in a triangle shape.</p>
<p>Set C</p>
<p>Pick up and place all the red blocks in a triangle shape.Pick up and place all the red blocks in a circular shape.</p>
<ol>
<li>Given the two sets of tasks, which set do you think has a larger distance/difference between them ?Set A Pick up and place the red block next to the green bowl.Push the red block next to the green bowl.</li>
</ol>
<p>Set D Push the red block next to the green bowl.Push the blue block next to the green bowl.</p>
<ol>
<li>Given the two sets of tasks, which set do you think has a larger distance/difference between them ?Set A Pick up and place all the green blocks in the green square.</li>
</ol>
<p>Push all the green blocks in the green square.</p>
<p>Set E Push all the green blocks in the green square.Push all the green blocks in the yellow square.Pick and place the green block into the green bin.</p>
<p>Given</p>
<ol>
<li>Given the two sets of tasks, which set do you think has a larger distance/difference between them ?Set B Push the block towards the green drawer.Push the bowl towards the green drawer.</li>
</ol>
<p>Set E Push the block towards the green drawer.Push the block towards the blue drawer.</p>
<ol>
<li>Given the two sets of tasks, which set do you think has a larger distance/difference between them ?</li>
</ol>
<p>Set C</p>
<p>Pick and place all the red blocks in a bowl.</p>
<p>Pick and place all the red blocks in a square shape.</p>
<p>Set D</p>
<p>Pick and place all the red blocks in a bowl.</p>
<p>Pick and place all the green blocks in a bowl.9. Given the two sets of tasks, which set do you think has a larger distance/difference between them ?Set C Move all the green bowls to the cupboard Move all the green bowls to the green square.</p>
<p>Set E Move all the green bowls to the green square.Move all the green bowls to the blue square.</p>
<ol>
<li>Given the two sets of tasks, which set do you think has a larger distance/difference between them ?Set D Swap the positions of the red block and the green block Swap the positions of the blue block and the green block.</li>
</ol>
<p>Set E</p>
<p>Swap the positions of the red block and the green block Swap the positions of the red block and the blue block.</p>
<p>Each set corresponds to a specific factor and all the factors are compared against each other.We count the number of responses in favour of each set (Tab. 6) and rank them.Scoring System for Distance Matrix: The distance between tasks numbered i and j is defined as follows:
distance[i, j] = 5 k=1 score k (i, j)
where:</p>
<p>• score 1 depends on the disparity in the main action of the tasks.A score of 5 is assigned if the actions are different, and 0 if they are the same.• score 2 depends on the disparity in the shape of the target locations in the tasks.A score of 4 is assigned if the shapes are different, and 0 otherwise.• Similarly, score 3 depends on the disparity in the shape of the objects in the tasks.A score of 3 is assigned if the shapes are different.• score 4 depends on the disparity in the color of the objects in the tasks.A score of 2 is assigned if not the same.• score 5 : depends on the disparity in the color of the target in the tasks.A score of 1 is assigned if they are different.</p>
<p>A higher score between tasks numbered i and j implies that the tasks are more different from each other as compared to the case when the score between two tasks is lower.We refer to this score as the distance between the tasks.</p>
<p>Visualization: To obtain the multidimensional scaling (MDS) plot , we use python's sklearn library.After obtaining pairwise distances for a set of tasks, the symmetric matrix obtained then undergoes MDS so that we can obtain a two dimensional plot to visualize the distance between the tasks and proceed to cluster them.The clusters obtained along with the tasks are shown in Fig. 10.</p>
<p>C. Environment and Training Details</p>
<p>We train multi-task visuomotor policies proposed by (Ha et al., 2023)</p>
<p>D. Task Details</p>
<p>The policies are trained on 10, 30, and 60 tasks.Below are these tasks which are all proposed by GPT-4 after understanding the scene.The visualization of the trajectories collected on these tasks is available on our website https://bbsea-embodied-ai.github.io.For comparison purposes, tasks from previous literature (Ha et al., 2023) is also mentioned.In addition to the task details, an analysis of the diversity present in the 4 sets of tasks used for policy training is also presented in this section.</p>
<p>The 10 tasks:</p>
<p>• move the green block into the bowl • move the red block away from the bowl Evaluation of policies is done on a set of 10, 30 and 60 proposed tasks as mentioned in Appendix C and Appendix D.</p>
<p>To compare the diversity in the tasks on which policies are trained on, we collect our 60 tasks along with the tasks obtained from previous work (Ha et al., 2023) and perform similar analysis as mentioned in Sec.4.1 where our scoring system ,as shown in Appendix B ,is used to obtain pairwise distances.We then perform MDS to obtain a two dimensional plot with the tasks plotted as points.The total area spanned by our tasks on the MDS plot covers a larger area as compared to the tasks which were used in previous work.(Ha et al., 2023) as shown in Fig. 8.The MDS plot obtained is a result of the distance matrix from a pairwise distance comparison between all the tasks.A larger distance implies that the tasks are more different from each other in comparison to the tasks which have a smaller distance between them.Since the area spanned by the tasks mentioned in BBSEA covers a larger area, we can conclude that the distances between the tasks are larger and hence they are more diverse.Furthermore, we performed the same experiment but this time, the 10 tasks used for policy learning in BBSEA and the 30 tasks used for policy learning were also included, and the areas covered by the sets of tasks are highlighted as shown in Fig. 9 which shows that the 60 tasks used are more diverse than the cases where 10 and 30 tasks are used in the BBSEA framework.</p>
<p>E. Analysis of the Task Completion Inference Module</p>
<p>Given a task, a sequence of actions are performed by the agent which in turn constructs the final scene graph that reflects the up-to-date states of the objects in the scene and their relationships.The reasoning capability of the GPT-4 is leveraged to determine the completion of the task after a scene graph is incorporated into a prompt.To measure the performance of the task completion inference module , the trajectories are collected and frames of the trajectories are analyzed through human supervision to check whether the task has been completed or not, and whether it matches the result obtained from the task completion module.In Tab. 7 we observe the false positives obtained when the task completion module labels the task as successfully completed but the trajectory is incomplete or wrong.For our experiment , we select 20 tasks and collect their trajectories after which we manually infer the accuracy of the task completion inference module for the trajectories which have been marked successful.From Tab. 7, it is observed that we get approximately 16 errors in 100 successful trajectories collected.In addition, we also note that for the task where the primitive action is 'Push', the performance of our task completion inference module will be poor along with cases of long horizon tasks which include color and spatial awareness.We also choose another set of tasks and record both the trajectories labelled as successful by the inference module as well as the trajectories labelled as unsuccessful.</p>
<p>A confusion matrix highlighting the true positives, false positives, false negatives and true negatives based on the result provided by the task completion inference module is shown in Fig. 7 Figure 7. Confusion matrix highlighting the capabilities of our task completion inference module.We observe a true positive rate of 81.82 % and a true negative rate rate of 84.69 %.</p>
<p>F. Prompts for Large Language Model</p>
<p>The complete prompts consist of base prompts, which include instructions along with a few shots of examples, as well as appended input data.Listing 3 illustrates the construction of prompts for task proposal, task decomposition and success inference in the form of Python code.We also present the base prompts for task proposal (Listing 4), task decomposition (Listing 5) and success inference (Listing 6).</p>
<p>A small number of examples are provided solely to help the LLM understand its task and the desired output format.(Ha et al., 2023) (depicted in red).Additionally, we showcase the areas spanned by 10 tasks (in orange) and 30 tasks (in blue) used in BBSEA for policy training.The expanded area covered by the BBSEA clusters demonstrates that our tasks are more widely dispersed and exhibit a higher level of diversity, as per our developed scoring system.Furthermore, it is observed that the area covered by the 60 tasks significantly surpasses the areas covered by the 30 and 10 tasks, indicating a greater degree of diversity.</p>
<p>Figure 10.An overview of the MDS plot obtained with all 100 tasks from the task proposal module which are clustered using K-Means Clustering.Please enlarge to see details of the proposed tasks.</p>
<p>BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents Listing 4. Base prompts for task proposal.Instructions and output format are provided in the base prompts for task proposal.</p>
<p>You are a curious baby.Given a scene graph, the goal is to propose as many diverse tasks as possible for a robot arm with a gripper.The nodes in the scene graph indicate the name, the state, the position and the bounding box (in the unit of meter) of an object.The positive direction of the x-axis represents the front, and the negative direction represents the rear.The positive direction of the y-axis represents the right side, and the negative direction represents the left side.The positive direction of the zaxis represents upward, and the negative direction represents downward.The edges indicate the spatial relationships between the objects, and no edges between two objects (nodes) means the two objects are far apart.The position of the robot is (0.0, 0.0, 0.0).Note: (1) The proposed tasks should be as diverse as possible; (2) It is necessary to consider the objects present in the scene, their state, attributes, position and spatial relationships;</p>
<p>(3) The proposed tasks are unrelated to each other; (4) One type of object may have multiple instances, nodes use numbers to distinguish them.However, when proposing a task, assume that you don't know the numerical labels corresponding to the instances and you only know the quantity of instances.More importantly, you should consider the primitive actions the robot arm could take: Pick(obj_name), PlaceOn(obj_name), PlaceAt(place_pos), Push(obj_name, direction, distance)</p>
<p>, RevoluteJointOpen(obj_name), RevoluteJointClose(obj_name), PrismaticJointOpen( obj_name), PrismaticJointClose(obj_name), Press(obj_name), and then propose feasible tasks.Here I will show you the implementation of the primitive actions for your reference.Pick(obj_name): Approach the object, close the gripper to grasp it and lift it up ( Parameters: obj_name --the name of the object which would be picked).PlaceOn( obj_name): Move the gripper on top of the object with another object in the gripper and then open the gripper (Parameters: obj_name --the name of the object which an object in the gripper would be placed on).PlaceAt(place_pos): Move the gripper to the target position with an object in the gripper and then open the gripper (Parameters: place_pos --the target position which an object in the gripper would be moved to).Push(obj_name, direction, distance): Close the gripper and then push the object in the specified direction by a specified distance (Parameters: obj_name --the name of the object which would be pushed; direction --the direction which the object would be moved in, it is direction vector [x, y]; distance --the distance which the object would be moved by, the distance is in the unit of meter).PrismaticJointOpen(obj_name):</p>
<p>Open the object with a prismatic joint (Parameters: obj_name --the name of the handle of the object with a prismatic joint).PrismaticJointClose(obj_name): Close the object with a prismatic joint (Parameters: obj_name --the name of the handle of the object with a prismatic joint).Press(obj_name): Close the gripper and then press the object (Parameters: obj_name --the name of the object which should be pressed).Below is an example to show the format: ''' scene graph:</p>
<p>[Nodes]:</p>
<ul>
<li>object_B 1 -&gt; relationship -&gt; object_A tasks: -task description 1 -task description 2 -... ''' Now you should read the scene graph I provide with you at first, and then think about diverse and feasible tasks.Think step by step, and imagine the process to accomplish the task with the primitive actions provided with you: Pick(obj_name), PlaceOn( obj_name), PlaceAt(place_pos), Push(obj_name, direction, distance), RevoluteJointOpen( obj_name), RevoluteJointClose(obj_name), PrismaticJointOpen(obj_name), PrismaticJointClose(obj_name), Press(obj_name).REMEMBER, YOU MUST make sure that you don't know the numerical labels corresponding to the instances, you only know the quantity of instances; DO NOT use primitive actions explicitly in the task description.</li>
</ul>
<p>Listing 5. Base prompts for task decomposition.The base prompts include instructions and a small number of examples which are provided solely to help the LLM understand its task and clarify the desired output format.You are a robot with a single arm in a tabletop robot manipulation environment.Given a task description and a scene graph, the goal is to decompose the task into subtasks and to call corresponding primitive actions, which, when performed in sequence, would solve the input task.The nodes in the scene graph indicate the name, the state, the position and the bounding box (in the unit of meter) of an object.The positive direction of the x-axis represents the front, and the negative direction represents the rear.The positive direction of the y-axis represents the right side, and the negative direction represents the left side.The positive direction of the zaxis represents upward, and the negative direction represents downward.One type of object may have multiple instances, nodes use numbers to distinguish them.The edges indicate the spatial relationships between the objects, and no edges between two objects (nodes) means the two objects are far apart.The position of the robot is (0.0, 0.0, 0.0).You should consider the primitive actions the robot arm could take: Pick(obj_name), PlaceOn(obj_name), PlaceAt(place_pos), Push(obj_name, direction, distance)</p>
<p>, RevoluteJointOpen(obj_name), RevoluteJointClose(obj_name), PrismaticJointOpen( obj_name), PrismaticJointClose(obj_name), Press(obj_name), and decompose a task into feasible sub-tasks.Here I will show you the implementation of the primitive actions for your reference.Pick(obj_name): Approach the object, close the gripper to grasp it and lift it up ( Parameters: obj_name --the name of the object which would be picked).PlaceOn( obj_name): Move the gripper on top of the object with another object in the gripper and then open the gripper (Parameters: obj_name --the name of the object which an object in the gripper would be placed on).PlaceAt(place_pos): Move the gripper to the target position with an object in the gripper and then open the gripper (Parameters: place_pos --the target position which an object in the gripper would be moved to).Push(obj_name, direction, distance): Close the gripper and then push the object in the specified direction by a specified distance (Parameters: obj_name --the name of the object which would be pushed; direction --the direction which the object would be moved in, it is direction vector [x, y]; distance --the distance which the object would be moved by, the distance is in the unit of meter).PrismaticJointOpen(obj_name):</p>
<p>Open the object with a prismatic joint (Parameters: obj_name --the name of the handle of the object with a prismatic joint).PrismaticJointClose(obj_name): Close the object with a prismatic joint (Parameters: obj_name --the name of the handle of the object with a prismatic joint).Press(obj_name): Close the gripper and then press the object (Parameters: obj_name --the name of the object which should be pressed).Below are some examples: ''' task description: move the red block onto the plate, the blue block onto the red block, and the green block on the blue block scene graph:</p>
<p>[Nodes]:</p>
<p>-red block --position: [0.Listing 6. Base prompts for success inference.The base prompts include instructions and a limited number of examples intended solely to enable the large language model to understand its task and output format and to engage in thorough reasoning before providing answers.</p>
<p>You are a robot with a single arm in a tabletop robot manipulation environment.Given a task description and a list of scene graphs, the goal is to infer if the task has been completed successfully.The list of scene graphs are arranged in chronological order (the first is the initial scene graph, and the last is the scene graph after the policy is executed).The nodes in the scene graph indicate the name, the state, the position and the bounding box (in the unit of meter) of an object.The positive direction of the x-axis represents the front, and the negative direction represents the rear.The positive direction of the y-axis represents the right side, and the negative direction represents the left side.The positive direction of the z-axis represents upward, and the negative direction represents downward.One type of object may have multiple instances, nodes use numbers to distinguish them.The edges indicate the spatial relationships between the objects, and no edges between two objects ( nodes) means the two objects are far apart.The position of the robot is (0.0, 0.0, 0.0).Note that you should firstly reason whether the task is completed based on the task description and scene graphs, then output the answer.The answer should be "yes" or " no" or "not sure".Below are some examples: ''' task description: move the blue block on the plate scene graph list:</p>
<p>---------- -red block -&gt; on top of -&gt; blue block success metric: The blue block is on the plate reasoning: The scene graph indicates the blue block is not on the plate.So the task was not accomplished.answer: no ''' ''' task description: stack the red block and the blue block on the plate scene graph list:</p>
<p>---------- -blue block -&gt; on top of -&gt; plate -red block -&gt; on top of -&gt; blue block success metric: The blue block is on top of the plate and the red block is on top of the blue block.reasoning: The scene graph indicates the blue block is on top of the plate and the red block is on top of the blue block.So "stack the red block and the blue block on the plate" has been accomplished.answer: yes ''' ''' task description: move the red block to the left of the plate scene graph list:</p>
<hr />
<p>G. Additional visualizations</p>
<p>We also present additional images from our project which shows the robotic arm in the tabletop environment performing various different tasks.The images are from the trajectories of the 60 tasks on which the policy is trained on.In addition, there are also images from the zero-shot generalization and few-shot generalization experiments.The full trajectories (including the animation) can also be found on the project website https://bbsea-embodied-ai.github.io.</p>
<p>Figure 1 .
1
Figure1.Left: An experienced elder instructs a toddler on playing with colorful blocks (image credit to GPT-4V).Right: Using the proposed brain-body synchronization scheme, foundation models (brain) teach an embodied agent (body) a variety of physical interaction skills.To accomplish this, the brain needs to propose interaction tasks that are compatible with the scene and the body's physical constraints, as well as define measurable success metrics for the suggested tasks.The body then synchronizes with the brain through trial and error, acquiring interaction skills solely based on feedback from the brain.</p>
<p>(a) Tasks proposed directly with GPT-4V.(b)Tasks proposed with our method.</p>
<p>Figure 3 .
3
Figure 3.Comparison between proposed tasks through GPT-4V (GPT4+Vision) and the task proposer in our pipeline.Ours can propose more context-relevant and feasible tasks for the agent to learn, leveraging easily digestible scene information in the graph.</p>
<p>Figure 4 .
4
Figure 4.An overview of the prompts used for the Task Proposer (left), Task Decomposer (middle), and the Success Inference (right) modules.These prompts ensure effective collection and completion inference of diverse and feasible tasks.Please note that all the prompts are fixed without tailoring to a specific task.</p>
<p>(a) Clusters by MDS using the task distance matrix with the rankings from our survey.(b) Exemplar tasks from corresponding clusters in (a).Enlarge for details.(c) Same clusters on MDS plot of text embeddings of the proposed tasks.</p>
<p>Figure 5 .
5
Figure5.Multidimensional Scaling (MDS) is performed on the (human-endorsed) distance matrix to obtain a 2D plot of the tasks, which are further clustered by K-Means (left).From the clusters, a few tasks are chosen and highlighted (middle).MDS is performed again but with the text embeddings of the tasks, however, the clusters from Fig.5aare now overlapped with each other, evidencing that text-embeddings may not characterize the task space compatible with human understanding.</p>
<p>Listing 2 .
2
Explanation of the primitive actions.Pick(obj_name): Approach the object, close the gripper to grasp it and lift it up.(Parameters: obj_name --the name of the object which would be picked) PlaceOn(obj_name): Move the gripper on top of the object with another object in the gripper and then open the gripper.(Parameters: obj_name --the name of the object which an object in the gripper would be placed on) PlaceAt(place_pos): Move the gripper to the target position with an object in the gripper and then open the gripper.( Parameters: place_pos --the target position which an object in the gripper</p>
<p>Figure 6 .
6
Figure 6.Visualization of the environments in which brain-body synchronization is performed on.</p>
<p>on the data collected by the brain-body synchronization in nine environments.A visualization of the environments is provided in Fig.6.We employ identical network architecture and hyperparameters as the official implementation during training time.The policies are trained with varying task quantities (10, 30, 60), and 200 demonstrations are collected for each task.The corresponding tasks are illustrated in Appendix D. During finetuning in Sec.4.6, we set the epoch to one and the batch size to 256 for rapid adaptation on a small amount of data.The tasks which policies are trained on and generalize to are visualized on our website https://bbsea-embodied-ai.github.io.</p>
<p>•</p>
<p>move the green block to the left side of the red block • push the bowl towards the rear • push the red block towards the right • push the green block towards the left • push the red block towards the front • place the red block next to the stick • gather the stick and the red block together near the green bin • align the red block and the stick along the x-axis.The 30 tasks: • move the green block into the bowl • move the red block away from the bowl • move the green block to the left side of the red block • push the bowl towards the rear • push the red block towards the right • push the green block towards the left • push the red block towards the front • place the red block next to the stick • gather the stick and the red block together near the green bin • move the blue block and the red block on opposite ends of the blue line • place the blue block and the red block side by side on the blue line • place the blocks on both sides of the blue line • move the blue block and the red block to the middle of the blue line and the red line respectively • move all the blue blocks to the front and all the red blocks to the back • move the turbo airplane toy to the right side of the red block • gather the turbo airplane toy and the red block near the drawer • move the turbo airplane toy to a position that is closer to the drawer than the red block • pick up the red block • swap the positions of the red block and the turbo airplane toy • sort the objects by moving the red block and the stick into the drawer and the green bin respectively • move the green block into the open drawer • press the catapult button to trigger the catapult • close the drawer • launch the green block using the catapult • align the red block and the turbo airplane toy along the y-axis • move the turbo airplane toy to the front and the red block to the back • arrange the red block and the turbo airplane toy side by side • place the green block on the catapult • move the green block next to the catapult Tasks from previous work (Scaling Up and Distilling Down): • Move the package into the mailbox and raise the flag • Move the toy from the right bin to the left bin • Move the vitamin bottle into the top drawer • Move the vitamin bottle into the middle drawer • Move the vitamin bottle into the bottom drawer • Move the pencil case into the top drawer • Move the pencil case into the middle drawer • Move the pencil case into the bottom drawer • Move the crayon box into the top drawer • Move the crayon box into the middle drawer • Move the crayon box into the bottom drawer • Move the horse toy into the top drawer • Move the horse toy into the middle drawer • Move the horse toy into the bottom drawer • Move the block into the catapult arm, press the button to shoot the block into the closest bin • Move the block into the catapult arm, press the button to shoot the block into the middle bin • Move the block into the catapult arm, press the button to shoot the block into the furthest bin • Balance the bus toy on the block D.1.Task Diversity for Policy Learning</p>
<p>Figure 8 .
8
Figure 8.An overview of the MDS plot, which showcases the area covered by the tasks used in BBSEA for policy training (highlighted in green) compared to the tasks previously utilized for policy training in the literature (Ha et al., 2023) (marked in red).The broader expanse of the green cluster indicates that our tasks are more spread out and exhibit greater diversity, according to the our scoring system.</p>
<p>Figure 9 .
9
Figure9.An overview of the MDS plot that highlights the area covered by the 60 tasks utilized in BBSEA for policy training (illustrated in green) in comparison to the tasks previously used for policy training in existing literature(Ha et al., 2023) (depicted in red).Additionally, we showcase the areas spanned by 10 tasks (in orange) and 30 tasks (in blue) used in BBSEA for policy training.The expanded area covered by the BBSEA clusters demonstrates that our tasks are more widely dispersed and exhibit a higher level of diversity, as per our developed scoring system.Furthermore, it is observed that the area covered by the 60 tasks significantly surpasses the areas covered by the 30 and 10 tasks, indicating a greater degree of diversity.</p>
<p>object_A (state) --position: (x, y, z), x_range: [min, max], y_range: [min, max], z_range: [min, max] -object_B (state) --position: (x, y, z), x_range: [min, max], y_range: [min, max], z_range: [min, max] -object_B 1 (state) --position: (x, y, z), x_range: [min, max], y_range: [min, max], z_range: [min, max] -... [Edges]:</p>
<p>[Nodes]: -red block --position: [0.40, -0.20, 0.08], x_range: [0.37, 0.43], y_range: [-0.23, -0.17], z_range: [0.05, 0.11] -blue block --position: [0.30, -0.20, 0.08], x_range: [0.27, 0.33], y_range: [-0.23, -0.17], z_range: [0.05, 0.12] -plate --position: [0.52, -0.21, 0.09], x_range: [0.44, 0.60], y_range: [-0.30, -0.13], z_range: [0.07, 0.11] [Edges]: ----------[Nodes]: -red block --position: [0.48, -0.20, 0.15], x_range: [0.45, 0.51], y_range: [-0.23, -0.17], z_range: [0.13, 0.18] -blue block --position: [0.30, -0.20, 0.08], x_range: [0.27, 0.33], y_range: [-0.23, -0.17], z_range: [0.05, 0.12] -plate --position: [0.52, -0.21, 0.09], x_range: [0.44, 0.60], y_range: [-0.30, -0.13], z_range: [0.07, 0.11] [Edges]:</p>
<p>[Nodes]: -red block --position: [0.40, -0.29, 0.08], x_range: [0.37, 0.43], y_range: [-0.32, -0.26], z_range: [0.05, 0.11] -blue block --position: [0.32, -0.20, 0.08], x_range: [0.29, 0.35], y_range: [-0.23, -0.17], z_range: [0.05, 0.11] -plate --position: [0.52, -0.21, 0.09], x_range: [0.44, 0.60], y_range: [-0.30, -0.13], z_range: [0.07, 0.11] [Edges]: ----------[Nodes]: -red block --position: [0.40, -0.29, 0.08], x_range: [0.37, 0.43], y_range: [-0.32, -0.26], z_range: [0.05, 0.11] -blue block --position: [0.48, -0.20, 0.15], x_range: [0.45, 0.51], y_range: [-0.23, -0.17], z_range: [0.12, 0.18] -plate --position: [0.52, -0.21, 0.09], x_range: [0.44, 0.60], y_range: [-0.30, -0.13], z_range: [0.07, 0.11] [Edges]: -blue block -&gt; on top of -&gt; plate ----------[Nodes]: -red block --position: [0.48, -0.20, 0.21], x_range: [0.45, 0.51], y_range: [-0.23, -0.17], z_range: [0.18, 0.24] -blue block --position: [0.48, -0.20, 0.15], x_range: [0.45, 0.51], y_range: [-0.23, -0.17], z_range: [0.12, 0.18] -plate --position: [0.52, -0.21, 0.09], x_range: [0.44, 0.60], y_range: [-0.30, -0.13], z_range: [0.07, 0.11] [Edges]:</p>
<p>[Nodes]: -red block --position: [0.40, -0.20, 0.08], x_range: [0.37, 0.43], y_range: [-0.23, -0.17], z_range: [0.05, 0.11] -green block --position: [0.05, -0.26, 0.08], x_range: [0.02, 0.08], y_range: [-0.29, -0.23], z_range: [0.05, 0.11]</p>
<p>Table 1 .
1
Feasibility rate (%) of the tasks proposed by different task proposal methods.
BaselinesScene 0Scene 1Scene 2Scene 3Scene 4BBSEA100.00100.00100.00100.0070.00BBSEA w/o bbox100.0060.00100.0070.0086.67BBSEA w/o bbox100.0070.00100.0060.0080.00&amp;positionsGPT-4V50.0062.5080.0060.0030.00GPT-4V-SG60.0060.0080.0070.0030.00</p>
<p>Table 2 .
2
Accuracy (%) of the tasks completion inference by different success detection methods.
BaselinesScene 0Scene 1Scene 2Scene 3Scene 4BBSEA100.00100.00100.0083.34100BBSEA w/o bbox83.34100.0066.6683.34100BBSEA w/o bbox66.6750.0050.0050.0050.00&amp; positionsGPT-4V50.0033.3350.000.0033.33GPT-4V-SG50.0050.0050.000.0033.33</p>
<p>Table 3 .
3
Success rate (%) of the data collection policy and the distilled policy across eight tasks.
TasksData CollectionDistilledPolicyPolicyPush the bowl towards the rear40.00100.00Close the drawer92.50100.00Align the red block with the stick32.5050.00Move the green block into the open drawer17.5075.00Push the red block towards the drawer42.5065.00Push the red block towards the right55.0062.50Launch the green block using the catapult32.5052.50Move the red block to the left of the stick35.0045.00Average43.4468.75</p>
<p>Table 4 .
4
Success rate (%) of distilled policies trained with 10, 30, and 60 tasks on four unseen tasks (zero-shot).
Tasks10-task30-task60-taskMove the green block into the0.005.7134.28green binGather the red block and the green block0.0030.0052.50into the green binPush the block towards rear0.0027.5050.00Move the turbo airplane toy towards the line0.000.0057.50Average0.0015.8048.57</p>
<p>Table 5 .
5
Fine-tuning performance in success rate (%) on various unseen tasks.The policies are pre-trained with varying number of tasks (0, 10, 30 and 60 tasks), and then fine-tuned with different number of demos(20, 50, 80, 100)across four unseen tasks.
Pre-trainedGatherBusBinDrawerpolicies2050801002050801002050801002050801000-tasks11.258.7513.7515.0013.7526.2527.5036.2518.7526.2537.5046.256.251.252.505.0010-tasks10.0018.7517.5021.2516.2538.7540.0040.0011.2522.5022.5041.250.000.000.002.5030-tasks28.7533.7535.0041.2522.5040.0040.0042.5018.7528.7545.0057.507.505.005.0011.5060-tasks33.7536.2542.5042.5030.0043.7540.0057.5021.2530.0038.7558.758.757.507.5015.00</p>
<p>This work is supported by the HKU-100 Award, the Microsoft Accelerate Foundation Models Research Program, and in part by the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust.Tang, Y., Yu, W., Tan, J., Zen, H., Faust, A., and Harada, T. Saytap: Language to quadrupedal locomotion.arXiv preprint arXiv:2306.07580,2023.Wang, L., Ling, Y., Yuan, Z., Shridhar, M., Bao, C., Qin, Y., Wang, B., Xu, H., and Wang, X. Gensim: Generating robotic simulation tasks via large language models.arXiv preprint arXiv:2310.01361,2023.
Wu, J., Antonova, R., Kan, A., Lepert, M., Zeng, A., Song,S., Bohg, J., Rusinkiewicz, S., and Funkhouser, T. Tidy-bot: Personalized robot assistance with large languagemodels. arXiv preprint arXiv:2305.05658, 2023.Xie, T., Zhao, S., Wu, C. H., Liu, Y., Luo, Q., Zhong, V.,Yang, Y., and Yu, T. Text2reward: Automated densereward function generation for reinforcement learning.arXiv preprint arXiv:2309.11489, 2023.Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J.Set-of-mark prompting unleashes extraordinary visualgrounding in gpt-4v. arXiv preprint arXiv:2310.11441,2023a.Zeng, A., Attarian, M., Ichter, B., Choromanski, K., Wong,A., Welker, S., Tombari, F., Purohit, A., Ryoo, M., Sind-hwani, V., et al. Socratic models: Composing zero-shotmultimodal reasoning with language. arXiv preprintarXiv:2204.00598, 2022.Zhan, A., Zhao, P., Pinto, L., Abbeel, P., and Laskin, M.A framework for efficient robotic manipulation. arXivpreprint arXiv:2012.07975, 2020.
Yang, Z., Li, L., Lin, K., Wang, J.,Lin, C.-C., Liu, Z., andWang, L. The dawn of lmms: Preliminary explorations with gpt-4v (ision).arXiv preprint arXiv:2309.17421,2023b.Yarats, D., Kostrikov, I., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.In International Conference on Learning Representations, 2021.URL https://openreview.net/forum?id=GY6-6sTvGaf.Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.-H., Gonzalez Arenas, M., Lewis Chiang, H.-T., Erez, T., Hasenclever, L., Humplik, J., Ichter, B., Xiao, T., Xu, P., Zeng, A., Zhang, T., Heess, N., Sadigh, D., Tan, J., Tassa, Y., and Xia, F. Language to rewards for robotic skill synthesis.Arxiv preprint arXiv:2306.08647,2023.</p>
<p>the two sets of tasks, which set do you think has a larger distance/difference between them ?Pick and place the red airplane toy into the green bin.Pick and place the red block into the green bin.Set D Pick and place the red block into the green bin.
Set BMove the block into the drawerMove the ball into the drawerSet CMove the block into the drawerMove the block into the bin6. Given the two sets of tasks, which set do you think hasa larger distance/difference between them ?Set B</p>
<p>Table 6 .
6
Responses collected and the corresponding votes for each factor which makes the tasks more distant.
FactorTotal CountSet A -Main Action166Set B -Shape of Object112Set C -Shape of Location130Set D -Object color53Set E -Location color39Total500</p>
<p>Table 7 .
7
False positives observed through manual checking of the trajectories collected from 20 tasks .
TasksErrorsTrajectoriesClose the drawer1200Move the green block into open drawer14200Align the red block and the stick along the x-axis8200Gather the red stick and red block into the green bin22200Arrange all the red blocks in a straight line29200Stack all the red blocks on top of each other0200Pick up the red block1200Swap the positions of the red block and turbo airplane toy21225Arrange all the red blocks in a straight line29200Move the green block into the bowl28180Push the green block towards the left169200Create a tower by stacking one block of each color193on top of each otherMove all the blocks to the corresponding colored stickers69200Move the blue block and the red block on opposite ends53200of the blue linePlace the blue block and the red block side by side35200on the blue linePress the catapult button to trigger the catapult0100Launch the green block using the catapult18200Sort all the blocks by color1493Move the turbo airplane toy to the front and the15200red block to the backMove all the red blocks to the left of the green blocks6206Total6033697</p>
<p>Objects should be stacked from bottom to top.Firstly, move the red block onto the plate.Secondly, move the blue block onto the red block.Thirdly, move the green block onto the blue block."move onto" can be done via Pick and PlaceOn.answer: -1.move the red block onto the plate | [Pick('red block'); PlaceOn('plate)] -2.move the blue block onto the red block | [Pick('blue block'); PlaceOn('red block')] -3.move the green block onto the blue block | [Pick('green block'); PlaceOn('blue block The positive direction of the y-axis represents the right side, and the negative direction represents the left side.The x of the plate is 0.52.So the x which the red block would be placed at: x = 0.52.The y_range of the plate is [-0.30,-0.13].So the y which the red block would be placed at: y = -0.30-about 0.06 = -0.36.The original z of the red block is 0.08.So the z which the red block would be placed at: z = 0.08 + about 0.02.So move the red block at [0.52, -0.36, 0.10].answer: -1.move the red block at [0.52, -0.36, 0.10] | [Pick('red block'); PlaceAt([0.52, -0.36, 0.10])] ''' Now I'd like you to help me decompose the following task into subtasks and call corresponding primitive actions.You should read the task description and the scene graph I provide with you, and think about how to decompose the task.Think step by step, and imagine the process to accomplish the task with the primitive actions provided with you: Pick(obj_name), PlaceOn(obj_name), PlaceAt(place_pos), Push( obj_name, direction, distance), RevoluteJointOpen(obj_name), RevoluteJointClose( obj_name), PrismaticJointOpen(obj_name), PrismaticJointClose(obj_name), Press(obj_name ).Use PrismaticJointOpen and PrismaticJointClose to open and close something that has a prismatic joint, like drawer.Note that the second and third parameters of Push are list of float and float respectively, do not use expressions composed of variables.
task description: move the red block to the left of the platescene graph:[Nodes]:-red block --position: [0.40, -0.20, 0.08], x_range: [0.37, 0.43], y_range: [-0.23,-0.17], z_range: [0.05, 0.11]-green block --position: [0.05, -0.26, 0.08], x_range: [0.02, 0.08], y_range: [-0.29,-0.23], z_range: [0.05, 0.11]-plate --position: [0.52, -0.21, 0.09], x_range: [0.44, 0.60], y_range: [-0.30,-0.13], z_range: [0.07, 0.11][Edges]:reasoning:40, -0.20, 0.08], x_range: [0.37, 0.43], y_range: [-0.23,-0.17], z_range: [0.05, 0.11]-blue block --position: [0.30, -0.20, 0.08], x_range: [0.27, 0.33], y_range: [-0.30,-0.1], z_range: [0.05, 0.12]-green block --position: [0.05, -0.26, 0.08], x_range: [0.02, 0.08], y_range: [-0.29,-0.23], z_range: [0.05, 0.11]-plate --position: [0.52, -0.21, 0.09], x_range: [0.44, 0.60], y_range: [-0.30,-0.13], z_range: [0.07, 0.11][Edges]:reasoning: ')]
University of Electronic Science and Technology of China. Correspondence to: Sizhe Yang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#51;&#53;&#57;&#57;&#54;&#57;&#57;&#49;&#52;&#52;&#121;&#97;&#110;&#103;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#51;&#53;&#57;&#57;&#54;&#57;&#57;&#49;&#52;&#52;&#121;&#97;&#110;&#103;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>, Qian Luo <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#117;&#111;&#113;&#105;&#97;&#110;&#49;&#64;&#99;&#111;&#110;&#110;&#101;&#99;&#116;&#46;&#104;&#107;&#117;&#46;&#104;&#107;">&#108;&#117;&#111;&#113;&#105;&#97;&#110;&#49;&#64;&#99;&#111;&#110;&#110;&#101;&#99;&#116;&#46;&#104;&#107;&#117;&#46;&#104;&#107;</a>.
Listing 3. Prompt construction. task_proposal_prompt = task_proposal_base_prompt + '\n' + ''''' + '\n' + 'scene graph:' + '\n' + scene_graph + 'tasks:' task_decomposition_prompt = task_decomposition_base_prompt + '\n' + ''''' + '\n' + 'task description: ' + task_desc + '\n' + 'scene graph:' + '\n ' + scene_graph + 'reasoning: ' success_inference_prompt = success_inference_base_prompt + '\n' + ''''' + '\n' + 'task description: ' + task_desc + '\n' + 'scene graph list:' + '\n' + scene_graph_list_str + ' success metric: ' ht
BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents
• align the red block and the stick along the x-axis• move the stick into the green bin success metric: The red block is on the left of the plate.In other words, the y of the red block is less than the minimum value of the plate's y_range.reasoning: The y of the red block is -0.05, which is larger than the minimum value of the plate's y_range (-0.30).So "move the red block to the left of the plate" has been accomplished.answer: no ''' Now I'd like you to help me infer whether the proposed task is completed.You should read the task description and the scene graph list I provide with you, and then complete the "success metric", "reasoning" and "answer".
M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Towards a unified agent with foundation models. N Di Palo, A Byravan, L Hasenclever, M Wulfmeier, N Heess, M Riedmiller, Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023arXiv preprint</p>
<p>Active task randomization: Learning visuomotor skills for sequential manipulation by proposing feasible and novel tasks. K Fang, T Migimatsu, A Mandlekar, L Fei-Fei, J Bohg, arXiv:2211.061342022arXiv preprint</p>
<p>Rvt: Robotic view transformer for 3d object manipulation. A Goyal, J Xu, Y Guo, V Blukis, Y.-W Chao, D Fox, 2023CoRL</p>
<p>Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. S Gu, E Holly, T Lillicrap, S Levine, IEEE international conference on robotics and automation. 2017. 2017IEEE</p>
<p>Scaling up and distilling down: Language-guided robot skill acquisition. H Ha, P Florence, S Song, P S Haibach-Beach, M E Perreault, A S Brian, D H Collier, Proceedings of the 2023 Conference on Robot Learning. the 2023 Conference on Robot Learning2023. 2023Motor learning and development. Human kinetics</p>
<p>N Hansen, X Wang, H Su, arXiv:2203.04955Temporal difference learning for model predictive control. 2022arXiv preprint</p>
<p>Life span motor development. K M Haywood, N Getchell, Human kinetics. 2021</p>
<p>Visual language maps for robot navigation. C Huang, O Mees, A Zeng, W Burgard, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023a</p>
<p>S Huang, Z Jiang, H Dong, Y Qiao, P Gao, H Li, Instruct2act, arXiv:2305.11176Mapping multi-modality instructions to robotic actions with large language model. 2023barXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.056082022barXiv preprint</p>
<p>W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, Voxposer, arXiv:2307.05973Composable 3d value maps for robotic manipulation with language models. 2023carXiv preprint</p>
<p>BC-Z: zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, of Proceedings of Machine Learning Research. A Faust, D Hsu, G Neumann, London, UKPMLR8-11 November 2021. 2021164Conference on Robot Learning</p>
<p>Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, Vima, General robot manipulation with multimodal prompts. 2022arXiv</p>
<p>YOLOv5 by Ultralytics. G Jocher, May 2020</p>
<p>Simple but effective: Clip embeddings for embodied ai. A Khandelwal, L Weihs, R Mottaghi, A Kembhavi, A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollar, R Girshick, Segment, Anything, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022. October 2023Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>K Lin, C Agia, T Migimatsu, M Pavone, J Bohg, arXiv:2303.12153Text2motion: From natural language instructions to feasible plans. 2023arXiv preprint</p>
<p>Summarizing robot experiences for failure explanation and correction. Z Liu, A Bahety, S Song, Reflect, 7th Annual Conference on Robot Learning. 2023</p>
<p>Grounding object relations in language-conditioned robotic manipulation with semantic-spatial reasoning. Q Luo, Y Li, Y Wu, arXiv:2303.179192023arXiv preprint</p>
<p>Language conditioned imitation learning over unstructured data. C Lynch, P Sermanet, 10.15607/RSS.2021.XVII.047Robotics: Science and Systems XVII, Virtual Event. D A Shell, M Toussaint, M A Hsieh, July 12-16, 20212021</p>
<p>Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, Eureka, arXiv:2310.12931Human-level reward design via coding large language models. 2023arXiv preprint</p>
<p>What matters in language conditioned robotic imitation learning over unstructured data. O Mees, L Hermann, W Burgard, 10.1109/LRA.2022.3196123IEEE Robotics and Automation Letters. 742022</p>
<p>ArXiv, abs/2303.08774Gpt-4 technical report. 2023257532815OpenAI</p>
<p>Alvinn: An autonomous land vehicle in a neural network. D A Pomerleau, Advances in neural information processing systems. 11988</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. A Rajeswaran, V Kumar, A Gupta, G Vezzani, J Schulman, E Todorov, S Levine, arXiv:1709.100872017arXiv preprint</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. D Shah, B Osiński, S Levine, Conference on Robot Learning. PMLR2023</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. PMLR2022a</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, Proceedings of the 6th Conference on Robot Learning (CoRL). the 6th Conference on Robot Learning (CoRL)2022b</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018</p>
<p>Large language models as generalizable policies for embodied tasks. A Szot, M Schwarzer, H Agrawal, B Mazoure, W Talbott, K Metcalf, N Mackraz, D Hjelm, A Toshev, arXiv:2310.17722y_range: [-0.29, -0.232023arXiv preprinttask description: move the block at the farthest left to the right scene graph: [Nodes]: -red block --position: [0.40, -0.20, 0.08], x_range: [0.37, 0.43], y_range: [-0.23, -0.17. 0.05, 0.11] -plate --position: [0.52, -0.21, 0.09], x_range: [0.44, 0.60], y_range: [-0.30, -0.13], z_range: [0.07, 0.11] [Edges]: reasoning: "farthest left" refers to the y-coordinate with the smallest value, and the y of the green block which is -0.26 is the smallest, so the green block is at the farthest left. so the robot should move the green block. Since the positive direction of the y-axis represents the right side, a potential right position is [0.05, 0.33, 0.08], so the robot should place the green block at [0.05, 0.33, 0.08</p>
<p>. Placeat, 0.05, 0.33, 0.08])</p>
<p>reasoning: The microwave starts off closed. It needs to be opened before objects can be put in them. After the task is done, it needs to be closed. Microwaves should be opened by pressing a button. Placeon, z_range: [0.06, 0.10] -bowl --position: [0.40, -0.20, 0.08drawer')] -3. close the drawer | [PrismaticJointClose('drawer handle')] ''' ''' task description: move the bowl into the microwave scene graph. 0.05, 0.31] -microwave handle --position: [0.32, -0.40, 0.18], x_range: [0.30, 0.34. 0.37, 0.43], y_range: [-0.23, -0.17], z_range: [0.05, 0.11. So firstly, open the door of the microwave. Secondly, move the bowl into the microwave. Thirdly, close the door of the microwave. answer: -1. open the door of the microwave | [Press('microwave button')] -2. move the bowl into the microwave | [Pick('bowl'</p>
<p>. Placeon, microwave')] -3. close the door of the microwave | [RevoluteJointClose('microwave handle')</p>            </div>
        </div>

    </div>
</body>
</html>
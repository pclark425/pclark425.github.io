<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-494 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-494</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-494</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-247362526</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.05227v1.pdf" target="_blank">Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods</a></p>
                <p><strong>Paper Abstract:</strong> Natural Language Generation (NLG) has made great progress in recent years due to the development of deep learning techniques such as pre-trained language models. This advancement has resulted in more fluent, coherent and even properties controllable (e.g. stylistic, sentiment, length etc.) generation, naturally leading to development in downstream tasks such as abstractive summarization, dialogue generation, machine translation, and data-to-text generation. However, the faithfulness problem that the generated text usually contains unfaithful or non-factual information has become the biggest challenge, which makes the performance of text generation unsatisfactory for practical applications in many real-world scenarios. Many studies on analysis, evaluation, and optimization methods for faithfulness problems have been proposed for various tasks, but have not been organized, compared and discussed in a combined manner. In this survey, we provide a systematic overview of the research progress on the faithfulness problem of NLG, including problem analysis, evaluation metrics and optimization methods. We organize the evaluation and optimization methods for different tasks into a unified taxonomy to facilitate comparison and learning across tasks. Several research trends are discussed further.</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e494.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e494.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Source-Reference Divergence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data / Source-Reference Divergence (heuristic reference mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mismatch between the information contained in the input source (e.g., article, table) and the human-written reference (e.g., summary) used for training/evaluation, where references often contain facts not present in the source leading models to learn and emit extrinsic hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NLG training / evaluation pipeline (dataset construction and training loop)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The dataset creation and supervised training pipeline for conditional NLG (e.g., abstractive summarization, table-to-text) that uses source documents and reference targets to learn conditional generation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>reference text / dataset documentation (natural-language reference summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training script / seq2seq model implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete / divergent specification in training data (source-reference divergence)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>References (targets) were collected or written in ways that include information not present in the source (e.g., introductory journalistic summaries, heuristically paired reference sentences). Models trained to maximize likelihood of these references thus learn to generate content not grounded in the source; the natural-language description implying that targets are faithful to sources does not hold in the data and hence in implementations that are agnostic to the divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training data / dataset (data preprocessing and supervision signals)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual annotation and dataset analysis (human labeling of reference faithfulness) and cross-dataset comparison (e.g., XSum vs CNN/DM)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>fraction / percentage of reference summaries judged unfaithful by annotators (manual annotation studies); dataset-level statistics (e.g., Maynez et al. style analyses); also downstream error rates and entity-level grounding checks</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large: causes systematic extrinsic hallucination in model outputs and dataset-dependent performance; e.g., the paper cites that Maynez et al. reported 76.9% of XSum reference summaries contained unfaithful content, and models produce much higher rates of unfaithful summaries on XSum vs CNN/DM (Table 5 shows extremely high unfaithfulness ratios on XSum for multiple systems).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High in some widely-used datasets: XSum references heavily divergent (reported ∼76.9% unfaithful), WikiBio and other auto-collected corpora reported two-thirds or similar levels of noisy/unaligned targets; prevalence varies by dataset and collection protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>heuristic or task-appropriate reference collection that is not strictly grounded in the source (ambiguous / implicit assumptions about what references contain); implementations (models/training code) typically maximize reference likelihood and therefore are agnostic to this divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>data filtering/purification (discard noisy pairs), constructing plan-based two-stage models that first extract/select source facts (skeleton) then generate from that plan, use entailment/QA/fact-alignment metrics during training or as rewards, dataset reannotation, and using evaluation metrics that align to the source (e.g., PARENT for table-to-text).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reported improvements in faithfulness when applied: data filtering and plan-based methods reduce hallucinations in some settings; specific numbers vary by paper/dataset (survey cites multiple works that improve faithfulness but no single aggregate percentage).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language generation / text summarization / data-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e494.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e494.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exposure Bias (train/inference mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exposure Bias between training and inference (teacher-forcing mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrepancy where models are trained under teacher forcing (conditioning on gold previous tokens) but at inference must condition on their own predictions, causing distributional shift and increased tendency to hallucinate or produce degenerate sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>seq2seq training and decoding pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The model training loop (teacher-forcing maximum likelihood) vs the decoding/inference procedure (autoregressive sampling/beam search) used in NLG experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>training description / experimental protocol (teacher-forcing MLE)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training script and inference/decoding implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>training-inference misalignment (implicit assumption omitted in description)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions often state the training objective (maximize log-likelihood with teacher forcing) but omit that this creates a mismatch with inference-time generation; implemented training procedure thus does not match actual inference distribution, leading to errors the description doesn't warn about.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / inference algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>theoretical analysis (literature), empirical observation of hallucinations and instability under beam search, and ablation / alternative-training studies (e.g., comparison with minimum risk training)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative analysis of generated outputs, counts/percentages of hallucinations under standard training vs alternative training (MRT), measures of beam search instability (e.g., hallucination incidence under larger beam sizes), and downstream evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: exposure bias is implicated in hallucinations; the survey reports that Minimum Risk Training (which avoids exposure bias) 'reduces the number of hallucinations substantially' and stabilizes beam search behavior, indicating nontrivial effects on both quality and reproducibility of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common across autoregressive seq2seq models trained with teacher forcing; cited broadly in the literature as a core limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>methodological mismatch (teacher-forcing training objective vs autoregressive inference) and lack of training objectives aligned to inference-time metrics or constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>sequence-level training objectives (Minimum Risk Training), contrastive learning, unlikelihood training, adversarial or reinforcement learning formulations, and training procedures that include model-generated prefixes.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reported improvements: MRT shown to substantially reduce hallucinations and stabilize beam search (qualitative/quantitative reductions reported in cited works), unlikelihood training reduces specific undesirable behaviors in dialogue models; exact numeric gains depend on task and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine translation, abstractive summarization, general seq2seq NLG</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e494.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e494.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eval-Metric Misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation Metric Misalignment (n-gram overlap vs factuality)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard n-gram overlap metrics (BLEU, ROUGE, METEOR) do not align with human judgements of factual consistency; relying on them in papers/documentation can misrepresent whether implementations produce faithful outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>evaluation / model selection pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The automatic evaluation stage used to compare model outputs to references and select best models (often via BLEU/ROUGE/METEOR) during experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper-reported evaluation claims and metric choice rationale</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation script / metric computation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>inadequate specification / misleading metric choice</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often report model improvements using BLEU/ROUGE but these metrics have low correlation with human judgments of factuality; the natural-language implication that high n-gram score implies factual correctness is therefore misleading and mismatched with actual verification code used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / model selection criteria</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>meta-evaluation studies comparing metric scores to human annotations (correlation analyses) and benchmarks that compare many metrics across datasets (FRANK, QAGS, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Pearson/Spearman correlation between metric scores and human factuality labels; the survey reports n-gram metrics correlations typically very low (e.g., correlations around 0.08–0.17 range in cited tables), while newer metrics achieve higher but still limited correlations (QA-based up to ~0.6 on some benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Misleading model ranking and selection; models optimized for n-gram metrics can be more fluent but less faithful, harming real-world applicability and reproducibility of claimed gains in factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread (BLEU/ROUGE remain default metrics in many experiments), leading to systemic evaluation misalignment across many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>historical reliance on n-gram metrics, task descriptions that equate surface similarity with fidelity, and lack of task-appropriate, standardized factuality metrics in codebases/documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>adopt entailment-based, QA-based, and fact-alignment metrics; perform meta-evaluation and benchmark comparisons (FRANK, SUMMAc, QAGS); report multiple metrics including human evaluation for factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>QA-based and entailment-based metrics show improved correlations with human judgments (survey: QA-based metrics perform better on many benchmarks, but correlations rarely exceed 0.6), so partial mitigation but still open problem.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLG evaluation (summarization, dialogue, data-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e494.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e494.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Annotation Subjectivity / Low Agreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Annotation Subjectivity and Low Inter-Annotator Agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotations of factual consistency are subjective and show low agreement, meaning descriptions of 'ground-truth' factual labels in papers/documentation can mismatch actual reproducible labels used in code/benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>human annotation / labeling pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The process and documentation that describe how human judges labeled outputs for factuality used to train or validate metrics and models.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>annotation protocol / dataset documentation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>annotation collection tools / label aggregation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous specification / noisy labeling procedure</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often treat faithfulness as binary and publish datasets/metrics built on those labels, but annotation protocols vary and human judgments show low agreement (subjectivity). The description in papers may not convey annotation variability, so the labels used downstream in code can be inconsistent or not well-calibrated across studies.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data labeling / gold standard construction</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reporting of inter-annotator agreement (Fleiss' Kappa) and comparative studies (crowd vs expert disagreement); cited works compute agreement statistics</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Fleiss' Kappa and other inter-annotator agreement metrics; the survey cites κ=0.58 for faithful/unfaithful and κ=0.39 for fine-grained error types (Pagnoni et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Creates unreliable gold labels for metric correlation studies and for supervised training of factuality classifiers, undermining reproducibility and transferability of evaluation and mitigation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed broadly in summarization annotation studies; fine-grained labeling even less reliable than binary judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>intrinsic subjectivity of factuality judgments, underspecified annotation guidelines, and inconsistent annotation protocols across studies.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>use more reliable annotation protocols (e.g., best-worst scaling/ranking which Tang et al. found more reliable), clearer guidelines, multi-annotator adjudication, and meta-evaluation across annotator pools.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Ranking-based Best-Worst Scaling reported as more reliable than rating-based approaches; more rigorous protocols improve agreement but do not eliminate subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>annotation for NLG evaluation (summarization, dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e494.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e494.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentence-level NLI Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-level NLI vs Document-level Factuality Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying off-the-shelf sentence-level NLI models (trained on datasets like MNLI) to document-level factuality evaluation is inconsistent with downstream NLG tasks and can give poor or misleading assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>entailment-based factuality evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Using an NLI classifier as an automatic scorer to decide whether generated text is entailed by source text during metric computation or reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric specification (apply NLI to judge entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>NLI model evaluation code / aggregation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>model-domain mismatch / aggregation insufficiency</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions that propose using sentence-level NLI often omit that premises in generation tasks are long and multi-sentence; direct application of sentence-level NLI trained on short premise-hypothesis pairs performs poorly. Implementations that naively apply sentence NLI (or aggregate naively) hence mis-evaluate paragraph-level factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation model choice and aggregation logic</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical evaluation and ablation: out-of-the-box NLI models performed poorly on summary-document entailment tasks; researchers proposed aggregation strategies (RankNLI, SummaC) and retraining on longer premises.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>ranking tasks (summary reranking) and correlation with human judgments; improvement measured by better reranking/selection and increased correlation after document-level adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Poor entailment judgments lead to wrong reranking decisions and unreliable metrics; better task-specific NLI or aggregation improves selection of faithful summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widely attempted but often misapplied until task-aware adaptations were introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>domain shift between NLI training data (short premises) and generation evaluation tasks (long documents); underspecified aggregation rules in descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>train/fine-tune NLI models on document-level or task-specific annotated pairs, use aggregation modules (e.g., CNN-based aggregation in SummaC), or create annotation-based NLI datasets for the target domain.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Task-aware NLI adaptations and fine-tuning significantly improve metric utility (e.g., SummaC and annotation-finetuned classifiers outperform vanilla MNLI-based approaches), but effectiveness depends on the scale and quality of task-specific labels.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLG evaluation / entailment-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e494.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e494.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language-Prior vs Source Reliance (CoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Prior Dominance / Counterfactual Source Reliance Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models and metric implementations can over-rely on language priors (what is probable in the language model) rather than evidence in the source; the CoCo method demonstrates and measures this mismatch by counterfactual masking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>scoring and factuality estimation pipeline (probability-based metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Metrics and scorers that use token probabilities from language models conditioned on source vs not-conditioned (prior vs posterior) to estimate how much the source influences generation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric rationale / scoring description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>probability scoring code (LM scoring with and without source context)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>model-behavior mismatch (language prior dominance not accounted for)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Documentation that treats model probabilities as indicative of faithfulness can be misleading because high-probability tokens may be driven by language priors; CoCo points out that a natural-language claim 'higher posterior probability implies faithfulness' is invalid unless prior influence is controlled for.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>scoring / metric computation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>counterfactual estimation via masking the source and comparing token probabilities (CoCo's masked vs unmasked comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>CoCo score computed as average difference between P(y_i | x, y_<i) and P(y_i | x_masked, y_<i) over selected tokens; larger difference indicates greater reliance on source.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Explains some false positives where fluent but ungrounded text scores well; highlights that metrics or selection procedures relying only on LM posteriors may prefer language-plausible but unfaithful outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Applicable whenever language-model-based scoring is used; the problem is pervasive in LM-scoring-based metrics and selection heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>over-reliance on parametric language priors in pre-trained models and lack of counterfactual checks in metric implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>use counterfactual estimation (CoCo) to penalize prior-driven tokens, design metrics that directly measure grounding to source (QA/entailment/fact-alignment), and incorporate post-editing or constrained decoding to enforce grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>CoCo provides a principled signal to detect prior-dominated tokens; using it or similar approaches can reduce selection of ungrounded outputs, though exact quantitative improvements depend on downstream use.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>metric design / model scoring in NLG</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e494.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e494.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA-metric Component Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of QA-based Factuality Metrics to Component Choices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>QA-based metrics' performance strongly depends on choices of subcomponents (answer extraction units, question generation, QA model, alignment metric); descriptions that omit these details risk misrepresenting metric behavior when other implementations are used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>QA-based factuality evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-component pipeline (answer selection, question generation, QA answering, answer alignment) used to compute factuality scores between generated text and source.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric specification / evaluation procedure</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation pipeline code integrating QG and QA models</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>under-specified implementation details / component selection sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often describe a QA-based metric in high-level terms; however, the survey highlights that component choices (e.g., NP chunks vs entities for answers, BART-QA2D QG, Electra-large QA, LERC alignment) critically determine metric correlation with human judgments. Implementations that differ on these components will produce divergent evaluations despite similar natural-language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>metric implementation and evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>systematic component ablations and meta-evaluation (QAFactEval showed which components boost performance)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>correlation with human judgments across benchmark datasets and ablation studies showing effect of swapping components</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>High: metric scores and consequent model comparisons can vary substantially with component choices; without precise implementation specs, reproducing reported metric performance is difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common: many QA-based metric papers used different pipelines leading to inconsistent results across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>underspecified method descriptions and lack of standardized component choices in metric papers/documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>publish full pipeline code and hyperparameters, perform and report component ablations, adopt standardized benchmark pipelines (meta-evaluation suites), and follow recommended configurations (e.g., QAFactEval optimized settings).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>When optimized components are used (as in QAFactEval), QA-based metrics' correlations with human judgments increased noticeably (survey reports such tuned settings 'boosted the performance of QA-based metric to a new level'), though absolute correlation ceilings remain <1.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLG evaluation / summarization metrics</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e494.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e494.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implementation Agnosticism to Dataset Divergence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Implementations Agnostic to Source-Reference Divergence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model training code that simply maximizes reference likelihood without accounting for dataset-specific source-reference divergence (i.e., assumes references are fully grounded) produces a mismatch between the dataset documentation/intent and the implemented training objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>model training implementation (MLE-based seq2seq)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Typical model training code that applies maximum likelihood estimation against provided references without additional grounding checks or constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper claims about supervised training on dataset presumed to be faithful</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training script / loss function implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implicit assumption mismatch (implementation assumes faithful targets)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Paper/dataset descriptions sometimes imply references are faithful to sources; actual training code naively maximizes likelihood and therefore learns to reproduce any information in references, including content ungrounded to sources. This is a gap between documentation/assumptions and implemented objective.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training objective / data supervision design</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical observation of model hallucinations and controlled data analyses showing reference noise; follow-up studies that filter data or change objectives and observe changes in hallucination rates.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>comparison of hallucination rates before/after data filtering or objective change; dataset-level statistics of ungrounded reference tokens; human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Models trained in this way produce substantial extrinsic hallucinations and their reported performance (on e.g., ROUGE) can be misleading regarding grounding; contributes to datasets' different model error profiles (XSum vs CNN/DM).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread in MLE-based NLG implementations unless dataset-specific handling is added.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>simplifying assumptions in experimental descriptions and default training recipes that do not enforce grounding constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>incorporate grounding-aware objectives (QA rewards, entailment rewards), data purification, plan-based generation, or two-stage extract-then-generate pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Survey cites multiple works showing reduced hallucination when grounding-aware methods are used, though exact quantitative gains depend on dataset and method.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLG model training / summarization</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On Faithfulness and Factuality in Abstractive Summarization <em>(Rating: 2)</em></li>
                <li>Evaluating the Factual Consistency of Abstractive Text Summarization <em>(Rating: 2)</em></li>
                <li>Handling Divergent Reference Texts when Evaluating Table-to-Text Generation <em>(Rating: 2)</em></li>
                <li>FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization <em>(Rating: 2)</em></li>
                <li>QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization <em>(Rating: 2)</em></li>
                <li>Minimum Risk Training for Neural Machine Translation (MRT) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-494",
    "paper_id": "paper-247362526",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Source-Reference Divergence",
            "name_full": "Data / Source-Reference Divergence (heuristic reference mismatch)",
            "brief_description": "A mismatch between the information contained in the input source (e.g., article, table) and the human-written reference (e.g., summary) used for training/evaluation, where references often contain facts not present in the source leading models to learn and emit extrinsic hallucinations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "NLG training / evaluation pipeline (dataset construction and training loop)",
            "system_description": "The dataset creation and supervised training pipeline for conditional NLG (e.g., abstractive summarization, table-to-text) that uses source documents and reference targets to learn conditional generation.",
            "nl_description_type": "reference text / dataset documentation (natural-language reference summaries)",
            "code_implementation_type": "training script / seq2seq model implementation",
            "gap_type": "incomplete / divergent specification in training data (source-reference divergence)",
            "gap_description": "References (targets) were collected or written in ways that include information not present in the source (e.g., introductory journalistic summaries, heuristically paired reference sentences). Models trained to maximize likelihood of these references thus learn to generate content not grounded in the source; the natural-language description implying that targets are faithful to sources does not hold in the data and hence in implementations that are agnostic to the divergence.",
            "gap_location": "training data / dataset (data preprocessing and supervision signals)",
            "detection_method": "manual annotation and dataset analysis (human labeling of reference faithfulness) and cross-dataset comparison (e.g., XSum vs CNN/DM)",
            "measurement_method": "fraction / percentage of reference summaries judged unfaithful by annotators (manual annotation studies); dataset-level statistics (e.g., Maynez et al. style analyses); also downstream error rates and entity-level grounding checks",
            "impact_on_results": "Large: causes systematic extrinsic hallucination in model outputs and dataset-dependent performance; e.g., the paper cites that Maynez et al. reported 76.9% of XSum reference summaries contained unfaithful content, and models produce much higher rates of unfaithful summaries on XSum vs CNN/DM (Table 5 shows extremely high unfaithfulness ratios on XSum for multiple systems).",
            "frequency_or_prevalence": "High in some widely-used datasets: XSum references heavily divergent (reported ∼76.9% unfaithful), WikiBio and other auto-collected corpora reported two-thirds or similar levels of noisy/unaligned targets; prevalence varies by dataset and collection protocol.",
            "root_cause": "heuristic or task-appropriate reference collection that is not strictly grounded in the source (ambiguous / implicit assumptions about what references contain); implementations (models/training code) typically maximize reference likelihood and therefore are agnostic to this divergence.",
            "mitigation_approach": "data filtering/purification (discard noisy pairs), constructing plan-based two-stage models that first extract/select source facts (skeleton) then generate from that plan, use entailment/QA/fact-alignment metrics during training or as rewards, dataset reannotation, and using evaluation metrics that align to the source (e.g., PARENT for table-to-text).",
            "mitigation_effectiveness": "Reported improvements in faithfulness when applied: data filtering and plan-based methods reduce hallucinations in some settings; specific numbers vary by paper/dataset (survey cites multiple works that improve faithfulness but no single aggregate percentage).",
            "domain_or_field": "natural language generation / text summarization / data-to-text",
            "reproducibility_impact": true,
            "uuid": "e494.0"
        },
        {
            "name_short": "Exposure Bias (train/inference mismatch)",
            "name_full": "Exposure Bias between training and inference (teacher-forcing mismatch)",
            "brief_description": "A discrepancy where models are trained under teacher forcing (conditioning on gold previous tokens) but at inference must condition on their own predictions, causing distributional shift and increased tendency to hallucinate or produce degenerate sequences.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "seq2seq training and decoding pipeline",
            "system_description": "The model training loop (teacher-forcing maximum likelihood) vs the decoding/inference procedure (autoregressive sampling/beam search) used in NLG experiments.",
            "nl_description_type": "training description / experimental protocol (teacher-forcing MLE)",
            "code_implementation_type": "training script and inference/decoding implementation",
            "gap_type": "training-inference misalignment (implicit assumption omitted in description)",
            "gap_description": "Natural-language descriptions often state the training objective (maximize log-likelihood with teacher forcing) but omit that this creates a mismatch with inference-time generation; implemented training procedure thus does not match actual inference distribution, leading to errors the description doesn't warn about.",
            "gap_location": "training procedure / inference algorithm",
            "detection_method": "theoretical analysis (literature), empirical observation of hallucinations and instability under beam search, and ablation / alternative-training studies (e.g., comparison with minimum risk training)",
            "measurement_method": "qualitative analysis of generated outputs, counts/percentages of hallucinations under standard training vs alternative training (MRT), measures of beam search instability (e.g., hallucination incidence under larger beam sizes), and downstream evaluation metrics",
            "impact_on_results": "Substantial: exposure bias is implicated in hallucinations; the survey reports that Minimum Risk Training (which avoids exposure bias) 'reduces the number of hallucinations substantially' and stabilizes beam search behavior, indicating nontrivial effects on both quality and reproducibility of outputs.",
            "frequency_or_prevalence": "Common across autoregressive seq2seq models trained with teacher forcing; cited broadly in the literature as a core limitation.",
            "root_cause": "methodological mismatch (teacher-forcing training objective vs autoregressive inference) and lack of training objectives aligned to inference-time metrics or constraints.",
            "mitigation_approach": "sequence-level training objectives (Minimum Risk Training), contrastive learning, unlikelihood training, adversarial or reinforcement learning formulations, and training procedures that include model-generated prefixes.",
            "mitigation_effectiveness": "Reported improvements: MRT shown to substantially reduce hallucinations and stabilize beam search (qualitative/quantitative reductions reported in cited works), unlikelihood training reduces specific undesirable behaviors in dialogue models; exact numeric gains depend on task and dataset.",
            "domain_or_field": "machine translation, abstractive summarization, general seq2seq NLG",
            "reproducibility_impact": true,
            "uuid": "e494.1"
        },
        {
            "name_short": "Eval-Metric Misalignment",
            "name_full": "Evaluation Metric Misalignment (n-gram overlap vs factuality)",
            "brief_description": "Standard n-gram overlap metrics (BLEU, ROUGE, METEOR) do not align with human judgements of factual consistency; relying on them in papers/documentation can misrepresent whether implementations produce faithful outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "evaluation / model selection pipeline",
            "system_description": "The automatic evaluation stage used to compare model outputs to references and select best models (often via BLEU/ROUGE/METEOR) during experimentation.",
            "nl_description_type": "paper-reported evaluation claims and metric choice rationale",
            "code_implementation_type": "evaluation script / metric computation code",
            "gap_type": "inadequate specification / misleading metric choice",
            "gap_description": "Papers often report model improvements using BLEU/ROUGE but these metrics have low correlation with human judgments of factuality; the natural-language implication that high n-gram score implies factual correctness is therefore misleading and mismatched with actual verification code used for evaluation.",
            "gap_location": "evaluation metrics / model selection criteria",
            "detection_method": "meta-evaluation studies comparing metric scores to human annotations (correlation analyses) and benchmarks that compare many metrics across datasets (FRANK, QAGS, etc.)",
            "measurement_method": "Pearson/Spearman correlation between metric scores and human factuality labels; the survey reports n-gram metrics correlations typically very low (e.g., correlations around 0.08–0.17 range in cited tables), while newer metrics achieve higher but still limited correlations (QA-based up to ~0.6 on some benchmarks).",
            "impact_on_results": "Misleading model ranking and selection; models optimized for n-gram metrics can be more fluent but less faithful, harming real-world applicability and reproducibility of claimed gains in factuality.",
            "frequency_or_prevalence": "Widespread (BLEU/ROUGE remain default metrics in many experiments), leading to systemic evaluation misalignment across many papers.",
            "root_cause": "historical reliance on n-gram metrics, task descriptions that equate surface similarity with fidelity, and lack of task-appropriate, standardized factuality metrics in codebases/documentation.",
            "mitigation_approach": "adopt entailment-based, QA-based, and fact-alignment metrics; perform meta-evaluation and benchmark comparisons (FRANK, SUMMAc, QAGS); report multiple metrics including human evaluation for factuality.",
            "mitigation_effectiveness": "QA-based and entailment-based metrics show improved correlations with human judgments (survey: QA-based metrics perform better on many benchmarks, but correlations rarely exceed 0.6), so partial mitigation but still open problem.",
            "domain_or_field": "NLG evaluation (summarization, dialogue, data-to-text)",
            "reproducibility_impact": true,
            "uuid": "e494.2"
        },
        {
            "name_short": "Annotation Subjectivity / Low Agreement",
            "name_full": "Human Annotation Subjectivity and Low Inter-Annotator Agreement",
            "brief_description": "Human annotations of factual consistency are subjective and show low agreement, meaning descriptions of 'ground-truth' factual labels in papers/documentation can mismatch actual reproducible labels used in code/benchmarks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "human annotation / labeling pipeline",
            "system_description": "The process and documentation that describe how human judges labeled outputs for factuality used to train or validate metrics and models.",
            "nl_description_type": "annotation protocol / dataset documentation",
            "code_implementation_type": "annotation collection tools / label aggregation code",
            "gap_type": "ambiguous specification / noisy labeling procedure",
            "gap_description": "Papers often treat faithfulness as binary and publish datasets/metrics built on those labels, but annotation protocols vary and human judgments show low agreement (subjectivity). The description in papers may not convey annotation variability, so the labels used downstream in code can be inconsistent or not well-calibrated across studies.",
            "gap_location": "data labeling / gold standard construction",
            "detection_method": "reporting of inter-annotator agreement (Fleiss' Kappa) and comparative studies (crowd vs expert disagreement); cited works compute agreement statistics",
            "measurement_method": "Fleiss' Kappa and other inter-annotator agreement metrics; the survey cites κ=0.58 for faithful/unfaithful and κ=0.39 for fine-grained error types (Pagnoni et al.).",
            "impact_on_results": "Creates unreliable gold labels for metric correlation studies and for supervised training of factuality classifiers, undermining reproducibility and transferability of evaluation and mitigation methods.",
            "frequency_or_prevalence": "Observed broadly in summarization annotation studies; fine-grained labeling even less reliable than binary judgments.",
            "root_cause": "intrinsic subjectivity of factuality judgments, underspecified annotation guidelines, and inconsistent annotation protocols across studies.",
            "mitigation_approach": "use more reliable annotation protocols (e.g., best-worst scaling/ranking which Tang et al. found more reliable), clearer guidelines, multi-annotator adjudication, and meta-evaluation across annotator pools.",
            "mitigation_effectiveness": "Ranking-based Best-Worst Scaling reported as more reliable than rating-based approaches; more rigorous protocols improve agreement but do not eliminate subjectivity.",
            "domain_or_field": "annotation for NLG evaluation (summarization, dialogue)",
            "reproducibility_impact": true,
            "uuid": "e494.3"
        },
        {
            "name_short": "Sentence-level NLI Mismatch",
            "name_full": "Sentence-level NLI vs Document-level Factuality Mismatch",
            "brief_description": "Applying off-the-shelf sentence-level NLI models (trained on datasets like MNLI) to document-level factuality evaluation is inconsistent with downstream NLG tasks and can give poor or misleading assessments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "entailment-based factuality evaluation pipeline",
            "system_description": "Using an NLI classifier as an automatic scorer to decide whether generated text is entailed by source text during metric computation or reranking.",
            "nl_description_type": "metric specification (apply NLI to judge entailment)",
            "code_implementation_type": "NLI model evaluation code / aggregation scripts",
            "gap_type": "model-domain mismatch / aggregation insufficiency",
            "gap_description": "Natural-language descriptions that propose using sentence-level NLI often omit that premises in generation tasks are long and multi-sentence; direct application of sentence-level NLI trained on short premise-hypothesis pairs performs poorly. Implementations that naively apply sentence NLI (or aggregate naively) hence mis-evaluate paragraph-level factuality.",
            "gap_location": "evaluation model choice and aggregation logic",
            "detection_method": "empirical evaluation and ablation: out-of-the-box NLI models performed poorly on summary-document entailment tasks; researchers proposed aggregation strategies (RankNLI, SummaC) and retraining on longer premises.",
            "measurement_method": "ranking tasks (summary reranking) and correlation with human judgments; improvement measured by better reranking/selection and increased correlation after document-level adaptation.",
            "impact_on_results": "Poor entailment judgments lead to wrong reranking decisions and unreliable metrics; better task-specific NLI or aggregation improves selection of faithful summaries.",
            "frequency_or_prevalence": "Widely attempted but often misapplied until task-aware adaptations were introduced.",
            "root_cause": "domain shift between NLI training data (short premises) and generation evaluation tasks (long documents); underspecified aggregation rules in descriptions.",
            "mitigation_approach": "train/fine-tune NLI models on document-level or task-specific annotated pairs, use aggregation modules (e.g., CNN-based aggregation in SummaC), or create annotation-based NLI datasets for the target domain.",
            "mitigation_effectiveness": "Task-aware NLI adaptations and fine-tuning significantly improve metric utility (e.g., SummaC and annotation-finetuned classifiers outperform vanilla MNLI-based approaches), but effectiveness depends on the scale and quality of task-specific labels.",
            "domain_or_field": "NLG evaluation / entailment-based metrics",
            "reproducibility_impact": true,
            "uuid": "e494.4"
        },
        {
            "name_short": "Language-Prior vs Source Reliance (CoCo)",
            "name_full": "Language Prior Dominance / Counterfactual Source Reliance Mismatch",
            "brief_description": "Models and metric implementations can over-rely on language priors (what is probable in the language model) rather than evidence in the source; the CoCo method demonstrates and measures this mismatch by counterfactual masking.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "scoring and factuality estimation pipeline (probability-based metrics)",
            "system_description": "Metrics and scorers that use token probabilities from language models conditioned on source vs not-conditioned (prior vs posterior) to estimate how much the source influences generation.",
            "nl_description_type": "metric rationale / scoring description",
            "code_implementation_type": "probability scoring code (LM scoring with and without source context)",
            "gap_type": "model-behavior mismatch (language prior dominance not accounted for)",
            "gap_description": "Documentation that treats model probabilities as indicative of faithfulness can be misleading because high-probability tokens may be driven by language priors; CoCo points out that a natural-language claim 'higher posterior probability implies faithfulness' is invalid unless prior influence is controlled for.",
            "gap_location": "scoring / metric computation",
            "detection_method": "counterfactual estimation via masking the source and comparing token probabilities (CoCo's masked vs unmasked comparisons)",
            "measurement_method": "CoCo score computed as average difference between P(y_i | x, y_&lt;i) and P(y_i | x_masked, y_&lt;i) over selected tokens; larger difference indicates greater reliance on source.",
            "impact_on_results": "Explains some false positives where fluent but ungrounded text scores well; highlights that metrics or selection procedures relying only on LM posteriors may prefer language-plausible but unfaithful outputs.",
            "frequency_or_prevalence": "Applicable whenever language-model-based scoring is used; the problem is pervasive in LM-scoring-based metrics and selection heuristics.",
            "root_cause": "over-reliance on parametric language priors in pre-trained models and lack of counterfactual checks in metric implementations.",
            "mitigation_approach": "use counterfactual estimation (CoCo) to penalize prior-driven tokens, design metrics that directly measure grounding to source (QA/entailment/fact-alignment), and incorporate post-editing or constrained decoding to enforce grounding.",
            "mitigation_effectiveness": "CoCo provides a principled signal to detect prior-dominated tokens; using it or similar approaches can reduce selection of ungrounded outputs, though exact quantitative improvements depend on downstream use.",
            "domain_or_field": "metric design / model scoring in NLG",
            "reproducibility_impact": true,
            "uuid": "e494.5"
        },
        {
            "name_short": "QA-metric Component Sensitivity",
            "name_full": "Sensitivity of QA-based Factuality Metrics to Component Choices",
            "brief_description": "QA-based metrics' performance strongly depends on choices of subcomponents (answer extraction units, question generation, QA model, alignment metric); descriptions that omit these details risk misrepresenting metric behavior when other implementations are used.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "QA-based factuality evaluation pipeline",
            "system_description": "A multi-component pipeline (answer selection, question generation, QA answering, answer alignment) used to compute factuality scores between generated text and source.",
            "nl_description_type": "metric specification / evaluation procedure",
            "code_implementation_type": "evaluation pipeline code integrating QG and QA models",
            "gap_type": "under-specified implementation details / component selection sensitivity",
            "gap_description": "Papers often describe a QA-based metric in high-level terms; however, the survey highlights that component choices (e.g., NP chunks vs entities for answers, BART-QA2D QG, Electra-large QA, LERC alignment) critically determine metric correlation with human judgments. Implementations that differ on these components will produce divergent evaluations despite similar natural-language descriptions.",
            "gap_location": "metric implementation and evaluation scripts",
            "detection_method": "systematic component ablations and meta-evaluation (QAFactEval showed which components boost performance)",
            "measurement_method": "correlation with human judgments across benchmark datasets and ablation studies showing effect of swapping components",
            "impact_on_results": "High: metric scores and consequent model comparisons can vary substantially with component choices; without precise implementation specs, reproducing reported metric performance is difficult.",
            "frequency_or_prevalence": "Common: many QA-based metric papers used different pipelines leading to inconsistent results across benchmarks.",
            "root_cause": "underspecified method descriptions and lack of standardized component choices in metric papers/documentation.",
            "mitigation_approach": "publish full pipeline code and hyperparameters, perform and report component ablations, adopt standardized benchmark pipelines (meta-evaluation suites), and follow recommended configurations (e.g., QAFactEval optimized settings).",
            "mitigation_effectiveness": "When optimized components are used (as in QAFactEval), QA-based metrics' correlations with human judgments increased noticeably (survey reports such tuned settings 'boosted the performance of QA-based metric to a new level'), though absolute correlation ceilings remain &lt;1.",
            "domain_or_field": "NLG evaluation / summarization metrics",
            "reproducibility_impact": true,
            "uuid": "e494.6"
        },
        {
            "name_short": "Implementation Agnosticism to Dataset Divergence",
            "name_full": "Model Implementations Agnostic to Source-Reference Divergence",
            "brief_description": "Model training code that simply maximizes reference likelihood without accounting for dataset-specific source-reference divergence (i.e., assumes references are fully grounded) produces a mismatch between the dataset documentation/intent and the implemented training objective.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "model training implementation (MLE-based seq2seq)",
            "system_description": "Typical model training code that applies maximum likelihood estimation against provided references without additional grounding checks or constraints.",
            "nl_description_type": "paper claims about supervised training on dataset presumed to be faithful",
            "code_implementation_type": "training script / loss function implementation",
            "gap_type": "implicit assumption mismatch (implementation assumes faithful targets)",
            "gap_description": "Paper/dataset descriptions sometimes imply references are faithful to sources; actual training code naively maximizes likelihood and therefore learns to reproduce any information in references, including content ungrounded to sources. This is a gap between documentation/assumptions and implemented objective.",
            "gap_location": "training objective / data supervision design",
            "detection_method": "empirical observation of model hallucinations and controlled data analyses showing reference noise; follow-up studies that filter data or change objectives and observe changes in hallucination rates.",
            "measurement_method": "comparison of hallucination rates before/after data filtering or objective change; dataset-level statistics of ungrounded reference tokens; human evaluation.",
            "impact_on_results": "Models trained in this way produce substantial extrinsic hallucinations and their reported performance (on e.g., ROUGE) can be misleading regarding grounding; contributes to datasets' different model error profiles (XSum vs CNN/DM).",
            "frequency_or_prevalence": "Widespread in MLE-based NLG implementations unless dataset-specific handling is added.",
            "root_cause": "simplifying assumptions in experimental descriptions and default training recipes that do not enforce grounding constraints.",
            "mitigation_approach": "incorporate grounding-aware objectives (QA rewards, entailment rewards), data purification, plan-based generation, or two-stage extract-then-generate pipelines.",
            "mitigation_effectiveness": "Survey cites multiple works showing reduced hallucination when grounding-aware methods are used, though exact quantitative gains depend on dataset and method.",
            "domain_or_field": "NLG model training / summarization",
            "reproducibility_impact": true,
            "uuid": "e494.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On Faithfulness and Factuality in Abstractive Summarization",
            "rating": 2,
            "sanitized_title": "on_faithfulness_and_factuality_in_abstractive_summarization"
        },
        {
            "paper_title": "Evaluating the Factual Consistency of Abstractive Text Summarization",
            "rating": 2,
            "sanitized_title": "evaluating_the_factual_consistency_of_abstractive_text_summarization"
        },
        {
            "paper_title": "Handling Divergent Reference Texts when Evaluating Table-to-Text Generation",
            "rating": 2,
            "sanitized_title": "handling_divergent_reference_texts_when_evaluating_tabletotext_generation"
        },
        {
            "paper_title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
            "rating": 2,
            "sanitized_title": "feqa_a_question_answering_evaluation_framework_for_faithfulness_assessment_in_abstractive_summarization"
        },
        {
            "paper_title": "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization",
            "rating": 2,
            "sanitized_title": "qafacteval_improved_qabased_factual_consistency_evaluation_for_summarization"
        },
        {
            "paper_title": "Minimum Risk Training for Neural Machine Translation (MRT)",
            "rating": 1,
            "sanitized_title": "minimum_risk_training_for_neural_machine_translation_mrt"
        }
    ],
    "cost": 0.029385249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods</p>
<p>Wei Li 
Baidu Inc
BeijingChina</p>
<p>Wenhao Wu wuwenhao@baidu.com 
Key Laboratory of Computational Linguistics
MOE
Peking University</p>
<p>Moye Chen chenmoye@baidu.com 
Baidu Inc
BeijingChina</p>
<p>Jiachen Liu liujiachen@baidu.com 
Baidu Inc
BeijingChina</p>
<p>Xinyan Xiao xiaoxinyan@baidu.com 
Baidu Inc
BeijingChina</p>
<p>Hua Wu wu_hua@baidu.com 
Baidu Inc
BeijingChina</p>
<p>Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods</p>
<p>Natural Language Generation (NLG) has made great progress in recent years due to the development of deep learning techniques such as pre-trained language models. This advancement has resulted in more fluent, coherent and even properties controllable (e.g. stylistic, sentiment, length etc.) generation, naturally leading to development in downstream tasks such as abstractive summarization, dialogue generation, machine translation, and data-to-text generation. However, the faithfulness problem that the generated text usually contains unfaithful or non-factual information has become the biggest challenge, which makes the performance of text generation unsatisfactory for practical applications in many real-world scenarios. Many studies on analysis, evaluation, and optimization methods for faithfulness problems have been proposed for various tasks, but have not been organized, compared and discussed in a combined manner. In this survey, we provide a systematic overview of the research progress on the faithfulness problem of NLG, including problem analysis, evaluation metrics and optimization methods. We organize the evaluation and optimization methods for different tasks into a unified taxonomy to facilitate comparison and learning across tasks. Several research trends are discussed further.</p>
<p>Introduction</p>
<p>Natural Language Generation (NLG) is the process of producing a natural language text from a textual or non-textual input in order to meet specified communicative goals (Gatt and Krahmer, 2018). The input of NLG varies with different task settings, however, the output is always readable natural language text. According to the type of input, the tasks of NLG can be mainly categorized into: text-to-text generation, data-to-text generation, and multimodality-to-text generation.</p>
<p>The text-to-text generation tasks take existing texts as input, and automatically produce a new, coherent text as output. The most common applications include: text summarization (Allahyari et al., 2017), dialogue generation (Li et al., 2016b), machine translation (Koehn, 2009), question generation (Du et al., 2017), paraphrase generation  etc. The data-to-text generation tasks automatically generate text from numerical or structured data such as table, key-value lists, and tuples. The example applications include: table-to-text generation (Liu et al., 2018b), KG-to-text generation (Ke et al., 2021), meaning-to-text generation (e.g. AMR-to-text) (Song et al., 2018) etc. The multimodality-to-text generation tasks transfer the semantics in multimodal input such as images or videos, into natural language texts. Typical tasks include image caption , visual storytelling (Huang et al., 2016), and video summarization (Ma et al., 2002).</p>
<p>From the perspective of input-output information transformation, the tasks of NLG can be divided into open-ended language generation and non-open-ended language generation. Open-ended language generation tasks refer to tasks that the input is incomplete and the output semantics are not contained by the input. For example, story generation is a classical open-ended language generation task, which tends to generate a complete story based on some leading sentences or keywords. Obviously, the model needs to create new information to completing storyline planning and generating meaningful stories. One of the greatest characteristics of the open-ended language generation tasks is that the information mapping between input and output is usually one-to-many. The same input can produce many outputs with different meanings.</p>
<p>By contrast, for non-open-ended language generation tasks, the input usually provides complete or even more information for the output. Machine translation is one typical non-open-ended language generation task where the input provides complete semantics for the output. Paraphrase generation can be regarded as an equivalent transformation of information, where the input and output semantics are exactly the same, but the language expression is different. In text summarization, input usually provides more information than output, so the summarization model needs to select salient information to produce summary output. Table 1 lists some common NLG tasks as well as their characteristics.  Pretraining-based Pretraining Objectives (e.g. BERT, T5, GPT3, BART, CTRL) Faithfulness</p>
<p>Developing of NLG</p>
<p>The research on NLG has a long history, starting from 1950s. The developing of NLG approaches can be mainly divided into four stages: template-based, statistical-based, neural-based and pretrainingbased, as shown in Table 2.</p>
<p>• Template-based. The earliest natural language generation system adopted the method of rules and templates to design different modules for text generation, which reflected the linguistic knowledge of vocabulary, grammar, syntax and even pragmatics designed by many experts. They usually consists of several different components including content planning, sentence planning and text realization, each performing a specified function.</p>
<p>• Statistical-based. Statistical language model further proposes a new idea of language modeling from the perspective of probability and statistics, which encodes the dependency between vocabulary and context in conditional probability. N-gram language model is the most popular statistical language model, which is usually coupled with template-based methods for re-ranking and selecting fluent generated texts.</p>
<p>• Neural-based. With the development of deep learning, the neural-based end-to-end methods have gradually occupied a dominant position, which can better model the statistical cooccurrence relationship between vocabulary and context through end-to-end training, thus significantly improves the performance of text generation. Various neural architectures have been explored for NLG, such as Recurrent Neural Network (RNN) (Graves, 2013;Zaremba et al., 2014), Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014) and self-attention Transformer network (Vaswani et al., 2017).</p>
<p>• Pretraining-based. Most recently, the pre-trained language generation model based on the Transformer architecture can better capture the linguistic knowledge of vocabulary, syntax and semantics, which greatly promotes the development of natural language generation. The rise of pre-trained language models (Brown et al., 2020;Devlin et al., 2018;Liu et al., 2019c) has led to strong text generation models for applications including text summarization Liu and Lapata, 2019;, dialogue generation (Bao et al., 2020;, data-to-text generation (Chen et al., 2020b), and machine translation . However, while these models generate fluent and grammatical text, they are prone to making factual errors that contradict the input text (Cao et al., 2017).</p>
<p>Challenges and Issues NLG faces four main challenges and issues at different development stages: fluency, informativeness, controllability and faithfulness, as shown in Figure 1.</p>
<p>• The fluency problem refers to whether the generated text is fluent, grammatical, and coherent.</p>
<p>• The informativeness problem refers to that the model generates redundant, meaningless, and general content, and the generated text is significantly insufficient in informativeness, diversity, and specificity.</p>
<p>• The controllability problem means that the generated text cannot satisfy the pre-given constraints, such as text style, attributes and content.</p>
<p>• The faithfulness problem means that the generated content is inconsistent with the input information, has hallucinations or non-factual information.</p>
<p>Traditional template-based methods can usually generate reliable and faithful texts, but limited by the diversity and generality of rules, the generated texts usually face the problems of fluency and informativeness. Benefiting from end-to-end training on large corpus, the neural-based methods can generate fluent and informative texts. However, due to the introduction of the probability sampling mechanism, they need to sample from the probability distribution estimated by the model each time.</p>
<p>Considering that the vocabulary is very large, generally in the order of 1000 ∼ 50000, the probability distribution inevitably contains a large number of long-tail words with low probability of occurrence, coupled with the randomness of probability sampling itself, the controllability and faithfulness of the neural-based NLG model is particularly serious. In the pre-training era, through self-supervised training on large-scale unlabeled corpora, the model generated text is outstanding in terms of fluency, informativeness and even controllability, but it still cannot solve the faithfulness problem.</p>
<p>The Faithfulness Problem</p>
<p>The faithfulness problem has become the biggest challenge in NLG, which largely limits the applicability of NLG algorithms in practical scenarios. For example, the researches on abstractive text summarization show that about 30% of summaries generated by state-of-the-art models have faithfulness issues (Cao et al., 2017;Falke et al., 2019;Kryściński et al., 2019;Pagnoni et al., 2021). This brings serious problems to the credibility and usability of abstractive summarization systems. </p>
<p>Tasks Source Output</p>
<p>Abstractive Summarization</p>
<p>The first vaccine for Ebola was approved by the FDA in 2019 in the US, five years after the initial outbreak in 2014. To produce the vaccine, scientists had to sequence the DNA of Ebola, then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started.  Faithfulness in Open-ended NLG For Open-ended NLG tasks, the model needs to leverage knowledge in Knowledge Graph or corpus to create new content that not contained by the input. The faithfulness problem in open-ended NLG tasks is defined as whether the generated content is factually consistent with the world knowledge or commonsense, often referred to as factual correctness. For example, the faithfulness in news article generation is whether the facts in the generated article actually exists or happen in the real world. One relevant research topic is fake news detection (Shu et al., 2017;Zhang and Ghorbani, 2020).</p>
<p>To address the faithfulness problem, a lot of automatic faithfulness evaluation metrics and meta evaluations for these metrics have been proposed. Besides, much effort has been devoted to optimizing faithfulness for different NLG tasks. The framework of existing researches is demonstrated in Figure 2. Since the research on faithfulness mainly focuses on non-open-ended tasks, such as text summarization, machine translation, knowledge-grounded dialogue generation and data-to-text generation, this paper mainly studies the faithfulness (i.e. factual consistency) in non-openended tasks. We conduct a comprehensive survey of existing researches on faithfulness, including problem analysis, evaluation metrics, and optimization approaches.</p>
<p>Structure of This Survey</p>
<p>The content typology of this survey is shown in Figure 3.</p>
<p>In Section 2, we give a systematic analysis on the faithfulness problem in NLG, including categorization of unfaithful errors, manual annotations, challenges for evaluating and optimizing faithfulness, cause analysis, and relations with other aspects.</p>
<p>In Section 3, we organize the various evaluation metrics proposed for faithfulness evaluation, and combine the meta-evaluations for these metrics to facilitate future research on faithfulness evaluations.</p>
<p>In Section 4, we summarize different optimization methods from both the perspective of tasks and methodology, and detail their relative advantages. Machine Translation Feng et al. [46,Weng et al. [180], Tu et al. [164] Data-to-text Generation Nie et al. [129], Wang [172] Dialogue Generation Kim et al. [82] Figure 3: The content typology of the survey.</p>
<p>Problem Analysis</p>
<p>In general, the task of natural language generation (NLG) targets at finding an optimal sequence y &lt;T +1 = (y 1 , y 2 , . . . , y T ) that satisfies:
y &lt;T +1 = arg max y &lt;T +1 ∈S logP θ (y &lt;T +1 |x) arg max y &lt;T +1 ∈S T t=1 logP θ (y t |y &lt;t , x)(1)
where T represents the number of tokens of the generated sequence, S represents a set containing all possible sequences, and P θ (y t |y &lt;t , x) is the conditional probability of the next token y t based on its previous tokens y &lt;t = (y 1 , y 2 , . . . , y t−1 ) and the source sequence x with model parameters θ.</p>
<p>Definition and Categorization</p>
<p>We define the output sequence as being unfaithful if it has a span y i , ..., y j , that is not supported by the input sequence x. The faithfulness issues, i.e. factual inconsistent with source sequence, can be divided into two categories:</p>
<p>• Intrinsic Error: the fact that is contradicted to the source sequence x due to synthesizing content using information present in x, which is also referred to "intrinsic hallucination" in Maynez et al. (2020). • Extrinsic Error: the fact that is neither supported nor contradicted by the source, which is also referred to "extrinsic hallucination" in Maynez et al. (2020).</p>
<p>Frank (Pagnoni et al., 2021) defines a fine-grained typology of factual errors for text summarization, which is theoretically grounded in frame semantics (Fillmore et al., 1976;Palmer et al., 2005) and linguistic discourse analysis (Brown et al., 1983). It can also be applied to other non-open-ended NLG tasks, such as dialogue generation, machine translation and table-to-text generation.</p>
<p>The fine-grained categories of factual errors mainly include:  (Maynez et al., 2020)); • Grammatical Error (GramE) denotes not well formed statements that make their meaning incomprehensible or ambiguous and cannot be verified against the source.</p>
<p>Despite all extrinsic errors are assumed incorrect,  and Maynez et al. (2020) find that much hallucinated content is factual, namely consistent with world knowledge. Factual hallucinations refer to content that is verifiable by world knowledge but not inferable from source text. For example, in text summarization, they find that more than half of the hallucinated entities are factual with respect to the source document and world knowledge. These factual hallucinations can be beneficial in a summary by providing useful background information. Thus, the extrinsic errors or OutEs can be further categorized into factual hallucinations and non-factual hallucinations.</p>
<p>Combining these definitions and categorization, we define a more thorough hierarchical typology of the faithfulness problems, as shown in Table 4. This typology provides us with the means to categorize the types of errors made by generation models, helping us gain deeper insights than simply categorizing content as faithful or unfaithful.</p>
<p>Challenges and Issues</p>
<p>Model Analysis A lot of researches make annotations with different granularity to analyze the faithfulness performance of existing language generation models. The results show that even the most powerful pertaining models suffer from serious unfaithful problems. Take the abstractive summarization task for example, the annotation results of the ratio of unfaithful summaries generated by several popular models including T5 (Raffel et al., 2019), BART (Lewis et al., 2019) and PEGASUS , are shown in Table 5. The annotation results are combined from Pagnoni et al. (2021) and Cao and Wang (2021). The above results show that all systems generate over 60% unfaithful summaries on the XSum dataset. Also, on the CNN/DM dataset, T5 and BART generate over 20% unfaithful summaries. On the one hand, the above results show the severity of the faithfulness problem of current models, and on the other hand, it also shows that the impact of different datasets is also very large. We will analyze the influence of dataset in Section 2.3.</p>
<p>Evaluation Common automatic evaluation metrics for text generation based on n-gram overlap -BLEU, ROUGE, and METEOR (Banerjee and Lavie, 2005;Lin, 2004;Papineni et al., 2002) -are insufficient to measure the faithfulness of the generated text. Kryściński et al. (2019) and Fabbri et al. (2021a) find that they have low correlation with human judgements of factuality, as shown in Table 6. So, a lot of new evaluation methods are proposed to evaluate the faithfulness of generated text for different tasks. We will describe them in Section 3.</p>
<p>Annotation Faithfulness annotation of NLG models is very difficult. Most existing work consider faithfulness as a binary concept, annotating generated text as faithful or unfaithful (Maynez et al., 2020). However, Falke et al. (2019) showed relatively low crowd-expert agreement, indicating the presence of subjectivity in the annotation process. Pagnoni et al. (2021) annotated the faithfulness of summarization systems in a more fine-grained manner, however, the inter-annotator agreement is also low. They collect human annotations from three independent annotators. The inter-annotator agreement in terms of Fleiss Kappa κ (Fleiss, 1971) is 0.58 for faithful or not, and 0.39 for specific unfaithful error types shown in Table 4, which all indicate low inter-annotator agreement. Tang et al. (2021) compared the reliability of ranking and rating-based human annotations of faithfulness in summarization models and found that ranking-based Best-Worst Scaling annotations are largely reliable than rating-based annotations. ρ p-val γ p-val ρ p-val γ p-val ρ p-val γ p-val BLEU 0.10 0.00 0.07 0.00 0.08 0.01 0.08 0.01 0.14 0.00 0.20 0.00 METEOR 0.14 0.00 0.11 0.00 0.12 0.00 0.10 0.00 0.15 0.00 0.10 0.00</p>
<p>Rouge-1 0.14 0.00 0.10 0.00 0.12 0.00 0.10 0.00 0.15 0.00 0.09 0.01</p>
<p>Rouge-2 0.12 0.00 0.08 0.00 0.08 0.00 0.07 0.01 0.17 0.00 0.14 0.00</p>
<p>Rouge-L 0.13 0.00 0.09 0.00 0.11 0.00 0.09 0.00 0.16 0.00 0.10 0.00</p>
<p>Cause Analysis</p>
<p>Many factors can affect the faithfulness of model-generated results, such as dataset, training method, and model expressiveness.</p>
<p>Data divergence between source and reference. The divergence between source and reference is one of the main reason for extrinsic hallucinations during generation. For example, in text summarization, summaries were usually written by journalists as introductions to the news articles they precede. These summaries, therefore, often have true additional information not found in the document. Such divergence issue between source and target is not uncommon in conditional text generation (Dhingra et al., 2019;Kryściński et al., 2019;Wiseman et al., 2017). The divergence may be a product of heuristic data collection, or it may be inevitable due to the nature of some NLG tasks, such as table-to-text generation and dialogue generation.</p>
<p>Existing models are usually agnostic to the source-reference divergence, making them vulnerable to hallucinations. Thus, models can generate texts that are not consistent with the input, yet would likely have reasonable model log-likelihood. This is the main reason why the same model performs differently on different datasets, such as the difference in summarization performance on the XSum dataset and the CNN/DM dataset. The XSum dataset is collected heuristically by simply taking the introductory sentence prefacing each article as its reference summary, so reference summaries often contain hallucinations. Maynez et al. (2020) reported that 76.9% of reference summaries contained unfaithful content. In contrast, the reference summaries of the CNN/DM datasets are all human-written with less hallucinations. Therefore, the faithfulness of the summarization model on the CNN/DM dataset is much better than that on the XSum dataset.</p>
<p>Exposure bias between training and inference. Wang and Sennrich (2020) state that exposure bias (Ranzato et al., 2015), a discrepancy between training and inference, is partially to blame for hallucinations. Specifically, the standard teacher-forcing training algorithm (Williams and Zipser, 1989) used by most existing work can lead to a discrepancy between what the model sees during training and test time, resulting in degenerate outputs with factual hallucinations (Maynez et al., 2020). Furthermore, the model is also only optimized to maximize the log-likelihood of the reference summary at the word-level, which does not necessarily reward models for being faithful.</p>
<p>Poor text representation. A model with poor input text representation will fail to do document level inference, often required for abstraction and generation, and will be vulnerable to such errors. For example, in text summarization, the percentage of system summaries with intrinsic hallucination was much higher than in gold summaries. This phenomenon particularly revealed the models' tendency to misrepresent information in the document due to the lack of document-level understanding and inference. To improve text representation, it is a common practice to leverage large pre-trained models for downstream NLG tasks. Pre-training can improve text generation, due to its exposure to vast amount of text through pretraining, allowing it to integrate background knowledge with generation. However, Longpre et al. (2021) have discovered that such models usually over-rely on the parametric </p>
<p>Automatic Evaluation Metrics</p>
<p>Recently, there has been wide empirical success in text summarization, machine translation, dialogue response generation, and other text generation tasks. For evaluation, these models generally rely on metrics like ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin, 2004), BLEU (Bilingual Evaluation Understudy) (Papineni et al., 2002) and METEOR (Metric for Evaluation of Translation with Explicit ORdering) (Banerjee and Lavie, 2005) that measure locally constrained n-gram overlap. However, these metrics cannnot evaluate the faithfulness of generated text.</p>
<p>Recently, much work focus on evaluating the factual consistency of generated text and propose various new metrics for different NLG tasks. We categorize these metrics into 4 types: Entailment-based, QA-based, Fact-based, and Others, as shown in Table 7.</p>
<p>NLI Model</p>
<p>Entail / Neutral / Contradict ? NLG Figure 4: The framework of entailment-based metrics.</p>
<p>Entailment-based Metrics</p>
<p>One of the most popular methods is to apply NLI (Natural Language Inference) to access the faithfulness of generated texts, that is whether the generated text is entailed, neutral, or conflicting with a given input, as shown in Figure 4. The basic hypothesis is that the content of generated texts should be entailed by or at least not conflict with the source text. Though an NLI model usually predicts three different scores for entailment, neutral, and contradiction, most work only utilize entailment score to evaluate faithfulness. Formally, given a source text x as a premise, a generated text y as a hypothesis, an NLI model N predicts the entailment score as N (x, y). The larger the N (x, y) is, the more faithful y given x. For evaluating the proposed metrics, most works report their correlations with human judgements, while some other works, especially entailment-based metrics, also propose ranking-based downstream tasks to demonstrate performances. We will also introduce these ranking tasks in the following.</p>
<p>Sentence-level NLI Traditional NLI tasks predict entailment scores between sentences. However, in the text generation scenario, the input text x takes various forms and often contains multiple sentences that severely challenge the application of NLI. Earlier attempts directly apply NLI classifiers to access the factual consistency between input text x and output text y. They study how NLI models trained on traditional NLI datasets like MNLI (Williams et al., 2018) perform. Falke et al. (2019) proposed to aggregate entailment scores between sentences of x and y to calculate the faithfulness score between x and y, namely RankNLI. Given sentences s y ∈ y, s x ∈ x, RankNLI formalizes faithfulness between y and x as:
1 |y| sy∈y max sx∈x N (s x , s y )(2)
They found that while entailment prediction should help with this problem, out-of-the-box NLI models performed poorly on this task. Falke et al. (2019) also proposes a summary re-ranking task to evaluate the performance of RankNLI. In this task, a better metric should help the summarization model to select more faithful summaries during the reranking process of beam search. They further analyze how different architectures of the NLI model N , such as ESIM (Chen et al., 2017), BERT (Devlin et al., 2018), affect the summary ranking task. Maynez et al. (2020) applied a much simpler strategy by directly using NLI models trained on MNLI to predict entailment score N (x, y). Barrantes et al. (2020) further found that applying ANLI dataset instead of MNLI dataset in Falke et al. (2019) to train the NLI classifier is more suitable for faithfulness evaluation. SummaC (Laban et al., 2021a) comprehensively revisits sentence-level NLI for accessing faithfulness. They apply a CNN module to aggregate the entailment score matrix between document and summary sentences, and demonstrate the potential of sentence-level NLI on various benchmarks.</p>
<p>Annotation-based</p>
<p>The major problem of sentence-level NLI metrics is that they are inconsistent with their downstream tasks, which often require the evaluator to predict paragraph-level entailment scores. Some work attempted to directly train an NLI classifier between source text x and target output y. A straightforward solution is to annotate a certain scale of samples for training the classifier. In text summarization, Aralikatte et al. (2021) and Gehrmann et al. (2021) finetuned NLI classifier on hundreds of (around 500) manual annotated samples for faithfulness evaluation and reported a good performance of this simple metric.</p>
<p>In dialog generation, Welleck et al. (2019b) constructed a Dialog NLI dataset (DialogNLI) for factual consistency evaluation. To save human labor, they annotate the relation triples of dialogue sentences instead. Based on relation triples, they inference the NLI labels by certain rules. Welleck et al. (2019b) also propose an utterance ranking task, which is often applied to evaluate the factual consistency of a dialogue model. In this task, given history utterances u &lt;t , a dialogue model is asked to select the next utterance u t with the lowest perplexities from a set of candidate utterances U:
u t = arg min u∈U − log p(u|u &lt;t ), where U = (U + , U − , U rand )(3)
where the generation probability p is calculated by the dialogue model, U + , U − , U rand represent the set of utterances that are entailed with u &lt;t , conflicted with u &lt;t or randomly selected, respectively.</p>
<p>The higher probability the model selects a u t from U + , the better factual consistency it has. Song et al. (2020a) proposed a human-annotated dataset, namely Key-value Profile Identification (KvPI), with single-turn conversations and corresponding attribute profiles. They further labeled NLI relations between each conversation and structured profile. With the NLI labels, they trained a classifier to predict entailment relations between structured attributes and generated utterances. Qin et al. (2021) proposed a human-annotated dataset CI-ToD, which incorporates NLI labels between various types of inputs including dialogue history, user query and the corresponding knowledge base. They propose a uniform model to assess all the factual consistency relations above. Nie et al.  (Mishra et al., 2021) suggested that the major bottleneck in the utility of NLI models is that traditional NLI datasets do not exhibit long premises. To solve this problem, they convert multiple-choice reading comprehension datasets into two-class NLI datasets using data augmentation methods. In addition to similar rule-based methods, they also applied text generation models to generate higher-quality samples.</p>
<p>Fine-grained Prediction Some works utilize fine-grained features to assess faithfulness. They convert direct predictions of N (x, y) into sequentially labeled fine-grained features in the generated text. DAE (Goyal and Durrett, 2020) applies a new formulation of entailment that decomposes it at the level of dependency arcs. Instead of making decisions at the sentence level, DAE sequentially predicts the entailment scores of each dependency arc in the generated sentence and aggregates them to obtain the final faithfulness score. Goyal and Durrett (2021) further explores the difference in error distribution between synthetic and human written summaries, and investigates how fine-grained supervision information can benefit faithfulness evaluation.</p>
<p>QA-based Metrics</p>
<p>Because assessing faithfulness requires logical inference over factual information, it is natural to utilize the reasoning ability of Question Answering (QA) models. Several recent works proposed QAbased factual evaluation metrics. As shown in Figure 5, these metrics often include two components: a question generation (QG) module and a QA module. The core idea of these metrics is to predict the matching score between source answers (key information units from source text) and target answers (key information units from generated text). The overall procedure of these metrics are summarized as following:</p>
<ol>
<li>
<p>Answer Selection: Extract information units from the generated text, which is viewed as target answers.</p>
</li>
<li>
<p>Question Generation: Conditioned upon the selected target answers, the QG module generates questions using the generated text as context.</p>
</li>
</ol>
<p>Question Answering:</p>
<p>The QA module answers the questions with the source text as context to retrieve source answers.</p>
<p>Answer Alignment Evaluation:</p>
<p>Calculate the matching score between source and target answers by a answer alignment metric to output the final evaluation score.</p>
<p>QAGS (Wang et al., 2020a) and FEQA (Durmus et al., 2020) are the earliest QA-based factual evaluation metrics. These two metrics share similar model architectures and processing procedures introduced above. In the procedure 1, QAGS extracted n-grams as the information units for target answers while FEQA extracted entities. In procedure 2-4, they both applied BERT-based QA modules, BART-based QG modules and token-level F1 as answer alignment metrics.</p>
<p>Several QA-based metrics followed the framework of QAGS and FEQA with moderate modifications. QuestEval Scialom et al. (2021) extended this framework by adding an extra procedure to measure the recall-oriented performance. The additional procedure generated question-answer pairs from the source document and answered the questions from the generated text. In contrast to QuestEval, QUALS Nan et al. (2021a) simplified the above procedure 1-3 by only one neural language model (QAGen). QUALS employs QAGen as proposed in (Shakeri et al., 2020), to generate both the questions and answers from the generated text. In particular, given a summary y, QAGen outputs a question-answer (q-a) pair jointly, separated by a special token <a>. Let LL y (q, a) be the average log likelihood of generating the q-a pair from the given summary y. Then given the input document x, QUALS simply evaluates the average log likelihood of the QAGen model producing the same q-a pairs, denoted as LL x (q, a). Formally, given a summary y and input document x, QUALS score is computed as follows:
QU ALS(x, y) = 1 M (q,a)∈y (LL x (q, a) − LL y (q, a))(4)
where M is the number of q-a pairs selected on the summary y. This simplification largely decreases the computational time and memory of the original QAGS.</p>
<p>QAFactEval Fabbri et al. (2021a) conducted extensive comparisons of QA-based metrics and demonstrated that carefully choosing the components of a QA-based metric is critical to performance. The optimized settings of QAFactEval in each procedure are listed in the following:</p>
<ol>
<li>
<p>Select NP chunks as the textual units as target answers;</p>
</li>
<li>
<p>Apply BART-QA2D (Demszky et al., 2018) for the QG module and filter low quality generated questions;</p>
</li>
</ol>
<p>Facts (e.g. entities, triples, n-gram) Facts (e.g. entities, triples, n-gram)</p>
<p>IE IE</p>
<p>Alignment</p>
<p>Matching Acc / Score ? NLG Figure 6: The framework of fact alignment-based metrics.</p>
<ol>
<li>Apply Electra-large (Clark et al., 2020) for QA; 4. Apply LERC  score as the answer alignment metric.</li>
</ol>
<p>With these carefully selected settings, Fabbri et al. (2021a) boosted the performance of QA-based metric to a new level.</p>
<p>In addition to the factual metrics in text summarization listed above, Honovich et al. (2021) proposed a QA-based metric Q 2 for evaluating factual consistency in open-domain dialogue generation. They utilized the entailment score predicted by an NLI model as the alignment metric for answer spans.</p>
<p>Fact-based Metrics</p>
<p>The most intuitive way to evaluating faithfulness is to count the fact overlap between generated text and source document, as shown in Figure 6. Facts can be represented in different forms, such as entities, n-grams and relation triples (subject, relation, object).</p>
<p>Factual inconsistency can occur at either the entity or the relation level. At the entity level, a model generated text may contain named entities that never appeared in the source document. At the relation level, the entities indeed exist in the source document but the relations between them are not in the source document.</p>
<p>Entity-based</p>
<p>EntityAlign Nan et al. (2021b) proposed an entity-based metrics that rely on off-the shelf tools to perform Named-Entity Recognition(NER). Let N (x) and N (y) denote the number of named-entities in the source (input document) and target (generated text), respectively. N (y ∩ x) denotes the number of entities found in the generated text that can find a match in the source document. If a named entity in the generated text consists of multiple words, it is considered a match as long as any n-gram of the named-entity can be found in the source document. The degree of faithfulness with respect to the source text is quantified:
prec = N (y ∩ x)/N (y).
SimAlign For machine translation, Sabet et al. (2020) proposed to leverage multilingual word embeddings -both static and contextualized -for word alignment between source language and translated language.</p>
<p>Ngram-based</p>
<p>PARENT For table-to-text generation task, Dhingra et al. (2019) modeled facts as n-grams, and developed a metric PARENT (Precision And Recall of Entailed Ngrams from the Table) which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. When computing precision, PARENT effectively uses a union of the reference and the table, to reward correct information missing from the reference. When computing recall, it uses an intersection of the reference and the table, to ignore extra incorrect information in the reference. The union and intersection are computed with the help of an entailment model to decide if a text n-gram is entailed by the table. The entailed precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a model is the average of instance-level PARENT scores across the evaluation set.</p>
<p>PARENT-T PARENT-T ) is a table-focused version of PARENT. When computing precision, PARENT-T considers an n-gram to be correct if it has a high probability of being entailed by the table. PARENT-T uses the word overlap model for computing entailment probability. For recall, PARENT-T only computes it against table to ensure that texts that mention more information from the table get higher scores. The system-level PARENT-T score for a model is the average of instance-level PARENT-T scores across the evaluation set.</p>
<p>Relation-based</p>
<p>TripleAlign Facts are usually represented by relation triples (subject, relation, object), where the subject has a relation to the object. To extract triples, Goodrich et al. (2019) first try to use OpenIE tool (Yates et al., 2007). However, OpenIE extracts triples with an unspecified schema instead of a fixed schema. In unspecific schema extraction, relation is extracted from the text between subject and object. In fixed schema extraction, a relation is predicted from a pre-defined relations set, which could be viewed as a classification task. Unspecific schema extraction makes the extracted triples hard to compare with each other. To resolve this problem, Goodrich et al. (2019) change to use relation extraction tools with fixed schema, which helps extracted triples easier to compare.</p>
<p>Other Metrics</p>
<p>Recently, there are some work evaluate faithfulness of text generation from other perspectives.</p>
<p>BARTScore Yuan et al. (2021) conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models, directly evaluating text through the lens of its probability of being generated from or generating other textual inputs and outputs. The general idea is that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. They operationalize this idea using BART (Lewis et al., 2019), an encoder-decoder based pre-trained model, and propose a metric BARTScore with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency,or factuality).
BART Score = m t=1 w t logp(y t |y &lt;t , x, θ)(5)
To evaluate faithfulness, they propose to compute the probability from source document to hypothesis p(y|x, θ). This direction measures how likely it is that the hypothesis y could be generated based on the source text x.</p>
<p>TokenLevelCLS  propose a general-purpose method for token level hallucination detection for conditional sequence generation tasks. Given the source input x, they first formulate the task of token-level hallucination detection as a sequence labeling problem where a binary label is predicted at each position of the generated text. They train a model with synthetic training data in the form of ((x, y), L y ) where L y are the labels at every position of y that indicate if each word is a hallucinated one or not. They leverage the BART model (Lewis et al., 2019) to mapping a corrupted sentence back to the original text it was derived from, without providing it any access to the source sentence, thereby encouraging it to insert new content as needed to ensure fluency. Then, they finetune a pre-trained language model (LM) on the synthetic data to help detect the token level hallucinations in various conditional sequence generation tasks.</p>
<p>CoCo CoCo  is proposed to evaluate the faithfulness of summarized texts via counterfactual estimation. They point out that the effect of language prior can be blamed to cause factual inconsistency. The intuition is that when texts are generated more relying on the source document rather than the language prior, they should be more likely to be faithful w.r.t. the source documents. They adopt the probabilities of the tokens of evaluated summaries to implement the automatic evaluation metric. Specifically, given the source document x and model-generated summary y, several key tokens y are first selected from y, then the source document x is masked according y to produce a masked version x .</p>
<p>x and x are feed into the same scoring model respectively to generate the probability of each token in y , i.e., P (y i |x, y &lt;i ) and P (y i |x , y &lt;i ), ∀y i ∈ y . The CoCo value is defined as:
CoCo = 1 |y | yi∈y P (y i |x, y &lt;i ) − P (y i |x , y &lt;i )(6)
ShannonScore Egan et al. (2021) proposed a reference-free metric ShannonScore to evaluate the quality of generated summary via Shannon Game. The Shannon information content of event E with the probability p(E) of happening is defined as I(E) = − log p(E). The ShannonScore performs the Shannon Game with a language model such as GPT-2 (Radford et al., 2019). The main assumption of this metric is that if y is a satisfactory summary of x, then I(x|y) &lt; I(x), as documents that have little to do with the summary should be much less likely than documents that are relevant to the summary after conditioning the language model. Thus they define an Information Difference metric of summary quality as: ID(x, y) = I(x) − I(x|y) (7) They further define the ShannonScore as the normalized Information Difference:
s(x, y) = I(x) − I(x|y) I(x) − I(x|x)(8)
where I(x|x) is the lower bound of I(x|y).</p>
<p>Meta Evaluation</p>
<p>As there are a lot of new metrics proposed to evaluate the faithfulness of language generation, they perform differently on different tasks. To verify the effectiveness of the above metrics, these work usually report the correlations between their own metrics and human-annotated factual consistency scores. However, it is difficult to compare each metric by the correlations as the diversity of annotating settings in different works and disagreement among different annotators. To directly compare the effectiveness of different kinds of faithfulness metrics, several work (Gabriel et al., 2021b;Koto et al., 2021) proposed benchmarks to conduct meta-evaluations of faithfulness metrics. For example, the popular benchmarks of evaluating the faithfulness metrics for abstractive summarization include FRANK (Pagnoni et al., 2021), SUMMAc (Laban et al., 2021b), QAGS (Wang et al., 2020a), FEQA (Durmus et al., 2020) and CoCo . Table 8 combines these benchmarks, showing the Pearson correlations between different types of faithfulness evaluation metrics and human annotations.</p>
<p>To facilitate reliable evaluation metrics for grounded dialogue generation, Dziri et al. (2021b) also proposed a benchmark BEGIN for evaluation of grounded dialogue generation systems. Table 8 shows that the meta-evaluation results in different benchmarks differ greatly. The same metric can also performs very different on different datasets. Overall, the QA-based metrics achieve better performance on most datasets and benchmarks. However, the correlations with human evaluations are not more than 0.6. Therefore, the faithfulness evaluation remains an open question of exploration.</p>
<p>Optimization Methods</p>
<p>A lot of optimization methods for the faithfulness problem have been proposed for different tasks, including abstractive summarization, dialogue generation, data-to-text generation and machine translation. However, most of these methods are general for different tasks. They can be categorized as factual guidance, auxiliary tasks, learning methods, post-editing, constrained decoding and others. In the following, we will organize the optimization methods for each task into the above categories to facilitate comparing and learning of different methods, as shown in Figure 3.</p>
<p>Faithfulness in Abstractive Summarization</p>
<p>The faithfulness problem has attracted more and more attention in abstractive summarization. A lot of mitigation methods have been studied, many of which also can be applied to other NLG tasks. In the following, we will introduce the main types of optimization methods for abstractive summarization.</p>
<p>Factual Guidance</p>
<p>Factual guidance is an intuitive and effective method for boosting faithfulness and informativeness in summarization tasks. Guidance can be defined as some signals which are fed into the model as   Figure 7 shows a simple seq2seq based factual guidance framework in which two encoders process origin source and extra guidance signals respectively, then a decoder generates final summaries considering the hidden states of both two encoders. Here, the guidance signals could be keywords, important sentences or other structures such as relations or semantic graphs. According to the types of guidance signals, factual encoders could be a Transformer network (for signal of sequence structure) or a Graph Attention Network (for signal of graph structure) Veličković et al. (2017). GSum Dou et al. (2021) is such a general and extensible framework that can take different kinds of external guidances as extra inputs to mitigate the unfaithful problems. We follow the basic classifications of guidance signals of GSum and then make an extension by supplying some different but effective ones. We divide guidance signals into three types: keywords, sentences and relations.</p>
<p>Keyword Guidance Keywords reflect the crucial information of the source text in a simplest way. They help the summarization models to focus on the most important parts of the source text and result in less factual errors. Li et al. (2018a) propose a Key Information Guide Network which encodes the keywords into the key information representation, to guide the process of generation. Firstly, they extract keywords from the text by using TextRank algorithm, then encode keywords by Bi-RNN network, and guide the generation process by cooperating keyword representations in both attention mechanism and the pointer mechanism. Saito et al. (2020) further combine pre-trained seq2seq model with token-level saliency models called CIT, in which a saliency model (Transfomer encoder with feed-forward layer) produces a score for each token in order to select important ones which are denoted as K. Then a combined textX = concat(K, X) is given to the seq2seq model as the input.</p>
<p>Sentence Guidance Keywords convey limited information of the source text, so some works turn to sentence-level guidance which contains more abundant information including keywords and the connections among them. Cao et al. (2018) propose Re3Sum which retrieves existing summary sentences as candidate templates, and then uses an extended seq2seq framework to jointly conduct template reranking and template-aware summary generation. Specifically, both the source text X and the soft template R are converted into hidden states with a RNN encoder. In the Rerank module, they measure the saliency of R according to its hidden state relevance to X. In the Rewrite module, a RNN decoder combines the hidden states of X and R to generate a summary Y . Song et al. (2020d) exploit PORL-HG model following the extract-then-rewrite famework. PORL-HG firstly selects some attractive sentences from the article by an extractor, then rewrites these sentences by a seq2seq-based abstractor. The model combines the extractor with the abstractor by a reinforcement learning network which regards the popularity score and ROUGE scores as rewards to make sure generated headlines are both attractive and faithful.</p>
<p>Relation Guidance Dou et al. (2021) argue that if we utilize full sentence as guidance signals, it may contain much unnecessary and irrelevant information which is not crucial in a summary and could distract the model from focusing on the actual important parts of the source text. To address this problem, some works use relation information in the form of relational triples as factual guidance. Cao et al. (2017) Figure 8: The framework of auxiliary task-based methods via multi-task learning, reinforcement learning and re-ranking. actual fact descriptions from the source text. They propose a dual-attention seq2seq framework to force the generation conditioned on both the source text and the extracted fact descriptions. Huang et al. (2020) present ASGARD framework which enhances the regular document encoder with an independent graph-structured encoder which improves upon Graph Attention Networks (Veličković et al., 2017) to maintain the global context and local characteristics of entities. They utilize <subject, predicate, object> triples extracted by OpenIE to construct a knowledge graph, then use the hidden states of input tokens represented by RoBERTa (Liu et al., 2019c) to initialize the graph nodes. During decoding, both the representations of source tokens and graph nodes are incorporated into each generation step via cross-attention mechanism. In this way, the knowledge graph can be used as an extra factual guidance during summary generation.</p>
<p>Most works use OpenIE to extract relations from source documents, then represent them as graph structures to improve seq2seq models. However, these OpenIE-based graphs only contain sparse relations between partial words, which cannot cover the overall semantic meaning of the source article. Wu et al. (2021a) propose BASS which firstly introduce an unified semantic graph to enhance the performance of multi-document summarization. To construct the semantic graph, they extract phrases and their relations from sentences by a two-stage merging in which tokens are firstly merged into phrases based on dependency parsing trees, then co-referent phrases are merged into graph nodes according to co-reference chains. Finally, the model encodes graph structures both in encoding and decoding processes, by applying the graph adjacent matrix as self-attention mask and using an graph-propagate attention mechanism to guide the decoding process.</p>
<p>Auxiliary Tasks</p>
<p>Unlike guidance methods which improve factual consistency explicitly, auxiliary task-based methods combine extra tasks which are correlative with factual correctness to boost the performance of summarization systems in an implicit way . There are three widely used frameworks that can easily involve auxiliary task into summarization task, which are reinforcement learning (RL) framework, multi-task learning framework and re-ranking framework, as shown in Figure 8. In the RL framework, it is common to design a score model for generated summaries to obtain a reward which will optimize the factual consistency of summarization models. As for the multi-task framework, a task-specific layer will be stacked over the shared-weight encoder. In this way, the summarization model and auxiliary model share the same semantic representations but have different learning objectives. The related auxiliary task can be seen as a supplement to the summarization task and will improve the performance of the original summarization system. As for the re-ranking framework, it firstly generates several candidate summaries, then a score model based on auxiliary tasks produces a score for each candidate, and finally the best one is selected as the summary. In the following, we will describe several common auxiliary tasks to improve faithfulness of abstractive summarization.</p>
<p>Entailment Task Natural Language Inference (NLI), in which a hypothesis sentence is classified as either entailed by, neutral or contradicting a premise sentence. Previous works (Barrantes et al., 2020;Fabbri et al., 2021b;Falke et al., 2019;Laban et al., 2021b;Li et al., 2018b) have proved that NLI tasks can improve faithfulness of summarization models. It can be incorporated into summarization models by multi-task learning, or acting as a RL reward, or utilized to re-ranking summary candidates. Li et al. (2018b) is the first work which incorporates entailment knowledge into abstractive summarization. They argue that a correct summary is semantic entailed by the source document. They propose an entailment-aware encoder under a multi-task learning framework, and an entailment-aware decoder under an RL framework with entailment rewards. In particular, they use shared weight encoders trained on both the summarization task (i.e. encoder+decoder) and the entailment task (i.e. encoder+classifier). Entailment prediction is regarded as an auxiliary task for summary generation. When decoding, they treat the entailment score as a special reward and combine the reward with a maximum likelihood training process by RL.</p>
<p>Following the idea that all information in a summary should be entailed by the source document, Falke et al. (2019) propose a re-ranking approach to select summaries with less unfaithful errors by entailment prediction models. They design a score function mentioned in Equation 2 to measure the entailment score of a generated summary y given its source document x. The candidate summary with the highest score σ(y) is selected as the model output after reranking. Barrantes et al. (2020) follow this idea and make a further step by applying the adversarial NLI dataset to train the NLI model. More accurate NLI model has more potential of selecting faithful summaries. Fabbri et al. (2021a) propose query-based summarization model which apply NLI score as one of the reinforcement learning rewards to improve factual coinsistency.</p>
<p>Question answering Task Generating factual consistent summaries not only needs the overall understanding of source text but also the discrimination between crucial and useless parts. Thus, it is a natural way to check a summarization model's comprehension and distinction abilities by a QA model. The QA-based methods mainly calculate a QA score by measuring the overlap degree of answers extracted from source text and from the generated summaries, then use the QA score as the reward in the RL framework or the reranking framework. The key procedures of QA-based tasks are mentioned in the Section 3.2. Following this idea, Nan et al. (2021a) incorporate a QA model (Equation 4) into the seq2seq architecture by a novel contrastive learning method. They firstly produce some candidate summaries, then sort them into positive samples and negative samples according to the QA score, finally improve faithfulness of models through contrastive learning over them (specifically introduced in Section 4.1.3).</p>
<p>Other Tasks Zhang et al. (2020d) develop a concise framework to quantify factual correctness of a generated summary using an information extraction model. They take a structured vector v to represent facts in the reference summaries. Each dimension of vector v is a binary variable which describes whether an event or an entity is present or not in the text.
v = f (y) = (v 1 , . . . , v m )(9)
Given the reference summary fact vector v and generated summary fact vectorv, a factual accuracy score s can be computed as:
s(v, v) = m i=1 1[v i =v i ] m(10)
Finally, they combine factual score with rouge score as reward via a reinforcement learning framework. Nan et al. (2021b) propose a series of simple but effective entity-level methods to improve factual consistency of abstractive summarization, including data filtering, multi-task learning, and joint entity and summary generation. For data filtering, they first apply Spacy NER (Honnibal and Montani, 2017) on reference summary to identify all named-entities. If any entities cannot find a match in the source document, they consider this sample as a noisy data and discard the sentence that contains the entity from the ground truth summary to make sure that there is no hallucination in the dataset. They also add a classification layer after the encoder of BART to identify summary-worth entities. As for decoding, they train the BART model to first generate the sequence of summary-worth entities and then the summary so that the salient named-entities can be incorporated into the cross-attention of the decoder.</p>
<p>Learning Methods</p>
<p>Contrastive Learning Cao and Wang (2021) observed that the commonly used maximum likelihood training method showed weak ability of distinguishing references from incorrect generations. Therefore, a potential solution is to design new learning objectives to improve the preference of factual summaries over inconsistent ones. Contrastive learning(CL) is such a paradigm which is first proposed in visual tasks and recently utilized in many NLP tasks. The main idea of contrastive learning shown in Figure 9 is to learn representations of similar samples staying close to each other, while dissimilar ones keeping away. The key point of CL is how to generate positive and negative samples. In visual tasks, it is common to construct positive samples by rotating, resizing, distorting the origin picture, and consider other images as negative samples. In this section, we will introduce several effective methods to construct positive and negative samples in summarization task, and how to involve them into the contrastive learning framework.</p>
<p>Cao and Wang (2021) designe a task-specific contrastive learning formulation (CLIFF) that teaches a summarizer to expand the margin between factually consistent summaries and incorrect peers. CLIFF uses three methods to construct positive samples, including paraphrasing with synonym substitution, randomly replacing words, and back-translation. As for negative samples, previous works often treat other samples in the same batch as negative ones. However, Cao and Wang (2021) argue that such negative samples are easy to distinguish because they are totally different from positive ones. It will be more effective to construct negative samples by making a small but crucial change based on the original references, so that the model can focus on the real important parts of the source text and enhance the ability of differentiating factual and non-factual summaries. Following this idea, CLIFF designs four strategies to create negative samples:</p>
<p>• Entity swap imitates intrinsic errors: swapping named entities in the references with other randomly selected entities of the same entity type in the source text. • Mask-and-fill with BART: replacing each named entity in a reference with a [MASK], then let BART generates new entities. • Source-conditioned regeneration: for each entity in the reference, feeding the text before it along with the origin source into BART, then combining the text before the entity with the generated text as a negative sample. • System generation: selecting system generated summaries with low probability as negative samples.</p>
<p>After constructing positive samples (denoted as P ) and negative samples (denoted as N ), CLIFF optimizes the contrastive learning objective in Equation 11 and combines it with typical cross-entropy loss to form the final training objective shown in Equation 12, where h i , h j , h k are representations for summary y i , y j , y k . sim calculates the cosine similarity between summary representations.
L CL = − 1 |P | 2 yi,yj ∈P,yi =yj log exp(sim(h i , h j )/τ ) y k ∈P ∪N,y k =yi exp(sim(h i , h k )/τ )(11)L = L CE + λL CL(12)
Except for entity-level replacement, Liu et al. (2021c) make a further step by switching the sentiment of some sentences by adding negation words or replacing opposite meaning words to generate more diverse negative samples. Liu et al. (2021b) argue that previous works (Cao and Wang, 2021;Liu et al., 2021c) mainly focus on entity faithfulness which is not equal to summary faithfulness. Thus, they propose a contrastive summarization framework CO2Sum and a span-level negative samples construction method LFN based on pre-trained language model. Specifically, they delete or disturb factual fragments in sentences and observe the language model probability of predicting the context based on these sentences in an iterative way to distinguish which fragments are important to the source text. After detecting most influential factual spans, they replace the fragment in the gold summary with embedding-similar article words to construct negative samples. They involve contrastive learning in both encoder and decoder. In the encoding procedure, they apply contrastive learning between source text and summaries by making the representations of the article and the ground truth summary closer, and make that of the article and the factual inconsistent summaries apart. The CL loss L Enc for the encoder is similar to Cao and Wang (2021). As for the decoder, CO2Sum applies contrastive learning between summaries with a max-margin loss  L Dec to force the model to increase the decoding probabilities of ground truth summaries while decrease the decoding probabilities of negative summaries. The margin loss L Dec and the final training objective L are shown as following.
L Dec = max 1 R i∈R (P s (T neg , i) − P s (T gold , i)) + η, 0(13)L = L CE + λ Enc L Enc + λ Dec L Dec(14)
where R means replaced positions with inconsistent facts, T neg and T neg denote the negative summary and ground truth summary respectively, P s (T, i) denotes the generation probability of the i-th position in sequence T . λ Enc and λ Dec denote the loss weights of the constrastive learning objectives in the encoder side and decoder side, respectively.</p>
<p>Most works construct negative samples by simply replacing some non-target sequences. Lee et al. (2021) argue that these explicit negative samples are suboptimal, since they are easily distinguishable from the correct output, especially when models are pre-trained with large corpus. Within the simple and explicit sample construction framework, models barely learn nothing. Thus, they propose a principled method called CLAPS to construct positive and negative samples implicitly by adding perturbations to the input sequence. To generate a negative example, they add a small perturbation (hard sample) to the hidden representation of target sequence, then minimize its conditional likelihood. As for positive examples, they adding a large perturbations while enforcing the model to have a high conditional likelihood. This will yield a negative example that is very close to the original representation of target sequence in the embedding space but is largely dissimilar in the semantics, while the generated positive example is far away from the original input sequence but has the same semantic as the target sequence. It could generate hard examples which the model might be difficult to discriminate, helping it learn more meaningful representations.</p>
<p>Post-Editing</p>
<p>Above methods require modification of model structures or extra sample construction processes to improve factual consistency, which may affect the informativeness (e.g. ROUGE scores) of summary results. Post-editing based methods improve factual consistency by adding an corrector to system-generated summaries. They consider generated summaries as drafts, and correct factual errors to form the final summaries. This process is quite similar to the human writing process, where people write a first draft, then review and edit it to make it better. Figure 10 shows the general framework of post-editing methods.  propose SpanFact, a suite of two factual correction models that leverage knowledge learned from question answering models to correct system-generated summaries through span selection and correction. SpanFact takes into account entity-level corrections and make them iteratively. Specifically, assume that the system summary has N entities. At time step i, they mask the i-th entity and use this masked sequence as a query to the QA model. The QA model will replace the wrong entities with the correct ones based on the source document. The corrected entity will then form an updated summary for use in the next step. Human evaluation demonstrates that SpanFact is able to correct about 26% unfaithful summaries, while barely destroying any otherwise correct summaries. Cao et al. (2020) simplify the post-editing procedures by directly training a seq2seq rewrite model on artificial unfaithful summaries as a corrector. They create a weakly-supervised training dataset based on the text transformations following Kryściński et al. (2019) which replace entities, numbers, numerals and pronouns in source documents with other tokens of the same type. The goal of the corrector is to generate correct summaries based on the unfaithful summaries and source documents.</p>
<p>As a standalone module, post-editing methods have been shown to be effective in improving the faithfulness of abstractive summarization systems while preserving their informativeness. However, it's more of an indirect solution than a fundamental solution to factual inconsistencies.</p>
<p>Constrained Decoding</p>
<p>Lexically constrained or guided decoding is a modification of beam search that enforces the inclusion of pre-specified words and phrases in the output. This is a general way to control specific tokens in the generated output without modifying the model structure or additional training data. Mao et al. (2020) propose CAS (Constrained Abstractive Summarization) to improve the factual consistency of summarization systems by constructing constrained token sets during dynamic beam search decoding. It only allows the generation process to end when all constraints are met. They focus on entities and noun phrases and select these types of words that are not present in the summaries generated by the unconstrained system to form constrained sets. Therefore, the model will generate more correct and faithful tokens during the inference process, effectively improving the faithfulness of abstractive summarization. Aralikatte et al. (2021) introduce the Foucs Attention Mechanism (FAME) for the transformer-based seq2seq architecture. FAME combines a standard contextual representation with a dynamic source-conditioned lexical bias layer, which encourages the decoder to actively generate tokens that are faithful to the input document. Zhao et al. (2020a) propose HERMAN which learns to recognize and verify quantity entities in candidate summaries, in order to re-rank the candidate summaries to select the one whose quantity terms are supported by the original text. During the training process, they use a BiLSTM-CRF decoder as a verification model to tag sequence labels and finally predict an overall label that indicates whether the output summary is faithful to the source input or not. At the test time, the same verification model is applied to rerank the candidate summaries, then select the best one with less hallucinations. Gabriel et al. (2021a) proposed Co-opNet, a generator-discriminator framework to do fact-checking for text generation. In this framework, the generator outputs a series of candidate summaries. Then the discriminator scores the factuality of these summaries using one of the following objectives: the overlap between the introduction of a scientific article and the predicted evidence spans in summaries, the ordering of predicted discourse roles, the coverage of predicted discourse roles, or the likelilood of adjacency between generated sentences. The best summary is selected by combining the scores of the generator and the discriminator.  propose an interesting method to detect factual errors by using the prior and posterior predicted probabilities of each token. They assume that if an entity is a factual error, giving the source should not provide more evidence for it, resulting in only small changes in the probabilities between the prior (i.e. without source) and the posterior (i.e. given source) language models. Based on this assumption, they use prior and posterior probabilities as key features of a classifier to predict the factuality of entities. Table 9: Different types of source that dialogue generation models should be faithful to in different tasks.</p>
<p>Other Methods</p>
<p>Source Type Methods</p>
<p>History Dialogue</p>
<p>DialogNLI (Welleck et al., 2019b),  Arun et al. (2020), Ghazvininejad et al. (2018) DECODE (Nie et al., 2021), CI-ToD (Qin et al., 2021) TransferTransfo (Mesgar et al., 2021), UL (Li et al., 2020a) Blender ( DialogNLI (Welleck et al., 2019b), DECODE (Nie et al., 2021) KvBERT (Song et al., 2020a), RCDG  TransferTransfo (Mesgar et al., 2021), UL (Li et al., 2020a) GDR ( User Query CI-ToD (Qin et al., 2021) (i.e. task-oriented dialogue)</p>
<p>Faithfulness in Dialogue Generation</p>
<p>Recently, the area of dialogue generation has made significant progress with end-to-end neural networks and large-scale pre-training (Bao et al., 2020;Roller et al., 2021). However, a long standing problem, faithfulness, still challenges current best dialog systems and attracts an increasing amount of attention. In general, the generated utterance should be faithful to its history utterances (Vinyals and Le, 2015). Different from other generation tasks like text summarization, various forms of dialog generation tasks include a diversity of background or knowledge inputs, with which the generated utterances should also be consist. In Table 9, we summarize different forms of inputs that have been studied in dialogue faithfulness. The optimization methods for dialogue faithfulness are similar to abstractive summarization, which consists of six types of methods. Some of them can also be utilized in summarization tasks.</p>
<p>Factual Guidance</p>
<p>Several works utilized various guidance information to improve factual consistency of dialogues. These methods incorporate relevant guidance information into the training or inference process of dialogue models. As shown in Figure 7, we categorize theses guidance into three types: implicit guidance, which is the guidance in vector representations; extracted guidance, which is the relevant textual information extracted from source inputs; retrieved guidance, which is information retrieved from open-domain knowledge.</p>
<p>Implicit Guidance Implicit guidance is usually representations that are automatically learned before or during the training process of a dialogue model. Li et al. (2016a)  step of their model, they fuse the embedding of the speaker into the text encoder.  present the PERSONA-CHAT dataset which provides persona profiles for each speaker. They also propose a memory-augmented dialogue system where persona profiles were saved and updated in memory. Guided by the profile memory, the generated dialogues are more consistent in personality.  combine dialog generation with style-transfer for a more stylized and contextrelevant chatbot. They fuse conversation modeling and non-parallel style transfer method by sharing a structured latent space to guide the decoding process.</p>
<p>Extractive Guidance Extractive guidance is often the important information extracted from the source input, which helps the model focus on the important parts of the input. Ghazvininejad et al. (2018) propose a knowledge-grounded conversation model, which extracts factual sentences from history dialog and utilizes them as factual guidance in the decoding process. Arun et al. (2020) extract tree-based meaning representations to improve the faithfulness of generated responses for task-oriented dialog systems. The extract structural knowledge efficiently guided the model to generate correct information. Rashkin et al. (2021) utilize control codes to encourage the model to generate responses that are faithful to the provided evidence. They apply three types of control codes, including entailment, objective voice and lexical precision, which are calculated during data pre-processing.  further construct fine-grained control codes by using lexical phrases as factual guidance. Based on these phrases, the generated responses are more relevant and faithful to the input.</p>
<p>Retrieved Guidance Retrieved guidance is usually from external knowledge. Dinan et al. (2019) create an open-domain dialogue dataset Wow, where each topic in the conversation is connected to Wikipedia articles. Then they design a memory network based dialog system which is enhanced by retrieved knowledge from Wikipedia. Augmented by knowledge guidance, their model is able to generate more precise responses. Shuster et al. (2021) propose a retrieval-augmented neural architectures, in which dialogues are generated grounded on retrieved knowledge. Specially, they apply a learnable retriever and designed a fine-grained interactions between history dialogue and knowledge.</p>
<p>Auxiliary Tasks</p>
<p>Many work utilize the auxiliary task of Natural Language Inference (NLI) to improve the factual consistency of dialogue systems. The main approaches include leveraging entailment scores to rerank candidate texts, or treating entailment as a reward for reinforcement learning, which are similar to the entailment-based methods for text summarization (described in Section 4.1.2).</p>
<p>Reranking-based Several works apply entailment score predicted by an NLI model to the reranking process for selecting more faithful generated text. As discussed in Section 3.1, several works (Nie et al., 2021;Qin et al., 2021;Song et al., 2020a;Welleck et al., 2019b) propose their factual evaluation metrics based on entailment. They utilize entailment scores predicted by their proposed metrics in the re-ranking process to improve faithfulness of dialogue models. For example, in Welleck et al. (2019b), given a persona P , previous utterances u &lt;=t , and the dialogue model outputs the score of a next-utterance candidate s i t+1 , the new score s re−rank t+1 after incorporating NLI relation is:
s re−rank t+1 = s t+1 + λs contradict t+1(15)
where s contradict t+1 is the highest contradiction score between s i t+1 and persona sentences in P , hyperparameter λ controls the NLI model's influence in re-ranking.</p>
<p>Reinforcement Learning Another type of methods incorporate entailment scores as a part of the reinforcement learning (RL) rewards, similar to Figure 8.  propose a RL-based model RCDG for generating persona consistent dialogues. Similar to the architecture of GANs (Generation Adversarial Neural Networks), RCG is composed of a generator and two evaluators to estimate the quality and consistency of generated utterances, respectively. The consistency evaluation is based on an NLI classifier to compute the entailment score. Mesgar et al. (2021) also propose an RL-based model TransferTransfo-RL for improving consistency between generated responses and personas. Differently, TransferTransfo-RL take the advantage of Actor-Critic (Mnih et al., 2016) learning approach, which also utilizes the entailment score as reward.</p>
<p>Learning Methods</p>
<p>As unfaithful generation relates to the deficiencies of training strategy, several works improve factual consistency of dialogue models by refining the training procedures. Li et al. (2020a) extend unlikelihood training to address various problems in generating dialogues, including over copying, repetitions, overuse frequent words, and factual inconsistency. Besides training with common maximum likelihood estimation (MLE), they apply unlikelihood loss (UL) to alleviate these problems. In the time step i during training, given an input-output pair (x, y), a dialogue model p θ and a set C containing sentences contradicting with y which the model should avoid to generate, the UL is defined as:
L U L (p θ , C, x, y) = − T t=1 yc∈C β(y c )(1 − p θ (y c |x, y &lt;t ))(16)
where β(y c ) is the weighting parameter for every y c ∈ C. Roller et al. (2021) followed this method for training a large-scale chatbot.</p>
<p>Constrained Decoding</p>
<p>Some works also focus on designing decoding strategies to improve consistency. Especially, these works apply constrained decoding during inference. These methods usually require the generated utterance to be semantically consistent with inputs based on certain semantic structure.</p>
<p>Balakrishnan et al. (2019) apply a tree-structured meaning representations (MR) in dialog systems.</p>
<p>Comparing to common flat MR, which is a flat list of key-value pairs, their MR is able to represent more fine-grained relations. Based on the proposed MR, they design a constrained decoding strategy on beam search which requires the MR of generated text not conflicting with the input. Similarly, Nye et al. (2021) propose a dual-system approach for faithful text generation, where the "system 1" is a common model for generation, and the "system 2" constrains and controls the generated sentences to be factually correct. The essence of this method is that it applies GPT-3 (Brown et al., 2020) to parse text into clear and correct logical symbols that are easy for "system 2" to check. During decoding, "system 2" selects correct candidates that are faithful to the given context.</p>
<p>Post-Editing</p>
<p>Several works focus on refining the generated dialogues without modifying the original model. These works mainly design an extra refining module to correct the factual errors in the generated dialogues. These models usually consist of three steps: generate, delete, and rewrite, similar to the summarization task shown in Figure 10. In the first step, the dialogue model normally generates utterances. In the second step, the rewrite module removes the incorrect contexts in the generated utterances, and then the third step rewrites them to the correct contexts. Song et al. (2020b) propose a post-editing based dialogue model, GDR, following the three steps introduced above. After the normal text generation procedure "Generate" in the first stage, GDR identifies and deletes conflict words in the second stage "Delete". Then GDR recovers the deleted words by a generation module in the last stage "Rewrite". Through the three stages above, GDR refines factual errors in the generated utterance. Dziri et al. (2021a) apply the similar strategy on knowledge grounded dialogue system. Different from GDR, their refining module NPH needs to rewrite based on knowledge graph. After deleting potential incorrect entities in the generated text, NPH based on graph neural network retrieves correct entities in the grounded knowledge graph for refining.</p>
<p>Other Methods</p>
<p>Kim et al. (2020) propose a dialogue system based on Rational Speech Act framework (Frank and Goodman, 2012), which enforces dialogue agents to refrain from uttering contradiction. The proposed model endows dialogue agents with public self-consciousness, helping them maintain consistency across each generation step by reflecting the distribution of imagined listeners across roles.</p>
<p>Faithfulness in Machine Translation</p>
<p>Neural machine translation (NMT) has achieved great success due to the ability to generate highquality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. The optimization methods for faithfulness in machine translation mainly include: incorporating auxiliary tasks like word alignment (4.3.1), improving learning methods like minimum risk training (4.3.2), utilizing constrained decoding methods like grid beam search (4.3.3), etc.</p>
<p>Auxiliary Tasks</p>
<p>Wang et al. (2020b) propose a multi-task learning paradigm with two auxilary tasks, including marked language model task and word alignment task, for building a faithfulness enhanced NMT (named FENMT). On the encoder side, FENMT employs a masked language model (MLM) task (Devlin et al., 2018) to infer the input words didn't be correctly translated. This task can enhance the ability of modeling the whole input sentence and give the decoder accurate and complete representations. On the decoder side, FENMT further uses a word alignment task to improve the alignment accuracy of the encoder decoder cross-attention to help the decoder to capture correct contextual representation. Furthermore, along with the NMT objective, an auxiliary max-margin objective based on contrastive learning is introduced in all decoding timesteps which prompts the decoder to translate fluent and faithful sentences.</p>
<p>To improve the ability of the decoder, Tu et al. (2017) propose to introduce a reconstruction loss to make translation can reconstruct the input sentence. Kong et al. (2019) propose to use a coverage difference ratio metric as a reward to train NMT. Feng et al. (2020); Garg et al. (2019);  propose to introduce word alignment information in Transformer to improve translation accuracy.</p>
<p>Learning Methods</p>
<p>Minimum Risk Training Wang and Sennrich (2020) hypothesise that exposure bias (Ranzato et al., 2015), a discrepancy between training and inference, is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Minimum Risk Training (MRT) is a sequence level objective that avoids this problem. Specifically, the objective function of MRT is the expected loss (risk) with respect to the posterior distribution:
R(θ) = (x,y)∈D y∈Y (x) P ( y|x; θ) ( y; y)(17)
in which the loss ( y; y) indicates the discrepancy between the gold translation y and the model prediction y. Due to the intractable search space, the posterior distribution Y (x) is approximated by a subspace S(x) by sampling a certain number of candidate translations, and normalizing:
P ( y|x; θ, α) = P ( y|x; θ) α y ∈S(x) P (y |x; θ) α(18)
where α is a hyper-parameter to control the sharpness of the subspace. Random sampling was used to generate candidate translations, and the reference translation was not added to the subspace. They find that Minimum Risk Training, which does not suffer from exposure bias, reduces the number of hallucinations substantially, and makes beam search with large beams more stable.</p>
<p>Adversarial Learning NMT models are still susceptible to input sentence perturbations and tend to produce hallucinatory outputs in the presence of some source perturbations . For example, Belinkov and Bisk (2017) find that NMT models can be immensely brittle to small perturbations applied to the inputs. Even if these perturbations are not strong enough to alter the meaning of an input sentence, they can nevertheless result in different and often incorrect translations. Belinkov and Bisk (2017) and Karpukhin et al. (2019) study how to use some synthetic noise and/or natural noise. Cheng et al. (2018) propose adversarial stability training to improve the robustness on arbitrary noise type including feature-level and word-level noise. Liu et al. (2018a) examine the homophonic noise for Chinese translation.</p>
<p>Formally, a set of adversarial examples Z(x; y) is generated with respect to a training sample (x; y) by solving an optimization problem:
{x |R(x , x) ≤ , argmax x J(x , y; θ)}(19)
where J(.) measures the possibility of a sample being adversarial, and R(x ; x) captures the degree of imperceptibility for a perturbation. Although it is difficult to give a precise definition of the degree of imperceptibility R(x ; x), l ∞ is usually used to bound the perturbations in image classification (Goodfellow et al., 2014). Robust Learning on Noisy Corpus Corpus-level noise in NMT parallel corpora tends to produce significant hallucinatory patterns (Raunak et al., 2021). NMT models also have a propensity to hallucinate more frequently under out-of-domain inputs (Müller et al., 2019). Several techniques can be used to improve learning robustness to corpus-level noise in NMT. Kang and Hashimoto (2020) propose a loss truncation method to reduce the impact of noisy references in sequence-tosequence training. Li et al. (2020b) propose a modification of expected risk minimization (ERM), namely Tilted-ERM, to reduce the effect of outliers during training. Corpus-level noise filtering that incorporating heuristics or filters (Junczys-Dowmunt, 2018;Zhang et al., 2020a) to remove invalid source-target pairs is also effective in reducing NMT hallucinations.</p>
<p>Constrained Decoding</p>
<p>One interesting method is lexically constrained decoding, a modification to beam search that allows the user to specify words and phrases that must appear in the system output. Three algorithms have been proposed for this: grid beam search (Hokamp and Liu, 2017), constrained beam search (Anderson et al., 2017) and dynamic beam allocation (Post and Vilar, 2018). These papers showed that these algorithms do a good job automatically placing constraints and improving results in tasks such as simulated post-editing, domain adaptation, and caption generation.</p>
<p>Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of prespecified lexical constraints while still taking advantage of the distribution learned from training data. The algorithm can be used with any model that generates a sequence y = y 0 , . . . , y T , by maximizing p(y|x) = t p(y t |x; y 0 , . . . , y t−1 ). Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate additional knowledge into a model's output without requiring any modification of the model parameters or training data, thus can improve faithfulness of translation output.</p>
<p>The computational complexities of grid beam search (Hokamp and Liu, 2017) are linear and constrained beam search are exponential in the number of constraints. Post and Vilar (2018) present a more efficient algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints.</p>
<p>Other Methods</p>
<p>Some work design special mechanism for using source representation more effectively. In the RNNbased NMT, Tu et al. (2016) and Mi et al. (2016) propose a coverage mechanism to improve the accuracy of translation outputs. Weng et al. (2020a) propose to model global representation in the source side to improve the source representation. Zheng et al. (2019) propose a capsule based module to control the source representation dynamically in the decoding process. Feng et al. (2020) propose a faithfulness part to optimize the contextual representation before feeding into the decoder. Weng et al. (2017) propose a bag-of-words loss to constrain decoding process.</p>
<p>Faithfulness in Data-to-Text Generation</p>
<p>Data-to-text generation (or table-to-text generation) has been widely studied for decades. Recently, deep neural networks have been successfully adopted in this task. However, the problem of unfaithfulness in data-to-text methods remains a significant challenge (especially in long-form text generation). In the data-to-document generation dataset (e.g., Rotowire (Wiseman et al., 2017) and MLB (Puduppully et al., 2019)), obvious gaps exist in the record generation (RG) precision between recent methods and faithful templated-based baselines. Moreover, the issue of generating unfaithful text remains a serious problem for noisy datasets constructed automatically (e.g., WikiBio (Lebret et al., 2016) and WikiPerson ). In WikiBio, almost two-thirds of the training instances contain unfaithful descriptions (Dhingra et al., 2019).</p>
<p>Factual Guidance</p>
<p>Compared to other text generation tasks, factual information is highly structured and exists explicitly in data-to-text tasks. Therefore the two-stage method, which plans the subset of input data to be described and then generates text from the plan, is popular in this research field. If a method explicitly considers the issue of faithfulness in the plan-to-text generation step, we categorize it into factual guidance optimization methods.  propose a two-stage table-to-text generation method SANA. SANA firstly constructs a skeleton using an autoregressive pointer network to select contents from the source table. SANA expands the skeleton to the final output in the second stage, considering the source table with an edit-based non-autoregressive model. Therefore, SANA could generate more faithful because the input of the second stage is directly extracted from the input table as strong factual guidance. Shen et al. (2020) model the table-to-text task as a segment-by-segment generation precedure and every segment is generated in two-stage. Firstly, proper data records are selected as factual guidance.</p>
<p>Secondly, text corresponding to the plan is generated by paying attention only to the selected input data records.</p>
<p>Auxilary Tasks</p>
<p>Liu et al. (2021a) extend the two-stage table-to-text generator to an augmented plan-based method. They create a pseudo training corpus for the plan-to-text phase, which covers all entities in the target description. In this way, there is no hallucinated entity in the training phase of the plan-to-text generating. Therefore the noise in the original corpus does not affect the plan-to-text generating step.</p>
<p>Learning methods</p>
<p>Neural sequence-to-sequence learning-based data-to-text models are often trained by maximum likelihood loss optimization. However, this kind of model suffers from the noise of the training data and leads to unfaithful output (also called divergence).  extend the maximum likelihood loss of attention-based Transformer with two additional losses. The first one is a latent matching disagreement loss which measures the distance between embeddings of the input table and the output text. The second one is an entity matching optimal-transport loss to measure the entity matching of the table and the output. Reinforcement learning (RL) based methods could directly optimize the faithfulness of data-to-text generation. Liu et al. (2019b) propose a force attention method to encourage the model to focus on uncovered data records. Then they adopt RL for information richness to generate more faithful descriptions for the input data. Rebuffel et al. (2020) propose an RL-based approach relying on the PARENT metric to reduce the issue of unfaithfulness.</p>
<p>Constrained Decoding</p>
<p>Tian et al. (2019) propose a confident decoding method to detect and avoid unfaithful generation in the decoder. For each decoder position, a confidence score consists of attention and a dedicated language model that only gives higher scores to common words. Therefore, the confidence score could be utilized to detect the generating of a word conveying source information without paying enough attention to the source data. Tian et al. (2019) believe that lower confidence scores indicate higher risks of generating unfaithful text. In the training time, a variational Bayes training framework is designed to ensure the model generates high confidence results. In the inference time, tokens generated with low confidence scores would be marked and skipped.</p>
<p>Filippova (2020) treats faithful data-to-text as a controllable text generation problem. In the training corpus, a prefix measuring the amount of noise is appended to the input sequence. Although datato-text models are trained without modification, we still categorize this method into constrained decoding because the controlling prefix guides the decoding. Rebuffel et al. (2022) extend the controllable text generation method to the fine-grained level. Word alignment labels are calculated through dependency parsing, and the labels guide the proposed weighted multi-branch neural decoder.</p>
<p>Other Methods</p>
<p>Considering that noise in the training corpus is critical to the faithfulness of data-to-text generation, pre-processing datasets is a direct and reasonable method. Nie et al. (2019) propose a neural data refinement method to reduce unaligned noise from original datasets. Wang (2019) observe that only 60% of the output contents in RotoWire could be grounded to the input records. They purify and enlarge the original dataset to a new RotoWire-FG dataset.</p>
<p>Faithfulness in Other NLG Tasks</p>
<p>Faithfulness is a common problem in NLG tasks. There are also some researches on faithfulness for other tasks, such as image caption, image-to-text radiology report generation and general factual language model. The methods for improving faithfulness include: factual guidance by knowledge graph, constrained decoding, and incorporating auxiliary tasks (e.g. entailment, word/entity alignment), etc.</p>
<p>Factual Language Model</p>
<p>Some work design language models that are conditioned on external, structured knowledge source to generate factual text. Logan IV et al. (2019) introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying information from an external knowledge graph. The KGLM maintains a dynamically growing local knowledge graph, a subset of the knowledge graph that contains entities that have already been mentioned in the text, and their related entities. When generating entity tokens, the model either decides to render a new entity that is  absent from the local graph, thereby growing the local knowledge graph, or to render a fact from the local graph. When rendering, the model combines the standard vocabulary with tokens available in the knowledge graph, thus supporting numbers, dates, and other rare tokens.</p>
<p>An example is shown in Figure 12. Initially, the graph is empty and the model uses the entity "Super Mario Land" to render the first three tokens, thus adding it and its relations to the local knowledge graph. After generating the next two tokens ("is", "a") using the standard language model, the model selects "Super Mario Land" as the parent entity, "Publication Date" as the relation to render, and copies one of the tokens of the date entity as the token ("1989" in this case). The factual completion capabilities of KGLM, which predicts the next word after a factual sentence (e.g., "Barack is married to"), is significantly more accurate. KGLM is able to generate accurate facts for rare entities, and can be controlled via modifications on the knowledge graph.</p>
<p>Factuality Detection</p>
<p>Factuality detection is an important task in practical applications. Hansen et al. (2020) build an ensemble learner that predicts news headline factuality using only eye-tracking measurements as they find that false headlines receive statistically significantly less visual attention than true headlines. Meng et al. (2020) propose a gradient-based adversarial training on transformer networks to the task of detecting check-worthy claims. Zhong et al. (2020) propose a graph-based reasoning approach utilizing factual structure of text for deepfake detection. Zellers et al. (2020) propose a controllable text generation model Grover to defend against fake news, which can generate fake news that are more trustworthy than human-written disinformation. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy versus 73% from best discriminators. As an important factuality detection task, fake news detection has been widely explored in the literature (Jain and Kasbe, 2018;Reis et al., 2019;Shu et al., 2017Shu et al., , 2019.</p>
<p>Constrained Text Generation</p>
<p>Generating text under specific lexical constraints is challenging, which can help generate more faithful and factual texts. Constrained text generation broadly falls into two categories, depending on whether inclusion of specified keywords in the output is mandatory.</p>
<p>In soft-constrained generation (Qin et al., 2019;Tang et al., 2019), keyword-text pairs are typically first constructed (sometimes along with other conditioning information), and a conditional text generation model is trained to capture their co-occurrence, so that the model learns to incorporate the constrained keywords into the generated text. While soft constrained models are easy to design, keywords are apt to be lost during generation, especially when multiple keywords must be included, or the keywords are less correlated. Soft enforcing algorithms such as attention and copy mechanisms (Bahdanau et al., 2014;Gu et al., 2016) can be helpful in preserving keywords, but do not guarantee that constraints will be included in the output sentence.</p>
<p>Hard-constrained generation (Hokamp and Liu, 2017;Miao et al., 2019;Post and Vilar, 2018;Welleck et al., 2019a), on the other hand, requires that all the lexical constraints be present in the output sentence. This approach typically involve sophisticated design of network architectures. Hokamp and Liu (2017) construct a lexical-constrained grid beam search decoding algorithm to incorporate constraints. However,  observe that a naive implementation of this algorithm has a high running time complexity. Miao et al. (2019) introduces a sampling-based conditional generation method, where the constraints are first placed in a template, then words in a random position are either inserted, deleted or updated under a Metropolis-Hastings-like scheme. However, individually sampling each token result in slow convergence, as the joint distribution of all the tokens in a sentence is highly correlated. Welleck et al. (2019a) propose a tree-based text generation scheme, where a token is first generated in an arbitrary position, and then the model recursively generates words to its left and right, yielding a binary tree. However, the constructed tree may not reflect the progressive hierarchy/granularity from high-level concepts to low-level details. Further, the time complexity of generating a sentence using this approach is O(n), like standard auto-regressive generation methods.  propose a novel non-autoregressive model for hard-constrained text generation, called POINTER (PrOgressive INsertion based TransformER). Given lexical constraints, POINTER first generates high-level words (e.g., informative nouns, verbs and adjectives) that bridge the keyword constraints, then these words are used as pivoting points at which to insert details of finer granularity. This process iterates until a sentence is finally completed by adding the least informative words (typically pronouns and prepositions). Anderson et al. (2017) uses constrained beam search to force the inclusion of selected tag words in image caption, and fixed, pre-trained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Constrained beam search is an approximate search algorithm capable of enforcing any constraints over resulting output sequences that can be expressed in a finite-state machine.</p>
<p>Image Caption</p>
<p>Image caption is an important type of multimodality-to-text generation. Image captioning models are prone to "hallucinating" objects that are not actually in a scene. Several examples are shown in Figure 13. The standard evaluation metrics only measure similarity to ground truth captions and cannot fully capture image relevance. Rohrbach et al. (2018) propose a image relevance metric to evaluate image captioning models with veridical visual labels and assess their rate of object hallucination. Anderson et al. (2017) use constrained beam search to force the inclusion of selected tag words in the output, and fixed pre-trained word embeddings to facilitate vocabulary expansion to previously unseen tag words.</p>
<p>Image-to-Text radiology report generation is a new type of image caption, which is an important application of natural language generation (NLG). It is to build assistive systems that take X-ray images of a patient and generate a textual report describing clinical observations in the images (Boag et al., 2020;Chen et al., 2020c;Jing et al., 2017;Li et al., 2018c;Liu et al., 2019a). This is a clinically important task, offering the potential to reduce radiologists' repetitive work and generally improve clinical communication (Kahn Jr et al., 2009). However, the automatic generated reports by neural models are not always factually complete or consistent. They usually face the issues of factual incompleteness and inconsistency (Boag et al., 2020;Liu et al., 2019a). Miura et al. (2021) show that existing image-to-text radiology report generation systems can be substantially improved by replacing widely used NLG metrics with simple alternatives. They propose two new simple rewards that can encourage the factual completeness and consistency of the generated reports. First, they propose the Exact Entity Match Reward (factENT) which captures the completeness of a generated report by measuring its coverage of entities in the radiology domain, compared with a reference report. The goal of the reward is to better capture disease and anatomical A man and a woman are playing with a frisbee.</p>
<p>A couple of cats laying on top of a bed.</p>
<p>A woman talking on a cell phone while sitting on a bench. A plate of food with broccoli and a fork. knowledge that are encoded in the entities. Second, they propose the Entailing Entity Match Reward (factENTNLI), which extends factENT with a natural language inference (NLI) model that further considers how inferentially consistent the generated entities are with their descriptions in the reference. They add NLI to control the overestimation of disease when optimizing towards factENT. They directly optimizes these two rewards with RL, showing that the proposed approach substantially improves performance on factual consistency and completeness.</p>
<p>Discussion</p>
<p>The faithfulness problem is the most critical challenge in modern NLG. As described above, we have discussed evaluation metrics and optimization methods for different NLG tasks. In the following, we discuss some of the limitations of current evaluation and optimization methods, and provide several research directions worth investigating in the general NLG.</p>
<p>Fine-grained and General Evaluation</p>
<p>Most existing faithfulness evaluation metrics measure the faithfulness of the generated text as a score, such as entailment score or QA matching score. They cannot distinguish between different types of factual errors and cannot locate specific error spans, which are detrimental to robust evaluation and repair. Moreover, most of them mainly focus on specific tasks, rather than be general to all NLG tasks. More fine-grained and general evaluation methods are needed to drive further developments in the field of NLG.</p>
<p>Intrinsic and Extrinsic Most of the existing evaluation metrics measure intrinsic and extrinsic factual errors as a unified metric without distinguishing them. We argue that intrinsic and extrinsic factual errors should be evaluated separately, as they have different definition and reference. The reference for the intrinsic error is the source text, however, the reference for the extrinsic error include the source text and world knowledge. As the extrinsic hallucinations contains both factual and non-factual information, it is necessary to distinguish them as the non-factual extrinsic hallucinations are harmful while the factual extrinsic hallucinations are usually beneficial for NLG tasks, such as dialogue generation. The field of fact checking is promising to help detect non-factual extrinsic hallucinations.</p>
<p>Fine-grained Error Types None of existing evaluation metrics can locate specific error spans and their fine-grained error types, such as predicate error, entity error, circumstance error or discourse link error. Fine-grained error types can help post-editing methods to fix unfaithful content and also provide richer insight to the researchers.</p>
<p>General Evaluation Existing evaluation metrics are mainly designed for specific tasks, such as text summarization or table-to-text generation. Although some of them can be applied to other NLG tasks, task-agnostic general metrics and evaluation benchmarks are lacking. Although the source and output texts of different tasks come in various forms, it is worth exploring the relationship between them and propose a general and fine-grained metric to evaluate faithfulness. Task-agnostic metrics with cross-domain robustness can help the research community to establish a unified benchmark, which is important and meaningful to help collaborate and standardize evaluation metrics for NLG tasks.</p>
<p>Reasoning-based Optimization</p>
<p>Most intrinsic errors are caused by misunderstanding the facts in the source context. Besides NLUbased pre-training on large scale corpus, to help models understand the facts correctly requires reasoning over the input context or world knowledge.</p>
<p>Numerical Reasoning The correctness of the numerals in the generated texts such as date, quantity and scalar are important for readers to get the correct information. As existing models usually model numerals in the same way as textual tokens, such as splitting numerals into sub-words or byte-pairs, they are more vulnerable to numerical errors. However, most of the existing optimization methods do not focus on the faithfulness of numerals. Tasks with quantities such as table-to-text generation, require numerical reasoning. Therefore, adding reasoning ability to numerical modeling is crucial for faithfulness optimization.</p>
<p>Grounded Language Representation The expressiveness of language generation models are vital to the faithfulness of the generated results. A model with poor input text representation will fail to do document-level understanding and inference. Language grounding is an active field aiming at enriching textual representations with visual information, which has been shown to improve performance on a variety of core NLP tasks (Baroni, 2016;Bruni et al., 2014;Kiela, 2017). Some recent work also propose unified-modal models for language understanding and generation UNIMO . Learning grounded language representation is a promising direction for improving the expressiveness of NLG models, thus improving the faithfulness of their generated texts.</p>
<p>Incorporating Causal Inference Existing language models are mostly correlation prediction models, where predictions are due to correlation rather than causal inference. Therefore, these correlational predictive models are not credible and may lead to errors in out-of-distribution or long-tailed situations. Clearly, the lack of causality is one of the main reasons for poor model generalization and faithfulness. Incorporating causal inference into language model may be the fundamental method to solve the faithfulness of NLG models.</p>
<p>Conclusion</p>
<p>In this survey, we conduct a systematic overview of the faithfulness problem across different NLG tasks. We organize and discuss the faithfulness analysis, evaluation metrics and optimization methods in a combined manner under a general categorization standard. The evaluation metrics for different NLG tasks are categorized into four types: Entailment-based, QA-based, Fact-based and Others. The optimization methods are categorized into six types: Factual Guidance, Auxiliary Task, Post-Editing, Learning Method, Constrained Decoding and Others. All the evaluation metrics and optimization methods are discussed and compared to facilitate both task-specific and task-agnostics understanding of the faithfulness problem. In addition, we propose potential future directions according to the challenges of this problem and the current research status of evaluation metrics and optimization methods.</p>
<p>Figure 5 :
5The framework of QA-based metrics.</p>
<p>Figure 7 :
7The framework of factual guidance. Usually, we use an oracle method to select guidance during training and use automatically extracted guidance at test time.additional inputs to the source document. Within this framework, the crucial points are what kind of information we need to feed into the model and how to feed it.</p>
<p>Figure 9 :
9Contrastive learning framework.</p>
<p>Figure 10 :
10The post-editing framework.</p>
<p>Roller et al., 2021), Balakrishnan et al. (2019) Nye et al. (2021), Kim et al. (2020) Persona Facts Li et al. (2016a), Zhang et al. (2018)</p>
<p>Song et al., 2020b), Kim et al. (2020) Unstructured Knowledge Rashkin et al. (2021), Wu et al. (2021b) (e.g. Wikipedia Documents) Dinan et al. (2019), Shuster et al. (2021)Structured KnowledgeKvBERT(Song et al., 2020a), CI-ToD(Qin et al., 2021) (e.g. Knowledge Graph) NPH(Dziri et al., 2021a) </p>
<p>Figure 11 :
11inject implicit speaker information into a LSTM-based model to improve the personality consistency. In each generation The framework of three types of factual guidance in dialogue generation.</p>
<p>Following
Ebrahimi et al. (2017);Goodfellow et al. (2014);Miyato et al. (2016), the white-box method to generate adversarial example stightly guided by the training loss. Given a parallel sentence pair (x; y), a set of adversarial examples A(x; y) specific to the NMT model are generated by:{x |R(x , x) ≤ , argmax x − log P (y|x ; θ)}(20)where they use the negative log translation probability to estimate J(.). The formula constructs adversarial examples that are expected to distort the current prediction and retain semantic similarity bounded by R. Cheng et al. (2019) propose a gradient-based adversarial learning approach, called AdvGen, to construct adversarial examples and use these examples to both attack as well as defend the NMT model to improving the robustness of NMT models, which consists of two parts: (1) attack the translation model with adversarial source examples;(2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs. For the generation of adversarial inputs, they propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs.</p>
<p>Figure 12 :
12An illustration example of the factual language model(Logan IV et al., 2019). It maintains a dynamically growing localized knowledge graph containing facts that are (possibly) conveyed in the sentence above. The graph is built by iteratively linking each detected entity to Wikidata, then adding any relations to previously mentioned entities.</p>
<p>Figure 13 :
13Several examples of image captioning with unfaithful object description.</p>
<p>Table 1 :
1Categories of common natural language generation tasks.Tasks 
Category 
Information Mapping </p>
<p>Text Summarization 
Text-to-Text 
Non-open-ended 
Machine Translation 
Text-to-Text 
Non-open-ended 
Sentence Simplication 
Text-to-Text 
Non-open-ended 
Paraphrase Generation 
Text-to-Text 
Non-open-ended 
Dialogue Generation 
Text-to-Text 
Open-ended 
Question Generation 
Text-to-Text 
Non-open-ended 
Story Generation 
Text-to-Text 
Open-ended 
Essay Generation 
Text-to-Text 
Open-ended 
News Generation 
Text-to-Text 
Open-ended 
Poetry Generation 
Text-to-Text 
Open-ended 
Table-to-Text Generation Data-to-Text 
Non-open-ended 
AMR-to-Text Generation Data-to-Text 
Non-open-ended 
Image Caption 
Multimodality-to-Text Non-open-ended 
Video Caption 
Multimodality-to-Text Non-open-ended 
Visual Storytelling 
Multimodality-to-Text Open-ended </p>
<p>Table 2 :
2Four paradigms in natural language generation.Paradigm 
Engineering 
Main Problems </p>
<p>Template-based </p>
<p>Manual Rules 
(Content Planning, Sentence Planning, 
Text Realization) </p>
<p>Fluency, Informativeness </p>
<p>Statistical-based 
Statistic Language Model 
(e.g. N-gram, Smoothing, Perplexity) 
Fluency, Informativeness </p>
<p>Neural-based 
Neural Architecture 
(e.g. RNN, LSTM, CNN, Transformer) 
Controllability, Faithfulness </p>
<p>Table 3 :
3Examples of unfaithful errors for several common NLG tasks. Red color denotes factual errors.</p>
<p>Table -
-Faithfulness in Non-open-ended NLGFor non-open NLG tasks, NLG models generate text based on the input which provides complete or even more information for the output text. The faithfulness problem in non-open NLG tasks is defined as whether the generated content is factually consistent with the input information, often referred to as factual consistency. For example, faithfulness in text summarization is whether the generated summary is factually consistent with and faithful to the input document. If the summary has hallucinations that not contained by the input document, then it's unfaithful to the input document. Similarly, faithfulness in machine translation is whether the translation is consistent with and faithful to the original language.to-Text 
Generation </p>
<p>Name: Frank Lino; Caption: FBI surveillance photo; Birth 
date: October 30, 1938; Birth place: Gravesend, Brooklyn, 
New York, United States; </p>
<p>Frank Lino (born 
October 30, 1938 
in Brooklyn) is an 
American criminal 
defense attorney. 
Text Summarization </p>
<p>Dialogue Generation </p>
<p>Machine Translation </p>
<p>Data-to-Text Generation </p>
<p>… </p>
<p>Metrics </p>
<p>Meta Evaluation </p>
<p>Story Generation </p>
<p>Fake News Detection </p>
<p>Visual storytelling </p>
<p>… </p>
<p>Knowledge Graph </p>
<p>Non-open-ended NLG 
Open-ended NLG </p>
<p>Factual Consistency 
Factual Correctness </p>
<p>Figure 2: The research framework on the faithfulness problem. </p>
<p>The faithfulness problem is widely existed in nearly all NLG tasks, such as text summarization, 
dialogue generation, machine translation and table-to-text generation. Unfaithful examples of several 
popular tasks are shown in Table 3. For the non-open-ended NLG tasks and open-ended NLG tasks, 
the definitions of faithfulness problem are different. </p>
<p>FRANK[132], NPH[39], etc. Data-to-Text Generation SANA [173], Segment[152], Entity [107]Faithfulness </p>
<p>Problem 
Analysis 2 </p>
<p>Definition&amp; 
Categorization </p>
<p>Challenges 
&amp;Issues </p>
<p>Model Analysis [18; 132], etc. </p>
<p>Evaluation Problem [43; 86] </p>
<p>Annotation Problem [45; 132] </p>
<p>Cause Analysis </p>
<p>Data Divergence [32; 117; 184] </p>
<p>Exposure Bais [117; 171], etc. </p>
<p>Poor Representation [114], etc. </p>
<p>Evaluation 
Metrics 3 </p>
<p>Meta Evaluation 
FRANK[132], SUMMAc[88], BEGIN[40], etc. </p>
<p>NLI-based Metrics 
DAE[60], FactCC[65], DialogNLI[178], etc. </p>
<p>QA-based Metrics 
QAGS[170], FEQA[65], Q 2 [69], QUALS[127] etc. </p>
<p>Fact-based Metrics </p>
<p>Entity 
EntityAlign[128], SimAlign[149] </p>
<p>N-gram 
PARENT[32], PARENT-T[175] </p>
<p>Relation 
TripleAlign[65], ArcsAlign[60] </p>
<p>Other Metrics 
BARTScore[190], COCO[187], TokenCLS[205] </p>
<p>Optimization 
Methods 4 </p>
<p>Factual 
Guidance </p>
<p>Abstractive 
Summarization </p>
<p>Keyword[36], Sentence[160], Relation[19] </p>
<p>Dialogue 
Generation </p>
<p>Inplicit[95], Extractive[57], Retrived[33] </p>
<p>Auxilary Tasks </p>
<p>Abstractive 
Summarization </p>
<p>Entailment[45; 94], QA[127], Others[200] </p>
<p>Dialogue 
Generation </p>
<p>NLI-reranking [178], NLI-RL [119; 156] </p>
<p>Data-to-Text 
Generation </p>
<p>EntityMatching [175], FocusAttn[106] </p>
<p>Machine 
Translation </p>
<p>FENMT [181], WordAlignment[46; 54; 194] </p>
<p>Post-Editing </p>
<p>Abstractive 
Summarization </p>
<p>SpanFact[35], FactCorrect[16], ContrastSel[23] </p>
<p>Dialogue 
Generation </p>
<p>GDR [157], NeuralPathHunter [39] 
Optimization 
Methods 4 </p>
<p>Learning 
Methods </p>
<p>Abstractive 
Summarization </p>
<p>CLIFF[18], CO2Sum[65], CLAPS[91] </p>
<p>Dialogue 
Generation </p>
<p>Unlikelihood learning [97; 148] </p>
<p>Machine 
Translation </p>
<p>MRT [171], Adversarial[11; 11; 26; 27; 79] </p>
<p>Constrained 
Decoding </p>
<p>Abstractive 
Summarization </p>
<p>CAS[116], FocusAttn[3], POINTER[199] </p>
<p>Dialogue 
Generation </p>
<p>Balakrishnan et al. [6], Nye et al. [131] </p>
<p>Machine 
Translation </p>
<p>GBS [67], CBS[2], DBA[135] </p>
<p>Data-to-Text 
Generation </p>
<p>ConfidentDecoding [163] </p>
<p>Other Methods </p>
<p>Abstractive 
Summarization </p>
<p>MoFE [28], HERMAN[202], ENTFA[17] </p>
<p>Table 4 :
4Hierarchical typology of unfaithful errors. Examples of text summarization are shown. The source input is the same as the first line inTable 3.To produce the vaccine, scientists have to show successful human trials, then sequence the DNA of the virus.Categorization 
Examples </p>
<p>Intrinsic 
Error </p>
<p>Semantic 
Frame 
Errors </p>
<p>Predicate Er-
ror (PredE) </p>
<p>The Ebola vaccine was rejected by the FDA in 
2019. </p>
<p>Entity Error 
(EntE) </p>
<p>Scientists say a vaccine for Ebola is unlikely to be 
ready this year. </p>
<p>Circumstance 
Error (CircE) </p>
<p>The first vaccine for Ebola was approved by the 
FDA in 2014. </p>
<p>Discourse 
Errors </p>
<p>Co-reference 
Error 
(CorefE) </p>
<p>The first vaccine for Ebola was approved in 2019. 
They say a vaccine for COVID-19 is unlikely to be 
ready this year. </p>
<p>Discourse 
Link 
Error 
(LinkE) </p>
<p>Extrinsic 
Error </p>
<p>Factual 
China has already started clinical trials of the 
COVID-19 vaccine. </p>
<p>Non-Factual 
China didn't start clinical trials of the COVID-19 
vaccine. </p>
<p>1 .
1Semantic Frame Errors capture factual errors in frame semantic and its core and non-core frame elements, including: • Predicate Error (PredE) denotes errors where the predicate is inconsistent with the source text; • Entity Error (EntE) denotes errors where the primary arguments (like entities) of the predicate are wrong or have the wrong attributes; • Circumstance Error (CircE) denotes errors where the arguments and predicates interact (e.g. location, time, manner, direction, modality) are wrong. 2. Discourse Errors capture erroneous links between discourse segments, including: • Co-reference Error (CorefE) denotes errors where pronouns and other types of references to previously mentioned entities either are incorrect or have no clear antecedents; • Discourse Link Error (LinkE) denotes incorrect discourse link between different statements. 3. Content Verifiable Errors capture erroneous information that cannot be verified against the source text, which are mainly caused by: • Out of Article Error (OutE) denotes information that cannot be deduced by from the original text (the same as extrinsic hallucinations</p>
<p>Table 5 :
5The ratio of unfaithful summaries annotated by human for different systems.System 
XSum 
CNN/DM </p>
<p>TransS2S 
96.9% 
74.8% </p>
<p>BERTSum 
83.7% 
27.2% </p>
<p>T5 
82.0% 
26.7% </p>
<p>BART 
66.7% 
24.7% </p>
<p>PEGASUS 
60.7% 
13.3% </p>
<p>Table 6 :
6The Person and Spearman correlation between different n-gram based metrics and human annotation of faithfulness on CNN/DM dataset, XSum dataset and their combination.All data 
CNN/DM 
XSum </p>
<p>Metrics 
Person 
Spearman 
Person 
Spearman 
Person 
Spearman </p>
<p>Table 7 :
7Categorization of Evaluation Metrics.Categories 
Methods 
Target Tasks </p>
<p>Entailment-based </p>
<p>DAE (Goyal and Durrett, 2021) 
Summarization, Paraphrasing </p>
<p>RankNLI (Falke et al., 2019) 
Summarization </p>
<p>SummaC (Laban et al., 2021b) 
Summarization </p>
<p>DialogNLI (Welleck et al., 2019c) 
Dialogue Generation </p>
<p>FactCC (Kryściński et al., 2019) 
Summarization </p>
<p>RCDG (Song et al., 2020c) 
Dialogue Generation </p>
<p>KvPI(Song et al., 2020a) 
Dialogue Generation </p>
<p>CI-ToD (Qin et al., 2021) 
Dialogue Generation </p>
<p>DECODE (Nie et al., 2021) 
Dialogue Generation </p>
<p>SentenceNLI (Mishra et al., 2021) 
Summarization </p>
<p>QA-based </p>
<p>QAGS (Wang et al., 2020a) 
Summarization </p>
<p>FEQA (Durmus et al., 2020) 
Summarization </p>
<p>QAFactEval (Fabbri et al., 2021a) 
Summarization </p>
<p>QuestEval (Scialom et al., 2021) 
Summarization </p>
<p>Q 2 (Honovich et al., 2021) 
Dialogue Generation </p>
<p>QUALS (Nan et al., 2021a) 
Summarization </p>
<p>Fact-based </p>
<p>SimAlign (Sabet et al., 2020) 
Machine Translation </p>
<p>EntityAlign (Nan et al., 2021b) 
Summarization </p>
<p>TripleAlign (Goodrich et al., 2019) Summarization </p>
<p>PARENT (Dhingra et al., 2019) 
Table-to-Text </p>
<p>PARENT-T (Wang et al., 2020b) 
Table-to-Text </p>
<p>Others </p>
<p>COCO (Xie et al., 2021) 
Summarization </p>
<p>TokenLevelCLS (Zhou et al., 2021) Machine Translation, Summarization </p>
<p>BARTScore (Yuan et al., 2021) 
16 NLG tasks </p>
<p>ShannonScore (Egan et al., 2021) 
Summarization </p>
<p>knowledge learned from large scale corpus over the provided input. Also, the dominant language 
model usually prompts the decoder generates common words to make sure outputs are fluent. </p>
<p>proposed a multi-domain dataset DECODE for evaluating consistency of dialogue. DECODE balances human-written contradicting dialogues with an equal number of non-contradicting dialogues from several public datasets. NLI models trained on DialogNLI or DECODE can be used for assessing the faithfulness of dialogues.Gupta et al. (2021) proposed a benchmark DialFact to evaluate and check knowledge errors in open domain dialog generation. They proposed a fact-checking pipeline for this benchmark, in which an NLI component is applied to check whether the generated utterance is supported by the retrieved evidences.Weak Supervision Because it is difficult to annotate a large-scale document-level NLI dataset, several recent works apply weakly supervised methods for training. Some work applied data augmentation methods to construct synthetic datasets.FactCC (Kryściński et al., 2019) constructed positive 
entailment samples by different heuristic methods for weak supervised training. For constructing 
various positive samples, they swap, deletes or insert textual units like entities, pronouns, numbers, etc. 
To better explain this metric, FactCC also provides an additional version, FactCCX, which highlights 
spans of evidence in source documents. Dziri et al. (2021b) introduced a new benchmark BEGIN 
for evaluating factual consistency of knowledge grounded dialogue systems. For the evaluation 
metric, they also proposed a NLI classifier by constructing adversarial samples similar to FactCC. 
SetenceNLI </p>
<p>Table 8 :
8Meta evaluations on several popular benchmarks. The results are Pearson correlations between different types of faithfulness evaluation metrics and human annotations.Metrics 
FRANK benchmark QAGS benchmark </p>
<p>leverage open information extraction and dependency parsing techniques to extractSummary 
Decoder </p>
<p>Auxiliary 
Classifier </p>
<p>Shared Encoder </p>
<p>Likelihood loss 
Auxiliary loss 
+ </p>
<p>Multi-task Learning </p>
<p>Summarization 
Model </p>
<p>Score Model </p>
<p>Generated 
Summary 
Reward </p>
<p>Reinforcement Learning </p>
<p>Reranking Model </p>
<p>Reranking </p>
<p>Candidate 
Summaries </p>
<p>Summary Result </p>
<p>[Super Mario Land] is a [1989] [side-scrolling] [platform video game] developed and published by [Nintendo] as a [launch title] for their [Game Boy] [handheld game console].Super Mario Land </p>
<p>Q647249 </p>
<p>Nintendo </p>
<p>Q8093 </p>
<p>21 April 1989 </p>
<p>Date </p>
<p>platform game </p>
<p>Q828322 </p>
<p>Side-scrolling video game </p>
<p>Q2281714 </p>
<p>Game Boy </p>
<p>Q186437 </p>
<p>andheld game console </p>
<p>Q941818 </p>
<p>launch game </p>
<p>Q1425505 </p>
<p>PUBLICATION 
DATE 
GENRE 
PLATFORM 
MANUFACTURER </p>
<p>INSTANCE 
OF </p>
<p>Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D Trippe, Juan B Gutierrez, Krys Kochut, arXiv:1707.02268Text summarization techniques: a brief survey. arXiv preprintMehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D Trippe, Juan B Gutierrez, and Krys Kochut. Text summarization techniques: a brief survey. arXiv preprint arXiv:1707.02268, 2017.</p>
<p>Guided Open Vocabulary Image Captioning with Constrained Beam Search. Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould, 10.18653/v1/D17-1098Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Guided Open Vocabulary Image Captioning with Constrained Beam Search. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 936-945, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1098. URL https: //aclanthology.org/D17-1098.</p>
<p>Focus attention: Promoting faithfulness and diversity in summarization. Rahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, Ryan T Mcdonald, 10.18653/v1/2021.acl-long.474Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Naviglithe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics12021Virtual EventRahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, and Ryan T. McDonald. Focus attention: Promoting faithfulness and diversity in summarization. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 6078-6095. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.474. URL https://doi.org/10.18653/v1/2021.acl-long.474.</p>
<p>Best practices for data-efficient modeling in NLG: how to train production-ready neural models with less data. Ankit Arun, Soumya Batra, Vikas Bhardwaj, Ashwini Challa, Pinar Donmez, Peyman Heidari, Hakan Inan, Shashank Jain, Anuj Kumar, Shawn Mei, Karthik Mohan, Michael White, 10.18653/v1/2020.coling-industry.7Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020 -Industry Track. Ann Clifton and Courtney Napolesthe 28th International Conference on Computational Linguistics, COLING 2020 -Industry TrackOnlineAnkit Arun, Soumya Batra, Vikas Bhardwaj, Ashwini Challa, Pinar Donmez, Peyman Heidari, Hakan Inan, Shashank Jain, Anuj Kumar, Shawn Mei, Karthik Mohan, and Michael White. Best practices for data-efficient modeling in NLG: how to train production-ready neural models with less data. In Ann Clifton and Courtney Napoles, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020 -Industry Track, Online, December 12, 2020, pages 64-77. International Committee on Computational Linguistics, 2020. doi: 10.18653/v1/2020. coling-industry.7. URL https://doi.org/10.18653/v1/2020.coling-industry.7.</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473arXiv preprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Constrained decoding for neural NLG from compositional representations in task-oriented dialogue. Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, Rajen Subba, 10.18653/v1/p19-1080Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Anna Korhonen, David R. Traum, and Lluís Màrquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, and Rajen Subba. Constrained decoding for neural NLG from compositional representations in task-oriented dialogue. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 831-844. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1080. URL https://doi.org/10.18653/v1/p19-1080.</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarizationSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72, 2005.</p>
<p>PLATO: pre-trained dialogue generation model with discrete latent variable. Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel RSiqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue generation model with discrete latent variable. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.</p>
<p>Tetreault, 10.18653/v1/2020.acl-main.9Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics20202020Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 85-96. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.9. URL https://doi.org/10.18653/v1/ 2020.acl-main.9.</p>
<p>Grounding distributional semantics in the visual world. Marco Baroni, Language and Linguistics Compass. 101Marco Baroni. Grounding distributional semantics in the visual world. Language and Linguistics Compass, 10(1):3-13, 2016.</p>
<p>Adversarial NLI for factual correctness in text summarisation models. CoRR, abs. Mario Barrantes, Benedikt Herudek, Richard Wang, Mario Barrantes, Benedikt Herudek, and Richard Wang. Adversarial NLI for factual correctness in text summarisation models. CoRR, abs/2005.11739, 2020. URL https://arxiv.org/abs/ 2005.11739.</p>
<p>Synthetic and natural noise both break neural machine translation. Yonatan Belinkov, Yonatan Bisk, arXiv:1711.02173arXiv preprintYonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine transla- tion. arXiv preprint arXiv:1711.02173, 2017.</p>
<p>Baselines for chest x-ray report generation. William Boag, Tzu-Ming Harry Hsu, Matthew Mcdermott, Gabriela Berner, Emily Alesentzer, Peter Szolovits, Machine Learning for Health Workshop. PMLRWilliam Boag, Tzu-Ming Harry Hsu, Matthew McDermott, Gabriela Berner, Emily Alesentzer, and Peter Szolovits. Baselines for chest x-ray report generation. In Machine Learning for Health Workshop, pages 126-140. PMLR, 2020.</p>
<p>Discourse analysis. Gillian Brown, D Gillian, Gillian R Brown, George Brown, Brown Yule, Gillian, Cambridge university pressGillian Brown, Gillian D Brown, Gillian R Brown, George Yule, and Brown Gillian. Discourse analysis. Cambridge university press, 1983.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Multimodal distributional semantics. Elia Bruni, Nam-Khanh Tran, Marco Baroni, Journal of artificial intelligence research. 49Elia Bruni, Nam-Khanh Tran, and Marco Baroni. Multimodal distributional semantics. Journal of artificial intelligence research, 49:1-47, 2014.</p>
<p>Factual error correction for abstractive summarization models. Meng Cao, Yue Dong, Jiapeng Wu, Jackie Chi Kit Cheung, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. Factual error correction for abstractive summarization models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6251-6258, 2020.</p>
<p>Meng Cao, Yue Dong, Jackie Chi Kit Cheung, arXiv:2109.09784arXiv: 2109.09784Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization. Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization. arXiv:2109.09784 [cs], August 2021. URL http://arxiv.org/ abs/2109.09784. arXiv: 2109.09784.</p>
<p>CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization. Shuyang Cao, Lu Wang, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsOnline and Punta CanaShuyang Cao and Lu Wang. CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633-6649, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology. org/2021.emnlp-main.532.</p>
<p>Faithful to the Original. Ziqiang Cao, Furu Wei, Wenjie Li, Sujian Li, arXiv:1711.04434arXiv: 1711.04434Fact Aware Neural Abstractive Summarization. Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. Faithful to the Original: Fact Aware Neural Abstractive Summarization. arXiv:1711.04434 [cs], November 2017. URL http://arxiv.org/ abs/1711.04434. arXiv: 1711.04434.</p>
<p>Retrieve, rerank and rewrite: Soft template based neural summarization. Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template based neural summarization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 152-161, 2018.</p>
<p>MOCHA: A dataset for training and evaluating generative reading comprehension metrics. Anthony Chen, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/2020.emnlp-main.528Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liuthe 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. MOCHA: A dataset for training and evaluating generative reading comprehension metrics. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6521-6532. Association for Computational Linguistics, 2020a. doi: 10.18653/v1/2020.emnlp-main.528. URL https://doi.org/10.18653/v1/2020.emnlp-main.528.</p>
<p>Enhanced LSTM for natural language inference. Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, Diana Inkpen, Proceedings of the 55th. Regina Barzilay and Min-Yen Kanthe 55thQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced LSTM for natural language inference. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th</p>
<p>Association for Computational Linguistics. 10.18653/v1/P17-1152Annual Meeting of the Association for Computational Linguistics. Vancouver, CanadaLong Papers1Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 1657-1668. Association for Computational Lin- guistics, 2017. doi: 10.18653/v1/P17-1152. URL https://doi.org/10.18653/v1/P17-1152.</p>
<p>Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection. Sihao Chen, Fan Zhang, Kazoo Sone, Dan Roth, 10.18653/v1/2021.naacl-main.475Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsSihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth. Improving Faithfulness in Abstractive Summa- rization with Contrast Candidate Generation and Selection. In Proceedings of the 2021 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5935-5941, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.475. URL https://aclanthology.org/2021. naacl-main.475.</p>
<p>Kgpt: Knowledge-grounded pre-training for data-to-text generation. Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang, arXiv:2010.02307arXiv preprintWenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. Kgpt: Knowledge-grounded pre-training for data-to-text generation. arXiv preprint arXiv:2010.02307, 2020b.</p>
<p>Generating radiology reports via memory-driven transformer. Zhihong Chen, Yan Song, Tsung-Hui Chang, Xiang Wan, arXiv:2010.16056arXiv preprintZhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating radiology reports via memory-driven transformer. arXiv preprint arXiv:2010.16056, 2020c.</p>
<p>Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai, Yang Liu, arXiv:1805.06130Towards robust neural machine translation. arXiv preprintYong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai, and Yang Liu. Towards robust neural machine translation. arXiv preprint arXiv:1805.06130, 2018.</p>
<p>Robust Neural Machine Translation with Doubly Adversarial Inputs. Yong Cheng, Lu Jiang, Wolfgang Macherey, 10.18653/v1/P19-1425Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsYong Cheng, Lu Jiang, and Wolfgang Macherey. Robust Neural Machine Translation with Doubly Adversarial Inputs. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics, pages 4324-4333, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1425. URL https://aclanthology.org/P19-1425.</p>
<p>Jesse Prafulla Kumar Choubey, Wenhao Vig, Nazneen Fatema Liu, Rajani, arXiv:2110.07166arXiv: 2110.07166MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization. Prafulla Kumar Choubey, Jesse Vig, Wenhao Liu, and Nazneen Fatema Rajani. MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization. arXiv:2110.07166 [cs], October 2021. URL http://arxiv.org/abs/2110.07166. arXiv: 2110.07166.</p>
<p>ELECTRA: pre-training text encoders as discriminators rather than generators. Kevin Clark, Minh-Thang Luong, Quoc V Le, Christopher D Manning, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pre-training text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.</p>
<p>Transforming question answering datasets into natural language inference datasets. CoRR, abs/1809.02922. Dorottya Demszky, Kelvin Guu, Percy Liang, Dorottya Demszky, Kelvin Guu, and Percy Liang. Transforming question answering datasets into natural language inference datasets. CoRR, abs/1809.02922, 2018. URL http://arxiv.org/ abs/1809.02922.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Handling Divergent Reference Texts when Evaluating Table-to-Text Generation. Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, William Cohen, 10.18653/v1/P19-1483Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Cohen. Handling Divergent Reference Texts when Evaluating Table-to-Text Generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4884-4895, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-1483. URL https://aclanthology.org/P19-1483.</p>
<p>Wizard of wikipedia: Knowledge-powered conversational agents. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.</p>
<p>Unified language model pre-training for natural language understanding and generation. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, Advances in Neural Information Processing Systems. 32Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Multi-fact correction in abstractive text summarization. Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, Jingjing Liu, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, and Jingjing Liu. Multi-fact correction in abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9320-9331, 2020.</p>
<p>Gsum: A general framework for guided neural abstractive summarization. Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. Gsum: A general framework for guided neural abstractive summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4830-4842, 2021.</p>
<p>Learning to ask: Neural question generation for reading comprehension. Xinya Du, Junru Shao, Claire Cardie, arXiv:1705.00106arXiv preprintXinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading comprehension. arXiv preprint arXiv:1705.00106, 2017.</p>
<p>FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization. Esin Durmus, He He, Mona Diab, 10.18653/v1/2020.acl-main.454arXiv:2005.03754Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsEsin Durmus, He He, and Mona Diab. FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055-5070, 2020. doi: 10.18653/v1/2020. acl-main.454. URL http://arxiv.org/abs/2005.03754. arXiv: 2005.03754.</p>
<p>Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. Nouha Dziri, Andrea Madotto, Osmar Zaïane, Avishek Joey Bose, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose. Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2197-2214, Online and Punta Cana, Dominican Republic, November 2021a. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-main.168.</p>
<p>Evaluating groundedness in dialogue systems: The BEGIN benchmark. CoRR, abs/2105.00071, 2021b. Nouha Dziri, Hannah Rashkin, Tal Linzen, David Reitter, Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. Evaluating groundedness in dialogue systems: The BEGIN benchmark. CoRR, abs/2105.00071, 2021b. URL https://arxiv.org/ abs/2105.00071.</p>
<p>Hotflip: White-box adversarial examples for text classification. Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou, arXiv:1712.06751arXiv preprintJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text classification. arXiv preprint arXiv:1712.06751, 2017.</p>
<p>Play the Shannon Game With Language Models: A Human-Free Approach to Summary Evaluation. Nicholas Egan, Oleg Vasilyev, John Bohannon, arXiv:2103.10918arXiv: 2103.10918Nicholas Egan, Oleg Vasilyev, and John Bohannon. Play the Shannon Game With Language Models: A Human-Free Approach to Summary Evaluation. arXiv:2103.10918 [cs], December 2021. URL http://arxiv.org/abs/2103.10918. arXiv: 2103.10918.</p>
<p>QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization. Alexander R Fabbri, Chien-Sheng Wu, Wenhao Liu, Caiming Xiong, arXiv:2112.08542arXiv: 2112.08542Alexander R. Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization. arXiv:2112.08542 [cs], December 2021a. URL http://arxiv.org/abs/2112.08542. arXiv: 2112.08542.</p>
<p>AnswerSumm: A Manually-Curated Dataset and Pipeline for Answer Summarization. Alexander R Fabbri, Xiaojian Wu, Srini Iyer, Haoran Li, Mona Diab, arXiv:2111.06474arXiv: 2111.06474Alexander R. Fabbri, Xiaojian Wu, Srini Iyer, Haoran Li, and Mona Diab. AnswerSumm: A Manually- Curated Dataset and Pipeline for Answer Summarization. arXiv:2111.06474 [cs], November 2021b. URL http://arxiv.org/abs/2111.06474. arXiv: 2111.06474.</p>
<p>Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference. Tobias Falke, Leonardo F R Ribeiro, Ido Prasetya Ajie Utama, Iryna Dagan, Gurevych, 10.18653/v1/P19-1213Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics, pages 2214-2220, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1213. URL https://aclanthology.org/P19-1213.</p>
<p>Modeling fluency and faithfulness for diverse neural machine translation. Yang Feng, Wanying Xie, Shuhao Gu, Chenze Shao, Wen Zhang, Zhengxin Yang, Dong Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Yang Feng, Wanying Xie, Shuhao Gu, Chenze Shao, Wen Zhang, Zhengxin Yang, and Dong Yu. Modeling fluency and faithfulness for diverse neural machine translation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 59-66, 2020.</p>
<p>Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. Katja Filippova, 10.18653/v1/2020.findings-emnlp.76Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsOnline, 2020Katja Filippova. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 864-870, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.76.</p>
<p>Frame semantics and the nature of language. J Charles, Fillmore, Annals of the New York Academy of Sciences: Conference on the origin and development of language and speech. New York280Charles J Fillmore et al. Frame semantics and the nature of language. In Annals of the New York Academy of Sciences: Conference on the origin and development of language and speech, volume 280, pages 20-32. New York, 1976.</p>
<p>Measuring nominal scale agreement among many raters. L Joseph, Fleiss, Psychological bulletin. 765378Joseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76 (5):378, 1971.</p>
<p>Predicting pragmatic reasoning in language games. C Michael, Frank, Noah D Goodman, Science. 3366084Michael C Frank and Noah D Goodman. Predicting pragmatic reasoning in language games. Science, 336(6084):998-998, 2012.</p>
<p>Discourse understanding and factual consistency in abstractive summarization. Saadia Gabriel, Antoine Bosselut, Jeff Da, Ari Holtzman, Jan Buys, Kyle Lo, Asli Celikyilmaz, Yejin Choi, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeSaadia Gabriel, Antoine Bosselut, Jeff Da, Ari Holtzman, Jan Buys, Kyle Lo, Asli Celikyilmaz, and Yejin Choi. Discourse understanding and factual consistency in abstractive summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 435-447, 2021a.</p>
<p>GO FIGURE: A Meta Evaluation of Factuality in Summarization. Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao, 10.18653/v1/2021.findings-acl.42Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsSaadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. GO FIGURE: A Meta Evaluation of Factuality in Summarization. In Findings of the Association for Com- putational Linguistics: ACL-IJCNLP 2021, pages 478-487, Online, August 2021b. Associ- ation for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.42. URL https: //aclanthology.org/2021.findings-acl.42.</p>
<p>Structuring latent spaces for stylized response generation. Xiang Gao, Yizhe Zhang, Sungjin Lee, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, 10.18653/v1/D19-1190Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wanthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsXiang Gao, Yizhe Zhang, Sungjin Lee, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. Structuring latent spaces for stylized response generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 1814-1823. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1190. URL https: //doi.org/10.18653/v1/D19-1190.</p>
<p>Jointly learning to align and translate with transformer models. Sarthak Garg, Stephan Peitz, arXiv:1909.02074arXiv preprintUdhyakumar Nallasamy, and Matthias PaulikSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. Jointly learning to align and translate with transformer models. arXiv preprint arXiv:1909.02074, 2019.</p>
<p>Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Albert Gatt, Emiel Krahmer, Journal of Artificial Intelligence Research. 61Albert Gatt and Emiel Krahmer. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artificial Intelligence Research, 61:65-170, 2018.</p>
<p>The GEM benchmark: Natural language generation. Sebastian Gehrmann, P Tosin, Karmanya Adewumi, Pawan Aggarwal, Aremu Sasanka Ammanamanchi, Antoine Anuoluwapo, Bosselut, Miruna-Adriana Khyathi Raghavi Chandu, Dipanjan Clinciu, Kaustubh D Das, Wanyu Dhole, Esin Du, Ondrej Durmus, Chris Dusek, Varun Emezue, Cristina Gangal, Tatsunori Garbacea, Yufang Hashimoto, Yacine Hou, Harsh Jernite, Yangfeng Jhamtani, Shailza Ji, Dhruv Jolly, Faisal Kumar, Aman Ladhak, Mounica Madaan, Khyati Maddela, Saad Mahajan, Mahamood, abs/2102.01672Bodhisattwa Prasad Majumder. Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo, Salomey Osei, Ankur P. Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei ZhouSebastian Gehrmann, Tosin P. Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Di- panjan Das, Kaustubh D. Dhole, Wanyu Du, Esin Durmus, Ondrej Dusek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo, Salomey Osei, Ankur P. Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hen- drik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics. CoRR, abs/2102.01672, 2021. URL https://arxiv.org/abs/2102.01672.</p>
<p>A knowledge-grounded neural conversation model. Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-Tau Yih, Michel Galley, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). Sheila A. McIlraith and Kilian Q. Weinbergerthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5110-5117. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.</p>
<p>J Ian, Goodfellow, arXiv:1412.6572Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprintIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.</p>
<p>Assessing The Factual Accuracy of Generated Text. Ben Goodrich, Vinay Rao, Mohammad Saleh, Peter J Liu, 10.1145/3292500.3330955arXiv:1905.13322Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningBen Goodrich, Vinay Rao, Mohammad Saleh, and Peter J. Liu. Assessing The Factual Accuracy of Generated Text. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 166-175, July 2019. doi: 10.1145/3292500.3330955. URL http://arxiv.org/abs/1905.13322. arXiv: 1905.13322.</p>
<p>Evaluating Factuality in Generation with Dependency-level Entailment. Tanya Goyal, Greg Durrett, 10.18653/v1/2020.findings-emnlp.322Findings of the Association for Computational Linguistics: EMNLP 2020. OnlineAssociation for Computational LinguisticsTanya Goyal and Greg Durrett. Evaluating Factuality in Generation with Dependency-level Entailment. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592-3603, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. findings-emnlp.322. URL https://aclanthology.org/2020.findings-emnlp.322.</p>
<p>Annotating and Modeling Fine-grained Factuality in Summarization. Tanya Goyal, Greg Durrett, 10.18653/v1/2021.naacl-main.114Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsTanya Goyal and Greg Durrett. Annotating and Modeling Fine-grained Factuality in Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1449-1462, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.114. URL https://aclanthology.org/2021.naacl-main.114.</p>
<p>Generating sequences with recurrent neural networks. Alex Graves, arXiv:1308.0850arXiv preprintAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.</p>
<p>Incorporating copying mechanism in sequence-to-sequence learning. Jiatao Gu, Zhengdong Lu, Hang Li, O K Victor, Li, arXiv:1603.06393arXiv preprintJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying mechanism in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393, 2016.</p>
<p>Dialfact: A benchmark for fact-checking in dialogue. CoRR, abs/2110.08222. Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, Caiming Xiong, Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. Dialfact: A benchmark for fact-checking in dialogue. CoRR, abs/2110.08222, 2021. URL https://arxiv.org/abs/2110. 08222.</p>
<p>Estimate and replace: A novel approach to integrating deep neural networks with existing applications. Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, Alon Jacovi, arXiv:1804.09028arXiv preprintGuy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon Jacovi. Estimate and replace: A novel approach to integrating deep neural networks with existing applications. arXiv preprint arXiv:1804.09028, 2018.</p>
<p>Factuality checking in news headlines with eye tracking. Christian Hansen, Casper Hansen, Jakob Grue Simonsen, Birger Larsen, Stephen Alstrup, Christina Lioma, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalChristian Hansen, Casper Hansen, Jakob Grue Simonsen, Birger Larsen, Stephen Alstrup, and Christina Lioma. Factuality checking in news headlines with eye tracking. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2013-2016, 2020.</p>
<p>Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search. Chris Hokamp, Qun Liu, arXiv:1704.07138arXiv: 1704.07138Chris Hokamp and Qun Liu. Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search. arXiv:1704.07138 [cs], May 2017. URL http://arxiv.org/abs/1704.07138. arXiv: 1704.07138.</p>
<p>spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. Matthew Honnibal, Ines Montani, 7To appearMatthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom em- beddings, convolutional neural networks and incremental parsing. To appear, 7(1):411-420, 2017.</p>
<p>$qˆ2$: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, Omri Abend, 10.18653/v1/2021.emnlp-main.619Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yihthe 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics20212021Virtual Event / Punta CanaOr Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. $qˆ2$: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 7856-7870. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021. emnlp-main.619. URL https://doi.org/10.18653/v1/2021.emnlp-main.619.</p>
<p>Improved lexically constrained decoding for translation and monolingual rewriting. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, Benjamin Van Durme, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1J Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. Improved lexically constrained decoding for translation and monolingual rewriting. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 839-850, 2019.</p>
<p>Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward. Luyang Huang, Lingfei Wu, Lu Wang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsLuyang Huang, Lingfei Wu, and Lu Wang. Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5094-5107, 2020.</p>
<p>Dhruv Batra, et al. Visual storytelling. Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesTing-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1233-1239, 2016.</p>
<p>Fake news detection. Akshay Jain, Amey Kasbe, 2018 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS). IEEEAkshay Jain and Amey Kasbe. Fake news detection. In 2018 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS), pages 1-5. IEEE, 2018.</p>
<p>On the automatic generation of medical imaging reports. Baoyu Jing, Pengtao Xie, Eric Xing, arXiv:1711.08195arXiv preprintBaoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. arXiv preprint arXiv:1711.08195, 2017.</p>
<p>Dual conditional cross-entropy filtering of noisy parallel corpora. Marcin Junczys-Dowmunt, arXiv:1809.00197arXiv preprintMarcin Junczys-Dowmunt. Dual conditional cross-entropy filtering of noisy parallel corpora. arXiv preprint arXiv:1809.00197, 2018.</p>
<p>Toward best practices in radiology reporting. Curtis P Charles E KahnJr, Elizabeth S Langlotz, John A Burnside, Carrino, S David, Channin, M David, Daniel L Hovsepian, Rubin, Radiology. 2523Charles E Kahn Jr, Curtis P Langlotz, Elizabeth S Burnside, John A Carrino, David S Channin, David M Hovsepian, and Daniel L Rubin. Toward best practices in radiology reporting. Radiology, 252(3):852-856, 2009.</p>
<p>A convolutional neural network for modelling sentences. Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom, arXiv:1404.2188arXiv preprintNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188, 2014.</p>
<p>Improved natural language generation via loss truncation. Daniel Kang, Tatsunori Hashimoto, arXiv:2004.14589arXiv preprintDaniel Kang and Tatsunori Hashimoto. Improved natural language generation via loss truncation. arXiv preprint arXiv:2004.14589, 2020.</p>
<p>Training on synthetic noise improves robustness to natural noise in machine translation. Vladimir Karpukhin, Omer Levy, Jacob Eisenstein, Marjan Ghazvininejad, arXiv:1902.01509arXiv preprintVladimir Karpukhin, Omer Levy, Jacob Eisenstein, and Marjan Ghazvininejad. Training on synthetic noise improves robustness to natural noise in machine translation. arXiv preprint arXiv:1902.01509, 2019.</p>
<p>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, Minlie Huang, arXiv:2106.10502arXiv preprintPei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, and Minlie Huang. Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. arXiv preprint arXiv:2106.10502, 2021.</p>
<p>Deep embodiment: grounding semantics in perceptual modalities. Douwe Kiela, University of Cambridge, Computer LaboratoryTechnical reportDouwe Kiela. Deep embodiment: grounding semantics in perceptual modalities. Technical report, University of Cambridge, Computer Laboratory, 2017.</p>
<p>Will I sound like me? improving persona consistency in dialogues through pragmatic self-consciousness. Hyunwoo Kim, Byeongchang Kim, Gunhee Kim, 10.18653/v1/2020.emnlp-main.65Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liuthe 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics20202020Hyunwoo Kim, Byeongchang Kim, and Gunhee Kim. Will I sound like me? improving persona consistency in dialogues through pragmatic self-consciousness. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 904-916. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.65. URL https://doi.org/10.18653/v1/2020.emnlp-main.65.</p>
<p>Statistical machine translation. Philipp Koehn, Cambridge University PressPhilipp Koehn. Statistical machine translation. Cambridge University Press, 2009.</p>
<p>Neural machine translation with adequacy-oriented learning. Xiang Kong, Zhaopeng Tu, Shuming Shi, Eduard Hovy, Tong Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Xiang Kong, Zhaopeng Tu, Shuming Shi, Eduard Hovy, and Tong Zhang. Neural machine translation with adequacy-oriented learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6618-6625, 2019.</p>
<p>FFCI: A Framework for Interpretable Automatic Evaluation of Summarization. Fajri Koto, Timothy Baldwin, Jey Han Lau, arXiv:2011.13662arXiv: 2011.13662Fajri Koto, Timothy Baldwin, and Jey Han Lau. FFCI: A Framework for Interpretable Automatic Evaluation of Summarization. arXiv:2011.13662 [cs], July 2021. URL http://arxiv.org/ abs/2011.13662. arXiv: 2011.13662.</p>
<p>Evaluating the Factual Consistency of Abstractive Text Summarization. Wojciech Kryściński, Bryan Mccann, Caiming Xiong, Richard Socher, arXiv:1910.12840arXiv: 1910.12840Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the Factual Consistency of Abstractive Text Summarization. arXiv:1910.12840 [cs], October 2019. URL http://arxiv.org/abs/1910.12840. arXiv: 1910.12840.</p>
<p>Summac: Re-visiting nli-based models for inconsistency detection in summarization. Philippe Laban, Tobias Schnabel, Paul N Bennett, Marti A Hearst, abs/2111.09525Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. Summac: Re-visiting nli-based models for inconsistency detection in summarization. CoRR, abs/2111.09525, 2021a. URL https://arxiv.org/abs/2111.09525.</p>
<p>SummaC: Re-Visiting NLIbased Models for Inconsistency Detection in Summarization. Philippe Laban, Tobias Schnabel, Paul N Bennett, Marti A Hearst, arXiv:2111.09525arXiv: 2111.09525Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. SummaC: Re-Visiting NLI- based Models for Inconsistency Detection in Summarization. arXiv:2111.09525 [cs], November 2021b. URL http://arxiv.org/abs/2111.09525. arXiv: 2111.09525.</p>
<p>Neural Text Generation from Structured Data with Application to the Biography Domain. Rémi Lebret, David Grangier, Michael Auli, 10.18653/v1/D16-1128Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsRémi Lebret, David Grangier, and Michael Auli. Neural Text Generation from Structured Data with Application to the Biography Domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1128.</p>
<p>Hallucinations in neural machine translation. Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, David Sussillo, Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, and David Sussillo. Hallucinations in neural machine translation. 2018.</p>
<p>. Seanie Lee, Dong Bok Lee, Sung Ju Hwang, Contrastive, With Adver-Sarial Per-Turbations For, Conditional, Generation, 2021Seanie Lee, Dong Bok Lee, and Sung Ju Hwang. CONTRASTIVE LEARNING WITH ADVER- SARIAL PER-TURBATIONS FOR CONDITIONAL TEXT GENERATION. page 25, 2021.</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461arXiv preprintMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.</p>
<p>Guiding generation for abstractive text summarization based on key information guide network. Chenliang Li, Weiran Xu, Si Li, Sheng Gao, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2Short PapersChenliang Li, Weiran Xu, Si Li, and Sheng Gao. Guiding generation for abstractive text summa- rization based on key information guide network. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 55-60, 2018a.</p>
<p>Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization. Haoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsHaoran Li, Junnan Zhu, Jiajun Zhang, and Chengqing Zong. Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1430-1441, Santa Fe, New Mexico, USA, August 2018b. Association for Computational Linguistics. URL https: //aclanthology.org/C18-1121.</p>
<p>Long Papers. The Association for Computer Linguistics. Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao, William B Dolan, 10.18653/v1/p16-1094Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016Berlin, Germany1A persona-based neural conversation modelJiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, and William B. Dolan. A persona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016a. doi: 10.18653/v1/ p16-1094. URL https://doi.org/10.18653/v1/p16-1094.</p>
<p>Deep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky, arXiv:1606.01541arXiv preprintJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforce- ment learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016b.</p>
<p>Don't say that! making inconsistent dialogue unlikely with unlikelihood training. Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, Jason Weston, Proceedings of the 58th. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreaultthe 58thMargaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, and Jason Weston. Don't say that! making inconsistent dialogue unlikely with unlikelihood training. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th</p>
<p>Online. 10.18653/v1/2020.acl-main.428Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics2020Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4715-4728. Association for Computational Linguistics, 2020a. doi: 10.18653/v1/ 2020.acl-main.428. URL https://doi.org/10.18653/v1/2020.acl-main.428.</p>
<p>Tilted empirical risk minimization. Tian Li, Ahmad Beirami, Maziar Sanjabi, Virginia Smith, arXiv:2007.01162arXiv preprintTian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical risk minimization. arXiv preprint arXiv:2007.01162, 2020b.</p>
<p>Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang, arXiv:2012.15409arXiv preprintWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409, 2020c.</p>
<p>Hybrid retrieval-generation reinforced agent for medical image report generation. Yuan Li, Xiaodan Liang, Zhiting Hu, Eric P Xing, Advances in neural information processing systems. 31Yuan Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. Hybrid retrieval-generation reinforced agent for medical image report generation. Advances in neural information processing systems, 31, 2018c.</p>
<p>Paraphrase generation with deep reinforcement learning. Zichao Li, Xin Jiang, Lifeng Shang, Hang Li, arXiv:1711.00279arXiv preprintZichao Li, Xin Jiang, Lifeng Shang, and Hang Li. Paraphrase generation with deep reinforcement learning. arXiv preprint arXiv:1711.00279, 2017.</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81, 2004.</p>
<p>Clinically accurate chest x-ray report generation. Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew Mcdermott, Willie Boag, Wei-Hung Weng, Peter Szolovits, Marzyeh Ghassemi, Machine Learning for Healthcare Conference. PMLRGuanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi. Clinically accurate chest x-ray report generation. In Machine Learning for Healthcare Conference, pages 249-269. PMLR, 2019a.</p>
<p>Robust neural machine translation with joint textual and phonetic embedding. Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, Zhongjun He, arXiv:1810.06729arXiv preprintHairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. arXiv preprint arXiv:1810.06729, 2018a.</p>
<p>Table-to-text generation by structure-aware seq2seq learning. Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, Zhifang Sui, Thirty-Second AAAI Conference on Artificial Intelligence. Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang Sui. Table-to-text generation by structure-aware seq2seq learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018b.</p>
<p>Towards Comprehensive Description Generation from Factual Attribute-value Tables. Tianyu Liu, Fuli Luo, Pengcheng Yang, Wei Wu, Baobao Chang, Zhifang Sui, 10.18653/v1/P19-1600Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsTianyu Liu, Fuli Luo, Pengcheng Yang, Wei Wu, Baobao Chang, and Zhifang Sui. Towards Comprehensive Description Generation from Factual Attribute-value Tables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5985-5996, Florence, Italy, 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1600.</p>
<p>Towards Faithfulness in Open Domain Table-to-text Generation from an Entity-centric View. Tianyu Liu, Xin Zheng, Baobao Chang, Zhifang Sui, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Tianyu Liu, Xin Zheng, Baobao Chang, and Zhifang Sui. Towards Faithfulness in Open Domain Table-to-text Generation from an Entity-centric View. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13415-13423, May 2021a.</p>
<p>Co2sum: Contrastive learning for factual-consistent abstractive summarization. Wei Liu, Huanqin Wu, Wenjing Mu, Zhen Li, Tao Chen, Dan Nie, arXiv:2112.01147arXiv preprintWei Liu, Huanqin Wu, Wenjing Mu, Zhen Li, Tao Chen, and Dan Nie. Co2sum: Contrastive learning for factual-consistent abstractive summarization. arXiv preprint arXiv:2112.01147, 2021b.</p>
<p>Text summarization with pretrained encoders. Yang Liu, Mirella Lapata, arXiv:1908.08345arXiv preprintYang Liu and Mirella Lapata. Text summarization with pretrained encoders. arXiv preprint arXiv:1908.08345, 2019.</p>
<p>Yang Liu, Yifei Sun, Vincent Gao, arXiv:2106.16188arXiv: 2106.16188Improving Factual Consistency of Abstractive Summarization on Customer Feedback. Yang Liu, Yifei Sun, and Vincent Gao. Improving Factual Consistency of Abstractive Summarization on Customer Feedback. arXiv:2106.16188 [cs], June 2021c. URL http://arxiv.org/abs/ 2106.16188. arXiv: 2106.16188.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019c.</p>
<p>Multilingual denoising pre-training for neural machine translation. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer, Transactions of the Association for Computational Linguistics. 8Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742, 2020.</p>
<p>Barack's wife hillary: Using knowledge-graphs for fact-aware language modeling. I V Robert L Logan, Nelson F Liu, Matthew E Peters, Matt Gardner, Sameer Singh, arXiv:1906.07241arXiv preprintRobert L Logan IV, Nelson F Liu, Matthew E Peters, Matt Gardner, and Sameer Singh. Barack's wife hillary: Using knowledge-graphs for fact-aware language modeling. arXiv preprint arXiv:1906.07241, 2019.</p>
<p>Entity-based knowledge conflicts in question answering. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris Dubois, Sameer Singh, arXiv:2109.05052arXiv preprintShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. arXiv preprint arXiv:2109.05052, 2021.</p>
<p>A user attention model for video summarization. Yu-Fei Ma, Lie Lu, Hong-Jiang Zhang, Mingjing Li, Proceedings of the tenth ACM international conference on Multimedia. the tenth ACM international conference on MultimediaYu-Fei Ma, Lie Lu, Hong-Jiang Zhang, and Mingjing Li. A user attention model for video summa- rization. In Proceedings of the tenth ACM international conference on Multimedia, pages 533-542, 2002.</p>
<p>Constrained abstractive summarization: Preserving factual consistency with constrained generation. Yuning Mao, Xiang Ren, Ji Heng, Jiawei Han, arXiv:2010.12723arXiv preprintYuning Mao, Xiang Ren, Heng Ji, and Jiawei Han. Constrained abstractive summarization: Preserving factual consistency with constrained generation. arXiv preprint arXiv:2010.12723, 2020.</p>
<p>On Faithfulness and Factuality in Abstractive Summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, 10.18653/v1/2020.acl-main.173Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On Faithfulness and Factuality in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online, July 2020. Asso- ciation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.173. URL https: //www.aclweb.org/anthology/2020.acl-main.173.</p>
<p>Gradient-based adversarial training on transformer networks for detecting check-worthy factual claims. Kevin Meng, Damian Jimenez, Fatma Arslan, Jacob Daniel Devasier, Daniel Obembe, Chengkai Li, arXiv:2002.07725arXiv preprintKevin Meng, Damian Jimenez, Fatma Arslan, Jacob Daniel Devasier, Daniel Obembe, and Chengkai Li. Gradient-based adversarial training on transformer networks for detecting check-worthy factual claims. arXiv preprint arXiv:2002.07725, 2020.</p>
<p>Improving Factual Consistency Between a Response and Persona Facts. Mohsen Mesgar, Edwin Simpson, Iryna Gurevych, 10.18653/v1/2021.eacl-main.44Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeAssociation for Computational LinguisticsMohsen Mesgar, Edwin Simpson, and Iryna Gurevych. Improving Factual Consistency Between a Response and Persona Facts. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 549-562, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.44. URL https://aclanthology.org/2021.eacl-main.44.</p>
<p>Coverage embedding models for neural machine translation. Haitao Mi, Zhiguo Baskaran Sankaran, Abe Wang, Ittycheriah, arXiv:1605.03148arXiv preprintHaitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. Coverage embedding models for neural machine translation. arXiv preprint arXiv:1605.03148, 2016.</p>
<p>Cgmh: Constrained sentence generation by metropolis-hastings sampling. Ning Miao, Hao Zhou, Lili Mou, Rui Yan, Lei Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. Cgmh: Constrained sentence generation by metropolis-hastings sampling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6834-6842, 2019.</p>
<p>Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization. Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Lorraine Xiang, Pavan Li, Kartik Kapanipathi, Talamadupula, 10.18653/v1/2021.naacl-main.104Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsAnshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, and Kartik Talamadupula. Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1322-1336, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.104. URL https://aclanthology.org/2021.naacl-main.104.</p>
<p>Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation. Yasuhide Miura, Yuhao Zhang, Emily Bao Tsai, Curtis P Langlotz, Dan Jurafsky, arXiv:2010.10042arXiv: 2010.10042Yasuhide Miura, Yuhao Zhang, Emily Bao Tsai, Curtis P. Langlotz, and Dan Jurafsky. Im- proving Factual Completeness and Consistency of Image-to-Text Radiology Report Genera- tion. arXiv:2010.10042 [cs], April 2021. URL http://arxiv.org/abs/2010.10042. arXiv: 2010.10042.</p>
<p>Adversarial training methods for semi-supervised text classification. Takeru Miyato, M Andrew, Ian Dai, Goodfellow, arXiv:1605.07725arXiv preprintTakeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. arXiv preprint arXiv:1605.07725, 2016.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Proceedings of the 33nd International Conference on Machine Learning. Maria-Florina Balcan and Kilian Q. Weinbergerthe 33nd International Conference on Machine LearningNew York City, NY, USA48Workshop and Conference ProceedingsVolodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 1928-1937. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/mniha16.html.</p>
<p>Domain robustness in neural machine translation. Mathias Müller, Annette Rios, Rico Sennrich, arXiv:1911.03109arXiv preprintMathias Müller, Annette Rios, and Rico Sennrich. Domain robustness in neural machine translation. arXiv preprint arXiv:1911.03109, 2019.</p>
<p>Improving factual consistency of abstractive summarization via question answering. Feng Nan, Cícero Nogueira, Henghui Santos, Patrick Zhu, Kathleen R Ng, Ramesh Mckeown, Dejiao Nallapati, Zhiguo Zhang, Andrew O Wang, Bing Arnold, Xiang, 10.18653/v1/2021.acl-long.536Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Naviglithe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics1Virtual EventFeng Nan, Cícero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen R. McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and Bing Xiang. Improving factual consistency of abstractive summarization via question answering. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 6881-6894. Association for Computational Linguistics, 2021a. doi: 10.18653/v1/2021.acl-long. 536. URL https://doi.org/10.18653/v1/2021.acl-long.536.</p>
<p>Entity-level factual consistency of abstractive text summarization. Feng Nan, Ramesh Nallapati, Zhiguo Wang, Henghui Cicero Dos Santos, Dejiao Zhu, Kathleen Zhang, Bing Mckeown, Xiang, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeFeng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen Mckeown, and Bing Xiang. Entity-level factual consistency of abstractive text summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2727-2733, 2021b.</p>
<p>A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation. Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, Chin-Yew Lin, 10.18653/v1/P19-1256Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsFeng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and Chin-Yew Lin. A Simple Recipe towards Reduc- ing Hallucination in Neural Surface Realisation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2673-2679, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1256.</p>
<p>I like fish, especially dolphins: Addressing contradictions in dialogue modeling. Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, Jason Weston, 10.18653/v1/2021.acl-long.134Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Naviglithe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics12021Virtual EventYixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, and Jason Weston. I like fish, especially dolphins: Addressing contradictions in dialogue modeling. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 1699-1713. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.134. URL https://doi.org/10.18653/v1/2021.acl-long.134.</p>
<p>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Maxwell Nye, Michael Tessler, Josh Tenenbaum, M Brenden, Lake, Advances in Neural Information Processing Systems. 342021Maxwell Nye, Michael Tessler, Josh Tenenbaum, and Brenden M Lake. Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsArtidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. Understanding Factuality in Abstrac- tive Summarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 4812-4829, Online, June 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.naacl-main.383.</p>
<p>The proposition bank: An annotated corpus of semantic roles. Martha Palmer, Daniel Gildea, Paul Kingsbury, Computational linguistics. 311Martha Palmer, Daniel Gildea, and Paul Kingsbury. The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71-106, 2005.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318, 2002.</p>
<p>Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation. Matt Post, David Vilar, 10.18653/v1/N18-1119Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Long PapersMatt Post and David Vilar. Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1314-1324, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1119. URL https://aclanthology.org/N18-1119.</p>
<p>Data-to-text Generation with Entity Modeling. Ratish Puduppully, Li Dong, Mirella Lapata, 10.18653/v1/P19-1195Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsRatish Puduppully, Li Dong, and Mirella Lapata. Data-to-text Generation with Entity Modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2023-2035, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-1195.</p>
<p>Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin Choi, Jianfeng Gao, arXiv:1906.02738Conversing by reading: Contentful neural conversation with on-demand machine reading. arXiv preprintLianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin Choi, and Jianfeng Gao. Conversing by reading: Contentful neural conversation with on-demand machine reading. arXiv preprint arXiv:1906.02738, 2019.</p>
<p>Don't be contradicted with anything! ci-tod: Towards benchmarking consistency for task-oriented dialogue system. Libo Qin, Tianbao Xie, Shijue Huang, Qiguang Chen, Xiao Xu, Wanxiang Che, 10.18653/v1/2021.emnlp-main.182Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yihthe 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics20212021Virtual Event / Punta CanaLibo Qin, Tianbao Xie, Shijue Huang, Qiguang Chen, Xiao Xu, and Wanxiang Che. Don't be contradicted with anything! ci-tod: Towards benchmarking consistency for task-oriented dialogue system. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 2357- 2367. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.182. URL https://doi.org/10.18653/v1/2021.emnlp-main.182.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.</p>
<p>Aurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.</p>
<p>Increasing faithfulness in knowledge-grounded dialogue with controllable features. Hannah Rashkin, David Reitter, Gaurav Singh Tomar, Dipanjan Das, 10.18653/v1/2021.acl-long.58Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Naviglithe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics12021Virtual EventHannah Rashkin, David Reitter, Gaurav Singh Tomar, and Dipanjan Das. Increasing faithfulness in knowledge-grounded dialogue with controllable features. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 704-718. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.58. URL https://doi.org/10.18653/v1/2021.acl-long.58.</p>
<p>The curious case of hallucinations in neural machine translation. Vikas Raunak, Arul Menezes, Marcin Junczys-Dowmunt, arXiv:2104.06683arXiv preprintVikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. The curious case of hallucinations in neural machine translation. arXiv preprint arXiv:2104.06683, 2021.</p>
<p>PARENTing via Model-Agnostic Reinforcement Learning to Correct Pathological Behaviors in Data-to-Text Generation. Clement Rebuffel, Laure Soulier, Geoffrey Scoutheeten, Patrick Gallinari, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, IrelandAssociation for Computational LinguisticsClement Rebuffel, Laure Soulier, Geoffrey Scoutheeten, and Patrick Gallinari. PARENTing via Model- Agnostic Reinforcement Learning to Correct Pathological Behaviors in Data-to-Text Generation. In Proceedings of the 13th International Conference on Natural Language Generation, pages 120-130, Dublin, Ireland, 2020. Association for Computational Linguistics.</p>
<p>Controlling hallucinations at word level in data-to-text generation. Clement Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten, Rossella Cancelliere, Patrick Gallinari, 1573-756X. doi: 10.1007/ s10618-021-00801-4Data Mining and Knowledge Discovery. 361Clement Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten, Rossella Cancelliere, and Patrick Gallinari. Controlling hallucinations at word level in data-to-text generation. Data Mining and Knowledge Discovery, 36(1):318-354, 2022. ISSN 1573-756X. doi: 10.1007/ s10618-021-00801-4.</p>
<p>Supervised learning for fake news detection. C S Julio, André Reis, Fabrício Correia, Adriano Murai, Fabrício Veloso, Benevenuto, IEEE Intelligent Systems. 342Julio CS Reis, André Correia, Fabrício Murai, Adriano Veloso, and Fabrício Benevenuto. Supervised learning for fake news detection. IEEE Intelligent Systems, 34(2):76-81, 2019.</p>
<p>Object Hallucination in Image Captioning. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko, 10.18653/v1/D18-1437Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object Hallucination in Image Captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4035-4045, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1437. URL https://aclanthology. org/D18-1437.</p>
<p>Recipes for building an open-domain chatbot. Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, Jason Weston, 10.18653/v1/2021.eacl-main.24Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online. Paola Merlo, Jörg Tiedemann, and Reut Tsarfatythe 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, OnlineAssociation for Computational Linguistics2021Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-domain chatbot. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 -23, 2021, pages 300-325. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.eacl-main.24. URL https://doi.org/10.18653/ v1/2021.eacl-main.24.</p>
<p>Simalign: High quality word alignments without parallel training data using static and contextualized embeddings. Jalili Masoud, Philipp Sabet, François Dufter, Hinrich Yvon, Schütze, arXiv:2004.08728arXiv preprintMasoud Jalili Sabet, Philipp Dufter, François Yvon, and Hinrich Schütze. Simalign: High quality word alignments without parallel training data using static and contextualized embeddings. arXiv preprint arXiv:2004.08728, 2020.</p>
<p>Abstractive summarization with combination of pre-trained sequence-to-sequence and saliency models. Itsumi Saito, Kyosuke Nishida, Kosuke Nishida, Junji Tomita, arXiv:2003.13028arXiv preprintItsumi Saito, Kyosuke Nishida, Kosuke Nishida, and Junji Tomita. Abstractive summarization with combination of pre-trained sequence-to-sequence and saliency models. arXiv preprint arXiv:2003.13028, 2020.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang , arXiv:2103.12693arXiv: 2103.12693Summarization Asks for Fact-based Evaluation. Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. QuestEval: Summarization Asks for Fact-based Evalua- tion. arXiv:2103.12693 [cs], April 2021. URL http://arxiv.org/abs/2103.12693. arXiv: 2103.12693.</p>
<p>Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence. Xiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, Dietrich Klakow, Proceedings of the 58th. the 58thXiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, and Dietrich Klakow. Neural Data-to-Text Gen- eration via Jointly Learning the Segmentation and Correspondence. In Proceedings of the 58th</p>
<p>10.18653/v1/2020.acl-main.641Annual Meeting of the Association for Computational Linguistics. Online, 2020. Association for Computational LinguisticsAnnual Meeting of the Association for Computational Linguistics, pages 7155-7165, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.641.</p>
<p>Fake news detection on social media: A data mining perspective. Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, Huan Liu, ACM SIGKDD explorations newsletter. 191Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. Fake news detection on social media: A data mining perspective. ACM SIGKDD explorations newsletter, 19(1):22-36, 2017.</p>
<p>Beyond news contents: The role of social context for fake news detection. Kai Shu, Suhang Wang, Huan Liu, Proceedings of the twelfth ACM international conference on web search and data mining. the twelfth ACM international conference on web search and data miningKai Shu, Suhang Wang, and Huan Liu. Beyond news contents: The role of social context for fake news detection. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 312-320, 2019.</p>
<p>Retrieval augmentation reduces hallucination in conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, 10.18653/v1/2021.findings-emnlp.320Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau YihPunta Cana, Dominican RepublicAssociation for Computational Linguistics2021Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 3784-3803. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.320. URL https://doi.org/10.18653/v1/2021.findings-emnlp.320.</p>
<p>Profile Consistency Identification for Open-domain Dialogue Agents. Haoyu Song, Yan Wang, Wei-Nan Zhang, Zhengyu Zhao, Ting Liu, Xiaojiang Liu, 10.18653/v1/2020.emnlp-main.539Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsOnline, November 2020aHaoyu Song, Yan Wang, Wei-Nan Zhang, Zhengyu Zhao, Ting Liu, and Xiaojiang Liu. Profile Consis- tency Identification for Open-domain Dialogue Agents. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6651-6662, Online, Novem- ber 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.539. URL https://aclanthology.org/2020.emnlp-main.539.</p>
<p>Generate, delete and rewrite: A three-stage framework for improving persona consistency of dialogue generation. Haoyu Song, Yan Wang, Weinan Zhang, Xiaojiang Liu, Ting Liu, 10.18653/v1/2020.acl-main.516Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreaultthe 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics2020Haoyu Song, Yan Wang, Weinan Zhang, Xiaojiang Liu, and Ting Liu. Generate, delete and rewrite: A three-stage framework for improving persona consistency of dialogue generation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 5821- 5831. Association for Computational Linguistics, 2020b. doi: 10.18653/v1/2020.acl-main.516. URL https://doi.org/10.18653/v1/2020.acl-main.516.</p>
<p>Generating Persona Consistent Dialogues by Exploiting Natural Language Inference. Haoyu Song, Wei-Nan Zhang, Jingwen Hu, Ting Liu, 10.1609/aaai.v34i05.6417arXiv:1911.05889Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Haoyu Song, Wei-Nan Zhang, Jingwen Hu, and Ting Liu. Generating Persona Consistent Dialogues by Exploiting Natural Language Inference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8878-8885, April 2020c. ISSN 2374-3468, 2159-5399. doi: 10.1609/aaai. v34i05.6417. URL http://arxiv.org/abs/1911.05889. arXiv: 1911.05889.</p>
<p>A graph-to-sequence model for amr-to-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, arXiv:1805.02473arXiv preprintLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. A graph-to-sequence model for amr-to-text generation. arXiv preprint arXiv:1805.02473, 2018.</p>
<p>Yun-Zhu Song, Hong-Han Shuai, Sung-Lin Yeh, Yi-Lun Wu, Lun-Wei Ku, Wen-Chih Peng, arXiv:2002.02095arXiv: 2002.02095Attractive or Faithful? Popularity-Reinforced Learning for Inspired Headline Generation. Yun-Zhu Song, Hong-Han Shuai, Sung-Lin Yeh, Yi-Lun Wu, Lun-Wei Ku, and Wen-Chih Peng. Attractive or Faithful? Popularity-Reinforced Learning for Inspired Headline Genera- tion. arXiv:2002.02095 [cs], February 2020d. URL http://arxiv.org/abs/2002.02095. arXiv: 2002.02095.</p>
<p>. Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric P Xing, Zhiting Hu, arXiv:1905.11553Target-guided open-domain conversation. arXiv preprintJianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric P Xing, and Zhiting Hu. Target-guided open-domain conversation. arXiv preprint arXiv:1905.11553, 2019.</p>
<p>Xiangru Tang, Ziming Alexander R Fabbri, Griffin Mao, Borui Adams, Haoran Wang, Li, arXiv:2109.09195Yashar Mehdad, and Dragomir Radev. Investigating crowdsourcing protocols for evaluating the factual consistency of summaries. arXiv preprintXiangru Tang, Alexander R Fabbri, Ziming Mao, Griffin Adams, Borui Wang, Haoran Li, Yashar Mehdad, and Dragomir Radev. Investigating crowdsourcing protocols for evaluating the factual consistency of summaries. arXiv preprint arXiv:2109.09195, 2021.</p>
<p>Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation. Ran Tian, Shashi Narayan, Thibault Sellam, Ankur P Parikh, Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur P. Parikh. Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation. October 2019.</p>
<p>Modeling coverage for neural machine translation. Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li, arXiv:1601.04811arXiv preprintZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. Modeling coverage for neural machine translation. arXiv preprint arXiv:1601.04811, 2016.</p>
<p>Neural machine translation with reconstruction. Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, Hang Li, Thirty-First AAAI Conference on Artificial Intelligence. Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. Neural machine translation with reconstruction. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.10903Graph attention networks. arXiv preprintPetar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.</p>
<p>A neural conversational model. CoRR, abs/1506.05869. Oriol Vinyals, Quoc V Le, Oriol Vinyals and Quoc V. Le. A neural conversational model. CoRR, abs/1506.05869, 2015. URL http://arxiv.org/abs/1506.05869.</p>
<p>Show and tell: A neural image caption generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156-3164, 2015.</p>
<p>Asking and answering questions to evaluate the factual consistency of summaries. Alex Wang, Kyunghyun Cho, Mike Lewis, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel RAlex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.</p>
<p>Tetreault, 10.18653/v1/2020.acl-main.450Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics2020Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 5008-5020. Association for Computational Linguistics, 2020a. doi: 10.18653/v1/2020.acl-main.450. URL https://doi.org/10.18653/ v1/2020.acl-main.450.</p>
<p>On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation. Chaojun Wang, Rico Sennrich, 10.18653/v1/2020.acl-main.326Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsChaojun Wang and Rico Sennrich. On Exposure Bias, Hallucination and Domain Shift in Neural Ma- chine Translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3544-3552, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.326. URL https://aclanthology.org/2020.acl-main.326.</p>
<p>Revisiting Challenges in Data-to-Text Generation with Fact Grounding. Hongmin Wang, 10.18653/v1/W19-8639Proceedings of the 12th International Conference on Natural Language Generation. the 12th International Conference on Natural Language GenerationTokyo, JapanAssociation for Computational LinguisticsHongmin Wang. Revisiting Challenges in Data-to-Text Generation with Fact Grounding. In Proceed- ings of the 12th International Conference on Natural Language Generation, pages 311-322, Tokyo, Japan, October 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-8639.</p>
<p>Sketch and Refine: Towards Faithful and Informative Table-to-Text Generation. Peng Wang, Junyang Lin, An Yang, Chang Zhou, Yichang Zhang, Jingren Zhou, Hongxia Yang, 10.18653/v1/2021.findings-acl.427Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsPeng Wang, Junyang Lin, An Yang, Chang Zhou, Yichang Zhang, Jingren Zhou, and Hongxia Yang. Sketch and Refine: Towards Faithful and Informative Table-to-Text Generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4831-4843, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.427.</p>
<p>Describing a Knowledge Base. Qingyun Wang, Xiaoman Pan, Lifu Huang, Boliang Zhang, Zhiying Jiang, Ji Heng, Kevin Knight, 10.18653/v1/W18-6502Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational LinguisticsQingyun Wang, Xiaoman Pan, Lifu Huang, Boliang Zhang, Zhiying Jiang, Heng Ji, and Kevin Knight. Describing a Knowledge Base. In Proceedings of the 11th International Conference on Natural Language Generation, pages 10-21, Tilburg University, The Netherlands, 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6502.</p>
<p>Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints. Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, Changyou Chen, 10.18653/v1/2020.acl-main.101Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsOnline, 2020bZhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou Chen. Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1072-1086, Online, 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.101.</p>
<p>Non-monotonic sequential text generation. Sean Welleck, Kianté Brantley, Hal Daumé Iii, Kyunghyun Cho, International Conference on Machine Learning. PMLRSean Welleck, Kianté Brantley, Hal Daumé Iii, and Kyunghyun Cho. Non-monotonic sequential text generation. In International Conference on Machine Learning, pages 6716-6726. PMLR, 2019a.</p>
<p>Dialogue natural language inference. Sean Welleck, Jason Weston, Arthur Szlam, Kyunghyun Cho, 10.18653/v1/p19-1363Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Anna Korhonen, David R. Traum, and Lluís Màrquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyLong Papers1Association for Computational LinguisticsSean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. Dialogue natural language infer- ence. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3731-3741. Association for Computational Linguis- tics, 2019b. doi: 10.18653/v1/p19-1363. URL https://doi.org/10.18653/v1/p19-1363.</p>
<p>Dialogue Natural Language Inference. Sean Welleck, Jason Weston, Arthur Szlam, Kyunghyun Cho, 10.18653/v1/P19-1363Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsSean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. Dialogue Natural Language Infer- ence. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731-3741, Florence, Italy, July 2019c. Association for Computational Linguistics. doi: 10.18653/v1/P19-1363. URL https://aclanthology.org/P19-1363.</p>
<p>Rongxiang Weng, Shujian Huang, Zaixiang Zheng, Xinyu Dai, Jiajun Chen, arXiv:1708.01771Neural machine translation with word predictions. arXiv preprintRongxiang Weng, Shujian Huang, Zaixiang Zheng, Xinyu Dai, and Jiajun Chen. Neural machine translation with word predictions. arXiv preprint arXiv:1708.01771, 2017.</p>
<p>Gret: Global representation enhanced transformer. Rongxiang Weng, Haoran Wei, Shujian Huang, Heng Yu, Lidong Bing, Weihua Luo, Jiajun Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Rongxiang Weng, Haoran Wei, Shujian Huang, Heng Yu, Lidong Bing, Weihua Luo, and Jiajun Chen. Gret: Global representation enhanced transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9258-9265, 2020a.</p>
<p>Towards Enhancing Faithfulness for Neural Machine Translation. Rongxiang Weng, Heng Yu, Xiangpeng Wei, Weihua Luo, 10.18653/v1/2020.emnlp-main.212Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineAssociation for Computational LinguisticsRongxiang Weng, Heng Yu, Xiangpeng Wei, and Weihua Luo. Towards Enhancing Faithfulness for Neural Machine Translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2675-2684, Online, November 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.212. URL https://www. aclweb.org/anthology/2020.emnlp-main.212.</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R Bowman, 10.18653/v1/n18-1101Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. Marilyn A. Walker, Heng Ji, and Amanda Stentthe 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018New Orleans, Louisiana, USAAssociation for Computational Linguistics1Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Marilyn A. Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics, 2018. doi: 10.18653/v1/n18-1101. URL https://doi.org/10. 18653/v1/n18-1101.</p>
<p>A learning algorithm for continually running fully recurrent neural networks. J Ronald, David Williams, Zipser, Neural computation. 12Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270-280, 1989.</p>
<p>Challenges in Data-to-Document Generation. Sam Wiseman, Stuart Shieber, Alexander Rush, 10.18653/v1/D17-1239Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSam Wiseman, Stuart Shieber, and Alexander Rush. Challenges in Data-to-Document Generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1239.</p>
<p>Bass: Boosting abstractive summarization with unified semantic graph. Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, Haifeng Wang, Proceedings of the 59th. the 59thWenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, and Haifeng Wang. Bass: Boosting abstractive summarization with unified semantic graph. In Proceedings of the 59th</p>
<p>Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers1Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6052-6067, 2021a.</p>
<p>A controllable model of grounded response generation. Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski, Jianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf, Bill Dolan, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI PressZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel- Kedziorski, Jianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf, and Bill Dolan. A controllable model of grounded response generation. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 14085-14093. AAAI Press, 2021b. URL https: //ojs.aaai.org/index.php/AAAI/article/view/17658.</p>
<p>Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, Bolin Ding, arXiv:2108.13134arXiv: 2108.13134Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, and Bolin Ding. Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. arXiv:2108.13134 [cs], September 2021. URL http://arxiv.org/abs/2108.13134. arXiv: 2108.13134.</p>
<p>Reducing word omission errors in neural machine translation: A contrastive learning approach. Zonghan Yang, Yong Cheng, Yang Liu, Maosong Sun, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsZonghan Yang, Yong Cheng, Yang Liu, and Maosong Sun. Reducing word omission errors in neural machine translation: A contrastive learning approach. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6191-6196, 2019.</p>
<p>Textrunner: open information extraction on the web. Alexander Yates, Michele Banko, Matthew Broadhead, J Michael, Oren Cafarella, Stephen Etzioni, Soderland, Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT). Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)Alexander Yates, Michele Banko, Matthew Broadhead, Michael J Cafarella, Oren Etzioni, and Stephen Soderland. Textrunner: open information extraction on the web. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 25-26, 2007.</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 342021Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Wojciech Zaremba, arXiv:1409.2329Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprintWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.</p>
<p>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, arXiv:1905.12616arXiv: 1905.12616Defending Against Neural Fake News. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending Against Neural Fake News. arXiv:1905.12616 [cs], December 2020. URL http://arxiv.org/abs/1905.12616. arXiv: 1905.12616.</p>
<p>Parallel corpus filtering via pre-trained language models. Boliang Zhang, Ajay Nagesh, Kevin Knight, arXiv:2005.06166arXiv preprintBoliang Zhang, Ajay Nagesh, and Kevin Knight. Parallel corpus filtering via pre-trained language models. arXiv preprint arXiv:2005.06166, 2020a.</p>
<p>Neural machine translation with explicit phrase alignment. Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Yang Liu, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 29Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, and Yang Liu. Neural machine translation with explicit phrase alignment. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:1001-1010, 2021.</p>
<p>Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, International Conference on Machine Learning. PMLRJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328-11339. PMLR, 2020b.</p>
<p>Personalizing dialogue agents: I have a dog, do you have pets too?. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, 10.18653/v1/P18-1205Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018. the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018Melbourne, AustraliaAssociation for Computational Linguistics1Iryna Gurevych and Yusuke MiyaoSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: I have a dog, do you have pets too? In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2204-2213. Association for Computational Linguistics, 2018. doi: 10.18653/v1/P18-1205. URL https://aclanthology.org/P18-1205/.</p>
<p>An overview of online fake news: Characterization, detection, and discussion. Xichen Zhang, Ali A Ghorbani, Information Processing &amp; Management. 572102025Xichen Zhang and Ali A Ghorbani. An overview of online fake news: Characterization, detection, and discussion. Information Processing &amp; Management, 57(2):102025, 2020.</p>
<p>Dialogpt: Large-scale generative pre-training for conversational response generation. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, arXiv:1911.00536arXiv preprintYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536, 2019.</p>
<p>POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training. Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, Bill Dolan, arXiv:2005.00558arXiv: 2005.00558Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, and Bill Dolan. POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training. arXiv:2005.00558 [cs], September 2020c. URL http://arxiv.org/abs/2005.00558. arXiv: 2005.00558.</p>
<p>Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports. Yuhao Zhang, Derek Merck, Emily Bao Tsai, Christopher D Manning, Curtis P Langlotz, arXiv:1911.02541arXiv: 1911.02541Yuhao Zhang, Derek Merck, Emily Bao Tsai, Christopher D. Manning, and Curtis P. Langlotz. Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Re- ports. arXiv:1911.02541 [cs], April 2020d. URL http://arxiv.org/abs/1911.02541. arXiv: 1911.02541.</p>
<p>Reducing quantity hallucinations in abstractive summarization. Zheng Zhao, B Shay, Bonnie Cohen, Webber, Findings of the Association for Computational Linguistics: EMNLP 2020. Zheng Zhao, Shay B Cohen, and Bonnie Webber. Reducing quantity hallucinations in abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2237-2249, 2020a.</p>
<p>Reducing Quantity Hallucinations in Abstractive Summarization. Zheng Zhao, Shay B Cohen, Bonnie Webber, 10.18653/v1/2020.findings-emnlp.203Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsOnline, 2020bZheng Zhao, Shay B. Cohen, and Bonnie Webber. Reducing Quantity Hallucinations in Abstractive Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2237-2249, Online, 2020b. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.findings-emnlp.203. URL https://aclanthology.org/2020.findings-emnlp.203.</p>
<p>Dynamic past and future for neural machine translation. Zaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-Yu Dai, Jiajun Chen, arXiv:1904.09646arXiv preprintZaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-Yu Dai, and Jiajun Chen. Dynamic past and future for neural machine translation. arXiv preprint arXiv:1904.09646, 2019.</p>
<p>Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin, arXiv:2010.07475Neural deepfake detection with factual structure of text. arXiv preprintWanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Neural deepfake detection with factual structure of text. arXiv preprint arXiv:2010.07475, 2020.</p>
<p>Detecting Hallucinated Content in Conditional Neural Sequence Generation. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, Marjan Ghazvininejad, doi: 10.18653/ v1/2021.findings-acl.120Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting Hallucinated Content in Conditional Neural Sequence Generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393-1404, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.findings-acl.120. URL https://aclanthology.org/2021.findings-acl.120.</p>            </div>
        </div>

    </div>
</body>
</html>